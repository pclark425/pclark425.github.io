<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2630 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2630</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2630</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-6eed3c7589dbb2e3eaa1a963704366b05a7f79e6</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6eed3c7589dbb2e3eaa1a963704366b05a7f79e6" target="_blank">Parallel and Distributed Thompson Sampling for Large-scale Accelerated Exploration of Chemical Space</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a scalable solution based on a parallel and distributed implementation of Thompson sampling (PDTS), and shows that, in small scale problems, PDTS performs similarly as parallel expected improvement (EI), a batch version of the most widely used BO heuristic.</p>
                <p><strong>Paper Abstract:</strong> Chemical space is so large that brute force searches for new interesting molecules are infeasible. High-throughput virtual screening via computer cluster simulations can speed up the discovery process by collecting very large amounts of data in parallel, e.g., up to hundreds or thousands of parallel measurements. Bayesian optimization (BO) can produce additional acceleration by sequentially identifying the most useful simulations or experiments to be performed next. However, current BO methods cannot scale to the large numbers of parallel measurements and the massive libraries of molecules currently used in high-throughput screening. Here, we propose a scalable solution based on a parallel and distributed implementation of Thompson sampling (PDTS). We show that, in small scale problems, PDTS performs similarly as parallel expected improvement (EI), a batch version of the most widely used BO heuristic. Additionally, in settings where parallel EI does not scale, PDTS outperforms other scalable baselines such as a greedy search, $\epsilon$-greedy approaches and a random search method. These results show that PDTS is a successful solution for large-scale parallel BO.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2630.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2630.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PDTS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Parallel and Distributed Thompson Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A scalable, fully parallel implementation of Thompson sampling for batch Bayesian optimization that generates each batch entry by independently sampling a posterior model and selecting the maximum under that sample, enabling thousands of simultaneous evaluations with a single posterior update per batch.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Parallel and Distributed Thompson Sampling (PDTS)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>PDTS applies Thompson sampling to the parallel/batch BO setting by: (1) computing the posterior p(θ | D_I) once per batch; (2) for each of S parallel workers sampling θ ~ p(θ | D_I) and selecting the candidate x_j that maximizes E[y_j | x_j, θ] (i.e., the deterministic prediction under that posterior sample); (3) optionally returning ranked lists to a central node to avoid duplicate selections; (4) executing the S expensive evaluations in parallel and returning results to update the global posterior. This removes the need for iterative "fantasized" updates per batch entry and permits full distribution of selection and evaluation across compute nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>High-throughput virtual screening / chemical space exploration (materials and drug discovery); general expensive black-box optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates a fixed-size batch S each iteration. Allocation decisions are made by independently sampling model parameters from the current posterior and greedily selecting the highest predicted candidate under each sampled model; a central controller can de-duplicate by selecting the highest-ranked unseen candidate from each worker's ranked list. The strategy effectively samples from the posterior over maximizers so allocation probability matches posterior belief that a candidate is optimal.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Qualitative focus on number of posterior updates and acquisition-function (AF) optimizations per batch; PDTS requires one posterior update per batch (instead of S sequential updates), plus S AF optimizations performed in parallel. Cost dominated by expensive f evaluations; PDTS minimizes additional CPU/optimization overhead and wall-clock latency by parallelizing AF optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Not explicitly framed as information-gain. Acquisition uses utility U(y)=y (Thompson sampling); exploration arises from posterior sampling variance rather than explicit expected-information objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Exploration emerges from posterior sampling variability: each sampled θ induces a different sampled objective f, and greedy maximization under that sampled model yields diverse exploration; exploitation occurs because individual sampled models still prefer high predicted-y candidates. There is no tunable explicit exploration parameter—exploration is implicit in posterior uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Implicit: stochastic posterior sampling across S independent draws produces diverse candidate choices; a central node can request ranked lists and choose non-duplicated top candidates to ensure diversity across the batch.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed batch size (S) per iteration and large total candidate library; also practical compute-budget constraints (CPU-years) in high-throughput screening.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Handles budget by selecting S candidates in parallel with a single posterior update, minimizing per-batch computation overhead and enabling large batch sizes that respect wall-clock and compute-node constraints; central deduplication enforces effective use of each batch slot.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Empirical discovery metrics used in experiments: recall of top 1% of molecules in a library (fraction of ground-truth top-1% found among sampled candidates); for CEP dataset also threshold-based metric (PCE > 10%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>CEP: PDTS achieved ~20x higher recall than Monte Carlo baseline (reported qualitatively) and is estimated to reduce the CEP screening compute from 30,000 CPU-years to ~1,500 CPU-years when using BO guidance; Malaria & One-dose: PDTS finds ~70% of the top 1% by sampling ≈6,000 molecules (versus ≈14,000 by Monte Carlo).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against parallel expected improvement (parallel EI), sequential EI/TS, greedy (mean-only exploitation), ε-greedy (greedy + random fraction), and Monte Carlo (uniform random sampling).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>PDTS performed similarly to parallel EI on small-batch synthetic problems; outperforms scalable baselines (greedy, ε-greedy variants and random) on large-scale molecule data-sets. Example: PDTS located ~70% of top-1% with ~6,000 samples while Monte Carlo required ~14,000. For CEP PDTS had ~20× higher recall than Monte Carlo and projected a 20× reduction in CPU-years.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Reported improvements: up to ~20× (CEP recall and CPU-year reduction relative to Monte Carlo) and ~2.3× fewer samples to reach similar coverage in Malaria/One-dose (14,000 → 6,000).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper discusses that PDTS trades some exploitative focus for scalability and diversity: PDTS can be more explorative than EI (observed in Hartmann function experiments, where EI focused on one minimum while PDTS explored multiple equivalent minima and underperformed). PDTS reduces computational cost per batch (one posterior update) at the expense of sampling variability-driven exploration; in large-batch, high-throughput contexts this tradeoff favors PDTS due to infeasibility of sequential batch construction.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Key recommendation: for very large batch sizes and massive candidate libraries, sampling-based batch construction via independent posterior draws (PDTS) is preferable because it minimizes per-batch computational overhead while producing diverse, uncertainty-aware selections; sequential batch methods (parallel EI) are preferable when batch sizes are small and more exploitative, precise batch coordination is beneficial.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Parallel and Distributed Thompson Sampling for Large-scale Accelerated Exploration of Chemical Space', 'publication_date_yy_mm': '2017-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2630.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2630.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Parallel EI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Parallel Expected Improvement (Monte Carlo fantasized parallel EI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A batch extension of the Expected Improvement acquisition that sequentially builds a batch by conditioning on fantasized outcomes for pending evaluations and approximating the expected acquisition via Monte Carlo ("fantasies").</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Practical Bayesian optimization of machine learning algorithms</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Parallel Expected Improvement (parallel EI with fantasized data)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Parallel EI constructs batch entries sequentially: the first point is chosen by maximizing EI given current data; for later points, it marginalizes over possible outcomes of pending points by sampling fantasized data from the model predictive distribution, updating the posterior with each fantasy, and optimizing the EI conditioned on these fantasized outcomes. Snoek et al.'s Monte Carlo averaging of fantasies is used to approximate the expectation over pending outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General Bayesian optimization of expensive black-box functions; used in small-batch optimization and hyperparameter tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates batch entries sequentially, each new candidate chosen by maximizing the expected improvement averaged over sampled fantasized outcomes for earlier pending candidates; this aims to select batch members that jointly maximize expected improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Cost measured in number of posterior updates, number of fantasized samples, and optimization runs of the acquisition for each batch entry; incurs S sequential AF optimizations per batch and repeated model updates for fantasy conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected Improvement (EI): expected positive improvement over current best (E[max(0, y - y_*)]).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>EI balances exploration and exploitation through the expected improvement utility: candidates with high mean prediction (exploitation) and/or high predictive uncertainty (exploration) can have high EI.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Diversity across batch entries is induced by fantasized outcomes and the sequential conditioning: later selections take into account predicted outcomes of earlier selected points to avoid redundant choices.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed batch size per iteration; computational budget for repeated posterior updates and AF optimizations.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Handles budget by constructing a batch via sequential conditioning; however, this imposes high computational overhead when S is large and limits parallelizability because later batch points depend on earlier fantasy-conditioned updates.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Uses standard BO metrics such as immediate regret and best-found objective values; in molecule experiments, comparable recall metrics when used where applicable (in small-batch settings).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>On small-batch synthetic experiments parallel EI produced similar performance to PDTS in many problems; outperformed PDTS on Hartmann function (more exploitative behavior), but differences were often small. No absolute numeric advantage for large-batch, since parallel EI was not used in large-scale molecule experiments due to scalability limits.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to PDTS, sequential EI, TS and other parallel methods in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Parallel EI and PDTS gave similar results on several synthetic problems; parallel EI outperformed PDTS on Hartmann (likely because EI was more exploitative and Hartmann has multiple equivalent minima). Parallel EI is computationally infeasible for very large batch sizes used in high-throughput screening.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>No large-batch efficiency gain reported; parallel EI suffers high overhead as batch size grows, limiting wall-clock efficiency for large S.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper highlights a tradeoff: parallel EI can produce more focused (exploitative) batches and slightly better performance on certain multimodal objectives but incurs heavy sequential computational overhead and poor scalability; PDTS sacrifices some of that sequential coordination in favor of parallelism and scalability.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>For small batches, parallel EI is competitive and can be slightly better on some problems; for very large batches and massive candidate libraries, the sequential fantasized approach is impractical, so methods that avoid iterative posterior updates (e.g., PDTS) are recommended.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Parallel and Distributed Thompson Sampling for Large-scale Accelerated Exploration of Chemical Space', 'publication_date_yy_mm': '2017-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2630.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2630.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PPES</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Parallel Predictive Entropy Search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A parallel extension of Predictive Entropy Search (PES) that selects batches to maximize expected reduction in entropy of the posterior over the global optimizer, explicitly optimizing information gain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Parallel predictive entropy search for batch global optimization of expensive objective functions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Parallel Predictive Entropy Search (PPES)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>PPES extends PES to batch selection: it recommends a set of points whose joint evaluation is expected to produce the largest reduction in entropy (uncertainty) about the location of the global maximizer x*. It explicitly quantifies and optimizes information gain (entropy reduction) across the batch.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Batch Bayesian optimization for expensive black-box functions.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates batch by directly optimizing expected posterior entropy reduction for the posterior of the maximizer, prioritizing experiments that maximally reduce uncertainty about x*.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Computational cost arises from calculating expected entropy reduction over joint evaluations; cost measured by number of required Monte Carlo samples and joint-optimization complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected reduction in entropy of the posterior over the optimizer (mutual information between batch observations and x*).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Explicit information-based selection: points are chosen to maximally reduce uncertainty (exploration-oriented) but entropy criterion implicitly focuses on regions likely to contain high values (exploitation).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Joint optimization of batch to maximize information gain tends to select diverse points that together reduce posterior entropy.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed batch size; computational budget for entropy calculations.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Addresses budget by optimizing a fixed-size batch for maximal expected information gain, but at high computational expense for large batches.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Entropy reduction as proxy for improving probability of identifying global optimizer; objective improvement metrics used in BO evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned alongside other parallel BO methods (PDTS, parallel EI, q-KG).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Mentioned as an information-theoretic batch design; computational cost for joint entropy calculations limits scalability to large batch sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Explicit information-maximization yields principled batch choices but does not scale to the thousands-of-evaluations regime in high-throughput screening without approximation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Parallel and Distributed Thompson Sampling for Large-scale Accelerated Exploration of Chemical Space', 'publication_date_yy_mm': '2017-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2630.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2630.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>q-KG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Parallel Knowledge Gradient (q-KG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A batch generalization of the Knowledge Gradient acquisition that chooses q samples to maximize expected improvement in final solution quality after the batch, i.e., expected incremental solution quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The parallel knowledge gradient method for batch Bayesian optimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Parallel Knowledge Gradient (q-KG)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>q-KG computes, for a candidate batch of size q, the expected incremental improvement in the final chosen solution (the expected value of information of the batch) and selects the batch that maximizes this expected utility. It generalizes the single-step knowledge-gradient policy to the batch setting.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Batch Bayesian optimization for expensive black-box objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates batch by maximizing the expected one-step-ahead improvement in solution quality (expected value of information) resulting from observing the batch.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Cost measured by the complexity of computing the expected value of the batch (Monte Carlo integrals over joint outcomes) and the optimization over batch candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected incremental solution-quality (value of information) rather than entropy; captures expected improvement in final decision quality.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Balances exploration and exploitation by directly optimizing the expected downstream benefit of observations: points are chosen if they are expected to lead to large improvements in the final recommended solution.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Joint optimization of the batch encourages complementary points that together increase expected information/value.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed batch size; expensive computations for expected-value calculations.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Designed to choose the most valuable batch for a single-step horizon but computational costs can limit application to very large candidate sets or batch sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Expected improvement in final solution quality (scalar), typically measured via expected objective value of the post-batch recommendation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned as a competing batch BO method (compared in literature, not directly compared experimentally in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Method is principled for value-based allocation but has computational scaling limits for large q and large candidate libraries.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>q-KG is attractive when accurate expected-value calculations are tractable; for massive parallel evaluations PDTS-style sampling scales better.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Parallel and Distributed Thompson Sampling for Large-scale Accelerated Exploration of Chemical Space', 'publication_date_yy_mm': '2017-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2630.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2630.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GP-UCB-PE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gaussian Process Upper Confidence Bound with Pure Exploration (GP-UCB-PE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A parallel extension of GP-UCB where the first point in a batch is chosen by UCB and remaining points are chosen to maximize predictive variance (pure exploration) within a region likely containing the optimizer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Parallel Gaussian process optimization with upper confidence bound and pure exploration</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GP-UCB-PE</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Under this procedure the first batch point is selected using the standard UCB acquisition (exploitation+uncertainty bonus); the remaining batch points are chosen to maximize predictive variance (pure exploration), constrained to a region that has high probability of containing the optimizer, thus mixing exploitation and targeted exploration in the batch.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Parallel Bayesian optimization with Gaussian process models.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates batch slots by selecting one UCB point (exploitation with uncertainty penalty) and filling the rest with high-variance points to explore the search space efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Computation dominated by GP predictive variance evaluations and candidate maximization; cost scales with number of candidates and GP update complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Implicit: predictive variance used as a proxy for information gain (pure exploration points maximize variance).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Explicit two-part scheme: UCB for balance on first point, and pure predictive-variance maximization for remaining batch points to encourage exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Filling batch with high-variance points encourages diversity spatially across the input space.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed batch size; GP update costs.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Selects a mixed batch to maximize information per batch under GP computational constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Not directly specified in paper (method mentioned in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned in related work as an alternative parallel BO design.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Designed to trade-off exploitation (one UCB pick) with broad exploration (variance-based picks) to accelerate identification of the optimizer in parallel settings.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Selecting a hybrid batch (UCB + high-variance) helps parallelize exploration-exploitation tradeoffs under GP models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Parallel and Distributed Thompson Sampling for Large-scale Accelerated Exploration of Chemical Space', 'publication_date_yy_mm': '2017-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2630.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2630.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GP-BUCB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gaussian Process Batch UCB (GP-BUCB) / GP-BUCP (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A batch method that applies UCB-style acquisition in parallel by updating predictive variances appropriately for pending evaluations, enabling parallelization of GP-based exploration-exploitation tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Parallelizing exploration-exploitation tradeoffs in Gaussian process bandit optimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GP-BUCB (Parallel UCB-style batch GP)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>GP-BUCB extends the Upper Confidence Bound acquisition to the parallel setting by accounting for pending experiments: because GP predictive variance depends only on input locations, one can update variance estimates for pending points to select additional batch points using UCB while preserving theoretical properties.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Batch Bayesian optimization with Gaussian process models (bandit settings).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates batch points by applying UCB acquisition with adjusted variances to account for pending evaluations, prioritizing points with high upper confidence bounds.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Measured by GP update and predictive-variance computations; designed to reduce sequential posterior update costs by leveraging variance-only dependence.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>UCB uses a confidence-bound (mean + beta * std) that implicitly balances exploration (via std) and exploitation (via mean).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Explicit via the UCB acquisition: exploitation through predictive mean, exploration through predictive standard deviation weighted by a parameter.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Diversity arises from choosing multiple high-UCB inputs and from variance-only updated selection for pending locations.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed batch size; need for parallel decisions with minimal sequential overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Constructs batches without iterative posterior reconditioning by exploiting GP variance structure; reduces per-batch sequential computations.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Not specified in this paper (cited for related methodology).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned among parallel extensions of UCB and other BO heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>GP-BUCB aims to preserve exploration-exploitation tradeoffs in parallel while reducing sequential coordination costs for GP models.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Variance-only updates provide a tractable way to parallelize UCB-based selection with theoretical guarantees in GP settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Parallel and Distributed Thompson Sampling for Large-scale Accelerated Exploration of Chemical Space', 'publication_date_yy_mm': '2017-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2630.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2630.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SimulatedMatching</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simulated Matching (Batch Bayesian optimization via simulation matching)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A batch proposal method that aims to produce a batch whose elements collectively mimic the set of samples a sequential policy would have recommended, thereby approximating the performance of sequential policies with a parallel batch.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Batch Bayesian optimization via simulation matching</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Simulated Matching (Azimi et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The method generates a batch D_K intended to be a good match for the set of samples a sequential policy π would produce; a batch is considered good if it contains, with high probability, a sample with objective value close to the best sample that sequential π would produce. The approach uses simulation to match batch outputs to sequential behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Batch Bayesian optimization; general expensive-function optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates batch entries to approximate the behavior of an effective sequential policy, attempting to capture high-value sequentially-suggested candidates in a single parallel batch.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Cost arises from simulating sequential policy outcomes and matching those via batch selection; depends on number of simulations and matching optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Operational objective is closeness to best-sequential-sample value (probability of obtaining near-best sample), not explicit information-theoretic metric.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Implicit—by matching a sequential policy π (which may include EI/UCB/etc.), the batch inherits π's exploration-exploitation balance.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Diversity arises indirectly by matching diverse sequential recommendations; no explicit diversity-enforcing mechanism beyond what π would do.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed batch size; desire to emulate sequential budget in parallel.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Seeks to pack the batch with candidates that together approximate the sequential policy's outputs under the same budget.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Probability that batch contains a sample close to the best sequential sample; closeness in objective value.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned as a related batch-design approach in the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Simulated matching targets approximating sequential policies in parallel; computational overhead of simulation matching constrains scalability.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Useful when one wants batch outputs closely emulating a known good sequential policy, but may not scale to very large pools or batch sizes without approximation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Parallel and Distributed Thompson Sampling for Large-scale Accelerated Exploration of Chemical Space', 'publication_date_yy_mm': '2017-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2630.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2630.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ε-greedy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Epsilon-greedy sampling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple baseline that selects most batch members greedily by model mean predictions but replaces a fraction ε of samples with uniformly random candidates to inject exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning from delayed rewards</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ε-greedy (greedy with random fraction)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Each batch is formed by selecting the top-ranked candidates by predictive mean for a (1-ε) fraction of the batch and selecting ε fraction uniformly at random; ε is a tunable scalar (e.g., 0.01–0.075 in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Baseline for high-throughput screening / batch sampling in molecular discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Mostly exploitative allocation with a fixed fraction of random exploratory picks per batch controlled by ε.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Low computational overhead: ranking by predictive mean and uniform random draws; cost scales with candidate evaluation for predictions but is cheap compared to complex posterior conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>None explicit; exploration via random sampling, not by maximizing expected information or reduction of uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Explicit parameter ε trades exploration (random picks) vs exploitation (mean-based picks).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Diversity provided by uniform random selection for the ε fraction; otherwise no explicit diversity mechanism among greedy picks.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed batch size; requires choosing ε to match problem.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Heuristic: set ε and sample accordingly each batch. Does not adapt ε automatically—requires tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Recall of top 1% (molecule experiments) and average ranking across repeated simulated experiments (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 1 (average rank across experiments, lower is better): ε=0.01 -> 3.42±0.28; ε=0.025 -> 3.02±0.25; ε=0.05 -> 2.86±0.23; ε=0.075 -> 3.20±0.26. PDTS achieved average rank 2.51±0.20.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared experimentally against PDTS, greedy, and Monte Carlo random sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>All ε-greedy variants performed worse than PDTS in average rank; the best ε tried (0.05) still ranked worse than PDTS (2.86 vs PDTS 2.51).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>No positive efficiency gain over PDTS reported; ε-greedy requires tuning and did not outperform PDTS across tested ε values.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper notes that ε-greedy injects exploration but needs ε tuned to problem; PDTS automatically balances exploration and exploitation via posterior sampling without hand-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>ε-greedy is a simple practical baseline but suboptimal compared to PDTS, which provides automatic, posterior-driven exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Parallel and Distributed Thompson Sampling for Large-scale Accelerated Exploration of Chemical Space', 'publication_date_yy_mm': '2017-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2630.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2630.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Greedy/Random Baselines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Greedy (mean-only) and Monte Carlo random sampling baselines</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Greedy selects candidates by predictive mean only (pure exploitation); Monte Carlo selects uniformly at random (pure exploration baseline). Both are used as scalable baselines in large-batch virtual screening.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Greedy (mean-driven) and Monte Carlo random sampling</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Greedy: rank candidates by predictive mean from Bayesian neural network (PBP) and choose top batch entries—ignores predictive variance. Monte Carlo random: select batch uniformly at random from remaining candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>High-throughput virtual screening and molecular discovery baseline strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Greedy: allocate all budget to highest-mean predicted candidates (exploitative). Monte Carlo: allocate budget uniformly at random (no model guidance).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Low: computing predictive means (greedy) or uniform sampling (Monte Carlo); cost dominated by forward passes through the model for mean predictions in greedy.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>None explicit (greedy ignores uncertainty; Monte Carlo not information-driven).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Greedy is purely exploitative; Monte Carlo is purely exploratory but unguided.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Monte Carlo gives maximal diversity by random sampling; greedy may lead to concentrated sampling in a neighborhood and no explicit diversity mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed batch sizes and total number of samples.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Greedy and Monte Carlo are straightforward: fill batch slots by ranking or random draws; greedy can exhaust promising region early.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Recall of top 1% in molecule datasets; CEP PCE>10% threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Greedy: in CEP initially outperforms PDTS but eventually PDTS overtakes (no exact numeric curves provided). Monte Carlo: in CEP PDTS had ~20× higher recall versus Monte Carlo; in Malaria and One-dose Monte Carlo required ~14,000 samples to reach similar coverage that PDTS achieves with ~6,000 samples.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Used as direct baselines for PDTS in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>PDTS significantly outperforms Monte Carlo and overtakes greedy over longer runs; greedy initially finds good molecules quickly in CEP but exhausts local region and is overtaken by PDTS.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Example: Monte Carlo needed ~14,000 versus PDTS ~6,000 (≈2.3× more samples) in some datasets; CEP: PDTS projected to reduce CPU-years from 30,000 to 1,500 (≈20×).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Greedy is low-cost but risks local exhaustion; Monte Carlo is low-cost but inefficient; PDTS balances exploration and exploitation automatically and provides large efficiency gains in large discovery problems.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Paper supports model-guided batch allocation (PDTS) over naive baselines for high-throughput discovery when model uncertainty is meaningful.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Parallel and Distributed Thompson Sampling for Large-scale Accelerated Exploration of Chemical Space', 'publication_date_yy_mm': '2017-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Practical Bayesian optimization of machine learning algorithms <em>(Rating: 2)</em></li>
                <li>Parallel predictive entropy search for batch global optimization of expensive objective functions <em>(Rating: 2)</em></li>
                <li>The parallel knowledge gradient method for batch Bayesian optimization <em>(Rating: 2)</em></li>
                <li>Parallelizing exploration-exploitation tradeoffs in Gaussian process bandit optimization <em>(Rating: 2)</em></li>
                <li>Batch Bayesian optimization via simulation matching <em>(Rating: 2)</em></li>
                <li>Kriging is well-suited to parallelize optimization <em>(Rating: 1)</em></li>
                <li>Batch Bayesian optimization via local penalization <em>(Rating: 1)</em></li>
                <li>Predictive entropy search for efficient global optimization of black-box functions <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2630",
    "paper_id": "paper-6eed3c7589dbb2e3eaa1a963704366b05a7f79e6",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "PDTS",
            "name_full": "Parallel and Distributed Thompson Sampling",
            "brief_description": "A scalable, fully parallel implementation of Thompson sampling for batch Bayesian optimization that generates each batch entry by independently sampling a posterior model and selecting the maximum under that sample, enabling thousands of simultaneous evaluations with a single posterior update per batch.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Parallel and Distributed Thompson Sampling (PDTS)",
            "system_description": "PDTS applies Thompson sampling to the parallel/batch BO setting by: (1) computing the posterior p(θ | D_I) once per batch; (2) for each of S parallel workers sampling θ ~ p(θ | D_I) and selecting the candidate x_j that maximizes E[y_j | x_j, θ] (i.e., the deterministic prediction under that posterior sample); (3) optionally returning ranked lists to a central node to avoid duplicate selections; (4) executing the S expensive evaluations in parallel and returning results to update the global posterior. This removes the need for iterative \"fantasized\" updates per batch entry and permits full distribution of selection and evaluation across compute nodes.",
            "application_domain": "High-throughput virtual screening / chemical space exploration (materials and drug discovery); general expensive black-box optimization.",
            "resource_allocation_strategy": "Allocates a fixed-size batch S each iteration. Allocation decisions are made by independently sampling model parameters from the current posterior and greedily selecting the highest predicted candidate under each sampled model; a central controller can de-duplicate by selecting the highest-ranked unseen candidate from each worker's ranked list. The strategy effectively samples from the posterior over maximizers so allocation probability matches posterior belief that a candidate is optimal.",
            "computational_cost_metric": "Qualitative focus on number of posterior updates and acquisition-function (AF) optimizations per batch; PDTS requires one posterior update per batch (instead of S sequential updates), plus S AF optimizations performed in parallel. Cost dominated by expensive f evaluations; PDTS minimizes additional CPU/optimization overhead and wall-clock latency by parallelizing AF optimization.",
            "information_gain_metric": "Not explicitly framed as information-gain. Acquisition uses utility U(y)=y (Thompson sampling); exploration arises from posterior sampling variance rather than explicit expected-information objectives.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Exploration emerges from posterior sampling variability: each sampled θ induces a different sampled objective f, and greedy maximization under that sampled model yields diverse exploration; exploitation occurs because individual sampled models still prefer high predicted-y candidates. There is no tunable explicit exploration parameter—exploration is implicit in posterior uncertainty.",
            "diversity_mechanism": "Implicit: stochastic posterior sampling across S independent draws produces diverse candidate choices; a central node can request ranked lists and choose non-duplicated top candidates to ensure diversity across the batch.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed batch size (S) per iteration and large total candidate library; also practical compute-budget constraints (CPU-years) in high-throughput screening.",
            "budget_constraint_handling": "Handles budget by selecting S candidates in parallel with a single posterior update, minimizing per-batch computation overhead and enabling large batch sizes that respect wall-clock and compute-node constraints; central deduplication enforces effective use of each batch slot.",
            "breakthrough_discovery_metric": "Empirical discovery metrics used in experiments: recall of top 1% of molecules in a library (fraction of ground-truth top-1% found among sampled candidates); for CEP dataset also threshold-based metric (PCE &gt; 10%).",
            "performance_metrics": "CEP: PDTS achieved ~20x higher recall than Monte Carlo baseline (reported qualitatively) and is estimated to reduce the CEP screening compute from 30,000 CPU-years to ~1,500 CPU-years when using BO guidance; Malaria & One-dose: PDTS finds ~70% of the top 1% by sampling ≈6,000 molecules (versus ≈14,000 by Monte Carlo).",
            "comparison_baseline": "Compared against parallel expected improvement (parallel EI), sequential EI/TS, greedy (mean-only exploitation), ε-greedy (greedy + random fraction), and Monte Carlo (uniform random sampling).",
            "performance_vs_baseline": "PDTS performed similarly to parallel EI on small-batch synthetic problems; outperforms scalable baselines (greedy, ε-greedy variants and random) on large-scale molecule data-sets. Example: PDTS located ~70% of top-1% with ~6,000 samples while Monte Carlo required ~14,000. For CEP PDTS had ~20× higher recall than Monte Carlo and projected a 20× reduction in CPU-years.",
            "efficiency_gain": "Reported improvements: up to ~20× (CEP recall and CPU-year reduction relative to Monte Carlo) and ~2.3× fewer samples to reach similar coverage in Malaria/One-dose (14,000 → 6,000).",
            "tradeoff_analysis": "Paper discusses that PDTS trades some exploitative focus for scalability and diversity: PDTS can be more explorative than EI (observed in Hartmann function experiments, where EI focused on one minimum while PDTS explored multiple equivalent minima and underperformed). PDTS reduces computational cost per batch (one posterior update) at the expense of sampling variability-driven exploration; in large-batch, high-throughput contexts this tradeoff favors PDTS due to infeasibility of sequential batch construction.",
            "optimal_allocation_findings": "Key recommendation: for very large batch sizes and massive candidate libraries, sampling-based batch construction via independent posterior draws (PDTS) is preferable because it minimizes per-batch computational overhead while producing diverse, uncertainty-aware selections; sequential batch methods (parallel EI) are preferable when batch sizes are small and more exploitative, precise batch coordination is beneficial.",
            "uuid": "e2630.0",
            "source_info": {
                "paper_title": "Parallel and Distributed Thompson Sampling for Large-scale Accelerated Exploration of Chemical Space",
                "publication_date_yy_mm": "2017-06"
            }
        },
        {
            "name_short": "Parallel EI",
            "name_full": "Parallel Expected Improvement (Monte Carlo fantasized parallel EI)",
            "brief_description": "A batch extension of the Expected Improvement acquisition that sequentially builds a batch by conditioning on fantasized outcomes for pending evaluations and approximating the expected acquisition via Monte Carlo (\"fantasies\").",
            "citation_title": "Practical Bayesian optimization of machine learning algorithms",
            "mention_or_use": "use",
            "system_name": "Parallel Expected Improvement (parallel EI with fantasized data)",
            "system_description": "Parallel EI constructs batch entries sequentially: the first point is chosen by maximizing EI given current data; for later points, it marginalizes over possible outcomes of pending points by sampling fantasized data from the model predictive distribution, updating the posterior with each fantasy, and optimizing the EI conditioned on these fantasized outcomes. Snoek et al.'s Monte Carlo averaging of fantasies is used to approximate the expectation over pending outcomes.",
            "application_domain": "General Bayesian optimization of expensive black-box functions; used in small-batch optimization and hyperparameter tuning.",
            "resource_allocation_strategy": "Allocates batch entries sequentially, each new candidate chosen by maximizing the expected improvement averaged over sampled fantasized outcomes for earlier pending candidates; this aims to select batch members that jointly maximize expected improvement.",
            "computational_cost_metric": "Cost measured in number of posterior updates, number of fantasized samples, and optimization runs of the acquisition for each batch entry; incurs S sequential AF optimizations per batch and repeated model updates for fantasy conditioning.",
            "information_gain_metric": "Expected Improvement (EI): expected positive improvement over current best (E[max(0, y - y_*)]).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "EI balances exploration and exploitation through the expected improvement utility: candidates with high mean prediction (exploitation) and/or high predictive uncertainty (exploration) can have high EI.",
            "diversity_mechanism": "Diversity across batch entries is induced by fantasized outcomes and the sequential conditioning: later selections take into account predicted outcomes of earlier selected points to avoid redundant choices.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed batch size per iteration; computational budget for repeated posterior updates and AF optimizations.",
            "budget_constraint_handling": "Handles budget by constructing a batch via sequential conditioning; however, this imposes high computational overhead when S is large and limits parallelizability because later batch points depend on earlier fantasy-conditioned updates.",
            "breakthrough_discovery_metric": "Uses standard BO metrics such as immediate regret and best-found objective values; in molecule experiments, comparable recall metrics when used where applicable (in small-batch settings).",
            "performance_metrics": "On small-batch synthetic experiments parallel EI produced similar performance to PDTS in many problems; outperformed PDTS on Hartmann function (more exploitative behavior), but differences were often small. No absolute numeric advantage for large-batch, since parallel EI was not used in large-scale molecule experiments due to scalability limits.",
            "comparison_baseline": "Compared to PDTS, sequential EI, TS and other parallel methods in experiments.",
            "performance_vs_baseline": "Parallel EI and PDTS gave similar results on several synthetic problems; parallel EI outperformed PDTS on Hartmann (likely because EI was more exploitative and Hartmann has multiple equivalent minima). Parallel EI is computationally infeasible for very large batch sizes used in high-throughput screening.",
            "efficiency_gain": "No large-batch efficiency gain reported; parallel EI suffers high overhead as batch size grows, limiting wall-clock efficiency for large S.",
            "tradeoff_analysis": "Paper highlights a tradeoff: parallel EI can produce more focused (exploitative) batches and slightly better performance on certain multimodal objectives but incurs heavy sequential computational overhead and poor scalability; PDTS sacrifices some of that sequential coordination in favor of parallelism and scalability.",
            "optimal_allocation_findings": "For small batches, parallel EI is competitive and can be slightly better on some problems; for very large batches and massive candidate libraries, the sequential fantasized approach is impractical, so methods that avoid iterative posterior updates (e.g., PDTS) are recommended.",
            "uuid": "e2630.1",
            "source_info": {
                "paper_title": "Parallel and Distributed Thompson Sampling for Large-scale Accelerated Exploration of Chemical Space",
                "publication_date_yy_mm": "2017-06"
            }
        },
        {
            "name_short": "PPES",
            "name_full": "Parallel Predictive Entropy Search",
            "brief_description": "A parallel extension of Predictive Entropy Search (PES) that selects batches to maximize expected reduction in entropy of the posterior over the global optimizer, explicitly optimizing information gain.",
            "citation_title": "Parallel predictive entropy search for batch global optimization of expensive objective functions",
            "mention_or_use": "mention",
            "system_name": "Parallel Predictive Entropy Search (PPES)",
            "system_description": "PPES extends PES to batch selection: it recommends a set of points whose joint evaluation is expected to produce the largest reduction in entropy (uncertainty) about the location of the global maximizer x*. It explicitly quantifies and optimizes information gain (entropy reduction) across the batch.",
            "application_domain": "Batch Bayesian optimization for expensive black-box functions.",
            "resource_allocation_strategy": "Allocates batch by directly optimizing expected posterior entropy reduction for the posterior of the maximizer, prioritizing experiments that maximally reduce uncertainty about x*.",
            "computational_cost_metric": "Computational cost arises from calculating expected entropy reduction over joint evaluations; cost measured by number of required Monte Carlo samples and joint-optimization complexity.",
            "information_gain_metric": "Expected reduction in entropy of the posterior over the optimizer (mutual information between batch observations and x*).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Explicit information-based selection: points are chosen to maximally reduce uncertainty (exploration-oriented) but entropy criterion implicitly focuses on regions likely to contain high values (exploitation).",
            "diversity_mechanism": "Joint optimization of batch to maximize information gain tends to select diverse points that together reduce posterior entropy.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed batch size; computational budget for entropy calculations.",
            "budget_constraint_handling": "Addresses budget by optimizing a fixed-size batch for maximal expected information gain, but at high computational expense for large batches.",
            "breakthrough_discovery_metric": "Entropy reduction as proxy for improving probability of identifying global optimizer; objective improvement metrics used in BO evaluations.",
            "performance_metrics": null,
            "comparison_baseline": "Mentioned alongside other parallel BO methods (PDTS, parallel EI, q-KG).",
            "performance_vs_baseline": null,
            "efficiency_gain": null,
            "tradeoff_analysis": "Mentioned as an information-theoretic batch design; computational cost for joint entropy calculations limits scalability to large batch sizes.",
            "optimal_allocation_findings": "Explicit information-maximization yields principled batch choices but does not scale to the thousands-of-evaluations regime in high-throughput screening without approximation.",
            "uuid": "e2630.2",
            "source_info": {
                "paper_title": "Parallel and Distributed Thompson Sampling for Large-scale Accelerated Exploration of Chemical Space",
                "publication_date_yy_mm": "2017-06"
            }
        },
        {
            "name_short": "q-KG",
            "name_full": "Parallel Knowledge Gradient (q-KG)",
            "brief_description": "A batch generalization of the Knowledge Gradient acquisition that chooses q samples to maximize expected improvement in final solution quality after the batch, i.e., expected incremental solution quality.",
            "citation_title": "The parallel knowledge gradient method for batch Bayesian optimization",
            "mention_or_use": "mention",
            "system_name": "Parallel Knowledge Gradient (q-KG)",
            "system_description": "q-KG computes, for a candidate batch of size q, the expected incremental improvement in the final chosen solution (the expected value of information of the batch) and selects the batch that maximizes this expected utility. It generalizes the single-step knowledge-gradient policy to the batch setting.",
            "application_domain": "Batch Bayesian optimization for expensive black-box objectives.",
            "resource_allocation_strategy": "Allocates batch by maximizing the expected one-step-ahead improvement in solution quality (expected value of information) resulting from observing the batch.",
            "computational_cost_metric": "Cost measured by the complexity of computing the expected value of the batch (Monte Carlo integrals over joint outcomes) and the optimization over batch candidates.",
            "information_gain_metric": "Expected incremental solution-quality (value of information) rather than entropy; captures expected improvement in final decision quality.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Balances exploration and exploitation by directly optimizing the expected downstream benefit of observations: points are chosen if they are expected to lead to large improvements in the final recommended solution.",
            "diversity_mechanism": "Joint optimization of the batch encourages complementary points that together increase expected information/value.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed batch size; expensive computations for expected-value calculations.",
            "budget_constraint_handling": "Designed to choose the most valuable batch for a single-step horizon but computational costs can limit application to very large candidate sets or batch sizes.",
            "breakthrough_discovery_metric": "Expected improvement in final solution quality (scalar), typically measured via expected objective value of the post-batch recommendation.",
            "performance_metrics": null,
            "comparison_baseline": "Mentioned as a competing batch BO method (compared in literature, not directly compared experimentally in this paper).",
            "performance_vs_baseline": null,
            "efficiency_gain": null,
            "tradeoff_analysis": "Method is principled for value-based allocation but has computational scaling limits for large q and large candidate libraries.",
            "optimal_allocation_findings": "q-KG is attractive when accurate expected-value calculations are tractable; for massive parallel evaluations PDTS-style sampling scales better.",
            "uuid": "e2630.3",
            "source_info": {
                "paper_title": "Parallel and Distributed Thompson Sampling for Large-scale Accelerated Exploration of Chemical Space",
                "publication_date_yy_mm": "2017-06"
            }
        },
        {
            "name_short": "GP-UCB-PE",
            "name_full": "Gaussian Process Upper Confidence Bound with Pure Exploration (GP-UCB-PE)",
            "brief_description": "A parallel extension of GP-UCB where the first point in a batch is chosen by UCB and remaining points are chosen to maximize predictive variance (pure exploration) within a region likely containing the optimizer.",
            "citation_title": "Parallel Gaussian process optimization with upper confidence bound and pure exploration",
            "mention_or_use": "mention",
            "system_name": "GP-UCB-PE",
            "system_description": "Under this procedure the first batch point is selected using the standard UCB acquisition (exploitation+uncertainty bonus); the remaining batch points are chosen to maximize predictive variance (pure exploration), constrained to a region that has high probability of containing the optimizer, thus mixing exploitation and targeted exploration in the batch.",
            "application_domain": "Parallel Bayesian optimization with Gaussian process models.",
            "resource_allocation_strategy": "Allocates batch slots by selecting one UCB point (exploitation with uncertainty penalty) and filling the rest with high-variance points to explore the search space efficiently.",
            "computational_cost_metric": "Computation dominated by GP predictive variance evaluations and candidate maximization; cost scales with number of candidates and GP update complexity.",
            "information_gain_metric": "Implicit: predictive variance used as a proxy for information gain (pure exploration points maximize variance).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Explicit two-part scheme: UCB for balance on first point, and pure predictive-variance maximization for remaining batch points to encourage exploration.",
            "diversity_mechanism": "Filling batch with high-variance points encourages diversity spatially across the input space.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed batch size; GP update costs.",
            "budget_constraint_handling": "Selects a mixed batch to maximize information per batch under GP computational constraints.",
            "breakthrough_discovery_metric": "Not directly specified in paper (method mentioned in related work).",
            "performance_metrics": null,
            "comparison_baseline": "Mentioned in related work as an alternative parallel BO design.",
            "performance_vs_baseline": null,
            "efficiency_gain": null,
            "tradeoff_analysis": "Designed to trade-off exploitation (one UCB pick) with broad exploration (variance-based picks) to accelerate identification of the optimizer in parallel settings.",
            "optimal_allocation_findings": "Selecting a hybrid batch (UCB + high-variance) helps parallelize exploration-exploitation tradeoffs under GP models.",
            "uuid": "e2630.4",
            "source_info": {
                "paper_title": "Parallel and Distributed Thompson Sampling for Large-scale Accelerated Exploration of Chemical Space",
                "publication_date_yy_mm": "2017-06"
            }
        },
        {
            "name_short": "GP-BUCB",
            "name_full": "Gaussian Process Batch UCB (GP-BUCB) / GP-BUCP (as cited)",
            "brief_description": "A batch method that applies UCB-style acquisition in parallel by updating predictive variances appropriately for pending evaluations, enabling parallelization of GP-based exploration-exploitation tradeoffs.",
            "citation_title": "Parallelizing exploration-exploitation tradeoffs in Gaussian process bandit optimization",
            "mention_or_use": "mention",
            "system_name": "GP-BUCB (Parallel UCB-style batch GP)",
            "system_description": "GP-BUCB extends the Upper Confidence Bound acquisition to the parallel setting by accounting for pending experiments: because GP predictive variance depends only on input locations, one can update variance estimates for pending points to select additional batch points using UCB while preserving theoretical properties.",
            "application_domain": "Batch Bayesian optimization with Gaussian process models (bandit settings).",
            "resource_allocation_strategy": "Allocates batch points by applying UCB acquisition with adjusted variances to account for pending evaluations, prioritizing points with high upper confidence bounds.",
            "computational_cost_metric": "Measured by GP update and predictive-variance computations; designed to reduce sequential posterior update costs by leveraging variance-only dependence.",
            "information_gain_metric": "UCB uses a confidence-bound (mean + beta * std) that implicitly balances exploration (via std) and exploitation (via mean).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Explicit via the UCB acquisition: exploitation through predictive mean, exploration through predictive standard deviation weighted by a parameter.",
            "diversity_mechanism": "Diversity arises from choosing multiple high-UCB inputs and from variance-only updated selection for pending locations.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed batch size; need for parallel decisions with minimal sequential overhead.",
            "budget_constraint_handling": "Constructs batches without iterative posterior reconditioning by exploiting GP variance structure; reduces per-batch sequential computations.",
            "breakthrough_discovery_metric": "Not specified in this paper (cited for related methodology).",
            "performance_metrics": null,
            "comparison_baseline": "Mentioned among parallel extensions of UCB and other BO heuristics.",
            "performance_vs_baseline": null,
            "efficiency_gain": null,
            "tradeoff_analysis": "GP-BUCB aims to preserve exploration-exploitation tradeoffs in parallel while reducing sequential coordination costs for GP models.",
            "optimal_allocation_findings": "Variance-only updates provide a tractable way to parallelize UCB-based selection with theoretical guarantees in GP settings.",
            "uuid": "e2630.5",
            "source_info": {
                "paper_title": "Parallel and Distributed Thompson Sampling for Large-scale Accelerated Exploration of Chemical Space",
                "publication_date_yy_mm": "2017-06"
            }
        },
        {
            "name_short": "SimulatedMatching",
            "name_full": "Simulated Matching (Batch Bayesian optimization via simulation matching)",
            "brief_description": "A batch proposal method that aims to produce a batch whose elements collectively mimic the set of samples a sequential policy would have recommended, thereby approximating the performance of sequential policies with a parallel batch.",
            "citation_title": "Batch Bayesian optimization via simulation matching",
            "mention_or_use": "mention",
            "system_name": "Simulated Matching (Azimi et al.)",
            "system_description": "The method generates a batch D_K intended to be a good match for the set of samples a sequential policy π would produce; a batch is considered good if it contains, with high probability, a sample with objective value close to the best sample that sequential π would produce. The approach uses simulation to match batch outputs to sequential behavior.",
            "application_domain": "Batch Bayesian optimization; general expensive-function optimization.",
            "resource_allocation_strategy": "Allocates batch entries to approximate the behavior of an effective sequential policy, attempting to capture high-value sequentially-suggested candidates in a single parallel batch.",
            "computational_cost_metric": "Cost arises from simulating sequential policy outcomes and matching those via batch selection; depends on number of simulations and matching optimization.",
            "information_gain_metric": "Operational objective is closeness to best-sequential-sample value (probability of obtaining near-best sample), not explicit information-theoretic metric.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Implicit—by matching a sequential policy π (which may include EI/UCB/etc.), the batch inherits π's exploration-exploitation balance.",
            "diversity_mechanism": "Diversity arises indirectly by matching diverse sequential recommendations; no explicit diversity-enforcing mechanism beyond what π would do.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Fixed batch size; desire to emulate sequential budget in parallel.",
            "budget_constraint_handling": "Seeks to pack the batch with candidates that together approximate the sequential policy's outputs under the same budget.",
            "breakthrough_discovery_metric": "Probability that batch contains a sample close to the best sequential sample; closeness in objective value.",
            "performance_metrics": null,
            "comparison_baseline": "Mentioned as a related batch-design approach in the literature.",
            "performance_vs_baseline": null,
            "efficiency_gain": null,
            "tradeoff_analysis": "Simulated matching targets approximating sequential policies in parallel; computational overhead of simulation matching constrains scalability.",
            "optimal_allocation_findings": "Useful when one wants batch outputs closely emulating a known good sequential policy, but may not scale to very large pools or batch sizes without approximation.",
            "uuid": "e2630.6",
            "source_info": {
                "paper_title": "Parallel and Distributed Thompson Sampling for Large-scale Accelerated Exploration of Chemical Space",
                "publication_date_yy_mm": "2017-06"
            }
        },
        {
            "name_short": "ε-greedy",
            "name_full": "Epsilon-greedy sampling",
            "brief_description": "A simple baseline that selects most batch members greedily by model mean predictions but replaces a fraction ε of samples with uniformly random candidates to inject exploration.",
            "citation_title": "Learning from delayed rewards",
            "mention_or_use": "use",
            "system_name": "ε-greedy (greedy with random fraction)",
            "system_description": "Each batch is formed by selecting the top-ranked candidates by predictive mean for a (1-ε) fraction of the batch and selecting ε fraction uniformly at random; ε is a tunable scalar (e.g., 0.01–0.075 in experiments).",
            "application_domain": "Baseline for high-throughput screening / batch sampling in molecular discovery.",
            "resource_allocation_strategy": "Mostly exploitative allocation with a fixed fraction of random exploratory picks per batch controlled by ε.",
            "computational_cost_metric": "Low computational overhead: ranking by predictive mean and uniform random draws; cost scales with candidate evaluation for predictions but is cheap compared to complex posterior conditioning.",
            "information_gain_metric": "None explicit; exploration via random sampling, not by maximizing expected information or reduction of uncertainty.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Explicit parameter ε trades exploration (random picks) vs exploitation (mean-based picks).",
            "diversity_mechanism": "Diversity provided by uniform random selection for the ε fraction; otherwise no explicit diversity mechanism among greedy picks.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed batch size; requires choosing ε to match problem.",
            "budget_constraint_handling": "Heuristic: set ε and sample accordingly each batch. Does not adapt ε automatically—requires tuning.",
            "breakthrough_discovery_metric": "Recall of top 1% (molecule experiments) and average ranking across repeated simulated experiments (Table 1).",
            "performance_metrics": "Table 1 (average rank across experiments, lower is better): ε=0.01 -&gt; 3.42±0.28; ε=0.025 -&gt; 3.02±0.25; ε=0.05 -&gt; 2.86±0.23; ε=0.075 -&gt; 3.20±0.26. PDTS achieved average rank 2.51±0.20.",
            "comparison_baseline": "Compared experimentally against PDTS, greedy, and Monte Carlo random sampling.",
            "performance_vs_baseline": "All ε-greedy variants performed worse than PDTS in average rank; the best ε tried (0.05) still ranked worse than PDTS (2.86 vs PDTS 2.51).",
            "efficiency_gain": "No positive efficiency gain over PDTS reported; ε-greedy requires tuning and did not outperform PDTS across tested ε values.",
            "tradeoff_analysis": "Paper notes that ε-greedy injects exploration but needs ε tuned to problem; PDTS automatically balances exploration and exploitation via posterior sampling without hand-tuning.",
            "optimal_allocation_findings": "ε-greedy is a simple practical baseline but suboptimal compared to PDTS, which provides automatic, posterior-driven exploration.",
            "uuid": "e2630.7",
            "source_info": {
                "paper_title": "Parallel and Distributed Thompson Sampling for Large-scale Accelerated Exploration of Chemical Space",
                "publication_date_yy_mm": "2017-06"
            }
        },
        {
            "name_short": "Greedy/Random Baselines",
            "name_full": "Greedy (mean-only) and Monte Carlo random sampling baselines",
            "brief_description": "Greedy selects candidates by predictive mean only (pure exploitation); Monte Carlo selects uniformly at random (pure exploration baseline). Both are used as scalable baselines in large-batch virtual screening.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Greedy (mean-driven) and Monte Carlo random sampling",
            "system_description": "Greedy: rank candidates by predictive mean from Bayesian neural network (PBP) and choose top batch entries—ignores predictive variance. Monte Carlo random: select batch uniformly at random from remaining candidates.",
            "application_domain": "High-throughput virtual screening and molecular discovery baseline strategies.",
            "resource_allocation_strategy": "Greedy: allocate all budget to highest-mean predicted candidates (exploitative). Monte Carlo: allocate budget uniformly at random (no model guidance).",
            "computational_cost_metric": "Low: computing predictive means (greedy) or uniform sampling (Monte Carlo); cost dominated by forward passes through the model for mean predictions in greedy.",
            "information_gain_metric": "None explicit (greedy ignores uncertainty; Monte Carlo not information-driven).",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Greedy is purely exploitative; Monte Carlo is purely exploratory but unguided.",
            "diversity_mechanism": "Monte Carlo gives maximal diversity by random sampling; greedy may lead to concentrated sampling in a neighborhood and no explicit diversity mechanism.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Fixed batch sizes and total number of samples.",
            "budget_constraint_handling": "Greedy and Monte Carlo are straightforward: fill batch slots by ranking or random draws; greedy can exhaust promising region early.",
            "breakthrough_discovery_metric": "Recall of top 1% in molecule datasets; CEP PCE&gt;10% threshold.",
            "performance_metrics": "Greedy: in CEP initially outperforms PDTS but eventually PDTS overtakes (no exact numeric curves provided). Monte Carlo: in CEP PDTS had ~20× higher recall versus Monte Carlo; in Malaria and One-dose Monte Carlo required ~14,000 samples to reach similar coverage that PDTS achieves with ~6,000 samples.",
            "comparison_baseline": "Used as direct baselines for PDTS in experiments.",
            "performance_vs_baseline": "PDTS significantly outperforms Monte Carlo and overtakes greedy over longer runs; greedy initially finds good molecules quickly in CEP but exhausts local region and is overtaken by PDTS.",
            "efficiency_gain": "Example: Monte Carlo needed ~14,000 versus PDTS ~6,000 (≈2.3× more samples) in some datasets; CEP: PDTS projected to reduce CPU-years from 30,000 to 1,500 (≈20×).",
            "tradeoff_analysis": "Greedy is low-cost but risks local exhaustion; Monte Carlo is low-cost but inefficient; PDTS balances exploration and exploitation automatically and provides large efficiency gains in large discovery problems.",
            "optimal_allocation_findings": "Paper supports model-guided batch allocation (PDTS) over naive baselines for high-throughput discovery when model uncertainty is meaningful.",
            "uuid": "e2630.8",
            "source_info": {
                "paper_title": "Parallel and Distributed Thompson Sampling for Large-scale Accelerated Exploration of Chemical Space",
                "publication_date_yy_mm": "2017-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Practical Bayesian optimization of machine learning algorithms",
            "rating": 2
        },
        {
            "paper_title": "Parallel predictive entropy search for batch global optimization of expensive objective functions",
            "rating": 2
        },
        {
            "paper_title": "The parallel knowledge gradient method for batch Bayesian optimization",
            "rating": 2
        },
        {
            "paper_title": "Parallelizing exploration-exploitation tradeoffs in Gaussian process bandit optimization",
            "rating": 2
        },
        {
            "paper_title": "Batch Bayesian optimization via simulation matching",
            "rating": 2
        },
        {
            "paper_title": "Kriging is well-suited to parallelize optimization",
            "rating": 1
        },
        {
            "paper_title": "Batch Bayesian optimization via local penalization",
            "rating": 1
        },
        {
            "paper_title": "Predictive entropy search for efficient global optimization of black-box functions",
            "rating": 1
        }
    ],
    "cost": 0.023157999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Parallel and Distributed Thompson Sampling for Large-scale Accelerated Exploration of Chemical Space</h1>
<p>José Miguel Hernández-Lobato ${ }^{<em> 1}$ James Requeima ${ }^{</em> 12}$ Edward O. Pyzer-Knapp ${ }^{34}$ Alán Aspuru-Guzik ${ }^{3}$</p>
<h4>Abstract</h4>
<p>Chemical space is so large that brute force searches for new interesting molecules are infeasible. High-throughput virtual screening via computer cluster simulations can speed up the discovery process by collecting very large amounts of data in parallel, e.g., up to hundreds or thousands of parallel measurements. Bayesian optimization (BO) can produce additional acceleration by sequentially identifying the most useful simulations or experiments to be performed next. However, current BO methods cannot scale to the large numbers of parallel measurements and the massive libraries of molecules currently used in high-throughput screening. Here, we propose a scalable solution based on a parallel and distributed implementation of Thompson sampling (PDTS). We show that, in small scale problems, PDTS performs similarly as parallel expected improvement (EI), a batch version of the most widely used BO heuristic. Additionally, in settings where parallel EI does not scale, PDTS outperforms other scalable baselines such as a greedy search, $\epsilon$-greedy approaches and a random search method. These results show that PDTS is a successful solution for large-scale parallel BO.</p>
<h2>1. Introduction</h2>
<p>Chemical space is huge: it is estimated to contain over $10^{60}$ molecules. Among these, fewer than 100 million compounds can be found in public repositories or databases (Reymond et al., 2012). This discrepancy between known</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>compounds and possible compounds indicates the potential for discovering many new compounds with highly desirable functionality (e.g., new energy materials, pharmaceuticals, dyes, etc.). While the vast size of chemical space makes this an enormous opportunity, it also presents a significant difficulty in the identification of new relevant compounds among the many unimportant ones. This challenge is so great that any discovery process relying purely on the combination of scientific intuition with trial and error experimentation is slow, tedious and in many cases infeasible.</p>
<p>To accelerate the search, high-throughput approaches can be used in a combinatorial exploration of small specific areas of chemical space (Rajan, 2008). These have led to the development of high-throughput virtual screening (Pyzer-Knapp et al., 2015; Gómez-Bombarelli et al., 2016) in which large libraries of molecules are created and then analyzed using theoretical and computational techniques, typically by running a large number of parallel simulations in a computer cluster. The objective is to reduce an initially very large library of molecules to a small set of promising leads for which expensive experimental evaluation is justified. However, even though these techniques only search a tiny drop in the ocean of chemical space, they can result in massive libraries whose magnitude exceeds traditional computational capabilities. As a result, at present, there is an urgent need to accelerate high-throughput screening approaches.</p>
<p>Bayesian optimization (BO) (Jones et al., 1998) can speed up the discovery process by using machine learning to guide the search and make improved decisions about what molecules to analyze next given the data collected so far. However, current BO methods cannot scale to the large number of parallel measurements and the massive libraries of candidate molecules currently used in high-throughput screening (Pyzer-Knapp et al., 2015). While there are BO methods that allow parallel data collection, these methods have typically been limited to tens of data points per batch (Snoek et al., 2012; Shahriari et al., 2014; González et al., 2016). In contrast, high-throughput screening may allow the simultaneous collection of thousands of data points via large-scale parallel computation. This creates a need for new scalable methods for parallel Bayesian optimization.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Illustration of Thompson sampling and PDTS.
To address the above difficulty, we present here a scalable solution for parallel Bayesian optimization based on a distributed implementation of the Thompson sampling heuristic (Thompson, 1933; Chapelle \&amp; Li, 2011). We show that, for the case of small batch sizes, the proposed parallel and distributed Thompson sampling (PDTS) method performs as well as a parallel implementation of expected improvement (EI) (Snoek et al., 2012; Ginsbourger et al., 2011), the most widely used Bayesian optimization heuristic. Parallel EI selects the batch entries sequentially and so EI proposals can't be parallelized, which limits its scalability properties. PDTS generates each batch of evaluation locations by selecting the different batch entries independently and in parallel. Consequently, PDTS is highly scalable and applicable to large batch sizes. We also evaluate the performance of PDTS in several real-world high-throughput screening experiments for material and drug discovery, where parallel EI is infeasible. In these problems, PDTS outperforms other scalable baselines such as a greedy search strategy, $\epsilon$-greedy approaches and a random search method. These results indicate that PDTS is a successful solution for largescale parallel Bayesian optimization.</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 Sequential Thompson sampling
    Input: initial data \(\mathcal{D}_{\mathcal{I}(1)}=\left\{\left(\mathbf{x}_{i}, y_{i}\right)\right\}_{i \in \mathcal{I}(1)}\)
    for \(t=1\) to \(T\) do
        Compute current posterior \(p\left(\boldsymbol{\theta} \mid \mathcal{D}_{\mathcal{I}(t)}\right)\)
        Sample \(\boldsymbol{\theta}\) from \(p\left(\boldsymbol{\theta} \mid \mathcal{D}_{\mathcal{I}(t)}\right)\)
        Select \(k \leftarrow \operatorname{argmax}_{j \notin \mathcal{I}(t)} \mathbf{E}\left[y_{j} \mid \mathbf{x}_{j}, \boldsymbol{\theta}\right]\)
        Collect \(y_{k}\) by evaluating \(f\) at \(\mathbf{x}_{k}\)
        \(\mathcal{D}_{\mathcal{I}(t+1)} \leftarrow \mathcal{D}_{\mathcal{I}(t)} \cup\left\{\left(\mathbf{x}_{k}, y_{k}\right)\right\}\)
    end for
</code></pre></div>

<h2>2. BO and Thompson Sampling</h2>
<p>Let us assume we have a large library of candidate molecules $\mathcal{M}=\left{m_{1}, \ldots, m_{|\mathcal{M}|}\right}$. Our goal is to identify a small subset of elements $\left{m_{i}\right} \subset \mathcal{M}$ for which the
$f\left(m_{i}\right)$ are as high as possible, with $f$ being an expensive-to-evaluate objective function. The objective $f$ could be, for example, an estimate of the power-conversion efficiency of organic photovoltaics, as given by expensive quantum mechanical simulations (Scharber et al., 2006), and we may want to identify the top $1 \%$ elements in $\mathcal{M}$ according to this score.</p>
<p>Bayesian optimization methods can be used to identify the inputs that maximize an expensive objective function $f$ by performing only a reduced number of function evaluations. For this, BO uses a model to make predictions for the value of $f$ at new inputs given data from previous evaluations. The next point to evaluate is then chosen by maximizing an acquisition function that quantifies the benefit of evaluating the objective at a particular location.</p>
<p>Let $\mathbf{x}<em _mathcal_M="|\mathcal{M">{1}, \ldots, \mathbf{x}</em>}|}$ be $D$-dimensional feature vectors for the molecules in $\mathcal{M}$ and let $\mathcal{D<em i="i">{\mathcal{I}}=\left{\left(\mathbf{x}</em>}, y_{i}\right): i \in I\right}$ be a dataset with information about past evaluations, where $I$ is a set with the indices of the molecules already evaluated, $\mathbf{x<em i="i">{i}$ is the feature vector for the $i$-th molecule in $\mathcal{M}$ and $y</em>}=f\left(m_{i}\right)$ is the result of evaluating the objective function $f$ on that molecule. We assume that the evaluations of $f$ are noise free, however, the methods described here can be applied to the case in which the objective evaluations are corrupted with additive Gaussian noise. BO typically uses a probabilistic model to describe how the $y_{i}$ in $\mathcal{D<em i="i">{\mathcal{I}}$ are generated as a function of the corresponding features $\mathbf{x}</em>}$ and some model parameters $\boldsymbol{\theta}$, that is, the model specifies $p\left(y_{i} \mid \mathbf{x<em _mathcal_I="\mathcal{I">{i}, \boldsymbol{\theta}\right)$. Given the data $\mathcal{D}</em>}}$ and a prior distribution $p(\boldsymbol{\theta})$, the model also specifies a posterior distribution $p\left(\boldsymbol{\theta} \mid \mathcal{D<em I="I" _in="\in" i="i">{\mathcal{I}}\right) \propto p(\boldsymbol{\theta}) \prod</em>} p\left(y_{i} \mid \mathbf{x<em j="j">{i}, \boldsymbol{\theta}\right)$. The predictive distribution for any $m</em>} \in \mathcal{M} \backslash\left{m_{i}: i \in I\right}$ is then given by $p\left(y_{j} \mid \mathbf{x<em _mathcal_I="\mathcal{I">{j}, \mathcal{D}</em>}}\right)=\int p\left(y_{j} \mid \mathbf{x<em _mathcal_I="\mathcal{I">{j}, \boldsymbol{\theta}\right) p\left(\boldsymbol{\theta} \mid \mathcal{D}</em>$. BO methods use this predictive distribution to compute an acquisition function (AF) given by}}\right) d \boldsymbol{\theta</p>
<p>$$
\alpha\left(\mathbf{x}<em _mathcal_I="\mathcal{I">{j} \mid \mathcal{D}</em>}}\right)=\mathbf{E<em j="j">{p\left(y</em>} \mid \mathbf{x<em _mathcal_I="\mathcal{I">{j}, \mathcal{D}</em>}}\right)}\left[U\left(y_{j} \mid \mathbf{x<em _mathcal_I="\mathcal{I">{j}, \mathcal{D}</em>\right)\right]
$$}</p>
<p>where $U\left(y_{j} \mid \mathbf{x}<em _mathcal_I="\mathcal{I">{j}, \mathcal{D}</em>}}\right)$ is the utility of obtaining value $y_{j}$ when evaluating $f$ at $m_{j}$. Eq. (1) is then maximized with respect to $j \notin I$ to select the next molecule $m_{j}$ on which to evaluate $f$. The most common choice for the utility is the improvement: $U\left(y_{j} \mid \mathbf{x<em _mathcal_I="\mathcal{I">{j}, \mathcal{D}</em>}}\right)=\max \left(0, y_{j}-y_{\star}\right)$, where $y_{\star}$ is equal to the best $y_{i}$ in $\mathcal{D<em j="j">{\mathcal{I}}$. In this case, Eq. (1) is called the expected improvement (EI) (Jones et al., 1998). Ideally, the AF should encourage both exploration and exploitation. For this, the expected utility should increase when $y</em>$ (to explore). The EI utility function satisfies these two requirements.}$ takes high values on average (to exploit), but also when there is high uncertainty about $y_{j</p>
<p>Thompson sampling (TS) (Thompson, 1933) can be understood as a version of the previous framework in which the utility function is defined as $U\left(y_{j} \mid \mathbf{x}<em _mathcal_I="\mathcal{I">{j}, \mathcal{D}</em>\right)$}}\right)=y_{j}$ and the expectation in (1) is taken with respect to $p\left(y_{j} \mid \mathbf{x}_{j}, \boldsymbol{\theta</p>
<p>instead of $p\left(y_{j} \mid \mathbf{x}<em _mathcal_I="\mathcal{I">{j}, \mathcal{D}</em>}}\right)$, with $\boldsymbol{\theta}$ being a sample from the posterior $p\left(\boldsymbol{\theta} \mid \mathcal{D<em j="j">{\mathcal{I}}\right)$. That is, when computing the AF, TS approximates the integral in $p\left(y</em>} \mid \mathbf{x<em _mathcal_I="\mathcal{I">{j}, \mathcal{D}</em>}}\right)=$ $\int p\left(y_{j} \mid \mathbf{x<em _mathcal_I="\mathcal{I">{j}, \boldsymbol{\theta}\right) p\left(\boldsymbol{\theta} \mid \mathcal{D}</em>}}\right) d \boldsymbol{\theta}$ by Monte Carlo, using a single sample from $p\left(\boldsymbol{\theta} \mid \mathcal{D<em j="j">{\mathcal{I}}\right)$ in the approximation. The TS utility function enforces only exploitation because the expected utility is insensitive to any variance in $y</em>}$. Despite this, TS still enforces exploration because of the variance produced by the Monte Carlo approximation to $p\left(y_{j} \mid \mathbf{x<em _mathcal_I="\mathcal{I">{j}, \mathcal{D}</em>}}\right)$. Under TS, the probability of evaluating the objective at a particular location matches the probability of that location being the maximizer of the objective, given the model assumptions and the data from past evaluations. Algorithm 1 contains the pseudocode for TS. The plots in the top of Figure 1 illustrate how TS works. The top-left plot shows several samples from a posterior distribution on $f$ induced by $p\left(\boldsymbol{\theta} \mid \mathcal{D<em _mathcal_I="\mathcal{I">{\mathcal{I}}\right)$ since each value of the parameters $\boldsymbol{\theta}$ corresponds to an associated value of $f$. Sampling from $p\left(\boldsymbol{\theta} \mid \mathcal{D}</em>\right)$ is then equivalent to selecting one of these samples for $f$. The selected sample represents the current AF, which is optimized in the top-right plot in Figure 1 to select the next evaluation.}</p>
<h3>2.1. Parallel BO</h3>
<p>So far we have considered the sequential evaluation setting, where BO methods collect just a single data point in each iteration. However, BO can also be applied in the parallel setting, which involves choosing a batch of multiple points to evaluate next in each iteration. For example, when we run $S$ parallel simulations in a computer cluster and each simulation performs one evaluation of $f$.</p>
<p>Snoek et al. (2012) describe how to extend sequential BO methods to the parallel setting. The idea is to select the first evaluation location in the batch in the same way as in the sequential setting. However, the next evaluation location is then selected while the previous one is still pending. In particular, given a set $K$ with indexes of pending evaluation locations, we choose a new location in the batch based on the expectation of the AF under all possible outcomes of the pending evaluations according to the predictions of the model. Therefore, at any point, the next evaluation location is obtained by optimizing the AF</p>
<p>$$
\begin{aligned}
&amp; \alpha_{\text {parallel }}\left(\mathbf{x}<em _mathcal_I="\mathcal{I">{j} \mid \mathcal{D}</em>\right)= \
&amp; \quad \mathbf{E}}}, \mathcal{K<em k="k">{p\left(\left{y</em>\right}<em k="k">{k \in \mathcal{K}} \mid\left{\mathbf{x}</em>\right}<em _mathcal_I="\mathcal{I">{k \in \mathcal{K}}, \mathcal{D}</em>}}\right)}\left[\alpha\left(\mathbf{x<em _mathcal_I="\mathcal{I">{j} \mid \mathcal{D}</em>\right)\right]
\end{aligned}
$$}} \cup \mathcal{D}_{\mathcal{K}</p>
<p>where $\mathcal{D}<em k="k">{\mathcal{K}}=\left{\left(y</em>}, \mathbf{x<em _in="\in" _mathcal_K="\mathcal{K" k="k">{k}\right)\right}</em>}}$ and $\alpha\left(\mathbf{x<em _mathcal_I="\mathcal{I">{j} \mid \mathcal{D}</em>}} \cup \mathcal{D<em k="k">{\mathcal{K}}\right)$ is given by (1). Computing this expression exactly is infeasible in most cases. Snoek et al. (2012) propose a Monte Carlo approximation in which the expectation in the second line is approximated by averaging across a few samples from the predictive distribution at the pending evaluations, that is, $p\left(\left{y</em>\right}<em k="k">{k \in \mathcal{K}} \mid\left{\mathbf{x}</em>\right}<em _mathcal_I="\mathcal{I">{k \in \mathcal{K}}, \mathcal{D}</em>\right)$. These samples are referred to as fantasized data.}</p>
<p>This approach for parallel BO has been successfully used
to collect small batches of data (about 10 elements in size), with EI as utility function and with a Gaussian process as the model for the data (Snoek et al., 2012). However, it lacks scalability to large batch sizes, failing when we need to collect thousands of simultaneous measurements. The reason for this is the high computational cost of adding a new evaluation to the current batch. The corresponding cost includes: (1) sampling the fantasized data, (2) updating the posterior predictive distribution to $p\left(y_{j} \mid \mathbf{x}<em _mathcal_I="\mathcal{I">{j}, \mathcal{D}</em>}} \cup \mathcal{D<em j="j">{\mathcal{K}}\right)$, which is required for evaluating $\alpha\left(\mathbf{x}</em>} \mid \mathcal{D<em _mathcal_K="\mathcal{K">{\mathcal{I}} \cup \mathcal{D}</em>$ is very large (e.g., when it contains millions of elements) and among all the remaining molecules we have to find one that maximizes the AF.}}\right)$, and (3) optimizing the Monte Carlo approximation to (2). Step (2) can be very expensive when the number of training points in $\mathcal{D}_{\mathcal{I}}$ is very large. This step is also considerably challenging when the model does not allow for exact inference, as it is often the case with Bayesian neural networks. Step (3) can also take a very long time when the library of candidate molecules $\mathcal{M</p>
<p>Despite these difficulties, the biggest disadvantage in this approach for parallel BO is that it cannot be parallelized since it is a sequential process in which (2) needs to be iteratively optimized, with each optimization step having a direct effect on the next one. This prevents this method from fully exploiting the acceleration provided by multiple processors in a computer cluster. The sequential nature of the algorithm is illustrated by the plot in the left of Figure 2. In this plot computer node 1 is controlling the BO process and decides the batch evaluation locations. Nodes $2, \ldots, 5$ then perform the evaluations in parallel. Note that steps (2) and (3) from the above description have been highlighted in green and magenta colors.</p>
<p>In the following section we describe an algorithm for batch BO which can be implemented in a fully parallel and distributed manner and which, consequently, can take full advantage of multiple processors in a computer cluster. This novel method is based on a parallel implementation of the Thompson sampling heuristic.</p>
<p>```
Algorithm 2 Parallel and distributed Thompson sampling
    Input: initial data (\mathcal{D}<em i="i">{\mathcal{I}(1)}=\left{\mathbf{x}</em>\right}}, y_{i<em _mathcal_I="\mathcal{I">{i \in \mathcal{I}(1)), batch size (S)
    for (t=1) to (T) do
        Compute current posterior (p\left(\boldsymbol{\theta} \mid \mathcal{D}</em>\right))
        for (s=1) to (S) do
            Sample (\boldsymbol{\theta}) from (p\left(\boldsymbol{\theta} \mid \mathcal{D}}(t)<em _mathcal_I="\mathcal{I" _notin="\notin" j="j">{\mathcal{I}(t)}\right))
            Select (k(s) \leftarrow \operatorname{argmax}</em>}(t)} \mathbf{E}\left[y_{j} \mid \mathbf{x<em k_s_="k(s)">{j}, \boldsymbol{\theta}\right] \mid) of (s)
            Collect (y</em>_{k(s) \ldots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .}) by evaluating (f) at (\mathbf{x</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. A visualization of one iteration of BO using parallel EI as implemented in (Snoek et al., 2012) and PDTS. Note that in PDTS the model is updated once and sample points are acquired independently by the nodes. With parallel EI, the the location of the next sample points is dependent on the location of previous sample points in the batch so these are computed sequentially.</p>
<h2>3. Parallel and Distributed Thompson Sampling</h2>
<p>We present an implementation of the parallel BO method from Section 2.1 based on the Thompson sampling (TS) heuristic. In particular, we propose to apply to (2) the same approximation that TS applied to (1). For this, we choose in (2) the same utility function used by TS in the sequential setting, that is, $U\left(y_{j} \mid \mathbf{x}<em _mathcal_I="\mathcal{I">{j}, \mathcal{D}</em>}} \cup \mathcal{D<em j="j">{\mathcal{K}}\right)=y</em>\right}}$. Then, we approximate the expectation with respect to $\left{y_{k<em k="k">{k \in \mathcal{K}}$ in (2) by Monte Carlo, averaging across just one sample of $\left{y</em>\right}<em k="k">{k \in \mathcal{K}}$ drawn from $p\left(\left{y</em>\right}<em k="k">{k \in \mathcal{K}} \mid\left{\mathbf{x}</em>\right}<em _mathcal_I="\mathcal{I">{k \in \mathcal{K}}, \mathcal{D}</em>}}\right)$. After that, $\alpha\left(\mathbf{x<em _mathcal_I="\mathcal{I">{j} \mid \mathcal{D}</em>}} \cup \mathcal{D<em _mathcal_I="\mathcal{I">{\mathcal{K}}\right)$ in (2) is approximated in the same way as in the sequential setting by first sampling $\boldsymbol{\theta}$ from $p\left(\boldsymbol{\theta} \mid \mathcal{D}</em>}} \cup \mathcal{D<em j="j">{\mathcal{K}}\right)$ and then approximating $p\left(y</em>} \mid \mathbf{x<em _mathcal_I="\mathcal{I">{j}, \mathcal{D}</em>}} \cup \mathcal{D<em j="j">{\mathcal{K}}\right)$ with $p\left(y</em>} \mid \mathbf{x<em k="k">{j}, \boldsymbol{\theta}\right)$. Importantly, in this process, sampling first $\left{y</em>\right}<em k="k">{k \in \mathcal{K}}$ from $p\left(\left{y</em>\right}<em k="k">{k \in \mathcal{K}} \mid\left{\mathbf{x}</em>\right}<em _mathcal_I="\mathcal{I">{k \in \mathcal{K}}, \mathcal{D}</em>}}\right)$ and then $\boldsymbol{\theta}$ from $p\left(\boldsymbol{\theta} \mid \mathcal{D<em _mathcal_K="\mathcal{K">{\mathcal{I}} \cup \mathcal{D}</em>}}\right)$ is equivalent to sampling $\boldsymbol{\theta}$ from just $p\left(\boldsymbol{\theta} \mid \mathcal{D<em _mathcal_I="\mathcal{I">{\mathcal{I}}\right)$. The reason for this is that updating a posterior distribution with synthetic data sampled from the model's predictive distribution produces on average the same initial posterior distribution. The result is that parallel TS with batch size $S$ is the same as running sequential TS $S$ times without updating the current posterior $p\left(\boldsymbol{\theta} \mid \mathcal{D}</em>\right)$, where each execution of sequential TS produces one of the evaluation locations in the batch. Importantly, these executions can be done in distributed manner, with each one running in parallel in a different node.}</p>
<p>The resulting parallel and distributed TS (PDTS) method is highly scalable and can be applied to very large batch sizes by running each execution of sequential TS on the same computer node that will then later evaluate $f$ at the selected evaluation location. Algorithm 2 contains the pseudocode for PDTS. The parallel nature of the algorithm is illustrated by the plot in the right of Figure 2. In this plot computer node 1 is controlling the BO process. To collect four new function evaluations in parallel, computer node 1 sends the current posterior $p\left(\boldsymbol{\theta} \mid \mathcal{D}<em j="j">{\mathcal{I}}\right)$ and $\mathcal{I}$ to nodes $2, \ldots, 5$. Each of them samples then a value for $\boldsymbol{\theta}$ from the posterior and optimizes its own AF given by $\mathbf{E}\left[y</em>$. The objective function is evaluated at the selected input and the resulting data is sent back to node 1. Figure 1 illustrates
how PDTS selects two parallel evaluation locations. For this, sequential TS is run twice.} \mid \mathbf{x}_{j}, \boldsymbol{\theta}\right]$, with $j \notin \mathcal{I</p>
<p>The scalability of PDTS makes it a promising method for parallel BO in high-throughput screening. However, in this type of problem, the optimization of the AF is done over a discrete set of molecules. Therefore, whenever we collect a batch of data in parallel with PDTS, several of the simultaneous executions of sequential TS may choose to evaluate the same molecule. A central computer node (e.g. the node controlling the BO process) maintaining a list of molecules currently selected for evaluation can be used to avoid this problem. In this case, each sequential TS node sends to the central node a ranked list with the top $S$ (the batch size) molecules according to its AF. From this list, the central node then selects the highest ranked molecule that has not been selected for evaluation before.</p>
<h2>4. Related Work</h2>
<p>Ginsbourger et al. (Ginsbourger et al., 2010) proposed the following framework for parallel BO: given a set of current observations $\mathcal{D}<em k="k">{\mathcal{I}}$ and pending experiments $\left{\mathbf{x}</em>\right}<em _mathcal_K="\mathcal{K">{k=1}^{K}$, an additional set of fantasies $\mathcal{D}</em>}}=\left{\left(\mathbf{x<em k="k">{k}, y</em>\right)\right}<em _mathcal_I="\mathcal{I">{k=1}^{K}$ can be assumed to be the result of those pending experiments. A step of Bayesian optimization can then be performed using the augmented dataset $\mathcal{D}</em>}} \cup \mathcal{D<em _mathcal_I="\mathcal{I">{\mathcal{K}}$ and the acquisition function $\alpha\left(\mathbf{x} \mid \mathcal{D}</em>}} \cup \mathcal{D<em k="k">{\mathcal{K}}\right)$. Two different values are proposed for the fantasies: the constant liar, where $y</em>$.}=L$ for some constant $L$ and all $k=1 \ldots \mathcal{K}$, and the Kriging believer, where $y_{k}$ is given by the GP predictive mean at $\mathbf{x}_{k</p>
<p>Snoek et al. (2012) compute a Monte Carlo approximation of the expected acquisition function over potential fantasies sampled from the model's predictive distribution. Recent methods have been proposed to modify the parallel EI procedure to recommend points jointly (Chevalier \&amp; Ginsbourger, 2013; Marmin et al., 2015; Wang et al., 2016).</p>
<p>Azimi et al. (2010) describe a procedure called simulated matching whose goal is to propose a batch $\mathcal{D}_{\mathcal{K}}$ of points which is a good match for the set of samples that a sequential BO policy $\pi$ would recommend. The authors consider a batch "good" if it contains a sample that yields, with high</p>
<p>probability, an objective value close to that of the best sample produced by a sequential execution of $\pi$.</p>
<p>Several authors have proposed to extend the upper confidence bound (UCB) heuristic to the parallel setting. Since the GP predictive variance depends only on the input location of the observations, Desautels et al. (2014) propose GP-BUCP acquisition which uses the UCB acquisition with this updated variance. Contal et al. (2013) introduce the Gaussian Process Upper Confidence Bound with Pure Exploration (GP-UCB-PE). Under this procedure, the first point is obtained using the standard UCB acquisition function while the remaining points are sequentially selected to be the ones yielding the highest predictive variance, while still lying in a region that contains the maximizer with high probability.</p>
<p>Shah \&amp; Ghahramani (2015) extend the Predictive Entropy Search (PES) heuristic to the parallel setting (PPES). PPES seeks to recommend a collection of samples $\mathcal{D}_{\mathcal{K}}$ that yields the greatest reduction in entropy for the posterior distribution of $\mathbf{x}^{*}$, the latent objective maximizer. Wu \&amp; Frazier (2016) propose the Parallel Knowledge Gradient Method which optimizes an acquisition function called the parallel knowledge gradient (q-KG), a measure of the expected incremental solution quality after $q$ samples.</p>
<p>An advantage of PDTS over parallel EI and other related methods is that the approximate marginalization of potential experimental outcomes adds no extra computational cost to our procedure and so PDTS is highly parallelizable. Finally, unlike other approaches, PDTS can be applied to a wide variety of models, such as GPs and Bayesian neural networks, since it only requires samples from an exact or approximate posterior distribution.</p>
<h2>5. Bayesian Neural Networks for High-throughput Screening</h2>
<p>Neural networks are well-suited for implementing BO on molecules. They produce state-of-the-art predictions of chemical properties (Ma et al., 2015; Mayr et al., 2016; Ramsundar et al., 2015) and can be applied to large data sets by using stochastic optimization (Bousquet \&amp; Bottou, 2008). Typical applications of neural networks focus on the deterministic prediction scenario. However, in large search spaces with multiple local optima (which is the case when navigating chemical space), it is desirable to use a probabilistic approach that can produce accurate estimates of uncertainty for efficient exploration and so, we use probabilistic back-propagation (PBP), a recently-developed technique for the scalable training of Bayesian neural networks (Hernández-Lobato \&amp; Adams, 2015). Note that other methods for approximate inference in Bayesian neural networks could have been chosen as well (Blundell et al., 2015; Snoek et al., 2015; Gal
\&amp; Ghahramani, 2016). We prefer PBP because it is fast and it does not require the tuning of hyper-parameters such as learning rates or regularization constants (HernándezLobato \&amp; Adams, 2015).</p>
<p>Given a dataset $\mathcal{D}<em i="i">{\mathcal{I}}=\left{\left(\mathbf{x}</em>\right)\right}}, y_{i<em i="i">{i \in \mathcal{I}}$, we assume that $y</em>}=f\left(\mathbf{x<em i="i">{i} ; \mathcal{W}\right)+\epsilon</em>}$, where $f(\cdot ; \mathcal{W})$ is the output of a neural network with weights $\mathcal{W}$. The network output is corrupted with additive noise variables $\epsilon_{i} \sim \mathcal{N}\left(0, \gamma^{-1}\right)$. The network has $L$ layers, with $V_{l}$ hidden units in layer $l$, and $\mathcal{W}=\left{\mathbf{W<em l="1">{l}\right}</em>+1\right)$ synaptic weight matrices. The +1 is introduced here to account for the additional per-layer biases. The activation functions for the hidden layers are rectifiers: $\varphi(x)=\max (x, 0)$.}^{L}$ is the collection of $V_{l} \times\left(V_{l-1</p>
<p>The likelihood for the network weights $\mathcal{W}$ and the noise precision $\gamma$ is</p>
<p>$$
p\left(\left{y_{i}\right}<em i="i">{i \in|\mathcal{I}|} \mathcal{W},\left{\mathbf{x}</em>\right}<em _in="\in" _mathcal_I="\mathcal{I" i="i">{i \in \mathcal{I}}, \gamma\right)=\prod</em>\right)
$$}} \mathcal{N}\left(y_{i} \mid f\left(\mathbf{x}_{i} ; \mathcal{W}\right), \gamma^{-1</p>
<p>We specify a Gaussian prior distribution for each entry in each of the weight matrices in $\mathcal{W}$ :</p>
<p>$$
p(\mathcal{W} \mid \lambda)=\prod_{l=1}^{L} \prod_{k=1}^{V_{l}} \prod_{j=1}^{V_{l-1}+1} \mathcal{N}\left(w_{k j, l} \mid 0, \lambda^{-1}\right)
$$</p>
<p>where $w_{k j, l}$ is the entry in the $k$-th row and $j$-th column of $\mathbf{W}<em 0="0">{l}$ and $\lambda$ is a precision parameter. The hyper-prior for $\lambda$ is gamma: $p(\lambda)=\operatorname{Gam}\left(\lambda \mid \alpha</em>=6$.}^{\lambda}, \beta_{0}^{\lambda}\right)$ with shape $\alpha_{0}^{\lambda}=6$ and inverse scale $\beta_{0}^{\lambda}=6$. This relatively low value for the shape and inverse scale parameters makes this prior weakly-informative. The prior for the noise precision $\gamma$ is also gamma: $p(\gamma)=\operatorname{Gam}\left(\gamma \mid \alpha_{0}^{\gamma}, \beta_{0}^{\gamma}\right)$. We assume that the $y_{i}$ have been normalized to have unit variance and, as above, we fix $\alpha_{0}^{\gamma}=6$ and $\beta_{0}^{\gamma</p>
<p>The exact computation of the posterior distribution for the model parameters $p(\mathcal{W}, \gamma, \lambda \mid \mathcal{D}_{\mathcal{I}})$ is not tractable in most cases. PBP approximates the intractable posterior on $\mathcal{W}, \gamma$ and $\lambda$ with the tractable approximation</p>
<p>$$
\begin{aligned}
q(\mathcal{W}, \gamma, \lambda)= &amp; {\left[\prod_{l=1}^{L} \prod_{k=1}^{V_{l}} \prod_{j=1}^{V_{l-1}+1} \mathcal{N}\left(w_{k j, l} \mid m_{k j, l}, v_{k j, l}\right)\right] } \
&amp; \operatorname{Gam}\left(\gamma \mid \alpha^{\gamma}, \beta^{\gamma}\right) \operatorname{Gam}\left(\lambda \mid \alpha^{\lambda}, \beta^{\lambda}\right)
\end{aligned}
$$</p>
<p>whose parameters are tuned by iteratively running an assumed density filtering (ADF) algorithm over the training data (Opper, 1998). The main operation in PBP is the update of the mean and variance parameters of $q$, that is, the $m_{k j, l}$ and $v_{k j, l}$ in (4), after processing each data point $\left{\left(\mathbf{x}<em i="i">{i}, y</em>\right)$. The matching of moments for the distributions on the weights is achieved}\right)\right}$. For this, PBP matches moments between the new $q$ and the product of the old $q$ with the corresponding likelihood factor $\mathcal{N}\left(y_{i} \mid f\left(\mathbf{x}_{i} ; \mathcal{W}\right), \gamma^{-1</p>
<p>by using well-known Gaussian ADF updates, see equations 5.12 and 5.1 in (Minka, 2001).</p>
<p>To compute the ADF updates, PBP finds a Gaussian approximation to the distribution of the network output $f\left(\mathbf{x}<em i="i">{i} ; \mathcal{W}\right)$ when $\mathcal{W} \sim q$. This is achieved by doing a forward pass of $\mathbf{x}</em>$ being randomly sampled from $q$. In this forward pass the non-Gaussian distributions followed by the output of the neurons are approximated with Gaussians that have the same means and variances as the original distributions. This is a Gaussian approximation by moment matching. We refer the reader to Hernández-Lobato \&amp; Adams (2015) for full details on PBP.}$ through the network, with the weights $\mathcal{W</p>
<p>After several ADF iterations over the data by PBP, we can then make predictions for the unknown target variable $y_{<em>}$ associated with a new feature vector $\mathbf{x}_{</em>}$. For this, we obtain a Gaussian approximation to $f\left(\mathbf{x}_{*} ; \mathcal{W}\right)$ when $\mathcal{W} \sim q$ by applying the forward pass process described above.</p>
<p>To implement TS, as described in Algorithm 1, we first sample the model parameters $\boldsymbol{\theta}$ from the posterior $p(\boldsymbol{\theta} \mid \mathcal{D}<em j="j">{\mathcal{I}})$ and then optimize the AF given by $\mathbf{E}\left[y</em>} \mid \mathbf{x<em j="j">{j}, \boldsymbol{\theta}\right]$, with $j \notin \mathcal{I}$. When the model is a Bayesian neural network trained with PBP, the corresponding operations are sampling $\mathcal{W}$ from $q$ and then optimizing the AF given by $f\left(\mathbf{x}</em>$. This last step requires the use of a deterministic neural network, with weight values given by the posterior sample from $q$, to make predictions on all the molecules that have not been evaluated yet. Then, the molecule with highest predictive value is selected for the next evaluation.} ; \mathcal{W}\right)$, with $j \notin \mathcal{I</p>
<h2>6. Experiments with GPs and Parallel EI</h2>
<p>We first compare the performance of our parallel and distributed Thompson sampling (PDTS) algorithm with the most popular approach for parallel BO: the parallel EI method from Section 2.1. Existing implementations of parallel EI such as spearmint ${ }^{1}$ use a Gaussian process (GP) model for the objective function. To compare with these methods, we also adopt a GP as the model in PDTS. Note that parallel EI cannot scale to the large batch sizes used in high-throughput screening. Therefore, we consider here only parallel optimization problems with small batch sizes and synthetic objective functions. Besides PDTS and parallel EI, we also analyze the performance of the sequential versions of these algorithms: TS and EI.</p>
<p>To implement Thompson sampling (TS) with a GP model, we approximate the non-parametric GP with a parametric approximation based on random features, as described in the supplementary material of (Hernández-Lobato et al., 2014). For the experiments, we consider a cluster with 11</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>nodes: one central node for controlling the BO process and 10 additional nodes for parallel evaluations. We assume that all objective evaluations take a very large amount of time and that the cost of training the GPs and recomputing and optimizing the AF is negligible in comparison. Thus, in practice, we perform these experiments in a sequential (non-parallel) fashion with the GP model being updated only in blocks of 10 consecutive data points at a time.</p>
<p>As objective functions we consider the two dimensional Bohachevsky and Branin-Hoo functions and the six dimensional Hartmann function, all available in Benchfunk ${ }^{2}$. We also consider the optimization of functions sampled from the GP prior over the 2D unit square using a squared exponential covariance function with fixed 0.1 length scale. After each objective evaluation, we compute the immediate regret (IR), which we define as the difference between the best objective value obtained so far and the minimum value of the objective function. The measurement noise is zero in these experiments.</p>
<p>Figure 3 reports mean and standard errors for the logarithm of the best IR seen so far, averaged across 50 repetitions of the experiments. In the plots, the horizontal axis shows the number of function evaluations performed so far. Note that in these experiments TS and EI update their GPs once per sample, while PDTS and parallel EI update only every 10 samples. Figure 3 shows that EI is better than TS in most cases, although the differences between these two methods are small in the Branin-Hoo function. However, EI is considerably much better than TS in Hartmann. The reason for this is that in Hartmann there are multiple equivalent global minima and TS tends to explore all of them. EI is by contrast more exploitative and focuses on evaluating the objective around only one of the minima. The differences between parallel EI and PDTS are much smaller, with both obtaining very similar results. The exception is again Hartmann, where parallel EI is much better than PDTS, probably because PDTS is more explorative than parallel EI. Interestingly, PDTS performs better than parallel EI on the random samples from the GP prior, although parallel EI eventually catches up.</p>
<p>These results indicate that PDTS performs in practice very similarly to parallel EI, one of the most popular methods for parallel BO.</p>
<h2>7. Experiments with Molecule Data Sets</h2>
<p>We describe the molecule data sets used in our experiments. The input features for all molecules are 512-bit Morgan circular fingerprints (Rogers \&amp; Hahn, 2010), calculated with a bond radius of 2, and derived from the canonical SMILES as implemented in the RDkit package (Landrum).</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Immediate regret in experiments with GPs, using TS, EI, PDTS and parallel EI for optimizing synthetic functions (first 3 plots) and functions sampled from a GP prior (fourth plot).</p>
<p>Harvard Clean Energy Project: The Clean Energy Project is the world's largest materials high-throughput virtual screening effort (Hachmann et al., 2014; 2011), and has scanned more than 3.5 million molecules to find those with high power conversion efficiency (PCE) using quantum-chemical techniques, taking over 30,000 years of CPU time. The target value within this data set is the power conversion efficiency (PCE), which is calculated for the 2.3 million publicly released molecules, using the Scharber model (Dennler et al., 2008) and frontier orbitals calculated at the BP86 (Perdew, 1986; Becke, 1993) def2-SVP (Weigend \&amp; Ahlrichs, 2005) level of theory.</p>
<p>Dose-Response Data Set: These data sets were obtained from the NCI-cancer database (Many authors). The doseresponse target value has a potential range of -100 to 100, and reports a percentage cell growth relative to a no-drug control. Thus, a value of +40 would correspond to a $60 \%$ growth inhibition and a value of -40 would correspond to $40 \%$ lethality. Molecules with a positive value for the dose-response are known as inhibitors, molecules with a score less than 0 have a cytotoxic effect. Results against the NCI-H23 cell line were taken against a constant logconcentration of -8.00 M and where multiple identical conditions were present in the data an average was used for the target variables. In this data set we are interested in finding molecules with smaller values of the target variable.</p>
<p>Malaria Data Set: The Malaria data set was taken from the P. falciparum whole cell screening derived by combining the GSK TCAMS data set, the Novatis-GNF Malaria Box data set and the St Jude's Research Hospital data set, as released through the Medicines for Malaria Venture website (Spangenberg et al., 2013). The target variable is the EC50 value, which is defined as the concentration of the drug which gives half maximal response. Much like the Dose response data set, the focus here is on minimization: the lower the concentration, the stronger the drug.</p>
<h3>7.1. Results</h3>
<p>We evaluate the gains produced by PDTS in experiments simulating a high throughput virtual screening setting. In these experiments, we sequentially sample molecules from
libraries of candidate molecules given by the data sets from Section 7. After each sampling step, we calculate the $1 \%$ recall, that is, the fraction of the top $1 \%$ of molecules from the original library that are found among the sampled ones. For the CEP data, we compute recall by focusing on molecules with PCE larger than $10 \%$. In all data sets, each sampling step involves selecting a batch of molecules among those that have not been sampled so far. In the Malaria and One-dose data sets we use batches of size 200. These data sets each contain about 20,000 molecules. By contrast, the CEP data set contains 2 million molecules. In this latter case, we use batches of size 500. We use Bayesian neural networks with one hidden layer and 100 hidden units.</p>
<p>We compare the performance of PDTS with two baselines. The first one, greedy, is a sampling strategy that only considers exploitation and does not perform any exploration. We implement this approach by selecting molecules according to the average of the probabilistic predictions generated by PBP. That is, the greedy approach ignores any variance in the predictions of the Bayesian neural network and generates batches by just ranking molecules according to the mean of the predictive distribution given by PBP. The second baseline is a Monte Carlo approach in which the batches of molecules are selected uniformly at random. These two baselines are comparable to PDTS in that they can be easily implemented in a large scale setting in which the library of candidate molecules contains millions of elements and data is sampled using large batch sizes.</p>
<p>In the Malaria and One-dose data sets, we average across 50 different realizations of the experiments. This is not possible in the CEP data set, which is 100 times larger than the two other data sets. In the CEP case, we report results for a single realization of the experiment (in a second realization we obtained similar results). Figure 4 shows the recall obtained by each method in the molecule data sets. PDTS significantly outperforms the Monte Carlo approach, and also offers better performance than greedy sampling. This shows the importance of building in exploration into the sampling strategy, rather than relying on purely exploitative methods. The greedy approach performs best in the</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Recall obtained by PDTS on each data set. For the CEP data, the recall for molecules with a PCE &gt; 10% is reported, whilst for One-dose and Malaria we report the recall for the molecules in the top 1%. In addition to the Monte Carlo sampling baseline, we also include results for a greedy sampling approach, in which there is no exploration, and the molecules are chosen according to the mean of the predictive distribution given by PBP. The overall lower performance of this greedy strategy illustrates the importance of exploration in this type of problems.</p>
<p>CEP data set. In this case, the greedy strategy initially finds better molecules than PDTS, but after a while PDTS overtakes, probably because a promising area of chemical space initially discovered by the greedy approach starts to become exhausted.</p>
<p>The previous results allow us to consider the savings produced by BO. In the CEP data set, PDTS achieves about 20 times higher recall values than the Monte Carlo approach, which is comparable to the exhaustive enumeration that was used to collect the CEP data. We estimate that, with BO, the CEP virtual screening process would have taken 1,500 CPU years instead of the 30,000 that were actually used. Regarding the One-dose and Malaria data sets, PDTS can locate in both sets about 70% of the top 1% molecules by sampling approximately 6,000 molecules. By contrast, the Monte Carlo approach would require sampling 14,000 molecules. This represents a significant reduction in the discovery time for new therapeutic molecules and savings in the economic costs associated with molecule synthesis and testing.</p>
<h3>7.2. Comparison with ε-greedy Approaches</h3>
<p>We can easily modify the greedy baseline from the previous section to include some amount of exploration by replacing a small fraction of the molecules in each batch with molecules chosen uniformly at random. This approach is often called ε-greedy (Watkins, 1989), where the variable ε indicates the fraction of molecules that are sampled uniformly at random. The disadvantage of the ε-greedy approach is that it requires the tuning of ε to the problem of interest whereas the amount of exploration is automatically set by PDTS.</p>
<p>We compared PDTS with different versions of ε-greedy in the same way as above, using ε = 0.01, 0.025, 0.05 and 0.075. The experiments with the One-dose and the Malaria data sets are similar to the ones done before. However, we now sub-sample the CEP data set to be able to average across 50 different realizations of the experiment: we choose 4,000 molecules uniformly at random and then collect data in batches of size 50 across 50 different repetitions of the screening process. We compute the average rank obtained by each method across the 3 × 50 = 150 simulated screening experiments. A ranking equal to 1 indicates that the method always obtains the highest recall at the end of the experiment, while a ranking equal to 5 indicates that the method always obtains the worst recall value. Table 1 shows that the lowest average rank is obtained by PDTS, which achieves better exploration-exploitation trade-offs than the ε-greedy approaches.</p>
<h2>8. Conclusions</h2>
<p>We have presented a Parallel and Distributed implementation of Thompson Sampling (PDTS), a highly scalable method for parallel Bayesian optimization. PDTS can be applied when scalability limits the applicability of competing approaches. We have evaluated the performance of PDTS in experiments with both Gaussian process and probabilistic neural networks. We show that PDTS compares favorably with parallel EI in problems with small batch sizes. We also demonstrate the effectiveness of PDTS on large scale real world applications that involve searching chemical space for new molecules with improved properties. We show that PDTS outperforms other scalable approaches on these applications, in particular, a greedy search strategy, ε-greedy approaches and a random search method.</p>
<p>Table 1. Average rank and standard errors by each method.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Rank</th>
</tr>
</thead>
<tbody>
<tr>
<td>ε = 0.01</td>
<td>3.42±0.28</td>
</tr>
<tr>
<td>ε = 0.025</td>
<td>3.02±0.25</td>
</tr>
<tr>
<td>ε = 0.05</td>
<td>2.86±0.23</td>
</tr>
<tr>
<td>ε = 0.075</td>
<td>3.20±0.26</td>
</tr>
<tr>
<td>PDTS</td>
<td>2.51±0.20</td>
</tr>
</tbody>
</table>
<h2>Acknowledgements</h2>
<p>J.M.H.L. acknowledges support from the Rafael del Pino Foundation. The authors thank Ryan P. Adams for useful discussions. A.A.-G. and E.O.P.-K. acknowledge the Department of Energy Program on Theory and modeling through grant DE-SC0008733.</p>
<h2>References</h2>
<p>Azimi, Javad, Fern, Alan, and Fern, Xiaoli Z. Batch Bayesian optimization via simulation matching. In NIPS, pp. 109-117, 2010.</p>
<p>Becke, Axel D. Density-functional thermochemistry. III. The role of exact exchange. The Journal of Chemical Physics, 98(7): 5648, 1993.</p>
<p>Blundell, Charles, Cornebise, Julien, Kavukcuoglu, Koray, and Wierstra, Daan. Weight uncertainty in neural networks. In ICML, pp. 1613-1622, 2015.</p>
<p>Bousquet, Olivier and Bottou, Léon. The tradeoffs of large scale learning. In NIPS, pp. 161-168, 2008.</p>
<p>Chapelle, Olivier and Li, Lihong. An empirical evaluation of Thompson sampling. In NIPS, pp. 2249-2257. 2011.</p>
<p>Chevalier, Clément and Ginsbourger, David. Fast computation of the multi-points expected improvement with applications in batch selection. In International Conference on Learning and Intelligent Optimization, pp. 59-69. Springer, 2013.</p>
<p>Contal, Emile, Buffoni, David, Robicquet, Alexandre, and Vayatis, Nicolas. Parallel Gaussian process optimization with upper confidence bound and pure exploration. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 225-240. Springer, 2013.</p>
<p>Dennler, G., Scharber, M. C., Ameri, T., Denk, P., Forberich, K., Waldauf, C., and Brabec, C. J. Design rules for donors in bulk-heterojunction tandem solar cells? towards 15\% energyconversion efficiency. Adv. Mater., 20(3):579-583, feb 2008.</p>
<p>Desautels, Thomas, Krause, Andreas, and Burdick, Joel W. Parallelizing exploration-exploitation tradeoffs in Gaussian process bandit optimization. Journal of Machine Learning Research, 15(1):3873-3923, 2014.</p>
<p>Gal, Yarin and Ghahramani, Zoubin. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In ICML, pp. 1050-1059, 2016.</p>
<p>Ginsbourger, David, Le Riche, Rodolphe, and Carraro, Laurent. Kriging is well-suited to parallelize optimization. In Computational Intelligence in Expensive Optimization Problems, pp. 131-162. Springer, 2010.</p>
<p>Ginsbourger, David, Janusevskis, Janis, and Le Riche, Rodolphe. Dealing with asynchronicity in parallel Gaussian process based global optimization. In 4th International Conference of the ERCIM WG on computing \&amp; statistics (ERCIM'11), 2011.</p>
<p>Gómez-Bombarelli, Rafael, Aguilera-Iparraguirre, Jorge, Hirzel, Timothy D., Duvenaud, David, Maclaurin, Dougal, BloodForsythe, Martin A., Chae, Hyun Sik, Einzinger, Markus,</p>
<p>Ha, Dong-Gwang, Wu, Tony, Markopoulos, Georgios, Jeon, Soonok, Kang, Hosuk, Miyazaki, Hiroshi, Numata, Masaki, Kim, Sunghan, Huang, Wenliang, Hong, Seong Ik, Baldo, Marc, Adams, Ryan P., and Aspuru-Guzik, Alán. Design of efficient molecular organic light-emitting diodes by a highthroughput virtual screening and experimental approach. Nature Materials, aug 2016.</p>
<p>Gonzlez, J., Dai, Z., Hennig, P., and Lawrence, N. Batch Bayesian optimization via local penalization. In AISTATS, pp. 648-657, 2016.</p>
<p>Hachmann, Johannes, Olivares-Amaya, Roberto, Atahan-Evrenk, Sule, Amador-Bedolla, Carlos, Sanchez-Carrera, Roel S., Gold-Parker, Aryeh, Vogt, Leslie, Brockway, Anna M., and Aspuru-Guzik, Alan. The Harvard Clean Energy Project: Large-Scale Computational Screening and Design of Organic Photovoltaics on the World Community Grid. J. Phys. Chem. Lett., 2(17):2241-2251, sep 2011.</p>
<p>Hachmann, Johannes, Olivares-Amaya, Roberto, Jinich, Adrian, Appleton, Anthony L., Blood-Forsythe, Martin A., Seress, László R., Román-Salgado, Carolina, Trepte, Kai, AtahanEvrenk, Sule, Er, Sleyman, Shrestha, Supriya, Mondal, Rajib, Sokolov, Anatoliy, Bao, Zhenan, and Aspuru-Guzik, Alán. Lead candidates for high-performance organic photovoltaics from high-throughput quantum chemistry - the Harvard Clean Energy Project. Energy Environ. Sci., 7(2):698-704, 2014.</p>
<p>Hernández-Lobato, José Miguel and Adams, Ryan P. Probabilistic backpropagation for scalable learning of bayesian neural networks. In ICML, pp. 1861-1869, 2015.</p>
<p>Hernández-Lobato, José Miguel, Hoffman, Matthew W, and Ghahramani, Zoubin. Predictive entropy search for efficient global optimization of black-box functions. In NIPS, pp. 918926, 2014.</p>
<p>Jones, Donald R, Schonlau, Matthias, and Welch, William J. Efficient global optimization of expensive black-box functions. Journal of Global optimization, 13(4):455-492, 1998.</p>
<p>Landrum, Greg. RDKit: Open-source cheminformatics.
Ma, Junshui, Sheridan, Robert P., Liaw, Andy, Dahl, George E., and Svetnik, Vladimir. Deep neural nets as a method for quantitative structure-activity relationships. Journal of Chemical Information and Modeling, 55(2):263-274, feb 2015.</p>
<p>Many authors. NCI Database Download Page. URL http:// cactus.nci.nih.gov/download/nci/.</p>
<p>Marmin, Sébastien, Chevalier, Clément, and Ginsbourger, David. Differentiating the multipoint expected improvement for optimal batch design. In International Workshop on Machine Learning, Optimization and Big Data, pp. 37-48. Springer, 2015.</p>
<p>Mayr, Andreas, Klambauer, Gnter, Unterthiner, Thomas, and Hochreiter, Sepp. Deeptox: Toxicity prediction using deep learning. Front. Environ. Sci., 3, feb 2016.</p>
<p>Minka, Thomas P. A family of algorithms for approximate Bayesian inference. PhD thesis, Massachusetts Institute of Technology, 2001.</p>
<p>Opper, Manfred. A Bayesian approach to on-line learning. In Saad, David (ed.), On-Line Learning in Neural Networks, pp. 363-378. Cambridge University Press (CUP), 1998.</p>
<p>Perdew, John P. Density-functional approximation for the correlation energy of the inhomogeneous electron gas. Phys. Rev. B, 33(12):8822-8824, jun 1986.</p>
<p>Pyzer-Knapp, Edward O., Suh, Changwon, Gómez-Bombarelli, Rafael, Aguilera-Iparraguirre, Jorge, and Aspuru-Guzik, Alán. What is high-throughput virtual screening? A perspective from organic materials discovery. Annu. Rev. Mater. Res., 45(1): 195-216, jul 2015.</p>
<p>Rajan, Krishna. Combinatorial Materials Sciences: Experimental Strategies for Accelerated Knowledge Discovery. Annu. Rev. Mater. Res., 38(1):299-322, aug 2008.</p>
<p>Ramsundar, Bharath, Kearnes, Steven, Riley, Patrick, Webster, Dale, Konerding, David, and Pande, Vijay. Massively multitask networks for drug discovery. arXiv preprint arXiv:1502.02072, 2015.</p>
<p>Reymond, Jean-Louis, Ruddigkeit, Lars, Blum, Lorenz, and van Deursen, Ruud. The enumeration of chemical space. Wiley Interdisciplinary Reviews: Computational Molecular Science, 2(5):717-733, apr 2012.</p>
<p>Rogers, David and Hahn, Mathew. Extended-connectivity fingerprints. Journal of Chemical Information and Modeling, 50(5): 742-754, may 2010.</p>
<p>Scharber, M.C., Mhlbacher, D., Koppe, M., Denk, P., Waldauf, C., Heeger, A.J., and Brabec, C.J. Design rules for donors in bulkheterojunction solar cells-towards $10 \%$ energy-conversion efficiency. Advanced Materials, 18(6):789-794, 2006.</p>
<p>Shah, Amar and Ghahramani, Zoubin. Parallel predictive entropy search for batch global optimization of expensive objective functions. In NIPS, pp. 3330-3338, 2015.</p>
<p>Shahriari, Bobak, Wang, Ziyu, Hoffman, Matthew W, BouchardCôté, Alexandre, and de Freitas, Nando. An entropy search portfolio for Bayesian optimization. arXiv preprint arXiv:1406.4625, 2014.</p>
<p>Snoek, Jasper, Larochelle, Hugo, and Adams, Ryan P. Practical Bayesian optimization of machine learning algorithms. In NIPS, pp. 2951-2959, 2012.</p>
<p>Snoek, Jasper, Rippel, Oren, Swersky, Kevin, Kiros, Ryan, Satish, Nadathur, Sundaram, Narayanan, Patwary, Md. Mostofa Ali, Prabhat, and Adams, Ryan P. Scalable Bayesian optimization using deep neural networks. In ICML, pp. 2171-2180, 2015.</p>
<p>Spangenberg, Thomas, Burrows, Jeremy N., Kowalczyk, Paul, McDonald, Simon, Wells, Timothy N. C., and Willis, Paul. The Open Access Malaria Box: A Drug Discovery Catalyst for Neglected Diseases. PLoS ONE, 8(6):e62906, jun 2013.</p>
<p>Thompson, William R. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3/4):285, dec 1933.</p>
<p>Wang, Jialei, Clark, Scott C, Liu, Eric, and Frazier, Peter I. Parallel Bayesian global optimization of expensive functions. arXiv preprint arXiv:1602.05149, 2016.</p>
<p>Watkins, Christopher John Cornish Hellaby. Learning from delayed rewards. PhD thesis, University of Cambridge England, 1989.</p>
<p>Weigend, Florian and Ahlrichs, Reinhart. Balanced basis sets of split valence triple zeta valence and quadruple zeta valence quality for H to Rn: Design and assessment of accuracy. Phys. Chem. Chem. Phys., 7(18):3297, 2005.</p>
<p>Wu, Jian and Frazier, Peter. The parallel knowledge gradient method for batch Bayesian optimization. In NIPS, pp. 31263134, 2016.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://github.com/HIPS/Spearmint&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{2}$ https://github.com/mwhoffman/benchfunk&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
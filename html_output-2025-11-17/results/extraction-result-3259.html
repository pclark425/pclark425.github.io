<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3259 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3259</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3259</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-75.html">extraction-schema-75</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <p><strong>Paper ID:</strong> paper-77668573e9180b9fe9ae932a5ce9de53c81b045e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/77668573e9180b9fe9ae932a5ce9de53c81b045e" target="_blank">Transfer in Deep Reinforcement Learning Using Knowledge Graphs</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This paper explores the use of knowledge graphs as a representation for domain knowledge transfer for training text-adventure playing reinforcement learning agents and demonstrates that their transfer learning methods let us learn a higher-quality control policy faster.</p>
                <p><strong>Paper Abstract:</strong> Text adventure games, in which players must make sense of the world through text descriptions and declare actions through text descriptions, provide a stepping stone toward grounding action in language. Prior work has demonstrated that using a knowledge graph as a state representation and question-answering to pre-train a deep Q-network facilitates faster control policy learning. In this paper, we explore the use of knowledge graphs as a representation for domain knowledge transfer for training text-adventure playing reinforcement learning agents. Our methods are tested across multiple computer generated and human authored games, varying in domain and complexity, and demonstrate that our transfer learning methods let us learn a higher-quality control policy faster.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3259.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3259.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Graph Deep Q-Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deep Q-learning agent that represents the partially-observable text-adventure state as an explicit knowledge graph (RDF triples) extracted from observations and uses that graph both as the state input and to prune the combinatorial action space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Playing text-adventure games with graph-based deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Two-network DQN architecture that encodes state and actions separately and computes Q-values via a pairwise interaction; state encoding includes a learned embedding of an explicitly maintained knowledge graph built from per-step OpenIE + rule-based extraction of RDF triples; training uses prioritized experience replay, modified epsilon-greedy, and temporal-difference loss. Action templates are filled from the game's vocabulary and pruned/ranked using the knowledge graph.</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>TextWorld, Jericho (human-authored games such as 9:05, Anchorhead, Afflicted, Lurking Horror); experiments include generated TextWorld 'home' games and commercial interactive fiction partitioned via walkthroughs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Learn control policies to complete text-adventure quests (reach intermediate or final checkpoints) where observations are textual; objectives measured as episodic reward, initial reward (first 50 episodes), final reward (after convergence), and steps-to-completion.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external memory (persistent knowledge graph) + training memory (prioritized experience replay buffer)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td>Per timestep the agent extracts RDF triples from the textual observation using OpenIE plus game-specific rules and incrementally adds them to a knowledge graph that (a) maps rooms and object locations/affordances, (b) distinguishes items in possession vs surroundings, and (c) is used as input to the state encoder and to prune/rank action templates. Additionally, the system uses prioritized experience replay (a replay buffer) during DQN training. The graph can be seeded from static textual guides before play (see separate seeding technique).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported experiments all use the knowledge-graph mechanism. Representative numbers: Slice-of-life target (9:05) KG-DQN Full (seeded + QA pretrain + source-to-target parameter transfer, dense reward): initial reward 2.7 ± 0.65, final reward 19.7 ± 2.0, steps 274.76 ± 21.45 (Table 2). Horror target (Anchorhead) KG-DQN Full (seeded + QA + transfer, dense reward): initial reward 4.1 ± 0.9, final reward 39.9 ± 0.53, steps 4334.3 ± 56.13 (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>The explicit knowledge graph (external memory) improves exploration and action pruning, leading to higher initial rewards, faster learning/convergence, higher final reward, and dramatically fewer steps-to-completion when combined with seeding and QA pretraining (e.g., KG-DQN Full reduced steps-to-completion on 9:05 from ~1267 to ~275 and increased final reward). Authors report up to ~80% improvement in convergence steps in some cases when transfer+seeding+QA are used.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td>Graph-based memory alone does not guarantee solving very complex games under sparse reward; a persistent 'bottleneck' was observed (e.g., Anchorhead stall at reward ≈7) that the graph/transfer helps but does not always overcome without dense oracle-augmented rewards. Knowledge graphs extracted from static guides are potentially disjoint from in-game graphs (no room map), and graph structures differ across games making direct graph transfer non-trivial.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Use an explicit knowledge graph as a persistent external memory to (a) encode partial observability and object affordances, and (b) prune the combinatorial action space; seed the graph from domain/game-specific textual guides when available; pretrain graph/state/action encoders using question-answering on oracle traces; transfer DQN parameters from source to target within a genre; and consider dense reward augmentation (oracle-derived checkpoints) to mitigate sparse-reward exploration issues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transfer in Deep Reinforcement Learning Using Knowledge Graphs', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3259.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3259.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG Seeding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Graph Seeding from Static Textual Guides</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A technique that initializes the agent's knowledge graph using RDF triples extracted from general or game-specific textual walkthroughs or guides prior to gameplay to provide a prior that biases exploration toward higher-utility actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-DQN (seeded)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same KG-DQN architecture as above but the knowledge graph is initialized (seeded) before gameplay by extracting triples from online text-adventure guides/walkthroughs using OpenIE + a subset of the observation-extraction rules; the seeded graph contains object affordances and common actions but may lack room-map structure.</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Same TextWorld and Jericho games used in the paper (TextWorld 'home' generated games, 9:05, Afflicted, Anchorhead, Lurking Horror).</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Provide an informed prior for exploration in text-adventure quests by preloading cross-game/genre knowledge (affordances, common object relations) to improve initial exploration and downstream policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external memory (preloaded persistent knowledge graph)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td>Extract RDF triples from static guides using OpenIE and game-specific rules; build a (potentially disjoint) graph containing affordances and relations and use it to initialize the agent's knowledge graph before training/play. During play the graph is then incrementally updated with observations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Seeding improves initial reward and/or convergence in multiple experiments. Examples: Slice-of-life source (TextWorld) KG-DQN seeded: initial reward 3.2 ± 0.57 vs KG-DQN no transfer 2.6 ± 0.73 (Table 2). Horror target (Anchorhead) KG-DQN seeded: final reward 26.6 ± 0.42 vs KG-DQN no transfer 6.8 ± 0.42 (Table 3) when combined with dense reward and QA pretraining/transfer (seeding alone shows large gains in horror domain).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>Seeding reduces wasted exploration at the start of an episode, raises initial reward, and—when combined with QA pretraining and parameter transfer—yields markedly faster learning and higher final performance, particularly in domains with strong genre-consistent affordances (e.g., horror).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td>Seed graphs may be disjoint and lack environment-specific structure (no room map) and can be less useful if the guide does not match the target game's specifics; seeding alone does not always overcome reward sparsity or certain puzzle bottlenecks.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>When available, extract and seed domain-specific knowledge (object affordances, common actions) from textual guides before training; combine seeding with QA pretraining and parameter transfer for maximal effect; ensure seed information is generalizable (avoid overfitting to guide-specific artifacts).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transfer in Deep Reinforcement Learning Using Knowledge Graphs', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3259.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3259.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QA Pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Question-Answering Pretraining from Oracle Traces</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pretraining parts of the DQN (state, graph, and action encoders) using a supervised question-answering model trained on oracle perfect playthrough traces (state-action pairs) so that the agent's perception and mapping from observations to high-utility actions are initialized before reinforcement learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reading Wikipedia to answer open-domain questions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-DQN (w/ QA pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A QA system (Chen et al., 2017 style reading-model) is trained on oracle-generated state-action traces; learned parameters for the observation/graph/action encoders are transferred to initialize corresponding parts of the KG-DQN before RL training on the source/target task.</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>TextWorld generated games (for QA training traces) and human-authored Jericho games used as source/target (Afflicted, Anchorhead, Lurking Horror, 9:05).</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Use supervised learning on oracle traces to give the agent a prior mapping from textual observations to desirable actions, reducing the amount of RL exploration needed to learn high-quality policies.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>indirectly uses external memory (initializes encoders that operate on the knowledge graph); not a memory store itself</td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td>Train a question-answering model on oracle walkthrough traces (state-action pairs). Transfer the trained encoder parameters to the KG-DQN modules responsible for encoding observations, graph, and actions. The QA pretraining thus bootstraps how the agent interprets/uses its knowledge-graph memory during RL.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>QA pretraining improves both initial reward and convergence speed. Examples: Slice-of-life source (TextWorld) KG-DQN w/ QA: initial reward 2.8 ± 0.61 vs no transfer 2.6 ± 0.73; final reward 4.9 ± 0.09 vs 4.7 ± 0.23 (Table 2). Horror source (Afflicted) KG-DQN w/ QA: initial reward 4.3 ± 1.34 vs no transfer 3.0 ± 1.3; final 15.1 ± 1.60 vs 14.1 ± 1.73 (Table 3). For Anchorhead target, KG-DQN w/ QA (dense reward) final reward 24.8 ± 0.6 vs no transfer 6.8 ± 0.42 (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>Pretraining on oracle traces gives the agent an inductive bias on how to interpret textual observations and how to update/use the knowledge graph effectively, improving early performance and ultimately leading to higher-quality policies and faster convergence when combined with seeding and parameter transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td>QA pretraining requires oracle traces or walkthroughs (which may not be available) and pretraining on very similar games to be most effective; does not by itself solve exploration under sparse reward for very complex games without other aids (e.g., dense oracle-augmented rewards).</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Use QA pretraining from oracle or high-quality walkthrough traces to initialize observation/graph/action encoders; combine with knowledge-graph seeding and source-to-target parameter transfer for best gains; ensure pretraining corpus is representative of the target genre/domain.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transfer in Deep Reinforcement Learning Using Knowledge Graphs', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Playing text-adventure games with graph-based deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Language Understanding for Text-based Games Using Deep Reinforcement Learning <em>(Rating: 2)</em></li>
                <li>Deep Reinforcement Learning with a Natural Language Action Space <em>(Rating: 1)</em></li>
                <li>TextWorld : A Learning Environment for Text-based Games <em>(Rating: 2)</em></li>
                <li>Nail: A general interactive fiction agent <em>(Rating: 2)</em></li>
                <li>Reading Wikipedia to answer open-domain questions <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3259",
    "paper_id": "paper-77668573e9180b9fe9ae932a5ce9de53c81b045e",
    "extraction_schema_id": "extraction-schema-75",
    "extracted_data": [
        {
            "name_short": "KG-DQN",
            "name_full": "Knowledge Graph Deep Q-Network",
            "brief_description": "A deep Q-learning agent that represents the partially-observable text-adventure state as an explicit knowledge graph (RDF triples) extracted from observations and uses that graph both as the state input and to prune the combinatorial action space.",
            "citation_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "mention_or_use": "use",
            "agent_name": "KG-DQN",
            "agent_description": "Two-network DQN architecture that encodes state and actions separately and computes Q-values via a pairwise interaction; state encoding includes a learned embedding of an explicitly maintained knowledge graph built from per-step OpenIE + rule-based extraction of RDF triples; training uses prioritized experience replay, modified epsilon-greedy, and temporal-difference loss. Action templates are filled from the game's vocabulary and pruned/ranked using the knowledge graph.",
            "game_or_benchmark_name": "TextWorld, Jericho (human-authored games such as 9:05, Anchorhead, Afflicted, Lurking Horror); experiments include generated TextWorld 'home' games and commercial interactive fiction partitioned via walkthroughs.",
            "task_description": "Learn control policies to complete text-adventure quests (reach intermediate or final checkpoints) where observations are textual; objectives measured as episodic reward, initial reward (first 50 episodes), final reward (after convergence), and steps-to-completion.",
            "uses_memory": true,
            "memory_type": "external memory (persistent knowledge graph) + training memory (prioritized experience replay buffer)",
            "memory_implementation_details": "Per timestep the agent extracts RDF triples from the textual observation using OpenIE plus game-specific rules and incrementally adds them to a knowledge graph that (a) maps rooms and object locations/affordances, (b) distinguishes items in possession vs surroundings, and (c) is used as input to the state encoder and to prune/rank action templates. Additionally, the system uses prioritized experience replay (a replay buffer) during DQN training. The graph can be seeded from static textual guides before play (see separate seeding technique).",
            "performance_with_memory": "Reported experiments all use the knowledge-graph mechanism. Representative numbers: Slice-of-life target (9:05) KG-DQN Full (seeded + QA pretrain + source-to-target parameter transfer, dense reward): initial reward 2.7 ± 0.65, final reward 19.7 ± 2.0, steps 274.76 ± 21.45 (Table 2). Horror target (Anchorhead) KG-DQN Full (seeded + QA + transfer, dense reward): initial reward 4.1 ± 0.9, final reward 39.9 ± 0.53, steps 4334.3 ± 56.13 (Table 3).",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "memory_benefits": "The explicit knowledge graph (external memory) improves exploration and action pruning, leading to higher initial rewards, faster learning/convergence, higher final reward, and dramatically fewer steps-to-completion when combined with seeding and QA pretraining (e.g., KG-DQN Full reduced steps-to-completion on 9:05 from ~1267 to ~275 and increased final reward). Authors report up to ~80% improvement in convergence steps in some cases when transfer+seeding+QA are used.",
            "memory_limitations_or_failures": "Graph-based memory alone does not guarantee solving very complex games under sparse reward; a persistent 'bottleneck' was observed (e.g., Anchorhead stall at reward ≈7) that the graph/transfer helps but does not always overcome without dense oracle-augmented rewards. Knowledge graphs extracted from static guides are potentially disjoint from in-game graphs (no room map), and graph structures differ across games making direct graph transfer non-trivial.",
            "best_practices_or_recommendations": "Use an explicit knowledge graph as a persistent external memory to (a) encode partial observability and object affordances, and (b) prune the combinatorial action space; seed the graph from domain/game-specific textual guides when available; pretrain graph/state/action encoders using question-answering on oracle traces; transfer DQN parameters from source to target within a genre; and consider dense reward augmentation (oracle-derived checkpoints) to mitigate sparse-reward exploration issues.",
            "uuid": "e3259.0",
            "source_info": {
                "paper_title": "Transfer in Deep Reinforcement Learning Using Knowledge Graphs",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "KG Seeding",
            "name_full": "Knowledge Graph Seeding from Static Textual Guides",
            "brief_description": "A technique that initializes the agent's knowledge graph using RDF triples extracted from general or game-specific textual walkthroughs or guides prior to gameplay to provide a prior that biases exploration toward higher-utility actions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "KG-DQN (seeded)",
            "agent_description": "Same KG-DQN architecture as above but the knowledge graph is initialized (seeded) before gameplay by extracting triples from online text-adventure guides/walkthroughs using OpenIE + a subset of the observation-extraction rules; the seeded graph contains object affordances and common actions but may lack room-map structure.",
            "game_or_benchmark_name": "Same TextWorld and Jericho games used in the paper (TextWorld 'home' generated games, 9:05, Afflicted, Anchorhead, Lurking Horror).",
            "task_description": "Provide an informed prior for exploration in text-adventure quests by preloading cross-game/genre knowledge (affordances, common object relations) to improve initial exploration and downstream policy learning.",
            "uses_memory": true,
            "memory_type": "external memory (preloaded persistent knowledge graph)",
            "memory_implementation_details": "Extract RDF triples from static guides using OpenIE and game-specific rules; build a (potentially disjoint) graph containing affordances and relations and use it to initialize the agent's knowledge graph before training/play. During play the graph is then incrementally updated with observations.",
            "performance_with_memory": "Seeding improves initial reward and/or convergence in multiple experiments. Examples: Slice-of-life source (TextWorld) KG-DQN seeded: initial reward 3.2 ± 0.57 vs KG-DQN no transfer 2.6 ± 0.73 (Table 2). Horror target (Anchorhead) KG-DQN seeded: final reward 26.6 ± 0.42 vs KG-DQN no transfer 6.8 ± 0.42 (Table 3) when combined with dense reward and QA pretraining/transfer (seeding alone shows large gains in horror domain).",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "memory_benefits": "Seeding reduces wasted exploration at the start of an episode, raises initial reward, and—when combined with QA pretraining and parameter transfer—yields markedly faster learning and higher final performance, particularly in domains with strong genre-consistent affordances (e.g., horror).",
            "memory_limitations_or_failures": "Seed graphs may be disjoint and lack environment-specific structure (no room map) and can be less useful if the guide does not match the target game's specifics; seeding alone does not always overcome reward sparsity or certain puzzle bottlenecks.",
            "best_practices_or_recommendations": "When available, extract and seed domain-specific knowledge (object affordances, common actions) from textual guides before training; combine seeding with QA pretraining and parameter transfer for maximal effect; ensure seed information is generalizable (avoid overfitting to guide-specific artifacts).",
            "uuid": "e3259.1",
            "source_info": {
                "paper_title": "Transfer in Deep Reinforcement Learning Using Knowledge Graphs",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "QA Pretraining",
            "name_full": "Question-Answering Pretraining from Oracle Traces",
            "brief_description": "Pretraining parts of the DQN (state, graph, and action encoders) using a supervised question-answering model trained on oracle perfect playthrough traces (state-action pairs) so that the agent's perception and mapping from observations to high-utility actions are initialized before reinforcement learning.",
            "citation_title": "Reading Wikipedia to answer open-domain questions",
            "mention_or_use": "use",
            "agent_name": "KG-DQN (w/ QA pretraining)",
            "agent_description": "A QA system (Chen et al., 2017 style reading-model) is trained on oracle-generated state-action traces; learned parameters for the observation/graph/action encoders are transferred to initialize corresponding parts of the KG-DQN before RL training on the source/target task.",
            "game_or_benchmark_name": "TextWorld generated games (for QA training traces) and human-authored Jericho games used as source/target (Afflicted, Anchorhead, Lurking Horror, 9:05).",
            "task_description": "Use supervised learning on oracle traces to give the agent a prior mapping from textual observations to desirable actions, reducing the amount of RL exploration needed to learn high-quality policies.",
            "uses_memory": true,
            "memory_type": "indirectly uses external memory (initializes encoders that operate on the knowledge graph); not a memory store itself",
            "memory_implementation_details": "Train a question-answering model on oracle walkthrough traces (state-action pairs). Transfer the trained encoder parameters to the KG-DQN modules responsible for encoding observations, graph, and actions. The QA pretraining thus bootstraps how the agent interprets/uses its knowledge-graph memory during RL.",
            "performance_with_memory": "QA pretraining improves both initial reward and convergence speed. Examples: Slice-of-life source (TextWorld) KG-DQN w/ QA: initial reward 2.8 ± 0.61 vs no transfer 2.6 ± 0.73; final reward 4.9 ± 0.09 vs 4.7 ± 0.23 (Table 2). Horror source (Afflicted) KG-DQN w/ QA: initial reward 4.3 ± 1.34 vs no transfer 3.0 ± 1.3; final 15.1 ± 1.60 vs 14.1 ± 1.73 (Table 3). For Anchorhead target, KG-DQN w/ QA (dense reward) final reward 24.8 ± 0.6 vs no transfer 6.8 ± 0.42 (Table 3).",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "memory_benefits": "Pretraining on oracle traces gives the agent an inductive bias on how to interpret textual observations and how to update/use the knowledge graph effectively, improving early performance and ultimately leading to higher-quality policies and faster convergence when combined with seeding and parameter transfer.",
            "memory_limitations_or_failures": "QA pretraining requires oracle traces or walkthroughs (which may not be available) and pretraining on very similar games to be most effective; does not by itself solve exploration under sparse reward for very complex games without other aids (e.g., dense oracle-augmented rewards).",
            "best_practices_or_recommendations": "Use QA pretraining from oracle or high-quality walkthrough traces to initialize observation/graph/action encoders; combine with knowledge-graph seeding and source-to-target parameter transfer for best gains; ensure pretraining corpus is representative of the target genre/domain.",
            "uuid": "e3259.2",
            "source_info": {
                "paper_title": "Transfer in Deep Reinforcement Learning Using Knowledge Graphs",
                "publication_date_yy_mm": "2019-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Language Understanding for Text-based Games Using Deep Reinforcement Learning",
            "rating": 2
        },
        {
            "paper_title": "Deep Reinforcement Learning with a Natural Language Action Space",
            "rating": 1
        },
        {
            "paper_title": "TextWorld : A Learning Environment for Text-based Games",
            "rating": 2
        },
        {
            "paper_title": "Nail: A general interactive fiction agent",
            "rating": 2
        },
        {
            "paper_title": "Reading Wikipedia to answer open-domain questions",
            "rating": 1
        }
    ],
    "cost": 0.013035999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Transfer in Deep Reinforcement Learning using Knowledge Graphs</h1>
<p>Prithviraj Ammanabrolu<br>School of Interactive Computing<br>Georgia Institute of Technology<br>Atlanta, GA<br>raj.ammanabrolu@gatech.edu</p>
<p>Mark O. Riedl<br>School of Interactive Computing<br>Georgia Institute of Technology<br>Atlanta, GA<br>riedl@cc.gatech.edu</p>
<h4>Abstract</h4>
<p>Text adventure games, in which players must make sense of the world through text descriptions and declare actions through text descriptions, provide a stepping stone toward grounding action in language. Prior work has demonstrated that using a knowledge graph as a state representation and question-answering to pre-train a deep Q-network facilitates faster control policy learning. In this paper, we explore the use of knowledge graphs as a representation for domain knowledge transfer for training text-adventure playing reinforcement learning agents. Our methods are tested across multiple computer generated and human authored games, varying in domain and complexity, and demonstrate that our transfer learning methods let us learn a higher-quality control policy faster.</p>
<h2>1 Introduction</h2>
<p>Text adventure games, in which players must make sense of the world through text descriptions and declare actions through natural language, can provide a stepping stone toward more realworld environments where agents must communicate to understand the state of the world and affect change in the world. Despite the steadily increasing body of research on text-adventure games (Bordes et al., 2010; He et al., 2016; Narasimhan et al., 2015; Fulda et al., 2017; Haroush et al., 2018; Côté et al., 2018; Tao et al., 2018; Ammanabrolu and Riedl, 2019), and in addition to the ubiquity of deep reinforcement learning applications (Parisotto et al., 2016; Zambaldi et al., 2019), teaching an agent to play text-adventure games remains a challenging task. Learning a control policy for a text-adventure game requires a significant amount of exploration, resulting in training runs that take hundreds of thousands of simulations (Narasimhan et al., 2015; Ammanabrolu and Riedl, 2019).</p>
<p>One reason that text-adventure games require so much exploration is that most deep reinforcement learning algorithms are trained on a task without a real prior. In essence, the agent must learn everything about the game from only its interactions with the environment. Yet, text-adventure games make ample use of commonsense knowledge (e.g., an axe can be used to cut wood) and genre themes (e.g., in a horror or fantasy game, a coffin is likely to contain a vampire or other undead monster). This is in addition to the challenges innate to the text-adventure game itself—games are puzzleswhich results in inefficient training.</p>
<p>Ammanabrolu and Riedl (2019) developed a reinforcement learning agent that modeled the text environment as a knowledge graph and achieved state-of-the-art results on simple text-adventure games provided by the TextWorld (Côté et al., 2018) environment. They observed that a simple form of transfer from very similar games greatly improved policy training time. However, games beyond the toy TextWorld environments are beyond the reach of state-of-the-art techniques.</p>
<p>In this paper, we explore the use of knowledge graphs and associated neural embeddings as a medium for domain transfer to improve training effectiveness on new text-adventure games. Specifically, we explore transfer learning at multiple levels and across different dimensions. We first look at the effects of playing a text-adventure game given a strong prior in the form of a knowledge graph extracted from generalized textual walk-throughs of interactive fiction as well as those made specifically for a given game. Next, we explore the transfer of control policies in deep Q-learning (DQN) by pre-training portions of a deep Q-network using question-answering and by DQN-to-DQN parameter transfer between games. We evaluate these techniques on two different sets of human authored and computer generated</p>
<p>games, demonstrating that our transfer learning methods enable us to learn a higher-quality control policy faster.</p>
<h2>2 Background and Related Work</h2>
<p>Text-adventure games, in which an agent must interact with the world entirely through natural language, provide us with two challenges that have proven difficult for deep reinforcement learning to solve (Narasimhan et al., 2015; Haroush et al., 2018; Ammanabrolu and Riedl, 2019): (1) The agent must act based only on potentially incomplete textual descriptions of the world around it. The world is thus partially observable, as the agent does not have access to the state of the world at any stage. (2) the action space is combinatorially large-a consequence of the agent having to declare commands in natural language. These two problems together have kept commercial text adventure games out of the reach of existing deep reinforcement learning methods, especially given the fact that most of these methods attempt to train on a particular game from scratch.</p>
<p>Text-adventure games can be treated as partially observable Markov decision processes (POMDPs). This can be represented as a 7-tuple of $\langle S, T, A, \Omega, O, R, \gamma\rangle$ : the set of environment states, conditional transition probabilities between states, words used to compose text commands, observations, conditional observation probabilities, the reward function, and the discount factor respectively (Côté et al., 2018).</p>
<p>Multiple recent works have explored the challenges associated with these games (Bordes et al., 2010; He et al., 2016; Narasimhan et al., 2015; Fulda et al., 2017; Haroush et al., 2018; Côté et al., 2018; Tao et al., 2018; Ammanabrolu and Riedl, 2019). Narasimhan et al. (2015) introduce the LSTM-DQN, which learns to score the action verbs and corresponding objects separately and then combine them into a single action. He et al. (2016) propose the Deep Reinforcement Relevance Network that consists of separate networks to encode state and action information, with a final Q-value for a state-action pair that is computed between a pairwise interaction function between these. Haroush et al. (2018) present the Action Elimination Network (AEN), which restricts actions in a state to the top-k most likely ones, using the emulator's feedback. Hausknecht et al. (2019b) design an agent that uses multiple mod- ules to identify a general set of game play rules for text games across various domains. None of these works study how to transfer policies between different text-adventure games in any depth and so there exists a gap between the two bodies of work.</p>
<p>Transferring policies across different textadventure games requires implicitly learning a mapping between the games' state and action spaces. The more different the domain of the two games, the harder this task becomes. Previous work (Ammanabrolu and Riedl, 2019) introduced the use of knowledge graphs and questionanswering pre-training to aid in the problems of partial observability and a combinatorial action space. This work made use of a system called TextWorld (Côté et al., 2018) that uses grammars to generate a series of similar (but not exact same) games. An oracle was used to play perfect games and the traces were used to pre-train portions of the agent's network responsible for encoding the observations, graph, and actions. Their results show that this form of pre-training improves the quality of the policy at convergence it does not show a significant improvement in the training time required to reach convergence. Further, it is generally unrealistic to have a corpus of very similar games to draw from. We build on this work, and explore modifications of this algorithm that would enable more efficient transfer in textadventure games.</p>
<p>Work in transfer in reinforcement learning has explored the idea of transferring skills (Konidaris and Barto, 2007; Konidaris et al., 2012) or transferring value functions/policies (Liu and Stone, 2006). Other approaches attempt transfer in model-based reinforcement learning (Taylor et al., 2008; Nguyen et al., 2012; Gasic et al., 2013; Wang et al., 2015; Joshi and Chowdhary, 2018), though traditional approaches here rely heavily on hand crafting state-action mappings across domains. Narasimhan et al. (2017) learn to play games by predicting mappings across domains using a both deep Q-networks and value iteration networks, finding that that grounding the game state using natural language descriptions of the game itself aids significantly in transferring useful knowledge between domains.</p>
<p>In transfer for deep reinforcement learning, Parisotto et al. (2016) propose the Actor-Mimic network which learns from expert policies for a source task using policy distillation and then ini-</p>
<p>tializes the network for a target task using these parameters. <em>Yin and Pan (2017)</em> also use policy distillation, using task specific features as inputs to a multi-task policy network and use a hierarchical experience sampling method to train this multitask network. Similarly, <em>Rusu et al. (2016)</em> attempt to transfer parameters by using frozen parameters trained on source tasks to help learn a new set of parameters on target tasks. <em>Rajendran et al. (2017)</em> attempt something similar but use attention networks to transfer expert policies between tasks. These works, however, do not study the requirements for enabling efficient transfer for tasks rooted in natural language, nor do they explore the use of knowledge graphs as a state representation.</p>
<h2>3 Knowledge Graphs for DQNs</h2>
<p>A knowledge graph is a directed graph formed by a set of semantic, or RDF, triples in the form of <em>〈subject, relation, object〉</em>—for example, <em>〈vampires, are, undead〉</em>. We follow the open-world assumption that what is not in our knowledge graph can either be true or false.</p>
<p><em>Ammanabrolu and Riedl (2019)</em> introduced the Knowledge Graph DQN (KG-DQN) and touched on some aspects of transfer learning, showing that pre-training portions of the deep Q-network using question answering system on perfect playthroughs of a game increases the quality of the learned control policy for a generated text-adventure game. We build on this work and use KG-DQN to explore transfer with both knowledge graphs and network parameters. Specifically we seek to transfer skills and knowledge from (a) static text documents describing game play and (b) from playing one text-adventure game to a second complete game in in the same genre (e.g., horror games). The rest of this section describes KG-DQN in detail and summarizes our modifications.</p>
<p>For each step that the agent takes, it automatically extracts a set of RDF triples from the received observation through the use of OpenIE ( [Angeli et al., 2015]) in addition to a few rules to account for the regularities of text-adventure games. The graph itself is more or less a map of the world, with information about objects' affordances and attributes linked to the rooms that they are place in in a map. The graph also makes a distinction with respect to items that are in the agent's</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The KG-DQN architecture.</p>
<p>possession or in their immediate surrounding environment. We make minor modifications to the rules used in <em>Ammanabrolu and Riedl (2019)</em> to better achieve such a graph in general interactive fiction environments.</p>
<p>The agent also has access to all actions accepted by the game's parser, following <em>Narasimhan et al. (2015)</em>. For general interactive fiction environments, we develop our own method to extract this information. This is done by extracting a set of templates accepted by the parser, with the objects or noun phrases in the actions replaces with a OBJ tag. An example of such a template is "place OBJ in OBJ". These OBJ tags are then filled in by looking at all possible objects in the given vocabulary for the game. This action space is of the order of $A = \mathcal{O(|V| \times |O|^2)}$ where $V$ is the number of action verbs, and $O$ is the number of distinct objects in the world that the agent can interact with. As this is too large a space for a RL agent to effectively explore, the knowledge graph is used to prune this space by ranking actions based on their presence in the current knowledge graph and the relations between the objects in the graph as in <em>Ammanabrolu and Riedl (2019)</em>.</p>
<p>The architecture for the deep Q-network consists of two separate neural networks—encoding state and action separately—with the final $Q$-value for a state-action pair being the result of a pairwise interaction function between the two (Figure 1). We train with a standard DQN training loop; the policy is determined by the $Q$-value of a particular state-action pair, which is updated using</p>
<p><sup>1</sup>We use the implementation of KG-DQN found at <a href="https://github.com/zajammanabrolu/KG-DQN">https://github.com/zajammanabrolu/KG-DQN</a></p>
<p>the Bellman equation (Sutton and Barto, 2018):</p>
<p>$$
\begin{aligned}
&amp; Q_{t+1}\left(s_{t+1}, a_{t+1}\right)= \
&amp; \qquad E\left[r_{t+1}+\gamma \max <em t="t">{a \in A</em>\right]
\end{aligned}
$$}} Q_{t}(s, a) \mid s_{t}, a_{t</p>
<p>where $\gamma$ refers to the discount factor and $r_{t+1}$ is the observed reward. The whole system is trained using prioritized experience replay [16], a modified version of $\epsilon$-greedy learning, and a temporal difference loss that is computed as:</p>
<p>$$
\begin{aligned}
L(\theta)= &amp; r_{k+1}+ \
&amp; \gamma \max <em _mathbf_k="\mathbf{k">{\mathbf{a} \in \mathbf{A}</em>}+1}} Q\left(\mathbf{s<em _mathbf_t="\mathbf{t">{\mathbf{t}}, \mathbf{a} ; \theta\right)-Q\left(\mathbf{s}</em> ; \theta\right)
\end{aligned}
$$}}, \mathbf{a}_{\mathbf{t}</p>
<p>where $\mathbf{A}<em _mathbf_t="\mathbf{t">{\mathbf{k + 1}}$ represents the action set at step $k+$ 1 and $\mathbf{s}</em>$ refer to the encoded state and action representations respectively.}}, \mathbf{a}_{\mathbf{t}</p>
<h2>4 Knowledge Graph Seeding</h2>
<p>In this section we consider the problem of transferring a knowledge graph from a static text resource to a DQN-which we refer to as seeding. KG-DQN uses a knowledge graph as a state representation and also to prune the action space. This graph is built up over time, through the course of the agent's exploration. When the agent first starts the game, however, this graph is empty and does not help much in the action pruning process. The agent thus wastes a large number of steps near the beginning of each game exploring ineffectively.</p>
<p>The intuition behind seeding the knowledge graph from another source is to give the agent a prior on which actions have a higher utility and thereby enabling more effective exploration. Textadventure games typically belong to a particular genre of storytelling-e.g., horror, sci-fi, or soap opera-and an agent is at a distinct disadvantage if it doesn't have any genre knowledge. Thus, the goal of seeding is to give the agent a strong prior.</p>
<p>This seed knowledge graph is extracted from online general text-adventure guides as well as game/genre specific guides when available. ${ }^{2}$ The graph is extracted from this the guide using a subset of the rules described in Section 3 used to extract information from the game observations, with the remainder of the RDF triples coming from OpenIE. There is no map of rooms in the environment that can be built, but it is possible to</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Select partial example of what a seed knowledge graph looks like. Ellipses indicate other similar entities and relations not shown.
extract information regarding affordances of frequently occurring objects as well as common actions that can be performed across a wide range of text-adventure games. This extracted graph is thus potentially disjoint, containing only this generalizable information, in contrast to the graph extracted during the rest of the exploration process. An example of a graph used to seed KG-DQN is given in Fig. 2. The KG-DQN is initialized with this knowledge graph.</p>
<h2>5 Task Specific Transfer</h2>
<p>The overarching goal of transfer learning in textadventure games is to be able to train an agent on one game and use this training on to improve the learning capabilities of another. There is growing body of work on improving training times on target tasks by transferring network parameters trained on source tasks (Rusu et al., 2016; Yin and Pan, 2017; Rajendran et al., 2017). Of particular note is the work by Rusu et al. (2016), where they train a policy on a source task and then use this to help learn a new set of parameters on a target task. In this approach, decisions made during the training of the target task are jointly made using the frozen parameters of the transferred policy network as well as the current policy network.</p>
<p>Our system first trains a question-answering system (Chen et al., 2017) using traces given by an oracle, as in Section 4. For commercial textadventure games, these traces take the form of state-action pairs generated using perfect walk-</p>
<p>through descriptions of the game found online as described in Section 4.</p>
<p>We use the parameters of the questionanswering system to pre-train portions of the deep Q-network for a different game within in the same domain. The portions that are pre-trained are the same parts of the architecture as in Ammanabrolu and Riedl (2019). This game is referred to as the source task. The seeding of the knowledge graph is not strictly necessary but given that state-of-theart DRL agents cannot complete real games, this makes the agent more effective at the source task.</p>
<p>We then transfer the knowledge and skills acquired from playing the source task to another game from the same genre-the target task. The parameters of the deep Q-network trained on the source game are used to initialize a new deep Qnetwork for the target task. All the weights indicated in the architecture of KG-DQN as shown in Fig. 1 are transferred. Unlike Rusu et al. (2016), we do not freeze the parameters of the deep Qnetwork trained on the source task nor use the two networks to jointly make decisions but instead just use it to initialize the parameters of the target task deep Q-network. This is done to account for the fact that although graph embeddings can be transferred between games, the actual graph extracted from a game is non-transferable due to differences in structure between the games.</p>
<h2>6 Experiments</h2>
<p>We test our system on two separate sets of games in different domains using the Jericho and TextWorld frameworks (Hausknecht et al., 2019a; Côté et al., 2018). The first set of games is "slice of life" themed and contains games that involve mundane tasks usually set in textual descriptions of normal houses. The second set of games is "horror" themed and contains noticeably more difficult games with a relatively larger vocabulary size and action set, non-standard fantasy names, etc. We choose these domains because of the availability of games in popular online gaming communities, the degree of vocabulary overlap within each theme, and overall structure of games in each theme. Specifically, there must be at least three games in each domain: at least one game to train the question-answering system on, and two more to train the parameters of the source and target task deep Q-networks. A summary of the statistics for the games is given in Table 1. Vocabulary overlap
is calculated by measuring the percentage of overlap between a game's vocabulary and the domain's vocabulary, i.e. the union of the vocabularies for all the games we use within the domain. We observe that in both of these domains, the complexity of the game increases steadily from the game used for the question-answering system to the target and then source task games.</p>
<p>We perform ablation tests within each domain, mainly testing the effects of transfer from seeding, oracle-based question-answering, and source-to-target parameter transfer. Additionally, there are a couple of extra dimensions of ablations that we study, specific to each of the domains and explained below. All experiments are run three times using different random seeds. For all the experiments we report metrics known to be important for transfer learning tasks (Taylor and Stone, 2009; Narasimhan et al., 2017): average reward collected in the first 50 episodes (init. reward), average reward collected for 50 episodes after convergence (final reward), and number of steps taken to finish the game for 50 episodes after convergence (steps). For the metrics tested after convergence, we set $\epsilon=0.1$ following both Narasimhan et al. (2015) and Ammanabrolu and Riedl (2019). We use similar hyperparameters to those reported in (Ammanabrolu and Riedl, 2019) for training the KG-DQN with action pruning, with the main difference being that we use 100 dimensional word embeddings instead of 50 dimensions for the horror genre.</p>
<h3>6.1 Slice of Life Experiments</h3>
<p>TextWorld uses a grammar to generate similar games. Following Ammanabrolu and Riedl (2019), we use TextWorld's "home" theme to generate the games for the question-answering system. TextWorld is a framework that uses a grammar to randomly generate game worlds and quests. This framework also gives us information such as instructions on how to finish the quest, and a list of actions that can be performed at each step based on the current world state. We do not let our agent access this additional solution information or admissible actions list. Given the relatively small quest length for TextWorld games-games can be completed in as little as 5 steps-we generate 50 such games and partition them into train and test sets in a 4:1 ratio. The traces are generated on the training set, and the question-answering system is</p>
<p>Table 1: Game statistics</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Slice of life</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Horror</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">QA/Source <br> TextWorld</td>
<td style="text-align: center;">Target <br> 9:05</td>
<td style="text-align: center;">QA <br> Lurking Horror</td>
<td style="text-align: center;">Source <br> Afflicted</td>
<td style="text-align: center;">Target <br> Anchorhead</td>
</tr>
<tr>
<td style="text-align: center;">Vocab size</td>
<td style="text-align: center;">788</td>
<td style="text-align: center;">297</td>
<td style="text-align: center;">773</td>
<td style="text-align: center;">761</td>
<td style="text-align: center;">2256</td>
</tr>
<tr>
<td style="text-align: center;">Branching factor</td>
<td style="text-align: center;">122</td>
<td style="text-align: center;">677</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">947</td>
<td style="text-align: center;">1918</td>
</tr>
<tr>
<td style="text-align: center;">Number of rooms</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">28</td>
</tr>
<tr>
<td style="text-align: center;">Completion steps</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">289</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">39</td>
</tr>
<tr>
<td style="text-align: center;">Words per obs.</td>
<td style="text-align: center;">65.1</td>
<td style="text-align: center;">45.2</td>
<td style="text-align: center;">68.1</td>
<td style="text-align: center;">81.2</td>
<td style="text-align: center;">114.2</td>
</tr>
<tr>
<td style="text-align: center;">New triples per obs.</td>
<td style="text-align: center;">6.4</td>
<td style="text-align: center;">4.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">12.6</td>
<td style="text-align: center;">17.0</td>
</tr>
<tr>
<td style="text-align: center;">\% Vocab overlap</td>
<td style="text-align: center;">19.70</td>
<td style="text-align: center;">21.45</td>
<td style="text-align: center;">22.80</td>
<td style="text-align: center;">14.40</td>
<td style="text-align: center;">66.34</td>
</tr>
<tr>
<td style="text-align: center;">Max. aug. reward</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">43</td>
</tr>
</tbody>
</table>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<h2>Bedroom</h2>
<p>This bedroom is extremely spare, with dirty laundry scattered haphazardly all over the floor. Cleaner clothing can be found in the dresser. A bathroom lies to the south, while a door to the east leads to the living room. On the end table are a telephone, a wallet and some keys.
$&gt;$ inventory
You are carrying:
some soiled clothing (being worn)
a gold watch (being worn)
$&gt;$ go south</p>
<h2>Bathroom</h2>
<p>This is a far from luxurious but still quite functional bathroom, with a sink, toilet and shower. The bedroom lies to the north.</p>
<p>Figure 3: Partial unseeded knowledge graph example given observations and actions in the game 9:05.
evaluated on the test set.
We then pick a random game from the test set to train our source task deep Q-network for this domain. For this training, we use the reward function provided by TextWorld: +1 for each action taken that moves the agent closer to finishing the quest; -1 for each action taken that extends the minimum number of steps needed to finish the quest from the current stage; 0 for all other situations.</p>
<p>We choose the game, 9:05 ${ }^{3}$ as our target task game due to similarities in structure in addition to the vocabulary overlap. Note that there are multiple possible endings to this game and we pick the simplest one for the purpose of training our agent.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<h2>Outside the Real Estate Office</h2>
<p>A grim little cul-de-sac, tucked away in a corner of the claustrophobic tangle of narrow, twisting avenues that largely constitute the older portion of Anchorhead. Like most of the streets in this city, it is ancient, shadowy, and leads essentially nowhere. The lane ends here at the real estate agent's office, which lies to the east, and winds its way back toward the center of town to the west. A narrow, garbage-choked alley opens to the southeast.
$&gt;$ go southeast
Alley
This narrow aperture between two buildings is nearly blocked with piles of rotting cardboard boxes and overstuffed garbage cans. Ugly, halfcrumbling brick walls to either side totter oppressively over you. The alley ends here at a tall, wooden fence. High up on the wall of the northern building there is a narrow, transom-style window.</p>
<p>Figure 4: Partial unseeded knowledge graph example given observations and actions in the game Anchorhead.</p>
<h3>6.2 Horror Experiments</h3>
<p>For the horror domain, we choose Lurking Hor$r o r^{k}$ to train the question-answering system on. The source and target task games are chosen as Afflicted $^{5}$ and Anchorhead ${ }^{6}$ respectively. However, due to the size and complexity of these two games some modifications to the games are required for the agent to be able to effectively solve them.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Reward curve for select experiments in the slice of life domain.</p>
<p>We partition each of these games and make them smaller by reducing the final goal of the game to an intermediate checkpoint leading to it. This checkpoints were identified manually using walkthroughs of the game; each game has a natural intermediate goal. For example, <em>Anchorhead</em> is segmented into 3 chapters in the form of objectives spread across 3 days, of which we use only the first chapter. The exact details of the games after partitioning is described in Table 1. For Lurking Horror, we report numbers relevant for the oracle walkthrough. We then pre-prune the action space and use only the actions that are relevant for the sections of the game that we have partitioned out. The majority of the environment is still available for the agent to explore but the game ends upon completion of the chosen intermediate checkpoint.</p>
<h3>6.3 Reward Augmentation</h3>
<p>The combined state-action space for a commercial text-adventure game is quite large and the corresponding reward function is very sparse in comparison. The default, implied reward signal is to receive positive value upon completion of the game, and no reward value elsewhere. This is problematic from an experimentation perspective as text-adventure games are too complex for even state-of-the-art deep reinforcement learning agents to complete. Even using transfer learning methods, a sparse reward signal usually results in ineffective exploration by the agent.</p>
<p>To make experimentation feasible, we augment the reward to give the agent a dense reward signal. Specifically, we use an oracle to generate state-action traces (identical to how as when training the question-answering system). An oracle is an agent that is capable of playing and finishing a game perfectly in the least number of steps possible. The state-action pairs generated using perfect walkthroughs of the game are then used as checkpoints and used to give the agent additional reward. If the agent encounters any of these state-action pairs when training, i.e. performs the right action given a corresponding state, it receives a proportional reward in addition to the standard reward built into the game. This reward is scaled based on the game and is designed to be less than the smallest reward given by the original reward function to prevent it from overpowering the built-in reward. We refer to agents using this technique as having "dense" reward and "sparse" reward otherwise. The agent otherwise receives no information from the oracle about how to win the game.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Reward curve for select experiments in the horror domain.</p>
<h3>7 Results/Discussion</h3>
<p>The structure of the experiments are such that the for each of the domains, the target task game is more complex than the source task game. The slice of life games are also generally less complex than the horror games; they have a simpler vocabulary and a more linear quest structure. Additionally, given the nature of interactive fiction games, it is nearly impossible—even for human players—to achieve completion in the minimum number of steps (as given by the steps to completion in Table 1); each of these games are puzzle based and require extensive exploration and interaction with various objects in the environment to complete.</p>
<p>Table 2 and Table 3 show results for the slice of life and horror domains, respectively. In both do-</p>
<p>Table 2: Results for the slice of life games. "KG-DQN Full" refers to KG-DQN when seeded first, trained on the source, then transferred to the target. All experiment with QA indicate pre-training. S, D indicate sparse and dense reward respectively.</p>
<table>
<thead>
<tr>
<th>Experiment</th>
<th>Init. Rwd.</th>
<th>Final Rwd.</th>
<th>Steps</th>
</tr>
</thead>
<tbody>
<tr>
<td>Source Game (TextWorld)</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>KG-DQN no transfer</td>
<td>$2.6 \pm 0.73$</td>
<td>$4.7 \pm 0.23$</td>
<td>$110.83 \pm 4.92$</td>
</tr>
<tr>
<td>KG-DQN w/ QA</td>
<td>$2.8 \pm 0.61$</td>
<td>$4.9 \pm 0.09$</td>
<td>$88.57 \pm 3.45$</td>
</tr>
<tr>
<td>KG-DQN seeded</td>
<td>$3.2 \pm 0.57$</td>
<td>$4.8 \pm 0.16$</td>
<td>$91.43 \pm 1.89$</td>
</tr>
<tr>
<td>Target Game (9:05)</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>KG-DQN untuned (D)</td>
<td>-</td>
<td>$2.5 \pm 0.48$</td>
<td>$1479.0 \pm 22.3$</td>
</tr>
<tr>
<td>KG-DQN no transfer (S)</td>
<td>-</td>
<td>-</td>
<td>$1916.0 \pm 33.17$</td>
</tr>
<tr>
<td>KG-DQN no transfer (D)</td>
<td>$0.8 \pm 0.32$</td>
<td>$16.5 \pm 1.58$</td>
<td>$1267.2 \pm 7.5$</td>
</tr>
<tr>
<td>KG-DQN w/ QA (S)</td>
<td>-</td>
<td>-</td>
<td>$1428.0 \pm 11.26$</td>
</tr>
<tr>
<td>KG-DQN w/ QA (D)</td>
<td>$1.3 \pm 0.24$</td>
<td>$17.4 \pm 1.84$</td>
<td>$1127.0 \pm 31.22$</td>
</tr>
<tr>
<td>KG-DQN seeded (D)</td>
<td>$1.4 \pm 0.35$</td>
<td>$16.7 \pm 2.41$</td>
<td>$1393.33 \pm 26.5$</td>
</tr>
<tr>
<td>KG-DQN Full (D)</td>
<td>$2.7 \pm 0.65$</td>
<td>$19.7 \pm 2.0$</td>
<td>$274.76 \pm 21.45$</td>
</tr>
</tbody>
</table>
<p>Table 3: Results for horror games. Note that the reward type is dense for all results. "KG-DQN Full" refers to KG-DQN seeded, transferred from source. All experiment with QA indicate pre-training.</p>
<table>
<thead>
<tr>
<th>Experiment</th>
<th>Init. Rwd.</th>
<th>Final Rwd.</th>
<th>Steps</th>
</tr>
</thead>
<tbody>
<tr>
<td>Source Game (Afflicted)</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>KG-DQN no transfer</td>
<td>$3.0 \pm 1.3$</td>
<td>$14.1 \pm 1.73$</td>
<td>$1934.7 \pm 85.67$</td>
</tr>
<tr>
<td>KG-DQN w/ QA</td>
<td>$4.3 \pm 1.34$</td>
<td>$15.1 \pm 1.60$</td>
<td>$1179 \pm 32.07$</td>
</tr>
<tr>
<td>KG-DQN seeded</td>
<td>$4.1 \pm 1.19$</td>
<td>$14.6 \pm 1.26$</td>
<td>$1125.3 \pm 49.57$</td>
</tr>
<tr>
<td>Target Game (Anchorhead)</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>KG-DQN untuned</td>
<td>-</td>
<td>$3.8 \pm 0.23$</td>
<td>-</td>
</tr>
<tr>
<td>KG-DQN no transfer</td>
<td>$1.0 \pm 0.34$</td>
<td>$6.8 \pm 0.42$</td>
<td>-</td>
</tr>
<tr>
<td>KG-DQN w/ QA</td>
<td>$3.6 \pm 0.91$</td>
<td>$24.8 \pm 0.6$</td>
<td>$4874 \pm 90.74$</td>
</tr>
<tr>
<td>KG-DQN seeded</td>
<td>$1.7 \pm 0.62$</td>
<td>$26.6 \pm 0.42$</td>
<td>$4937 \pm 42.93$</td>
</tr>
<tr>
<td>KG-DQN full</td>
<td>$4.1 \pm 0.9$</td>
<td>$39.9 \pm 0.53$</td>
<td>$4334.3 \pm 56.13$</td>
</tr>
</tbody>
</table>
<p>mains seeding and QA pre-training improve performance by similar amounts from the baseline on both the source and target task games. A series of t-tests comparing the results of the pre-training and graph seeding with the baseline KG-DQN show that all results are significant with $p&lt;0.05$. Both the pre-training and graph seeding perform similar functions in enabling the agent to explore more effectively while picking high utility actions.</p>
<p>Even when untuned, i.e. evaluating the agent on the target task after having only trained on the source task, the agent shows better performance than training on the target task from scratch using the sparse reward. As expected, we see a further gain in performance when the dense reward function is used for both of these domains as well. In the horror domain, the agent fails to converge to a state where it is capable of finishing the game without the dense reward function due to the horror games being more complex.</p>
<p>When an agent is trained using on just the target task horror game, Anchorhead, it does not converge to completion and only gets as far as achieving a reward of approximately 7 (max. observed reward from the best model is 41). This corre-
sponds to a point in the game where the player is required to use a term in an action that the player has never observed before, "look up Verlac" when in front of a certain file cabinet-"Verlac" being the unknown entity. Without seeding or QA pretraining, the agent is unable to cut down the action space enough to effectively explore and find the solution to progress further. The relative effectiveness of the gains in initial reward due to seeding appears to depend on the game and the corresponding static text document. In all situations except Anchohead, seeding provides comparable gains in initial reward as compared to QA - there is no statistical difference between the two when performing similar t-tests.</p>
<p>When the full system is used-i.e. we seed the knowledge graph, pre-train QA, then train the source task game, then the target task game using the augmented reward function-we see a significant gain in performance, up to an $80 \%$ gain in terms of completion steps in some cases. The bottleneck at reward 7 is still difficult to pass, however, as seen in Fig. 6, in which we can see that the agent spends a relatively long time around this reward level unless the full transfer technique is</p>
<p>used. We further see in Figures 5, 6 that transferring knowledge results in the agent learning this higher quality policy much faster. In fact, we note that training a full system is more efficient than just training the agent on a single task, i.e. training a QA system then a source task game for 50 episodes then transferring and training a seeded target task game for 50 episodes is more effective than just training the target task game by itself for even 150+ episodes.</p>
<h2>8 Conclusions</h2>
<p>We have demonstrated that using knowledge graphs as a state representation enables efficient transfer between deep reinforcement learning agents designed to play text-adventure games, reducing training times and increasing the quality of the learned control policy. Our results show that we are able to extract a graph from a general static text resource and use that to give the agent knowledge regarding domain specific vocabulary, object affordances, etc. Additionally, we demonstrate that we can effectively transfer knowledge using deep Q-network parameter weights, either by pretraining portions of the network using a questionanswering system or by transferring parameters from a source to a target game. Our agent trains faster overall, including the number of episodes required to pre-train and train on a source task, and performs up to $80 \%$ better on convergence than an agent not utilizing these techniques.</p>
<p>We conclude that knowledge graphs enable transfer in deep reinforcement learning agents by providing the agent with a more explicit-and interpretable-mapping between the state and action spaces of different games. This mapping helps overcome the challenges twin challenges of partial observability and combinatorially large action spaces inherent in all text-adventure games by allowing the agent to better explore the stateaction space.</p>
<h2>9 Acknowledgements</h2>
<p>This material is based upon work supported by the National Science Foundation under Grant No. IIS1350339. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.</p>
<h2>References</h2>
<p>Prithviraj Ammanabrolu and Mark O. Riedl. 2019. Playing text-adventure games with graph-based deep reinforcement learning. In Proceedings of 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACLHLT 2019.</p>
<p>Gabor Angeli, Johnson Premkumar, Melvin Jose, and Christopher D. Manning. 2015. Leveraging Linguistic Structure For Open Domain Information Extraction. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers).</p>
<p>Antoine Bordes, Nicolas Usunier, Ronan Collobert, and Jason Weston. 2010. Towards understanding situated natural language. In Proceedings of the 2010 International Conference on Artificial Intelligence and Statistics.</p>
<p>Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading Wikipedia to answer opendomain questions. In Association for Computational Linguistics (ACL).</p>
<p>Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben Kybartas, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, and Adam Trischler. 2018. TextWorld : A Learning Environment for Text-based Games. In Proceedings of the ICML/IJCAI 2018 Workshop on Computer Games, page 29.</p>
<p>Nancy Fulda, Daniel Ricks, Ben Murdoch, and David Wingate. 2017. What can you do with a rock? affordance extraction via word embeddings. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17, pages 1039-1045.</p>
<p>Milica Gasic, Catherine Breslin, Matthew Henderson, Dongho Kim, Martin Szummer, Blaise Thomson, Pirros Tsiakoulis, and Steve J. Young. 2013. Pomdp-based dialogue manager adaptation to extended domains. In SIGDIAL Conference.</p>
<p>Matan Haroush, Tom Zahavy, Daniel J Mankowitz, and Shie Mannor. 2018. Learning How Not to Act in Text-Based Games. In Workshop Track at ICLR 2018, pages 1-4.</p>
<p>Matthew Hausknecht, Prithviraj Ammanabrolu, MarcAlexandre Côté, and Xingdi Yuan. 2019a. Interactive fiction games: A colossal adventure. CoRR, abs/1909.05398.</p>
<p>Matthew Hausknecht, Ricky Loynd, Greg Yang, Adith Swaminathan, and Jason D. Williams. 2019b. Nail: A general interactive fiction agent. CoRR, abs/1902.04259.</p>
<p>Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari Ostendorf. 2016. Deep Reinforcement Learning with a Natural Language Action Space. In Association for Computational Linguistics (ACL).</p>
<p>Girish Joshi and Girish Chowdhary. 2018. Crossdomain transfer in reinforcement learning using target apprentice. In Proceedings of the International Conference on Robotics and Automation, pages $7525-7532$.</p>
<p>George Konidaris and Andrew G. Barto. 2007. Building portable options: Skill transfer in reinforcement learning. In IJCAI.</p>
<p>George Konidaris, Ilya Scheidwasser, and Andrew G. Barto. 2012. Transfer in reinforcement learning via shared features. The Journal of Machine Learning Research, 13:1333-1371.</p>
<p>Long-Ji Lin. 1993. Reinforcement learning for robots using neural networks. Ph.D. thesis, Carnegie Mellon University.</p>
<p>Yaxin Liu and Peter Stone. 2006. Value-function-based transfer for reinforcement learning using structure mapping. In $A A A I$.</p>
<p>Karthik Narasimhan, Regina Barzilay, and Tommi Jaakkola. 2017. Deep transfer in reinforcement learning by language grounding. Journal of Artificial Intelligence Research, 63.</p>
<p>Karthik Narasimhan, Tejas Kulkarni, and Regina Barzilay. 2015. Language Understanding for Textbased Games Using Deep Reinforcement Learning. In Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Trung Thanh Nguyen, Tomi Silander, and Tze-Yun Leong. 2012. Transferring expectations in modelbased reinforcement learning. In NIPS.</p>
<p>Emilio Parisotto, Jimmy Ba, and Ruslan R. Salakhutdinov. 2016. Actor-mimic: Deep multitask and transfer reinforcement learning. CoRR, abs/1511.06342.</p>
<p>Janarthanan Rajendran, Aravind S. Lakshminarayanan, Mitesh M. Khapra, P. Prasanna, and Balaraman Ravindran. 2017. Attend, adapt and transfer: Attentive deep architecture for adaptive transfer from multiple sources in the same domain. In $I C L R$.</p>
<p>Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. 2016. Progressive neural networks. CoRR, abs/1606.04671.</p>
<p>Richard S Sutton and Andrew G Barto. 2018. Reinforcement Learning: An Introduction. MIT Press.</p>
<p>Ruo Yu Tao, Marc-Alexandre Côté, Xingdi Yuan, and Layla El Asri. 2018. Towards solving text-based
games by producing adaptive action spaces. In Proceedings of the 2018 NeurIPS Workshop on Wordplay: Reinforcement and Language Learning in Text-based Games.</p>
<p>Matthew E. Taylor, Nicholas K. Jong, and Peter Stone. 2008. Transferring instances for model-based reinforcement learning. In ECML/PKDD.</p>
<p>Matthew E. Taylor and Peter Stone. 2009. Transfer learning for reinforcement learning domains: A survey. Journal of Machine Learning Research, 10:1633-1685.</p>
<p>Zhuoran Wang, Tsung-Hsien Wen, Pei hao Su, and Yannis Stylianou. 2015. Learning domainindependent dialogue policies via ontology parameterisation. In SIGDIAL Conference.
H. Yin and S. J. Pan. 2017. Knowledge transfer for deep reinforcement learning with hierarchical experience replay. In Proceedings of the ThirtyFirst AAAI Conference on Artificial Intelligence, AAAI'17, pages 1640-1646. AAAI Press.</p>
<p>Vinicius Zambaldi, David Raposo, Adam Santoro, Victor Bapst, Yujia Li, Igor Babuschkin, Karl Tuyls, David Reichert, Timothy Lillicrap, Edward Lockhart, Murray Shanahan, Victoria Langston, Razvan Pascanu, Matthew Botvinick, Oriol Vinyals, and Peter Battaglia. 2019. Deep reinforcement learning with relational inductive biases. In International Conference on Learning Representations.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://ifdb.tads.org/viewgame?id= qzftg3j8nh5f34i2&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{4}$ https://ifdb.tads.org/viewgame?id= jhbd0kja1t57uop
${ }^{5}$ https://ifdb.tads.org/viewgame?id= ep14q2933rczoo9x
${ }^{6}$ https://ifdb.tads.org/viewgame?id= op0uw1gn1tjqmjt7&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
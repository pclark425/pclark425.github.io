<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3015 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3015</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3015</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-270380045</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.06576v4.pdf" target="_blank">OccamLLM: Fast and Exact Language Model Arithmetic in a Single Step</a></p>
                <p><strong>Paper Abstract:</strong> Despite significant advancements in text generation and reasoning, Large Language Models (LLMs) still face challenges in accurately performing complex arithmetic operations. To achieve accurate calculations, language model systems often enable LLMs to generate code for arithmetic operations. However, this approach compromises speed and security and, if finetuning is involved, risks the language model losing prior capabilities. We propose a framework that enables exact arithmetic in a single autoregressive step , providing faster, more secure, and more interpretable LLM systems with arithmetic capabilities. We use the hidden states of an LLM to control a symbolic architecture which performs arithmetic. Our implementation using Llama 3 8B Instruct with OccamNet as a symbolic model (OccamLlama) achieves 100% accuracy on single arithmetic operations ( + , − , × , ÷ , sin , cos , log , exp , √ ), outperforming GPT 4o and on par with GPT 4o using a code interpreter. OccamLlama also outperforms GPT 4o both with and without a code interpreter on mathematical problem solving benchmarks involving challenging arithmetic, thus enabling small LLMs to match the arithmetic performance of even much larger models. We will make our code public shortly.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3015.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3015.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OccamLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OccamLLM (OccamLlama when using Llama)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neurosymbolic framework that leaves the base LLM frozen and uses its per-token hidden states to initialize a symbolic model (OccamNet) which evaluates exact arithmetic in a single autoregressive step; a learned switch routes generation to either the LLM or OccamNet per token.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OccamLLM / OccamLlama (OccamLlama 8B and 70B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>System combining a frozen transformer LLM (Llama 3 Instruct, 8B or 70B) and a separately trained symbolic decoder that maps LLM hidden states to OccamNet softmax-layer weights; a learned binary switch (sigmoid MLP) routes outputs. LLM weights are not fine-tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Single-operation arithmetic (+, −, ×, ÷, sqrt, power, log, exp, sin, cos) on large integers and floats; single-step and some multi-step arithmetic via a 2-layer OccamNet (up to 3 ops); arithmetic within word problems (AddSub, MultiArith, MultiArith-Float, GSM8K, MATH401, Single Eq, SVAMP).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Decoder maps LLM internal hidden-state representations to a probability distribution (weights) over symbolic functions in OccamNet; OccamNet samples (or the max-probability) DAG which implements a specific symbolic function and evaluates it on parsed numeric inputs; switch learns when to substitute OccamNet output for LLM token generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Trained decoders (weight-decoder and switch) achieve perfect or near-perfect arithmetic on held-out synthetic data; non-textual training (decoder trained only on numeric expressions) still generalizes to word problems, indicating arithmetic representations are present in LLM hidden states; multilingual OOD tests show robustness; empirical benchmarks show OccamLlama 8B/70B obtain 100% accuracy on single-op arithmetic tests (9000 problems) and strong gains on math benchmarks compared to LLM baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Limitations documented: single-layer OccamNet cannot natively evaluate compound expressions (e.g., chained sums) generated by the LLM; switch can misroute when LLM outputs fractions/percentages or compound inline expressions not seen during switch training; some LLM generations append digits to OccamNet outputs requiring engineering workarounds.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Learned external symbolic module controlled by LLM hidden states (decoder-to-symbolic-model initialization) plus a learned switch; no fine-tuning of LLM; alternative interventions discussed (code interpreters, finetuning, chain-of-thought prompting) are compared.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Enabled exact arithmetic evaluation in a single autoregressive step (no code execution), increased speed (single forward pass vs. multi-token code generation), improved security (no arbitrary code execution), and interpretability (explicit symbolic function chosen). Led to dramatic increases in arithmetic accuracy vs. unfused LLMs and outperformed or matched code-interpreter baselines with far fewer tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Single-op arithmetic: OccamLlama (8B & 70B) 100.0 ± 0.0% accuracy on {+,−,×,÷,sqrt,exp,log,sin,cos} over 9000 problems; GPT‑4o (no code) average ~39.5 ± 0.5% on same tasks; GPT‑4o + Code Interpreter (subset) 99.8 ± 0.1% (missed 3/1800). Multi-step (two-layer OccamNet): one-step 99.9 ± 0.17%, two-step 98.2 ± 0.45%, three-step 96.1 ± 0.64% (Table 3). Math benchmarks: OccamLlama 8B average ≈ 89.9 ± 1.29% across selected reasoning datasets; OccamLlama 70B average ≈ 94.6 ± 0.98% (per reported tables). OccamLlama uses OccamNet in a single forward pass; GPT‑4o+Code generated >54 tokens on average for the same arithmetic prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Switch misfires on fraction/percentage formats and compound expressions (due to single-layer OccamNet limits and limited switch training distribution); LLM sometimes appends extra digits to OccamNet outputs; single-layer OccamNet cannot evaluate chained operations created inline by the LLM without additional decomposition or a deeper OccamNet; integration with advanced decoding (e.g., speculative decoding) not explored.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Direct comparison to exact symbolic computation: OccamLLM delegates numeric computation to a symbolic evaluator (OccamNet) that returns exact arithmetic, matching or exceeding calculator-like accuracy; authors contrast OccamLLM with code-based tool use (GPT‑4o + Code) and with finetuning approaches that risk catastrophic forgetting—OccamLLM attains calculator-level exactness while keeping LLM frozen.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'OccamLLM: Fast and Exact Language Model Arithmetic in a Single Step', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3015.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3015.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OccamNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OccamNet: a probabilistic symbolic-function architecture</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A symbolic/neurosymbolic architecture that parameterizes a probability distribution over compositions of primitive functions by using softmax layers whose sampled one-hot connections define DAGs (symbolic programs) which are evaluated on numeric inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OccamNet: A Fast Neural Model for Symbolic Regression at Scale.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OccamNet (Complete OccamNet variant used)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A complete-occamnet variant: an l-layer MLP-like structure with softmax selection of inputs per activation node; primitives include arithmetic ops and transcendental functions; probability over functions defined by product of sampled connection probabilities; used here as a deterministic symbolic evaluator after decoder initialization.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Represents and evaluates symbolic single- and multi-operator arithmetic expressions (primitives: +, −, ×, ÷, sqrt, power, log, exp, sin, cos); can represent up to l compositions (complete OccamNet includes repeats and skip-connections to allow shallower compositions).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Parametrizes distributions over DAGs (symbolic expressions) by softmax weights per connection; sampling yields sparse DAGs that are executed as deterministic symbolic functions on numeric inputs; decoder learns to set OccamNet softmax weights from LLM hidden states to prefer the correct function.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Used as the arithmetic engine for OccamLLM; sampling-based training and REINFORCE-style (rescaled) loss leads to OccamNet initializations that produce exact answers across synthetic arithmetic datasets; one can inspect weights to interpret selected functions (interpretable selection).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Authors note OccamNet multi-representation ambiguity (same function can be represented by many DAGs) complicates cross-entropy; also single-layer networks cannot represent chained multiple-operator expressions. EQL alternative cited but argued less suitable.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Decoder trains to output OccamNet softmax-layer weights from LLM hidden states; training uses Monte-Carlo sampling of OccamNet and a REINFORCE/cross-entropy-style loss; OccamNet itself is not fine-tuned as a separate end-to-end gradient to LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>When initialized by the decoder, OccamNet evaluates exact arithmetic functions that the LLM would otherwise approximate or mishandle, converting approximate LLM outputs into exact symbolic evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>As part of OccamLlama, OccamNet-based evaluations achieved 100% accuracy on single-op arithmetic tests and near-100% for up to three-operator expressions when using a two-layer OccamNet (one-step 99.9%, two-step 98.2%, three-step 96.1%).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Expressive limitations when shallow (single layer): cannot evaluate expressions requiring more compositions than layer depth; multiple DAGs can represent same function making canonical-target cross-entropy formulation nontrivial; scaling sampling cost increases with larger OccamNets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Serves as an explicit symbolic calculator within the LLM system; compared qualitatively to running externally generated code (tool use) but is internal and interpretable; provides deterministic arithmetic comparable to calculators/algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'OccamLLM: Fast and Exact Language Model Arithmetic in a Single Step', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3015.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3015.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM hidden-state decoder & switch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OccamLLM decoder (weight decoders) and switch decoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Per-token MLP-based decoders that map LLM hidden states (optionally weighted across layers) to (a) OccamNet softmax-layer weight initializations selecting symbolic functions and (b) a binary routing score that chooses whether to use OccamNet output or LLM token output.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OccamLLM decoders (weight-decoders and switch-decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder_i(h) = MLP_i(sum_j w_i,j h_j) + W*_i for each OccamNet softmax layer; switch is sigmoid(MLP_switch(sum_j w_switch,j h_j)). MLPs are small two-layer networks; decoders are trained while LLM remains frozen.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Controls what symbolic function OccamNet will evaluate for arithmetic tasks and decides when to substitute OccamNet outputs for token-generation during word problems and arithmetic queries.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Hidden states of the transformer encode arithmetic-relevant features; linear combinations across transformer layers followed by small MLPs can decode those features into parameters that bias OccamNet toward the intended symbolic function; binary switch decodes when the numeric answer token should come from OccamNet.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Training the decoders on synthetic datasets yields high accuracy; non-textual (numbers-only) decoder training still generalizes to text-based word problems, indicating arithmetic structure is present in hidden states; ablation-like observations: the system functions with frozen LLM weights and with decoders trained separately.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Switch can fail when confronted with text forms not seen in switch training (fractions, percentages, compound inline expressions), revealing limits in training coverage rather than a failure of representational decoding per se.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Trained decoders (supervised/rescaled REINFORCE for OccamNet decoder, binary cross-entropy for switch); no LLM fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Enables the LLM to 'control' an exact symbolic evaluator using internal representations, converting approximate LLM numeric behavior into exact computation without altering LLM weights.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Decoder trained on 80k examples (40k single queries + 40k concatenated queries) with sampled OccamNet draws (100 samples per token in 1-layer runs) achieves function-selection sufficient to yield OccamLlama 100% single-op accuracy; switch trained on 50k examples (80k for 70B variant) achieves routing adequate for high benchmark performance, though some datasets (GSM8K, Single Eq) show switch-related errors.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Switch misroutes for out-of-distribution textual formats; decoder depends on parser that extracts numeric tokens from text (parser failures produce errors); sample inefficiency early in training due to sparse reward across large OccamNet function space.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>This intervention treats the LLM as providing latent intent/representation and uses a symbolic evaluator to execute exact arithmetic—conceptually similar to a human using internal working memory to signal a calculator which operation to perform.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'OccamLLM: Fast and Exact Language Model Arithmetic in a Single Step', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3015.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3015.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama 3 (base LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 3 Instruct (8B and 70B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The base frozen transformer language models used by OccamLLM experiments; provide hidden states and natural-language reasoning while arithmetic is delegated to OccamNet.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 3 Instruct (8B and 70B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based instruction-tuned LLMs (Llama 3 Instruct) used as the frozen language model backbone; hidden size reported (4096 for 8B, 8192 for 70B) and layerwise hidden states are aggregated by decoders.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Left to perform reasoning, prompt-driven decomposition, and sometimes arithmetic when OccamNet is not invoked (word-problem reasoning, selection of numeric tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>LLM produces hidden states that (1) encode numeric and operation context which decoders can map to symbolic functions and (2) in many contexts can themselves output approximate arithmetic results or structured steps but with limited exactness on large numeric tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>OccamLLM experiments demonstrate that Llama 3 hidden states are sufficient for the decoder to select correct OccamNet functions even when the decoder was trained on numbers-only examples; baseline Llama 3 performance is far worse on high-precision arithmetic tasks (e.g., near 0% on 7-digit multiplication/division).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Llama 3 alone fails on many arithmetic tasks (e.g., low or zero accuracy on large-digit multiplication/division) indicating its internal autoregressive outputs are not reliably exact for arithmetic without a symbolic module.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Kept frozen; used as a provider of hidden states for the decoder and as the reasoning component in OccamLLM. Authors also experimented with prompting (system prompts like 'Solve step by step.') when evaluating reasoning datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Using Llama 3 as a frozen backbone with OccamNet yields large arithmetic accuracy improvements without finetuning the LLM; shows smaller models can match or exceed larger proprietary models in arithmetic when paired with a symbolic evaluator.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Llama 3 8B baseline: poor arithmetic on 7-digit tasks (e.g., below ~50% on 7-digit addition; 0% on 7-digit multiplication in Table 2); when used as OccamLlama backbone performance is inherited by the hybrid system (OccamLlama 8B and 70B metrics above).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>LLM outputs approximate or improperly formatted numeric answers (fractions, percentages, chained expressions) that require the switch to suppress OccamNet usage; LLM tendency to append digits to OccamNet outputs required heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>As the 'reasoner' in the hybrid system, Llama 3 handles language and decomposition tasks akin to human reasoning, while OccamNet performs exact calculation like a calculator.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'OccamLLM: Fast and Exact Language Model Arithmetic in a Single Step', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3015.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3015.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT‑4o (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT‑4o (2024-05-13)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art proprietary LLM baseline evaluated in the paper for arithmetic capability both with and without a code interpreter tool; used for performance comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT‑4o (and GPT‑4o + Code Interpreter)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's GPT‑4-family model (GPT‑4o) evaluated in standard chat configuration and with a code-interpreter tool (external code execution). Used as comparative baselines rather than modified within the paper's method.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Same set of arithmetic tasks and math benchmarks: 7-digit integer arithmetic, transcendental functions, and word-problem datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>When using code-interpreter, GPT‑4o outputs program code that, when executed by the tool, computes exact arithmetic externally; without tool use, GPT‑4o relies on internal autoregressive patterns and learned heuristics to produce numeric answers (often approximate).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirically, GPT‑4o without tool use misses many arithmetic cases (e.g., ~39.5% average on table of arithmetic tasks), while GPT‑4o + Code achieves near-perfect accuracy on the tested subset (99.8% on ~1800 examples) at the cost of many generated tokens and external code execution.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Even with a code interpreter, GPT‑4o+Code did not always improve math-benchmark performance relative to GPT‑4o on some datasets (authors report GPT‑4o+Code sometimes underperforms GPT‑4o across math benchmarks), suggesting current code-calling practices are not a panacea for math reasoning beyond small tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Tool use (external code execution via code-interpreter); also chain-of-thought prompting and system prompts used in evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Code interpreter yields near-exact arithmetic but with high token-generation overhead and potential security concerns due to arbitrary code execution; OccamLLM achieves similar or better accuracy with fewer tokens and without executing generated code.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Arithmetic table: GPT‑4o (no code) average ~39.5 ± 0.5% on 7-digit tasks; GPT‑4o + Code Interpreter (subset tested) 99.8 ± 0.1% accuracy (missed 3/1800 examples). On broader reasoning benchmarks GPT‑4o and GPT‑4o+Code show mixed results — OccamLlama 70B outperforms GPT‑4o on average across the selected mathematical problem solving benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Without external code, internal autoregressive outputs are error-prone for high-precision arithmetic (wrong digit positions, rounding issues); code-based method is slower (many tokens) and raises security concerns about executing LLM-generated code.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>GPT‑4o + Code effectively uses symbolic/programmatic execution (like a human writing code and running it) to obtain exact answers; OccamLLM provides an internal symbolic mechanism achieving similar exactness but integrated and interpretable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'OccamLLM: Fast and Exact Language Model Arithmetic in a Single Step', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3015.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3015.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Finetuning / PEFT (prior approaches)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-tuning and Parameter-Efficient Fine-Tuning approaches for arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior interventions that improve LLM arithmetic by continuing training on arithmetic datasets (full fine-tuning or PEFT), which can produce high arithmetic accuracy but risk catastrophic forgetting of original LLM capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Finetuning / PEFT approaches (general)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Approaches that update LLM weights (full or adapter/LoRA-style PEFT) on arithmetic-specific datasets to improve numeric computation performance within the model's weights.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Arithmetic datasets (add, subtract, multiply, divide, powers) and math reasoning benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Directly modifies model weights so internal computation (transformer activations and logits) encode arithmetic algorithms or memorized mappings, enabling the model to output accurate numeric results without external tools.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Prior work cited reports near-perfect arithmetic after specialized training on synthetic arithmetic datasets; PEFT can partially mitigate catastrophic forgetting but risk remains.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Authors highlight catastrophic forgetting risks (loss of prior linguistic capabilities) when finetuning on narrow datasets; also limited generality when trained only on arithmetic formats (lack of robustness on word problems).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Weight updates via full finetuning or PEFT on arithmetic datasets; contrasted with OccamLLM's frozen-LLM approach.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Can produce near-perfect results on in-distribution arithmetic datasets but may degrade broader language performance and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Prior literature reports near-perfect accuracy on arithmetic when models are trained specifically for those tasks (cited qualitatively in related work), but concrete numbers vary by study/dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Catastrophic forgetting, limited generalization outside arithmetic-format training data, and need for large annotated datasets or PEFT tuning strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Finetuned models close the gap to symbolic calculators on trained distributions but do so by embedding computation into weights rather than delegating to an explicit symbolic evaluator.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'OccamLLM: Fast and Exact Language Model Arithmetic in a Single Step', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3015.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3015.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompting / Chain-of-Thought / Tool-calling (mentioned baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt engineering (chain-of-thought), tool-calling (Toolformer/Calc‑X), and code interpreters</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Described classes of interventions for improving arithmetic in LLMs: chain-of-thought prompting to elicit intermediate reasoning, and training models to call external calculators or code interpreters (Toolformer, Calc‑X) to compute exact numeric answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Prompting (CoT) and Tool Use frameworks (Toolformer, Calc‑X, code interpreter workflows)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>CoT: prompting that encourages step-by-step reasoning; Tool-calling: training or prompting to insert API/tool calls (external calculator or code execution) that compute numbers exactly; Toolformer/Calc‑X are frameworks to teach LMs to use tools.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Arithmetic and multi-step math word problems where exact numeric computation may be deferred to a tool or via stepwise reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Chain-of-thought elicits intermediate steps that can improve reasoning and arithmetic chaining; tool-calling delegates computation to external precise executors (calculators, code), while the LLM manages high-level logic and tool API usage.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Prior work and evaluated baselines show that code interpreter/tool use yields high accuracy on numeric tasks (GPT‑4o + Code showed ~99.8% on subset), but at cost of generation length, runtime, and possible security concerns; CoT improves reasoning but does not guarantee exact numeric outputs for high-precision arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Code-interpreter pipelines can be slower and introduce security/execution risk; CoT alone often insufficient for high-digit precision arithmetic and may still produce arithmetic errors.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Prompting (CoT), training for tool calls (Toolformer/Calc‑X), and using code-execution environments.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Improves arithmetic accuracy when combined with tool execution; however OccamLLM demonstrates similar or better accuracy with fewer tokens and without running arbitrary code.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>GPT‑4o + Code: 99.8 ± 0.1% on a 1800-example subset for simple arithmetic; chain-of-thought improves reasoning benchmarks but does not reach exact arithmetic levels shown by OccamLLM on high-digit tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Higher latency, token-generation overhead, and security exposure for code tools; CoT may not fix numeric precision errors; tool-calling requires reliable tool-integration training or annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Tool-calling mirrors a human writing and executing code or using a calculator; OccamLLM internalizes that idea by connecting symbolic computation directly to LLM hidden representations without external code execution.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'OccamLLM: Fast and Exact Language Model Arithmetic in a Single Step', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>OccamNet: A Fast Neural Model for Symbolic Regression at Scale. <em>(Rating: 2)</em></li>
                <li>Calc-x and calcformers: Empowering arithmetical chain-of-thought through interaction with symbolic systems. <em>(Rating: 2)</em></li>
                <li>Toolformer: Language models can teach themselves to use tools. <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 1)</em></li>
                <li>GPT Can Solve Mathematical Problems Without a Calculator. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3015",
    "paper_id": "paper-270380045",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "OccamLLM",
            "name_full": "OccamLLM (OccamLlama when using Llama)",
            "brief_description": "A neurosymbolic framework that leaves the base LLM frozen and uses its per-token hidden states to initialize a symbolic model (OccamNet) which evaluates exact arithmetic in a single autoregressive step; a learned switch routes generation to either the LLM or OccamNet per token.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OccamLLM / OccamLlama (OccamLlama 8B and 70B)",
            "model_description": "System combining a frozen transformer LLM (Llama 3 Instruct, 8B or 70B) and a separately trained symbolic decoder that maps LLM hidden states to OccamNet softmax-layer weights; a learned binary switch (sigmoid MLP) routes outputs. LLM weights are not fine-tuned.",
            "arithmetic_task_type": "Single-operation arithmetic (+, −, ×, ÷, sqrt, power, log, exp, sin, cos) on large integers and floats; single-step and some multi-step arithmetic via a 2-layer OccamNet (up to 3 ops); arithmetic within word problems (AddSub, MultiArith, MultiArith-Float, GSM8K, MATH401, Single Eq, SVAMP).",
            "reported_mechanism": "Decoder maps LLM internal hidden-state representations to a probability distribution (weights) over symbolic functions in OccamNet; OccamNet samples (or the max-probability) DAG which implements a specific symbolic function and evaluates it on parsed numeric inputs; switch learns when to substitute OccamNet output for LLM token generation.",
            "evidence_for_mechanism": "Trained decoders (weight-decoder and switch) achieve perfect or near-perfect arithmetic on held-out synthetic data; non-textual training (decoder trained only on numeric expressions) still generalizes to word problems, indicating arithmetic representations are present in LLM hidden states; multilingual OOD tests show robustness; empirical benchmarks show OccamLlama 8B/70B obtain 100% accuracy on single-op arithmetic tests (9000 problems) and strong gains on math benchmarks compared to LLM baselines.",
            "evidence_against_mechanism": "Limitations documented: single-layer OccamNet cannot natively evaluate compound expressions (e.g., chained sums) generated by the LLM; switch can misroute when LLM outputs fractions/percentages or compound inline expressions not seen during switch training; some LLM generations append digits to OccamNet outputs requiring engineering workarounds.",
            "intervention_type": "Learned external symbolic module controlled by LLM hidden states (decoder-to-symbolic-model initialization) plus a learned switch; no fine-tuning of LLM; alternative interventions discussed (code interpreters, finetuning, chain-of-thought prompting) are compared.",
            "effect_of_intervention": "Enabled exact arithmetic evaluation in a single autoregressive step (no code execution), increased speed (single forward pass vs. multi-token code generation), improved security (no arbitrary code execution), and interpretability (explicit symbolic function chosen). Led to dramatic increases in arithmetic accuracy vs. unfused LLMs and outperformed or matched code-interpreter baselines with far fewer tokens.",
            "performance_metrics": "Single-op arithmetic: OccamLlama (8B & 70B) 100.0 ± 0.0% accuracy on {+,−,×,÷,sqrt,exp,log,sin,cos} over 9000 problems; GPT‑4o (no code) average ~39.5 ± 0.5% on same tasks; GPT‑4o + Code Interpreter (subset) 99.8 ± 0.1% (missed 3/1800). Multi-step (two-layer OccamNet): one-step 99.9 ± 0.17%, two-step 98.2 ± 0.45%, three-step 96.1 ± 0.64% (Table 3). Math benchmarks: OccamLlama 8B average ≈ 89.9 ± 1.29% across selected reasoning datasets; OccamLlama 70B average ≈ 94.6 ± 0.98% (per reported tables). OccamLlama uses OccamNet in a single forward pass; GPT‑4o+Code generated &gt;54 tokens on average for the same arithmetic prompts.",
            "notable_failure_modes": "Switch misfires on fraction/percentage formats and compound expressions (due to single-layer OccamNet limits and limited switch training distribution); LLM sometimes appends extra digits to OccamNet outputs; single-layer OccamNet cannot evaluate chained operations created inline by the LLM without additional decomposition or a deeper OccamNet; integration with advanced decoding (e.g., speculative decoding) not explored.",
            "comparison_to_humans_or_symbolic": "Direct comparison to exact symbolic computation: OccamLLM delegates numeric computation to a symbolic evaluator (OccamNet) that returns exact arithmetic, matching or exceeding calculator-like accuracy; authors contrast OccamLLM with code-based tool use (GPT‑4o + Code) and with finetuning approaches that risk catastrophic forgetting—OccamLLM attains calculator-level exactness while keeping LLM frozen.",
            "uuid": "e3015.0",
            "source_info": {
                "paper_title": "OccamLLM: Fast and Exact Language Model Arithmetic in a Single Step",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "OccamNet",
            "name_full": "OccamNet: a probabilistic symbolic-function architecture",
            "brief_description": "A symbolic/neurosymbolic architecture that parameterizes a probability distribution over compositions of primitive functions by using softmax layers whose sampled one-hot connections define DAGs (symbolic programs) which are evaluated on numeric inputs.",
            "citation_title": "OccamNet: A Fast Neural Model for Symbolic Regression at Scale.",
            "mention_or_use": "use",
            "model_name": "OccamNet (Complete OccamNet variant used)",
            "model_description": "A complete-occamnet variant: an l-layer MLP-like structure with softmax selection of inputs per activation node; primitives include arithmetic ops and transcendental functions; probability over functions defined by product of sampled connection probabilities; used here as a deterministic symbolic evaluator after decoder initialization.",
            "arithmetic_task_type": "Represents and evaluates symbolic single- and multi-operator arithmetic expressions (primitives: +, −, ×, ÷, sqrt, power, log, exp, sin, cos); can represent up to l compositions (complete OccamNet includes repeats and skip-connections to allow shallower compositions).",
            "reported_mechanism": "Parametrizes distributions over DAGs (symbolic expressions) by softmax weights per connection; sampling yields sparse DAGs that are executed as deterministic symbolic functions on numeric inputs; decoder learns to set OccamNet softmax weights from LLM hidden states to prefer the correct function.",
            "evidence_for_mechanism": "Used as the arithmetic engine for OccamLLM; sampling-based training and REINFORCE-style (rescaled) loss leads to OccamNet initializations that produce exact answers across synthetic arithmetic datasets; one can inspect weights to interpret selected functions (interpretable selection).",
            "evidence_against_mechanism": "Authors note OccamNet multi-representation ambiguity (same function can be represented by many DAGs) complicates cross-entropy; also single-layer networks cannot represent chained multiple-operator expressions. EQL alternative cited but argued less suitable.",
            "intervention_type": "Decoder trains to output OccamNet softmax-layer weights from LLM hidden states; training uses Monte-Carlo sampling of OccamNet and a REINFORCE/cross-entropy-style loss; OccamNet itself is not fine-tuned as a separate end-to-end gradient to LLM.",
            "effect_of_intervention": "When initialized by the decoder, OccamNet evaluates exact arithmetic functions that the LLM would otherwise approximate or mishandle, converting approximate LLM outputs into exact symbolic evaluations.",
            "performance_metrics": "As part of OccamLlama, OccamNet-based evaluations achieved 100% accuracy on single-op arithmetic tests and near-100% for up to three-operator expressions when using a two-layer OccamNet (one-step 99.9%, two-step 98.2%, three-step 96.1%).",
            "notable_failure_modes": "Expressive limitations when shallow (single layer): cannot evaluate expressions requiring more compositions than layer depth; multiple DAGs can represent same function making canonical-target cross-entropy formulation nontrivial; scaling sampling cost increases with larger OccamNets.",
            "comparison_to_humans_or_symbolic": "Serves as an explicit symbolic calculator within the LLM system; compared qualitatively to running externally generated code (tool use) but is internal and interpretable; provides deterministic arithmetic comparable to calculators/algorithms.",
            "uuid": "e3015.1",
            "source_info": {
                "paper_title": "OccamLLM: Fast and Exact Language Model Arithmetic in a Single Step",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LLM hidden-state decoder & switch",
            "name_full": "OccamLLM decoder (weight decoders) and switch decoder",
            "brief_description": "Per-token MLP-based decoders that map LLM hidden states (optionally weighted across layers) to (a) OccamNet softmax-layer weight initializations selecting symbolic functions and (b) a binary routing score that chooses whether to use OccamNet output or LLM token output.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OccamLLM decoders (weight-decoders and switch-decoder)",
            "model_description": "Decoder_i(h) = MLP_i(sum_j w_i,j h_j) + W*_i for each OccamNet softmax layer; switch is sigmoid(MLP_switch(sum_j w_switch,j h_j)). MLPs are small two-layer networks; decoders are trained while LLM remains frozen.",
            "arithmetic_task_type": "Controls what symbolic function OccamNet will evaluate for arithmetic tasks and decides when to substitute OccamNet outputs for token-generation during word problems and arithmetic queries.",
            "reported_mechanism": "Hidden states of the transformer encode arithmetic-relevant features; linear combinations across transformer layers followed by small MLPs can decode those features into parameters that bias OccamNet toward the intended symbolic function; binary switch decodes when the numeric answer token should come from OccamNet.",
            "evidence_for_mechanism": "Training the decoders on synthetic datasets yields high accuracy; non-textual (numbers-only) decoder training still generalizes to text-based word problems, indicating arithmetic structure is present in hidden states; ablation-like observations: the system functions with frozen LLM weights and with decoders trained separately.",
            "evidence_against_mechanism": "Switch can fail when confronted with text forms not seen in switch training (fractions, percentages, compound inline expressions), revealing limits in training coverage rather than a failure of representational decoding per se.",
            "intervention_type": "Trained decoders (supervised/rescaled REINFORCE for OccamNet decoder, binary cross-entropy for switch); no LLM fine-tuning.",
            "effect_of_intervention": "Enables the LLM to 'control' an exact symbolic evaluator using internal representations, converting approximate LLM numeric behavior into exact computation without altering LLM weights.",
            "performance_metrics": "Decoder trained on 80k examples (40k single queries + 40k concatenated queries) with sampled OccamNet draws (100 samples per token in 1-layer runs) achieves function-selection sufficient to yield OccamLlama 100% single-op accuracy; switch trained on 50k examples (80k for 70B variant) achieves routing adequate for high benchmark performance, though some datasets (GSM8K, Single Eq) show switch-related errors.",
            "notable_failure_modes": "Switch misroutes for out-of-distribution textual formats; decoder depends on parser that extracts numeric tokens from text (parser failures produce errors); sample inefficiency early in training due to sparse reward across large OccamNet function space.",
            "comparison_to_humans_or_symbolic": "This intervention treats the LLM as providing latent intent/representation and uses a symbolic evaluator to execute exact arithmetic—conceptually similar to a human using internal working memory to signal a calculator which operation to perform.",
            "uuid": "e3015.2",
            "source_info": {
                "paper_title": "OccamLLM: Fast and Exact Language Model Arithmetic in a Single Step",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Llama 3 (base LLM)",
            "name_full": "Llama 3 Instruct (8B and 70B)",
            "brief_description": "The base frozen transformer language models used by OccamLLM experiments; provide hidden states and natural-language reasoning while arithmetic is delegated to OccamNet.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama 3 Instruct (8B and 70B)",
            "model_description": "Transformer-based instruction-tuned LLMs (Llama 3 Instruct) used as the frozen language model backbone; hidden size reported (4096 for 8B, 8192 for 70B) and layerwise hidden states are aggregated by decoders.",
            "arithmetic_task_type": "Left to perform reasoning, prompt-driven decomposition, and sometimes arithmetic when OccamNet is not invoked (word-problem reasoning, selection of numeric tokens).",
            "reported_mechanism": "LLM produces hidden states that (1) encode numeric and operation context which decoders can map to symbolic functions and (2) in many contexts can themselves output approximate arithmetic results or structured steps but with limited exactness on large numeric tasks.",
            "evidence_for_mechanism": "OccamLLM experiments demonstrate that Llama 3 hidden states are sufficient for the decoder to select correct OccamNet functions even when the decoder was trained on numbers-only examples; baseline Llama 3 performance is far worse on high-precision arithmetic tasks (e.g., near 0% on 7-digit multiplication/division).",
            "evidence_against_mechanism": "Llama 3 alone fails on many arithmetic tasks (e.g., low or zero accuracy on large-digit multiplication/division) indicating its internal autoregressive outputs are not reliably exact for arithmetic without a symbolic module.",
            "intervention_type": "Kept frozen; used as a provider of hidden states for the decoder and as the reasoning component in OccamLLM. Authors also experimented with prompting (system prompts like 'Solve step by step.') when evaluating reasoning datasets.",
            "effect_of_intervention": "Using Llama 3 as a frozen backbone with OccamNet yields large arithmetic accuracy improvements without finetuning the LLM; shows smaller models can match or exceed larger proprietary models in arithmetic when paired with a symbolic evaluator.",
            "performance_metrics": "Llama 3 8B baseline: poor arithmetic on 7-digit tasks (e.g., below ~50% on 7-digit addition; 0% on 7-digit multiplication in Table 2); when used as OccamLlama backbone performance is inherited by the hybrid system (OccamLlama 8B and 70B metrics above).",
            "notable_failure_modes": "LLM outputs approximate or improperly formatted numeric answers (fractions, percentages, chained expressions) that require the switch to suppress OccamNet usage; LLM tendency to append digits to OccamNet outputs required heuristics.",
            "comparison_to_humans_or_symbolic": "As the 'reasoner' in the hybrid system, Llama 3 handles language and decomposition tasks akin to human reasoning, while OccamNet performs exact calculation like a calculator.",
            "uuid": "e3015.3",
            "source_info": {
                "paper_title": "OccamLLM: Fast and Exact Language Model Arithmetic in a Single Step",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT‑4o (baseline)",
            "name_full": "GPT‑4o (2024-05-13)",
            "brief_description": "A state-of-the-art proprietary LLM baseline evaluated in the paper for arithmetic capability both with and without a code interpreter tool; used for performance comparison.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT‑4o (and GPT‑4o + Code Interpreter)",
            "model_description": "OpenAI's GPT‑4-family model (GPT‑4o) evaluated in standard chat configuration and with a code-interpreter tool (external code execution). Used as comparative baselines rather than modified within the paper's method.",
            "arithmetic_task_type": "Same set of arithmetic tasks and math benchmarks: 7-digit integer arithmetic, transcendental functions, and word-problem datasets.",
            "reported_mechanism": "When using code-interpreter, GPT‑4o outputs program code that, when executed by the tool, computes exact arithmetic externally; without tool use, GPT‑4o relies on internal autoregressive patterns and learned heuristics to produce numeric answers (often approximate).",
            "evidence_for_mechanism": "Empirically, GPT‑4o without tool use misses many arithmetic cases (e.g., ~39.5% average on table of arithmetic tasks), while GPT‑4o + Code achieves near-perfect accuracy on the tested subset (99.8% on ~1800 examples) at the cost of many generated tokens and external code execution.",
            "evidence_against_mechanism": "Even with a code interpreter, GPT‑4o+Code did not always improve math-benchmark performance relative to GPT‑4o on some datasets (authors report GPT‑4o+Code sometimes underperforms GPT‑4o across math benchmarks), suggesting current code-calling practices are not a panacea for math reasoning beyond small tasks.",
            "intervention_type": "Tool use (external code execution via code-interpreter); also chain-of-thought prompting and system prompts used in evaluation.",
            "effect_of_intervention": "Code interpreter yields near-exact arithmetic but with high token-generation overhead and potential security concerns due to arbitrary code execution; OccamLLM achieves similar or better accuracy with fewer tokens and without executing generated code.",
            "performance_metrics": "Arithmetic table: GPT‑4o (no code) average ~39.5 ± 0.5% on 7-digit tasks; GPT‑4o + Code Interpreter (subset tested) 99.8 ± 0.1% accuracy (missed 3/1800 examples). On broader reasoning benchmarks GPT‑4o and GPT‑4o+Code show mixed results — OccamLlama 70B outperforms GPT‑4o on average across the selected mathematical problem solving benchmarks.",
            "notable_failure_modes": "Without external code, internal autoregressive outputs are error-prone for high-precision arithmetic (wrong digit positions, rounding issues); code-based method is slower (many tokens) and raises security concerns about executing LLM-generated code.",
            "comparison_to_humans_or_symbolic": "GPT‑4o + Code effectively uses symbolic/programmatic execution (like a human writing code and running it) to obtain exact answers; OccamLLM provides an internal symbolic mechanism achieving similar exactness but integrated and interpretable.",
            "uuid": "e3015.4",
            "source_info": {
                "paper_title": "OccamLLM: Fast and Exact Language Model Arithmetic in a Single Step",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Finetuning / PEFT (prior approaches)",
            "name_full": "Fine-tuning and Parameter-Efficient Fine-Tuning approaches for arithmetic",
            "brief_description": "Prior interventions that improve LLM arithmetic by continuing training on arithmetic datasets (full fine-tuning or PEFT), which can produce high arithmetic accuracy but risk catastrophic forgetting of original LLM capabilities.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Finetuning / PEFT approaches (general)",
            "model_description": "Approaches that update LLM weights (full or adapter/LoRA-style PEFT) on arithmetic-specific datasets to improve numeric computation performance within the model's weights.",
            "arithmetic_task_type": "Arithmetic datasets (add, subtract, multiply, divide, powers) and math reasoning benchmarks.",
            "reported_mechanism": "Directly modifies model weights so internal computation (transformer activations and logits) encode arithmetic algorithms or memorized mappings, enabling the model to output accurate numeric results without external tools.",
            "evidence_for_mechanism": "Prior work cited reports near-perfect arithmetic after specialized training on synthetic arithmetic datasets; PEFT can partially mitigate catastrophic forgetting but risk remains.",
            "evidence_against_mechanism": "Authors highlight catastrophic forgetting risks (loss of prior linguistic capabilities) when finetuning on narrow datasets; also limited generality when trained only on arithmetic formats (lack of robustness on word problems).",
            "intervention_type": "Weight updates via full finetuning or PEFT on arithmetic datasets; contrasted with OccamLLM's frozen-LLM approach.",
            "effect_of_intervention": "Can produce near-perfect results on in-distribution arithmetic datasets but may degrade broader language performance and generalization.",
            "performance_metrics": "Prior literature reports near-perfect accuracy on arithmetic when models are trained specifically for those tasks (cited qualitatively in related work), but concrete numbers vary by study/dataset.",
            "notable_failure_modes": "Catastrophic forgetting, limited generalization outside arithmetic-format training data, and need for large annotated datasets or PEFT tuning strategies.",
            "comparison_to_humans_or_symbolic": "Finetuned models close the gap to symbolic calculators on trained distributions but do so by embedding computation into weights rather than delegating to an explicit symbolic evaluator.",
            "uuid": "e3015.5",
            "source_info": {
                "paper_title": "OccamLLM: Fast and Exact Language Model Arithmetic in a Single Step",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Prompting / Chain-of-Thought / Tool-calling (mentioned baselines)",
            "name_full": "Prompt engineering (chain-of-thought), tool-calling (Toolformer/Calc‑X), and code interpreters",
            "brief_description": "Described classes of interventions for improving arithmetic in LLMs: chain-of-thought prompting to elicit intermediate reasoning, and training models to call external calculators or code interpreters (Toolformer, Calc‑X) to compute exact numeric answers.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Prompting (CoT) and Tool Use frameworks (Toolformer, Calc‑X, code interpreter workflows)",
            "model_description": "CoT: prompting that encourages step-by-step reasoning; Tool-calling: training or prompting to insert API/tool calls (external calculator or code execution) that compute numbers exactly; Toolformer/Calc‑X are frameworks to teach LMs to use tools.",
            "arithmetic_task_type": "Arithmetic and multi-step math word problems where exact numeric computation may be deferred to a tool or via stepwise reasoning.",
            "reported_mechanism": "Chain-of-thought elicits intermediate steps that can improve reasoning and arithmetic chaining; tool-calling delegates computation to external precise executors (calculators, code), while the LLM manages high-level logic and tool API usage.",
            "evidence_for_mechanism": "Prior work and evaluated baselines show that code interpreter/tool use yields high accuracy on numeric tasks (GPT‑4o + Code showed ~99.8% on subset), but at cost of generation length, runtime, and possible security concerns; CoT improves reasoning but does not guarantee exact numeric outputs for high-precision arithmetic.",
            "evidence_against_mechanism": "Code-interpreter pipelines can be slower and introduce security/execution risk; CoT alone often insufficient for high-digit precision arithmetic and may still produce arithmetic errors.",
            "intervention_type": "Prompting (CoT), training for tool calls (Toolformer/Calc‑X), and using code-execution environments.",
            "effect_of_intervention": "Improves arithmetic accuracy when combined with tool execution; however OccamLLM demonstrates similar or better accuracy with fewer tokens and without running arbitrary code.",
            "performance_metrics": "GPT‑4o + Code: 99.8 ± 0.1% on a 1800-example subset for simple arithmetic; chain-of-thought improves reasoning benchmarks but does not reach exact arithmetic levels shown by OccamLLM on high-digit tasks.",
            "notable_failure_modes": "Higher latency, token-generation overhead, and security exposure for code tools; CoT may not fix numeric precision errors; tool-calling requires reliable tool-integration training or annotation.",
            "comparison_to_humans_or_symbolic": "Tool-calling mirrors a human writing and executing code or using a calculator; OccamLLM internalizes that idea by connecting symbolic computation directly to LLM hidden representations without external code execution.",
            "uuid": "e3015.6",
            "source_info": {
                "paper_title": "OccamLLM: Fast and Exact Language Model Arithmetic in a Single Step",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "OccamNet: A Fast Neural Model for Symbolic Regression at Scale.",
            "rating": 2,
            "sanitized_title": "occamnet_a_fast_neural_model_for_symbolic_regression_at_scale"
        },
        {
            "paper_title": "Calc-x and calcformers: Empowering arithmetical chain-of-thought through interaction with symbolic systems.",
            "rating": 2,
            "sanitized_title": "calcx_and_calcformers_empowering_arithmetical_chainofthought_through_interaction_with_symbolic_systems"
        },
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools.",
            "rating": 2,
            "sanitized_title": "toolformer_language_models_can_teach_themselves_to_use_tools"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "GPT Can Solve Mathematical Problems Without a Calculator.",
            "rating": 1,
            "sanitized_title": "gpt_can_solve_mathematical_problems_without_a_calculator"
        }
    ],
    "cost": 0.02063025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>OccamLLM: Fast and Exact Language Model Arithmetic in a Single Step
3 Sep 2024</p>
<p>Owen Dugan odugan@mit.edu 
Donato M Jiménez-Benetó donatojb@mit.edu 
Charlotte Loh cloh@mit.edu 
Zhuo Chen chenzhuo@mit.edu 
Rumen Dangovski rumenrd@mit.edu 
Marin Soljačić soljacic@mit.edu </p>
<p>Department of Physics
Massachusetts Institute of Technology Cambridge
MA</p>
<p>Department of Physics
Massachusetts Institute of Technology Cambridge
MA</p>
<p>Department of EECS
Massachusetts Institute of Technology Cambridge
MA</p>
<p>Department of Physics
Massachusetts Institute of Technology Cambridge
MA</p>
<p>Department of EECS
Massachusetts Institute of Technology Cambridge
MA</p>
<p>Department of Physics
Massachusetts Institute of Technology Cambridge
MA</p>
<p>OccamLLM: Fast and Exact Language Model Arithmetic in a Single Step
3 Sep 2024229501F131FEE1822532973E06DAF108arXiv:2406.06576v4[cs.CL]
Despite significant advancements in text generation and reasoning, Large Language Models (LLMs) still face challenges in accurately performing complex arithmetic operations.Language model systems often enable LLMs to generate code for arithmetic operations to achieve accurate calculations.However, this approach compromises speed and security, and fine-tuning risks the language model losing prior capabilities.We propose a framework that enables exact arithmetic in a single autoregressive step, providing faster, more secure, and more interpretable LLM systems with arithmetic capabilities.We use the hidden states of a LLM to control a symbolic architecture that performs arithmetic.Our implementation using Llama 3 with OccamNet as a symbolic model (OccamLlama) achieves 100% accuracy on single arithmetic operations (+, −, ×, ÷, sin , cos , log , exp , √ ), outperforming GPT 4o with and without a code interpreter.Furthermore, OccamLlama outperforms GPT 4o with and without a code interpreter on average across a range of mathematical problem solving benchmarks, demonstrating that OccamLLMs can excel in arithmetic tasks, even surpassing much larger models.We will make our code public shortly.</p>
<p>Introduction</p>
<p>Since the release of GPT 3, Large Language Models (LLMs) have dramatically improved in their text generation and reasoning capabilities.This has enabled success in downstream applications including machine translation [1,2], sentiment analysis [3,4,5], and interactive dialogue generation Table 1: OccamLLM is the only approach to improving the arithmetic capabilities of a pretrained LLM which 1) enables single-pass arithmetic, 2) does not risk catastrophic forgetting from finetuning, 3) does not require arbitrary code execution, and 4) provides an interpretable process.</p>
<p>No Catastrophic</p>
<p>No Arbitrary Single Pass Forgetting Code Execution Interpretable Fine Tuning
✓ ✗ ✓ ✗ Tool Use ✗ ✗ ✗ ✓ OccamLLM ✓ ✓ ✓ ✓
[6], with language models even surpassing human experts on some academic benchmarks that require reading comprehension, reasoning and coding [7].However even industry-leading LLMs such as GPT 4 cannot reach 100% accuracy on simple arithmetic [8], limiting their ability to perform basic mathematical tasks.This hinders potential applications of LLMs ranging from chat-bot physics tutors to LLM-powered automated research that could accelerate scientific discovery and technological innovation.The poor arithmetic performance of LLMs is particularly acute for small LLM agents, limiting their usage in smartphone or in multi-agent applications.</p>
<p>To enable accurate calculations, language model systems often resort to running code written by a LLM.However, this comes at the cost of speed; the model must perform multiple autoregressive steps to generate code that performs the appropriate arithmetic operations.This increased decoding time may negatively impact applications such as multi-agent workflows [9,10] where speed is essential.At the same time, code-based LLM arithmetic mechanisms may increase system vulnerability by providing a mechanism for arbitrary LLM-generated code execution.</p>
<p>We propose an alternative, a framework which enables exact and interpretable LLM arithmetic in a single autoregressive step, providing faster and more secure arithmetic capabilities in LLM systems.</p>
<p>Our framework uses the hidden states of a LLM to control a symbolic architecture that performs arithmetic.Although our method can in principle work with any symbolic architecture, in this paper we use an interpretable neurosymbolic architecture known as OccamNet [11,12] because of its interpretability and scalability.Therefore, we term our method OccamLLM, or OccamLlama when using a Llama model as the LLM.</p>
<p>Our core contributions are as follows:</p>
<ol>
<li>We develop a framework for exact and interpretable LLM arithmetic in a single autoregressive step without catastrophic forgetting [13] or vulnerability from code generation.We explore how to train OccamLlama, including data generation, decoder architecture, and loss function.2. We benchmark OccamLlama on arithmetic tasks, demonstrating that OccamLlama achieves 100% accuracy on arbitrary single arithmetic operations (+, −, ×, ÷, sin , cos , log , exp , √ ), more than double the accuracy of GPT 4o.OccamLlama performs slightly better than GPT 4o with Code Interpreter while answering in on average more than 50x fewer generation tokens.3. We benchmark on mathematical problem solving tasks, showing that OccamLlama can sustain long generations.OccamLlama outperforms both GPT 4o and GPT 4o with code interpreter on average across the benchmarks we tested.</li>
</ol>
<p>Related Work</p>
<p>Arithmetic Performance in LLMs.Prior research has trained models on synthetic data, finding that such models can achieve near-perfect accuracy on addition [14,15], subtraction [15], multiplication [14,15], division [15], and raising to powers [15].These prior models have been tested only on arithmetic datasets, so their generality has not been assessed.Other work focuses on finetuning LLMs which are already trained on large amounts of general-purpose data on math datasets.Both full-parameter [16,17] and parameter-efficient (PEFT) [18] finetuning strategies have been applied.However, finetuning on a single dataset carries the risk of catastrophic forgetting of an LLM's previously acquired linguistic skills [19].While PEFT techniques have been shown to partially mitigate this effect, this area is still one of active research [20,21].</p>
<p>Figure 1: The OccamLLM system.For each autoregressive step, the language model hidden states for that token are fed into a decoder block which assigns weights to OccamNet.The system feeds the most recent numbers from the text into OccamNet, which then evaluates the sparse function specified by its weights.The decoder then determines whether to use the LLM output or the OccamNet output.</p>
<p>LLMs with Tool Use.Another thrust of prior research has focused on LLM tool use, which we believe is most directly related to our methods.Calc-X [22] introduces a technique to offload arithmetic computations to an external tool like a calculator.The authors curated a large dataset of arithmetic problems and trained a language model that learns to interact with a calculator through the use of tags to signify the calling of the external tool.Several other works [23,24,25] follow a similar idea, using crowd workers to annotate tool calls and using this data to train language models to interact with external tools such as a web searching tool, a calculator, or a translation system.These approaches can be prohibitively expensive in annotation costs; Toolformer [26] overcomes this cost by using in-context learning and a language model to generate datasets containing the necessary 'API' tool calls via a self-supervised loss.Further, the above methods all require finetuning of the LLM, placing the LLM at risk of losing generality and its original language modelling abilities through catastrophic forgetting.In contrast, our approach does not involve training the language model.Our 'external tool' is a symbolic model which can be trained to correctly use the hidden states of the language model to perform the required arithmetic computations.The language model is kept frozen throughout this process.Unlike other tool-calling approaches, where the cost of data annotation to train for tool-calling interaction can be prohibitively expensive, in our method, each task only requires manually annotating tens of prompts, a high annotation efficiency.Other prior methods leverage prompt engineering to improve arithmetic performance of LLMs; this is done either through chain-of-thought [27], or to encourage LLMs to use a code interpreter [28,29,30].Contrary to these methods, our approach does not use code kernels; this provides several advantages: 1) it enables tool use without expending compute on autoregressive steps for token generation, and 2) it avoids running potentially incorrect or malicious code generated by language models.</p>
<p>3 Methods</p>
<p>OccamLLM: Combining a Language Model with a Symbolic Model</p>
<p>In short, the OccamLLM system combines a language model with a symbolic model, namely OccamNet, that can perform arithmetic operations like addition and subtraction.For each token, the corresponding internal hidden states of the language model are fed into a decoder module which initializes the symbolic model such that it executes the operation required by the task described in the input text.A string parser feeds the necessary numbers from the text into OccamNet, which evaluates the desired expression.Finally, a decoder determines whether to use the language model output or the OccamNet output for generating the next token.Modified from [11].</p>
<p>In the example shown in Figure 1, a decoder determines how to initialize OccamNet from the language model hidden states, choosing to have OccamNet perform addition.The text parser then feeds the numbers 6 and 7 into OccamNet, which adds the numbers, returning 13.Finally, a decoder decides to use the OccamNet output instead of the language model output, so the system outputs 13.The new sentence, including the 13, is tokenized and fed back to the LLM to continue autoregressive generation.The language model might later generate "Since she ate two apples, she now has," at which point the switch will again trigger OccamNet, this time implementing 13 − 2 and returning 11.</p>
<p>In the subsections below, we describe the OccamLLM system which from our experiments we find to be most performant, even oupterforming GPT 4o in several benchmarks.For an analysis of alternate architectures and losses, see Appendix D.</p>
<p>OccamNet</p>
<p>OccamNet is a symbolic architecture that provides an interpretable way of parametrizing probability distributions over a space of functions [11].We leave a more thorough explanation of OccamNet to [11] and Appendix E, describing only the relevant components here.</p>
<p>An l-layer OccamNet with primitives P and n inputs is an architecture that defines a probability distribution over the space of functions representable as compositions of the primitives in P up to depth l.For example, a two-layer OccamNet with primitives P = {sin, cos} and one input represents a probability distribution over the set F = {x, sin(x), cos(x), sin(sin(x)), sin(cos(x)), cos(sin(x)), sin(sin(x))}.</p>
<p>OccamNet has the structure of an n-input, l-internal-activation-layer multilayer perceptron with the biases removed and the activations in each layer replaced by the primitives P, as shown in Figure 2a.Activation functions may have multiple inputs.We rename the linear layers softmax layers, denote the weights of the ith softmax layer as W (i) , and denote the combined weights of OccamNet as W.</p>
<p>We define the probability distribution which OccamNet parametrizes by specifying how to sample from it.For each softmax layer output node (shown in red in Figure 2), we select a single connection to that node from a softmax layer input node by sampling from the distribution given by the softmax of the weights of the connections to the different inputs.This process produces a directed acyclic graph (DAG) defining a computational path through the OccamNet activations, such as the one shown in Figure 2b.In this way, each DAG represents a function on the inputs of OccamNet.</p>
<p>To ensure that OccamNet can represent all possible compositions of functions in P up to depth l, we include the following modifications to the OccamNet architecture: 1) for each softmax layer, we concatenate its inputs with the previous softmax layer's inputs to enable the representation of functions with fewer than l compositions, and 2) we repeat primitives in the ith activation layer A l−i times, where A is the maximum number of inputs of any of the primitives, to ensure that a sufficient number of each primitive is available at each layer.We refer to this modified architecture as complete OccamNet as it can represent the complete set of desired functions.The resulting architecture is shown in Figure 7 in the appendix.</p>
<p>In principle, OccamLLM can work with any symbolic model, i.e., any model that can parameterize a set of symbolic functions or a distribution over such functions.We choose OccamNet as opposed to, for example, a transformer [31] or recurrent neural network [32], for two reasons: 1) OccamNet is interpretable, which we hypothesize makes controlling OccamNet an easier task for a decoder to learn, and 2) OccamNet is parallelizable over multiple samples, allowing for scalable training.</p>
<p>OccamLLM Decoder</p>
<p>The OccamLLM decoder takes the hidden states of a language model and outputs an initialization for OccamNet.This gives the LLM control over which function to apply on the inputs.The decoder acts on each input token separately, producing a different OccamNet initialization for each.Therefore, the arithmetic operations predicted may change along an input sequence, allowing OccamNet's use for different computations in a single multi-token generation.This is crucial in multi-step reasoning scenarios where OccamNet is employed several times for different purposes.</p>
<p>Many decoder architectures are possible.We choose to parameterize the weights of each softmax layer of OccamNet independently, as (W (1) , . . ., W (l) ) = (Decoder 1 (h), . . ., Decoder l (h)), where h are the hidden states of the language model.We choose
Decoder i (h) = MLP i   L j=1 w i,j h j   + W * (i)(1)
where h j are the hidden states of the jth layer of the language model, w i,j are trainable weights, MLP i are two-layer multilayer perceptrons (MLPs), and W * (i) are untrained weights which initialize all functions to have approximately equal probabilities according to the initialization scheme described in [11] and explained in Appendix E.4.</p>
<p>OccamLLM Switch</p>
<p>We similarly train a decoder for a switch that, for each input token, is fed the hidden states of the language model and selects whether to use the output of OccamNet or the output of the language model.The decoder outputs a single number from 0 to 1, where all numbers less than or equal to 0.5 correspond to using the output of the language model and all numbers greater than 0.5 correspond to using the output of OccamNet.We choose the following architecture for the switch decoder:
Decoder switch (h) = sigmoid   MLP switch   L j=1 w switch,j h j     .(2)</p>
<p>Data Generation</p>
<p>We create synthetic datasets to train the OccamLLM decoders, which contain instruction prompts for diverse arithmetic tasks.To generate datasets of arbitrary size, we create prompts with placeholders for numbers.Each prompt includes a question with number placeholders, the sampling value range for each number, and a function that computes the answer to the query given the sampled input numbers.The prompts fall into two main categories: purely arithmetic tasks and reasoning problems.</p>
<p>Purely arithmetic prompts are formed by expressions including only symbols, without any natural language added, such as "3 + 85 =."We create prompts using the following operations:
+(•, •), −(•, •), ×(•, •), ÷(•, •), sqrt(•), power(•, •), log e (•)
, exp(•), sin(•), and cos(•).</p>
<p>We also include word problems that require one or two reasoning steps.We generated 150 single step word problems and 40 multi-step reasoning problems which we modified from examples in the MultiArith training dataset [33].</p>
<p>OccamNet Decoder Training Data</p>
<p>For training the decoder that controls the weights of OccamNet, we created two types of examples, single queries and concatenated queries.For single queries, we select a single prompt from the problems generated as discussed in Section 3.2.We use the Llama 3 Instruct chat template and fill in the query as the user input and the result as the assistant response, prepending "Answer = " to the later in randomly selected samples (see Appendix A.1.1 for further details).For the concatenated queries of examples, we select a random number of prompts and concatenate the query-response pairs without using the Llama 3 Instruct chat template.The OccamNet decoder is trained to predict only the results of the last query in the sequence.This strategy helps OccamLLM to learn which operation to perform without becoming confused by earlier text, which is useful for continuous generation.To create the training dataset, each example is sampled by first randomly selecting whether to create a single or concatenated query, then randomly selecting the type(s) of prompt(s) used, and finally randomly sampling the input values from the range corresponding to each selected prompt.</p>
<p>OccamLLM Switch Training Data</p>
<p>To train the switch, we generate examples of possible LLM outputs for given input expressions and label the outputs with sequences of 0s or 1s corresponding to whether the language model or the OccamNet output should be used for the next token.Some examples correspond to the prompts described in Section 3.2.For such examples, the LLM output is set to "The answer is" or "Answer = " and the label sequence is all 0s with a 1 at the last token to indicate the system should use OccamNet only to compute the answer.We also manually created and labeled several other examples for diverse scenarios to explicitly teach the system in which cases it should or should not use OccamNet (see Appendix A.1.2 for further details).</p>
<p>To create the training dataset, we concatenate a random number of the above user input -assistant output pairs in a conversational fashion, using the Llama 3 Instruct chat template.</p>
<p>OccamLLM Training</p>
<p>We train the OccamLLM decoder and the switch separately, as they do not share weights.In all cases, the weights of the LLM are kept frozen.In the first step, we train the system to predict the answer to examples generated by the method explained in Section 3.2.1.The OccamNet decoder processes the hidden states corresponding to the last token of the response and sets the weights of OccamNet such that the correct arithmetic expression is sampled.In this step, we use a rescaled REINFORCE [34] loss, which can also be interpreted as a Monte-Carlo estimate of the cross-entropy loss (see Appendix D.2):
L(x, y; W ) = − f ∼p W R(f (x), y) log p W [f ] f ∼p W R(f (x), y) ,(3)
where
p W [f ] ≡ ON(f ; Decoder W (h(x)))
is the probability distribution represented by the decoderinitialized OccamNet.</p>
<p>Minimizing this loss steers the decoder towards assigning higher probabilities to the functions that maximize the reward R(f (x), y), which measures the similarity between the correct answer y and the prediction of OccamNet f (x).We find setting R(f (x), y) = 1 if f (x) = y, and 0 otherwise, most effective.We discuss the OccamNet loss in more detail in Appendix D.</p>
<p>The second step involves training the decoder to route the outputs to OccamNet when needed.We train the switch decoder alone, freezing the weights of the OccamNet decoder of the previous step and minimizing the binary cross-entropy loss between the switch output and the desired output for each token.The OccamLLM switch decoder learns when to route the output to OccamNet in diverse contexts.</p>
<p>Experiments</p>
<p>For all OccamLLM results, we use Llama 3 8B Instruct and Llama 3 70B Instruct [35] as the underlying language models.As such, we call our models OccamLlama 8B and OccamLlama 70B, respectively.We use a 1 layer Complete OccamNet with primitives
P = {+(•, •), −(•, •), ×(•, •), ÷(•, •), sqrt(•), power(•, •), log e (•), exp(•), sin(•), cos(•)}.
This single layer OccamNet can be invoked by the LLM several times during generation to perform complex arithmetic operations accurately.To use the trained OccamLlama for inference, we sample the highest probability function from OccamNet as described in Appendix E.3.</p>
<p>We benchmark our methods against unmodified Llama 2 7B Chat (Llama 2 7B) [36], unmodified Llama 3 8B Instruct (Llama 3 8B) [35], gpt-3.5-turbo-0125(GPT 3.5 Turbo), gpt-4o-2024-05-13 (GPT 4o) [37], and gpt-4o-2024-05-13 with Code Interpreter (GPT 4o + Code).To reduce costs, for GPT 4o with Code Interpreter, we test a random subset of 200 datapoints for each dataset.To determine if a model output is correct, we parse all numbers in the model output and if one of them "matches" the correct answer, we determine that the result is correct.We mark each correct result as 100% accuracy and each incorrect result as 0% accuracy.For each model on each dataset, we report the mean accuracy and the standard error of the mean.To determine if a number matches the result, we first determine how many places after the decimal d the number should be accurate to.If the number is an integer, we set d to 2. Otherwise, we set d to the number of places after the decimal in the model output, clipped between 2 and 5. Finally we state that a number "matches" the result if the number and the result differ by less than 10 −d .We present further experiment details, including additional experiments, hyperparameters, and prompts in Appendix A.</p>
<p>Simple Arithmetic Problems</p>
<p>To evaluate OccamLlama and the baselines on purely arithmetic expressions, we create several synthetic datasets.For each of the operations in {+, −, ×, ÷}, the inputs are random 7-digit positive or negative integers.For √ , the inputs are random 7-digit positive integers.For the logarithms, the examples are log-uniformly sampled in the interval (10 −10 , 10 10 ); for the exponentials, they are uniformly sampled in the interval (−10, 10), and for sines and cosines they are uniformly sampled in the interval (−2π, 2π).</p>
<p>The results of these evaluations are shown in Table 2.More detailed results, including relative error and results for 3-and 5-digit arithmetic, are shown in Appendix A.5.</p>
<p>Both OccamLlama 8B and 70B have 100.0 ± 0.0% accuracy on all tasks, missing 0 out of 9000 problems.On the other hand, we tested GPT 4o with Code Interpreter on fewer problems to save cost, and it missed 3 out of the 1800 problems it faced, achieving an accuracy of 99.8 ± 0.1%.</p>
<p>Furthermore, GPT 4o with Code Interpreter generates on average more than 54 tokens to answer these problems, whereas our model uses OccamNet on the first forward pass.This means that, barring advanced decoding techniques such as speculative decoding [38], GPT 4o would need to be more than 50x faster than OccamLlama per forward pass to be comparable in answer generation speed on these tasks.</p>
<p>Table 2 demonstrates that arithmetic with LLMs is still challenging; state-of-the-art proprietary language models like GPT 4o achieve less than 40% accuracy on 7-digit division and fail to perform any 7-digit multiplications correctly.Open source LLMs fall farther behind, with Llama 3 8B achieving below 50% on relatively simple tasks such as 7-digit addition.</p>
<p>Mathematical Problem Solving</p>
<p>To test the performance of OccamLlama on more general mathematical problem solving tasks, we evaluate our method and baselines on the following six benchmarks: AddSub [39], GSM8K [40], MultiArith [33], MATH401 [8], Single Eq [41], and SVAMP [42].All but MATH401 are word problems requiring longer generation and a mix of reasoning and arithmetic capabilities.MATH401 also includes multistep arithmetic problems which require more than one call to OccamLlama.We selected these datasets (including the MultiArith Float dataset described below) before testing any methods on them to ensure unbiased selection of benchmarks.
A d d S u b G S M 8 K M u l t i A r i t h M u l t i A r i t h F l o a t M A T H 4 0 1 S i n g l e E q S V A M P A v e
Because many of the arithmetic operations required in these datasets are relatively simple, we also create MultiArith Float, a modification of MultiArith in which we select problems which are arithmetically more challenging, while requiring similar levels of reasoning.To this end, we select prompts having input numbers that can be replaced with floats.For instance, 3.5 feet or $39.95 are reasonable but 3.5 people is not.Furthermore, we sample input values from ranges larger than those appearing in the MultiArith dataset, in cases where it is reasonable.Float operations and larger additions and multiplications are more difficult for the baseline LLMs but do not make a difference for OccamLLM, so this dataset is particularly useful to show the advantages of the system we propose.Figure 3 shows the results of these evaluations.More detailed results are shown in Appendix A.5.</p>
<p>OccamLlama 70B outperforms both GPT 4o and GPT 4o + Code on average across the benchmarks, demonstrating OccamLlama's strong mathematical problem solving capability.We also note that GPT 4o + Code does not outperform GPT 4o on average, suggesting that existing implementations of LLMs with code generation may not help with mathematical problem solving.</p>
<p>We now consider the performance of OccamLlama 8B, the smaller OccamLlama model.On Mul-tiArith Float and MATH401, two datasets requiring challenging arithmetic, OccamLlama 8B outperforms not only Llama 3 8B but also GPT 4o and GPT 4o + Code.At the same time, most other datasets in this benchmark do not involve challenging arithmetic, meaning that Llama 3 8B is well suited to solve these tasks without assistance; most of the difficulty of these tasks lies in the reasoning rather than in the arithmetic computations.This is further supported by the fact that GPT 4o with Code Interpreter never substantially outperforms and sometimes underperforms GPT 4o on these tasks.As such, it is remarkable that OccamLlama 8B can achieve comparable accuracy to Llama 3 8B even when it is trained on very different data and evaluated on tasks without challenging arithmetic.</p>
<p>The only datasets for which OccamLlama 8B performs noticeably worse than Llama 3 8B are GSM8K and Single Eq, but we believe this results from an imperfect OccamLlama switch, likely stemming from text which is outside of the switch training distribution (see Section 4.3).Fortunately, in Appendix C, we find that the OccamNet decoder is quite robust to out of distribution data and that both the OccamNet and switch decoders generalize well to unseen languages.This suggests that, with relatively little data, it should be possible to teach the switch to handle these unseen cases, something we leave for future work.</p>
<p>In Figure 4, we show example generations from OccamLlama 8B for both arithmetic and reasoning tasks.These generations demonstrate how the OccamLlama switch learns to balance OccamNet  outputs with LLM outputs, effectively distributing the work between a reasoner (Llama) and a calculator (OccamNet).Because the language model is unaware of the OccamLlama system, its generations behave as if it possesses an interior calculator even though it is actually using a tool.In this way, we combine the benefits of a language model finetuned on arithmetic with the benefits of a language model finetuned to use code for arithmetic, all without any finetuning.</p>
<p>Limitations</p>
<p>In our experiments, we use a single-layer OccamNet as the symbolic network, enabling evaluation of single-operation arithmetic problems.This sometimes poses a challenge on reasoning problems when the base language model generates compound expressions requiring more than one operation to evaluate, such as 3 + 5 + 7 =.A single-layer OccamNet cannot evaluate these expressions.We attempted to overcome this by prompting Llama to break down compound expressions into multiple steps, but we find it difficult to coerce Llama to follow these instructions.Another challenge is that Llama often generates expressions in fractions or percentages, which also constitute compound expressions that are not properly handled by the OccamLLM system.Fortunately, we observed that these compound expressions were typically simple enough for the LLM to evaluate without OccamNet.Therefore, in our experiments, we trained the OccamLLM switch to avoid using OccamNet for compound operations, largely mitigating this issue.Future work could explore other solutions such as integrating a two-layer OccamNet as the symbolic network.We found that these issues are particularly acute in the GSM8K and Single Eq datasets, where the expressions generated by Llama are not prevalent in the switch training data, causing it to sometimes incorrectly trigger OccamNet and degrade performance, as discussed more in Appendix A.5.</p>
<p>Furthermore, we found that the language model sometimes appends further digits to OccamLlama outputs, defeating the purpose of OccamLlama generations.To address this issue, we append "\n\n." to every number computed with OccamNet, emulating the usual behavior of Llama.</p>
<p>These techniques demonstrate a design paradigm of OccamLlama: by tuning the behaviors of OccamNet and the switch, we can often avoid finetuning the LLM.</p>
<p>Discussion</p>
<p>We presented OccamLLM, a system enabling exact and interpretable language model arithmetic in a single autoregressive step.Our method does not require modifying the weights of the underlying language model, thereby avoiding risks of catastrophic forgetting.Furthermore, our method avoids security risks arising from running code generated by a language model while outperforming top LLM code generation methods (GPT 4o + Code) on average across our benchmarks.</p>
<p>We benchmarked our method on challenging arithmetic tasks, achieving 100% accuracy where GPT 4o achieves only 40% performance on average.We also benchmarked our method on mathematical problem solving tasks, demonstrating that the OccamLlama switch can accurately balance the LLM for reasoning and OccamNet for arithmetic, outperforming even GPT 4o and GPT 4o with Code Interpreter on average.</p>
<p>Our work could enable smaller LLMs to be as performant as much larger LLMs in arithmetic.Moreover, integrating OccamLLM with larger LLMs like GPT 4o could further improve their arithmetic abilities without requiring a code interpreter.Furthermore, at present, OccamLLM may not integrate with more advanced decoding techniques such as speculative decoding [38,43].We hope to explore these avenues in future work.</p>
<p>Broader Impact</p>
<p>We believe that, in addition to enabling fast, safe, and interpretable arithmetic, OccamLLM demonstrates a new paradigm for tool use.As a proof of concept for more complex tool use, we further train OccamLlama 8B with a two layer Complete OccamNet with the primitives
P = {Addition(•, •), Subtraction(•, •), Multiplication(•, •), Division(•, •)},
which enables OccamLlama to perform up to three arithmetic operations (e.g., 2 • 7 + 3/2) in a single autoregressive step.We find that this two-layer OccamLlama can reach near 100% accuracy, even when performing three arithmetic operations in a single autoregressive step, as shown in Table 3.This demonstrates that OccamLLM can be used to perform more complex operations, including composing multiple different tools.For future work, we plan to explore integrating other tools beyond calculators through a similar technique.This is facilitated by the fact that there are no restrictions on OccamNet's activations; in principle, tools could be placed inside activations of OccamNet, enabling OccamNet to serve as a sort of a mixture of experts for tools.While some tools, like querying a search engine, may still be most effective when integrated into language model systems through language, we believe this work demonstrates that some tools are more effective when they can be more tightly integrated into the language model.</p>
<p>Appendix A Further Experiment Details and Results</p>
<p>For training and evaluation OccamLlama 8B, we used a single 32 GB NVIDIA Tesla V100 GPU.For OccamLlama 70B, we used two 80 GB NVIDIA A100 GPU.</p>
<p>In the experiments presented in Section 4, for each of the weight decoders and the switch, we used two-layer MLPs of input size 4096/8192 (Llama 3 8B/70B Instruct hidden size), intermediate size 64 and final size equal to the number of weights in the corresponding OccamNet layer or switch.</p>
<p>In the two-layer experiments presented in Section 6, for each of the weight decoders, we used two-layer MLPs of input size 4096 (Llama 3 8B Instruct hidden size), intermediate size 512, and final size equal to the number of weights in the corresponding OccamNet layer.We did not train a switch for this experiment as we did not test long-form generations.</p>
<p>A.1 Training Dataset</p>
<p>A.1.1 OccamNet Decoder</p>
<p>To train the OccamNet decoder, we created a training dataset consisting of a 80,000 examples split in 40,000 single queries and 40,000 sequences of concatenated queries.In the first case, we sampled a single prompt of those described in 3.2 and formatted it using the Llama 3 Instruct chat template.In the second case, we concatenated multiple prompts described in 3.2 without the chat template.The remaining 20% corresponds to single or multi step problems reasoning prompts.The inputs were sampled with various ranges, sometimes as floats and sometimes as integers, depending on the context of the problem.Because a single-OccamNet-layer OccamLlama cannot solve a multi-step reasoning problem in a single step, we never end the multiple-query examples with a multi-step reasoning problem.</p>
<p>We first iterated the 80,000 examples, prepending "Answer = " to the assistant response, thus training OccamNet to predict the result after the "=".Next, we validated the model on out-of-distribution examples where "Answer = " was not appended.We noticed that the accuracy on this task was improving during training, but after the full dataset was iterated it still didn't perform as well as when evaluated in-distribution.Therefore, we continued to train the model using examples of the same dataset but with no "Answer = " at the beginning of the assistant response.The model rapidly learned the new task.We stopped at 28,000 iterations of this second stage.</p>
<p>For the two-layer OccamNet run, we generated a large set of programmatically generated prompts of the form 3 + 97 • −4 =, with the Llama 3 Instruct chat template applied.</p>
<p>A.1.2 Switch Decoder</p>
<p>To train the switch decoder, we created a dataset of 50,000 examples (80,000 for OccamLlama 80B).</p>
<p>For each example, the tokens previous to the numbers that should be computed using OccamNet, which are the ones that the switch should not route to the LLM, are labeled with a 1, and all the rest are labeled with a 0.</p>
<p>Half of the examples consist of a single prompt corresponding to a simple arithmetic expression as the ones described in Section 3.2.The token immediately at the beginning of the assistant response is labeled with a 1.Therefore, the trained system will answer directly to simple arithmetic queries that OccamNet can compute.</p>
<p>The remaining 25,000 examples consist each of a series of prompts which are formatted in the Llama 3 Instruct chat template in a conversational style.The input-output pairs used to create each sequence of prompts are distributed in the following way:</p>
<p>• 25% of these pairs are created by taking one of the simple arithmetic expressions as input.The output is selected randomly between answering directly at the beginning of the assistant response, adding "Answer = " before the answer, or repeating the input expression before the answer.These examples train the switch to trigger OccamNet in different scenarios where the LLM needs to compute an answer.• 70% of the pairs come from a collection of 43 manually created and labeled examples, which illustrate in which cases the switch should route to OccamNet and, importantly, in which cases it shouldn't.This collection was designed to cover a wide variety of situations where the LLM might need to use OccamNet for computations.Furthermore, it includes cases where the LLM should avoid calling OccamNet because doing so would produce a bad prediction.This is the case, for example, of instances where the LLM attempts to add three numbers simultaneously.If it were to use the 1-layer OccamNet, which can take 2 inputs at most, the result would be incorrect.• The remaining 5% of the prompts come from multi-step reasoning problems.We set the output for these not to a full response, but only "The answer is ".In such cases, a single-layer OccamNet cannot compute the answer, so the output tokens are labeled with a 0. This trains the system to avoid routing to OccamNet when the later cannot compute the answer.</p>
<p>Below, we present some manually created examples and explain what case they cover.The tokens of the numbers in bold were marked with a 1 to signal that the switch should route to OccamNet, and the rest of the numbers and all of the text tokens were marked with a 0 to signal that OccamNet should not be called in these cases.</p>
<p>The cases we cover with these prompts are:</p>
<ol>
<li>Single-layer OccamNet should be called to perform two-input arithmetic operations.2. OccamNet should not be called to write numbers that already appeared and do not require further arithmetic.3. OccamNet should not be called to round numbers.4. OccamNet should not be called to perform inline operations nested within a larger expression because it might contradict the assumption that the last two numbers are involved in the operation.5. OccamNet should not be used to convert magnitudes when the LLM does not explicitly write the conversion factor and relies on its internal knowledge; without the numeric value of the factor present in the text, OccamNet can't give the correct result.6. OccamNet can't compute percentages, as this operation was not included in the training set of operations.It could be done in two steps.7. Single-layer OccamNet can't compute a fraction of a number, as this would require two operations.8. Single-layer OccamNet can't perform operations with more than 2 arguments.</li>
</ol>
<p>User Query Assistant Response Cases</p>
<p>An author writes 7.6 pages per session.After 9 sessions, the total pages written are</p>
<p>The author writes 7.6 pages per session.After 9 sessions, the author will have written 7.6 ×9 = 68.4</p>
<p>pages.The answer is 68.</p>
<p>1, 2, 3</p>
<p>OccamLlama: We experimented with OccamLlama prompts, but discovered that not including a system prompt was most effective.</p>
<p>A.4 Generation parameters</p>
<p>For OccamLlama, Llama 2 7B and Llama 3 8B, we use the default values of T = 0.6 and Top-P = 0.9.For GPT 3.5 Turbo, GPT 4o, and GPT 4o with Code Interpreter, we use the default values of T = 1.0 and Top-P = 1.0.</p>
<p>A.5 Experimental Results</p>
<p>Tables 5 and 6 show in more detail the accuracy of OccamLlama and other baselines on arithmetic and mathematical problem solving tasks.We measure accuracy as described in the main text.</p>
<p>We note here that on datasets with challenging arithmetic, in particular Multiarith Float and MATH401, OccamLlama 8B outperforms even GPT 4o and GPT 4o Code.In fact, on MultiArith Float, Occam-Llama 8B is nearly 10 percentage points more accurate than GPT 4o + Code and and more than 40 percentage points more accurate than Llama 3 8B.Similarly, on MATH401, OccamLlama 8B is 7 percentage points more accurate than GPT 4o + Code and nearly 25 percentage points more accurate than Llama 3 8B.Although MATH401 does not include word problems, it does include some arithmetic expressions that require multiple calls to OccamNet to solve, meaning it requires both reasoning (to determine how to break up the arithmetic expression) and arithmetic capabilities.</p>
<p>The only datasets on which OccamLlama 8B performs substantially worse than Llama 3 8B are GSM8K [40] and Single Eq [41].We believe a contributor to this is that these datasets include many problems that involve either fractions and percentages, which Llama does not convert to decimal format, or equations with unknown variables.As such, Llama often calls OccamNet with expressions such as "multiplying by 3/4 gives," "5% of this gives," or "adding 5 to both sides of x-5 = 11 gives."Because the switch is not trained on many examples like these in which the number is not in decimal format, it does not realize that OccamNet should not be used in these cases.Therefore, the switch triggers OccamNet, which is not capable of performing the correct operation (these types of operations are not acheivable with a 1-layer OccamNet).Future work could address this issue by training the switch with more data on this type of situation or by training an OccamLlama with a two layer OccamNet.</p>
<p>Finally, as noted in the main text, OccamLlama 70B achieves significant performance improvement over OccamLlama 8B across a number of benchmarks and outperforms GPT 4o and GPT 4o + Code on average.This demonstrates that OccamLLM improves with the base language model and suggests that combining OccamLLM with more capable models such as GPT 4o could be a promising avenue for future research.</p>
<p>Relative error is another important metric that complements accuracy.It measures by how much the answer differs from the true result.For two models with a similar accuracy metric, the relative error they achieve can be very different.Table 7 shows the relative error for the arithmetic experiments.</p>
<p>An answer marked correct can have a nonzero relative error because of machine precision limits and because the answer does not report an infinite number of digits.</p>
<p>Interestingly, Llama 2 performs exceptionally poorly on division.By examining outputs, we see that this is because Llama 2 produces an approximately correct output but with the decimal place in the wrong position, leading to a result that is off by many orders of magnitude.</p>
<p>B Example OccamLlama Generations</p>
<p>In this section, we include example OccamLlama 8B generations from the MATH401 and MultiArith-Float datasets.We randomly selected three examples for each dataset.OccamNet outputs are included in green.We omit prompt formatting to save space.Similarly, although outputs from OccamNet are always followed by "\n\n," we omit these newlines to save space, instead adding a period and space after each OccamNet generation.</p>
<p>By chance, all six responses happen to be correct.</p>
<p>We also include three miscellaneous prompts and OccamLlama 8B generations to demonstrate OccamLlama's ability to handle more challenging prompts.</p>
<p>B.2 MultiArith-Float Example 1</p>
<p>Prompt:</p>
<p>Mike had 30 video games but 6 of them weren't working.If he wanted to sell the working games for 85.53 each, how much money could he earn?</p>
<p>Response:</p>
<p>Mike had 30 video games.6 weren't working, so he had 30 -6 = 24.He can sell 24 games for 85.53 each.24 x 85.53 is 2052.720.So Mike could earn 2052.72 dollars.</p>
<p>Example 2</p>
<p>Prompt:</p>
<p>While playing a trivia game, Adam answered 6 questions correct in the first half and 3 questions correct in the second half.If each question was worth 59.08 points, what was his final score?</p>
<p>Response:</p>
<p>Adam got 6 questions correct in the first half, and 3 questions correct in the second half.So he got 6 + 3 = 9.Each question was worth 59.08 points.So his score is 9 * 59.08 = 531.720.The answer is 531.72.</p>
<p>Example 3</p>
<p>Prompt:</p>
<p>Lana's favorite band was holding a concert where tickets were 181.92 dollars each. Lana bought 5 tickets for herself and her friends and 4 extra tickets in case anyone else wanted to go.How much did she spend?</p>
<p>Response:</p>
<p>Lana bought 5 tickets for herself and her friends.Then 4 extra tickets.So in total she bought 5 + 4 = 9.Each ticket was 181.92 dollars.So 9 tickets will be 9 x 181.92 = 1637.280.The answer is 1637.280.</p>
<p>B.3 Miscellaneous prompts</p>
<p>These miscellaneous prompts demonstrate OccamLlama's ability to handle more challenging prompts.Examples 1 and 2 show that OccamLlama can handle prompts where the numbers are provided as words and not as decimals.This works because we can choose to parse strings such as "three" and "four" as 3 and 4, respectively, when parsing inputs to OccamNet.Example 3 shows that OccamLlama can handle prompts in which many irrelevant numbers are included.This works because OccamLlama uses Llama to reason and select the correct numbers to use before feeding these numbers into OccamNet.</p>
<p>Example 1</p>
<p>Prompt:</p>
<p>Six minus seven =?</p>
<p>Response:</p>
<p>-1. Six minus seven is equal to -1.</p>
<p>Example 2</p>
<p>Prompt:</p>
<p>Three + four = ?Response: The number of planets discovered in 1932 was 100.The number of planets discovered in 1933 was 200.So the total number of planets discovered is 100 + 200 = 300.300.</p>
<p>C OccamLLM Robustness Experiments</p>
<p>We find that OccamLlama displays remarkable generalization capabilities on out-of-distribution problems.To demonstrate this, we show below two out-of-distribution tasks on which OccamLlama performs remarkably well.</p>
<p>C.1 Non-textual Training</p>
<p>First, we train the OccamNet decoder from scratch, using only numeric expressions and absolutely no text at all.This means that any problem with text, such as a word problem, is far out-of-distribution of the OccamNet decoder's training data.We test this model (using the standard router), which we denote OccamLlama 8B Arith, on the mathematical reasoning benchmarks and obtain remarkably good results, shown in Table 8.</p>
<p>The OccamLlama 8B Arith performs on par with the model trained with both numbers and text, even achieving higher accuracy on some benchmarks.This shows that the OccamLLM framework is robust, and points towards the fact that the representations of arithmetic that are built in the transformer body of the LLM and extracted by the OccamLLM Decoder are very general.</p>
<p>In contrast, we expect that finetuning Llama to perform arithmetic using only numeric examples and no text whatsoever would lead to extreme catastrophic forgetting and poor arithmetic performance on word problems.As such, we believe this data shows a remarkable generalization and robustness of OccamLLM.</p>
<p>C.2 Multilingual Reasoning</p>
<p>To further demonstrate OccamLlama's generalization capabilities and also show that OccamLlama can handle non-English generation, we tested OccamLlama on the Multilingual Grade School Math Benchmark (MGSM) [44], a dataset consisting of GSM8K translated into 10 languages (Bengali, Chinese, French, German, Japanese, Russian, Spanish, Swahili, Telugu, and Thai).For these experiments, we prompted the LLMs to write their answers in the same language as the problem statement.Otherwise, the LLM would typically respond always in English, defeating the purpose of the experiment.We compute the drop in accuracy when switching from English to another language, given by the accuracy of a model on the English dataset minus the accuracy of the model on the dataset in a given language.The results are shown in Figure 5.</p>
<p>The table above shows that OccamLlama and Llama have similar performance drops between the English dataset and each non-English language dataset.On most languages and on average, OccamLlama has a smaller performance drop than Llama.The fact that OccamLlama (the decoders for which have never been trained on other languages) has a smaller average out-of-distribution performance drop than Llama (a model trained on over 750 billion tokens of non-English text) is in our opinion quite remarkable.</p>
<p>We believe that this test demonstrates OccamLlama's ability to handle many languages and its robustness against out-of-distribution data.</p>
<p>D Alternative Architectures and Losses D.1 Alternative Architectures</p>
<p>As discussed in the main text, although OccamLLM works most naturally with OccamNet, it can also work with other symbolic architectures such as the EQL network [45,46], or architectures that can represent probability distributions over symbolic expressions, such as transformers [31] or recurrent neural networks (RNNs) [32].</p>
<p>However, in practice we believe OccamNet is the most effective architecture for this use case.We find that because EQL does not represent a probability distribution over functions, it easily gets stuck in local minima.</p>
<p>Regarding transformers and RNNs, we believe that OccamNet possesses a key advantage of being interpretable; simply by looking at the weights, it is possible for a human to determine which functions OccamNet assigns a high probability.We believe that this interpretability will make OccamNet easy for a decoder to initialize with the desired distribution.On the other hand, an RNN or transformer have substantially more complex relations between the weights and corresponding probability distribution, which we hypothesize would make learning a decoder for such models difficult.</p>
<p>This leads us to a key point: transformers and RNNs are effective for modeling complex multimodal distributions, but for this problem, we want to select a single function for each token, so the extra expressivity of these models is unneeded and likely detrimental to performance.We believe that OccamNet, a much simpler architecture, enables better parameter efficiency and performance.</p>
<p>D.2 Alternative Losses</p>
<p>In this section we discuss alternative possible losses and how we arrived at the loss in Equation 3.</p>
<p>We considered two loss functions which are natural when optimizing a probability distribution: 1) a cross-entropy loss, and 2) a REINFORCE [34] loss.Each of these requires only a slight modification to reach Equation 3.This discussion thus illustrates how our loss combines benefits from both the cross-entropy and the reinforcement-learning losses.</p>
<p>Cross-Entropy Loss</p>
<p>The cross-entropy loss is effective at modeling probability distributions.Given a ground truth distribution q x [f ] conditioned on the input text x, the cross-entropy loss is given by L(x, y; W
) = − f q x [f ] log p W [f ].(4)
Unfortunately, for OccamLLM, the ground-truth distribution q x [f ] is not uniquely specified.In particular the only constraints on q x [f ] are that it is is normalized and satisfies q x [f ] = 0 if f is not the desired function (i.e., f (x) ̸ = y).Since the same function can be represented in many ways in the OccamNet network (a property true of many function representations), multiple f may satisfy f (x) = y, so q x is underparametrized.</p>
<p>The most natural choice for q x is to weight each valid function equally:
q x [f ] = c x if f (x) = y 0 otherwise (5)
where c x is a constant chosen such that q x is normalized, given by the inverse of the number of functions f satisfying f (x) = y.However, determining c x requires testing every possible function f , which may be infeasible for large OccamNet networks.Further, this q x requires OccamNet to learn a superposition of functions, which may be challenging given its relatively low parameter count.</p>
<p>Another option is to choose a canonical form f * for each function and to set q x to be a 1-hot distribution that is nonzero only at f * .Although this removes the challenge of learning a superposition, it still requires sampling nearly all functions in OccamNet due to the sparsity of q x .</p>
<p>Ideally, we would like to find a q x with the following conditions:</p>
<p>• It enables the cross-entropy loss to be calculated by sampling from OccamNet.This allows us to avoid needing to iterate through and evaluate every f (x) each time we compute the loss, since we can instead obtain a Monte-Carlo estimate.</p>
<p>• It is minimized when p W is a 1-hot probability distribution.This ensures that OccamNet can represent the optimal distribution.• It has q x [f ] ̸ = 0 for all f satisfying f (x) = y.This improves sample-efficiency by increasing the probability of sampling an f with q x [f ] &gt; 0.</p>
<p>A solution is to set
q x [f ] = c x p W [f ] if f (x) = y 0 otherwise (6)
where c x is chosen such that q x is normalized.This gives a loss
L(x, y; W ) = − f q x [f ] • log p W [f ] = − f c x p W [f ] • δ(f (x) − y) • log p W [f ] ≈ − c x N f ∼p W δ(f (x) − y) • log p W [f ] ≈ − f ∼p W δ(f (x) − y) • log p W [f ] f ∼p W δ(f (x) − y)
, where
δ(f (x) − y) = 1 if f (x) = y 0 otherwise (7)
and in the last step we used the fact that c x can be approximated as
c x = 1 f p W [f ]δ(f (x) − y) ≈ N f ∼p W δ(f (x) − y)
.</p>
<p>This loss is easily computed by sampling from p W , it satisfies q W &gt; 0 for all f satisfying f (x) = y, and it is minimized when p W is a delta function centered at any f satisfying f (x) = y, as desired.</p>
<p>Note that
L(x, y; W ) = − f ∼p W δ(f (x) − y) • log p W [f ] f ∼p W δ(f (x) − y)(8)
is exactly the loss given in Equation 3with R(f (x), y) = δ(f (x) − y).Thus, we have shown how Equation 3 can be interpreted as a cross-entropy loss.Equation 3 with general R(f (x), y) can be seen as a cross-entropy loss with a "smoothed" ground truth distribution q x given by q
x ∝ p W [f ] • R(f (x), y).
REINFORCE Loss Reinforcement-learning losses are effective for exploring large search spaces.We use a modification of the REINFORCE [34] loss because it is relatively simple to implement.Future work could explore more sophisticated variants of this algorithm, such as Proximal Policy Optimization [47].</p>
<p>The standard REINFORCE loss applied to OccamLLM gives
L(x, y; W ) = − 1 N f ∼p W R(f (x), y) • log p W [f ].
Note that for sparse R, there will be very few nonzero R(f (x), y) sampled, so, since we are dividing by N , the gradient signal will be small.We modify REINFORCE by dividing by the the sum of the rewards for all samples instead of by N to ensure that correct functions sampled only a few times still receive a large training step update.This once again produces Equation 3.</p>
<p>We find that using a delta function for our reward is most effective because it most accurately represents the sparse reward of the problem.Further, as shown above, this loss provides a Monte-Carlo estimate of the the cross entropy loss.Due to the sparse reward, many samples may initially be required to obtain an accurate estimate of the loss.However, as OccamNet approaches the desired distribution, the loss's sample efficiency will improve.</p>
<p>E Background on OccamNet</p>
<p>This section is heavily modified from [11].</p>
<p>We divide this section into the following subsections:</p>
<p>1.In Section E.1, we describe OccamNet's architecture in more detail.2. In Section E.2, we describe OccamNet's sampling process.</p>
<ol>
<li>In Section E.3, we describe OccamNet's probability distribution.4. In Section E.4, we describe OccamNet's initialization process.</li>
</ol>
<p>E.1 OccamNet Architecture</p>
<p>As described in the main text, we start from a predefined collection of N primitive functions P.</p>
<p>OccamNet represents a distribution over compositions of functions in P. From now on, we denote the ith primitive function in the lth layer as ϕ (l)</p>
<p>i .We begin indexing the primitives from 0 and the layers from 1, because we treat the inputs as the 0th layer.So, for example, in Figure 6a, ϕ
2 = ϕ (2) 2 = ϕ (3) 2 = sin.(1)
Each OccamNet layer consists of two sublayers, which we denote the arguments and image sublayers, shown in Figure 6a.For an L-layer OccamNet, each of these sublayers is reproduced L times.The lth softmax layer connects the (l − 1)th image layer with the lth arguments layer.For 1 ≤ l ≤ L, we denote the lth arguments sublayer hidden state as h (l) and the lth image sublayer hidden state as h (l) .So, h (2) would represent the middle layer of nodes labeled P in Figure 6a.We further write
h (l) = h (l) 1 , . . . , h (l) M (l) ⊤ , h (l) = h (l) 1 , . . . , h (l) N (l) ⊤ ,(9)
where l) is the number of primitives in layer l, and α[ϕ] is the arity of function ϕ.We also define h (0) to be the input layer (an image sublayer) and h (L+1) to be the output layer (an arguments sublayer).
M (l) = 0≤k&lt;N (l) α ϕ (l) k , N(
In a standard OccamNet layer, each primitive is repeated exactly once in each layer.However, in Complete OccamNet, each primitive in the lth layer is repeated A L−l times, where A is the maximum arity of the primitives.This is shown in Figure 7 in the transition from 7a to 7b.Complete OccamNet also concatenates each image layer to the next image layer, as shown in Figure 7c.</p>
<p>E.2 Sampling from OccamNet</p>
<p>In this section, we more carefully describe OccamNet's sampling process.We sample a connection to each arguments layer node from the distribution given by the softmax of the softmax-layer weights leading to that node.In particular, if w (l)</p>
<p>i are the weights of the lth softmax layer leading to the ith node of the lth argument's layer, when we sample we produce a sparse matrix
SAMPLE             softmax(w (l) 1 ) . . . softmax(w (l) M (l) )            (10)
where the SAMPLE function samples a one-hot row vector for each row based on the categorical probability distribution defined by softmax(w).To evaluate this sample, we simply evaluate a forward pass through the network, treating the sampled sparse matrices from the softmax layers as the weights of linear layers:
h (l) =       h (l) 1 . . . h (l) M (l)       ≡ SAMPLE             softmax(w (l) 1 ) . . . softmax(w (l) M (l) )             h (l−1) ,(11)
To complete the picture of the forward pass, we formalize how we deal with activations accepting multiple inputs.We define the action of the activation functions as follows:
h (l) i = ϕ (l) i h (l) j , . . . , h (l) j+α[ϕ (l) i ]−1 , j = 0≤k&lt;i α ϕ (l) k .(12)</p>
<p>E.3 OccamNet's Probability Distribution</p>
<p>OccamNet parametrizes a probability distribution over all functions which it can sample.In particular, when OccamNet samples a function, it is really sampling a directed acyclic graph (DAG) which defines a computational path to compute a function.The probability of sampling a computational graph is equal to the product of the probabilities of the connections in the DAG which are connected to the output node.</p>
<p>Note that multiple computational graphs can correspond to the same function.In this paper, when we refer to a function sampled from OccamNet or the probability of a function according to OccamNet, we use function as a shorthand for a particular computational graph corresponding to that function.Although this underspecifies the computational graph in question, this is never an issue because we always refer to functions in abstract.</p>
<p>When using OccamLlama for inference, we select the maximum probability function by sampling 100 functions from OccamNet, evaluating their probabilities as described above and selecting the maximum one.</p>
<p>E.4 Initialization</p>
<p>This section describes how we calculate W * from the main text.We wish to initialize W * such that p W * [f 1 ] = p W * [f 2 ] for all f 1 and f 2 .Below, we assume that skip connections do not exist.However, the algorithm also works for skip connections, requiring only a small modification to Equation 14.</p>
<p>Unfortunately, such an initialization is impossible for any OccamNet with two or more layers containing primitives with more than one argument.However, it is possible to initialize OccamNet such that a lower bound q W * of the true probability p W * is independent of f.</p>
<p>Define the probability of a function f up to a given node as the product of the probabilities of the edges that lead to that node in the DAG of f .Intuitively, q W [f ] approximates p W [f ] by maintaining a lower bound on the probability of f up to each node of an OccamNet and propagating that lower bound through the computational graph given by f .</p>
<p>To define q W more precisely, let q (l) i [f ] and q (l)</p>
<p>i [f ] be the probability bounds corresponding to the ith node of the lth image or arguments sublayer.We have suppressed the dependence on W for notational convenience.We compute these probabilities starting with the inputs, for which we set q (0) i = 1.We then propagate probabilities to the arguments layers according to q (l+1) i = softmax(w
(l+1) i ) j q (l) j , (13)
where j is the node in the lth image layer which f connects to the ith node of (l + 1)th arguments layer.Similarly, we propagate probabilities to the image layers according to
q (l) i = n+α[ϕ (l) i ]−1 k=n q (l) k , n = i−1 j=1 α ϕ (l) j .(14)
Finally, we define q W [f ] = q (L+1) 0</p>
<p>[f ].</p>
<p>In practice q W [f ] ≤ p W [f ], where equality holds for many functions.In fact, q W [f ] &lt; p W [f ] only when part of the DAG of f is used as input to two different arguments nodes.In cases such as these, the portion of the DAG that is used twice multiplicatively contributes the probability of its edges to q W [f ] twice, artificially suppressing its value.However, because q W [f ] is a lower bound, initializing W * to equalize q W * still has the desired effect of ensuring adequate coverage for each f in the initial probability distribution of OccamNet.</p>
<p>With this primer, we can now define the algorithm to initialize W * such that q W * [f ] is uniform.</p>
<p>The algorithm traverses through OccamNet layer by layer and establishes as an invariant that, after assigning the weights up to the lth layer, q</p>
<p>i [f ] are equal for all i and f .This implies that, after assigning the weights up to the lth layer, q (l) i [f ] are equal for all f , but not necessarily for all i.We denote the common value of q (l) i [f ] as q (l) and the common value of q
(l) i [f ] as q (l) i .
The algorithm starts with input layer, where q (0) i = 1 automatically.Once the invariant above is true for a given l, the algorithm sets
w * (l+1) i j = log   min k q (l) k q (l) j  (15)
for all i, j, where w * (l+1) i j denotes the weight connecting the jth node in the lth image layer to the ith node in the (l + 1)th arguments layer.This establishes the invariant for l + 1 because q (l) j softmax(w * (l+1) i</p>
<p>) j = q (l) j exp w * (l+1) i j k exp w * (l+1) i k = q (l)
j min k q (l) k /q (l) j k min m q (l) m /q (l) k = 1 k 1/q (l) k
, which is a constant over both i and j, so q (l+1) i</p>
<p>[f ] is a constant over both i and f.The algorithm repeats the above procedure until it has traversed the entire network.</p>
<p>In summary, the algorithm involves the following steps:</p>
<ol>
<li>
<p>Set l = 0 and q (l) i = 1. 2. Increment l by 1.</p>
</li>
<li>
<p>Set W * (l) according to Equation 15.</p>
</li>
<li>
<p>If l &lt; L + 1, Compute q (l+1) and q (l+1) i</p>
</li>
</ol>
<p>Figure 2 :
2
Figure 2: a) A schematic of the OccamNet architecture, with softmax layers in grey and their outputs in red.b) A Directed Acyclic Graph (DAG) (with edges not connected to the output removed for clarity) formed by sampling from OccamNet.This DAG corresponds to the function sin(sin(x 1 ) • exp(x 0 )).Modified from[11].</p>
<p>Figure 4 :
4
Figure 4: Examples from Llama 3 8B Instruct and OccamLlama 8B on (top) an arithmetic problem and (bottom) a mathematical reasoning problem from the MultiArith Float dataset.In OccamLlama, the LLM performs reasoning, the switch predicts when to use OccamNet, and OccamNet performs arithmetic operations.OccamNet's inputs and outputs are highlighted in purple and green, respectively.</p>
<p>40% of the sampled</p>
<p>prompts correspond to simple arithmetic, concretely +, −, ×, and ÷.We sampled from various input value ranges, chosen at random: integers in [−10, 10], integers in [−100, 100], integers in [−1000, 1000], integers in [−20000, 20000], floating numbers in [−1, 1], and floating point numbers in [−1000, 1000].Another 40% corresponds to complex arithmetic involving square roots, logarithms, exponentials, trigonometric functions and computing one number to the power of another.For the square root and the logarithm, we sampled integers uniformly in either[1, 100]  or[1, 20000]  and floats uniformly in either [0.01, 100] or [0.01, 20000].For the exponential, we sampled integers and floats in [−10, 10].For the powers, we sampled the base as either an integer in [1, 25] or a float in [0.1, 25] and the exponent as an integer in [−6, 6].</p>
<p>. 63.0 + 0.074 = 63.074.So the answer is 63.074.</p>
<ol>
<li>3 + 4 = 7 .
347
Let me know if you have any other questions!Example 3 Prompt: In 1932, 100 planets were discovered and in 1933, another 200 were found.What is the total number of planets discovered between 1932 and 1933?Response:</li>
</ol>
<p>Figure 5 :
5
Figure5: Model performance degradation for each language relative to English in the MGSM dataset.OccamLlama 8B's performance degradation is considerably less than Llama 3 8B's performance degradation, demonstrating strong multilingual and generalization capabilities.</p>
<p>Figure 6 :
6
Figure 6: a) An example OccamNet, with image layers boxed in green and arguments layers boxed in blue.We denote the inputs as the 0th image layer and the outputs as the (L + 1)th arguments layer.Nodes in the arguments layers are represented with a P because of their probabilistic nature.b) A demonstration of the dropped connections from sampled paths in OccamNet.All light grey paths are dropped from the final symbolic form of the sampled function because they are not directly connected to the outputs.</p>
<p>Figure 7 :
7
Figure 7: The progression of enhancements leading to a Complete OccamNet from a standard OccamNet.a) A standard OccamNet without repeated activations or skip connections.b) The same OccamNet as in a) with activations repeated in earlier layers.c) The same OccamNet as in b) with added skip connections.This is a Complete OccamNet.</p>
<p>. 5 .
5
Return to step 2 until l = L + 1.</p>
<p>Table 2 :
2
Accuracy on arithmetic tasks, in percentages.The OccamLlama column corresponds to the results of both OccamLlama 8B and OccamLlama 70B.Higher is better.Bold indicates best performance for each row.
OccamLlama Llama 2Llama 3GPT 3.5 GPT 4oGPT 4o8B / 70B7B Chat 8b InstructTurboCodeAddition100.0±0.019.2±1.244.9±1.665.2±1.5 95.7±0.6 100.0±0.0Subtraction100.0±0.08.7±0.934.4±1.559.8±1.6 85.6±1.1 99.5±0.5Multiplication100.0±0.00.0±0.00.0±0.00.0±0.00.0±0.099.0±0.7Division100.0±0.02.8±0.535.3±1.510.7±1.0 38.6±1.5 100.0±0.0Square Root100.0±0.00.0±0.00.0±0.00.9±0.318.6±1.2 100.0±0.0Exponential100.0±0.00.3±0.23.1±0.512.5±1.0 23.2±1.3 100.0±0.0Logarithm100.0±0.00.1±0.10.0±0.017.1±1.2 21.3±1.3 100.0±0.0Sine100.0±0.07.6±0.87.0±0.813.4±1.1 39.3±1.5 100.0±0.0Cosine100.0±0.00.8±0.31.5±0.46.7±0.832.8±1.5 100.0±0.0AVERAGE100.0±0.04.4±0.214.0±0.420.7±0.4 39.5±0.5 99.8±0.1</p>
<p>Figure3: Accuracy of OccamLlama and baselines on mathematical problem solving tasks.Higher is better.OccamLlama 8B achieves accuracy comparable to Llama 3 8B on benchmarks with simple arithmetic, higher accuracy than GPT 4o and GPT 4o + Code on on tasks with challenging arithmetic, and accuracy above Llama 3 8B and similar to GPT 3.5 Turbo on average.OccamLlama 70B outperforms GPT 4o and GPT 4o + Code on average.
100OccamLlama 8B OccamLlama 70BLlama 3 8B Llama 3 70BGPT 3.5 Turbo GPT 4oGPT 4o +CodeAccuracy (%)40 60 80200r a g e</p>
<p>Table 3 :
3
Accuracy on multistep arithmetic.
OccamLlamaLlama 38b InstructOne-Step99.9±0.178.1±1.3Two-Step98.2±0.457.8±1.6Three-Step96.1±0.640.2±1.6AVERAGE98.1±0.358.7±0.9</p>
<p>Table 6 :
6
Percent accuracy on reasoning tasks.Higher is Better.Bold indicates best performance.
OccamLlamaOccamLlamaLlama 3Llama 3GPT 3.5GPT 4oGPT 4o8B70B8b Instruct70B InstructTurboCodeAddSub91.6±1.496.5±0.993.4±1.297.2±0.895.4±1.197.0±0.997.5±1.1GSM8K73.5±1.290.1±0.879.8±1.194.8±0.684.8±1.096.1±0.594.0±1.7MultiArith99.2±0.498.5±0.599.8±0.298.2±0.597.2±0.799.7±0.299.5±0.5MultiArith Float98.2±0.597.7±0.657.3±2.076.3±1.777.3±1.796.2±0.889.5±2.2MATH40185.0±1.889.5±1.560.3±2.471.6±2.363.1±2.476.6±2.178.0±2.9Single Eq92.9±1.196.9±0.896.3±0.897.6±0.797.8±0.698.0±0.699.0±0.7SVAMP88.6±1.093.2±0.886.3±1.194.5±0.787.8±1.096.2±0.696.5±1.3AVERAGE89.9±1.294.6±0.981.9±1.590.0±1.286.2±1.494.2±1.093.4±1.7</p>
<p>Table 8 :
8
Accuracy on reasoning tasks.Higher is Better.
OccamLlama OccamLlama Llama 2Llama 3GPT 3.5 GPT 4oGPT 4o8B8B Arith7B Chat 8b InstructTurboCodeAddSub91.6±1.492.7±1.378.0±2.193.4±1.295.4±1.1 97.0±0.9 97.5±1.1GSM8K73.5±1.271.6±1.236.0±1.379.8±1.184.8±1.0 96.1±0.5 94.0±1.7MultiArith99.2±0.498.5±0.576.0±1.799.8±0.297.2±0.7 99.7±0.2 99.5±0.5MultiArith Float98.2±0.595.3±0.923.3±1.757.3±2.077.3±1.7 96.2±0.8 89.5±2.2MATH40185.0±1.885.8±1.743.9±2.560.3±2.463.1±2.4 76.6±2.1 78.0±2.9Single Eq92.9±1.192.1±1.279.1±1.896.3±0.897.8±0.6 98.0±0.6 99.0±0.7SVAMP88.6±1.088.8±1.061.5±1.586.3±1.187.8±1.0 96.2±0.6 96.5±1.3AVERAGE89.9±1.189.3±1.156.8±1.881.9±1.386.2±1.2 94.2±0.8 93.4±1.5
AcknowledgementsWe would like to thank Andrew Ma and Di Luo for their thoughtful discussions.The authors acknowledge the MIT SuperCloud and Lincoln Laboratory Supercomputing Center for providing HPC resources that have contributed to the research results reported within this paper.Research was sponsored by the Department of the Air Force Artificial Intelligence Accelerator and was accomplished under Cooperative Agreement Number FA8750-19-2-1000.The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Department of the Air Force or the U.S. Government.The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein.This work is also supported in part by the National Science Foundation under Cooperative Agreement PHY-2019786 (The NSF AI Institute for Artificial Intelligence and Fundamental Interactions, http: //iaifi.org/).User QueryAssistant Response CasesI have 10 oranges and 6 apple trees, each of them with 3 apples.How much fruit do I have?The total number of pieces of fruit is 10 oranges + 6 trees times 3 apples = 10 + (6 ×3) = 10 + 18 =28.The answer is 28.A.2 Training HyperparametersFor all 1-layer OccamNet training runs, we used a batch size of 1, a learning rate of 6e − 4 and a weight decay parameter of 0.01.We use gradient accumulation to achieve an effective batch size of 8.We used a constant learning rate scheduler.We take 1000 samples from OccamNet per token.For the 2-layer OccamNet run, we used a batch size of 1, a learning rate of 1e − 4 and a weight decay parameter of 0.01.We use the gradient accumulation technique to achieve an effective batch size of 8. We used a constant learning rate scheduler.We take 50,000 samples from OccamNet per token.A.3 PromptingFor the division arithmetic tasks, we found that the language models often did not return decimals.As such, we appended "Give the answer in decimals."to these prompts.Similarly, for the trigonometric functions evaluations, we explicitly ask the language models to take the input as radians, by formatting the prompts as "cos(X rad) =".For some models, we provide system prompting to guide the model toward the correct behavior.We break down prompting by model below:Llama 2/3: We did not provide a system prompt for the arithmetic tasks.For the reasoning tasks, we used the system prompt "Solve step by step."GPT 3.5 Turbo: We do not use a system prompt for GPT 3.5 Turbo.GPT 4o:We did not use a system prompt, except for the MATH401 dataset, where we noticed that GPT 4o was returning fractions instead of decimals.As such, on MATH401 we used the system prompt "Give your answer in decimals."GPT 4o + Code: We used the system prompt "Write and run code to answer math questions.Do not format numbers.Give all answers in decimals."
Prompting large language model for machine translation: A case study. Biao Zhang, Barry Haddow, Alexandra Birch, 2023</p>
<p>Multilingual machine translation with large language models: Empirical results and analysis. Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen, Lei Li, 2023</p>
<p>Llms to the moon? reddit market sentiment analysis with large language models. Xiang Deng, Vasilisa Bashlovkina, Feng Han, Simon Baumgartner, Michael Bendersky, Companion Proceedings of the ACM Web Conference 2023, WWW '23 Companion. New York, NY, USAAssociation for Computing Machinery2023</p>
<p>Zengzhi Wang, Qiming Xie, Yi Feng, Zixiang Ding, Zinong Yang, Rui Xia, arXiv:2304.04339Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study. arXiv e-prints. April 2023</p>
<p>Sentiment analysis in the era of large language models: A reality check. Wenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan, Lidong Bing, 2023</p>
<p>. Josh Openai, Steven Achiam, Sandhini Adler, Lama Agarwal, Ilge Ahmad, Florencia Akkaya, Diogo Leoni Aleman, Janko Almeida, Sam Altenschmidt, Shyamal Altman, Red Anadkat, Igor Avila, Suchir Babuschkin, Valerie Balaji, Paul Balcom, Haiming Baltescu, Mohammad Bao, Jeff Bavarian, Irwan Belgum, . . Bello, Barret Zoph, 2024Gpt-4 technical report</p>
<p>Gemini: A family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, David Silver, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy Lillicrap, Angeliki Lazaridou, Orhan Firat, . . , Oriol Vinyals, 2024</p>
<p>How well do large language models perform in arithmetic tasks?. Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, 2023</p>
<p>Sung Joon, Joseph C Park, Carrie J O'brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Generative agents: Interactive simulacra of human behavior. 2023</p>
<p>Large language model based multi-agents: A survey of progress and challenges. Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, V Nitesh, Olaf Chawla, Xiangliang Wiest, Zhang, 2024</p>
<p>Owen Dugan, Rumen Dangovski, Allan Costa, Samuel Kim, Pawan Goyal, Joseph Jacobson, Marin Soljačić, arXiv:2007.10784OccamNet: A Fast Neural Model for Symbolic Regression at Scale. arXiv e-prints. July 2020</p>
<p>Julia Balla, Sihao Huang, Owen Dugan, Rumen Dangovski, Marin Soljacic, arXiv:2210.00563AI-Assisted Discovery of Quantitative and Formal Models in Social Science. October 2022arXiv e-prints</p>
<p>Catastrophic interference in connectionist networks: The sequential learning problem. Michael Mccloskey, Neal J Cohen, Psychology of Learning and Motivation. 241989Academic Press</p>
<p>Arithmetic with Language Models: from Memorization to Computation. Davide Maltoni, Matteo Ferrara, arXiv:2308.01154August 2023arXiv e-prints</p>
<p>GPT Can Solve Mathematical Problems Without a Calculator. Zhen Yang, Ming Ding, Qingsong Lv, Zhihuan Jiang, Zehai He, Yuyi Guo, Jinfeng Bai, Jie Tang, arXiv:2309.03241September 2023arXiv e-prints</p>
<p>Improving large language model fine-tuning for solving math problems. Yixin Liu, Avi Singh, C Daniel Freeman, John D Co-Reyes, Peter J Liu, 2023</p>
<p>Mathcoder: Seamless code integration in llms for enhanced mathematical reasoning. Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, Hongsheng Li, 2023</p>
<p>Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models. Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, Soujanya Poria, Roy Ka-Wei Lee, 2023</p>
<p>Catastrophic forgetting in connectionist networks. French Robert, Trends in cognitive sciences. 341999</p>
<p>Bayesian parameter-efficient fine-tuning for overcoming catastrophic forgetting. Haolin Chen, Philip N Garner, 2024</p>
<p>Delving into parameter-efficient fine-tuning in code change learning: An empirical study. Shuo Liu, Jacky Keung, Zhen Yang, Fang Liu, Qilin Zhou, Yihan Liao, 2024</p>
<p>Calc-x and calcformers: Empowering arithmetical chain-of-thought through interaction with symbolic systems. Marek Kadlčík, Michal Štefánik, Ondřej Sotolář, Vlastimil Martinek, 2023</p>
<p>Internet-augmented dialogue generation. Mojtaba Komeili, Kurt Shuster, Jason Weston, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. Preslav Smaranda Muresan, Aline Nakov, Villavicencio, the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational LinguisticsMay 20221</p>
<p>Webgpt: Browser-assisted question-answering with human feedback. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman2022</p>
<p>. Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze, Alicia Cheng, Taylor Jin, Leslie Bos, Yu Baker, Yaguang Du, Hongrae Li, Huaixiu Lee, Amin Steven Zheng, Marcelo Ghafouri, Yanping Menegali, Maxim Huang, Dmitry Krikun, James Lepikhin, Dehao Qin, Yuanzhong Chen, Zhifeng Xu, Adam Chen, Maarten Roberts, Vincent Bosma, Yanqi Zhao, Chung-Ching Zhou, Igor Chang, Will Krivokon, Marc Rusch, Pranesh Pickett, Laichee Srinivasan, Kathleen Man, Meier-Hellstern, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak2022Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben HutchinsonQuoc Le. Lamda: Language models for dialog applications</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, 2023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, 2023</p>
<p>Pal: Program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, 2023</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, 2023</p>
<p>Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification. Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, Hongsheng Li, 2023</p>
<p>Attention Is All You Need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, arXiv:1706.03762June 2017arXiv e-prints</p>
<p>Robin M Schmidt, arXiv:1912.05911Recurrent Neural Networks (RNNs): A gentle Introduction and Overview. arXiv e-prints. November 2019</p>
<p>Solving general arithmetic word problems. Subhro Roy, Dan Roth, 2016</p>
<p>Simple statistical gradient-following algorithms for connectionist reinforcement learning. Ronald J Williams, Mach. Learn. 83-4May 1992</p>
<p>Llama 3 model card. A I , Meta , 2024</p>
<p>. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, arXiv:2307.09288July 2023Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunovand Thomas Scialom. Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv e-prints</p>
<p>Hello gpt-4o. Openai, 2024</p>
<p>Yaniv Leviathan, Matan Kalman, Yossi Matias, arXiv:2211.17192Fast Inference from Transformers via Speculative Decoding. arXiv e-prints. November 2022</p>
<p>Learning to solve arithmetic word problems with verb categorization. Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, Nate Kushman, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Alessandro Moschitti, Bo Pang, Walter Daelemans, the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)Doha, QatarAssociation for Computational LinguisticsOctober 2014</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, ArXiv, abs/2110.141682021</p>
<p>Oren Etzioni, and Siena Dumas Ang. Parsing algebraic word problems into equations. Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Transactions of the Association for Computational Linguistics. 32015</p>
<p>Are NLP models really able to solve simple math word problems?. Arkil Patel, Satwik Bhattamishra, Navin Goyal, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, Yichao Zhou, Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterAssociation for Computational LinguisticsJune 2021Kristina Toutanova</p>
<p>Benjamin Spector, Chris Re, arXiv:2308.04623Accelerating LLM Inference with Staged Speculative Decoding. August 2023arXiv e-prints</p>
<p>Language models are multilingual chain-of-thought reasoners. Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, Jason Wei, 2022</p>
<p>Georg Martius, Christoph H Lampert, arXiv:1610.02995Extrapolation and learning equations. October 2016arXiv e-prints</p>
<p>Learning equations for extrapolation and control. Subham Sahoo, Christoph Lampert, Georg Martius, Proceedings of the 35th International Conference on Machine Learning. Jennifer Dy, Andreas Krause, the 35th International Conference on Machine LearningStockholmsmässan, Stockholm SwedenPMLR10-15 Jul 201880</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal Policy Optimization Algorithms. arXiv e-prints. July 2017</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1032 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1032</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1032</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-248406096</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2204.12844v2.pdf" target="_blank">Accelerating Robot Learning of Contact-Rich Manipulations: A Curriculum Learning Study</a></p>
                <p><strong>Paper Abstract:</strong> The Reinforcement Learning (RL) paradigm has been an essential tool for automating robotic tasks. Despite the advances in RL, it is still not widely adopted in the industry due to the need for an expensive large amount of robot interaction with its environment. Curriculum Learning (CL) has been proposed to expedite learning. However, most research works have been only evaluated in simulated environments, from video games to robotic toy tasks. This paper presents a study for accelerating robot learning of contact-rich manipulation tasks based on Curriculum Learning combined with Domain Randomization (DR). We tackle complex industrial assembly tasks with position-controlled robots, such as insertion tasks. We compare different curricula designs and sampling approaches for DR. Based on this study, we propose a method that significantly outperforms previous work, which uses DR only (No CL is used), with less than a fifth of the training time (samples). Results also show that even when training only in simulation with toy tasks, our method can learn policies that can be transferred to the real-world robot. The learned policies achieved success rates of up to 86\% on real-world complex industrial insertion tasks (with tolerances of $\pm 0.01~mm$) not seen during the training.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1032.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1032.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>No-CL DR Baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain Randomization Only Baseline (No Curriculum)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An embodied SAC agent trained with domain randomization only (uniform sampling) without an explicit curriculum; used as the baseline in experiments to compare curriculum effects on sample efficiency and sim2real transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SAC + TCN force-control agent (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Soft Actor-Critic (off-policy, max-entropy) policy with a Time Convolutional Network (TCN) mapping proprioception and F/T sensor inputs to a 24‑dim action vector that parameterizes a parallel position-force controller (PID/PI gains, selection matrix, and commanded offsets). Replay buffer with distributed prioritized experience replay was used.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent / physical robot (transferred)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Peg-in-hole assembly / industrial insertion tasks</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Simulated Gazebo peg-in-hole toy training environments (cylinders, hexagonal prism, cuboid, triangular prism) with randomized physical parameters; transferred and evaluated on real-world industrial insertion tasks (motor pulley, shaft, bearing) with sub-millimeter tolerances (±0.01 mm). Complexity arises from contact-rich dynamics, high stiffness position-controlled robot, tight tolerances, friction, and unstable grasps for round objects.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Task difficulty characterized by peg shapes (symmetry, corners), insertion tolerance (±0.01 mm for industrial tests), contact stiffness, friction, initial pose uncertainty, and episode horizon (max 1000 steps of 50 ms); domain randomization parameter set size referenced in Table I (Nr parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium (toy training) to high (industrial real-world tasks with ±0.01 mm tolerances)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Domain randomization of simulator numerical parameters (Table I) sampled uniformly per episode (UDR); ranges scaled by curriculum level when used; random seeds and multiple object shapes used for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (full DR ranges used during baseline training in simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate, average completion time, cumulative reward, contact force profiles</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Baseline trained with 100,000 time steps (this study, reduced from previous 500,000) showed lower real-world success rates than the proposed adaptive curriculum method; baseline often faster when successful but with much lower success on industrial tasks (exact baseline success rates not explicitly reported in text); collision thresholds: Fmax = 50 N (sim), 30 N (real).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Baseline (DR-only) demonstrates that high variation alone can improve transferability but can produce suboptimal, high-variance policies; the paper argues that adding a curriculum to guide complexity progression improves sample efficiency and final performance compared to uniform DR alone.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Domain Randomization (uniform sampling) without Curriculum Learning</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Tested on novel simulated peg shapes (trapezoid, star prism) and real-world industrial insertion tasks; baseline transferred but achieved substantially lower success rates on the real industrial tasks vs the best curriculum method and struggled with star-shaped peg alignment in real frictional settings.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Trained in this study with up to 100,000 time steps (one-fifth of the 500,000 steps used in prior work); previous work required ~500,000 time steps plus real-world refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>DR-only can enable sim2real transfer but may produce high-variance/suboptimal policies and lower success on complex contact-rich tasks; lacks the sample-efficiency and robustness achieved when combined with curriculum strategies and adaptive sampling.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1032.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1032.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linear-UDR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linear Curriculum with Uniform Domain Randomization (UDR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curriculum that linearly increases difficulty over episodes while sampling domain-randomized parameters uniformly within curriculum-determined subranges.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SAC + TCN force-control agent (Linear UDR)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same embodied SAC + TCN agent and controller parameterization as baseline; training schedule uses a linearly increasing curriculum level L_ep (L_ep = ep/ep_max) to restrict DR ranges early and expand them over time; DR sampling is uniform within the curriculum-determined subranges.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent / physical robot (transferred)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Peg-in-hole assembly (toy training with staged difficulty)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Training environments where each domain randomization parameter ψ_i is limited to [ψ_low, ψ_low + ψ_high * L_ep], so early episodes have lower variation and easier instances (e.g., shorter initial distances, lower stiffness), later episodes increase full variation.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Complexity is adjusted by curriculum level L_ep (0 to 1) which scales the per-parameter randomized ranges; tasks measured by distance-to-goal, stiffness, friction, and peg geometry. Episode cap 1000 steps.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>progressively from low to high (driven by linear schedule)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Domain randomization ranges scaled linearly with L_ep; sampling distribution: uniform within the scaled ranges (UDR).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>low initially (L_ep near 0) to high at end (L_ep = 1)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Cumulative reward during training, success rate and average completion time on held-out peg shapes and real tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Linear curricula performed only slightly better than no-curriculum in transfer to novel environments (text: linear curricula performed just slightly better than not using a curriculum at all); exact numeric values not given in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Linear scheduling reduced initial complexity but did not sufficiently guide exploration for robust transfer; paper reports that naive linear curricula can appear to aid learning but may not generalize well to novel, harder conditions compared to adaptive curricula.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum Learning (linear) + Domain Randomization (uniform sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Tested on novel simulated shapes and real-world tasks; linear curricula yielded modest improvements in training curves but poorer sim2real transfer relative to adaptive curricula, indicating weaker generalization to unseen, higher-complexity instances.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Training sessions capped at 100,000 time steps in this study; linear curricula improved cumulative reward somewhat but were outperformed by adaptive strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A simple linear increase of allowed DR ranges can speed apparent learning but may produce policies that do not generalize well to novel, more complex instances; curriculum design matters for transfer.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1032.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1032.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linear-GDR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linear Curriculum with Gaussian Domain Randomization (GDR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A linearly evolving curriculum where domain-randomization parameters are sampled from Gaussians centered on the curriculum level to allow occasional easier/harder tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SAC + TCN force-control agent (Linear GDR)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same SAC + TCN agent; uses linear curriculum level L_ep to define Gaussian means for each randomized parameter (mean centered at L_ep), sampling from N(mu=L_ep, sigma^2) clipped to parameter bounds, providing stochasticity around curriculum difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent / physical robot (transferred)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Peg-in-hole assembly with curriculum-scaled Gaussian DR</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Training tasks have DR parameter sampling that is Gaussian around the curriculum level, so most episodes match nominal difficulty but occasionally produce easier/harder instances; complexity increases linearly over training.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Curriculum level L_ep (0→1) used as Gaussian mean; complexity measured by the same task parameters (distance, stiffness, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>progressively from low to high (linear schedule)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Gaussian Domain Randomization (NDR) with mean at L_ep and hyperparameter variance; allows low-probability sampling of easier tasks to mitigate catastrophic forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>moderate-to-high as L_ep increases; stochastic variation around curriculum level</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Cumulative reward, success rate, completion time</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Gaussian sampling improved agents' performance during learning compared to Uniform sampling under linear curricula (text: using Gaussian instead of Uniform significantly improves agents' performance during learning); exact numeric values not provided in text.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Using Gaussian sampling around curriculum level provides a trade-off: keeps general difficulty increasing while still occasionally presenting easier tasks to reduce catastrophic forgetting and improve learning stability.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum Learning (linear) + Domain Randomization (Gaussian sampling centered on curriculum level)</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Showed better learning curves than Uniform sampling under linear curricula, but adaptive curricula outperformed linear ones in final transfer performance.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Training sessions used up to 100,000 time steps; GDR gave improved cumulative reward vs UDR for same sample budget.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Gaussian sampling around curriculum level is beneficial compared to uniform DR under a linear curriculum, as it both raises difficulty over time and allows occasional easier episodes to mitigate forgetting.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1032.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1032.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adp-UDR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adaptive Curriculum with Uniform Domain Randomization (Adaptive UDR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adaptive curriculum that increases or decreases difficulty based on recent agent performance, combined with uniform sampling of domain randomized parameters within curriculum-defined subranges.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SAC + TCN force-control agent (Adaptive UDR)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>SAC + TCN agent where curriculum level L_ep is adjusted adaptively using recent success rate P compared to thresholds L_thld_up and L_thld_down; DR ranges set by L_ep and sampled uniformly within those ranges.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent / physical robot (transferred)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Peg-in-hole assembly with adaptive curriculum and uniform DR</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Environment instances are generated from DR parameter subranges defined by the adaptive curriculum; easy tasks when agent performs poorly and harder tasks as performance surpasses thresholds. Complexity adapts to agent competence.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Curriculum level L_ep (adaptively stepped) controls complexity of randomized parameter ranges (distance, stiffness, friction, object shapes).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>dynamically adjusted from low to high based on agent performance</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Domain randomization ranges scaled by L_ep and sampled uniformly per episode (UDR); the effective variation seen by the agent evolves with curriculum.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>adaptive, tends to increase as agent demonstrates competence (low→high)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Cumulative reward, success rate, completion time</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>In simulation, Adaptive UDR achieved strong training performance and in some simulated friction-poor settings solved tasks 20–50% faster than the best method (Adp. Curriculum GDR DyRe), but its success rate on transfer was lower (text: Adp. Curriculum UDR was 20–50% faster in simulation but the final adp. GDR DyRe had at least 19% higher success). Exact numeric success rates not provided in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Adaptive curriculum tailors complexity to agent competence, reducing exploration into fatal states early and increasing variation as competence grows; compared to linear curricula, adaptation improved learning and robustness though uniform sampling can make policies faster but less robust in real, frictional settings.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Adaptive Curriculum Learning + Domain Randomization (uniform sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Adaptive curricula generalized better than linear curricula in simulation; however, Uniform sampling under adaptive curricula sometimes led to faster but less transferable policies (more brittle under real-world friction and contact dynamics) compared to Gaussian-sampled adaptive curricula with dynamic reward.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Trained within the study's 100,000 time-step budget; adaptive curricula showed improved sample efficiency versus baseline and linear curricula.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Adaptive curriculum evolution improves sample efficiency and learning stability by adjusting complexity to agent performance; however, sampling strategy matters—UDR can yield faster but less robust policies compared to GDR with dynamic reward.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1032.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1032.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adp-GDR-DyRe (Best)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adaptive Curriculum with Gaussian Domain Randomization and Dynamic Reward (Adp. Curriculum GDR DyRe)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's proposed best method: an adaptive curriculum that controls domain-randomization ranges, samples parameters from Gaussians centered on curriculum level, and scales the reward dynamically by curriculum difficulty to encourage learning under hardest conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SAC + TCN force-control agent (Adp. Curriculum GDR DyRe)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Soft Actor-Critic agent using a Time Convolutional Network to map proprioceptive and force-torque inputs to 24 continuous action parameters that configure a parallel position-force controller; trained with adaptive curriculum, Gaussian-sampled DR around curriculum level, and a dynamically scaled reward r_dt = r * L_ep.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent; policy transferred to a physical robot (UR3e) without further real-world fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Peg-in-hole assembly (toy training) and transferred to real industrial insertion tasks (motor pulley, shaft, bearing)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Training in Gazebo on toy peg-in-hole shapes with randomized physical parameters (Table I) controlled by curriculum level L_ep; evaluation included novel simulated shapes (trapezoid, star prism) and real-world industrial tasks with sub-millimeter tolerances (±0.01 mm), stiff contacts, friction, and unstable grasps.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Complexity governed by curriculum level L_ep scaling per-parameter DR ranges (distance-to-goal, contact stiffness, friction, object geometry) and by task-specific metrics: insertion tolerance (±0.01 mm), episode horizon (max 1000 steps), collision thresholds (Fmax 50 N sim, 30 N real).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>trained from low to high difficulty adaptively; final evaluation on high-complexity industrial tasks</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Domain Randomization with Gaussian sampling N(µ=L_ep, σ^2) for each parameter within bounds (Table I); curriculum defines subranges [ψ_low, ψ_low + ψ_high * L_ep].</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>adaptive and high at later curriculum stages; designed to provide progressively greater environmental variation while occasionally sampling easier instances to prevent forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate (primary), average completion time (secondary), cumulative reward during training, contact force magnitude (safety/damage proxy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Achieved up to 86% success rate on real-world complex industrial insertion tasks (±0.01 mm tolerances) when policies trained purely in simulation and transferred without real-world retraining; applied ~30% less contact force on average in comparisons and required only one-fifth of previous sample budget (100,000 time steps vs ~500,000). In simulation, Adp. Curriculum UDR could be 20–50% faster but had at least 19% lower success rate than this method.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Explicitly discussed: curriculum reduces immediate complexity (safer, easier instances) to improve exploration and sample efficiency, while DR maintains variation for transfer; adaptive curricula with Gaussian sampling balance increasing complexity and maintaining occasional easier examples to reduce catastrophic forgetting. Trade-offs observed: policies prioritized contact force minimization over speed, yielding slower but higher-success and lower-force behavior; more variation (DR) improves transferability but requires guided ordering (curriculum) to remain sample-efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Real-world high-complexity, high-variation transfer: success up to 86% on industrial insertion tasks (±0.01 mm) after training in simulation with adaptive GDR and dynamic reward.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Adaptive Curriculum Learning + Domain Randomization (Gaussian sampling) + Dynamic reward scaling by curriculum level</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Successfully generalized from toy simulated training to novel simulated peg shapes and to real-world industrial insertion tasks (not seen during training). Achieved high success (up to 86%) and lower contact forces relative to baseline; showed better real-world robustness than linear or uniform-sampled curricula.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Trained successfully within 100,000 time steps (one-fifth the sample budget used in prior work), demonstrating substantial sample efficiency gains via curriculum-guided DR and reward scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining an adaptive curriculum with Gaussian-sampled domain randomization and a curriculum-scaled reward yields large sample-efficiency gains and improved sim2real transfer for contact-rich, high-precision manipulation: policies trained purely in simulation transferred to real industrial tasks with up to 86% success and ~30% lower contact force, while requiring ~1/5 the training samples of prior DR-only approaches. Trade-offs include slower completion times (due to prioritizing low contact force) and the need for prior knowledge to order DR parameter difficulty ranges.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>Curriculum learning <em>(Rating: 2)</em></li>
                <li>Active domain randomization <em>(Rating: 1)</em></li>
                <li>Variable compliance control for robotic peg-in-hole assembly: A deep-reinforcement-learning approach <em>(Rating: 2)</em></li>
                <li>Metareinforcement learning for robotic industrial insertion tasks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1032",
    "paper_id": "paper-248406096",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "No-CL DR Baseline",
            "name_full": "Domain Randomization Only Baseline (No Curriculum)",
            "brief_description": "An embodied SAC agent trained with domain randomization only (uniform sampling) without an explicit curriculum; used as the baseline in experiments to compare curriculum effects on sample efficiency and sim2real transfer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "SAC + TCN force-control agent (baseline)",
            "agent_description": "Soft Actor-Critic (off-policy, max-entropy) policy with a Time Convolutional Network (TCN) mapping proprioception and F/T sensor inputs to a 24‑dim action vector that parameterizes a parallel position-force controller (PID/PI gains, selection matrix, and commanded offsets). Replay buffer with distributed prioritized experience replay was used.",
            "agent_type": "simulated agent / physical robot (transferred)",
            "environment_name": "Peg-in-hole assembly / industrial insertion tasks",
            "environment_description": "Simulated Gazebo peg-in-hole toy training environments (cylinders, hexagonal prism, cuboid, triangular prism) with randomized physical parameters; transferred and evaluated on real-world industrial insertion tasks (motor pulley, shaft, bearing) with sub-millimeter tolerances (±0.01 mm). Complexity arises from contact-rich dynamics, high stiffness position-controlled robot, tight tolerances, friction, and unstable grasps for round objects.",
            "complexity_measure": "Task difficulty characterized by peg shapes (symmetry, corners), insertion tolerance (±0.01 mm for industrial tests), contact stiffness, friction, initial pose uncertainty, and episode horizon (max 1000 steps of 50 ms); domain randomization parameter set size referenced in Table I (Nr parameters).",
            "complexity_level": "medium (toy training) to high (industrial real-world tasks with ±0.01 mm tolerances)",
            "variation_measure": "Domain randomization of simulator numerical parameters (Table I) sampled uniformly per episode (UDR); ranges scaled by curriculum level when used; random seeds and multiple object shapes used for evaluation.",
            "variation_level": "high (full DR ranges used during baseline training in simulation)",
            "performance_metric": "Success rate, average completion time, cumulative reward, contact force profiles",
            "performance_value": "Baseline trained with 100,000 time steps (this study, reduced from previous 500,000) showed lower real-world success rates than the proposed adaptive curriculum method; baseline often faster when successful but with much lower success on industrial tasks (exact baseline success rates not explicitly reported in text); collision thresholds: Fmax = 50 N (sim), 30 N (real).",
            "complexity_variation_relationship": "Baseline (DR-only) demonstrates that high variation alone can improve transferability but can produce suboptimal, high-variance policies; the paper argues that adding a curriculum to guide complexity progression improves sample efficiency and final performance compared to uniform DR alone.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Domain Randomization (uniform sampling) without Curriculum Learning",
            "generalization_tested": true,
            "generalization_results": "Tested on novel simulated peg shapes (trapezoid, star prism) and real-world industrial insertion tasks; baseline transferred but achieved substantially lower success rates on the real industrial tasks vs the best curriculum method and struggled with star-shaped peg alignment in real frictional settings.",
            "sample_efficiency": "Trained in this study with up to 100,000 time steps (one-fifth of the 500,000 steps used in prior work); previous work required ~500,000 time steps plus real-world refinement.",
            "key_findings": "DR-only can enable sim2real transfer but may produce high-variance/suboptimal policies and lower success on complex contact-rich tasks; lacks the sample-efficiency and robustness achieved when combined with curriculum strategies and adaptive sampling.",
            "uuid": "e1032.0"
        },
        {
            "name_short": "Linear-UDR",
            "name_full": "Linear Curriculum with Uniform Domain Randomization (UDR)",
            "brief_description": "A curriculum that linearly increases difficulty over episodes while sampling domain-randomized parameters uniformly within curriculum-determined subranges.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "SAC + TCN force-control agent (Linear UDR)",
            "agent_description": "Same embodied SAC + TCN agent and controller parameterization as baseline; training schedule uses a linearly increasing curriculum level L_ep (L_ep = ep/ep_max) to restrict DR ranges early and expand them over time; DR sampling is uniform within the curriculum-determined subranges.",
            "agent_type": "simulated agent / physical robot (transferred)",
            "environment_name": "Peg-in-hole assembly (toy training with staged difficulty)",
            "environment_description": "Training environments where each domain randomization parameter ψ_i is limited to [ψ_low, ψ_low + ψ_high * L_ep], so early episodes have lower variation and easier instances (e.g., shorter initial distances, lower stiffness), later episodes increase full variation.",
            "complexity_measure": "Complexity is adjusted by curriculum level L_ep (0 to 1) which scales the per-parameter randomized ranges; tasks measured by distance-to-goal, stiffness, friction, and peg geometry. Episode cap 1000 steps.",
            "complexity_level": "progressively from low to high (driven by linear schedule)",
            "variation_measure": "Domain randomization ranges scaled linearly with L_ep; sampling distribution: uniform within the scaled ranges (UDR).",
            "variation_level": "low initially (L_ep near 0) to high at end (L_ep = 1)",
            "performance_metric": "Cumulative reward during training, success rate and average completion time on held-out peg shapes and real tasks",
            "performance_value": "Linear curricula performed only slightly better than no-curriculum in transfer to novel environments (text: linear curricula performed just slightly better than not using a curriculum at all); exact numeric values not given in main text.",
            "complexity_variation_relationship": "Linear scheduling reduced initial complexity but did not sufficiently guide exploration for robust transfer; paper reports that naive linear curricula can appear to aid learning but may not generalize well to novel, harder conditions compared to adaptive curricula.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum Learning (linear) + Domain Randomization (uniform sampling)",
            "generalization_tested": true,
            "generalization_results": "Tested on novel simulated shapes and real-world tasks; linear curricula yielded modest improvements in training curves but poorer sim2real transfer relative to adaptive curricula, indicating weaker generalization to unseen, higher-complexity instances.",
            "sample_efficiency": "Training sessions capped at 100,000 time steps in this study; linear curricula improved cumulative reward somewhat but were outperformed by adaptive strategies.",
            "key_findings": "A simple linear increase of allowed DR ranges can speed apparent learning but may produce policies that do not generalize well to novel, more complex instances; curriculum design matters for transfer.",
            "uuid": "e1032.1"
        },
        {
            "name_short": "Linear-GDR",
            "name_full": "Linear Curriculum with Gaussian Domain Randomization (GDR)",
            "brief_description": "A linearly evolving curriculum where domain-randomization parameters are sampled from Gaussians centered on the curriculum level to allow occasional easier/harder tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "SAC + TCN force-control agent (Linear GDR)",
            "agent_description": "Same SAC + TCN agent; uses linear curriculum level L_ep to define Gaussian means for each randomized parameter (mean centered at L_ep), sampling from N(mu=L_ep, sigma^2) clipped to parameter bounds, providing stochasticity around curriculum difficulty.",
            "agent_type": "simulated agent / physical robot (transferred)",
            "environment_name": "Peg-in-hole assembly with curriculum-scaled Gaussian DR",
            "environment_description": "Training tasks have DR parameter sampling that is Gaussian around the curriculum level, so most episodes match nominal difficulty but occasionally produce easier/harder instances; complexity increases linearly over training.",
            "complexity_measure": "Curriculum level L_ep (0→1) used as Gaussian mean; complexity measured by the same task parameters (distance, stiffness, etc.).",
            "complexity_level": "progressively from low to high (linear schedule)",
            "variation_measure": "Gaussian Domain Randomization (NDR) with mean at L_ep and hyperparameter variance; allows low-probability sampling of easier tasks to mitigate catastrophic forgetting.",
            "variation_level": "moderate-to-high as L_ep increases; stochastic variation around curriculum level",
            "performance_metric": "Cumulative reward, success rate, completion time",
            "performance_value": "Gaussian sampling improved agents' performance during learning compared to Uniform sampling under linear curricula (text: using Gaussian instead of Uniform significantly improves agents' performance during learning); exact numeric values not provided in text.",
            "complexity_variation_relationship": "Using Gaussian sampling around curriculum level provides a trade-off: keeps general difficulty increasing while still occasionally presenting easier tasks to reduce catastrophic forgetting and improve learning stability.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum Learning (linear) + Domain Randomization (Gaussian sampling centered on curriculum level)",
            "generalization_tested": true,
            "generalization_results": "Showed better learning curves than Uniform sampling under linear curricula, but adaptive curricula outperformed linear ones in final transfer performance.",
            "sample_efficiency": "Training sessions used up to 100,000 time steps; GDR gave improved cumulative reward vs UDR for same sample budget.",
            "key_findings": "Gaussian sampling around curriculum level is beneficial compared to uniform DR under a linear curriculum, as it both raises difficulty over time and allows occasional easier episodes to mitigate forgetting.",
            "uuid": "e1032.2"
        },
        {
            "name_short": "Adp-UDR",
            "name_full": "Adaptive Curriculum with Uniform Domain Randomization (Adaptive UDR)",
            "brief_description": "An adaptive curriculum that increases or decreases difficulty based on recent agent performance, combined with uniform sampling of domain randomized parameters within curriculum-defined subranges.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "SAC + TCN force-control agent (Adaptive UDR)",
            "agent_description": "SAC + TCN agent where curriculum level L_ep is adjusted adaptively using recent success rate P compared to thresholds L_thld_up and L_thld_down; DR ranges set by L_ep and sampled uniformly within those ranges.",
            "agent_type": "simulated agent / physical robot (transferred)",
            "environment_name": "Peg-in-hole assembly with adaptive curriculum and uniform DR",
            "environment_description": "Environment instances are generated from DR parameter subranges defined by the adaptive curriculum; easy tasks when agent performs poorly and harder tasks as performance surpasses thresholds. Complexity adapts to agent competence.",
            "complexity_measure": "Curriculum level L_ep (adaptively stepped) controls complexity of randomized parameter ranges (distance, stiffness, friction, object shapes).",
            "complexity_level": "dynamically adjusted from low to high based on agent performance",
            "variation_measure": "Domain randomization ranges scaled by L_ep and sampled uniformly per episode (UDR); the effective variation seen by the agent evolves with curriculum.",
            "variation_level": "adaptive, tends to increase as agent demonstrates competence (low→high)",
            "performance_metric": "Cumulative reward, success rate, completion time",
            "performance_value": "In simulation, Adaptive UDR achieved strong training performance and in some simulated friction-poor settings solved tasks 20–50% faster than the best method (Adp. Curriculum GDR DyRe), but its success rate on transfer was lower (text: Adp. Curriculum UDR was 20–50% faster in simulation but the final adp. GDR DyRe had at least 19% higher success). Exact numeric success rates not provided in main text.",
            "complexity_variation_relationship": "Adaptive curriculum tailors complexity to agent competence, reducing exploration into fatal states early and increasing variation as competence grows; compared to linear curricula, adaptation improved learning and robustness though uniform sampling can make policies faster but less robust in real, frictional settings.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Adaptive Curriculum Learning + Domain Randomization (uniform sampling)",
            "generalization_tested": true,
            "generalization_results": "Adaptive curricula generalized better than linear curricula in simulation; however, Uniform sampling under adaptive curricula sometimes led to faster but less transferable policies (more brittle under real-world friction and contact dynamics) compared to Gaussian-sampled adaptive curricula with dynamic reward.",
            "sample_efficiency": "Trained within the study's 100,000 time-step budget; adaptive curricula showed improved sample efficiency versus baseline and linear curricula.",
            "key_findings": "Adaptive curriculum evolution improves sample efficiency and learning stability by adjusting complexity to agent performance; however, sampling strategy matters—UDR can yield faster but less robust policies compared to GDR with dynamic reward.",
            "uuid": "e1032.3"
        },
        {
            "name_short": "Adp-GDR-DyRe (Best)",
            "name_full": "Adaptive Curriculum with Gaussian Domain Randomization and Dynamic Reward (Adp. Curriculum GDR DyRe)",
            "brief_description": "The paper's proposed best method: an adaptive curriculum that controls domain-randomization ranges, samples parameters from Gaussians centered on curriculum level, and scales the reward dynamically by curriculum difficulty to encourage learning under hardest conditions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "SAC + TCN force-control agent (Adp. Curriculum GDR DyRe)",
            "agent_description": "Soft Actor-Critic agent using a Time Convolutional Network to map proprioceptive and force-torque inputs to 24 continuous action parameters that configure a parallel position-force controller; trained with adaptive curriculum, Gaussian-sampled DR around curriculum level, and a dynamically scaled reward r_dt = r * L_ep.",
            "agent_type": "simulated agent; policy transferred to a physical robot (UR3e) without further real-world fine-tuning",
            "environment_name": "Peg-in-hole assembly (toy training) and transferred to real industrial insertion tasks (motor pulley, shaft, bearing)",
            "environment_description": "Training in Gazebo on toy peg-in-hole shapes with randomized physical parameters (Table I) controlled by curriculum level L_ep; evaluation included novel simulated shapes (trapezoid, star prism) and real-world industrial tasks with sub-millimeter tolerances (±0.01 mm), stiff contacts, friction, and unstable grasps.",
            "complexity_measure": "Complexity governed by curriculum level L_ep scaling per-parameter DR ranges (distance-to-goal, contact stiffness, friction, object geometry) and by task-specific metrics: insertion tolerance (±0.01 mm), episode horizon (max 1000 steps), collision thresholds (Fmax 50 N sim, 30 N real).",
            "complexity_level": "trained from low to high difficulty adaptively; final evaluation on high-complexity industrial tasks",
            "variation_measure": "Domain Randomization with Gaussian sampling N(µ=L_ep, σ^2) for each parameter within bounds (Table I); curriculum defines subranges [ψ_low, ψ_low + ψ_high * L_ep].",
            "variation_level": "adaptive and high at later curriculum stages; designed to provide progressively greater environmental variation while occasionally sampling easier instances to prevent forgetting.",
            "performance_metric": "Success rate (primary), average completion time (secondary), cumulative reward during training, contact force magnitude (safety/damage proxy)",
            "performance_value": "Achieved up to 86% success rate on real-world complex industrial insertion tasks (±0.01 mm tolerances) when policies trained purely in simulation and transferred without real-world retraining; applied ~30% less contact force on average in comparisons and required only one-fifth of previous sample budget (100,000 time steps vs ~500,000). In simulation, Adp. Curriculum UDR could be 20–50% faster but had at least 19% lower success rate than this method.",
            "complexity_variation_relationship": "Explicitly discussed: curriculum reduces immediate complexity (safer, easier instances) to improve exploration and sample efficiency, while DR maintains variation for transfer; adaptive curricula with Gaussian sampling balance increasing complexity and maintaining occasional easier examples to reduce catastrophic forgetting. Trade-offs observed: policies prioritized contact force minimization over speed, yielding slower but higher-success and lower-force behavior; more variation (DR) improves transferability but requires guided ordering (curriculum) to remain sample-efficient.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "Real-world high-complexity, high-variation transfer: success up to 86% on industrial insertion tasks (±0.01 mm) after training in simulation with adaptive GDR and dynamic reward.",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Adaptive Curriculum Learning + Domain Randomization (Gaussian sampling) + Dynamic reward scaling by curriculum level",
            "generalization_tested": true,
            "generalization_results": "Successfully generalized from toy simulated training to novel simulated peg shapes and to real-world industrial insertion tasks (not seen during training). Achieved high success (up to 86%) and lower contact forces relative to baseline; showed better real-world robustness than linear or uniform-sampled curricula.",
            "sample_efficiency": "Trained successfully within 100,000 time steps (one-fifth the sample budget used in prior work), demonstrating substantial sample efficiency gains via curriculum-guided DR and reward scaling.",
            "key_findings": "Combining an adaptive curriculum with Gaussian-sampled domain randomization and a curriculum-scaled reward yields large sample-efficiency gains and improved sim2real transfer for contact-rich, high-precision manipulation: policies trained purely in simulation transferred to real industrial tasks with up to 86% success and ~30% lower contact force, while requiring ~1/5 the training samples of prior DR-only approaches. Trade-offs include slower completion times (due to prioritizing low contact force) and the need for prior knowledge to order DR parameter difficulty ranges.",
            "uuid": "e1032.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2,
            "sanitized_title": "domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world"
        },
        {
            "paper_title": "Curriculum learning",
            "rating": 2,
            "sanitized_title": "curriculum_learning"
        },
        {
            "paper_title": "Active domain randomization",
            "rating": 1,
            "sanitized_title": "active_domain_randomization"
        },
        {
            "paper_title": "Variable compliance control for robotic peg-in-hole assembly: A deep-reinforcement-learning approach",
            "rating": 2,
            "sanitized_title": "variable_compliance_control_for_robotic_peginhole_assembly_a_deepreinforcementlearning_approach"
        },
        {
            "paper_title": "Metareinforcement learning for robotic industrial insertion tasks",
            "rating": 1,
            "sanitized_title": "metareinforcement_learning_for_robotic_industrial_insertion_tasks"
        }
    ],
    "cost": 0.01670075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Accelerating Robot Learning of Contact-Rich Manipulations: A Curriculum Learning Study</p>
<p>Cristian C Beltran-Hernandez 
Damien Petit 
Ixchel G Ramirez-Alpizar 
Kensuke Harada 
Accelerating Robot Learning of Contact-Rich Manipulations: A Curriculum Learning Study
PREPRINT APRIL 2022 1Index Terms-reinforcement learningcurriculum learningdomain randomizationsim2realforce and motion controlrobotic assembly
The Reinforcement Learning (RL) paradigm has been an essential tool for automating robotic tasks. Despite the advances in RL, it is still not widely adopted in the industry due to the need for an expensive large amount of robot interaction with its environment. Curriculum Learning (CL) has been proposed to expedite learning. However, most research works have been only evaluated in simulated environments, from video games to robotic toy tasks. This paper presents a study for accelerating robot learning of contact-rich manipulation tasks based on Curriculum Learning combined with Domain Randomization (DR). We tackle complex industrial assembly tasks with position-controlled robots, such as insertion tasks. We compare different curricula designs and sampling approaches for DR. Based on this study, we propose a method that significantly outperforms previous work, which uses DR only (No CL is used), with less than a fifth of the training time (samples). Results also show that even when training only in simulation with toy tasks, our method can learn policies that can be transferred to the real-world robot. The learned policies achieved success rates of up to 86% on real-world complex industrial insertion tasks (with tolerances of ±0.01 mm) not seen during the training.</p>
<p>I. INTRODUCTION</p>
<p>Reinforcement Learning (RL) has been proven to be successful at learning complex behaviors to solve a variety of robotic contact-rich tasks [1], [2], [3]. However, RL solutions are still not widely adopted in real-world industrial tasks. One reason is that RL still requires an expensive and large amount of robot interaction with its environment to learn a successful policy. The more complex the target task is, the more interaction (samples) is required.</p>
<p>To tackle this problem, domain transfer methods such as Domain Randomization (DR) and Curriculum Learning (CL) have been introduced. The concept of CL, where the learning process can be made more efficient by following a curriculum that defines an order in which tasks should be learned, has been introduced in previous works [4], [5]. Additionally, DR of visual and physical properties of a task has been shown to improve the performance of tasks in novel domains [6], [7]. However, most of these results have been only validated in simulated environments, from video games to robotic toy tasks, or in real-world toy environments. In this work, we tackle the problem of improving sample efficiency and performance when learning real-world complex industrial assembly tasks with rigid position-controlled robots. To this end, we seek to answer the question: Does the order in which the different environments (or tasks) are presented to the agent (through DR) affect the training sample efficiency and performance of the learned policy?</p>
<p>We hypothesize that on top of DR, guiding a RL agent's training with a curriculum (presenting tasks in increasing order of difficulty) towards the desired behavior can increase sample efficiency. The reasoning is that the curriculum helps reduce the overall exploration needed to achieve the desired goal while DR enhances domain transferability. This paper presents a study of the combination of CL with DR. More specifically, we compare different curricula designs and different approaches at sampling values for DR. As a result, we propose a novel method that significantly outperforms our previous work [8]. In [8], only DR is used to improve sim2real transferability without CL. Experimental results in simulation and real-world environments show that our novel method can be trained with only a fifth of the training samples required by our previous method and still successfully learn to solve the target insertion tasks. Furthermore, the learned policies transferred to the real world achieved high success rates (up to 86%) on industrial level insertion tasks, with tolerances of ±0.01 mm, not seen during the training. This work's contributions are as follows:</p>
<p>• A study of the application of Curriculum Learning to a learning framework for rigid robots solving contact-rich manipulation tasks. • A novel learning framework combining curriculum learning with domain randomization to accelerate learning and domain transfer. • An improved reward function to guide the learning of force sensor-based contact-rich manipulation tasks. The reward perceived by the agent is dynamically discounted by the curriculum's level of difficulty. For our target domain, the idea is to encourage the agent to learn to solve the hardest tasks as discussed in Section III-F4. • An empirical study of the different methods considered in this paper was conducted. Novel tasks not seen during training were used to validate the performance of each method, both in simulation and in the real world, including complex industrial insertion tasks. Additionally, we study the impact of different components of our proposed method in the Appendix.</p>
<p>Additionally, this work's source-code 1 will soon be open to the research community. The rest of this paper is organized as follows, related work is discussed in Section II. The case study for this work and the proposed method are explained in Section III. Experimental results and comparisons with alternative methods and our previous work are shown in Section IV. Ablation studies are described in the Appendix.</p>
<p>II. RELATED WORK</p>
<p>The related work to the topic of this paper, accelerating robot learning of contact-rich manipulation tasks, is introduced and discussed in this section.</p>
<p>A. Reinforcement Learning for contact-rich manipulation tasks</p>
<p>Plenty of methods have been proposed to accelerate automation of robotic assembly tasks, such as peg-in-hole tasks. From search strategies to align the peg with the hole [9], [10], to learning-based methods [11], [12]. Some researchers have explicitly focused on the domain transfer of assembly tasks from simulation to real-world environments. In [13], a meta-RL technique is applied to transfer experiences and generalize better to the real world. In [14], system identification of the real robot (KUKA LBR iiwa) with its simulated counterpart is performed to improve sim2real transferability. While RL-based policies have been proposed and proven to have the potential to solve assembly tasks in the real world, there is still a lack of adoption of such methods in real industrial assembly tasks. One reason for this gap between research and industry is the sample efficiency of such learning methods; a large amount of interaction of the agent with its environment is still necessary to learn robust policies. We aim to contribute to this area by proposing a more sample-efficient approach based on CL and DR, i.e., less time is required to train a successful policy without decreasing its transferability.</p>
<p>B. Domain Randomization</p>
<p>In the context of machine learning, DR has been proposed as a technique to improve domain transfer, such as going from one task to a harder one or moving from a simulated environment to a real-world environment, in particular for training vision-based models [6] or sim2real models [7]. In [15] an empirical study is presented to examine the effects of DR on agent generalization. Their results show that DR may lead to suboptimal, high-variance policies, which [15] attributes to the uniform sampling of environment parameters. Following those results, this study proposed the use of DR based on CL and empirically studied the effect of the DR's sampling strategy. 1 At https://github.com/cambel/robot-learning-cl-dr</p>
<p>C. Curriculum Learning</p>
<p>The concept of curriculum learning in the context of machine learning was first proposed by Bengio et al. [4]. As mentioned before, CL can be understood as learning from easier to harder tasks, i.e., the order in which information is presented affects the policy's ability to learn. A comprehensive survey on Curriculum Learning applied to Reinforcement Learning has been presented in [16]. Most CL approaches have been validated mainly on simulated environments, such as toy examples (e.g., grid worlds, cart-pole, and low-dimensional environments), video games, and simulated robotic environments. Few research works have focused on real-world robotic environments, such as in [17] where a robot is trained to shoot a ball into a goal, [18], [19] where reaching tasks with a robot arm are tackled, and [20] which focused on two tasks moving a cube to a target pose and cube stacking. Most recently, [21] presented a CL method focused on a specific automotive production task, trained on simulation, and transferred to its real-world equivalent. Similarly, [22] proposes a method to enable a robot to conduct anchor-bolt insertion, a peg-in-hole task for holes in concrete. On the other hand, our study focuses on tackling various real-world complex industrial assembly insertion tasks, trained only on toy peg-in-hole simulated environments. Furthermore, we make an exhaustive study on the performance of several approaches to combine DR and CL. As a result, we propose a method to accelerate learning and domain transfer to real-world environments by an adaptive curriculum that affects how DR and the reward signal are considered during training.</p>
<p>III. MATERIALS AND METHODS</p>
<p>A. Problem Statement</p>
<p>In the present study, we consider the peg-in-hole assembly task that requires the mating of two components. One of the components is grasped and manipulated by the robot manipulator, while the second component has a fixed position via fixtures to a support surface. The proposed method is designed for position-controlled robot manipulators with access to force/torque information at the robot's end-effector (e.g., F/T sensor at the robot wrist), especially those robots where low-level torque control is not available. Thus, sensor-based force control is necessary to realize contact-rich manipulation tasks for such a type of robot.</p>
<p>B. System Overview</p>
<p>Our propose method aims to improve the sample efficiency of the training phase. Figure 1 shows the overall system architecture which is based on our previous work [8]. There are two control loops. The inner loop has an adaptive compliance controller; we choose to use a parallel position-force controller that was proven to work well for this kind of contact-rich manipulation tasks [23]. The inner loop runs at a control frequency of 500 Hz, which is the maximum available in the Universal Robots e-series robotic arms 2 . The outer loop runs Fig. 1: Overview of the system used for this study. The input is the goal pose, optionally the desired contact force can be defined, otherwise is considered as 0 N. at a lower control frequency to account for the computation time required by the learning algorithm. Our system considers control of the 6 degrees of freedom of the Cartesian space at the robot's end-effector (position and orientation). To this previous learning control framework, we added the PID gains scheduling approach discussed in Section III-D1. Additionally, a new dense reward function is proposed and described in Section III-C1. Similarly, a DR method based on CL is implemented on top of this learning control framework, see Section III-E and Section III-F.</p>
<p>C. Reinforcement Learning</p>
<p>The reinforcement learning framework followed in this work is modelled as an episodic Markov Decision Process (MDP), M , that has finite time steps with a limit of T steps per episode. For a given task, the MDP consists of a state s ∈ S, action space a ∈ A, state transition function p(s(t + 1) | s(t), a(t)), which is the probability of transitioning to state s given that the action a is taken while being in state s, and a reward function r(s, a), which provides an immediate numerical reward for being in state s and taking the action a. The goal of RL is to find a policy π * that maximizes the expected sum of discounted future rewards given by R(t) = T i γr(s(t), a(t)), where γ is the discount factor [1].</p>
<p>We used a soft actor-critic (SAC) approach to learning the policy. SAC [24] is an off-policy actor-critic deep RL algorithm based on maximal entropy. SAC aims to maximize the expected reward while also optimizing maximal entropy. The SAC agent optimizes a maximal-entropy objective, encouraging exploration according to a temperature parameter α. The core idea of this method is to succeed at the task while acting as randomly as possible. Since SAC is an offpolicy algorithm, it uses a replay buffer to reuse information from recent rollouts for sample-efficient training. Additionally, we used the distributed prioritized experience replay approach for further improvement [25]. Our implementation of the SAC algorithm was based on the TF2RL repository 3 .</p>
<p>The agent's policy maps a multi-modal state, the robot's proprioception and the Force-Torque sensor data, to the robot's actions, detailed in III-D, using a Time Convolutional Network (TCN); this policy representation and its improved performance over a simple neural network-policy were introduced in our previous work [8].</p>
<p>1) Reward function: On one hand, in our previous work [8], the reward function is defined only in terms of the contact force and distance between the current position of the robot's endeffector and the target position. The reason is to encourage the agent to get closer to the target position; the faster, the better, while discouraging any contact force. On the other hand, in this work, we propose the inclusion of the velocity of the robot's end-effector in the reward signal. While it is ideal that the agent achieves the task as fast as possible, high speeds are not desirable when the robot is close to the environment or in contact with it, as it can generate large contact forces. Thus, the proposed reward function has the following shape:
r(s, a) = w 1 r xv + w 2 r F + w 3 ρ (1)
where r xv is the component of the reward associated with the position and velocity of the robot's end-effector. r xv aims to encourage the agent to get closer and keep closer to the target position. Besides, the agent is encourage to move faster, if it is far from the target pose, or slower when it is close to the target pose. r xv is defined as:
r xv = (1 − tanh(5|x|)(1 − |ẋ|) + (|ẋ|/2) 2(2)
Where x is the distance between the robot's end-effector and the target position, andẋ is its velocity. A visualization of this reward component is shown in Figure 2. The component of the reward Eq. (1) associated with the contact force is defined as:
r F = −1/(1 + e −15|Fg−Fext|+5 )(3)
where F g is the desired insertion force, F ext is the contact force. The reward is always negative as a discount reward to encourage minimal contact force. However, due to the nature of the task, contact with the environment is unavoidable, so an S shape function is proposed to allow small contact forces while strongly discouraging large ones. A visualization of this reward component is shown in Figure 3 The position, velocity and contact force are normalized by the maximum value allowed for each one. Finally, ρ is defined as follows:
ρ =    500, Task completed −200, Collision −1, Otherwise(4)
The task was considered completed if the Euclidean distance between the robot's end-effector and goal positions was less than 1 mm. The agent is encouraged to complete the task as quickly as possible by discounting the reward for every time step taken. Similar to our previous work [23], we imposed a collision constraint where the agent was penalized for colliding with the environment by giving it a large negative reward and immediately ending the episode. A collision is defined as exceeding the force limit F max . F max is a hyper-parameter that was defined as 50N in simulation or 30N in the real robot. Lastly, each component was weighted via w; all ws are hyperparameters. The performance of our new reward signal approach versus the reward signal proposed in our previous work [23] is shown in Appendix A. </p>
<p>D. Compliance Control in Task Space</p>
<p>The agent's action space is based on our previous work [23], which consists of learning the force control parameters of a traditional sensor-based force controller. More specifically, we learn the parameters of a parallel position-force controller.</p>
<p>The parallel position-force control requires the fine-tuning of three sets of parameters; gains of a PID for position tracking, gains of a PI for force tracking, and a selection matrix that defines the degree of control between force and position. The controller follows the control law shown in Eq. (5)
x c = S(K x p x e + K x dẋe + a x ) + (I − S)(K f p F e + K f i F e dt),(5)
where F e = F g − F ext (goal contact force minus sensed contact force), x e = x g − x (goal pose minus current pose), a x represents an arbitrary translation/rotation given by the agent, and x c is the commanded positions to the robot. The selection matrix is
S = diag(s 1 , ..., s 6 ), s j ∈ [0, 1]
The parameters to be learned are K x p , K f p , S, and a x . One for each of the 6 Cartesian degrees of freedom. The remaining parameters K x d and K f i were defined proportionally to the K x p and K f p respectively. Therefore, the action space consists of 24 parameters. Each parameter is bounded to a continuous range of valid values. More details are provided in [8].</p>
<p>1) PID Gains Scheduling: An additional concept is explored in this paper, PID gains scheduling [26]. Parallel force control for sub-millimeter tolerance insertion tasks tends to get stuck in the region very close to the alignment of the peg onto the hole. In the presence of very small position errors, the PID position controller barely generates any signal. On the other hand, the PI controller overcome the position PID controller due to small contact forces or noise coming from the F/T sensor. Therefore, on insertion tasks with sub-millimeter tolerances, the force controller does not move towards the target pose due to small resistance from the contact with the environment. To address this issue, we introduce PID gains scheduling, where once x e has been reduced to a position error of less than 1 cm, the PID gains are then scaled up based on the position error K x p = K x p * (1/x e ). As x e approaches zero, the value of K x p has a hyperbolic growth, thus the value of K x p is bounded to be maximum kn = 10 times its current value (the agent's chosen value). Appendix B provides a comparison between the use of a traditional PID versus the PID gains scheduling on our proposed method.</p>
<p>E. Domain Randomization</p>
<p>Domain randomization (DR) [6] is a popular method in robot learning to increase the generalization capabilities of policies trained in simulation, facilitating the transfer of the policy to a real-world robotic agent with minimal to no further refinement of the policy. In principle, the goal of DR is to provide enough variability to the simulated environment during training to generalize better to real-world conditions. In robot learning, DR randomizes a set of numerical parameters, N r , of a physics simulator. With each parameter ψ i being sample from a randomization space Ψ ∈ R Nr . Each parameter is bounded
on a close interval {[ψ low i , ψ high i ]} Nr i=1 .
For every episode, a new set of parameters is sample from the randomization space ψ i ∈ Ψ. The most common approach is to draw sample uniformly from the randomization space. In this work, the randomized aspects of the peg-in-hole tasks are defined in Table I.</p>
<p>F. Curriculum Learning</p>
<p>Curriculum Learning comes from the notion that the order in which information is organized and presented to a learner impacts the learner's performance and training speed. This idea can be observed in the way humans learn, starting with simple concepts and gradually progressing to more complicated problems [27], [5]. CL can also be observed in the way we train animals [28].</p>
<p>In this work, we follow the notion that starting with easier tasks can help the agent learn better when presented with more difficult tasks later on. We consider the CL problem in the context of DR, where the goal is to reduce the training time by guiding the learning process without loosing domain transferability. Then, the problem becomes how to select parameters from the randomized space Ψ to guide the agent's training. To this end, we consider four main approaches:</p>
<p>• Curriculum-based DR: The DR parameter's range of values is determined by the curriculum. • The curriculum's evolution: a linear approach vs an adaptive approach. • The DR sampling strategy: a Uniform distribution (UDR) vs a Gaussian distribution (GDR). • A dynamic reward function based on the curriculum vs a standard reward function. 1) Curriculum-based Domain Randomization: We tackle the problem of defining a strategy to reduce the complexity of choosing a value for each randomization parameter. Though each parameter of the randomized space ψ can be considered a degree of freedom that can be controlled to define the training tasks, adding new parameters would increases the difficulty of choosing the sequence of tasks to train the agent. Therefore, in order to simplify the problem while preserving the benefits of domain randomization, we propose the following approach: we represent the difficulty level L of a task as a numerical value in a close interval [0, 1], from easiest to hardest. Then, a sub-set of each randomization parameter ψ i is defined based on the difficulty level L ep at the beginning of each episode during training:
ψ i : <a href="6">ψ low i , ψ low i + ψ high i * L ep </a>
where we assume that the parameter's set ψ i is defined in ascending order, such that, at low and at high, the task is relatively the easiest and the hardest, respectively. The parameters considered in this work and their corresponding set are shown in Table I.</p>
<p>2) Adaptive Curriculum Learning: We consider two approaches to update the curriculum difficulty; on the one hand, the naive approach is to monotonically increase the difficulty in a linear way, regardless of the agent's performance, i.e.,
L ep = ep/ep max(7)
with L ep being a constraint equal to 1 if the current episode number exceeds a defined maximum number of episodes. On the other hand, we propose an adaptive curriculum based on the agent's performance P during the last few episodes. The agent's performance is computed as the success rate of the last few episodes. Based on the agent's performance, the curriculum's level is updated by a defined step size L step . Two thresholds are also defined. If the agent's performances surpass L thld up or fall below L thld down such thresholds, then the curriculum's level is increased or decreased respectively. Algorithm 1 describe our adaptive curriculum approach.</p>
<p>Algorithm 1 Adaptive Curriculum Learning Evolution
P = 0 for Every episode ep do
Update ψ based on L ep Eq. (6) Sample task from ψ // +1: success, -1: failure P += rollout current policy π on task Update policy if P ≥ L thld up then L ep += L step P = 0 Consider newest rollouts else if P ≤ L thld down then L ep −= L step P = 0 Consider newest rollouts</p>
<p>3) Domain Randomization Sampling Strategy:</p>
<p>We consider the type of distribution from which the randomized parameters are sampled. Instead of the typical uniform distribution (UDR), we propose the use of a normal distribution (NDR), N (µ, σ 2 ), with the mean being centered around the current curriculum's level L ep , and variance is a hyperparameter. The reason behind this choice is to keep increasing the general difficulty of the task with the increment of the difficulty level, but with a small probability, the curriculum can generate an easier task than the difficulty level to reduce the catastrophic forgetting problem [29], [30]. 4) Dynamic Reward Function: Lastly, for our target task domain, we consider desirable for the RL agent to learn to handle the hardest conditions to improve transferability to the real-world environment. To this end, we propose and evaluate a dynamically evolving reward with respect to the curriculum level difficulty. More specifically, the reward r, as defined in Section III-C1, is scaled by the current difficulty level L ep ; thus, the full reward would be obtained only when the agent reaches and maintains the hardest level.
r d t = r * L ep(8)
where r d t stands for the dynamic reward at time t. In other words, at each time step, the reward obtained by the agent is a fraction of the full possible reward for reaching such state.</p>
<p>IV. EXPERIMENTS AND RESULTS</p>
<p>Through the following experiments, we aimed to understand the performance of our proposed method compared  to alternative approaches, in terms of sample efficiency and generalization. To that end, experiments were performed with novel tasks not seen during training in simulation and in the real-world environment, using insertion tasks with medium grade industrial-level tolerances (±0.01 mm).</p>
<p>The baseline used throughout these experiments was based on our previous work [8], which mainly focused on the use of DR to enhance domain transferability. This experimental section focus on comparing the different curricula designs, sampling strategies for DR, and curriculum-based dynamic reward. As such, our previous work [8] has been updated to include the new reward function and PID gain scheduling approach, proposed and described in Section III-C1 and III-D1 respectively, which is used as the baseline in this study. Ablation studies of these components of our proposed method are discussed in the Appendix.</p>
<p>A. Experimental Setup</p>
<p>A simulated environment was used both for training and validation. The Gazebo simulator [31] version 9 was used. The choice of simulation environment is discussed in Section V. Two real-world environments were used for validation purpose only; no further re-training was performed on the target domains 4 . The components of the real-world setup is described in Figure 4. Both environments consist of a Universal Robot 3 e-series robot arm with a control frequency up to 500 Hz. The robotic arm had a force/torque sensor mounted at its endeffector. In the simulation, the peg was considered as part of the robot's end-effector, as shown in Figure 5. The realworld robot simply used a Robotiq Hand-e parallel gripper. For the toy environments described in Section IV-E1, this parallel gripper and a cuboid holder facilitate achieving a strong and stable grasp, similar to the simulation environment. However, for the industrial insertion tasks, we avoided the used of custom-made holders for the real-wold tasks, which increased the difficulty of the tasks as discussed in Section IV-E2.</p>
<p>Our implementation of the RL agent that controlled both the simulated and real robot was developed on top of the Robot Operating System (ROS) [32] with the Universal Robot ROS 4 Despite Gazebo's simulation of the high-stiffness robot being accurate, the robot controllers respond faster than the real robot (maybe due to safety speed reduction on the side of the real robot controller, which we have not modify). Therefore, a minimal calibration is required. From our experience, scaling the reference trajectory or the command send to the controller by a factor of two worked well enough. A rough similarity between the simulation and real robot controller is enough to enable straightforward sim2real transfer. Fig. 4: Real experiment environment with a 6-degree-offreedom UR3e robotic arm. WRS2020 Task board is shown, along side the three insertion tasks used for validation, motor pulley, bearing, and shaft. Each task has industrial level submm tolerances. Driver 5 . In both environments, training of the RL agent was performed on a computer with an Intel i9-10900X CPU and NVIDIA® Quadro RTX™ 8000 GPU. See the accompanying video 6</p>
<p>B. Training</p>
<p>The training phase consisted of the repeated execution of the insertion task using a variety of peg shapes and physical parameters of the simulator, as described in Table I. An episode was defined as a maximum of 1000 time steps, with each step being 50 ms. Early termination of an episode occurs under three conditions; 1) the target goal is reached, the peg inserted, and within distance from the full insertion, as described in Table I. 2) the robot collides with the task board, i.e., a large contact force is sensed at any point during the task (more than 50 N in simulation or more than 30 N with the real-world robot). 3) the agent gets stuck, thus, the cumulative reward decreases to less than a set value R min . 5 </p>
<p>C. Learning performance</p>
<p>First, we compare the learning performance of the approaches presented in Section III-F1, III-F2, and III-F3:</p>
<p>• Baseline: DR without curriculum learning (No Curriculum), as described in Section IV. • Linear curriculum with Uniform distribution for DR (Linear Curriculum UDR). • Linear curriculum with Gaussian distribution for DR (Linear Curriculum GDR). • Adaptive curriculum with Uniform distribution for DR (Adp. Curriculum UDR). • Adaptive curriculum with Gaussian distribution for DR (Adp. Curriculum GDR). Each training session had a maximum of 100, 000 time steps, one-fifth of the training time required in our previous work [8]. As described in Section III-E, each episode is generated with a different set of values for the randomization parameters. Figure 6 shows the cumulative reward per method during a complete training session. Each training session was repeated with different random seeds. The average value and standard deviation are shown as the bold line and shadow region, respectively. The results are a preliminary highlight of the significant improvement of applying Curriculum Learning compared to the baseline, which relied primarily on Domain Randomization alone. Furthermore, the adaptive curricula had a considerable performance above a simple linear increment of the curricula difficulty. Finally, using a Gaussian distribution instead of a Uniform distribution for the sampling of Domain Randomization parameters also significantly improves the agents' performance during learning. The dynamic reward (DyRe) approach discussed in Section III-F4 is not included here as the scale of the reward is different.</p>
<p>D. Evaluating learned Policies</p>
<p>Next, we evaluated the performance of the learned policies on novel conditions not seen during training. Each policy was executed 100 times with different initial conditions and randomized parameters (with a fixed random seed for a fair comparison). More specifically, the peg shapes used for testing were a trapezoid prism and star prism, as shown in Figure 5. The trapezoid introduces a non-symmetric-shaped peg. The star prism peg is more challenging due to its sharp corners that make the peg prone to getting stuck during the aligning phase, making the overall insertion task harder to complete The results are shown in Table II. They include a comparison of the overall success rate and the average time needed to complete the task; failure cases are not included in the computation of the average time. Two main conclusions can be drawn from these results; 1) A curriculum may seem to have a better learning performance, but the resulting policy may not transfer well to novel environments, as is the case with the Linear Curricula methods. Such linear curriculum approaches performed just slightly better than not using a curriculum at all.</p>
<p>2) The most successful methods are not necessarily the fastest. Our simulation environment did not handle very well friction between the peg and the task board, due to the high stiffness of the robot joints. In this almost friction-less world, the Adp. Curriculum UDR method is able to solve the tasks between 20% to 50% faster than our best method Adp. Curriculum GDR DyRe. However, the success rate of our method is at least 19% higher. The main reason for such results is that since contact force and collision avoidance have higher priority than speed during learning, our proposed method moves slower when the peg gets closer to the task board so the contact force is reduced. This conclusion is further supported by the real-world experimental data described next in Section IV-E3.  </p>
<p>E. Real-world experiments</p>
<p>We performed two sets of experiments to evaluate the transferability of the learned policies to the real world and to novel tasks. The experiments were performed using the baseline (No Curriculum) method and our newly proposed method (Adp. Curriculum DyRe GDR), which achieved the best results from the evaluation in simulation. The first set of tasks consisted of the same trapezoid and star prismshaped pegs as the simulation experiments, which were not presented during training. A simplified peg was 3D printed using PLA material, as shown in Figure 7a. The second set of tasks consisted of novel industrial level insertion tasks (See Figure 7b), similarly, these tasks were unseen during training in simulation. Both sets of tasks had sub-millimeter tolerances. Thirty trials were performed per method and task. The success rate and average completion time were measured where the initial position and orientation of the robot's end-effector at each trial were different and randomly sampled from a fixed random seed to fairly compare both methods. Each trial had a 500-time steps limit, i.e., 25 s. 1) Primitive Shaped Pegs: The 3D printed pegs were designed with a cuboid holder, as shown in Figure 7a, to increase the stability of the grasp. As a result, the stiffness of the contact is very high, as the task board was also firmly fixed to the workspace. Additionally, there is high friction due to the PLA material used for 3D printing and the imperfections on the printed surface. The high stiffness and friction made the task challenging. The results are shown in Table III. As mentioned before, for this test, the baseline was also trained with only one-fifth of the samples shown to be needed [8] to learn a successful policy. Thus, the learned policy's less refined force control tends to apply too much force to complete the task quickly, but the high friction and the corners of the star-shaped hole cause the peg to get stuck easily. Therefore, the baseline method struggled to align the star prism peg and to get unstuck. On the other hand, our newly proposed approach successfully adapted to the real-world environment and succeeded at the novel tasks without further re-training the policies, just a straightforward sim-to-real transfer.</p>
<p>2) Industrial Level Insertion Tasks: The second set of tasks used for evaluation consisted of 3 insertion tasks with industrial level tolerances. These were chosen from the assembly task  used in the Industrial Robots Assembly Challenge of the World Robot Summit 2020 edition [33]. The tasks, as shown in Figure 7b, were the insertion of a pulley into a motor shaft, a shaft into a bearing, and a bearing into a plate. Similar to the previous tasks, the learned policies were directly transferred from the simulation environment without further training. These tasks are considerably more challenging as the grasp's stability significantly impacts the success. All three manipulated objects are round and grasped directly with a standard parallel gripper. Thus, torques applied along the direction of the grasp could easily change the object's orientation in the gripper. Small orientation changes significantly affect these very tight insertion tasks.</p>
<p>The results are shown in Table IV. Our newly proposed method (Adp. Curriculum DyRe GDR) achieved a high success rate in all the tasks. For the motor pulley and the shaft tasks, our method also solves the task faster by finding the right fit faster. Our method is less likely to get stuck as it applies less contact force as shown in Figure 8 and 9 In the case of the bearing task, the baseline method, when successful, is slightly faster as it tends to apply higher contact force and move faster once the parts are aligned. However, the same high contact force makes it harder to find the proper alignment, thus resulting in a very low success rate. As a result, our newly proposed method outperforms the baseline method, achieving a much higher success rate. These results are better appreciated in the supplemented video 7 .</p>
<p>3) Learning Force Control: In addition to the success rate and time to completion, we compare the detailed performance of the two methods. Figure 8 shows the performance of both methods side by side for the three industrial insertion tasks. For simplicity, only the z-axis (i.e., the insertion direction), distance error (mm), and contact force (N) are displayed. As shown in Figure 8, our proposed method is more time-efficient and applies less contact force to the coupling part. Less contact force is desirable to avoid damage to either the assembly part or the robot. Similarly, Figure 9 shows the comparison of trials where both agents fail to complete the task on time. Though both agents failed, our method again shows a reduced exertion of contact force. In both cases, our method applies about 30% less contact force.</p>
<p>V. DISCUSSION</p>
<p>Training a reinforcement learning agent with a curriculum that starts from easier tasks with reduced risk of encountering (a) Trapezoid and star prism pegs (b) Motor Pulley, Shaft, and Bearing. Fig. 7: Real-world experimental scenarios. Left: 3D printed primitive shape-pegs, different from the ones used for training in simulation. Right: Industrial level insertion tasks from the WRS2020 Robotics Assembly Challenge [33].   fatal states (e.g., a collision during a manipulation task) improves the learning sample efficiency and overall performance. In this case study, in particular, we integrate CL to contactrich peg insertion tasks. A task is defined by various physical parameters as described in Table I. We aim to allow the agent to carefully explore more states by presenting tasks in increas-ing order of difficulty, e.g., by reducing the stiffness of the contact between the peg and the board, the agent is less likely to apply excessive contact force to the environment (i.e., a collision). At the same time, starting with easier tasks, such as a shorter distance from the initial position to goal one, reduces the overall exploration. The curriculum is design around a domain randomization approach to preserve and enhance the domain transferability benefits from DR. Nevertheless, the type of curricula is very relevant for achieving better performance, both in terms of sample efficiency and success rate, as seen in the results in Sect. IV-E1. From our previous work [8], we demonstrated that with sufficient domain randomization-based training in simulation (at least 500, 000 time steps or about 8 hours) and further retraining in the real-robot environment, it is possible to learn policies that adapt to novel domains successfully. The present paper introduce a study on Curriculum Learning to tackle the problem of sample-efficiency, and the need to retrain in the target domain. The experimental results on the realrobot environment confirms that our newly proposed method is effective in learning contact-rich force control tasks. Despite that our proposed method was trained only in simulation with one-fifth of the training time used in our previous work [8] and without further retraining, it achieves a high success rate on novel tasks, including challenging industrial insertion tasks with sub-millimeter tolerances.</p>
<p>The main limitation of our proposed method is the assumption that the domain randomization parameter ranges are organized in order from easy to difficult. Prior knowledge is required to determine the difficulty of a physical parameters on a given task. Such prior knowledge is task-specific and not necessarily easy to obtain or even impossible to determine manually. For example, considering only the peg-in-hole task and the stiffness between the peg and the task board, a lowstiffness can intuitively seem easier to handle for a highstiffness robot arm, as less careful force control is required to achieve the task without generating large contact forces. However, depending on the material and how low the stiffness is, the peg may get stuck, or the task board may be deformed to such an extent that the task becomes impossible to solve. To tackle this problem, an interesting future avenue is to add a layer of learning, following works such as [15], [34]. The main idea of this line of research work is to train a neural network to define the RL agent's tasks. In other words, the network learns to choose the best values for the randomization parameters at each episode to increase the performance of the learned policy. The approaches differ in how the new network is trained, and how the agent's performance is defined, such as the cumulative reward, success rate, or another evaluation metric. Following these self-learned curricula approaches reduces the burden on prior knowledge, though as discussed in [15], a self-learned curriculum can provide an insight into incompatibilities between the task and randomization ranges. Therefore, such approaches may allow the use of many other parameters of the physics simulator for domain randomization, potentially increasing the transferability to novel domains. Nevertheless, the possible downside is the requirement of longer training sessions due to the added complexity.</p>
<p>Another future avenue to further improve the presented work is the choice of a simulation environment. At the time of writing this paper, there are various physics engine simulators available that simulate the contact dynamics between bodies with different degrees of accuracy, among other capabilities.</p>
<p>Our choice of the Gazebo simulator was motivated by its realistic simulation of rigid position-controlled robots. Additionally, the availability of ROS controllers that worked the same in the simulated and the real-world robot reduces the implementation burden and facilitates sim2real transfer. Nonetheless, other simulators provide better contact dynamics and are better adapted for Reinforcement Learning applications, such as Mujoco [35], or Nvidia Isaac Sim [36]. Working with such simulators would be a significant improvement, as Domain Randomization is also easier to implement for vision-based learning methods and physical parameters of the simulated environment.</p>
<p>VI. CONCLUSIONS This paper has studied the application of different approaches that combine Curriculum Learning with Domain Randomization to learn contact-rich manipulation tasks, particularly assembly tasks such as peg insertion. Based on such a study, we proposed to improve sample efficiency and generalization by training an agent purely in simulation, with the training being guided with CL and enhanced with DR. Additionally, this work introduced two enhancements to our learning framework, a new dense reward function and a PID gain scheduling approach, described in Section III-C1 and III-D1 respectively, and validated in the Appendix.</p>
<p>The learning framework proposed in this work is based on our previous work [8] where a combination of sim2real with DR was proposed. Our previous methods still required a considerably large amount of agent's interaction with its environment and additional refinement in the real-world environment to learn a robust policy. On the contrary, our novel approach can be trained purely in simulation with only toy insertion tasks. Empirical results showed that with our proposed method a successful policy can be learned using only one-fifth of the samples needed in our previous work. Such policies can be straightforwardly transferred to real-world environments and still achieve a high success rate, up to 86%, on novel complex industrial insertion tasks not seen during training.</p>
<p>APPENDIX</p>
<p>In this section we compare the performance of the proposed method where the improvements presented in this work; the new dense reward function (Sect. III-C1), and the addition of a PID gains scheduling to the force controller (Sect. III-D). Similar to the experimental setup described in Section IV-D, each method was evaluate on simulation by executing their corresponding learned policy over a 100 trials for each task.</p>
<p>A. Reward Functions</p>
<p>The newly proposed dense reward function includes the robot's end-effector velocity combined with the error position. Our aim is to encourage the agent to move faster while being far from the target pose, but to move slower when closer to the target position to reduce the risk of high contact forces. For a fair comparison, the implementation of the Old reward function method and our proposed New reward function were identical except for the type of reward function. Both methods are based on our proposed method (Adp. Curriculum GDR DyRe), as described in Section III-F. Figure 10 shows the comparison of the training session, and the overall cumulative reward for each method. In addition, both approaches were tested on two novel tasks on simulation, the same tasks described in Section IV-D. The results, displayed in Table V, shows a significant improvement in performance. Our approach using the New reward achieved a higher success rate. Additionally, our approach also was more efficient at solving the tasks. In average, the task are solved at least twice as fast.   </p>
<p>B. Force Controller Position PID types</p>
<p>Furthermore, we updated the PID position controller of our force controller to enhance the performance of the agent when the position error is very small. Similarly, both method were identical except for the implementation of the PID position controller and based on our proposed method (Adp. Curriculum GDR DyRe), as described in Section III-F. The results of evaluating each method on novel tasks not seen during training are shown in Figure 11, for the learning curve, and in Table VI. The results show a considerable improvement of performance when using our proposed PID gain scheduling approach. The success rate achieved by our PID gain scheduling approach is about twice the achieved with the Normal PID approach. On top of that, our PID gain scheduling approach can solve the tasks in less than half the time required by the Normal PID approach.   </p>
<p>Fig. 2 :Fig. 3 :
23Visualization of the position-velocity-based component of the reward function. r xv in Equation (2) Visualization of the contact-force-based component of the reward function. r F in Equation (3)</p>
<p>Fig. 5 :
5Simulated peg-in-hole environments. The cylinder, hexagonal prism, cuboid and triangular prism were used during training. The trapezoid prism and the star prism were used for testing.</p>
<p>Fig. 6 :
6Learning curve comparison using the cumulative reward of the overall training session. Each method was trained three times. The results are aggregated as the average cumulative reward and corresponding standard deviation, represented by the bold line and the shadow region. during the allowed time limit of 50 seconds (same time limit as during training).</p>
<p>Fig. 8 :Fig. 9 :
89Agents performance on WRS2020 insertion tasks. For clarity, only the z-axis (Insertion direction) distance error and contact force are displayed. Comparison was made for each task when both methods successfully completed the task. Performance of both methods where both failed to complete the task within the time limit.</p>
<p>Fig. 10 :
10Learning curve comparison.</p>
<p>Fig. 11 :
11Learning curve comparison.</p>
<p>TABLE I :
IDomain Randomization parameters and their maximum range of values</p>
<p>ROS driver for Universal Robot robotic arms developed in collaboration between Universal Robots and the FZI Research Center for Information Technology https://github.com/UniversalRobots/Universal Robots ROS Driver6 Graphical 
abstract 
and 
experimental 
results: 
https://youtu.be/ FVQC5OcGjs </p>
<p>TABLE II :
IIEvaluation of learned policies on novel conditions.</p>
<p>TABLE III :
IIIEvaluating learned policies on the real-world 
environment, using 2 toy scenarios not seen during training 
on simulation. </p>
<p>TABLE IV :
IVEvaluating learned policies on the real-world environment, using 2 toy scenarios not seen during training on 
simulation. </p>
<p>TABLE V :
VSuccess rate on novel tasks on the simulated environment.</p>
<p>TABLE VI :
VISuccess rate on novel tasks on the simulated environment.
Robot details at https://www.universal-robots.com/e-series/
TF2RL: Deep-reinforcement-learning library using TensorFlow 2.0. https://github.com/keiohta/tf2rl
Supplemental video: https://youtu.be/ FVQC5OcGjs</p>
<p>Reinforcement learning: An introduction. R S Sutton, A G Barto, MIT pressR. S. Sutton and A. G. Barto, Reinforcement learning: An introduction. MIT press, 2018.</p>
<p>Reinforcement learning in robotics: A survey. J Kober, J A Bagnell, J Peters, The International Journal of Robotics Research. 3211J. Kober, J. A. Bagnell, and J. Peters, "Reinforcement learning in robotics: A survey," The International Journal of Robotics Research, vol. 32, no. 11, pp. 1238-1274, 2013.</p>
<p>Deep reinforcement learning: a survey. H Wang, N Liu, Y Zhang, D Feng, F Huang, D Li, Y.-M Zhang, Frontiers of Information Technology &amp; Electronic Engineering. 2112H.-n. Wang, N. Liu, Y.-y. Zhang, D.-w. Feng, F. Huang, D.-s. Li, and Y.-m. Zhang, "Deep reinforcement learning: a survey," Frontiers of Information Technology &amp; Electronic Engineering, vol. 21, no. 12, pp. 1726-1744, 2020.</p>
<p>Curriculum learning. Y Bengio, J Louradour, R Collobert, J Weston, Proceedings of the 26th annual international conference on machine learning. the 26th annual international conference on machine learningY. Bengio, J. Louradour, R. Collobert, and J. Weston, "Curriculum learning," in Proceedings of the 26th annual international conference on machine learning, pp. 41-48, 2009.</p>
<p>Flexible shaping: How learning in small steps helps. K A Krueger, P Dayan, Cognition. 1103K. A. Krueger and P. Dayan, "Flexible shaping: How learning in small steps helps," Cognition, vol. 110, no. 3, pp. 380-394, 2009.</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel, "Domain randomization for transferring deep neural networks from sim- ulation to the real world," in 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 23-30, 2017.</p>
<p>Learning dexterous in-hand manipulation. O M Andrychowicz, B Baker, M Chociej, R Jozefowicz, B Mcgrew, J Pachocki, A Petron, M Plappert, G Powell, A Ray, The International Journal of Robotics Research. 391O. M. Andrychowicz, B. Baker, M. Chociej, R. Jozefowicz, B. McGrew, J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray, et al., "Learning dexterous in-hand manipulation," The International Journal of Robotics Research, vol. 39, no. 1, pp. 3-20, 2020.</p>
<p>Variable compliance control for robotic peg-in-hole assembly: A deepreinforcement-learning approach. C C Beltran-Hernandez, D Petit, I G Ramirez-Alpizar, K Harada, Applied Sciences. 10196923C. C. Beltran-Hernandez, D. Petit, I. G. Ramirez-Alpizar, and K. Harada, "Variable compliance control for robotic peg-in-hole assembly: A deep- reinforcement-learning approach," Applied Sciences, vol. 10, no. 19, p. 6923, 2020.</p>
<p>Intuitive pegin-hole assembly strategy with a compliant manipulator. H Park, J.-H Bae, J.-H Park, M.-H Baeg, J Park, IEEE ISR 2013. IEEEH. Park, J.-H. Bae, J.-H. Park, M.-H. Baeg, and J. Park, "Intuitive peg- in-hole assembly strategy with a compliant manipulator," in IEEE ISR 2013, pp. 1-5, IEEE, 2013.</p>
<p>Search strategies for peg-in-hole assemblies with position uncertainty. S R Chhatpar, M S Branicky, Proceedings 2001 IEEE/RSJ International Conference on Intelligent Robots and Systems. Expanding the Societal Role of Robotics in the the Next Millennium (Cat. No. 01CH37180). 2001 IEEE/RSJ International Conference on Intelligent Robots and Systems. Expanding the Societal Role of Robotics in the the Next Millennium (Cat. No. 01CH37180)IEEE3S. R. Chhatpar and M. S. Branicky, "Search strategies for peg-in-hole assemblies with position uncertainty," in Proceedings 2001 IEEE/RSJ International Conference on Intelligent Robots and Systems. Expanding the Societal Role of Robotics in the the Next Millennium (Cat. No. 01CH37180), vol. 3, pp. 1465-1470, IEEE, 2001.</p>
<p>A learning approach to robot-agnostic force-guided high precision assembly. J Luo, H Li, 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2021J. Luo and H. Li, "A learning approach to robot-agnostic force-guided high precision assembly," in 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 2151-2157, IEEE, 2021.</p>
<p>A peg-in-hole task strategy for holes in concrete. A Y Yasutomi, H Mori, T Ogata, 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE2021A. Y. Yasutomi, H. Mori, and T. Ogata, "A peg-in-hole task strategy for holes in concrete," in 2021 IEEE International Conference on Robotics and Automation (ICRA), pp. 2205-2211, IEEE, 2021.</p>
<p>Metareinforcement learning for robotic industrial insertion tasks. G Schoettler, A Nair, J A Ojea, S Levine, E Solowjow, 2020G. Schoettler, A. Nair, J. A. Ojea, S. Levine, and E. Solowjow, "Meta- reinforcement learning for robotic industrial insertion tasks," in 2020</p>
<p>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEEIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 9728-9735, IEEE, 2020.</p>
<p>Sim2real transfer for reinforcement learning without dynamics randomization. M Kaspar, J D M Osorio, J Bock, 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEEM. Kaspar, J. D. M. Osorio, and J. Bock, "Sim2real transfer for rein- forcement learning without dynamics randomization," in 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 4383-4388, IEEE, 2020.</p>
<p>Active domain randomization. B Mehta, M Diaz, F Golemo, C J Pal, L Paull, PMLRConference on Robot Learning. B. Mehta, M. Diaz, F. Golemo, C. J. Pal, and L. Paull, "Active domain randomization," in Conference on Robot Learning, pp. 1162-1176, PMLR, 2020.</p>
<p>Curriculum learning for reinforcement learning domains: A framework and survey. S Narvekar, B Peng, M Leonetti, J Sinapov, M E Taylor, P Stone, Journal of Machine Learning Research. 21S. Narvekar, B. Peng, M. Leonetti, J. Sinapov, M. E. Taylor, and P. Stone, "Curriculum learning for reinforcement learning domains: A framework and survey," Journal of Machine Learning Research, vol. 21, pp. 1-50, 2020.</p>
<p>Purposive behavior acquisition for a real robot by vision-based reinforcement learning. M Asada, S Noda, S Tawaratsumida, K Hosoda, Machine learning. 232M. Asada, S. Noda, S. Tawaratsumida, and K. Hosoda, "Purposive behavior acquisition for a real robot by vision-based reinforcement learning," Machine learning, vol. 23, no. 2, pp. 279-303, 1996.</p>
<p>Active learning of inverse models with intrinsically motivated goal exploration in robots. A Baranes, P.-Y Oudeyer, Robotics and Autonomous Systems. 611A. Baranes and P.-Y. Oudeyer, "Active learning of inverse models with intrinsically motivated goal exploration in robots," Robotics and Autonomous Systems, vol. 61, no. 1, pp. 49-73, 2013.</p>
<p>Accelerating reinforcement learning for reaching using continuous curriculum learning. S Luo, H Kasaei, L Schomaker, 2020 International Joint Conference on Neural Networks (IJCNN). IEEES. Luo, H. Kasaei, and L. Schomaker, "Accelerating reinforcement learning for reaching using continuous curriculum learning," in 2020 International Joint Conference on Neural Networks (IJCNN), pp. 1-8, IEEE, 2020.</p>
<p>Learning by playing solving sparse reward tasks from scratch. M Riedmiller, R Hafner, T Lampe, M Neunert, J Degrave, T Wiele, V Mnih, N Heess, J T Springenberg, International conference on machine learning. PMLRM. Riedmiller, R. Hafner, T. Lampe, M. Neunert, J. Degrave, T. Wiele, V. Mnih, N. Heess, and J. T. Springenberg, "Learning by playing solving sparse reward tasks from scratch," in International conference on machine learning, pp. 4344-4353, PMLR, 2018.</p>
<p>Deep reinforcement learning for robotic control in highdexterity assembly tasks-a reward curriculum approach. L Leyendecker, M Schmitz, H A Zhou, V Samsonov, M Rittstieg, D Lütticke, 2021 Fifth IEEE International Conference on Robotic Computing (IRC). IEEE2021L. Leyendecker, M. Schmitz, H. A. Zhou, V. Samsonov, M. Rittstieg, and D. Lütticke, "Deep reinforcement learning for robotic control in high- dexterity assembly tasks-a reward curriculum approach," in 2021 Fifth IEEE International Conference on Robotic Computing (IRC), pp. 35-42, IEEE, 2021.</p>
<p>Curriculum-based offline network training for improvement of peg-in-hole task performance for holes in concrete. A Y Yasutomi, H Mori, T Ogata, 2022 IEEE/SICE International Symposium on System Integration (SII). IEEE2022A. Y. Yasutomi, H. Mori, and T. Ogata, "Curriculum-based offline network training for improvement of peg-in-hole task performance for holes in concrete," in 2022 IEEE/SICE International Symposium on System Integration (SII), pp. 712-717, IEEE, 2022.</p>
<p>Learning force control for contact-rich manipulation tasks with rigid position-controlled robots. C C Beltran-Hernandez, D Petit, I G Ramirez-Alpizar, T Nishi, S Kikuchi, T Matsubara, K Harada, IEEE Robotics and Automation Letters. C. C. Beltran-Hernandez, D. Petit, I. G. Ramirez-Alpizar, T. Nishi, S. Kikuchi, T. Matsubara, and K. Harada, "Learning force control for contact-rich manipulation tasks with rigid position-controlled robots," IEEE Robotics and Automation Letters, pp. 1-1, 2020.</p>
<p>Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. T Haarnoja, A Zhou, P Abbeel, S Levine, abs/1801.01290ICML. T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, "Soft actor-critic: Off- policy maximum entropy deep reinforcement learning with a stochastic actor," ICML, vol. abs/1801.01290, 2018.</p>
<p>Distributed prioritized experience replay. D Horgan, J Quan, D Budden, G Barth-Maron, M Hessel, H Van Hasselt, D Silver, International Conference on Learning Representations. D. Horgan, J. Quan, D. Budden, G. Barth-Maron, M. Hessel, H. van Hasselt, and D. Silver, "Distributed prioritized experience replay," in International Conference on Learning Representations, 2018.</p>
<p>Gain-scheduled pid controller design. V Veselỳ, A Ilka, Journal of process control. 238V. Veselỳ and A. Ilka, "Gain-scheduled pid controller design," Journal of process control, vol. 23, no. 8, pp. 1141-1148, 2013.</p>
<p>A day of great illumination: Bf skinner's discovery of shaping. G B Peterson, Journal of the experimental analysis of behavior. 823G. B. Peterson, "A day of great illumination: Bf skinner's discovery of shaping," Journal of the experimental analysis of behavior, vol. 82, no. 3, pp. 317-328, 2004.</p>
<p>Reinforcement today. B F Skinner, American Psychologist. 13394B. F. Skinner, "Reinforcement today.," American Psychologist, vol. 13, no. 3, p. 94, 1958.</p>
<p>Catastrophic interference in connectionist networks: The sequential learning problem. M Mccloskey, N J Cohen, Psychology of learning and motivation. 24ElsevierM. McCloskey and N. J. Cohen, "Catastrophic interference in connec- tionist networks: The sequential learning problem," in Psychology of learning and motivation, vol. 24, pp. 109-165, Elsevier, 1989.</p>
<p>Catastrophic forgetting in connectionist networks. R M French, Trends in cognitive sciences. 34R. M. French, "Catastrophic forgetting in connectionist networks," Trends in cognitive sciences, vol. 3, no. 4, pp. 128-135, 1999.</p>
<p>Design and use paradigms for gazebo, an open-source multi-robot simulator. N Koenig, A Howard, 2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE3N. Koenig and A. Howard, "Design and use paradigms for gazebo, an open-source multi-robot simulator," in 2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), vol. 3, pp. 2149- 2154, IEEE, 2004.</p>
<p>Ros: an open-source robot operating system. M Quigley, K Conley, B Gerkey, J Faust, T Foote, J Leibs, R Wheeler, A Y Ng, ICRA workshop on open source software. Kobe, Japan3M. Quigley, K. Conley, B. Gerkey, J. Faust, T. Foote, J. Leibs, R. Wheeler, and A. Y. Ng, "Ros: an open-source robot operating system," in ICRA workshop on open source software, vol. 3, p. 5, Kobe, Japan, 2009.</p>
<p>Industrial robotics category assembly challenge rules and regulations. 2021"Industrial robotics category assembly challenge rules and regulations," 2021. https://wrs.nedo.go.jp/wrs2020/challenge/download/Rules/ DetailedRules Assembly EN.pdf (accessed on Apr 1th, 2022).</p>
<p>Automatic curriculum graph generation for reinforcement learning agents. M Svetlik, M Leonetti, J Sinapov, R Shah, N Walker, P Stone, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence31M. Svetlik, M. Leonetti, J. Sinapov, R. Shah, N. Walker, and P. Stone, "Automatic curriculum graph generation for reinforcement learning agents," in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 31, 2017.</p>
<p>Mujoco: A physics engine for modelbased control. E Todorov, T Erez, Y Tassa, 2012 IEEE/RSJ international conference on intelligent robots and systems. IEEEE. Todorov, T. Erez, and Y. Tassa, "Mujoco: A physics engine for model- based control," in 2012 IEEE/RSJ international conference on intelligent robots and systems, pp. 5026-5033, IEEE, 2012.</p>
<p>Gpu-accelerated robotic simulation for distributed reinforcement learning. J Liang, V Makoviychuk, A Handa, N Chentanez, M Macklin, D Fox, PMLRConference on Robot Learning. J. Liang, V. Makoviychuk, A. Handa, N. Chentanez, M. Macklin, and D. Fox, "Gpu-accelerated robotic simulation for distributed reinforce- ment learning," in Conference on Robot Learning, pp. 270-282, PMLR, 2018.</p>            </div>
        </div>

    </div>
</body>
</html>
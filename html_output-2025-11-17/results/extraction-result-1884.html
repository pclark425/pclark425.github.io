<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1884 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1884</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1884</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-37.html">extraction-schema-37</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</div>
                <p><strong>Paper ID:</strong> paper-277622178</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.04677v1.pdf" target="_blank">The Disruption Index Measures Displacement Between a Paper and Its Most Cited Reference</a></p>
                <p><strong>Paper Abstract:</strong> Initially developed to capture technical innovation and later adapted to identify scientific breakthroughs, the Disruption Index (D-index) offers the first quantitative framework for analyzing transformative research. Despite its promise, prior studies have struggled to clarify its theoretical foundations, raising concerns about potential bias. Here, we show that-contrary to the common belief that the D-index measures absolute innovation-it captures relative innovation: a paper's ability to displace its most-cited reference. In this way, the D-index reflects scientific progress as the replacement of older answers with newer ones to the same fundamental question-much like light bulbs replacing candles. We support this insight through mathematical analysis, expert surveys, and large-scale bibliometric evidence. To facilitate replication, validation, and broader use, we release a dataset of D-index values for 49 million journal articles (1800-2024) based on OpenAlex.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1884.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1884.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Citation-metrics_bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Biases of citation-based evaluation metrics (impact factor, h-index, short-term citations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper documents that common citation-based evaluation systems (journal impact factors, h-index, short-term citation counts) preferentially reflect popularity and incremental work rather than transformational novelty, producing incentives that favor safe, incremental research.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Disruption Index Measures Displacement Between a Paper and its Most Cited Reference</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_system_type</strong></td>
                            <td>citation metrics (impact factor, h-index, short-term citations)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Disruption Index (D-index) and expert retrospective ratings</td>
                        </tr>
                        <tr>
                            <td><strong>bias_magnitude</strong></td>
                            <td>Qualitative statement that citation metrics favor incremental work; examples: Watson & Crick (D=0.96, top 1%) vs Human Genome Project (D=-0.017, bottom 6%) despite similar citation counts; only ~1% of papers have b_p < 1 and can meaningfully displace top references; median b_p = 119 (1/(1+b_p) ≈ 0.01) leading to typical D-values near zero (characteristic -0.002; median D ≈ -0.0001).</td>
                        </tr>
                        <tr>
                            <td><strong>relationship_type</strong></td>
                            <td>nonlinear inverse/multiplicative: D_p ≈ (1/(1+b_p)) * d_p (Eq.8) - i.e., D-index decreases roughly inversely with the burden factor b_p and multiplicatively with local displacement d_p</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>Citation metrics produce early recognition bias: short citation windows (e.g., 5 years) undercount disruptive work which often stabilizes after ≥10 years; short windows favor large teams because small-team disruptive work accrues citations more slowly ('sleeping beauties').</td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Cross-disciplinary (analysis across many fields using OpenAlex taxonomy; examples from biology, computer science, physics, engineering, medicine, economics)</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>Breakthroughs (high D) overwhelmingly displace within the same field: among 49,077 high-impact (≥100 citations) and highly disruptive (D>0.2) papers 1900-2020, empirical probability that a displacing paper shares a field with its most-cited reference = 0.52 vs combinatorial expected 0.014 (≈37× higher). Science vs technology asymmetry: in science 62% of papers are consolidating (D≤0) vs technology (patents) where 62% are disruptive (paper reports this cross-domain contrast).</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_studied</strong></td>
                            <td>short-term citation counts, journal impact factors, h-index (discussed as commonly used proxies)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>Expert retrospective ratings (global expert survey), long-term stabilized D-index, Nobel-winning papers used as benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap</strong></td>
                            <td>D-index aligns well with expert judgments: average D for expert-nominated disruptive papers = 0.21 (top 1%); for expert-nominated consolidating papers = -0.011 (bottom 13%); AUC = 0.83 for D-index vs expert labels. By contrast citation counts can fail to distinguish disruptive vs consolidating (example: two highly-cited but different D outcomes).</td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>Yes — transformational (high D) work is rare: only ~1% have b_p < 1 (able to displace top ref). Transformational papers (D>0.2) selected (N=49,077 high-impact) show strong within-field displacement; transformational work tends to be recognized more slowly (often >10 years). Incremental papers cluster near D≈0 (characteristic -0.002).</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_failures</strong></td>
                            <td>Yes — the paper documents multiple interacting failures: citation inflation over time, missing references, and short citation windows can all bias measured disruptiveness; these issues can compound (e.g., short window + citation inflation + missing refs can together understate disruptive work), though no single multiplicative factor is universally quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>not applicable (no ML/automated evaluation system tested in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_bias</strong></td>
                            <td>not examined (no automated models trained in this study)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_tested</strong></td>
                            <td>Extending citation windows (to 10+ years) to stabilize D-index; empirical re-analysis shows the negative effect of team size on D-index is recovered with long citation windows — i.e., longer windows reduce bias that favored large teams in short-window analyses (quantitative turning point reported around 10 years).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples</strong></td>
                            <td>Consolidating papers that achieved high recognition: Ketterle et al. (Bose-Einstein condensation) D = -0.58 (bottom 3%) yet Nobel Prize 2001; Human Genome paper D = -0.017 (bottom 6%) but highly cited — demonstrates that high recognition does not always imply high D (transformational displacement).</td>
                        </tr>
                        <tr>
                            <td><strong>moderating_factors</strong></td>
                            <td>Citation window length (short windows favor large teams), team size (larger teams appear less disruptive in long windows; short windows can invert this), missing reference data (zero-backward-citation records distort trends), and burden factor b_p (dominant moderator of D).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size_and_methods</strong></td>
                            <td>Multiple large-scale bibliometric analyses using OpenAlex: dataset release of 49 million journal articles (1800-2024); specific analyses: 1,000 papers for Zipf fits (Fig.2), 22 million OpenAlex journal papers for decomposition (Fig.3), 42 million for D-index distribution, 49,077 high-impact/high-disruption papers for field overlap, regression on 1.7 million papers to study team size vs D under different windows; methods include analytic decomposition (Eq.2–8), regression with yearly fixed effects, expert survey (N=20 scholars, 190 nominations) and AUC evaluation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1884.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1884.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>D-index_decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Disruption Index decomposition into local displacement (d_p) and knowledge burden (b_p)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper mathematically decomposes the D-index into a local displacement factor d_p (intrinsic tendency to displace references) and a knowledge burden factor b_p (ratio of most-cited reference impact to the focal paper), showing that D ≈ (1/(1+b_p)) * d_p and that b_p dominates most D values.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Disruption Index Measures Displacement Between a Paper and its Most Cited Reference</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_system_type</strong></td>
                            <td>citation-metric based disruptiveness measure (D-index)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>D-index decomposed into d_p (local displacement probability difference) and b_p (C_max / C_p)</td>
                        </tr>
                        <tr>
                            <td><strong>bias_magnitude</strong></td>
                            <td>Empirical medians: d_p median = -0.2; b_p median = 119 (implying 1/(1+b_p) ≈ 0.01); combined characteristic D ≈ -0.002; actual median D ≈ -0.0001. Only ~1% of papers have b_p < 1 (able to overcome burden).</td>
                        </tr>
                        <tr>
                            <td><strong>relationship_type</strong></td>
                            <td>multiplicative inverse: D_p ≈ (1/(1+b_p)) * d_p — D scales linearly with d_p but is attenuated by the inverse of (1+b_p); thus high b_p strongly attenuates D even when d_p is positive.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>Because b_p depends on relative citation impacts, citation inflation over time increases typical b_p values (more cumulative canonical citations), causing average D to cluster near zero over long time-spans unless displacement against a canonical reference occurs.</td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Cross-field (general bibliometric across OpenAlex journal articles spanning disciplines)</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>Paper reports aggregate distributions across fields: 98.9% of papers have b_p > 1; only ~1% b_p < 1. Field-specific numbers not broken out in full detail, but breakthrough cases (high D) concentrated within same topical fields (52% overlap vs expected 1.4%).</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_studied</strong></td>
                            <td>D-index components (d_p, b_p) as refined proxies for novelty</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>Expert nominations and recognition (e.g., Breakthrough Papers dataset, Nature editors list, Nobel laureates) used for validation</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap</strong></td>
                            <td>By decomposing D, authors explain why raw D values are small for most papers despite potential local novelty: attenuation by b_p (median 119) produces small D even if d_p positive. No single scalar correlation between citation counts and 'true' novelty provided, but AUC 0.83 for D vs expert labels indicates good discriminative validity for disruptive cases.</td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>Yes — most papers are incremental (d_p often negative or near zero and b_p >>1), while transformational papers require both positive d_p and b_p < 1; quantified: 32% have positive d_p but 98.9% have b_p >1, leaving only ~1% able to displace canonical references.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_failures</strong></td>
                            <td>The decomposition shows how a 'local' novelty signal (d_p) can be masked by global citation structure (b_p); thus failing of proxy metrics can be additive (local signal present but globally attenuated) rather than purely multiplicative across other biases.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>not applicable</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_bias</strong></td>
                            <td>not applicable</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_tested</strong></td>
                            <td>Interpretative recommendation: focus on sign of D-index (displacement vs consolidation) and on d_p for within-field differences; use longer citation windows to allow d_p and b_p to stabilize.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples</strong></td>
                            <td>High d_p but large b_p -> small D (i.e., locally novel work that fails to displace canonical reference and thus receives small D) — this explains many seemingly novel but low-D papers.</td>
                        </tr>
                        <tr>
                            <td><strong>moderating_factors</strong></td>
                            <td>Citation distribution shape (Zipf's a and b parameters), reference length (shown empirically to be less relevant after accounting for d_p and b_p), and field canonicality (presence of very highly-cited reference increases b_p).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size_and_methods</strong></td>
                            <td>Analytic derivation (Eq.2–8) validated with empirical fits: Zipf parameters estimated from 1,000 OpenAlex papers (a ≈ 1.89, b ≈ 0.60; more generally a ≈ 2.0, b ≈ 1.4 in other samples); distributions estimated on 22 million (d_p, b_p) and 42 million D-index values.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1884.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1884.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Citation-window_bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bias introduced by short citation windows in evaluating novelty (5-year vs long windows)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper shows that using short citation windows (commonly 5 years) underestimates disruptive/transformational work — disruptive papers and small-team work accrue citations more slowly and often require ≥10 years to stabilize D-index estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Disruption Index Measures Displacement Between a Paper and its Most Cited Reference</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_system_type</strong></td>
                            <td>temporal citation-window based evaluation (short-window citation metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>D-index computed under different citation windows (1,3,5,10,20,25 years) and comparison to stabilized long-window D</td>
                        </tr>
                        <tr>
                            <td><strong>bias_magnitude</strong></td>
                            <td>Short 5-year windows can invert observed relationships: earlier studies using 5-year windows reported a positive marginal effect of team size on D, while extending windows to 10+ years recovers a negative effect; turning point reported at ~10 years. Regression analyses use cohorts with citation windows of 1,3,5,10,20,25 years (sample sizes: 47,129 for 2019; 271,496 for 2017; 444,675 for 2015; 536,463 for 2010; 344,582 for 2000; 226,358 for 1995).</td>
                        </tr>
                        <tr>
                            <td><strong>relationship_type</strong></td>
                            <td>threshold/time-to-stabilization effect: D-index values grow/change over time and typically stabilize after a long tail; short windows systematically underreport D for slow-recognition works</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>Disruptive papers often exhibit delayed recognition; D-index may take ≥10 years to stabilize; small-team disruptive work is particularly prone to delayed citation accumulation ('sleeping beauties').</td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Cross-disciplinary; regression sample includes multi-field OpenAlex journal articles under various cohorts</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>Not broken down by field for the window effect in detail, but claim that small-team delayed recognition is general and affects detection across fields; field-specific stabilization times not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_studied</strong></td>
                            <td>short-term citation counts and D-index computed with short windows</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>Long-window stabilized D-index and expert judgment used as validation</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap</strong></td>
                            <td>Using a 5-year window produced different statistical inferences (e.g., team-size effect) than long windows; extending to longer windows recovers previously reported negative team-size effect. No single overall percentage error reported, but regression results qualitatively change sign.</td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>Transformational work (disruptive D>0) is more affected (under-counted) by short windows than incremental work; quantitative turning point for biased inference around 10 years.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_failures</strong></td>
                            <td>Interacts with team-size and citation inflation biases (short windows amplify biases favoring large teams and can combine with missing-data artifacts).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>not applicable</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_bias</strong></td>
                            <td>not applicable</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_tested</strong></td>
                            <td>Extending citation windows to 10+ years; re-running regression analyses recovered negative team-size effect (i.e., counteracts short-window bias).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples</strong></td>
                            <td>No direct counterexample where short window succeeds for disruptive papers; rather, short windows produce misleading positive team-size association in Petersen et al. (replicated situation).</td>
                        </tr>
                        <tr>
                            <td><strong>moderating_factors</strong></td>
                            <td>Team size (large teams accumulate citations faster), citation accrual dynamics (sleeping-beauty behavior), and cohort year (older cohorts have longer windows to stabilize).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size_and_methods</strong></td>
                            <td>Regression analysis on 1.7 million papers with 1 ≤ k_f ≤ 10 coauthors, 5 ≤ r_f ≤ 50 references, 10 ≤ c_f ≤ 1000 citations (matching Petersen et al. parameters); cohort analyses with specific sample sizes across years as noted above; OLS with yearly fixed effects.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1884.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1884.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Citation_inflation_effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Citation inflation and its effect on temporal trends of disruptiveness</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper discusses citation inflation (growing numbers of papers and references over time) and debates whether it causes apparent declines in average D-index; it shows that the true burden is the impact of the most-cited reference (b_p) rather than raw reference length, and that reported declines may be partly due to dataset artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Disruption Index Measures Displacement Between a Paper and its Most Cited Reference</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_system_type</strong></td>
                            <td>long-term bibliometric trend analysis (temporal evaluation metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>D-index and its temporal trend over decades</td>
                        </tr>
                        <tr>
                            <td><strong>bias_magnitude</strong></td>
                            <td>Prior study (Park et al., 2023) reported consistent decline in average D over six decades; replication issues: removing zero-reference works reduces the decline (Holst et al., 2024), but Park et al.'s findings reportedly remain robust even after excluding zero-reference items (Park et al., 2025). The paper reports median b_p = 119 and argues citation inflation increases knowledge burden over time, tending to shrink D-values.</td>
                        </tr>
                        <tr>
                            <td><strong>relationship_type</strong></td>
                            <td>long-term downward pressure (attenuating) on measured disruptiveness due to growing cumulative citations to canonical works; confounded by data artifacts</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>Over multi-decade scales, average D-index may decline due to citation inflation and growing knowledge burden; however, dataset artefacts (zero-reference records) can produce spurious trends. Authors argue focusing on sign of D reduces reference-length confounding.</td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Cross-disciplinary historical trend (1900–2024, OpenAlex)</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>Not quantified per field for inflation effect in detail; authors emphasize effect relates to presence of canonical, highly-cited references in fields and is therefore field-dependent in magnitude.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_studied</strong></td>
                            <td>average D-index over time and reference-length statistics</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>Analytic decomposition (d_p, b_p), empirical stability checks, and comparisons across datasets (Web of Science vs OpenAlex) and data-cleaning variants (exclusion of zero-reference works)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap</strong></td>
                            <td>Quantitative discrepancies between studies: magnitude of reported temporal decline in D varies by data source and treatment of zero-reference items; no single unified numeric gap given but authors report substantial sensitivity to data artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>Citation inflation disproportionately compresses D values toward zero, making transformational work (which must overcome canonical references) relatively rarer in measured D distributions over time.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_failures</strong></td>
                            <td>Yes — citation inflation interacts with missing-reference artifacts and short-window choices to bias temporal analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>not applicable</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_bias</strong></td>
                            <td>not applicable</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_tested</strong></td>
                            <td>Data cleaning choices (exclude zero-reference works), focusing on sign of D rather than means, and using decomposition into d_p and b_p to interpret trends.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples</strong></td>
                            <td>Authors show that after controlling for zero-reference artifacts and using long windows some earlier claims of steep declines may be smaller or robust depending on treatment (cites Holst et al. 2024 and Park et al. 2025 debates).</td>
                        </tr>
                        <tr>
                            <td><strong>moderating_factors</strong></td>
                            <td>Data coverage (missing references), type of scholarly work included (journals vs books vs chapters), and the presence of canonical highly-cited references in a field.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size_and_methods</strong></td>
                            <td>Large-scale OpenAlex analyses (49 million journal articles 1800–2024); comparisons with Web of Science–based studies and replication attempts by other teams.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1884.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1884.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Field_alignment_of_breakthroughs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Within-field displacement pattern of breakthrough (disruptive) papers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirically, highly disruptive papers tend to displace their most-cited reference within the same topical field far more often than expected by chance, indicating breakthroughs often address the same fundamental question rather than random recombination across fields.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Disruption Index Measures Displacement Between a Paper and its Most Cited Reference</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_system_type</strong></td>
                            <td>topic/field-alignment analysis as part of bibliometric evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>High D-index (D>0.2) and field overlap using 292-category OpenAlex taxonomy</td>
                        </tr>
                        <tr>
                            <td><strong>bias_magnitude</strong></td>
                            <td>Among 49,077 high-impact (>100 citations) & highly disruptive (D>0.2) papers (1900–2020), empirical probability that the displacing paper shares a field with its most-cited reference = 0.52 vs combinatorial expected p ≈ 0.014 (≈37× higher).</td>
                        </tr>
                        <tr>
                            <td><strong>relationship_type</strong></td>
                            <td>strong enrichment (factor ~37) for within-field displacement among highly disruptive papers relative to random expectation</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>Not specifically temporal in this analysis; result aggregated over 1900–2020.</td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Multiple fields (OpenAlex 292-category taxonomy; examples include molecular biology, discrete mathematics, organic chemistry, computer science, physics, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>Quantitative field-by-field differences not exhaustively reported, but cross-domain contrast provided: science papers predominantly consolidating (62% D≤0) vs patents/technology show reversed polarity (62% disruptive).</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_studied</strong></td>
                            <td>D-index as indicator of within-field displacement vs random combinatorial expectation</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>Top-nominated breakthrough datasets (expert nominations and Nature editors list) and topic taxonomy overlap used as indirect ground-truth</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap</strong></td>
                            <td>Highly disruptive papers are far more likely to be topic-aligned with their most-cited reference than random (0.52 vs 0.014), supporting D-index as capturing purposeful within-field displacement rather than random novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>Transformational (high D) papers show strong within-field topical alignment with displaced references; incremental papers do not show this pattern to the same extent.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_failures</strong></td>
                            <td>Not directly examined here, though authors note that mixing document types (journals vs chapters) can distort results and recommend focusing on journal articles.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>not applicable</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_bias</strong></td>
                            <td>not applicable</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_tested</strong></td>
                            <td>None; this is descriptive empirical analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples</strong></td>
                            <td>No strong counterexample presented; case studies (Watson & Crick displacing Pauling & Corey; Turing reframing Gödel) support within-field displacement pattern.</td>
                        </tr>
                        <tr>
                            <td><strong>moderating_factors</strong></td>
                            <td>Taxonomy granularity (292-category), citation threshold (analysis uses high-impact papers >100 citations), and selection of highly disruptive threshold (D>0.2).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size_and_methods</strong></td>
                            <td>Empirical analysis of 49,077 high-impact and highly disruptive papers (1900–2020) using OpenAlex field labels and combinatorial probability calculation (Eq.9) for expected overlap by chance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1884.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1884.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Team-size_and_recognition_delay</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Interaction of team size with detection of disruptiveness under different citation windows</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper shows that team size correlates with measured disruptiveness differently depending on citation window: short windows produce a positive marginal effect of team size on D (favoring large teams), but long windows (≥10 years) recover a negative association (small teams more disruptive).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Disruption Index Measures Displacement Between a Paper and its Most Cited Reference</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_system_type</strong></td>
                            <td>bibliometric evaluation using D-index with regression controls</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>D-index computed across varying citation windows; team size (number of coauthors) as predictor</td>
                        </tr>
                        <tr>
                            <td><strong>bias_magnitude</strong></td>
                            <td>Short-window analyses (5 years) reported positive marginal effect of team size on D (Petersen et al.), but using extended windows the negative effect reported by Wu et al. (2019) re-emerges. The regression dataset included 1.7M papers (as used to demonstrate change in sign), with cohort analyses across millions of papers (47k to 536k per cohort). Turning point around 10 years.</td>
                        </tr>
                        <tr>
                            <td><strong>relationship_type</strong></td>
                            <td>time-moderated effect: sign of relationship changes with citation-window length (short-window: positive; long-window: negative), indicating a non-stationary temporal moderation.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>Recognition of small-team disruptive work is slower; D-index for small teams increases relative to large teams as citation window lengthens toward 10+ years.</td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Cross-disciplinary (large aggregated sample spanning many fields)</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>Not broken down by field in detail for this interaction; claim that the pattern is general because small teams often produce sleeping-beauty papers across disciplines.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_studied</strong></td>
                            <td>D-index under varying citation windows; team size as covariate</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>Long-window D-index stabilization and prior findings (Wu et al. 2019) plus replication exercises</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap</strong></td>
                            <td>Short-window analyses produced qualitatively different inference about team-size effect; no single scalar gap reported but demonstrated sign reversal in regression coefficients when window extended to ~10 years.</td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>Transformational (disruptive) outputs are more likely from small teams in long-window analyses; short windows mask this and make large teams appear more disruptive.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_failures</strong></td>
                            <td>Short citation windows + skewed citation accrual rates (sleeping-beauties) create interacting biases that mischaracterize team-size vs disruptiveness relationship.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>not applicable</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_bias</strong></td>
                            <td>not applicable</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_tested</strong></td>
                            <td>Use of longer citation windows (10+ years) in regression analyses; authors report recovery of negative team-size effect when windows are extended.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples</strong></td>
                            <td>Petersen et al. (2024/2025) results (positive team-size effect) are counterexamples explained by short-window bias; reanalysis recovers original negative effect.</td>
                        </tr>
                        <tr>
                            <td><strong>moderating_factors</strong></td>
                            <td>Citation-window length, team size, citation accrual speed of papers (sleeping beauties), and covariates controlled in regression (log transforms for right-skewed variables).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size_and_methods</strong></td>
                            <td>Regression using OLS with yearly fixed effects on a dataset of 1.7 million papers (with specified inclusion ranges); cohort analyses with sample sizes for multiple publication years: 47,129 (2019), 271,496 (2017), 444,675 (2015), 536,463 (2010), 344,582 (2000), 226,358 (1995).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Papers and patents are becoming less disruptive over time <em>(Rating: 2)</em></li>
                <li>Dataset Artefacts are the Hidden Drivers of the Declining Disruptiveness in Science <em>(Rating: 2)</em></li>
                <li>The disruption index suffers from citation inflation: Re-analysis of temporal CD trend and relationship with team size reveal discrepancies <em>(Rating: 2)</em></li>
                <li>Large teams develop and small teams disrupt science and technology <em>(Rating: 2)</em></li>
                <li>Are disruption index indicators convergently valid? The comparison of several indicator variants with assessments by peers <em>(Rating: 2)</em></li>
                <li>Is disruption decreasing, or is it accelerating <em>(Rating: 2)</em></li>
                <li>Slowed canonical progress in large fields of science <em>(Rating: 1)</em></li>
                <li>Funding risky research <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1884",
    "paper_id": "paper-277622178",
    "extraction_schema_id": "extraction-schema-37",
    "extracted_data": [
        {
            "name_short": "Citation-metrics_bias",
            "name_full": "Biases of citation-based evaluation metrics (impact factor, h-index, short-term citations)",
            "brief_description": "The paper documents that common citation-based evaluation systems (journal impact factors, h-index, short-term citation counts) preferentially reflect popularity and incremental work rather than transformational novelty, producing incentives that favor safe, incremental research.",
            "citation_title": "The Disruption Index Measures Displacement Between a Paper and its Most Cited Reference",
            "mention_or_use": "use",
            "evaluation_system_type": "citation metrics (impact factor, h-index, short-term citations)",
            "novelty_measure": "Disruption Index (D-index) and expert retrospective ratings",
            "bias_magnitude": "Qualitative statement that citation metrics favor incremental work; examples: Watson & Crick (D=0.96, top 1%) vs Human Genome Project (D=-0.017, bottom 6%) despite similar citation counts; only ~1% of papers have b_p &lt; 1 and can meaningfully displace top references; median b_p = 119 (1/(1+b_p) ≈ 0.01) leading to typical D-values near zero (characteristic -0.002; median D ≈ -0.0001).",
            "relationship_type": "nonlinear inverse/multiplicative: D_p ≈ (1/(1+b_p)) * d_p (Eq.8) - i.e., D-index decreases roughly inversely with the burden factor b_p and multiplicatively with local displacement d_p",
            "temporal_pattern": "Citation metrics produce early recognition bias: short citation windows (e.g., 5 years) undercount disruptive work which often stabilizes after ≥10 years; short windows favor large teams because small-team disruptive work accrues citations more slowly ('sleeping beauties').",
            "field_studied": "Cross-disciplinary (analysis across many fields using OpenAlex taxonomy; examples from biology, computer science, physics, engineering, medicine, economics)",
            "field_differences": "Breakthroughs (high D) overwhelmingly displace within the same field: among 49,077 high-impact (≥100 citations) and highly disruptive (D&gt;0.2) papers 1900-2020, empirical probability that a displacing paper shares a field with its most-cited reference = 0.52 vs combinatorial expected 0.014 (≈37× higher). Science vs technology asymmetry: in science 62% of papers are consolidating (D≤0) vs technology (patents) where 62% are disruptive (paper reports this cross-domain contrast).",
            "proxy_metric_studied": "short-term citation counts, journal impact factors, h-index (discussed as commonly used proxies)",
            "ground_truth_measure": "Expert retrospective ratings (global expert survey), long-term stabilized D-index, Nobel-winning papers used as benchmarks",
            "proxy_truth_gap": "D-index aligns well with expert judgments: average D for expert-nominated disruptive papers = 0.21 (top 1%); for expert-nominated consolidating papers = -0.011 (bottom 13%); AUC = 0.83 for D-index vs expert labels. By contrast citation counts can fail to distinguish disruptive vs consolidating (example: two highly-cited but different D outcomes).",
            "incremental_vs_transformational": "Yes — transformational (high D) work is rare: only ~1% have b_p &lt; 1 (able to displace top ref). Transformational papers (D&gt;0.2) selected (N=49,077 high-impact) show strong within-field displacement; transformational work tends to be recognized more slowly (often &gt;10 years). Incremental papers cluster near D≈0 (characteristic -0.002).",
            "multiple_proxy_failures": "Yes — the paper documents multiple interacting failures: citation inflation over time, missing references, and short citation windows can all bias measured disruptiveness; these issues can compound (e.g., short window + citation inflation + missing refs can together understate disruptive work), though no single multiplicative factor is universally quantified.",
            "automated_system_performance": "not applicable (no ML/automated evaluation system tested in this paper)",
            "training_data_bias": "not examined (no automated models trained in this study)",
            "intervention_tested": "Extending citation windows (to 10+ years) to stabilize D-index; empirical re-analysis shows the negative effect of team size on D-index is recovered with long citation windows — i.e., longer windows reduce bias that favored large teams in short-window analyses (quantitative turning point reported around 10 years).",
            "counter_examples": "Consolidating papers that achieved high recognition: Ketterle et al. (Bose-Einstein condensation) D = -0.58 (bottom 3%) yet Nobel Prize 2001; Human Genome paper D = -0.017 (bottom 6%) but highly cited — demonstrates that high recognition does not always imply high D (transformational displacement).",
            "moderating_factors": "Citation window length (short windows favor large teams), team size (larger teams appear less disruptive in long windows; short windows can invert this), missing reference data (zero-backward-citation records distort trends), and burden factor b_p (dominant moderator of D).",
            "sample_size_and_methods": "Multiple large-scale bibliometric analyses using OpenAlex: dataset release of 49 million journal articles (1800-2024); specific analyses: 1,000 papers for Zipf fits (Fig.2), 22 million OpenAlex journal papers for decomposition (Fig.3), 42 million for D-index distribution, 49,077 high-impact/high-disruption papers for field overlap, regression on 1.7 million papers to study team size vs D under different windows; methods include analytic decomposition (Eq.2–8), regression with yearly fixed effects, expert survey (N=20 scholars, 190 nominations) and AUC evaluation.",
            "uuid": "e1884.0"
        },
        {
            "name_short": "D-index_decomposition",
            "name_full": "Disruption Index decomposition into local displacement (d_p) and knowledge burden (b_p)",
            "brief_description": "The paper mathematically decomposes the D-index into a local displacement factor d_p (intrinsic tendency to displace references) and a knowledge burden factor b_p (ratio of most-cited reference impact to the focal paper), showing that D ≈ (1/(1+b_p)) * d_p and that b_p dominates most D values.",
            "citation_title": "The Disruption Index Measures Displacement Between a Paper and its Most Cited Reference",
            "mention_or_use": "use",
            "evaluation_system_type": "citation-metric based disruptiveness measure (D-index)",
            "novelty_measure": "D-index decomposed into d_p (local displacement probability difference) and b_p (C_max / C_p)",
            "bias_magnitude": "Empirical medians: d_p median = -0.2; b_p median = 119 (implying 1/(1+b_p) ≈ 0.01); combined characteristic D ≈ -0.002; actual median D ≈ -0.0001. Only ~1% of papers have b_p &lt; 1 (able to overcome burden).",
            "relationship_type": "multiplicative inverse: D_p ≈ (1/(1+b_p)) * d_p — D scales linearly with d_p but is attenuated by the inverse of (1+b_p); thus high b_p strongly attenuates D even when d_p is positive.",
            "temporal_pattern": "Because b_p depends on relative citation impacts, citation inflation over time increases typical b_p values (more cumulative canonical citations), causing average D to cluster near zero over long time-spans unless displacement against a canonical reference occurs.",
            "field_studied": "Cross-field (general bibliometric across OpenAlex journal articles spanning disciplines)",
            "field_differences": "Paper reports aggregate distributions across fields: 98.9% of papers have b_p &gt; 1; only ~1% b_p &lt; 1. Field-specific numbers not broken out in full detail, but breakthrough cases (high D) concentrated within same topical fields (52% overlap vs expected 1.4%).",
            "proxy_metric_studied": "D-index components (d_p, b_p) as refined proxies for novelty",
            "ground_truth_measure": "Expert nominations and recognition (e.g., Breakthrough Papers dataset, Nature editors list, Nobel laureates) used for validation",
            "proxy_truth_gap": "By decomposing D, authors explain why raw D values are small for most papers despite potential local novelty: attenuation by b_p (median 119) produces small D even if d_p positive. No single scalar correlation between citation counts and 'true' novelty provided, but AUC 0.83 for D vs expert labels indicates good discriminative validity for disruptive cases.",
            "incremental_vs_transformational": "Yes — most papers are incremental (d_p often negative or near zero and b_p &gt;&gt;1), while transformational papers require both positive d_p and b_p &lt; 1; quantified: 32% have positive d_p but 98.9% have b_p &gt;1, leaving only ~1% able to displace canonical references.",
            "multiple_proxy_failures": "The decomposition shows how a 'local' novelty signal (d_p) can be masked by global citation structure (b_p); thus failing of proxy metrics can be additive (local signal present but globally attenuated) rather than purely multiplicative across other biases.",
            "automated_system_performance": "not applicable",
            "training_data_bias": "not applicable",
            "intervention_tested": "Interpretative recommendation: focus on sign of D-index (displacement vs consolidation) and on d_p for within-field differences; use longer citation windows to allow d_p and b_p to stabilize.",
            "counter_examples": "High d_p but large b_p -&gt; small D (i.e., locally novel work that fails to displace canonical reference and thus receives small D) — this explains many seemingly novel but low-D papers.",
            "moderating_factors": "Citation distribution shape (Zipf's a and b parameters), reference length (shown empirically to be less relevant after accounting for d_p and b_p), and field canonicality (presence of very highly-cited reference increases b_p).",
            "sample_size_and_methods": "Analytic derivation (Eq.2–8) validated with empirical fits: Zipf parameters estimated from 1,000 OpenAlex papers (a ≈ 1.89, b ≈ 0.60; more generally a ≈ 2.0, b ≈ 1.4 in other samples); distributions estimated on 22 million (d_p, b_p) and 42 million D-index values.",
            "uuid": "e1884.1"
        },
        {
            "name_short": "Citation-window_bias",
            "name_full": "Bias introduced by short citation windows in evaluating novelty (5-year vs long windows)",
            "brief_description": "The paper shows that using short citation windows (commonly 5 years) underestimates disruptive/transformational work — disruptive papers and small-team work accrue citations more slowly and often require ≥10 years to stabilize D-index estimates.",
            "citation_title": "The Disruption Index Measures Displacement Between a Paper and its Most Cited Reference",
            "mention_or_use": "use",
            "evaluation_system_type": "temporal citation-window based evaluation (short-window citation metrics)",
            "novelty_measure": "D-index computed under different citation windows (1,3,5,10,20,25 years) and comparison to stabilized long-window D",
            "bias_magnitude": "Short 5-year windows can invert observed relationships: earlier studies using 5-year windows reported a positive marginal effect of team size on D, while extending windows to 10+ years recovers a negative effect; turning point reported at ~10 years. Regression analyses use cohorts with citation windows of 1,3,5,10,20,25 years (sample sizes: 47,129 for 2019; 271,496 for 2017; 444,675 for 2015; 536,463 for 2010; 344,582 for 2000; 226,358 for 1995).",
            "relationship_type": "threshold/time-to-stabilization effect: D-index values grow/change over time and typically stabilize after a long tail; short windows systematically underreport D for slow-recognition works",
            "temporal_pattern": "Disruptive papers often exhibit delayed recognition; D-index may take ≥10 years to stabilize; small-team disruptive work is particularly prone to delayed citation accumulation ('sleeping beauties').",
            "field_studied": "Cross-disciplinary; regression sample includes multi-field OpenAlex journal articles under various cohorts",
            "field_differences": "Not broken down by field for the window effect in detail, but claim that small-team delayed recognition is general and affects detection across fields; field-specific stabilization times not quantified.",
            "proxy_metric_studied": "short-term citation counts and D-index computed with short windows",
            "ground_truth_measure": "Long-window stabilized D-index and expert judgment used as validation",
            "proxy_truth_gap": "Using a 5-year window produced different statistical inferences (e.g., team-size effect) than long windows; extending to longer windows recovers previously reported negative team-size effect. No single overall percentage error reported, but regression results qualitatively change sign.",
            "incremental_vs_transformational": "Transformational work (disruptive D&gt;0) is more affected (under-counted) by short windows than incremental work; quantitative turning point for biased inference around 10 years.",
            "multiple_proxy_failures": "Interacts with team-size and citation inflation biases (short windows amplify biases favoring large teams and can combine with missing-data artifacts).",
            "automated_system_performance": "not applicable",
            "training_data_bias": "not applicable",
            "intervention_tested": "Extending citation windows to 10+ years; re-running regression analyses recovered negative team-size effect (i.e., counteracts short-window bias).",
            "counter_examples": "No direct counterexample where short window succeeds for disruptive papers; rather, short windows produce misleading positive team-size association in Petersen et al. (replicated situation).",
            "moderating_factors": "Team size (large teams accumulate citations faster), citation accrual dynamics (sleeping-beauty behavior), and cohort year (older cohorts have longer windows to stabilize).",
            "sample_size_and_methods": "Regression analysis on 1.7 million papers with 1 ≤ k_f ≤ 10 coauthors, 5 ≤ r_f ≤ 50 references, 10 ≤ c_f ≤ 1000 citations (matching Petersen et al. parameters); cohort analyses with specific sample sizes across years as noted above; OLS with yearly fixed effects.",
            "uuid": "e1884.2"
        },
        {
            "name_short": "Citation_inflation_effect",
            "name_full": "Citation inflation and its effect on temporal trends of disruptiveness",
            "brief_description": "The paper discusses citation inflation (growing numbers of papers and references over time) and debates whether it causes apparent declines in average D-index; it shows that the true burden is the impact of the most-cited reference (b_p) rather than raw reference length, and that reported declines may be partly due to dataset artifacts.",
            "citation_title": "The Disruption Index Measures Displacement Between a Paper and its Most Cited Reference",
            "mention_or_use": "use",
            "evaluation_system_type": "long-term bibliometric trend analysis (temporal evaluation metrics)",
            "novelty_measure": "D-index and its temporal trend over decades",
            "bias_magnitude": "Prior study (Park et al., 2023) reported consistent decline in average D over six decades; replication issues: removing zero-reference works reduces the decline (Holst et al., 2024), but Park et al.'s findings reportedly remain robust even after excluding zero-reference items (Park et al., 2025). The paper reports median b_p = 119 and argues citation inflation increases knowledge burden over time, tending to shrink D-values.",
            "relationship_type": "long-term downward pressure (attenuating) on measured disruptiveness due to growing cumulative citations to canonical works; confounded by data artifacts",
            "temporal_pattern": "Over multi-decade scales, average D-index may decline due to citation inflation and growing knowledge burden; however, dataset artefacts (zero-reference records) can produce spurious trends. Authors argue focusing on sign of D reduces reference-length confounding.",
            "field_studied": "Cross-disciplinary historical trend (1900–2024, OpenAlex)",
            "field_differences": "Not quantified per field for inflation effect in detail; authors emphasize effect relates to presence of canonical, highly-cited references in fields and is therefore field-dependent in magnitude.",
            "proxy_metric_studied": "average D-index over time and reference-length statistics",
            "ground_truth_measure": "Analytic decomposition (d_p, b_p), empirical stability checks, and comparisons across datasets (Web of Science vs OpenAlex) and data-cleaning variants (exclusion of zero-reference works)",
            "proxy_truth_gap": "Quantitative discrepancies between studies: magnitude of reported temporal decline in D varies by data source and treatment of zero-reference items; no single unified numeric gap given but authors report substantial sensitivity to data artifacts.",
            "incremental_vs_transformational": "Citation inflation disproportionately compresses D values toward zero, making transformational work (which must overcome canonical references) relatively rarer in measured D distributions over time.",
            "multiple_proxy_failures": "Yes — citation inflation interacts with missing-reference artifacts and short-window choices to bias temporal analyses.",
            "automated_system_performance": "not applicable",
            "training_data_bias": "not applicable",
            "intervention_tested": "Data cleaning choices (exclude zero-reference works), focusing on sign of D rather than means, and using decomposition into d_p and b_p to interpret trends.",
            "counter_examples": "Authors show that after controlling for zero-reference artifacts and using long windows some earlier claims of steep declines may be smaller or robust depending on treatment (cites Holst et al. 2024 and Park et al. 2025 debates).",
            "moderating_factors": "Data coverage (missing references), type of scholarly work included (journals vs books vs chapters), and the presence of canonical highly-cited references in a field.",
            "sample_size_and_methods": "Large-scale OpenAlex analyses (49 million journal articles 1800–2024); comparisons with Web of Science–based studies and replication attempts by other teams.",
            "uuid": "e1884.3"
        },
        {
            "name_short": "Field_alignment_of_breakthroughs",
            "name_full": "Within-field displacement pattern of breakthrough (disruptive) papers",
            "brief_description": "Empirically, highly disruptive papers tend to displace their most-cited reference within the same topical field far more often than expected by chance, indicating breakthroughs often address the same fundamental question rather than random recombination across fields.",
            "citation_title": "The Disruption Index Measures Displacement Between a Paper and its Most Cited Reference",
            "mention_or_use": "use",
            "evaluation_system_type": "topic/field-alignment analysis as part of bibliometric evaluation",
            "novelty_measure": "High D-index (D&gt;0.2) and field overlap using 292-category OpenAlex taxonomy",
            "bias_magnitude": "Among 49,077 high-impact (&gt;100 citations) & highly disruptive (D&gt;0.2) papers (1900–2020), empirical probability that the displacing paper shares a field with its most-cited reference = 0.52 vs combinatorial expected p ≈ 0.014 (≈37× higher).",
            "relationship_type": "strong enrichment (factor ~37) for within-field displacement among highly disruptive papers relative to random expectation",
            "temporal_pattern": "Not specifically temporal in this analysis; result aggregated over 1900–2020.",
            "field_studied": "Multiple fields (OpenAlex 292-category taxonomy; examples include molecular biology, discrete mathematics, organic chemistry, computer science, physics, etc.)",
            "field_differences": "Quantitative field-by-field differences not exhaustively reported, but cross-domain contrast provided: science papers predominantly consolidating (62% D≤0) vs patents/technology show reversed polarity (62% disruptive).",
            "proxy_metric_studied": "D-index as indicator of within-field displacement vs random combinatorial expectation",
            "ground_truth_measure": "Top-nominated breakthrough datasets (expert nominations and Nature editors list) and topic taxonomy overlap used as indirect ground-truth",
            "proxy_truth_gap": "Highly disruptive papers are far more likely to be topic-aligned with their most-cited reference than random (0.52 vs 0.014), supporting D-index as capturing purposeful within-field displacement rather than random novelty.",
            "incremental_vs_transformational": "Transformational (high D) papers show strong within-field topical alignment with displaced references; incremental papers do not show this pattern to the same extent.",
            "multiple_proxy_failures": "Not directly examined here, though authors note that mixing document types (journals vs chapters) can distort results and recommend focusing on journal articles.",
            "automated_system_performance": "not applicable",
            "training_data_bias": "not applicable",
            "intervention_tested": "None; this is descriptive empirical analysis.",
            "counter_examples": "No strong counterexample presented; case studies (Watson & Crick displacing Pauling & Corey; Turing reframing Gödel) support within-field displacement pattern.",
            "moderating_factors": "Taxonomy granularity (292-category), citation threshold (analysis uses high-impact papers &gt;100 citations), and selection of highly disruptive threshold (D&gt;0.2).",
            "sample_size_and_methods": "Empirical analysis of 49,077 high-impact and highly disruptive papers (1900–2020) using OpenAlex field labels and combinatorial probability calculation (Eq.9) for expected overlap by chance.",
            "uuid": "e1884.4"
        },
        {
            "name_short": "Team-size_and_recognition_delay",
            "name_full": "Interaction of team size with detection of disruptiveness under different citation windows",
            "brief_description": "The paper shows that team size correlates with measured disruptiveness differently depending on citation window: short windows produce a positive marginal effect of team size on D (favoring large teams), but long windows (≥10 years) recover a negative association (small teams more disruptive).",
            "citation_title": "The Disruption Index Measures Displacement Between a Paper and its Most Cited Reference",
            "mention_or_use": "use",
            "evaluation_system_type": "bibliometric evaluation using D-index with regression controls",
            "novelty_measure": "D-index computed across varying citation windows; team size (number of coauthors) as predictor",
            "bias_magnitude": "Short-window analyses (5 years) reported positive marginal effect of team size on D (Petersen et al.), but using extended windows the negative effect reported by Wu et al. (2019) re-emerges. The regression dataset included 1.7M papers (as used to demonstrate change in sign), with cohort analyses across millions of papers (47k to 536k per cohort). Turning point around 10 years.",
            "relationship_type": "time-moderated effect: sign of relationship changes with citation-window length (short-window: positive; long-window: negative), indicating a non-stationary temporal moderation.",
            "temporal_pattern": "Recognition of small-team disruptive work is slower; D-index for small teams increases relative to large teams as citation window lengthens toward 10+ years.",
            "field_studied": "Cross-disciplinary (large aggregated sample spanning many fields)",
            "field_differences": "Not broken down by field in detail for this interaction; claim that the pattern is general because small teams often produce sleeping-beauty papers across disciplines.",
            "proxy_metric_studied": "D-index under varying citation windows; team size as covariate",
            "ground_truth_measure": "Long-window D-index stabilization and prior findings (Wu et al. 2019) plus replication exercises",
            "proxy_truth_gap": "Short-window analyses produced qualitatively different inference about team-size effect; no single scalar gap reported but demonstrated sign reversal in regression coefficients when window extended to ~10 years.",
            "incremental_vs_transformational": "Transformational (disruptive) outputs are more likely from small teams in long-window analyses; short windows mask this and make large teams appear more disruptive.",
            "multiple_proxy_failures": "Short citation windows + skewed citation accrual rates (sleeping-beauties) create interacting biases that mischaracterize team-size vs disruptiveness relationship.",
            "automated_system_performance": "not applicable",
            "training_data_bias": "not applicable",
            "intervention_tested": "Use of longer citation windows (10+ years) in regression analyses; authors report recovery of negative team-size effect when windows are extended.",
            "counter_examples": "Petersen et al. (2024/2025) results (positive team-size effect) are counterexamples explained by short-window bias; reanalysis recovers original negative effect.",
            "moderating_factors": "Citation-window length, team size, citation accrual speed of papers (sleeping beauties), and covariates controlled in regression (log transforms for right-skewed variables).",
            "sample_size_and_methods": "Regression using OLS with yearly fixed effects on a dataset of 1.7 million papers (with specified inclusion ranges); cohort analyses with sample sizes for multiple publication years: 47,129 (2019), 271,496 (2017), 444,675 (2015), 536,463 (2010), 344,582 (2000), 226,358 (1995).",
            "uuid": "e1884.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Papers and patents are becoming less disruptive over time",
            "rating": 2
        },
        {
            "paper_title": "Dataset Artefacts are the Hidden Drivers of the Declining Disruptiveness in Science",
            "rating": 2
        },
        {
            "paper_title": "The disruption index suffers from citation inflation: Re-analysis of temporal CD trend and relationship with team size reveal discrepancies",
            "rating": 2
        },
        {
            "paper_title": "Large teams develop and small teams disrupt science and technology",
            "rating": 2
        },
        {
            "paper_title": "Are disruption index indicators convergently valid? The comparison of several indicator variants with assessments by peers",
            "rating": 2
        },
        {
            "paper_title": "Is disruption decreasing, or is it accelerating",
            "rating": 2
        },
        {
            "paper_title": "Slowed canonical progress in large fields of science",
            "rating": 1
        },
        {
            "paper_title": "Funding risky research",
            "rating": 1
        }
    ],
    "cost": 0.019567,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>The Disruption Index Measures Displacement Between a Paper and its Most Cited Reference</p>
<p>Yiling Lin 
Linzhuo Li linzhuoli@zju.edu.cn 
School of Computing and Information
University of Pittsburgh
15260PittsburghPA</p>
<p>Lingfei Wu 
School of Computing and Information
University of Pittsburgh
15260PittsburghPA</p>
<p>Department of Sociology
Zhejiang University
310058HangzhouChina</p>
<p>The Disruption Index Measures Displacement Between a Paper and its Most Cited Reference
DA7C602C115FDCAC7BDC6539F7327146
Initially developed to capture technical innovation and later adapted to identify scientific breakthroughs, the Disruption Index (D-index) offers the first quantitative framework for analyzing transformative research.Despite its promise, prior studies have struggled to clarify its theoretical foundations, raising concerns about potential bias.Here, we show that-contrary to the common belief that the D-index measures absolute innovation-it captures relative innovation: a paper's ability to displace its most-cited reference.In this way, the D-index reflects scientific progress as the replacement of older answers with newer ones to the same fundamental question-much like light bulbs replacing candles.We support this insight through mathematical analysis, expert surveys, and large-scale bibliometric evidence.To facilitate replication, validation, and broader use, we release a dataset of D-index values for 49 million journal articles (1800-2024) based on OpenAlex.</p>
<p>Introduction</p>
<p>Scientific breakthroughs are the engine of progress, driving technological advancements, economic growth, and societal transformation.From the development of mRNA-based COVID-19 vaccines to the rise of artificial intelligence (AI) and progress in quantum computing, transformative discoveries have reshaped industries and redefined global competitiveness.Recognizing the profound impact of such breakthroughs, national agencies and global alliances are increasingly focused on securing leadership in science and technology, making the ability to identify and support emerging research frontiers a strategic priority.</p>
<p>However, for decades, decision-makers have relied on citation-based indicators-such as journal impact factors and the h-index-to identify high-impact research.These metrics often reflect popularity rather than innovation, shaping hiring, promotion, and funding decisions in ways that favor incremental research from established scholars (Franzoni et al., 2022).This evaluation system creates incentives for researchers to maximize publication counts on familiar topics rather than pursue high-risk, high-reward discoveries.As a result, there is growing concern about scientific stagnation (Chu &amp; Evans, 2021), where transformative ideas struggle to gain recognition within an ecosystem optimized for productivity over breakthrough potential (Bhattacharya &amp; Packalen, 2020).</p>
<p>To address this gap, the Disruption Index (D-index)-originally developed to measure technological innovation (Funk &amp; Owen-Smith, 2017)-was introduced as the first metric to capture scientific breakthroughs (Wu et al., 2019), offering an alternative to traditional citation counts.The D-index has gained wide interest among scholars, funding agencies, media, and the general public due to its success in highlighting landmark discoveries.For example, it assigns a high disruption score to Watson and Crick's 1953 DNA paper (D = 0.96, top 1%) and a low score to the 1999 Human Genome Project paper (D = -0.017,bottom 6%), even though both have similarly high citation counts.This distinction, invisible to citation metrics alone, underscores the D-index's ability to differentiate truly novel work from cumulative efforts.</p>
<p>Despite widespread interest, efforts to expand and replicate the D-index have been hindered by limited theoretical clarity, lack of large-scale open data, and inconsistent implementation.For example, a study of historical D-index trends using Web of Science data reported a consistent decline over the past six decades (Park et al., 2023).Replication attempts using alternative data sources-without access to Web of Science-found that the decline is much smaller after removing zero-reference works, raising concerns about whether the original finding was an artifact (Holst et al., 2024).However, the dataset Holst et al. used (Z. Lin et al., 2023) contained roughly three times more zero-reference records than the original study, as it combined journal articles-which typically include references-with other scholarly works, such as book chapters, which often do not.Recent analyses confirm that Park et al.'s findings remain robust even after excluding zero-reference items (Park et al., 2025).</p>
<p>A second example concerns the relationship between team size and the D-index.Using Web of Science data, Wu et al. (2019) reported a consistent negative association between team size and disruption.In contrast, Petersen, Arroyave, andPammolli (2024, 2025), using Microsoft Academic data, found a positive marginal effect after controlling for covariates, raising the possibility that the earlier findings were influenced by untested confounders.However, while Wu et al. used the longest available time window in calculating the D-index, the replication relied on a fixed 5-year window, which may bias results in favor of large teams.Since small teams often take longer to accumulate citations-acting as "sleeping beauties"-short windows can understate their disruptive impact.Reproducing Petersen et al.'s results using the same model, data, and software, Wu et al. confirmed the positive effect under a 5-year window, but showed that the negative association reemerges when extending the window, with a clear turning point at 10 years (Lin et al., 2025).</p>
<p>Without a clear understanding of its theoretical foundations or standardized practices for datasets and code, the D-index risks remaining a fun intellectual toy model rather than a practical tool for research evaluation and policy-making.This raises a central question: what does the D-index actually measure?</p>
<p>In this paper, we position the D-index as a foundational metric for identifying breakthrough research.Contrary to the common belief that it measures absolute innovation (Park et al., 2023;Wu et al., 2019), we show that the D-index captures relative innovation-a paper's ability to displace its most-cited reference.In doing so, it reflects scientific progress as the replacement of older answers with better ones to the same fundamental question-much like light bulbs replacing candles.We support this insight through mathematical analysis, expert surveys, and large-scale bibliometric evidence.To facilitate replication, verification, and broader use, we release a dataset of D-index values for 49 million journal articles (1800-2024) based on OpenAlex.</p>
<p>The Disruption Index Captures Displacement Between a Paper and its Most Cited Reference</p>
<p>Calculating the D-index begins by classifying all subsequent papers citing a focal paper p into three types: those that cite p but not its references (type i), those that cite both p and its references (type j), and those that cite only its references but not p (type k).The D-index is then defined as the difference between the number of type i and type j papers, normalized by the total number of type i, j, and k papers (Funk &amp; Owen-Smith, 2017;Wu et al., 2019).This formulation captures the extent to which a focal paper displaces its predecessors in subsequent literature.The formula is shown in Eq. 1, with N denoting the number of each type of paper.A simplified illustration is provided in Fig. 1.  (Wu et al., 2019).</p>
<p>Through years of investigation, we find that the D-index captures displacement between a paper and its most cited reference.This is because to achieve a high D-index, a paper must compete against its references for future citations, primarily against the most cited reference due to the long-tail distribution of citation impacts among these references.We can see this insight by rearranging Eq. 1:</p>
<p>Eq. ( 2)
𝐷 𝑝 = 𝑁 𝑖 − 𝑁 𝑗 ( ) / 𝑁 𝑖 + 𝑁 𝑗 ( ) 1+𝑁 𝑘 / 𝑁 𝑖 + 𝑁 𝑗 ( ) ≈ 1 1+𝑅 𝑘 𝑑 𝑝
Here, we decompose the D-index into two terms, d p and R k .d p = (N i -N j )/(N i +N j ) is a "local" measure reflecting the focal paper p's intrinsic innovative level based on the two types of citing papers of it.The other term, R k = N k /(N i +N j ), approximates C total /(N i +N j ), where N k (the exclusive citations to the references) serves as a proxy for the total citations to the references C total .This approximation holds because shared citations between the focal paper and its references are generally an order of magnitude smaller than total reference citations.As a result, R k quantifies the ratio of citation impact between the focal paper's references and the focal paper itself.</p>
<p>Notably, this competition primarily occurs between the focal paper and its most cited reference, as reference citations follow Zipf's law:</p>
<p>Eq. ( 3)
𝐶 𝑟 ∝ 𝑐 1 (𝑏+𝑟) 𝑎
where c is a constant, rank r represents the decreasing rank of the reference by citations, and C r denotes the citation impact of the corresponding reference.Parameter a indicates how unequal the citation distribution is, and parameter b is a fitted constant.Empirical results are shown in Fig. 2. To quantify the dominance of the most-cited reference (r = 1) over the total citations to all references, we calculate the following ratio:</p>
<p>Eq. ( 4)
𝐶 𝑚𝑎𝑥 / 𝐶 𝑡𝑜𝑡𝑎𝑙 = 𝑐 1 (𝑏+1) 𝑎 / 𝑟 = 1 𝑁 ∑ 𝑐 1 (𝑏+𝑟) 𝑎
where C max represents the citation count of the most cited reference, and C total is the total number of citations across all references.Note that C total can also be expressed as:</p>
<p>Eq. ( 5)
𝐶 𝑡𝑜𝑡𝑎𝑙 = 𝑟 = 1 𝑁 ∑ 𝑐 1 (𝑏+𝑟) 𝑎 ≈ 𝑐 1 𝑁 ∫ 𝑑𝑥 (𝑏+𝑥) 𝑎 ≈ 𝑐( (𝑏+𝑁) 1−𝑎 1−𝑎 − (𝑏+1) 1−𝑎 1−𝑎 )
Thus, the ratio C max /C total simplifies to:</p>
<p>Eq. ( 6)
𝐶 𝑚𝑎𝑥 / 𝐶 𝑡𝑜𝑡𝑎𝑙 ≈ 1 (𝑏+1) 𝑎 /( (𝑏+𝑁) 1−𝑎 1−𝑎 − (𝑏+1) 1−𝑎 1−𝑎 ) ≈ 1 (𝑏+1) 𝑎 / (𝑏+1) 1−𝑎 𝑎−1 ≈ 𝑎−1 1+𝑏
This is because a &gt;1 (Fig. 2c), so the exponent (1 - a) is negative, making (b + N) 1-a a rapidly shrinking term as N increases.As a result, its influence is negligible.Substituting the empirical values a = 2.0 and b = 1.4 into Eq.6, we estimate C max /C total 0.42, closely matching the ≈ empirical value of 0.40 and validating our analytical reasoning (Fig. 2e).</p>
<p>Given that C total 2.5C max we can rewrite Eq. 2 as: ≈ Eq. ( 7)
𝐷 𝑝 = 1 1+𝑅 𝑘 𝑑 𝑝 ≈ 1 1+𝐶 𝑡𝑜𝑡𝑎𝑙 / 𝑁 𝑖 + 𝑁 𝑗 ( ) 𝑑 𝑝 ≈ 1 1+2.5<em>𝐶 𝑚𝑎𝑥 / 𝑁 𝑖 + 𝑁 𝑗 ( ) 𝑑 𝑝 = 1 1+2.5</em>𝐶 𝑚𝑎𝑥 /𝐶 𝑝 𝑑 𝑝 where C p = N i +N j , or, Eq. (8) 𝐷 𝑝 ≈ 1 1+𝑏 𝑝 𝑑 𝑝
Where b p = C max /C p .Eq. 8 shows that the D-index is primarily determined by two variables, d p and b p .We define these terms, discuss their interpretations, and present their empirical values below.</p>
<p>The local displacement factor, d p = N i /C p -N j /C p = p i -p j , measures the disparity between the probability of two types of citing papers, those that cite only the focal paper p and those that cite it along with its references.This metric reflects the intrinsic innovativeness of the focal paper.If d p &gt; 0, the focal paper tends to undermine the influence of prior work; if d f &lt; 0, it consolidates and enhances previous contributions; and if d f = 0, it is neutral.In our dataset, only 38% of papers are disruptive, with a median d p of -0.2 (Fig. 3a).</p>
<p>The knowledge burden factor, b p = C max /C p , represents the ratio of the most-cited reference's citation impact to that of the focal paper.The name draws from the "burden of knowledge" theory (Jones, 2009) and captures the challenge a paper faces in displacing or consolidating its most influential predecessor.A truly disruptive paper must surpass the impact of its most-cited reference-thus carrying little burden (b p &lt; 1).</p>
<p>In our dataset, however, most papers fall short of this standard: b p has a median value of 119, yielding 1/(1+b p ) ≈ 0.01 (Fig. 3b).This helps explain why most papers have D-index values close to zero-not because they lack the potential to displace or build upon prior work, but because their influence remains largely localized within a field rather than recognized across fields.As Newton famously said, great papers may "stand on the shoulders of giants"-but not all who stand on giants become giants themselves.Only a small fraction of papers-about 1%-manage to shed the burden of past knowledge and displace their predecessors (b p &lt; 1).</p>
<p>Combining these two factors yields a characteristic D-index value of -0.2 × 0.01 = -0.002(Fig. 3c), helping explain why most papers have a small negative D-index.Of course, this is a rough approximation-the actual median D-index is even smaller, at approximately -0.0001.This reflects how science typically progresses: through incremental contributions by the many, with only a few papers redefining the field.Notably, the average D-index for Nobel Prize-winning papers (N = 877, 1902-2009) is just 0.1 (Wu et al., 2019).</p>
<p>The Disruption Index Captures How New Answers Replace Old Ones to the Same Scientific Question</p>
<p>Given the difficulty of displacing science, how do high D-index papers emerge?Do they represent true breakthroughs, displacing their most-cited reference with genuine alternatives, or can the D-index be manipulated to obscure sources and claim intellectual credit across fields?</p>
<p>To investigate, we re-analyzed data from our 2019 global breakthrough survey (Wu et al., 2019).In 2019, we conducted an open-ended survey on identifying breakthrough research in science, performed in person, over the telephone, or using Skype, approved by the University of Chicago Institutional Review Board (IRB18-1248).The survey asked scholars across various fields to propose papers that either disrupt or consolidate science in their fields, using the following definitions: (a) Consolidating (labeled "developmental" in the survey) papers: Extensions or improvements of previous theory, methods, or findings; (b) Disruptive papers: Punctuated advances beyond previous theory, methods, or findings.</p>
<p>We provided respondents with examples like the BTW model (Bak et al., 1987) and Bose-Einstein condensation (Davis et al., 1995) papers to illustrate disrupting and consolidating papers.Respondents then proposed three to ten disruptive and developing papers.Our panel included scientists from ten prominent research-intensive institutions across the United States, China, Japan, France, and Germany, with training in mathematics, physics, chemistry, biology, medicine, engineering, computer science, psychology, and economics.</p>
<p>Among the 20 scholars who submitted 190 responses, all nominations for the most disruptive paper aligned with our measure, and all but six for the most consolidating paper did as well.The average D-index of papers nominated as disruptive is 0.21, placing them in the top 1% of most disruptive papers.The average D-index of papers nominated as consolidating is −0.011, placing them in the bottom 13%.This analysis yielded an overall area under the curve (AUC) of 0.83, indicating strong agreement between the D-index and expert judgment.</p>
<p>For the current research, we re-analyzed the data to select the top nominated papers from the survey and identified their most cited references, as presented in Table 1.In addition to the nominated papers from the global expert survey, we analyze a second set of extraordinary papers selected by Nature editors to celebrate the journal's 150th anniversary (10 Extraordinary Nature Papers, 2019), in order to enhance the representativeness of our dataset in capturing groundbreaking science.Notably, Watson and Crick's 1953 paper on the structure of DNA appears in both sets, reflecting broad consensus on its breakthrough significance.This set of Nature breakthrough papers is also highly disruptive, confirming the strong alignment between the D-index and expert judgment.</p>
<p>Our review of these two sets of breakthrough papers and their most-cited references reveals a consistent pattern: displacement within the same fundamental question, with newer work offering clearer or more powerful answers.In biology, for example, Watson and Crick's 1953 paper (Watson &amp; Crick, 1953) displaced Pauling and Corey's competing model (Pauling &amp; Corey, 1953) by introducing the double-helix structure of DNA-correcting the earlier triple-helix hypothesis and transforming molecular biology.In computer science, Turing's 1937 work (Turing, 1937) reframed Gödel's incompleteness theorems (Gödel, 1931) by introducing the Turing machine, laying the foundation for modern computation.</p>
<p>To examine whether the insight from case studies holds at scale, we quantified how often displacing papers (D &gt; 0) and their most-cited references share overlapping topics.OpenAlex journal articles are classified into an average of two fields (e.g., Discrete Mathematics, Molecular Biology, and Organic Chemistry) using a 292-category taxonomy (Sinha et al., 2015).We analyzed 49,077 high-impact (&gt;100 citations) and highly disruptive (D&gt;0.2) papers (1900-2020) alongside their most-cited references.If displacement were random, combinatorial calculations predict a 0.014 probability that these highly disruptive papers share a field with their most-cited reference (Eq.9).Yet, empirical analysis shows a 0.52 probability-37 times higher than expected.The high topic alignment between displacing papers and their top references suggests that breakthroughs often occur as purposeful innovations, rather than merely from the random recombination of prior knowledge (Weitzman, 1998).</p>
<p>Eq. ( 9)
𝑝 = 1 − 290 2 ( ) 292 2 ( ) ≈ 0. 014</p>
<p>Evaluating the Robustness of the Disruption Index</p>
<p>Our understanding of what the D-index actually measures-the displacing relationship between a focal paper and its most-cited reference-sheds light on ongoing debates about its technical complexity (Bentley et al., 2023;Holst et al., 2024;Leydesdorff et al., 2021;Macher et al., 2023), as well as its patterns and interpretation (Park et al., 2023).Below, we address recent concerns aimed at informing better decisions on these technical issues.</p>
<p>1 The D-index and Reference Length</p>
<p>As science advances, more papers are published every year, and each paper cites more prior papers.This phenomenon is called "citation inflation" due to its similarity with monetary inflation caused by an increase in the money supply in economic systems (Pan et al., 2018).A recent study raised concerns about whether this could confound the observed decline in the average value of the D-index of all papers (Park et al., 2023), and make the temporal analysis of the D-index challenging in general (Petersen et al., 2024(Petersen et al., , 2025)).The rationale is that the more references a focal paper includes, the less likely it is to have a high D-index, as it becomes increasingly difficult to eclipse all its references.If this reasoning holds, the D-index may converge toward zero as citation counts continue to inflate.</p>
<p>While this rationale helps highlight the growing knowledge burden faced by papers over time, it is not directly relevant to understanding the D-index.As we have shown, the true burden-captured by the knowledge burden factor b p -is not about the number of references but about the impact of the most-cited reference.In other words, the challenge lies not in how many prior works exist, but in which "giant" in the canonical literature the focal paper attempts to displace-not the era in which the paper is published.To better illustrate this point, we show that the D-index is independent of reference length after accounting for the local displacement index (d p ) and the burden factor (b p ). See Fig. 4. We acknowledge that the analysis here is based on approximations.In research evaluation, when assessing the overall innovation performance of a collection of papers, focusing on the sign of the D-index rather than its average value can help minimize the influence of reference length (Petersen et al., 2024).</p>
<p>2 The D-index and Citation Window Length.</p>
<p>The D-index has a life cycle: it changes as the focal paper and its references receive more citations and stabilizes when citation growth ceases (Bornmann &amp; Tekles, 2019;Lin et al., 2022).Therefore, to calculate a stabilized D-index, the citation window, i.e., the time window for analyzing subsequent citations to the focal paper, must not be too short.Recent studies have used a five-year citation window (Park et al., 2023;Petersen et al., 2024Petersen et al., , 2025), but we do not recommend this approach, as the D-index may take ten years or more to fully stabilize-especially for disruptive papers, which tend to accumulate recognition more slowly than consolidating ones (Bornmann &amp; Tekles, 2019;Lin et al., 2022).</p>
<p>The short, five-year citation window has caused issues; for example, recent research observed the positive marginal effect of team size on the D-index while accounting for various confounders (Petersen et al., 2024(Petersen et al., , 2025)), contrary to previous reports (Wu et al., 2019).This is because it takes a longer time for small teams to accumulate citations compared to large teams.As a result, a short time window underestimates the D-index of small teams (see Extended Data Fig. 7 in previous research (Wu et al., 2019)).Here we show in Fig. 5 that with an extended citation window, the negative effect of team size on the D-index reported in (Wu et al., 2019) is recovered using the same model suggested by (Petersen et al., 2024).</p>
<p>Eq. ( 10 Eq. 10 controls for the temporal change of the D-index using yearly fixed effects, denoted by D t .</p>
<p>The results of the ordinary least squares (OLS) estimation, conducted using the STATA 13.0 package reg, are shown in Fig. 5.These results are based on a dataset of 1.7 million papers with 1 ≤ k f ≤ 10 coauthors, 5 ≤ r f ≤ 50 references, and 10 ≤ c f ≤ 1000 citations, following the same parameters as in (Petersen et al., 2024(Petersen et al., , 2025)).The independent variables are modeled using a logarithmic transform due to their right-skewed distribution.The marginal effects of team size on the D-index are calculated with all other covariates held at their mean values.This figure is reproduced from our early research, where we also controlled for cohort effects and found that the time window effect remains consistent (Lin et al., 2025).</p>
<p>3 The D-index's Discriminative Power and Emerging Alternatives</p>
<p>Figure 6.Alternative versions of D-index.The figure is reproduced from our earlier research (Wu et al., 2019).(a) A simplified citation network comprising focal papers (diamonds), references (circles), and subsequent work (rectangles).Subsequent work may cite (1) only the focal work (i, green), (2) only its references (k, black), or (3) both focal work and references (j, brown).A reference identified as popular is colored in red, and self-citations are shown by dashed lines (with corresponding subsequent work colored in light brown).(b) Five definitions of the D-index are provided for comparison.D 0 is the definition used in the main text.D 1 is defined the same way as D 0 , but with self-citations excluded.D 2 is defined the same way as D 0 but only considers popular references.In the empirical analysis, we identified references as popular that received citations within the top quartile of the total citation distribution (≥24 citations).D 3 simplifies D 0 by only measuring the fraction of papers that cite the focal paper and not its references, among all papers citing the focal paper.D 4 is similar to D 3 but weighted by the number of citations.For example, if a single referenced paper is cited five times, then it receives a count of five rather than one.(c) All alternative measures to the D-index decrease consistently with team size.D 0 and D 1 are indexed by the right y-axis and other disruption measures are indexed by the left y-axis.100,000 randomly selected Web of Science papers (97,188 papers remained after excluding missing data) are used to calculate these values.</p>
<p>Recent research has raised concerns about the D-index's discriminative power, particularly because its numerator is bounded while its denominator is unbounded (Eq.1).This formulation tends to produce values close to zero, potentially undermining its discriminative power (Petersen et al., 2024).To address this issue, some studies have explored alternative versions of the D-index (Bornmann et al., 2020).The analysis in the previous sections has clarified why the D-index is typically small, and we now highlight the unique value of the original D-index for two key reasons.</p>
<p>Eq. ( 8)
𝐷 𝑝 ≈ 1 1+𝑏 𝑝 𝑑 𝑝
First, the original D-index is highly effective in identifying revolutionary work.Our decomposition of the D-index into a local displacement factor (d p ) and a knowledge burden factor (b p ) explains why most D-index values cluster near zero: the majority of papers cite canonical literature with much higher citation impact.While some scholars view this as a limitation, it actually highlights a small subset of papers (b p &lt; 1, less than 1%) whose role in displacing or consolidating their most-cited reference is substantial.These papers stand apart from the vast majority, whose influence remains minimal in comparison-regardless of whether their contribution is disruptive or consolidating (as indicated by the sign of d p ).</p>
<p>For example, given a local displacement index d p = 0.5, maintaining this displacing effect globally is increasingly difficult.If the focal paper has the same citation impact as its top reference (b p =1), which is already rare, D f is reduced to 0.5/2= 0.25 (see Eq. 8).If the focal paper has twice the citation impact of its top reference (b p =0.5), D p reduces to 0.5/1.5=0.33.Only when the focal paper has ten times the citation impact of its top reference (b p =0.1), D p is largely preserved: 0.5/1.1=0.45.However, such occurrences are extremely rare and may only happen a couple of times in a decade (0.03%), especially if the top reference is canonical literature.In other words, when a highly positive D-index is observed, it means the idea presented by the paper not only substitutes its top reference but is also well-recognized by the field and beyond.Therefore, if the goal is to identify paradigm-shifting work in the history of science, the original D-index (D p ) has a higher discriminative power.If the goal is only to identify different contributions within the scope of normal science, the local displacement index (d p ) is more effective.</p>
<p>Second, alternative versions of the D-index exhibit similar behaviors.For example, a recent study stated that "the results of a factor analysis show that the different variants measure similar dimensions" (Bornmann et al., 2020).Our previous research (Wu et al., 2019) has also considered five versions of the D-index, all of which demonstrated consistent correlation with another variable, team size (Fig. 6).This includes a version (D 3 ) that has excluded self-citations, which may, therefore, ease the concern raised in (Petersen et al., 2024).</p>
<p>Additionally, recent research proposed that the original may not be accurate and should be weighted by the number of citations to account for both the magnitude and reach of high D-index papers (Bentley et al., 2023).This is similar to the weighted version (D 4 ) presented in Fig. 6.</p>
<p>The D-index and Betweenness Centrality in Citation Networks</p>
<p>Recent literature suggests that the D-index is a specific form of node centrality in citation networks: betweenness (Gebhart &amp; Funk, 2023).Betweenness centrality measures how often a node appears on the shortest paths between other nodes, indicating its role as a bridge within the network (Freeman, 1977).While we agree with this topological interpretation, we would like to emphasize that it should not confuse the originality and meaning of the D-index.</p>
<p>First, using node centrality to measure paper importance has a long history (Price, 1965), inspiring the PageRank algorithm in information retrieval (Brin &amp; Page, 1998).However, to our knowledge, these network measures rarely leverage the hidden time dimension as the D-index does.The D-index uniquely captures this temporal dimension, highlighting papers with high values as "gatekeepers in time" or "structural holes in time."</p>
<p>Second, interpreting the D-index as merely betweenness centrality in networks risks focusing on the strategic advantage of high D-index papers as "knowledge brokers" and ignoring their inherent intellectual contributions.While being a knowledge broker in social networks often reflects social capital advantages (Burt, 2004;Granovetter, 1973), attaining this role in citation networks-especially across time-is hard-earned.For example, in our Breakthrough Papers Dataset, the 1998 small-world paper by Duncan Watts and Steve Strogatz (Watts &amp; Strogatz, 1998) displaced Stanley Milgram's 1967paper (Milgram, 1967) (see Table 1).It is an oversimplification to assume that subsequent citations of Watts and Strogatz were simply due to ignorance of Milgram's work, especially considering that researchers are actually trained to discover and cite the original literature.Based on our interview with experts, the high D-index of Watts and Strogatz correctly reflects its radical advancement from Milgram's work, by providing a novel mathematical framework to quantify the small-world phenomenon beyond social networks.</p>
<p>The Asymmetry of the D-index Distribution</p>
<p>Regarding the interpretation of the D-index, it is important to note that papers with a negative D can also significantly contribute to science, as described by the term "consolidation," hence the name "CD-index" (Funk &amp; Owen-Smith, 2017).For example, Wolfgang Ketterle et al.'s paper on Bose-Einstein Condensation (Davis et al., 1995), which validated the theory proposed by Albert Einstein and Satyendra Nath Bose through lab experiments, has a D=-0.58 (bottom 3% among all papers) (Wu et al., 2019).Ketterle won the Nobel Prize in Physics in 2001 for this work.Another example is the 2001 human genome paper (Lander et al., 2001), an important milestone in genomic research resulting from massive international collaboration (D = -0.017,bottom 6%).Both of these works consolidate revolutionary scientific ideas-the Einstein-Bose condensation theory and the DNA structure-rather than displacing them, yet they still represent fundamental progress in science.</p>
<p>The asymmetric distribution of the D-index (62% D 0) suggests that consolidating innovation ≤ is the norm in science.This pattern differs from that in technology, where more patents have a positive D-index (62%) than a negative one (38%), based on open-source data we published (Y.Lin et al., 2023).It would be interesting to explore whether this reflects fundamental differences in the level of path dependency between science and technology.</p>
<p>Opening the D-index Data for 49 Million OpenAlex Journal Articles</p>
<p>A recent study raised concerns about missing data affecting the calculation of the D-index (Holst et al., 2024).Accurate D-index calculation relies on the complete retrieval of paper references.Missing values in paper references can distort the D-index.For example, a paper with references but no citations results in a zero D-index.This can be misleading as papers with many citations but evenly split between subsequent papers that also cite its reference or disregarding them also result in D = 0. Similarly, papers with citations but no references may misleadingly result in a zero D-index.This can distort the explanation of results, as our previous analysis shows that achieving a high positive D-index is very difficult and a rare event in the real world.</p>
<p>These issues can affect the D-index itself and skew downstream analyses correlating D with other variables, especially if missing data is unevenly distributed.To ensure the D-index is as reliable as possible, we only include papers with one or more references and citations in our analysis in this manuscript and previous studies (Lin et al., 2022(Lin et al., , 2023;;Wu et al., 2019).After all, the D-index measures intellectual contribution based on citation practices, reflecting how a focal work relates to preceding major ideas as determined by the subsequent papers.Without reference or citation data, this analysis is meaningless.</p>
<p>We also recommend focusing on one type of scholarly work at a time in calculating the D-index rather than mixing journals, conferences, theses, books, or essays, which have different citation practices and could affect D-index interpretation as in (Z.Lin et al., 2023).In our previous studies, we typically focused on journal articles to minimize issues from varying citation practices.This is because citation practices are more established for academic journals, and the peer-review mechanism further serves as quality control for these norms.Indeed, without an open and scalable infrastructure, the D-index risks remaining an intellectual toy model rather than a practical tool for research evaluation and policy-making.To bridge this gap and facilitate replication, validation, and broader use, we release a dataset of D-index values for 49 million journal articles (1800-2024) based on OpenAlex.The dataset is available at [https://dataverse.harvard.edu/dataverse/OpenAlex_D_index],along with the code and workflow used to generate it from the raw OpenAlex data.</p>
<p>Conclusion and Discussion</p>
<p>In conclusion, we suggest that the D-index measures how a new idea displaces older ones while addressing similar questions or phenomena (Small, 1978).Rather than assessing the inventive level of a single paper in isolation, the D-index captures the displacement relationship between a focal paper and its most-cited reference.This perspective emphasizes the continuity and evolution of scientific knowledge, highlighting historical trajectories rather than sudden ruptures.While "Disruption" remains a compelling name, it may be more accurate to interpret the D-index as a Displacement Index.We hope this clarification improves understanding of the metric and promotes its more appropriate use in research evaluation (Leibel &amp; Bornmann, 2024).</p>
<p>This study, along with the released D-index dataset, has the potential for a significant and lasting impact on research evaluation, funding policy, and open science.First, it empowers scholars, institutions, and media worldwide with a free and accessible dataset for identifying breakthrough papers.Second, it supports the Science of Science community by providing an open, regularly updated D-index dataset based on OpenAlex-the largest open scholarly database globally-ensuring a robust and sustainable infrastructure for evaluating innovation.Third, it informs the design of new funding mechanisms by collaborating with agencies and policymakers to translate D-index insights into practical evaluation strategies, starting with discussions at our planned policy-focused workshop.Together, these efforts promote transparency, accessibility, and a deeper understanding of breakthrough science.</p>
<p>Figure 1 .
1
Figure 1.Simplified illustration of D-index.The figure is reproduced from our earlier research(Wu et al., 2019).</p>
<p>Figure 2 .
2
Figure 2. Zipf's law of reference citation impacts.We randomly selected 1,000 OpenAlex journal papers from 1900 to 2020 with three or more references.This reference length threshold ensures accurate parameter estimation.(a) Citation impact of a paper's references plotted by decreasing rank (blue dots), with a Zipf's law fit overlaid (red line).The estimated parameters are a = 1.89 and b = 0.60.(b) The distribution of the reference lengths, with an average of 29.(c) The distribution of the estimated a values for the 1,000 papers; 99.3% of them are all greater than 1, with an average of 2. (d) The distribution of the estimated b values, with an average of 1.4.(e) The distribution of empirical values of C max /C total , with an average of 0.40 (red solid line), closely matching the theoretical prediction of 0.42 (red dashed line).</p>
<p>Figure 3 .
3
Figure 3. Decomposing the D-index.The distribution of the local displacement index (a) and the knowledge burden factor (b) for 22 million OpenAlex journal papers with ten or more citations.This citation threshold ensures sufficient variation in the data.With too few citations, the variables-being ratios of natural numbers-take on only a few discrete values, making it difficult to observe continuous change.Among these papers, 65% have negative d p , 32% have positive d p , and 3% have a d p = 0.For the knowledge burden factor, 98.9% of papers have b p &gt;1, 1% have b p &lt;1, and 0.1% have b p =1.The dominance of negative d p (56%) and b p greater than one (98.8%)remains highly consistent when we include all papers with at least one citation and one reference.(c) The distribution of the D-index across 42 million OpenAlex journal articles (1900 -2020) with one or more citations and references.38% of these papers have a positive D-index, while 62% have zero or negative values.</p>
<p>Figure 4 .
4
Figure 4. D-index is independent of reference length after accounting for d f and b f .We select 929,900 papers with an average local displacing index (d p ) of 0.01 and ten or more citations, with values ranging from 0 to 0.05.We then further divided them into subgroups based on the burden factor, including b p =1 (N = 1,022), b p =10 (N = 15,682 papers), and b p =100 (N = 16,706).The empirical values of the D-index for these papers align with their theoretical predictions.</p>
<p>Figure 5 .
5
Figure 5.The negative impact of team size on the D-index is recovered with long-term citations.To examine how citation window length moderates the relationship between team size and the D-index, we analyzed six annual cohorts of papers, each receiving citations from subsequent papers published through 2020.Our dataset includes 47,129 papers from 2019, 271,496 from 2017, 444,675 from 2015, 536,463 from 2010, 344,582 from 2000, and 226,358 from 1995, corresponding to citation windows of 1, 3, 5, 10, 20, and 25 years, respectively.The regression coefficients (slopes) estimated from Eq. 10 are presented, with marginal effects calculated while holding all other covariates at their mean values.Light green confidence intervals are shown around the regression lines.This figure is reproduced from our early research, where we also controlled for cohort effects and found that the time window effect remains consistent(Lin et al., 2025).</p>
<p>Table 1 . Breakthrough Papers Nominated in a Global Expert Survey and Their Most-Cited References (highlighted in gray). Table 2. Breakthrough Papers Selected for Nature's 150th Anniversary and Their Most-Cited References (highlighted in gray).
1
Acknowledgments.We are grateful for support from the National Science Foundation grant SOS: DCI 2239418 (L.W.).
extraordinary Nature papers. 2019. November 4</p>
<p>Is disruption decreasing, or is it accelerating. Alexander Bentley, R Valverde, S Borycz, J Vidiella, B Horne, B D Duran-Nebreda, S O'brien, M J , 2023In arXiv [cs.DL</p>
<p>Self-organized criticality: an explanation of 1/f noise. P Bak, C Tang, K Wiesenfeld, Physical Review Letters. 593811987</p>
<p>Working Paper Series). J Bhattacharya, M Packalen, 10.3386/w26752Stagnation and Scientific Incentives. National Bureau of Economic Research2020. 26752</p>
<p>Are disruption index indicators convergently valid? The comparison of several indicator variants with assessments by peers. L Bornmann, S Devarakonda, A Tekles, G Chacko, Quantitative Science Studies. 132020</p>
<p>Disruption index depends on length of citation window. L Bornmann, A Tekles, 10.3145/epi.2019.mar.07Profesional de La Información. 2822019</p>
<p>The anatomy of a large-scale hypertextual Web search engine. Computer Networks and ISDN Systems. S Brin, L Page, 199830</p>
<p>Structural Holes and Good Ideas. R S Burt, The American Journal of Sociology. 11022004</p>
<p>Slowed canonical progress in large fields of science. J S G Chu, J A Evans, 2021118e2021636118Proceedings of the National Academy of Sciences of the United States of America</p>
<p>Bose-Einstein condensation in a gas of sodium atoms. K B Davis, M Mewes, M R Andrews, N J Van Druten, D S Durfee, D M Kurn, W Ketterle, Physical Review Letters. 75221995</p>
<p>C Franzoni, P Stephan, R Veugelers, Funding risky research. Entrepreneurship and Innovation Policy and the Economy. 20221</p>
<p>A Set of Measures of Centrality Based on Betweenness. L C Freeman, Sociometry. 4011977</p>
<p>A dynamic network measure of technological change. R J Funk, J Owen-Smith, Management Science. 6332017</p>
<p>A Mathematical Framework for Citation Disruption. T Gebhart, R Funk, arXiv [cs.SI]. arXiv2023</p>
<p>K Gödel, Über formal unentscheidbare Sätze der Principia Mathematica und verwandter Systeme I. Monatshefte fur Mathematik. 1931</p>
<p>The Strength of Weak Ties. M S Granovetter, The American Journal of Sociology. 7861973</p>
<p>Dataset Artefacts are the Hidden Drivers of the Declining Disruptiveness in Science. V Holst, A Algaba, F Tori, S Wenmackers, V Ginis, arXiv [cs.DL2024</p>
<p>The Burden of Knowledge and the "Death of the Renaissance Man": Is Innovation Getting Harder? The Review of Economic Studies. B F Jones, 200976</p>
<p>Initial sequencing and analysis of the human genome. E S Lander, … International Human Genome Sequencing Consortium.L M Linton, … International Human Genome Sequencing Consortium.B Birren, … International Human Genome Sequencing Consortium.C Nusbaum, … International Human Genome Sequencing Consortium.M C Zody, … International Human Genome Sequencing Consortium.J Baldwin, … International Human Genome Sequencing Consortium.K Devon, … International Human Genome Sequencing Consortium.K Dewar, … International Human Genome Sequencing Consortium.M Doyle, … International Human Genome Sequencing Consortium.W Fitzhugh, … International Human Genome Sequencing Consortium.R Funke, … International Human Genome Sequencing Consortium.D Gage, … International Human Genome Sequencing Consortium.K Harris, … International Human Genome Sequencing Consortium.A Heaford, … International Human Genome Sequencing Consortium.J Howland, … International Human Genome Sequencing Consortium.L Kann, … International Human Genome Sequencing Consortium.J Lehoczky, … International Human Genome Sequencing Consortium.R Levine, … International Human Genome Sequencing Consortium.P Mcewan, … International Human Genome Sequencing Consortium.Nature. 40968222001</p>
<p>What do we know about the disruption index in scientometrics? An overview of the literature. C Leibel, L Bornmann, Scientometrics. 12912024</p>
<p>A proposal to revise the disruption indicator. L Leydesdorff, A Tekles, L Bornmann, 10.3145/epi.2021.ene.21Profesional de La Información. 3012021</p>
<p>New directions in science emerge from disconnection and discord. Y Lin, J A Evans, L Wu, Journal of Informetrics. 1611012342022</p>
<p>Remote collaboration fuses fewer breakthrough ideas. Y Lin, C B Frey, L Wu, Nature. 62379892023</p>
<p>Team size and its negative impact on the Disruption Index. Y Lin, L Li, L Wu, arXiv [cs.SI]. arXiv2025</p>
<p>SciSciNet: A large-scale open data lake for the science of science research. Z Lin, Y Yin, L Liu, D Wang, Scientific Data. 1013152023</p>
<p>J T Macher, C Rutzer, R Weder, arXiv [econ.GN]. arXivThe Illusive Slump of Disruptive Patents. 2023</p>
<p>The small world problem. S L Milgram, Psychology Today. 21967</p>
<p>The memory of science: Inflation, myopia, and the knowledge network. R K Pan, A M Petersen, F Pammolli, S Fortunato, Journal of Informetrics. 1232018</p>
<p>Papers and patents are becoming less disruptive over time. M Park, E Leahey, R J Funk, Nature. 79422023</p>
<p>Robust evidence for declining disruptiveness: Assessing the role of zero-backward-citation works. M Park, E Leahey, R J Funk, arXiv [cs.SI2025</p>
<p>A proposed structure for the nucleic acids. L Pauling, R B Corey, 195339Proceedings of the National Academy of Sciences of the United States of America</p>
<p>The disruption index suffers from citation inflation: Re-analysis of temporal CD trend and relationship with team size reveal discrepancies. A M Petersen, F J Arroyave, F Pammolli, Journal of Informetrics. 1911016052025</p>
<p>The disruption index is biased by citation inflation. A M Petersen, F Arroyave, F Pammolli, Quantitative Science Studies. 2024</p>
<p>NETWORKS OF SCIENTIFIC PAPERS. D J Price, Science. 36831965</p>
<p>An Overview of Microsoft Academic Service (MAS) and Applications. A Sinha, Z Shen, Y Song, H Ma, D Eide, B.-J Hsu, ) Paul, K Wang, Proceedings of the 24th International Conference on World Wide Web. the 24th International Conference on World Wide Web2015</p>
<p>Cited Documents as Concept Symbols. H G Small, Social Studies of Science. 831978</p>
<p>On computable numbers, with an application to the entscheidungsproblem. A M Turing, Proceedings of the London Mathematical Society. Third Series. 11937</p>
<p>Molecular structure of nucleic acids; a structure for deoxyribose nucleic acid. J D Watson, F H Crick, Nature. 17143561953</p>
<p>Collective dynamics of "small-world" networks. D J Watts, S H Strogatz, Nature. 39366841998</p>
<p>Recombinant growth. M L Weitzman, The Quarterly Journal of Economics. 11321998</p>
<p>Large teams develop and small teams disrupt science and technology. L Wu, D Wang, J A Evans, Nature. 56677442019</p>            </div>
        </div>

    </div>
</body>
</html>
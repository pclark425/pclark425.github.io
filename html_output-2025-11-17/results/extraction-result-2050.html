<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2050 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2050</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2050</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-50.html">extraction-schema-50</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <p><strong>Paper ID:</strong> paper-277066529</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.12923v1.pdf" target="_blank">Lifelong Reinforcement Learning with Similarity-Driven Weighting by Large Models</a></p>
                <p><strong>Paper Abstract:</strong> Lifelong Reinforcement Learning (LRL) holds significant potential for addressing sequential tasks, but it still faces considerable challenges. A key difficulty lies in effectively preventing catastrophic forgetting and facilitating knowledge transfer while maintaining reliable decision-making performance across subsequent tasks in dynamic environments. To tackle this, we propose a novel framework, SDW (Similarity-Driven Weighting Framework), which leverages large-language-model-generated dynamic functions to precisely control the training process. The core of SDW lies in two functions pre-generated by large models: the task similarity function and the weight computation function. The task similarity function extracts multidimensional features from task descriptions to quantify the similarities and differences between tasks in terms of states, actions, and rewards. The weight computation function dynamically generates critical training parameters based on the similarity information, including the proportion of old task data stored in the Replay Buffer and the strategy consistency weight in the loss function, enabling an adaptive balance between learning new tasks and transferring knowledge from previous tasks. By generating function code offline prior to training, rather than relying on large-model inference during the training process, the SDW framework reduces computational overhead while maintaining efficiency in sequential task scenarios. Experimental results on Atari and MiniHack sequential tasks demonstrate that SDW significantly outperforms existing lifelong reinforcement learning methods.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2050.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2050.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SDW</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Similarity-Driven Weighting Framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A lifelong reinforcement learning framework that uses LLM-generated functions (task similarity and weight computation) produced offline to dynamically adjust replay-buffer sampling and a strategy-consistency loss, balancing retention and adaptation across sequential tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>LLM-generated</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4o, GPT-3.5, GLM4-9B</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>The LLM is prompted offline to generate two functions: (1) a Task Similarity Calculation Function that maps task descriptions (objectives, state/action spaces, reward definitions) to a multidimensional similarity vector S = [S_state, S_action, S_reward, ...] with values in [0,1]; and (2) a Weight Computation Function that maps S to training parameters (w_buffer: target proportion of old-task data in the replay buffer; lambda_consistency: weight of strategy-consistency loss). During sequential training the pre-generated functions drive dynamic replay insertion probability and scale the consistency loss, effectively producing an adaptive curriculum over previously seen tasks without calling the LLM at runtime.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>MiniHack (NetHack-based sandbox) and Atari (ALE)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>MiniHack: procedurally generated maps, high task diversity (rooms, corridors, key-room, river, hide-and-seek, corridor-battle), sparse + binary rewards (+1 goal reward per task), randomized starting/goal positions, compositional subgoals, varying visibility/hazards; requires exploration, planning and memory. Atari: diverse arcade games with image observations, discrete action spaces, varied objectives and short-to-medium horizon gameplay; standardized benchmark for RL.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Replay buffer management (dynamic insertion probability), strategy-consistency loss term (regularization), standard policy optimization loss (L_policy); ablation studies test buffer-only and loss-only variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td>MiniHack (two-round, 15 tasks): SDW+GPT-4o: Task Performance P = -0.151, Forgetting F = -0.505, Transfer T = 0.290; SDW+GPT-3.5: P ≈ -0.160, F ≈ -0.096, T ≈ 0.194; SDW+GLM4-9B: P ≈ -0.152, F ≈ -0.264, T ≈ 0.134. Atari (three games, 3 rounds): SDW+GPT-4o: P = 965.46, F = -0.058, T = 0.124; SDW+GPT-3.5: P = 966.10, F = -0.045, T = 0.069; SDW+GLM4-9B: P = 933.61, F = -0.011, T = 0.078.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td>Baselines (no LLM-generated weighting): EWC (MiniHack) P = -0.216, F = 0.175, T = 0.217; CLEAR (MiniHack) P = -0.165, F = 0.055, T = -0.330; Atari baselines: EWC P ≈ 425.72, F = 0.113, T = -0.137; CLEAR P = 932.46, F = 0.054, T = -0.024.</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Measured by the paper's T metric: SDW variants show positive and larger T (e.g., 0.290 for SDW+GPT-4o on MiniHack) indicating improved transfer compared to baselines (some baselines negative or much smaller). No separate zero-shot/few-shot unseen-task generalization results reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>LLMs are used only offline once to generate functions, so runtime training avoids repeated LLM inference; exact API call counts, inference time, or monetary cost are not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Performance varies by LLM: GLM4-9B showed lower transfer in some setups; baseline methods (CLEAR, EWC) fail when tasks are highly dissimilar due to conflicting objectives. The approach depends on the quality of task descriptions and LLM-generated functions; no rigorous sensitivity analysis of prompt/function errors is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Ablations include: MG (LLM-Based Loss+Buffer Module Generation), PA (LLM-Based Parameter Assignment), SDW(Buffer only), SDW(Loss only), full SDW. Reported results (MiniHack): MG shows high forgetting (F = 0.580) and poor P; PA shows worst transfer (T = -0.939); SDW(Buffer only): P ≈ -0.164, F ≈ -0.260, T ≈ -0.066; SDW(Loss only): P ≈ -0.166, F ≈ -0.341, T ≈ 0.259; full SDW: P = -0.151, F = -0.505, T = 0.290. Authors conclude both buffer and loss components are necessary for best results.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td>Three LLMs tested: GPT-4o, GPT-3.5, GLM4-9B. Results indicate differences: GPT-4o often yields best transfer (T) and best overall balance on MiniHack; GPT-3.5 produced slightly higher Atari P in one reported figure but lower transfer than GPT-4o; GLM4-9B showed generally lower transfer. Exact parameter counts for GPT models not reported; GLM4-9B has '9B' in name.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Offline LLM-generated task-similarity and weight-computation functions produce an effective adaptive curriculum over tasks: they substantially reduce catastrophic forgetting and improve transfer compared to standard LRL baselines; both buffer control and strategy-consistency regularization are necessary (ablation shows one without the other degrades results); using LLMs offline reduces runtime compute while still capturing task relationships; higher-capability LLMs (GPT-4o) produce better transfer/retention in experiments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2050.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2050.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pourcel et al. autotelic agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Autotelic agent framework (Pourcel et al., 2024) referenced</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced work where an LLM is combined with goal-conditioned RL to generate open-ended goals: the LLM estimates goal difficulty/learnability and generates reward-function code and goal descriptions to optimize goal generation and learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>LLM-generated goals</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>LLM produces goal descriptions and reward-function code conditioned on context and estimated difficulty/learnability; used to drive goal-conditioned RL agents (autotelic learning).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Goal-conditioned RL / open-ended learning (general)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Open-ended goal space, requires goal difficulty estimation and adaptive goal sampling for curriculum; aims to facilitate autonomous discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Goal-conditioned RL, reward-code generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Referenced as not addressing catastrophic forgetting in multi-task lifelong learning according to this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Paper cites this work as showing LLMs can generate reward functions and goals and improve goal generation efficiency, but notes it does not address catastrophic forgetting in LRL.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2050.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2050.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LEAGUE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LEAGUE framework (Li et al., 2024) referenced</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced framework integrating LLMs with Task and Motion Planning and Deep RL: LLMs decompose tasks, create skill operators, and generate dense rewards to accelerate policy learning while maintaining a skill library.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>LLM-generated skill decomposition / dense rewards</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>LLM decomposes high-level tasks into skill operators and dense reward signals; maintains a skill library to support new task learning (implicit curriculum via skills and decomposition).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Robotic manipulation / Task & Motion Planning (general)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Requires skill composition, hierarchical planning, dense reward shaping to accelerate learning of complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Skill library, task decomposition, dense reward generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Cited as an example where LLMs generate task decomposition, skills, and rewards to accelerate policy learning; used as related work motivating LLM use in LRL.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2050.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2050.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mao 2024</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based planning to build skill graphs (Mao, 2024) referenced</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced dissertation/work where LLMs are used to select or generate skills and construct skill graphs to guide agents in complex tasks, deciding if new skills are necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>LLM-generated skill graph / planner</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>LLM acts as a planner to build skill graphs and select skills for agents, enabling structured progression through complex tasks (implicit curriculum via skill graph).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>General skill acquisition / open-world exploration (e.g., Minecraft mentioned in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Compositional skills, hierarchical dependencies, open-world exploration tasks requiring skill discovery and reuse.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Skill graph, skill library</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Mentioned as an approach where LLMs structure skill acquisition, relevant for curriculum-like organization of learning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2050.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2050.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tziafas & Kasaei</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Skill library expansion via LLMs (Tziafas and Kasaei, 2024) referenced</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced approach using LLMs to iteratively expand a robotic skill library: a 'wake' phase generates and validates tasks in simulation and a 'sleep' phase abstracts experiences into new skills and optimizes via memory replay.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>LLM-generated tasks/skills (alternating phases)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>LLM generates candidate tasks/skills in a wake phase and abstracts/optimizes them in a sleep phase, producing a continual curriculum of skills and tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Robotics skill acquisition</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Requires skill abstraction, validation in simulation, and memory replay for continual improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Skill library, memory replay</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Cited as related work demonstrating LLMs can generate and refine skills; relevant to curriculum generation via skill accumulation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2050.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2050.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Meng et al. (2024)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM semantic observation generator (Meng et al., 2024) referenced</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced work that uses LLM-derived semantic information embedded into the agent's observation/state to aid knowledge transfer and task learning, effectively augmenting the state rather than generating curricula.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>LLM-generated semantic state augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>LLM generates semantic hints/embeddings from task descriptions that are concatenated with observations to improve transfer; not an explicit curriculum generator but can shape learning.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>General RL tasks (semantic-augmented states)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Augmented state spaces increase complexity and compute; can facilitate transfer when semantics are informative.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td>Task descriptions encoded as language embeddings incorporated into state (per referenced paper summary).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>State-augmentation via language embeddings, SAC used in referenced work</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Referenced as increasing computational demand due to expanded state space and requiring frequent LLM inference during training.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Increased state dimensionality requires more training time and computational resources; continuous LLM inference in training is costly.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Paper notes semantic state augmentation helps transfer but that continuous LLM use during training increases compute and complexity.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2050.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2050.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chen et al. (unnumbered)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pretrained LLM-generated task hints and language-guided rewards (Chen et al.) referenced</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced study using pretrained LLMs (e.g., GPT-3.5) to generate task hints/content from textual descriptions that are embedded and used to generate language-guided rewards and drive policy adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>LLM-generated hints and reward shaping</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-3.5 (example in citation)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>LLM generates task hints and content from descriptions; embeddings combined with observations input to transformer policy; used to create language-guided rewards aiding adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Language-guided RL (general)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Uses textual hints to shape reward/learning; increases state/modal complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td>Task hints embedded into policy input as additional observation features.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Language embeddings, transformer policies, language-guided reward generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Noted that continuous pretrained LLM use during training increases computational demand (no precise cost reported).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Expanding state space and continuous LLM queries increase training time and require high-performance hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>LLM-generated hints and rewards can aid policy adaptation but at substantial computational cost when used during training.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2050.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2050.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LAMOL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Lifelong language learning with LAMOL (Sun et al., 2019) referenced</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referential method that uses language modeling to generate pseudo-samples of previous tasks to mitigate catastrophic forgetting in lifelong language learning; limited to language domain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>LLM-generated pseudo-sample replay (language-specific)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Generate pseudo-samples of previous tasks for joint training to reduce forgetting (applicable to language tasks but not general RL according to the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Lifelong language learning</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Language modeling domain; not directly applicable to embodied or RL tasks without adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Pseudo-sample generation, joint training</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Paper notes LAMOL is limited to language tasks and does not generalize well to robotics or games.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Shows pseudo-sample generation can reduce forgetting in language LLL but is not directly applicable to general RL domains.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>An autotelic agent framework that combines LLMs with goal-conditioned RL (Pourcel et al., 2024) <em>(Rating: 2)</em></li>
                <li>A Framework for LLM-based Lifelong Learning in Robot Manipulation (Mao, 2024) <em>(Rating: 2)</em></li>
                <li>LEAGUE <em>(Rating: 2)</em></li>
                <li>Meng et al. (semantic embeddings from LLMs for RL) <em>(Rating: 2)</em></li>
                <li>Tziafas and Kasaei (continuous skill library expansion with LLMs) <em>(Rating: 2)</em></li>
                <li>LAMOL <em>(Rating: 1)</em></li>
                <li>Chen et al. (LLM-generated task hints and language-guided rewards) <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2050",
    "paper_id": "paper-277066529",
    "extraction_schema_id": "extraction-schema-50",
    "extracted_data": [
        {
            "name_short": "SDW",
            "name_full": "Similarity-Driven Weighting Framework",
            "brief_description": "A lifelong reinforcement learning framework that uses LLM-generated functions (task similarity and weight computation) produced offline to dynamically adjust replay-buffer sampling and a strategy-consistency loss, balancing retention and adaptation across sequential tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generator_type": "LLM-generated",
            "llm_model_name": "GPT-4o, GPT-3.5, GLM4-9B",
            "llm_model_size": null,
            "curriculum_description": "The LLM is prompted offline to generate two functions: (1) a Task Similarity Calculation Function that maps task descriptions (objectives, state/action spaces, reward definitions) to a multidimensional similarity vector S = [S_state, S_action, S_reward, ...] with values in [0,1]; and (2) a Weight Computation Function that maps S to training parameters (w_buffer: target proportion of old-task data in the replay buffer; lambda_consistency: weight of strategy-consistency loss). During sequential training the pre-generated functions drive dynamic replay insertion probability and scale the consistency loss, effectively producing an adaptive curriculum over previously seen tasks without calling the LLM at runtime.",
            "domain_name": "MiniHack (NetHack-based sandbox) and Atari (ALE)",
            "domain_characteristics": "MiniHack: procedurally generated maps, high task diversity (rooms, corridors, key-room, river, hide-and-seek, corridor-battle), sparse + binary rewards (+1 goal reward per task), randomized starting/goal positions, compositional subgoals, varying visibility/hazards; requires exploration, planning and memory. Atari: diverse arcade games with image observations, discrete action spaces, varied objectives and short-to-medium horizon gameplay; standardized benchmark for RL.",
            "state_conditioning": false,
            "state_conditioning_details": null,
            "novelty_mechanism": false,
            "novelty_mechanism_details": null,
            "complementary_systems": "Replay buffer management (dynamic insertion probability), strategy-consistency loss term (regularization), standard policy optimization loss (L_policy); ablation studies test buffer-only and loss-only variants.",
            "performance_llm_curriculum": "MiniHack (two-round, 15 tasks): SDW+GPT-4o: Task Performance P = -0.151, Forgetting F = -0.505, Transfer T = 0.290; SDW+GPT-3.5: P ≈ -0.160, F ≈ -0.096, T ≈ 0.194; SDW+GLM4-9B: P ≈ -0.152, F ≈ -0.264, T ≈ 0.134. Atari (three games, 3 rounds): SDW+GPT-4o: P = 965.46, F = -0.058, T = 0.124; SDW+GPT-3.5: P = 966.10, F = -0.045, T = 0.069; SDW+GLM4-9B: P = 933.61, F = -0.011, T = 0.078.",
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": null,
            "performance_no_curriculum": "Baselines (no LLM-generated weighting): EWC (MiniHack) P = -0.216, F = 0.175, T = 0.217; CLEAR (MiniHack) P = -0.165, F = 0.055, T = -0.330; Atari baselines: EWC P ≈ 425.72, F = 0.113, T = -0.137; CLEAR P = 932.46, F = 0.054, T = -0.024.",
            "has_curriculum_comparison": true,
            "task_diversity_metrics": null,
            "transfer_generalization_results": "Measured by the paper's T metric: SDW variants show positive and larger T (e.g., 0.290 for SDW+GPT-4o on MiniHack) indicating improved transfer compared to baselines (some baselines negative or much smaller). No separate zero-shot/few-shot unseen-task generalization results reported.",
            "computational_cost": "LLMs are used only offline once to generate functions, so runtime training avoids repeated LLM inference; exact API call counts, inference time, or monetary cost are not reported.",
            "failure_modes_limitations": "Performance varies by LLM: GLM4-9B showed lower transfer in some setups; baseline methods (CLEAR, EWC) fail when tasks are highly dissimilar due to conflicting objectives. The approach depends on the quality of task descriptions and LLM-generated functions; no rigorous sensitivity analysis of prompt/function errors is provided.",
            "long_horizon_performance": null,
            "specialized_domain_performance": null,
            "ablation_studies": "Ablations include: MG (LLM-Based Loss+Buffer Module Generation), PA (LLM-Based Parameter Assignment), SDW(Buffer only), SDW(Loss only), full SDW. Reported results (MiniHack): MG shows high forgetting (F = 0.580) and poor P; PA shows worst transfer (T = -0.939); SDW(Buffer only): P ≈ -0.164, F ≈ -0.260, T ≈ -0.066; SDW(Loss only): P ≈ -0.166, F ≈ -0.341, T ≈ 0.259; full SDW: P = -0.151, F = -0.505, T = 0.290. Authors conclude both buffer and loss components are necessary for best results.",
            "model_size_scaling": "Three LLMs tested: GPT-4o, GPT-3.5, GLM4-9B. Results indicate differences: GPT-4o often yields best transfer (T) and best overall balance on MiniHack; GPT-3.5 produced slightly higher Atari P in one reported figure but lower transfer than GPT-4o; GLM4-9B showed generally lower transfer. Exact parameter counts for GPT models not reported; GLM4-9B has '9B' in name.",
            "key_findings_curriculum_effectiveness": "Offline LLM-generated task-similarity and weight-computation functions produce an effective adaptive curriculum over tasks: they substantially reduce catastrophic forgetting and improve transfer compared to standard LRL baselines; both buffer control and strategy-consistency regularization are necessary (ablation shows one without the other degrades results); using LLMs offline reduces runtime compute while still capturing task relationships; higher-capability LLMs (GPT-4o) produce better transfer/retention in experiments.",
            "uuid": "e2050.0"
        },
        {
            "name_short": "Pourcel et al. autotelic agent",
            "name_full": "Autotelic agent framework (Pourcel et al., 2024) referenced",
            "brief_description": "Referenced work where an LLM is combined with goal-conditioned RL to generate open-ended goals: the LLM estimates goal difficulty/learnability and generates reward-function code and goal descriptions to optimize goal generation and learning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "curriculum_generator_type": "LLM-generated goals",
            "llm_model_name": null,
            "llm_model_size": null,
            "curriculum_description": "LLM produces goal descriptions and reward-function code conditioned on context and estimated difficulty/learnability; used to drive goal-conditioned RL agents (autotelic learning).",
            "domain_name": "Goal-conditioned RL / open-ended learning (general)",
            "domain_characteristics": "Open-ended goal space, requires goal difficulty estimation and adaptive goal sampling for curriculum; aims to facilitate autonomous discovery.",
            "state_conditioning": null,
            "state_conditioning_details": null,
            "novelty_mechanism": null,
            "novelty_mechanism_details": null,
            "complementary_systems": "Goal-conditioned RL, reward-code generation",
            "performance_llm_curriculum": null,
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": null,
            "performance_no_curriculum": null,
            "has_curriculum_comparison": null,
            "task_diversity_metrics": null,
            "transfer_generalization_results": null,
            "computational_cost": null,
            "failure_modes_limitations": "Referenced as not addressing catastrophic forgetting in multi-task lifelong learning according to this paper.",
            "long_horizon_performance": null,
            "specialized_domain_performance": null,
            "ablation_studies": null,
            "model_size_scaling": null,
            "key_findings_curriculum_effectiveness": "Paper cites this work as showing LLMs can generate reward functions and goals and improve goal generation efficiency, but notes it does not address catastrophic forgetting in LRL.",
            "uuid": "e2050.1"
        },
        {
            "name_short": "LEAGUE",
            "name_full": "LEAGUE framework (Li et al., 2024) referenced",
            "brief_description": "Referenced framework integrating LLMs with Task and Motion Planning and Deep RL: LLMs decompose tasks, create skill operators, and generate dense rewards to accelerate policy learning while maintaining a skill library.",
            "citation_title": "",
            "mention_or_use": "mention",
            "curriculum_generator_type": "LLM-generated skill decomposition / dense rewards",
            "llm_model_name": null,
            "llm_model_size": null,
            "curriculum_description": "LLM decomposes high-level tasks into skill operators and dense reward signals; maintains a skill library to support new task learning (implicit curriculum via skills and decomposition).",
            "domain_name": "Robotic manipulation / Task & Motion Planning (general)",
            "domain_characteristics": "Requires skill composition, hierarchical planning, dense reward shaping to accelerate learning of complex tasks.",
            "state_conditioning": null,
            "state_conditioning_details": null,
            "novelty_mechanism": null,
            "novelty_mechanism_details": null,
            "complementary_systems": "Skill library, task decomposition, dense reward generation",
            "performance_llm_curriculum": null,
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": null,
            "performance_no_curriculum": null,
            "has_curriculum_comparison": null,
            "task_diversity_metrics": null,
            "transfer_generalization_results": null,
            "computational_cost": null,
            "failure_modes_limitations": null,
            "long_horizon_performance": null,
            "specialized_domain_performance": null,
            "ablation_studies": null,
            "model_size_scaling": null,
            "key_findings_curriculum_effectiveness": "Cited as an example where LLMs generate task decomposition, skills, and rewards to accelerate policy learning; used as related work motivating LLM use in LRL.",
            "uuid": "e2050.2"
        },
        {
            "name_short": "Mao 2024",
            "name_full": "LLM-based planning to build skill graphs (Mao, 2024) referenced",
            "brief_description": "Referenced dissertation/work where LLMs are used to select or generate skills and construct skill graphs to guide agents in complex tasks, deciding if new skills are necessary.",
            "citation_title": "",
            "mention_or_use": "mention",
            "curriculum_generator_type": "LLM-generated skill graph / planner",
            "llm_model_name": null,
            "llm_model_size": null,
            "curriculum_description": "LLM acts as a planner to build skill graphs and select skills for agents, enabling structured progression through complex tasks (implicit curriculum via skill graph).",
            "domain_name": "General skill acquisition / open-world exploration (e.g., Minecraft mentioned in related work)",
            "domain_characteristics": "Compositional skills, hierarchical dependencies, open-world exploration tasks requiring skill discovery and reuse.",
            "state_conditioning": null,
            "state_conditioning_details": null,
            "novelty_mechanism": null,
            "novelty_mechanism_details": null,
            "complementary_systems": "Skill graph, skill library",
            "performance_llm_curriculum": null,
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": null,
            "performance_no_curriculum": null,
            "has_curriculum_comparison": null,
            "task_diversity_metrics": null,
            "transfer_generalization_results": null,
            "computational_cost": null,
            "failure_modes_limitations": null,
            "long_horizon_performance": null,
            "specialized_domain_performance": null,
            "ablation_studies": null,
            "model_size_scaling": null,
            "key_findings_curriculum_effectiveness": "Mentioned as an approach where LLMs structure skill acquisition, relevant for curriculum-like organization of learning.",
            "uuid": "e2050.3"
        },
        {
            "name_short": "Tziafas & Kasaei",
            "name_full": "Skill library expansion via LLMs (Tziafas and Kasaei, 2024) referenced",
            "brief_description": "Referenced approach using LLMs to iteratively expand a robotic skill library: a 'wake' phase generates and validates tasks in simulation and a 'sleep' phase abstracts experiences into new skills and optimizes via memory replay.",
            "citation_title": "",
            "mention_or_use": "mention",
            "curriculum_generator_type": "LLM-generated tasks/skills (alternating phases)",
            "llm_model_name": null,
            "llm_model_size": null,
            "curriculum_description": "LLM generates candidate tasks/skills in a wake phase and abstracts/optimizes them in a sleep phase, producing a continual curriculum of skills and tasks.",
            "domain_name": "Robotics skill acquisition",
            "domain_characteristics": "Requires skill abstraction, validation in simulation, and memory replay for continual improvement.",
            "state_conditioning": null,
            "state_conditioning_details": null,
            "novelty_mechanism": null,
            "novelty_mechanism_details": null,
            "complementary_systems": "Skill library, memory replay",
            "performance_llm_curriculum": null,
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": null,
            "performance_no_curriculum": null,
            "has_curriculum_comparison": null,
            "task_diversity_metrics": null,
            "transfer_generalization_results": null,
            "computational_cost": null,
            "failure_modes_limitations": null,
            "long_horizon_performance": null,
            "specialized_domain_performance": null,
            "ablation_studies": null,
            "model_size_scaling": null,
            "key_findings_curriculum_effectiveness": "Cited as related work demonstrating LLMs can generate and refine skills; relevant to curriculum generation via skill accumulation.",
            "uuid": "e2050.4"
        },
        {
            "name_short": "Meng et al. (2024)",
            "name_full": "LLM semantic observation generator (Meng et al., 2024) referenced",
            "brief_description": "Referenced work that uses LLM-derived semantic information embedded into the agent's observation/state to aid knowledge transfer and task learning, effectively augmenting the state rather than generating curricula.",
            "citation_title": "",
            "mention_or_use": "mention",
            "curriculum_generator_type": "LLM-generated semantic state augmentation",
            "llm_model_name": null,
            "llm_model_size": null,
            "curriculum_description": "LLM generates semantic hints/embeddings from task descriptions that are concatenated with observations to improve transfer; not an explicit curriculum generator but can shape learning.",
            "domain_name": "General RL tasks (semantic-augmented states)",
            "domain_characteristics": "Augmented state spaces increase complexity and compute; can facilitate transfer when semantics are informative.",
            "state_conditioning": true,
            "state_conditioning_details": "Task descriptions encoded as language embeddings incorporated into state (per referenced paper summary).",
            "novelty_mechanism": null,
            "novelty_mechanism_details": null,
            "complementary_systems": "State-augmentation via language embeddings, SAC used in referenced work",
            "performance_llm_curriculum": null,
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": null,
            "performance_no_curriculum": null,
            "has_curriculum_comparison": null,
            "task_diversity_metrics": null,
            "transfer_generalization_results": null,
            "computational_cost": "Referenced as increasing computational demand due to expanded state space and requiring frequent LLM inference during training.",
            "failure_modes_limitations": "Increased state dimensionality requires more training time and computational resources; continuous LLM inference in training is costly.",
            "long_horizon_performance": null,
            "specialized_domain_performance": null,
            "ablation_studies": null,
            "model_size_scaling": null,
            "key_findings_curriculum_effectiveness": "Paper notes semantic state augmentation helps transfer but that continuous LLM use during training increases compute and complexity.",
            "uuid": "e2050.5"
        },
        {
            "name_short": "Chen et al. (unnumbered)",
            "name_full": "Pretrained LLM-generated task hints and language-guided rewards (Chen et al.) referenced",
            "brief_description": "Referenced study using pretrained LLMs (e.g., GPT-3.5) to generate task hints/content from textual descriptions that are embedded and used to generate language-guided rewards and drive policy adaptation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "curriculum_generator_type": "LLM-generated hints and reward shaping",
            "llm_model_name": "GPT-3.5 (example in citation)",
            "llm_model_size": null,
            "curriculum_description": "LLM generates task hints and content from descriptions; embeddings combined with observations input to transformer policy; used to create language-guided rewards aiding adaptation.",
            "domain_name": "Language-guided RL (general)",
            "domain_characteristics": "Uses textual hints to shape reward/learning; increases state/modal complexity.",
            "state_conditioning": true,
            "state_conditioning_details": "Task hints embedded into policy input as additional observation features.",
            "novelty_mechanism": null,
            "novelty_mechanism_details": null,
            "complementary_systems": "Language embeddings, transformer policies, language-guided reward generation",
            "performance_llm_curriculum": null,
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": null,
            "performance_no_curriculum": null,
            "has_curriculum_comparison": null,
            "task_diversity_metrics": null,
            "transfer_generalization_results": null,
            "computational_cost": "Noted that continuous pretrained LLM use during training increases computational demand (no precise cost reported).",
            "failure_modes_limitations": "Expanding state space and continuous LLM queries increase training time and require high-performance hardware.",
            "long_horizon_performance": null,
            "specialized_domain_performance": null,
            "ablation_studies": null,
            "model_size_scaling": null,
            "key_findings_curriculum_effectiveness": "LLM-generated hints and rewards can aid policy adaptation but at substantial computational cost when used during training.",
            "uuid": "e2050.6"
        },
        {
            "name_short": "LAMOL",
            "name_full": "Lifelong language learning with LAMOL (Sun et al., 2019) referenced",
            "brief_description": "Referential method that uses language modeling to generate pseudo-samples of previous tasks to mitigate catastrophic forgetting in lifelong language learning; limited to language domain.",
            "citation_title": "",
            "mention_or_use": "mention",
            "curriculum_generator_type": "LLM-generated pseudo-sample replay (language-specific)",
            "llm_model_name": null,
            "llm_model_size": null,
            "curriculum_description": "Generate pseudo-samples of previous tasks for joint training to reduce forgetting (applicable to language tasks but not general RL according to the paper).",
            "domain_name": "Lifelong language learning",
            "domain_characteristics": "Language modeling domain; not directly applicable to embodied or RL tasks without adaptation.",
            "state_conditioning": null,
            "state_conditioning_details": null,
            "novelty_mechanism": null,
            "novelty_mechanism_details": null,
            "complementary_systems": "Pseudo-sample generation, joint training",
            "performance_llm_curriculum": null,
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": null,
            "performance_no_curriculum": null,
            "has_curriculum_comparison": null,
            "task_diversity_metrics": null,
            "transfer_generalization_results": null,
            "computational_cost": null,
            "failure_modes_limitations": "Paper notes LAMOL is limited to language tasks and does not generalize well to robotics or games.",
            "long_horizon_performance": null,
            "specialized_domain_performance": null,
            "ablation_studies": null,
            "model_size_scaling": null,
            "key_findings_curriculum_effectiveness": "Shows pseudo-sample generation can reduce forgetting in language LLL but is not directly applicable to general RL domains.",
            "uuid": "e2050.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "An autotelic agent framework that combines LLMs with goal-conditioned RL (Pourcel et al., 2024)",
            "rating": 2
        },
        {
            "paper_title": "A Framework for LLM-based Lifelong Learning in Robot Manipulation (Mao, 2024)",
            "rating": 2
        },
        {
            "paper_title": "LEAGUE",
            "rating": 2
        },
        {
            "paper_title": "Meng et al. (semantic embeddings from LLMs for RL)",
            "rating": 2
        },
        {
            "paper_title": "Tziafas and Kasaei (continuous skill library expansion with LLMs)",
            "rating": 2
        },
        {
            "paper_title": "LAMOL",
            "rating": 1
        },
        {
            "paper_title": "Chen et al. (LLM-generated task hints and language-guided rewards)",
            "rating": 2
        }
    ],
    "cost": 0.0172855,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Lifelong Reinforcement Learning with Similarity-Driven Weighting by Large Models</p>
<p>Zhiyi Huang huangzhiyi@qiyuanlab.com 
Qiyuan Lab</p>
<p>Xiaohan Shan shanxiaohan@qiyuanlab.com 
Qiyuan Lab</p>
<p>Jianmin Li lijianmin@qiyuanlab.com 
Qiyuan Lab</p>
<p>Lifelong Reinforcement Learning with Similarity-Driven Weighting by Large Models
6B4C3299EAB24A92AA6C2F399499CDA1
Lifelong Reinforcement Learning (LRL) holds significant potential for addressing sequential tasks, but it still faces considerable challenges.A key difficulty lies in effectively preventing catastrophic forgetting and facilitating knowledge transfer while maintaining reliable decision-making performance across subsequent tasks in dynamic environments.To tackle this, we propose a novel framework, SDW (Similarity-Driven Weighting Framework), which leverages large-language-model-generated dynamic functions to precisely control the training process.The core of SDW lies in two functions pre-generated by large models: the task similarity function and the weight computation function.The task similarity function extracts multidimensional features from task descriptions to quantify the similarities and differences between tasks in terms of states, actions, and rewards.The weight computation function dynamically generates critical training parameters based on the similarity information, including the proportion of old task data stored in the Replay Buffer and the strategy consistency weight in the loss function, enabling an adaptive balance between learning new tasks and transferring knowledge from previous tasks.By generating function code offline prior to training, rather than relying on large-model inference during the training process, the SDW framework reduces computational overhead while maintaining efficiency in sequential task scenarios.Experimental results on Atari and MiniHack sequential tasks demonstrate that SDW significantly outperforms existing lifelong reinforcement learning methods.</p>
<p>Introduction</p>
<p>Reinforcement Learning (RL) has achieved remarkable success in solving single tasks, such as game control, robotic manipulation, and decision optimization.However, in realworld scenarios, intelligent systems often encounter a sequence of tasks that require continuous learning and adaptation while retaining previously acquired knowledge.To address these challenges, researchers have proposed a class of algorithms known as Lifelong Reinforcement Learning (LRL), which aim to enable agents to solve sequential decision-making problems in dynamic environments.LRL has gained increasing attention in recent research due to its importance in applications such as autonomous robotics [Liu et al., 2019], games [Sur et al., 2022], and adaptive control [Lu et al., 2019], where agents must not only acquire new skills but also maintain strong performance on previously learned tasks.However, designing effective LRL algorithms that balance learning new tasks and retaining past knowledge remains a significant challenge.</p>
<p>The practical challenges of existing LRL methods, such as CLEAR [Rolnick et al., 2019] and Elastic Weight Consolidation (EWC) [Kirkpatrick et al., 2017], become evident when applied to tasks with significant differences.We validated the CLEAR algorithm in the MiniHack environment, as illustrated in Figure 1.The results show a noticeable decline in performance on later tasks.This behavior can be attributed to its strategy of combining data from earlier tasks with new task data during training.When the gap between tasks is large, this approach struggles to reconcile conflicting task objectives, making it difficult for the algorithm to effectively optimize for the new task.The shared training process introduces interference, as the agent is forced to balance learning objectives that are not well-aligned across tasks.A similar phenomenon is observed with EWC, as shown in Figure 2. EWC attempts to retain performance on earlier tasks by constraining parameter updates during new task training.However, this rigid preservation of earlier knowledge does not account for task relationships and instead imposes strict limitations on the decision-making space for subsequent tasks.When earlier tasks are fundamentally different from later tasks, these constraints hinder the agent's ability arXiv: 2503.12923v1 [cs.LG] 17 Mar 2025 to fully adapt to the new task, leading to suboptimal performance.These observations highlight a fundamental limitation of existing methods: both CLEAR and EWC fail to model and leverage task relationships effectively.By treating tasks as independent and enforcing rigid constraints or replay mechanisms, they are unable to dynamically adapt to the evolving nature of task sequences.This underscores the need for more advanced approaches that can explicitly model inter-task dependencies, adaptively balance knowledge retention and new task learning, and address the unique challenges posed by diverse and complex task sequences.</p>
<p>Recently, large language models (LLMs) have achieved groundbreaking success in natural language processing [Achiam et al., 2023] and other fields such as law [Cheong et al., 2024], medical [Wang et al., 2024] and communication [Kan et al., 2024], demonstrating their ability to extract and utilize complex relationships in data [Hong et al., 2024].These capabilities make LLMs a promising tool for addressing the challenges of LRL, particularly in analyzing task relationships and guiding multi-task learning sequences.</p>
<p>Inspired by these advancements, we propose a novel LRL framework that leverages LLMs to improve task performance, mitigate forgetting, and enhance transferability.The framework introduces two key functions: one to quantitatively analyze task relationships and measure their relevance, and another to dynamically map task relevance to critical training parameters, such as loss calculation and buffer updates.This approach enables agents to adaptively control their training process, improving task retention while accelerating the learning of new tasks.</p>
<p>Unlike previous methods that require deep involvement of LLMs, our framework uses LLMs only during the function generation stage, ensuring lightweight integration.By effectively capturing task relationships and dynamically adjusting the training process, our framework significantly outperforms existing methods in task performance, forgetting mitigation, and knowledge transfer.Furthermore, it offers a novel perspective for integrating large models into LRL, advancing the development of AI systems capable of lifelong learning.</p>
<p>The main contributions of this work are as follows:</p>
<p>• We propose a novel lifelong reinforcement learning framework, SDW, which leverages the task relationship reasoning capabilities of LLMs to dynamically optimize multi-task learning processes.Compared to traditional methods, SDW significantly improves the efficiency of learning new tasks while effectively mitigating forgetting, providing a new solution for lifelong learning.</p>
<p>• We guide the large language model to generate two key functions: a task similarity calculation function, which quantitatively evaluates the relationships between tasks based on their descriptions, state distributions, and reward structures, and a weight calculation function, which determines the optimal training weights for balancing new and old tasks.By integrating these functions into the training process, we achieve a more reasonable balance between learning new tasks and the retention of old tasks.</p>
<p>• We implement a lightweight LLM integration strategy that utilizes LLMs only during the pretraining phase for task relationship reasoning, making the training process fully independent of LLMs.This design allows us to significantly reduce computational overhead while avoiding reliance on high-performance LLMs, ensuring that our framework remains efficient and scalable when handling large-scale task sequences.</p>
<p>• Our experimental results on MiniHack and Atari demonstrate that SDW significantly outperforms existing algorithms in mitigating catastrophic forgetting and improving the learning of new tasks, highlighting its effectiveness in lifelong reinforcement learning scenarios.</p>
<p>2 Related Work</p>
<p>Lilfelong Reinforcement Learning</p>
<p>Lifelong reinforcement learning aims to endow agents with the ability to accumulate knowledge over sequential tasks while mitigating catastrophic forgetting.Prior studies in LRL can be broadly categorized into three approaches:</p>
<p>Regularization-based Knowledge Retention</p>
<p>These methods use regularization terms to preserve knowledge and reduce interference with earlier tasks.Notable examples include Elastic Weight Consolidation (EWC) [Kirkpatrick et al., 2017]and Online EWC [Huszár, 2018], which mitigate forgetting by allowing suboptimal solutions for new tasks.While effective, these methods often face limitations in scalability and adaptability to highly dynamic task environments.</p>
<p>Experience Replay-based Methods</p>
<p>Experience replay methods store past task experiences and replay them during training, enabling agents to avoid catastrophic forgetting while improving learning performance on new tasks.Algorithms such as CLEAR [Rolnick et al., 2019] and PnC [Schwarz et al., 2018] fall into this category.Although these methods improve task performance to some extent, they struggle in scenarios with large task spaces due to their lack of task specificity and inefficiencies in prioritizing relevant experiences.</p>
<p>Network Structure-based Methods These approaches dynamically adjust the network architecture, such as freezing specific parameters or adding new modules, allowing agents to flexibly adapt to new tasks.An exam-ple is the SANE [Powers et al., 2022a], which effectively isolates task-specific knowledge.However, these methods tend to increase network complexity significantly, limiting their practicality in large-scale task deployments.</p>
<p>Advances in LLMs and Lifelong Learning</p>
<p>To overcome the limitations of traditional LRL methods in dynamic task environments, researchers have begun exploring the integration of LLMs, which possess powerful reasoning and knowledge transfer capabilities, into lifelong reinforcement learning frameworks.Meanwhile, studies on the lifelong learning capabilities of LLMs have also emerged.For instance, [Sun et al., 2019] proposed LAMOL, a simple yet effective method for lifelong language learning (LLL) based on language modeling.LAMOL addresses catastrophic forgetting by solving current tasks while generating pseudosamples of previous tasks for joint training.However, it is limited to language tasks and does not generalize well to other domains, such as robotics or games.</p>
<p>Additionally, research on the combination of LLMs and RL has made progress as well.Research also explores combining LLMs with reinforcement learning, focusing on task exploration and goal generation.[Pourcel et al., 2024] proposed an autotelic agent framework that combines LLMs with goalconditioned RL to address open-ended goal generation and learning.The LLM estimates goal difficulty and learnability based on context, generating reward function codes and goal descriptions to optimize the goal generation and learning process.This approach outperforms traditional methods in terms of goal generation efficiency and adaptation speed.However, it does not address catastrophic forgetting in multi-task scenarios, which remains a critical limitation in LRL.</p>
<p>Combining LLMs with LRL</p>
<p>While prior works have explored the potential of LLMs in lifelong learning and reinforcement learning separately, recent studies focus on integrating LLMs into lifelong reinforcement learning frameworks to address the unique challenges of dynamic multi-task environments.The integration of large language models with lifelong reinforcement learning has emerged as a promising direction, leveraging LLMs' reasoning capabilities and vast prior knowledge to enhance the adaptability and scalability of LRL frameworks.Current research primarily falls into two main categories:</p>
<p>Using LLMs as Planners to Build Skill Graphs This approach leverages LLMs to construct skill graphs, enabling agents to identify actions for complex tasks.[Mao, 2024] proposed a framework where LLMs are used to select skills for agents and determine whether new skills are necessary.Similarly, [Yuan et al., 2023]leveraged LLMs' prior knowledge to discover relationships between skills, constructing skill graphs to explore open-world environments such as Minecraft.</p>
<p>Li et al. [Li et al., 2024] introduced the LEAGUE framework, which integrates LLMs with Task and Motion Planning and Deep Reinforcement Learning.LLMs decompose tasks, create skill operators, and generate dense rewards to accelerate policy learning while maintaining a skill library for new problem scenarios.Tziafas and Kasaei [Tziafas and Kasaei, 2024] proposed an approach to continuously expand a robotic skill library.Their approach alternates between a 'wake' phase, where an LLM generates and validates tasks in a simulator, and a 'sleep' phase, where the LLM abstracts experiences into new skills and optimizes knowledge via memory replay.</p>
<p>Using LLMs as Semantic Generators for Observations</p>
<p>Another line of research treats LLMs as an environment-like entity, where task-relevant language generated by the LLM is encoded and embedded into the state space to facilitate knowledge transfer and task learning.</p>
<p>[ Meng et al., 2024] proposed a framework that embeds semantic information generated by LLMs into task states, enabling effective knowledge transfer.The system uses the SAC algorithm to learn task actions by encoding task descriptions as language embeddings, which are incorporated into the state space during training.</p>
<p>[Chen et al., ] utilized pretrained LLMs (e.g., GPT-3.5) to generate task hints and content from textual descriptions.These task-related embeddings are combined with observation information, input into a transformer model, and used to generate language-guided rewards.This approach helps policies adapt better to tasks.</p>
<p>Although research in this area is still in its early stages, these methods show great potential for enhancing task understanding and enabling efficient knowledge transfer in lifelong learning scenarios.However, expanding the state space increases complexity, requiring more iterations and computational resources.Furthermore, continuous interaction with large models during training significantly increases computational demand, leading to longer training times and requiring high-performance hardware to process the expanded state space efficiently.</p>
<p>Method</p>
<p>We propose a Similarity-Driven Weighting (SDW) framework, a lifelong reinforcement learning algorithm designed to address the challenge of catastrophic forgetting in sequential task learning.The core idea of SDW is to adaptively determine key parameter settings in the continual training process based on the relationships between tasks.Instead of relying on fixed or manually tuned parameters, SDW leverages the reasoning capabilities of large models to analyze task similarities and generate two essential functions: a task similarity calculation function and a weight calculation function.These functions dynamically quantify the extent of similarity between tasks and adjust the retention of knowledge-both in terms of data and policies-from previously learned tasks.By tailoring the degree of knowledge transfer and retention to the specific relationships between tasks, SDW ensures a more flexible and efficient learning process across task sequences, significantly improving performance in lifelong reinforcement learning scenarios.We provide more in-depth insights into the design of SDW in the Appendix.</p>
<p>Framework Workflow</p>
<p>The SDW framework operates in two main stages: the function generation stage and the sequential task training stage, as illustrated in Figure 3.During the function generation stage, SDW leverages LLMs to generate two key functions: the Task Similarity Calculation Function and the Weight Calculation Function.The prompts for the LLM are constructed using task information (e.g., task characteristics, continual learning process, replay buffer usage) and design requirements, which specify the desired outputs of these functions.This information provides essential contextual knowledge about the relationships between tasks in a lifelong learning setting.The Task Similarity Calculation Function computes a similarity vector between consecutive tasks based on their environmental encodings, which represents the relationship between the current task and the immediately preceding task.This vector captures various aspects of task similarity and serves as the foundation for subsequent adjustments.The Weight Calculation Function uses the similarity vector to dynamically adjust key parameters during the training process, ensuring that the framework effectively balances knowledge retention and transfer across tasks.</p>
<p>In the sequential task training stage, the framework applies the functions generated by the LLM to process and optimize the continual learning process.Specifically, the similarity vector computed by the Task Similarity Calculation Function informs the Weight Calculation Function to adjust sampling weights for the replay buffer and key weights in the loss function.These adjustments enable the framework to incrementally train the model on new tasks while mitigating catastrophic forgetting.</p>
<p>What needs to be emphasized is that SDW confines the use of LLMs to a one-time inference during the function generation stage, after which the generated functions are applied</p>
<p>Function Generation</p>
<p>In the function generation phase, SDW leverages LLMs to automatically generate two core functions: the task similarity function and the weight computation function.These two functions are used to quantify the multidimensional similarity between tasks and to dynamically generate training parameters based on the computed similarity.The input for the task similarity function is generated from the descriptions of the tasks.Task descriptions may include information such as objectives, state space, action space, and reward functions.Based on these descriptions, the LLM extracts task features and generates a feature representation that characterizes the task.</p>
<p>The task similarity function outputs a multidimensional similarity vector, denoted as S = [S state , S action , S reward , . . .], which quantifies task similarity across multiple features, such as states, actions, and rewards.Each dimension in S lies in the range [0, 1], where values closer to 1 indicate greater similarity for the respective feature, and values closer to 0 indicate significant differences.The function is formally defined as:
F similarity : (d current , d previous ) → S,
where d current and d previous are feature vectors extracted from the descriptions of the current and previous tasks.The logic for feature extraction and similarity computation is automatically generated by the LLM, ensuring that the function captures nuanced, multidimensional relationships between tasks.This approach enables a more comprehensive understanding of the differences and similarities across tasks, which is critical for effective continual learning.</p>
<p>The weight computation function uses the similarity vector S to dynamically determine two key training parameters: the weight w buffer for adjusting the proportion of old task data in the replay buffer, and the weight λ consistency for scaling the strategy distance loss in the total loss function.Formally, the function is defined as:
F weight : S → (w buffer , λ consistency ),
where both w buffer and λ consistency lie in the range [0, 1].Higher similarity values in S lead to a larger w buffer , preserving more old task data in the replay buffer to emphasize knowledge retention.Similarly, higher similarity increases λ consistency , prioritizing consistency with the previous task's strategy by assigning greater importance to the strategy distance loss.</p>
<p>By leveraging the multidimensional information in S, the LLM-generated weight computation function flexibly adjusts these parameters based on the specific relationships between tasks.This dynamic adjustment ensures a balance between knowledge retention and adaptation, enabling the framework to optimize training efficiency while mitigating catastrophic forgetting.The integration of these functions forms the foundation of the SDW framework's adaptive approach to continual learning in a principled and adaptive manner.</p>
<p>Sequential Task Training</p>
<p>In sequential task training, the SDW framework dynamically adjusts key training parameters through the synergy of the task similarity function and the weight computation function, balancing the utilization of historical knowledge with the learning requirements of the new task.Specifically, the task similarity function generates a multidimensional similarity vector S based on features like states, actions, and rewards, which provides a precise characterization of the relationship between the new and old tasks.Using S as input, the weight computation function produces two critical parameters: w buffer , which controls the proportion of old task data in the replay buffer, and λ consistency , which regulates the weight of the strategy consistency loss in the total loss function.</p>
<p>Replay Buffer Management.The dynamic management of the replay buffer is governed by w buffer , which determines the balance between old and new task data in the buffer.A higher w buffer retains more old task data, strengthening the utilization of historical knowledge, while a lower w buffer reduces the proportion of old task data, minimizing interference with new task learning.To maintain this balance, SDW employs a dynamic insertion probability P insert to control the entry of new task data into the buffer: P insert = P base +λ• 1 − wbuffer pold , where P base is a base insertion probability that ensures new task data always has some opportunity to enter the buffer, and λ is a scaling factor that adjusts the insertion probability based on the deviation of the actual proportion of old task data, p old , from the target proportion, w buffer .When p old &gt; w buffer , the insertion probability increases, allowing more new task data to enter the buffer and reducing the dominance of old task data.Conversely, when p old &lt; w buffer , the insertion probability decreases, prioritizing the retention of old task data.This dynamic adjustment mechanism ensures that the replay buffer is adaptively managed to suit the learning needs of different tasks and effectively leverage historical knowledge.</p>
<p>Loss Function Design.In terms of the loss function design, the total loss in SDW consists of two components: the policy optimization loss L policy and the strategy consistency loss L consistency , expressed as: L total = L policy + λ consistency • L consistency .Here, L policy focuses on optimizing the policy for the current task, enabling it to meet the objectives of the new task, while L consistency constrains the current policy to maintain consistency with the historical policy, facilitating knowledge transfer.The dynamic adjustment of λ consistency is also derived from the similarity vector S, reflecting the relationship between tasks and the need for knowledge transfer.</p>
<p>Experiments</p>
<p>Experimental Setup</p>
<p>To evaluate the effectiveness of our proposed method, we conducted experiments on a series of continual learning problems.Specifically, we adopted a multi-round training framework wherein tasks in the environment were trained in multiple batches.In each batch, all tasks were trained sequentially, with the current model evaluated on all tasks at fixed intervals during training.This framework is widely used for evaluating lifelong learning methods, such as those in the Cora benchmark [Powers et al., 2022b].We assessed the performance of the algorithm using the following metrics:</p>
<p>Metrics</p>
<p>To facilitate the definition of metrics, we first introduce the following notations:</p>
<p>n: The total number of tasks in the sequential learning setting.</p>
<p>r i,j : The performance of the model on task i before and after training on task j.</p>
<p>r i,all,max : The maximum performance of the model on task i across all evaluations.</p>
<p>-P : The task performance metric.</p>
<p>-F : The forgetting metric.</p>
<p>-T : The knowledge transfer metric.</p>
<p>We now define the metrics used to evaluate the performance of different algorithms in sequential learning:</p>
<p>• Task Performance (P)</p>
<p>This metric evaluates the overall performance of the model across tasks:
P = 1 n n j=1 1 j j i=1 r i,j ,
• Catastrophic Forgetting (F) This metric quantifies the amount of knowledge lost from previously learned tasks after learning subsequent tasks:
F = 1 n − 1 n j=2 1 j − 1 j−1 i=1 r i,j−1 − r i,j |r i,all,max | s.t. i &lt; j,
A positive value of F indicates that the model's performance on previous tasks has deteriorated after training on subsequent tasks.</p>
<p>• Knowledge Transfer (T)</p>
<p>This metric measures how much prior knowledge contributes to learning new tasks:
T = 1 n n j=1   1 n − j n i=j+1 r i,j − r i,j−1 |r i,all,max |   s.t. i &gt; j.
A positive value of T indicates that learning previous tasks improves the learning of subsequent tasks.</p>
<p>Evaluation on the Minihack Environment</p>
<p>Minihack is a sandbox environment created by [Samvelyan et al., 2021], based on the NetHack game.It uses the NLE interface to communicate with the game and provides a rich set of diverse task environments.For our experiments, we selected 15 tasks from the Minihack environment, categorized into seven groups.Detailed task descriptions and the full task list are provided in the appendix.We configured the training framework for two rounds, with each task trained for 1e7 steps per round, resulting in a total of 3e8 steps.To validate the generality of our approach, we conducted experiments using three LLMs-GPT-4o, GPT-3.5, and GLM4-9B-and compared our method against five lifelong reinforcement learning (LRL) methods.For all baseline algorithms, we use the default hyperparameter settings as provided in their official implementations unless otherwise stated.</p>
<p>The experimental results in Table1 highlight the effectiveness of SDW in addressing lifelong learning challenges within the Minihack environment.When paired with LLMs such as GLM4-9B and GPT-4o, SDW demonstrates substantial performance improvements over traditional methods.Specifically, SDW+GPT-4o achieves the best overall task performance (-0.151) and excels in mitigating catastrophic forgetting (-0.505), underscoring its robustness in retaining knowledge across tasks.Additionally, it achieves the highest knowledge transfer score (0.290), showcasing its ability to generalize and adapt to new tasks effectively.Compared to baseline approaches such as EWC and SANE, SDW consistently outperforms across all metrics, demonstrating its strong potential for advancing lifelong learning frameworks.</p>
<p>Evaluation on the Atari Environment</p>
<p>The Atari environment, based on the Arcade Learning Environment (ALE), provides a diverse set of classic games and tasks, serving as a standard benchmark for reinforcement learning methods.[Mnih, 2013] In our experiments, we selected three representative tasks: SpaceInvadersNoFrameskip-v4, BeamRiderNoFrameskip-v4, and MsPacmanNoFrameskip-v4.Each task was trained over three rounds, with 5 million steps per round, resulting in a total of 45 million training steps.</p>
<p>We evaluated three large-scale models-GPT-4o, GPT-3.5, and GLM4-9B-and compared their performance with the same lifelong reinforcement learning baselines, ensuring consistency across all experiments.For all baseline algorithms, we use the default hyperparameter settings as provided in their official implementations.</p>
<p>The experimental results in Table 2 demonstrate that SDW variants consistently outperform the baselines across all metrics.In task performance (P), SDW+GPT3.5 achieves the highest score (966.1),closely followed by SDW+GPT4o (965.46), while baselines EWC (425.72) and Clear (932.46)fall significantly behind.For catastrophic forgetting (F), SDW methods effectively mitigate forgetting with negative rates (-0.058 for SDW+GPT4o, -0.045 for SDW+GPT3.5), in contrast to the positive rates of EWC (0.113) and Clear (0.054).Most notably, SDW+GPT4o excels in knowledge transfer (T), achieving the highest score (0.124), surpassing its counterparts (SDW+GPT3.5:0.069, SDW+GLM4-9B: 0.078) and baselines (EWC: -0.137, Clear: -0.024).These findings highlight the superior adaptability and scalability of SDW+GPT4o in continual learning scenarios, likely due to its enhanced reasoning and prior knowledge integration.However, further investigation is needed to understand the relatively lower transfer performance of SDW+GLM4-9B and explore the scalability of SDW methods in more diverse tasks.</p>
<p>Ablations</p>
<p>To further investigate the effectiveness of our proposed SDW (Selective Dynamic Weighting) framework, we designed and tested several variants of SDW to isolate its contributions:  • LLM-Based Loss and Buffer Module Generation(MG): By providing relevant code and task-specific information as input, large language models (LLMs) autonomously generate customized loss computation functions and buffer modules.
P ↑ F ↓ T ↑ EWC 425
• LLM-Based Parameter Assignment(PA): By leveraging task-specific information, task order, and reward functions as input, the large model dynamically configures parameters for loss computation and buffer management, enabling the agent to efficiently adapt and train for each specific task.</p>
<p>• SDW with loss function only(L)</p>
<p>• SDW with buffer function only(B)</p>
<p>Through a systematic comparison of these variants, we aim to delineate the specific contributions of key components within the SDW framework.The experiments are carried out in the MiniHack sandbox, adhering to the same setup detailed in Section 4.3.For this evaluation, ChatGPT-4o is utilized as the underlying LLM.</p>
<p>The results in Table 3    performs worst in catastrophic forgetting (Metric 2: 0.580 vs. Clear's 0.055), while PA shows the weakest knowledge transfer (Metric 3: -0.939 vs. Clear's -0.330).Adding the buffer component (SDW+Buffer) improves knowledge transfer (Metric 3: -0.066) but exacerbates forgetting (Metric 2: -0.260).Modifying the loss function (SDW+Loss) enhances knowledge transfer (Metric 3: 0.259) and reduces forgetting (Metric 2: -0.341) but fails to improve performance (Metric 1: -0.166).The complete SDW framework achieves the best balance, with the highest scores in knowledge transfer (Metric 3: 0.290) and performance (Metric 1: -0.151), and the lowest forgetting (Metric 2: -0.505).This suggests that the synergy between buffer and loss components is essential for optimizing results.</p>
<p>Conclusion and Future Work</p>
<p>In this paper, we proposed SDW, a novel framework for lifelong reinforcement learning that dynamically adjusts learning strategies based on task similarity.A key feature of SDW is its use of large pre-trained models to analyze task relationships, enabling a task similarity-driven weight computation mechanism that effectively balances knowledge retention and transfer.This design allows SDW to mitigate catastrophic forgetting while promoting efficient knowledge sharing across sequential tasks.Experimental results validate the effectiveness of SDW, demonstrating improved task performance, reduced forgetting, and enhanced knowledge transfer.</p>
<p>For future work, we aim to further leverage the capabilities of large-scale pre-trained models to replace manually designed components, such as reward function design and action-assisted generation.By automating these processes, SDW can become more generalized and adaptable to complex, open-ended lifelong reinforcement learning scenarios.</p>
<p>A Insights into the Design and Mechanisms of SDW</p>
<p>A.1 Modular Design of SDW</p>
<p>In the SDW framework, we avoid directly generating task weights w, as this would require the LLM to search within a highly complex and high-dimensional function space F, mapping task descriptions to weight values.Instead, SDW adopts a structured, two-step design inspired by the Chainof-Thought (CoT) paradigm.First, the LLM generates a task similarity function S, which evaluates the relationships between tasks based on their descriptions, state distributions, and reward structures.This similarity function maps pairs of tasks to a similarity score S(T new , T</p>
<p>old ) ∈ [0, 1], providing a quantitative measure of task relationships in a relatively constrained function space S.</p>
<p>With the similarity scores as input, the LLM then generates a weight calculation function g, which determines the training weights by considering both the similarity between the new task and old tasks as well as the relationships among old tasks.Formally, the weights are computed as
w = g S(T new , T (i) old ), {S(T (i) old , T (j) old )} j̸ =i .
This second step operates in a smaller, structured subspace G, as it relies on a limited similarity matrix rather than the full task descriptions.</p>
<p>This two-step design offers several key advantages.First, it compresses the search space, as the complexity of generating S and g (i.e., S + G) is significantly lower than directly searching the full function space F. By effectively reducing the dimensionality of the problem, the LLM avoids blind exploration in a vast space and instead focuses on reasoning within smaller, interpretable subspaces.Second, the modular approach improves interpretability, since the task similarity function S provides an explicit representation of task relationships, making the reasoning process more understandable.Finally, this design enhances robustness, as isolating the complex weight computation step into a smaller subspace reduces the risk of instability or errors.</p>
<p>Mathematically, let F denote the original function space for directly generating weights.By introducing the intermediate spaces S for similarity functions and G for weight calculation functions, the overall complexity is reduced such that
S + G ≪ F.
The space S is constrained by task feature dependencies, while G depends only on structured similarity scores, both of which are significantly smaller than F. This reduction in complexity ensures a more efficient and reliable generative process while maintaining flexibility and interpretability.</p>
<p>A.2 Dynamic Adjustment of Task Parameters</p>
<p>It is important to note that w buffer and λ consistency are complementary mechanisms, addressing the influence of old tasks on the current task from two perspectives.w buffer directly controls the proportion of old task data retained in the replay buffer, thereby influencing knowledge transfer.A higher w buffer prioritizes the use of old task data, which is particularly effective when the new task is highly similar to previous tasks, allowing the model to efficiently reuse relevant knowledge.Conversely, when the new task is dissimilar, lowering w buffer reduces interference from irrelevant or outdated data, ensuring the new task is learned independently and effectively.</p>
<p>On the other hand, λ consistency adjusts the regularization that encourages consistency between the policies of old and new tasks.By aligning the new policy with established strategies, λ consistency promotes smoother transitions and avoids abrupt changes that could degrade performance on old tasks.When task similarities are high, increasing λ consistency ensures behavioral consistency and efficient knowledge reuse.However, when the new task requires greater independence, lowering λ consistency allows the model to explore novel strategies without being overly constrained by past behaviors.</p>
<p>Both w buffer and λ consistency are dynamically adjusted based on the task similarity vector S. When S indicates high similarity between the new and old tasks, SDW increases both w buffer and λ consistency , ensuring greater data retention and behavioral consistency to maximize knowledge transfer.Conversely, when the new task requires more independence, SDW reduces these parameters to minimize interference, enabling the model to adapt more freely.This dynamic adjustment mechanism ensures that task similarities are leveraged when beneficial, while avoiding negative transfer or overfitting to old task strategies.</p>
<p>B Experimental Environment and Evaluation</p>
<p>B.1 Atari</p>
<p>This section provides a concise overview of the three Atari environments used in our experiments: SpaceInvadersN oF rameskip − v4, BeamRiderN oF rameskip − v4, and M sP acmanN oF rameskip − v4.These environments, part of the Arcade Learning Environment (ALE), are instantiated using the Gym library and are designed with distinct objectives, action spaces, and observation formats.The following descriptions of the Atari Games are based on the official documentation in https: //www.gymlibrary.dev/environments/atari/completelist/</p>
<p>In SpaceInvadersN oF rameskip − v4, the player controls a laser cannon to destroy incoming space invaders before they reach Earth.Points are awarded for eliminating invaders, with higher scores for those in the back rows.The game ends when all lives are lost or the invaders reach the planet.In BeamRiderN oF rameskip − v4, the player pilots a spaceship, aiming to destroy enemy ships, dodge attacks, and avoid space debris, earning points for successfully eliminating enemies.Meanwhile, M sP acmanN oF rameskip − v4 tasks the player with navigating a maze to collect pellets while avoiding ghosts.</p>
<p>All three environments share a discrete action space of 18 actions, with game-specific reduced action sets available for more meaningful gameplay (e.g., movement, firing, or combined actions).Observations are provided as RGB images with dimensions (210, 160, 3) by default, but alternative ob-servation formats, including 128-byte RAM representations and grayscale images, are also supported.Additionally, each environment allows for multiple modes and difficulty levels, offering flexible configurations for various experimental setups.</p>
<p>Version History: These environments have undergone several iterations.v0 represents the initial release, v4 removes action stickiness, and v5 reintroduces action stickiness while removing stochastic frameskipping.These updates improve the environments' suitability for reinforcement learning research by ensuring a balance between complexity and reproducibility.</p>
<p>For further details regarding these environments and their configurations, please refer to the official documentation provided by the Arcade Learning Environment.The information summarized here is based on the documentation available at this source.</p>
<p>B.2 Minihack</p>
<p>This section provides an overview of the MiniHack environments used in our experiments.Each task is characterized by unique features and challenges, including procedurally generated maps, randomization, and varying levels of complexity.The following descriptions of the MiniHack environments are based on the official documentation provided by Mikayel Samvelyan et al.The full documentation is available at https://github.com/facebookresearch/minihack.</p>
<p>Figure 1 :
1
Figure 1: Performance of the CLEAR algorithm on the MiniHack benchmark.The x-axis represents training steps, and the y-axis represents the reward.A new task is introduced every 10 million steps.</p>
<p>Figure 2 :
2
Figure 2: Performance of the EWC algorithm on the MiniHack benchmark.The x-axis represents training steps, and the y-axis represents the reward.A new task is introduced every 10 million steps.</p>
<p>Figure 3 :
3
Figure 3: The Workflow of SDW.</p>
<p>throughout the sequential task training phase to compute taskspecific weights and adjustments without requiring further LLM inference.This design avoids embedding LLM inference into the training process, significantly reducing computational overhead and ensuring efficient and scalable training.Unlike previous works (e.g., [Meng et al., 2024], [Chen et al., ]) that rely on frequent LLM inference during training, SDW decouples LLM usage from the training loop, allowing us to leverage high-parameter LLMs without compromising efficiency.</p>
<p>Figure 4 :
4
Figure 4: SDW performance comparison on Minihack.</p>
<p>Figure 5 :
5
Figure 5: SDW performance comparison on Atari.</p>
<p>Figure 6 :
6
Figure 6: Ablation Study of SDW.</p>
<p>Table 1 :
1
SDW performance comparison on Minihack.
P ↑F ↓T ↑EWC-0.2160.1750.217Clear-0.1650.055 -0.330Online-EWC-0.2890.2660.177PnC-0.2860.0750.150SANE-0.1620.3540.097SDW+GPT4o-0.151 -0.151 -0.151 -0.505 -0.505 -0.505 0.290 0.290 0.290SDW+GPT3.5-0.160 -0.0960.194SDW+GLM4-9B -0.152 -0.2640.134</p>
<p>Table 2 :
2
SDW performance comparison on Atari.
.720.113 -0.137Clear932.460.054 -0.024SDW+GPT4o965.46 -0.058 -0.058 -0.058 0.124 0.124 0.124SDW+GPT3.5966.1 966.1 966.1 -0.0450.069SDW+GLM4-9B 933.61 -0.0110.078</p>
<p>show that baseline methods (Clear, MG, and PA) exhibit poor balance across metrics.MG, due to the complete absence of chain-of-thought (CoT) reasoning,
P ↑F ↓T ↑Clear-0.1650.055 -0.330MG-0.3840.580 -0.325PA-0.1650.075 -0.939SDW(Only BUffer)-0.164 -0.260 -0.066SDW(Only Loss)-0.166 -0.3410.259SDW(Buffer + Loss) -0.151 -0.151 -0.151 -0.505 -0.505 -0.505 0.290 0.290 0.290</p>
<p>Table 3 :
3
Ablation Study of SDW.</p>
<p>batch_replay_ratio = np.clip( batch_replay_ratio, initial_value, max_value)
return batch_replay_ratioListing 9: Weight computation function generated by GLM4-9B: Part ii
Room:The Room tasks take place in a single square room, where the agent's objective is to navigate toward the staircase leading to the next level.There are multiple variants of this environment:1. Room Sizes: The room sizes are either 5x5 or 15x15.• MiniHack-Room-5x5-v0 and MiniHack-Room-15x15-v0: Fixed starting and goal positions.• MiniHack-Room-Random-5x5-v0 and MiniHack-Room-Random-15x15-v0: Randomized starting and goal positions.2. Complexity Additions: Complexity increases through various factors:• Monsters: Introduced in MiniHack-Room-Monster-5x5-v0 and MiniHack-Room-Monster-15x15-v0.• Teleportation Traps: Present inMiniHack-Room-Trap-5x5-v0 and MiniHack-Room-Trap-15x15-v0.• Dark Rooms: Where visibility is restricted to adjacent grid cells (MiniHack-Room-Dark-5x5-v0 and MiniHack-Room-Dark-15x15-v0).• Ultimate Challenge: Combines all the above elements (MiniHack-Room-Ultimate-5x5-v0 and MiniHack-Room-Ultimate-15x15-v0).Agent Capabilities:The agent can attack monsters by moving into adjacent cells, but stepping on a lava tile results in instant death.In dark rooms, visibility is limited to adjacent cells.4. Reward: A reward of +1 is given upon reaching the goal.Corridor TasksThe Corridor tasks challenge the agent to navigate procedurally generated rooms and corridors by using the RANDOM CORRIDORS command.The staircase leading to the next level is located in one of the rooms.1. Randomized Layouts: The positions and sizes of rooms and corridors are randomized for each episode.2. Variants: These tasks vary in complexity based on the number of rooms:• MiniHack-Corridor-R2-v0: Two rooms.• MiniHack-Corridor-R3-v0: Three rooms.3. Reward: A reward of +1 is given upon reaching the goal.KeyRoom TasksThe KeyRoom tasks require the agent to retrieve a key, navigate to a locked door, and unlock it to reach the staircase.1. Task Features:• The key, door, and staircase locations are randomized.• The agent's action space includes standard movement, pickup, and apply actions.2. Variants:• Room sizes include 5x5 (MiniHack-KeyRoom-S5-v0) and 15x15 (MiniHack-KeyRoom-S15-v0).• Dark versions (MiniHack-KeyRoom-Dark-S5-v0 and MiniHack-KeyRoom-Dark-S15-v0) restrict visibility to adjacent cells, making the key invisible unless nearby.3. Reward: A reward of +1 is given upon reaching the goal.River TasksThe River tasks require the agent to cross a river by pushing boulders into the water to create a path.1. Task Features:• Boulders pushed into water create walkable tiles.• The agent must avoid pushing boulders into lava, which results in failure.Variants:• Narrow river crossing (MiniHack-River-Narrow-v0).• Addition of monsters (MiniHack-River-Monster-v0).• Lava hazards (MiniHack-River-Lava-v0). 3. Reward: A reward of +1 is given upon reaching the goal.HideNSeek TasksThe HideNSeek tasks place the agent in a procedurally generated room with trees, clouds, and a powerful monster.1. Task Features:• Trees block movement and line of sight.• Clouds obscure visibility but allow movement.• The agent must avoid detection by the monster while navigating to the goal.Variants:• Standard version (MiniHack-HideNSeek-v0).• Lava hazards (MiniHack-HideNSeek-Lava-v0).Reward:A reward of +1 is given upon reaching the goal.CorridorBattle TasksThe CorridorBattle tasks challenge the agent to defeat a horde of monsters by strategically using narrow corridors.1. Task Features:• Fighting in corridors allows the agent to engage monsters one at a time, minimizing damage.• Dark versions (MiniHack-CorridorBattle-Dark-v0) require the agent to remember the number of defeated monsters to plan subsequent actions.Reward:A reward of +1 is given upon reaching the goal.C Prompts and Code Generated by LLMs C.1 PromptsThis section provides the prompts we used in SDW experiments.• This prompt is used for task similarity function generation:There is now a training environment containing n tasks, and the model will sequentially train on these n tasks.After completing the training of each task, the current model will be tested on all tasks.Based on the test results for all tasks, the model's catastrophic forgetting, knowledge transfer, and task performance will be calculated.• This prompt is used for weight computation function(buffer part) generation:There is now an additional parameter: batch replay ratio, which represents the proportion of past task experiences in the buffer.I hope you can design a calculation method to determine the value of this parameter based on the task relevance vector.Please provide a calculation function where the input is the similarity vector, and the output is the parameter value.C.2 CodeThis section provides the code generated by different LLMs using prompt in Section B.1.
Achiam, arXiv:2303.08774Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. 2023. 2023arXiv preprint</p>
<p>Learning with language inference and tips for continual reinforcement learning. Chen , </p>
<p>Engaging legal experts towards responsible llm policies for legal advice. Cheong, The 2024 ACM Conference on Fairness, Accountability, and Transparency. 2024. 2024</p>
<p>Data interpreter: An llm agent for data science. Hong, arXiv:2402.186792024. 2024arXiv preprint</p>
<p>Note on the quadratic penalties in elastic weight consolidation. Huszár, Proceedings of the National Academy of Sciences. the National Academy of SciencesFerenc Huszár2018. 2018115</p>
<p>Mobile-llama: Instruction finetuning open-source llm for network analysis in 5g networks. Kan, IEEE Network. 2024. 2024</p>
<p>Overcoming catastrophic forgetting in neural networks. Kirkpatrick, Proceedings of the national academy of sciences. the national academy of sciences2017. 2017114</p>
<p>Empowering continual robot learning throug guided skill acquisition with language models. Li, First Workshop on Vision-Language Models for Navigation and Manipulation at ICRA 2024. 2024. 2024</p>
<p>Lifelong federated reinforcement learning: a learning architecture for navigation in cloud robotic systems. Liu, IEEE Robotics and Automation Letters. 442019. 2019</p>
<p>Adaptive online planning for continual lifelong learning. Lu, arXiv:1912.011882019. 2019arXiv preprint</p>
<p>A Framework for LLM-based Lifelong Learning in Robot Manipulation. Jerry W Mao, Mao, 2024. 2024Massachusetts Institute of TechnologyPhD thesis</p>
<p>Cora: Benchmarks, baselines, and metrics as a platform for continual reinforcement learning agents. Meng, arXiv:1312.5602Intrinsically Motivated Open-ended Learning Workshop at NeurIPS 2024. David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, Gregory Wayne, PMLR2024. 2024. 2013. 2024. 2022a. 2022. 2022. 201932arXiv preprintAdvances in neural information processing systems</p>
<p>Samvelyan, arXiv:2109.13202Minihack the planet: A sandbox for open-ended reinforcement learning research. 2021. 2021arXiv preprint</p>
<p>Progress &amp; compress: A scalable framework for continual learning. Schwarz, International conference on machine learning. PMLR2018. 2018</p>
<p>Lifelong robot library learning: Bootstrapping composable and generalizable skills for embodied control with language models. Sun, arXiv:1909.033292024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2019. 2019. 2022. 2024arXiv preprintProceedings of the Second International Conference on AI-ML Systems</p>
<p>Jmlr: Joint medical llm and retrieval training for enhancing reasoning and professional question answering capability. Wang , arXiv:2402.17887arXiv:2303.16563Skill reinforcement learning and planning for open-world long-horizon tasks. 2024. 2024. 2023. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
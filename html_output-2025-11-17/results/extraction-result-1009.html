<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1009 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1009</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1009</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-211171741</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2002.07911v1.pdf" target="_blank">Generating Automatic Curricula via Self-Supervised Active Domain Randomization</a></p>
                <p><strong>Paper Abstract:</strong> Goal-directed Reinforcement Learning (RL) traditionally considers an agent interacting with an environment, prescribing a real-valued reward to an agent proportional to the completion of some goal. Goal-directed RL has seen large gains in sample efficiency, due to the ease of reusing or generating new experience by proposing goals. In this work, we build on the framework of self-play, allowing an agent to interact with itself in order to make progress on some unknown task. We use Active Domain Randomization and self-play to create a novel, coupled environment-goal curriculum, where agents learn through progressively more difficult tasks and environment variations. Our method, Self-Supervised Active Domain Randomization (SS-ADR), generates a growing curriculum, encouraging the agent to try tasks that are just outside of its current capabilities, while building a domain-randomization curriculum that enables state-of-the-art results on various sim2real transfer tasks. Our results show that a curriculum of co-evolving the environment difficulty along with the difficulty of goals set in each environment provides practical benefits in the goal-directed tasks tested.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1009.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1009.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SS-ADR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Supervised Active Domain Randomization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that jointly learns a goal curriculum via asymmetric self-play (Alice/Bob) and an environment curriculum via Active Domain Randomization (ADR) particles to produce robust policies that transfer zero-shot to real robots without any extrinsic reward during training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Bob (policy trained under SS-ADR)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A goal-conditioned policy trained with Deep Deterministic Policy Gradients (DDPG) inside the SS-ADR framework; Bob is the executor in the asymmetric self-play pair and learns from intrinsic time-based rewards (Equation 2) generated during self-play episodes across randomized environments.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>robotic agent (simulated during training, deployed on physical Poppy Ergo Jr robots for zero-shot transfer)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>ErgoPusher and ErgoReacher (simulated randomized MDPs and real-world instantiations)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Continuous-control robotics tasks: ErgoPusher (3DoF arm pushes a puck to a goal) and ErgoReacher (4DoF arm moves end-effector to a goal). Environments are varied by simulator parameter randomizations such as puck friction and damping (Pusher) and joint torques / gains (Reacher). Intuitively hard / held-out test environments are created by setting low friction/damping ("icy" puck) or extremely low torques leading to non-recoverable states.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Characterized by task difficulty and number of randomized simulation parameters (N_rand); e.g., ErgoPusher uses N_rand = 2 (puck damping, puck friction), ErgoReacher uses N_rand = 8 (multiple joint torques/gains). Task difficulty additionally measured by whether dynamics create non-recoverable states (e.g., very low torques).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>ErgoPusher: low-to-medium (N_rand=2, simpler domain); ErgoReacher: high (N_rand=8, higher-dimensional randomized parameters); held-out "intuitively hard" test environments represent high task difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Environment variation measured as the dimensionality and range of the domain randomization space Ξ (Ξ ∈ R^{N_rand}), and by the ADR particle distribution over Ξ; number of ADR/SVPG particles N = 8 controls exploration of environment instances.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>Training variation: medium-to-high (structured by ADR particles across the specified parameter ranges). ErgoPusher has lower randomized dimensionality (N_rand=2) while ErgoReacher has higher (N_rand=8).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Final distance to goal (lower is better); variance/spread of final distances across trials; zero-shot transfer average final distance across 25 trials on real robot; learning curves over training steps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: SS-ADR attains lower final distance-to-goal and lower variance (more consistent performance) than Uniform Domain Randomization (UDR) and than Unsupervised-Default on both in-distribution and held-out harder environments; exact numeric values are not reported in the paper text (figures only).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — the paper explicitly argues and demonstrates that co-evolving the goal curriculum and environment curriculum (i.e., coupling environment variation and goal difficulty) yields better generalization: increasing environment variation (via ADR) while matching goal difficulty (via self-play) avoids degenerate training (impossible goals or impossible environments) and produces multiplicative benefits compared to varying only goals or only environments. It also notes a trade-off in ADR: learned environment rewards can be exploited to create impossible environments, which SS-ADR mitigates by using self-supervised (Alice) rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Coupled curriculum learning: asymmetric self-play (goal curriculum) + Active Domain Randomization (environment curriculum); policies trained with DDPG on intrinsic self-play rewards while ADR particles (SVPG, N=8) select environment instances.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Zero-shot transfer to real Poppy Ergo Jr robots was performed: SS-ADR outperformed UDR and the self-play-only baseline (Unsupervised-Default) in average final distance and consistency across multiple real-world parameter instantiations (different motor torques or puck friction regimes). Held-out hard test environments (outside training distribution) showed SS-ADR maintained stronger performance and lower variance than baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Training used 1,000,000 unlabelled self-play interactions; policies were evaluated every 5,000 timesteps (200 evaluations total); reported curves are mean-averaged across 4 seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Co-evolving environment variation and goal difficulty via SS-ADR produces more robust, lower-variance policies that generalize better to held-out and real-world conditions than Uniform Domain Randomization and than using self-play alone; ADR alone can exploit simulator physics to produce impossibly hard environments, but replacing ADR's learned discriminator reward with self-supervised asymmetric self-play rewards mitigates exploitability and stabilizes curriculum learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating Automatic Curricula via Self-Supervised Active Domain Randomization', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1009.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1009.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Alice/Bob self-play</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Asymmetric Self-Play (Alice and Bob)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-supervised curriculum mechanism where Alice sets goals (operates in the reference environment) and Bob attempts to achieve them; rewards are time-based so Alice proposes goals just beyond Bob's current capability and Bob learns to expand capability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Intrinsic motivation and automatic curricula via asymmetric self-play</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Alice (goal setter) and Bob (goal executor)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Two DDPG policies: Alice proposes goals (and signals STOP) in the reference environment; Bob receives the goal and attempts to reach it in randomized environments. Rewards are intrinsic and time-based (r_a = υ * max(0, t_b - t_a); r_b = -υ * t_b), encouraging Alice to pick tasks just beyond Bob's horizon and Bob to improve.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agents used to generate curricula for embodied robotic policies; Bob's policy is later deployed on physical robots.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Reference environment (E_ref) for Alice; randomized environment instances (E_rand) for Bob sampled by ADR particles</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>E_ref is the default simulator instantiation (no randomization) used by Alice to find achievable goals; E_rand are environment instances with perturbed simulator parameters (friction, damping, torques, gains) sampled from a domain randomization space Ξ.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Goal difficulty via time-to-completion (t_a, t_b) and reachability; environment complexity via randomized parameter settings (N_rand and parameter ranges). Alice's intrinsic reward indirectly measures complexity as tasks where Bob's time to completion is high or impossible.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>Not numeric beyond the task instance; complexity is implicitly defined relative to Bob's current capability (curriculum 'horizon').</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Environment variation is provided by ADR particles sampling from the domain randomization space Ξ; ADR uses SVPG with N particles to explore environment variation.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>Varies during training as ADR particles evolve; paper uses N=8 ADR particles. Variation is therefore dynamic and not fixed (medium-to-high depending on the task).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Alice/Bob intrinsic rewards based on time and success (used for learning); downstream evaluation uses Bob's final distance-to-goal and transfer performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Not reported as standalone numeric values for Alice/Bob; their effectiveness is shown indirectly via improved Bob policy performance (see SS-ADR entry).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Explicit: Alice selects goals in E_ref that are easy there but difficult in E_rand; ADR particles are trained with Alice's reward so the environment curriculum evolves in lockstep with goal difficulty, producing a coupled goal-environment curriculum to avoid degenerate training (impossible goals or impossible environments).</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Asymmetric self-play (Alice/Bob) operating together with ADR: Alice trained to set progressively harder goals; Bob trained to achieve them in randomized environments; both trained with DDPG on intrinsic self-play rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Using Alice to set goals while training ADR particles with Alice's rewards leads to co-evolved curricula that produce policies (Bob) that generalize better to held-out and real-world environments compared to baselines without environment curriculum.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Alice and Bob trained within the same 1,000,000 unlabelled self-play interactions budget; specifics of per-agent sample usage not separately enumerated.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Asymmetric self-play provides a self-supervised measure of curriculum difficulty (based on the 'horizon' between Alice and Bob) that, when used to train ADR particles, structures environment variation and prevents ADR exploitability issues; coupling self-play goals with ADR yields more useful training curricula than either method alone.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating Automatic Curricula via Self-Supervised Active Domain Randomization', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1009.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1009.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ADR particles</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Active Domain Randomization (ADR) particles (SVPG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learned environment-sampling mechanism that uses Stein Variational Policy Gradient (SVPG) particles to select informative environment parameterizations from a domain randomization space, optimizing a learned reward signal to produce a curriculum of environment difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Active domain randomization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ADR SVPG particles</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A set of parameterized samplers (μ_{φ_i}) trained with SVPG updates to propose environment instances (values of ξ in Ξ); particles are updated using returns computed from rewards that measure discrepancy or difficulty, here replaced or guided by Alice's intrinsic reward to mitigate exploitability.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>environment-sampler agents (meta-agents controlling simulated environment instances)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Domain randomization space Ξ (simulator parameter space)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Ξ is the set of simulator parameters (e.g., friction, damping, actuator torques, gains); each particle proposes specific ξ that instantiates a new MDP in the simulator for training the embodied agent.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Measured by the dimensionality and range of Ξ (N_rand) and by the discriminator or self-supervised reward J(μ_{φ_i}) indicating how difficult proposed environments are for the current policy.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>Particle proposals can span from easy (close to reference) to very hard (far in parameter space); paper uses N_rand values like 2 (Pusher) and 8 (Reacher) to indicate lower vs higher environment parameter complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Number of ADR/SVPG particles N (here N = 8), and the diversity induced by the kernel k(·,·) in SVPG updates; variation is the spread of particle proposals across Ξ.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>Dynamic and learned; initialized to cover specified parameter bounds and concentrated by SVPG updates towards informative environment regions (medium-to-high variation depending on particle spread).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Utility measured by downstream policy performance (Bob's final distance-to-goal) and particle return objective J(μ_{φ_i}) computed from Alice's reward when replacing discriminator-based reward.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: ADR particles trained with Alice's self-supervised reward produce environment curricula that, when coupled with Alice/Bob self-play, lead to better downstream policy performance (SS-ADR) than uniform randomization; exact particle-level numeric returns not given in text.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — ADR provides variation across complexity axes by selecting environment instances of varying difficulty; the paper discusses a trade-off where ADR's learned reward can be exploited to propose impossible environments, and shows that conditioning ADR updates on Alice's reward alleviates this exploitability and yields productive curricula.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Active Domain Randomization using SVPG with N=8 particles, updated using Alice's intrinsic reward (instead of discriminator reward) to co-evolve environment curricula with goal curricula.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>ADR particles trained with self-supervised rewards produce environment variations that help policies generalize better at zero-shot to unseen and real-world instantiations compared to uniform domain randomization and ADR with learned discriminator rewards (which can be exploited).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>ADR particles updated throughout the 1,000,000 self-play interactions; number of environment proposals equals number of episodes sampled during training (evaluations every 5,000 timesteps reported).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Learning which environment instances to expose the agent to (via ADR) is more effective than uniform random sampling; coupling ADR with Alice's self-play reward creates a stable, informative environment curriculum and mitigates ADR exploitability that arises with discriminator-based rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating Automatic Curricula via Self-Supervised Active Domain Randomization', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Active domain randomization <em>(Rating: 2)</em></li>
                <li>Intrinsic motivation and automatic curricula via asymmetric self-play <em>(Rating: 2)</em></li>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>Diversity is all you need: Learning skills without a reward function <em>(Rating: 1)</em></li>
                <li>Hindsight experience replay <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1009",
    "paper_id": "paper-211171741",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "SS-ADR",
            "name_full": "Self-Supervised Active Domain Randomization",
            "brief_description": "A method that jointly learns a goal curriculum via asymmetric self-play (Alice/Bob) and an environment curriculum via Active Domain Randomization (ADR) particles to produce robust policies that transfer zero-shot to real robots without any extrinsic reward during training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Bob (policy trained under SS-ADR)",
            "agent_description": "A goal-conditioned policy trained with Deep Deterministic Policy Gradients (DDPG) inside the SS-ADR framework; Bob is the executor in the asymmetric self-play pair and learns from intrinsic time-based rewards (Equation 2) generated during self-play episodes across randomized environments.",
            "agent_type": "robotic agent (simulated during training, deployed on physical Poppy Ergo Jr robots for zero-shot transfer)",
            "environment_name": "ErgoPusher and ErgoReacher (simulated randomized MDPs and real-world instantiations)",
            "environment_description": "Continuous-control robotics tasks: ErgoPusher (3DoF arm pushes a puck to a goal) and ErgoReacher (4DoF arm moves end-effector to a goal). Environments are varied by simulator parameter randomizations such as puck friction and damping (Pusher) and joint torques / gains (Reacher). Intuitively hard / held-out test environments are created by setting low friction/damping (\"icy\" puck) or extremely low torques leading to non-recoverable states.",
            "complexity_measure": "Characterized by task difficulty and number of randomized simulation parameters (N_rand); e.g., ErgoPusher uses N_rand = 2 (puck damping, puck friction), ErgoReacher uses N_rand = 8 (multiple joint torques/gains). Task difficulty additionally measured by whether dynamics create non-recoverable states (e.g., very low torques).",
            "complexity_level": "ErgoPusher: low-to-medium (N_rand=2, simpler domain); ErgoReacher: high (N_rand=8, higher-dimensional randomized parameters); held-out \"intuitively hard\" test environments represent high task difficulty.",
            "variation_measure": "Environment variation measured as the dimensionality and range of the domain randomization space Ξ (Ξ ∈ R^{N_rand}), and by the ADR particle distribution over Ξ; number of ADR/SVPG particles N = 8 controls exploration of environment instances.",
            "variation_level": "Training variation: medium-to-high (structured by ADR particles across the specified parameter ranges). ErgoPusher has lower randomized dimensionality (N_rand=2) while ErgoReacher has higher (N_rand=8).",
            "performance_metric": "Final distance to goal (lower is better); variance/spread of final distances across trials; zero-shot transfer average final distance across 25 trials on real robot; learning curves over training steps.",
            "performance_value": "Qualitative: SS-ADR attains lower final distance-to-goal and lower variance (more consistent performance) than Uniform Domain Randomization (UDR) and than Unsupervised-Default on both in-distribution and held-out harder environments; exact numeric values are not reported in the paper text (figures only).",
            "complexity_variation_relationship": "Yes — the paper explicitly argues and demonstrates that co-evolving the goal curriculum and environment curriculum (i.e., coupling environment variation and goal difficulty) yields better generalization: increasing environment variation (via ADR) while matching goal difficulty (via self-play) avoids degenerate training (impossible goals or impossible environments) and produces multiplicative benefits compared to varying only goals or only environments. It also notes a trade-off in ADR: learned environment rewards can be exploited to create impossible environments, which SS-ADR mitigates by using self-supervised (Alice) rewards.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Coupled curriculum learning: asymmetric self-play (goal curriculum) + Active Domain Randomization (environment curriculum); policies trained with DDPG on intrinsic self-play rewards while ADR particles (SVPG, N=8) select environment instances.",
            "generalization_tested": true,
            "generalization_results": "Zero-shot transfer to real Poppy Ergo Jr robots was performed: SS-ADR outperformed UDR and the self-play-only baseline (Unsupervised-Default) in average final distance and consistency across multiple real-world parameter instantiations (different motor torques or puck friction regimes). Held-out hard test environments (outside training distribution) showed SS-ADR maintained stronger performance and lower variance than baselines.",
            "sample_efficiency": "Training used 1,000,000 unlabelled self-play interactions; policies were evaluated every 5,000 timesteps (200 evaluations total); reported curves are mean-averaged across 4 seeds.",
            "key_findings": "Co-evolving environment variation and goal difficulty via SS-ADR produces more robust, lower-variance policies that generalize better to held-out and real-world conditions than Uniform Domain Randomization and than using self-play alone; ADR alone can exploit simulator physics to produce impossibly hard environments, but replacing ADR's learned discriminator reward with self-supervised asymmetric self-play rewards mitigates exploitability and stabilizes curriculum learning.",
            "uuid": "e1009.0",
            "source_info": {
                "paper_title": "Generating Automatic Curricula via Self-Supervised Active Domain Randomization",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "Alice/Bob self-play",
            "name_full": "Asymmetric Self-Play (Alice and Bob)",
            "brief_description": "A self-supervised curriculum mechanism where Alice sets goals (operates in the reference environment) and Bob attempts to achieve them; rewards are time-based so Alice proposes goals just beyond Bob's current capability and Bob learns to expand capability.",
            "citation_title": "Intrinsic motivation and automatic curricula via asymmetric self-play",
            "mention_or_use": "use",
            "agent_name": "Alice (goal setter) and Bob (goal executor)",
            "agent_description": "Two DDPG policies: Alice proposes goals (and signals STOP) in the reference environment; Bob receives the goal and attempts to reach it in randomized environments. Rewards are intrinsic and time-based (r_a = υ * max(0, t_b - t_a); r_b = -υ * t_b), encouraging Alice to pick tasks just beyond Bob's horizon and Bob to improve.",
            "agent_type": "simulated agents used to generate curricula for embodied robotic policies; Bob's policy is later deployed on physical robots.",
            "environment_name": "Reference environment (E_ref) for Alice; randomized environment instances (E_rand) for Bob sampled by ADR particles",
            "environment_description": "E_ref is the default simulator instantiation (no randomization) used by Alice to find achievable goals; E_rand are environment instances with perturbed simulator parameters (friction, damping, torques, gains) sampled from a domain randomization space Ξ.",
            "complexity_measure": "Goal difficulty via time-to-completion (t_a, t_b) and reachability; environment complexity via randomized parameter settings (N_rand and parameter ranges). Alice's intrinsic reward indirectly measures complexity as tasks where Bob's time to completion is high or impossible.",
            "complexity_level": "Not numeric beyond the task instance; complexity is implicitly defined relative to Bob's current capability (curriculum 'horizon').",
            "variation_measure": "Environment variation is provided by ADR particles sampling from the domain randomization space Ξ; ADR uses SVPG with N particles to explore environment variation.",
            "variation_level": "Varies during training as ADR particles evolve; paper uses N=8 ADR particles. Variation is therefore dynamic and not fixed (medium-to-high depending on the task).",
            "performance_metric": "Alice/Bob intrinsic rewards based on time and success (used for learning); downstream evaluation uses Bob's final distance-to-goal and transfer performance.",
            "performance_value": "Not reported as standalone numeric values for Alice/Bob; their effectiveness is shown indirectly via improved Bob policy performance (see SS-ADR entry).",
            "complexity_variation_relationship": "Explicit: Alice selects goals in E_ref that are easy there but difficult in E_rand; ADR particles are trained with Alice's reward so the environment curriculum evolves in lockstep with goal difficulty, producing a coupled goal-environment curriculum to avoid degenerate training (impossible goals or impossible environments).",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Asymmetric self-play (Alice/Bob) operating together with ADR: Alice trained to set progressively harder goals; Bob trained to achieve them in randomized environments; both trained with DDPG on intrinsic self-play rewards.",
            "generalization_tested": true,
            "generalization_results": "Using Alice to set goals while training ADR particles with Alice's rewards leads to co-evolved curricula that produce policies (Bob) that generalize better to held-out and real-world environments compared to baselines without environment curriculum.",
            "sample_efficiency": "Alice and Bob trained within the same 1,000,000 unlabelled self-play interactions budget; specifics of per-agent sample usage not separately enumerated.",
            "key_findings": "Asymmetric self-play provides a self-supervised measure of curriculum difficulty (based on the 'horizon' between Alice and Bob) that, when used to train ADR particles, structures environment variation and prevents ADR exploitability issues; coupling self-play goals with ADR yields more useful training curricula than either method alone.",
            "uuid": "e1009.1",
            "source_info": {
                "paper_title": "Generating Automatic Curricula via Self-Supervised Active Domain Randomization",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "ADR particles",
            "name_full": "Active Domain Randomization (ADR) particles (SVPG)",
            "brief_description": "A learned environment-sampling mechanism that uses Stein Variational Policy Gradient (SVPG) particles to select informative environment parameterizations from a domain randomization space, optimizing a learned reward signal to produce a curriculum of environment difficulty.",
            "citation_title": "Active domain randomization",
            "mention_or_use": "use",
            "agent_name": "ADR SVPG particles",
            "agent_description": "A set of parameterized samplers (μ_{φ_i}) trained with SVPG updates to propose environment instances (values of ξ in Ξ); particles are updated using returns computed from rewards that measure discrepancy or difficulty, here replaced or guided by Alice's intrinsic reward to mitigate exploitability.",
            "agent_type": "environment-sampler agents (meta-agents controlling simulated environment instances)",
            "environment_name": "Domain randomization space Ξ (simulator parameter space)",
            "environment_description": "Ξ is the set of simulator parameters (e.g., friction, damping, actuator torques, gains); each particle proposes specific ξ that instantiates a new MDP in the simulator for training the embodied agent.",
            "complexity_measure": "Measured by the dimensionality and range of Ξ (N_rand) and by the discriminator or self-supervised reward J(μ_{φ_i}) indicating how difficult proposed environments are for the current policy.",
            "complexity_level": "Particle proposals can span from easy (close to reference) to very hard (far in parameter space); paper uses N_rand values like 2 (Pusher) and 8 (Reacher) to indicate lower vs higher environment parameter complexity.",
            "variation_measure": "Number of ADR/SVPG particles N (here N = 8), and the diversity induced by the kernel k(·,·) in SVPG updates; variation is the spread of particle proposals across Ξ.",
            "variation_level": "Dynamic and learned; initialized to cover specified parameter bounds and concentrated by SVPG updates towards informative environment regions (medium-to-high variation depending on particle spread).",
            "performance_metric": "Utility measured by downstream policy performance (Bob's final distance-to-goal) and particle return objective J(μ_{φ_i}) computed from Alice's reward when replacing discriminator-based reward.",
            "performance_value": "Qualitative: ADR particles trained with Alice's self-supervised reward produce environment curricula that, when coupled with Alice/Bob self-play, lead to better downstream policy performance (SS-ADR) than uniform randomization; exact particle-level numeric returns not given in text.",
            "complexity_variation_relationship": "Yes — ADR provides variation across complexity axes by selecting environment instances of varying difficulty; the paper discusses a trade-off where ADR's learned reward can be exploited to propose impossible environments, and shows that conditioning ADR updates on Alice's reward alleviates this exploitability and yields productive curricula.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Active Domain Randomization using SVPG with N=8 particles, updated using Alice's intrinsic reward (instead of discriminator reward) to co-evolve environment curricula with goal curricula.",
            "generalization_tested": true,
            "generalization_results": "ADR particles trained with self-supervised rewards produce environment variations that help policies generalize better at zero-shot to unseen and real-world instantiations compared to uniform domain randomization and ADR with learned discriminator rewards (which can be exploited).",
            "sample_efficiency": "ADR particles updated throughout the 1,000,000 self-play interactions; number of environment proposals equals number of episodes sampled during training (evaluations every 5,000 timesteps reported).",
            "key_findings": "Learning which environment instances to expose the agent to (via ADR) is more effective than uniform random sampling; coupling ADR with Alice's self-play reward creates a stable, informative environment curriculum and mitigates ADR exploitability that arises with discriminator-based rewards.",
            "uuid": "e1009.2",
            "source_info": {
                "paper_title": "Generating Automatic Curricula via Self-Supervised Active Domain Randomization",
                "publication_date_yy_mm": "2020-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Active domain randomization",
            "rating": 2,
            "sanitized_title": "active_domain_randomization"
        },
        {
            "paper_title": "Intrinsic motivation and automatic curricula via asymmetric self-play",
            "rating": 2,
            "sanitized_title": "intrinsic_motivation_and_automatic_curricula_via_asymmetric_selfplay"
        },
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2,
            "sanitized_title": "domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world"
        },
        {
            "paper_title": "Diversity is all you need: Learning skills without a reward function",
            "rating": 1,
            "sanitized_title": "diversity_is_all_you_need_learning_skills_without_a_reward_function"
        },
        {
            "paper_title": "Hindsight experience replay",
            "rating": 1,
            "sanitized_title": "hindsight_experience_replay"
        }
    ],
    "cost": 0.011642,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Generating Automatic Curricula via Self-Supervised Active Domain Randomization</p>
<p>Sharath Chandra Raparthy 
Bhairav Mehta 
Université de Montréal</p>
<p>Florian Golemo 
Université de Montréal</p>
<p>Liam Paull 
Université de Montréal</p>
<p>CIFAR AI Chair 4 Element AI</p>
<p>Mila 
Generating Automatic Curricula via Self-Supervised Active Domain Randomization</p>
<p>Goal-directed Reinforcement Learning (RL) traditionally considers an agent interacting with an environment, prescribing a real-valued reward to an agent proportional to the completion of some goal. Goal-directed RL has seen large gains in sample efficiency, due to the ease of reusing or generating new experience by proposing goals. In this work, we build on the framework of self-play, allowing an agent to interact with itself in order to make progress on some unknown task. We use Active Domain Randomization and self-play to create a novel, coupled environment-goal curriculum, where agents learn through progressively more difficult tasks and environment variations. Our method, Self-Supervised Active Domain Randomization (SS-ADR), generates a growing curriculum, encouraging the agent to try tasks that are just outside of its current capabilities, while building a domain-randomization curriculum that enables state-of-the-art results on various sim2real transfer tasks. Our results show that a curriculum of co-evolving the environment difficulty along with the difficulty of goals set in each environment, provides practical benefits in the goal-directed tasks tested.</p>
<p>Introduction</p>
<p>The classic Markov Decision Process (MDP)-based formulation of RL can be extended with goals to contextualize actions and enable higher sample-efficiency (see e.g. [Schaul et al., 2015] [Andrychowicz et al., 2017]). These methods work by allowing the agent to set its own goals, rather than exclusively relying on the environment to provide these. However, when setting new goals, the onus falls on the experimenter to decide which goals to use. Not all experience is equally useful for learning. As a result, past works have resorted to simple random sampling [Andrychowicz et al., 2017] or learning an expensive generative model to generate relevant goals [Held et al., 2017]. * Correspondence to raparths@mila.quebec Figure 1: Self-Supervised Active Domain Randomization learns (SS-ADR) robust policies (h) via self-play by co-evolving a goal curriculum, set by Alice (e), alongside an environment curriculum, set by the ADR particles (j). The randomized environments (c) and goals (g) slowly increase in difficulty, leading to strong zero shot transfer on all environments tested.</p>
<p>In the framework of self-play, the agent can set goals for itself, using only unlabelled interactions with the environment (i.e., no evaluation of the true reward function). While many heuristics for this self-play goal curriculum exist, we focus on the framework of Asymmetric Self-Play [Sukhbaatar et al., 2017], which learns a goal-setting policy via time-based heuristics. The idea is that the most "productive" goals for an agent to see are just out of the agent's understanding or horizon. If goals are too easy or too hard, the experience will not be useful, making the horizon approach a strong option to pursue.</p>
<p>However, in certain cases, just learning a goal curriculum via self-play is not enough. In robotic RL, policies trained purely in the simulation have proved difficult to transfer to the real world, a problem known as "reality gap" [Jakobi et al., 1995]. One leading approach for this sim2real transfer is Domain Randomization (DR) , where a simulator's parameters are perturbed, generating a space of related-but-different environments, all of which an agent tries to solve before transferring to a real robot. Nevertheless, like the goal curriculum issue, the issue once again be-comes a question of which environments to show the agent. Recently, [Mehta et al., 2019] empirically showed that not all generated environments are equally useful for learning, leading to Active Domain Randomization (ADR). ADR defines a curriculum learning problem in the environment randomized space, using learned rewards to search for an optimal curriculum.</p>
<p>As our work deals with both robotics and goal-directed RL, we combine ADR and Asymmetric Self Play to propose Self-Supervised Active Domain Randomization (SS-ADR). SS-ADR couples the environment and goal space, learning a curriculum across both simultaneously. SS-ADR can transfer to real-world robotic tasks without ever evaluating the true reward function during training, learning a policy completely via self-supervised reward signals. We show that this coupling generates strong robotic policies in all environments tested, even across multiple robots and simulation settings.</p>
<p>Background</p>
<p>Reinforcement Learning</p>
<p>We consider a Markov Decision Process (MDP), M, defined by (S, A, T , R, γ), where S is the state space, A is the action space, T : S × A → S is the transition function, R : S × A → R and γ is the discount factor. Formally, the agent receives a state s t ∈ S at the timestep t and takes an action a t based on the policy π θ . The environment gives a reward of r t and the agent transitions to next state s t+1 . The goal of RL is to find a policy π θ which maximizes the expected return from each state s t where the return R t is given by R t = ∞ k=0 γ k r t+k . Goal-directed RL often appends a goal (some g in a goal space G) to the state, and requires the goal when evaluating the reward function (i.e R : S × G × A → R)</p>
<p>Curriculum Learning</p>
<p>Curriculum learning is a strategy of training machine learning models on a series of gradually increasing tasks (from easy to hard) [Bengio et al., 2009]. In curriculum learning, the focus lies on the order of tasks, often abstracting away the particular learning of the task itself. In general, task curricula are crafted in such a way that the future task is just beyond the agent's current capabilities. However, when an explicit ordering of task difficulty is not available, careful design of the curriculum is required to overcome optimization failures.</p>
<p>Self-Play</p>
<p>We consider the self-play framework by [Sukhbaatar et al., 2017], which proposes an unsupervised way of learning to explore the environment. In this method, the agent has two brains: Alice, which sets a task, and Bob, which finishes the assigned task. The novelty of this method can be attributed to the elegant reward design given by Equations 1 and 2,
r a = υ * max(0, t b − t a )
(1)
r b = −υ * t b(2)
where t a is the timesteps taken by Alice to set a task, t b is the timesteps taken by Bob to finish the task set by Alice and υ is the scaling factor. This reward design allows selfregulating feedback between both agents, as Alice focuses on tasks that are just beyond Bob's horizon: Alice tries to propose tasks that are easy for her, yet difficult for Bob. This evolution of tasks forces the two agents to construct a curriculum for exploration automatically.</p>
<p>However, in the original work, the unsupervised self-play is used only as supplementary experience. In order to learn better policies on a target task, Bob still requires a majority of trajectories where the reward is evaluated from the environment.</p>
<p>Domain Randomization</p>
<p>Domain Randomization [Sadeghi and Levine, 2017],  is a technique in which we provide enough variability during the training time such that during the test time, the model generalizes well on potentially unseen data. It requires the explicit definition of a set of N rand simulation parameters like friction, damping, etc., and a randomization space Ξ ∈ R N rand . During every episode, a set of parameters ξ ∈ Ξ are sampled to generate a new MDP when passed through the simulator (S). If J(π θ ) is the cumulative return of the policy π θ (·; ξ) in the MDP parameterized by ξ, then the goal is to maximize this expected return across the distribution of such MDPs. The hope is that, when this model is deployed on an unseen environment, like a real robot (in a zeroshot transfer scenario), the policy generalizes well enough to maintain strong performance.</p>
<p>Active Domain Randomization
ADR [Mehta et al., 2019
] is a framework that searches for most informative environment instances, unlike the uniform sampling in DR . ADR formulates this as an RL problem, where the sampling policy is parameterized by Stein's Variational Policy Gradient (SVPG) [Liu et al., 2017], to learn a set of particles {µ φi } N i=1 which control which environments are shown to the agent. The particles undergo interacting updates, which can be written as:
µ φi ← µ φi + N Σ N j=1 [∇ µ φ j J(µ φj )k(µ φi , µ φj ) + α∇ µ φ j k(µ φi , µ φj )],(3)
where J(µ φi ) denotes the sampled return from particle i, the learning rate and temperature α are hyperparameters.</p>
<p>The particles are trained by using learned discriminatorbased rewards r D [Eysenbach et al., 2018] , which measure the discrepancies between the trajectories from the reference E ref and randomized environment instances E i .
r D = log D ψ (y|τ i ∼ π(·; E i ))(4)
The authors claim that ADR finds environments which are difficult for the current agent policy to solve via learnable discrepancies between the reference (generally, easier) environment, and a proposed randomized instance. While the formulation benefits from learned rewards, ADR also suffers from an exploitability problem, as the authors mention in the paper's appendix. Equation 4 finds (and rewards) environments where the discrepancy can be maximized, leading to situations where the method exploits the physics of simulation via generation of "impossible to solve" environments. The original work proposed iteratively adjusting the bounds of the randomization space as workaround for the exploitability issue.</p>
<p>Related Work</p>
<p>The idea of curriculum leaning was first proposed by [Elman, 1993], who showed that the curriculum of tasks is beneficial in language processing. Later, [Bengio et al., 2009] extended this idea to various vision and language tasks which showed faster learning and better convergence. While many of these require some human specifications, recently, automatic task generation has gained interest in the RL community. This body of work includes automatic curriculum produced by adversarial training [Held et al., 2017], reverse curriculum [Florensa et al., 2017] [Forestier et al., 2017], teacher-student curriculum learning [Matiisen et al., 2017] [Graves et al., 2017] etc. However, many papers exploit (a) distinct tasks rather than continuous task spaces (b) state or reward-based "progress" heuristics. Our work builds upon the naturally growing curriculum formulation of Sukhbaatar et al. (2017), fixing some of its issues with stability-inducing properties.</p>
<p>Curriculum learning has also been studied through the lens of Self-Play. Self-play has been successfully applied to many games such as checkers [Samuel, 1959] and Go [Silver et al., 2016]. Recently an interesting asymmetric self-play strategy has been proposed [Sukhbaatar et al., 2017], which models a game between two variants of the same agent, Alice and Bob, enabling exploration of the environment without requiring any extrinsic reward. However, in this work, we use the self-play framework for learning a curriculum of goals, rather than for its traditional exploration-driven use case.</p>
<p>Despite the success in deep-RL, training RL algorithms on physical robots remains a difficult problem and often impractical due to safety concerns. Simulators played a huge role in transferring policies to the real robot safely, and many different methods have been proposed for the same , [Prakash et al., 2018], [Chebotar et al., 2018]. DR  is one of the popular methods which generates a multitude of environment instances by uniformly sampling the environment parameters from a fixed range. However, [Mehta et al., 2019] showed that DR suffers from high variance due to unstructured task space and instead proposed a novel algorithm that learns to sample the most informative environment instances. In our work, we use ADR formulation while mitigating some of the critical issues like exploitability by substituting the learned reward with the self-supervised reward.</p>
<p>Method</p>
<p>ADR allows for curriculum learning in an environment space: given some black box agent, trajectories are used to differentiate between the difficulty of environments, regardless of the goal set in the particular environment instance. In goaldirected RL, the goal itself may be the difference between a useful episode and a useless one. In particular, certain goals within the same environment instance may vary in difficulty; on the other hand, the same goal may vary in terms of reachability in different environments. ADR provides a curriculum in environment space, but with goal-directed environments, we have a new dimension to consider; one that the standard ADR formulation does not account for.</p>
<p>In order to build proficient, generalizable agents, we need to evolve a curriculum in goal space alongside a curriculum in environment space; otherwise, we may find degenerate solutions by proposing impossible goals with any environment, or vice versa. As shown in [Sukhbaatar et al., 2017], selfplay provides a way for policies to learn without environment interaction, but when used only for goal curricula, requires interleaving of self play trajectories alongside reward-evaluated rollouts for best performance.</p>
<p>To this end, we propose Self-Supervised Active Domain Randomization (SS-ADR), summarized in Algorithm 1. SS-ADR learns a curriculum in the joint goal-environment space, producing strong, generalizable policies without ever evaluating an environment reward function during training.</p>
<p>SS-ADR learns two additional policies: Alice and Bob. Alice and Bob are trained in the same format described in Algorithm 1 and [Sukhbaatar et al., 2017]. Alice sets a goal in the environment, and eventually signals a STOP action. The environment is reset to the starting state, and now uses Bob's policy to attempt to achieve the goal Alice has set. Bob sees Alice's goal state appended to the current state, while Alice sees the current state appended to it's initial state. Alice and Bob are trained via DDPG [Silver et al., 2014], using Equations 1 and 2 to generate rewards for each trajectory based on the time each agent took to complete the task (denoted by t a and t b ).</p>
<p>The reward structure forces Alice to focus on horizons: her reward is maximized when she can do something quickly that Bob cannot do at all. Considering the synchrony of policy updates for each agent, we presume that the goal set by Alice is not far out of Bob's current reach.</p>
<p>However, before Bob operates in the environment, the environment is randomized (e.g. object frictions are perturbed or robot torques are changed). Alice, who operates in the reference environment, E ref (an environment given as the "default"), tries to find goals that are easy in the reference environment (E ref ), but difficult in the randomized ones (E rand ). Since the randomizations themselves are prescribed by the ADR particles, when we train the ADR particles with Alice's reward (i.e Equation 1 is evaluated separately for each randomization tested), we get a co-evolution on both curriculum levels. The curriculum in both goal and environment space evolve in difficulty simultaneously, leading to state-of-the-art performance in goal-directed, real world robotic tasks.</p>
<p>Implementation</p>
<p>Across all experiments, all networks share the same network architecture and hyperparameters. For each Alice and Bob policy, we use Deep Deterministic Policy Gradients [Silver et al., 2014], using an implementation from [Fujimoto et al., 2018]. Each actor and the critic have two hidden layers with Algorithm 1 Self Supervised ADR 1: Input: Ξ: Randomization space, S: Simulator (S : Ξ → E), ξ ref : reference parameters 2: Initialize π a : Alice's policy, π b : Bob's policy, µ φ : SVPG particles 3: for T max timesteps do 4: t a ← 0 5:
E ref ← S(ξ ref ) 6:
Observe the initial state s o 7:</p>
<p>while a ta is not STOP do 
a t b ∼ π b (s t b , s * ; E rand ) 20: end while 21:
Compute Alice's reward using Eq (1)   22: Compute Bob's reward using Eq (2) 23:</p>
<p>Update the particles using Eq (3) 24:</p>
<p>with r a update Alice's policy π a : 25:
θ a ← θ a + α 1 ∇ θa J(π a ) 26:
with r b update Bob's policy π b : 27:
θ b ← θ b + α 2 ∇ θ b J(π b ) 28: end for
tion. For Alice's stopping policy (which signals the STOP action), we use a multi-layered perceptron with two hidden layers consisting of 300 neurons each. All networks use the Adam optimizer [Kingma and Ba, 2014] with standard hyperparameters from the Pytorch implementation 1 . We use a learning rate α 1 = α 2 = 0.001, discount factor γ = 0.99, reward scaling factor υ = 0.1 and number of ADR/SVPG particles N = 8. In all our self-play experiments, we consider 1 million unlabelled self-play interactions and plot the mean-averaged learning curves across 4 seeds. 2 All of the corresponding code and experiments can be found in the supplementary material.</p>
<p>Results</p>
<p>In order to evaluate our method, we perform various experiments on continuous control robotic tasks both in simulation and real world. We used the following environments from  • ErgoPusher: A 3DoF robotic arm similar to [Haarnoja et al., 2018] that has to push the puck to the goal ( Figure  3)</p>
<p>For the sim-to-real experiments, we recreated the simulation environment on the real Poppy Ergo. Jr robots [Lapeyre, 2014] shown in Figures 2b and 3b. All simulated experiments are run across 4 seeds. We evaluate the policy on (a) the default environment and (b) an intuitively hard environment which lies outside the training domain, for every 5000 timesteps, accounting to 200 evaluations in total over 1 million timesteps. Unlike the self-play framework proposed in [Sukhbaatar et al., 2017], we do not explicitly train Bob on the target task with extrinsic rewards from the environment to learn a policy. Instead, we evaluate the policy trained only with intrinsic rewards, making the approach completely selfsupervised.</p>
<p>We compare our method against two different baselines:</p>
<p>• Uniform Domain Randomization (UDR): We use UDR, which generates a multitude of tasks by uniformly sampling parameters from a given range as our first baseline. The environment space generated by UDR is unstructured, where the difficulty greatly varies. Here the curriculum of goal space is not considered. • Unsupervised Default: We use the self-play framework to generate a naturally growing curriculum of goals as our second baseline. Here, only the goal curriculum (and not the coupled environment curriculum) is considered. Figure 4: On the default (in-distribution) environment, both the self-play method, shown as Unsupervised-Default, and SS-ADR show strong performance. Even on an easier task, we see issues with UDR, which is unstable in both performance and convergence throughout training. Shown is final distance to goal, lower is better.</p>
<p>Simulation Experiments</p>
<p>We explore the significance of SS-ADR's performance on the ErgoPusher and ErgoReacher tasks. In the ErgoPusher task, we vary puck damping and puck friction (N rand = 2). In order to create an intuitively hard environment, we lower the values of these two parameters, which creates an "icy" surface, ensuring that the puck needs to be hit carefully to complete the difficult task. Figure 5: When we test the Reacher policies on a harder, held-out test environment (i.e where torques are dropped to a minimum, leading to non-recoverable states in the MDP), we see that only SS-ADR converges with low variance and strong performance. Both UDR and Unsupervised-Default struggle on the held out environment. Shown is final distance to goal, lower is better.</p>
<p>For the ErgoReacher task, we increase the randomization dimensions (N rand = 8) making it hard to intuitively in- Figure 6: Final distance to goal, lower is better. In the Pusher environment, we see the same narrative as in Figure 4; UDR struggles even in the easy, in-distribution environment, while both self-play methods converge quickly with low variance. fer the environment complexity. However, for the demonstration purposes, we create an intuitively hard environment by assigning extremely low torques and gains for each joint. We adapt the parameter ranges from the GitHub repository of Mehta et al.  From Figure 4 and 6 we can see that both Unsupervised-Default and SS-ADR significantly outperform UDR both in terms of variance and average final distance. This highlights that the uniform sampling in UDR can lead to unpredictable and inconsistent behaviour. To actually see the benefits of environment-goal curriculum over solely goal curriculum, we evaluate on the intuitively-hard environments (outside of the training parameter distribution, as described above). From Figure 5 and 7, we can see that our method, SS-ADR, which co-evolves environment and goal curriculum, outperforms Unsupervised-Default, which omits the environment curriculum. This shows that the coupling curriculum enables strong generalization performance over the standard self-play formulation. Figure 8: On various instantiations of the real robot (parameterized by motor torques), SS-ADR outperforms UDR in terms of performance (lower is better) and spread. While SS-ADR's performance is almost consistent with or better than that of the Unsupervised-Default.</p>
<p>Sim-to-Real Transfer Experiments</p>
<p>In this section, we explore the zero-shot transfer performance of the trained policies in the simulator. To test our policies on real-robots, we take the four independent trained policies of both ErgoReacher and ErgoPusher and deploy them onto the real-robots without any fine-tuning. We roll out each policy per seed for 25 independent trails and evaluate the average final distance across 25 trails. To evaluate the generalization, we change the task definitions (and therefore the MDPs) of the puck friction (across low, high, and standard frictions in a box pushing environment) in case of ErgoPusher and joint torques (across a wide spectrum of choices) on ErgoReacher. In general, lower values in both settings correspond to harder tasks, due to construction of the robot and the intrinsic difficulty of the task itself.</p>
<p>From the Figures 8 and 9, we see that SS-ADR outperforms both baselines in terms of accuracy and consistency, leading to robust performance across all environment variants tested. Zero-shot policy transfer is a difficult and dangerous task, meaning that low spread (i.e consistent performance) is required for deployed robotic RL agents. As we can see in the plots, simulation alone is not the answer (leading to poor performance of UDR), while self-play also fails sometimes to generate curricula that allow for strong, generalizable policies. However, by utilizing both methods together, and co-evolving the two curriculum spaces, we see multiplicative benefits of using curriculum learning in each separately. </p>
<p>Conclusion</p>
<p>In this work, we proposed Self-Supervised Active Domain Randomization (SS-ADR), which co-evolves curricula in a joint goal-environment task space to create strong, robust policies that can transfer zero-shot onto real world robots. Our method requires no evaluation of training environment reward functions, and learns this joint curriculum entirely through self-play. SS-ADR is a feasible approach to train new policies in goal-directed RL settings, and outperforms all baselines in both tasks (in simulated and real variants) tested.</p>
<p>∼ π a (s o , s ta ; E ref )</p>
<p>Figure 2 )Figure 2 :
22 and[Mehta et al., 2019]:• ErgoReacher: A 4DoF robotic arm where the endeffector has to reach the goal (ErgoReacher is a 4 DoF robotic arm, with both simulation and real world environments. The goal is to move the end effector to several imaginary goals (pink dot) as fast as possible, actuated with the four motors.</p>
<p>Figure 3 :
3ErgoPusher is a 3DoF robotic arm, with the goal of bringing a secondary object to an imaginary goal (pink dot).</p>
<p>Figure 7 :
7Final distance to goal, lower is better. Both self-play methods show higher variance in simulation in the Pusher environment, despite the fact that SS-ADR has better overall performance.</p>
<p>Figure 9 :
9We see the difference between the various methods clearly in the Pusher environment, where SS-ADR outperforms all other baselines. Lower is better.
https://pytorch.org/ 2 This is unlike the original results of [Sukhbaatar et al., 2017], where the x-axis labels consider only labeled interaction.
AcknowledgementsThe authors gratefully acknowledge the Natural Sciences and Engineering Research Council of Canada (NSERC), the Fonds de Recherche Nature et Technologies Quebec (FQRNT), Calcul Quebec, Compute Canada, the Canada Research Chairs, Canadian Institute for Advanced Research (CI-FAR) and Nvidia for donating a DGX-1 for computation. BM would like to thank IVADO for financial support. FG would like to thank MITACS for their funding and support.
Hindsight experience replay. CoRR. Andrychowicz, abs/1707.01495Proceedings of the 26th Annual International Conference on Machine Learning, ICML '09. the 26th Annual International Conference on Machine Learning, ICML '09New York, NY, USA; Ankur Handa, Viktor Makoviychuk, Miles Macklin; Issac, Nathan DYevgen ChebotarCurriculum learningReferences [Andrychowicz et al., 2017] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. CoRR, abs/1707.01495, 2017. [Bengio et al., 2009] Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learn- ing. In Proceedings of the 26th Annual International Con- ference on Machine Learning, ICML '09, New York, NY, USA, 2009. ACM. [Chebotar et al., 2018] Yevgen Chebotar, Ankur Handa, Viktor Makoviychuk, Miles Macklin, Jan Issac, Nathan D.</p>
<p>Closing the sim-to-real loop: Adapting simulation randomization with real world experience. Dieter Ratliff, ; Fox, Jeffrey L Elman, abs/1810.05687Cognition. 481CoRRLearning and development in neural networks: The importance of starting smallRatliff, and Dieter Fox. Closing the sim-to-real loop: Adapting simulation randomization with real world expe- rience. CoRR, abs/1810.05687, 2018. [Elman, 1993] Jeffrey L Elman. Learning and development in neural networks: The importance of starting small. Cog- nition, 48(1), 1993.</p>
<p>Diversity is all you need: Learning skills without a reward function. [ Eysenbach, abs/1802.06070CoRR[Eysenbach et al., 2018] Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills without a reward function. CoRR, abs/1802.06070, 2018.</p>
<p>Reverse curriculum generation for reinforcement learning. [ Florensa, abs/1707.05300CoRR[Florensa et al., 2017] Carlos Florensa, David Held, Markus Wulfmeier, and Pieter Abbeel. Reverse curriculum gener- ation for reinforcement learning. CoRR, abs/1707.05300, 2017.</p>
<p>Intrinsically motivated goal exploration processes with automatic curriculum learning. [ Forestier, abs/1708.02190CoRR[Forestier et al., 2017] Sébastien Forestier, Yoan Mollard, and Pierre-Yves Oudeyer. Intrinsically motivated goal ex- ploration processes with automatic curriculum learning. CoRR, abs/1708.02190, 2017.</p>
<p>Addressing function approximation error in actor-critic methods. CoRR, abs/1802.09477. [ Fujimoto, PMLRProceedings of The 2nd Conference on Robot Learning. Aude Billard, Anca DraganThe 2nd Conference on Robot Learning87Sim-to-real transfer with neural-augmented robot simulation[Fujimoto et al., 2018] Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in actor-critic methods. CoRR, abs/1802.09477, 2018. [Golemo et al., 2018] Florian Golemo, Adrien Ali Taiga, Aaron Courville, and Pierre-Yves Oudeyer. Sim-to-real transfer with neural-augmented robot simulation. In Aude Billard, Anca Dragan, Jan Peters, and Jun Morimoto, edi- tors, Proceedings of The 2nd Conference on Robot Learn- ing, volume 87 of Proceedings of Machine Learning Re- search. PMLR, 29-31 Oct 2018.</p>
<p>Automated curriculum learning for neural networks. [ Graves, abs/1704.03003CoRR[Graves et al., 2017] Alex Graves, Marc G. Bellemare, Ja- cob Menick, Rémi Munos, and Koray Kavukcuoglu. Au- tomated curriculum learning for neural networks. CoRR, abs/1704.03003, 2017.</p>
<p>Composable deep reinforcement learning for robotic manipulation. [ Haarnoja, abs/1803.06773rensa, and Pieter Abbeel. Xinyang Geng, Carlos FloDavid HeldAutomatic goal generation for reinforcement learning agents. CoRR, abs/1705.06366[Haarnoja et al., 2018] Tuomas Haarnoja, Vitchyr Pong, Au- rick Zhou, Murtaza Dalal, Pieter Abbeel, and Sergey Levine. Composable deep reinforcement learning for robotic manipulation. CoRR, abs/1803.06773, 2018. [Held et al., 2017] David Held, Xinyang Geng, Carlos Flo- rensa, and Pieter Abbeel. Automatic goal generation for reinforcement learning agents. CoRR, abs/1705.06366, 2017.</p>
<p>Noise and the reality gap: The use of simulation in evolutionary robotics. [ Jakobi, European Conference on Artificial Life. Springer[Jakobi et al., 1995] Nick Jakobi, Phil Husbands, and Inman Harvey. Noise and the reality gap: The use of simulation in evolutionary robotics. In European Conference on Arti- ficial Life. Springer, 1995.</p>
<p>Adam: A method for stochastic optimization. Ba ; Diederik Kingma, Jimmy Kingma, Ba, International Conference on Learning Representations. 12[Kingma and Ba, 2014] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. Interna- tional Conference on Learning Representations, 12 2014.</p>
<p>Poppy: open-source, 3d printed and fully-modular robotic platform for science, art and education. Lapeyre ; Matthieu Lapeyre, 11Lapeyre, 2014] Matthieu Lapeyre. Poppy: open-source, 3d printed and fully-modular robotic platform for science, art and education. 11 2014.</p>
<p>Structured domain randomization: Bridging the reality gap by contextaware synthetic data. [ Liu, abs/1810.10093Real single-image flight without a single real image. Robotics: Science and Systems XIII. Aayush Prakash, Shaad Boochoon, Mark Brophy, David Acuna, Eric Cameracci, Gavriel State, Omer Shapira, and Stan BirchfieldCoRR2Teacher-student curriculum learning. CoRR, abs/1707.00183[Liu et al., 2017] Yang Liu, Prajit Ramachandran, Qiang Liu, and Jian Peng. Stein variational policy gradient. CoRR, abs/1704.02399, 2017. [Matiisen et al., 2017] Tambet Matiisen, Avital Oliver, Taco Cohen, and John Schulman. Teacher-student curriculum learning. CoRR, abs/1707.00183, 2017. [Mehta et al., 2019] Bhairav Mehta, Manfred Diaz, Florian Golemo, Christopher J. Pal, and Liam Paull. Active do- main randomization. CoRR, abs/1904.04762, 2019. [Prakash et al., 2018] Aayush Prakash, Shaad Boochoon, Mark Brophy, David Acuna, Eric Cameracci, Gavriel State, Omer Shapira, and Stan Birchfield. Structured do- main randomization: Bridging the reality gap by context- aware synthetic data. CoRR, abs/1810.10093, 2018. [Sadeghi and Levine, 2017] Fereshteh Sadeghi and Sergey Levine. Cad2rl: Real single-image flight without a sin- gle real image. Robotics: Science and Systems XIII, Jul 2017.</p>
<p>Mastering the game of Go with deep neural networks and tree search. Arthur L Samuel, Samuel, Schaul, PMLRSome studies in machine learning using the game of checkers. IBM Journal of Research and Development. David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc LanctotBejing, China; Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach3Nature. Sukhbaatar et al., 2017] Sainbayar Sukhbaatar. IlyaSamuel, 1959] Arthur L. Samuel. Some studies in machine learning using the game of checkers. IBM Journal of Re- search and Development, 3, 1959. [Schaul et al., 2015] Tom Schaul, Dan Horgan, Karol Gre- gor, and David Silver. Universal value function approxi- mators. In Proceedings of the 32nd International Confer- ence on International Conference on Machine Learning - Volume 37, ICML'15. JMLR.org, 2015. [Silver et al., 2014] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deterministic policy gradient algorithms. In Eric P. Xing and Tony Jebara, editors, Proceedings of the 31st Inter- national Conference on Machine Learning, volume 32 of Proceedings of Machine Learning Research, Bejing, China, 22-24 Jun 2014. PMLR. [Silver et al., 2016] David Silver, Aja Huang, Chris J. Mad- dison, Arthur Guez, Laurent Sifre, George van den Driess- che, Julian Schrittwieser, Ioannis Antonoglou, Veda Pan- neershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587), January 2016. [Sukhbaatar et al., 2017] Sainbayar Sukhbaatar, Ilya</p>
<p>Intrinsic motivation and automatic curricula via asymmetric self-play. Arthur Kostrikov, Rob Szlam, Fergus, Kostrikov, Arthur Szlam, and Rob Fergus. Intrinsic moti- vation and automatic curricula via asymmetric self-play.</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. Corr ; Tobin, abs/1703.05407Intelligent Robots and Systems. CoRR, abs/1703.05407, 2017. [Tobin et al., 2017] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain randomization for transferring deep neural net- works from simulation to the real world. In Intelligent Robots and Systems (IROS), 2017 IEEE/RSJ International Conference on. IEEE, 2017.</p>            </div>
        </div>

    </div>
</body>
</html>
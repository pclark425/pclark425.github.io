<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-864 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-864</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-864</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-21.html">extraction-schema-21</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <p><strong>Paper ID:</strong> paper-0fc0e93100a25ea2b561d92d1d15dc2e1cc6fbff</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0fc0e93100a25ea2b561d92d1d15dc2e1cc6fbff" target="_blank">Neuro-Symbolic Reinforcement Learning with First-Order Logic</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a novel RL method for text-based games with a recent neuro-symbolic framework called Logical Neural Network, which can learn symbolic and interpretable rules in their differentiable network.</p>
                <p><strong>Paper Abstract:</strong> Deep reinforcement learning (RL) methods often require many trials before convergence, and no direct interpretability of trained policies is provided. In order to achieve fast convergence and interpretability for the policy in RL, we propose a novel RL method for text-based games with a recent neuro-symbolic framework called Logical Neural Network, which can learn symbolic and interpretable rules in their differentiable network. The method is first to extract first-order logical facts from text observation and external word meaning network (ConceptNet), then train a policy in the network with directly interpretable logical operators. Our experimental results show RL training with the proposed method converges significantly faster than other state-of-the-art neuro-symbolic methods in a TextWorld benchmark.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e864.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e864.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FOL-LNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>First-Order Logic Logical Neural Network agent (FOL-LNN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuro-symbolic RL agent that converts textual observations into first-order logic predicates (using a semantic parser and ConceptNet) and trains an interpretable AND-OR Logical Neural Network (LNN) with a DQN-style RL loop to produce action policies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>FOL-LNN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Pipeline agent: (1) semantic parser converts current text observation (and history) into propositional logic statements; (2) external ConceptNet (word-meaning network) is queried to obtain class/type labels for words (e.g., direction, money); (3) a FOL converter uses the propositional logic, ConceptNet classes, and observation history to produce first-order logical predicate facts (including visited/initial flags); (4) these FOL predicates are fed as inputs to an LNN structured as an AND layer (multiple conjunction gates) followed by a single OR gate; (5) the LNN's connection weights are trained via a DQN-style replay-driven RL update using observed rewards. The LNN supports interpretable continuous-valued logical nodes (thresholding with alpha) and rule extraction by inspecting high-weight connections.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Coin-Collector (TextWorld)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A TextWorld text-based navigation and item-collection benchmark where an agent receives a textual description of the current room (connections, objects) and must navigate connected rooms to find a coin; the environment is explicitly modeled as a POMDP where observations do not reveal complete global state. Difficulty varies by distractor (dead-end) rooms per step.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>ConceptNet (commonsense word-meaning network) to map words in parsed propositional logic to semantic classes/types; a semantic parser (text -> propositional logic) is also used (described as the FOL converter's front-end), and the agent uses its own observation history (visited room history) as an internal externality for FOL generation.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>ConceptNet provides semantic class/type labels (categorical labels such as 'direction' or 'money'); the semantic parser outputs propositional logic statements (textual/structured logical propositions) which the FOL converter transforms into structured first-order predicate facts (e.g., find(x)=True, visited(x)=False, initial(x)=True).</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Symbolic first-order logic predicate store derived from current observation and observation history: the agent represents belief as a set of FOL predicates (e.g., find(x), visited(x), initial(x), all-are-visited) where some predicates (visited/initial) are maintained/derived from a visited-room history. These symbolic predicates constitute the agent's effective belief state input to the LNN.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>At each timestep the agent: (1) obtains o_t, (2) parses o_t (and recent history o_{t-1},...) into propositional logic items, (3) queries ConceptNet to determine the semantic class/type for each relevant word, (4) converts propositional items plus class labels and stored visited-room history into first-order logic predicates (including updating visited(x) and initial(x) flags from history), (5) these predicates replace/augment the agent's current symbolic belief inputs to the LNN. The LNN's weights are then updated with samples from the replay buffer using observed rewards; predicate truth is represented as continuous values thresholded by alpha to yield interpretable True/False regions.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy via neuro-symbolic reinforcement learning (DQN-style training on an LNN encoder). The LNN embodies learned logical rules (AND-OR structure) mapping belief predicates to action-value estimates; planning is implicit in the learned mapping rather than explicit search-based planning.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Rule-driven action selection from the LNN: the LNN learns conjunction/disjunction rules (e.g., 'if find(direction) AND NOT visited(direction) AND NOT initial(direction) -> go direction' or 'if find(direction) AND all_are_visited AND initial(direction) -> go direction') that produce 'go x' actions; navigation is achieved by these learned logical rules and episodic discovery incentives rather than explicit graph search/A* shortest-path computation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>On 50 unseen test games (moving-average results reported): FOL-LNN (using ConceptNet + semantic parsing + FOL conversion) -- Easy: epoch 200 reward 0.98, steps 17.1; Medium: epoch 200 reward 0.97, steps 30.7; Hard: epoch 200 reward 0.98, steps 43.5. (Values are average reward / average number of steps to goal from Table 1.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td>Baseline neuro-only LSTM-DQN++ (textual input, no explicit ConceptNet-driven FOL conversion) -- epoch 200: Easy reward 0.10, steps 90.9; Medium reward 0.00, steps 99.6; Hard reward 0.00, steps 99.9. (These baseline numbers are reported in Table 1 for comparison.)</td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using an external commonsense resource (ConceptNet) plus observation history to convert text into typed first-order predicates substantially improves learning speed, generalization, and interpretability: the FOL-LNN agent converges much faster and solves unseen games where neuro-only baselines fail; the agent maintains belief symbolically (visited/initial flags) and incorporates ConceptNet class labels directly into the FOL representation used by the LNN; navigation behavior emerges as concise logical rules (visit unvisited directions, return to initial from dead-ends) rather than explicit path search algorithms. The paper does not present an explicit ablation that removes ConceptNet alone, but comparison to neuro-only and other neuro-symbolic baselines demonstrates the benefit of structured logical inputs and LNN rule learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neuro-Symbolic Reinforcement Learning with First-Order Logic', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning dynamic belief graphs to generalize on text-based games <em>(Rating: 2)</em></li>
                <li>Playing text-adventure games with graph-based deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Text-based rl agents with commonsense knowledge: New challenges, environments and baselines <em>(Rating: 1)</em></li>
                <li>Neural logic machines <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-864",
    "paper_id": "paper-0fc0e93100a25ea2b561d92d1d15dc2e1cc6fbff",
    "extraction_schema_id": "extraction-schema-21",
    "extracted_data": [
        {
            "name_short": "FOL-LNN",
            "name_full": "First-Order Logic Logical Neural Network agent (FOL-LNN)",
            "brief_description": "A neuro-symbolic RL agent that converts textual observations into first-order logic predicates (using a semantic parser and ConceptNet) and trains an interpretable AND-OR Logical Neural Network (LNN) with a DQN-style RL loop to produce action policies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "FOL-LNN",
            "agent_description": "Pipeline agent: (1) semantic parser converts current text observation (and history) into propositional logic statements; (2) external ConceptNet (word-meaning network) is queried to obtain class/type labels for words (e.g., direction, money); (3) a FOL converter uses the propositional logic, ConceptNet classes, and observation history to produce first-order logical predicate facts (including visited/initial flags); (4) these FOL predicates are fed as inputs to an LNN structured as an AND layer (multiple conjunction gates) followed by a single OR gate; (5) the LNN's connection weights are trained via a DQN-style replay-driven RL update using observed rewards. The LNN supports interpretable continuous-valued logical nodes (thresholding with alpha) and rule extraction by inspecting high-weight connections.",
            "environment_name": "Coin-Collector (TextWorld)",
            "environment_description": "A TextWorld text-based navigation and item-collection benchmark where an agent receives a textual description of the current room (connections, objects) and must navigate connected rooms to find a coin; the environment is explicitly modeled as a POMDP where observations do not reveal complete global state. Difficulty varies by distractor (dead-end) rooms per step.",
            "is_partially_observable": true,
            "external_tools_used": "ConceptNet (commonsense word-meaning network) to map words in parsed propositional logic to semantic classes/types; a semantic parser (text -&gt; propositional logic) is also used (described as the FOL converter's front-end), and the agent uses its own observation history (visited room history) as an internal externality for FOL generation.",
            "tool_output_types": "ConceptNet provides semantic class/type labels (categorical labels such as 'direction' or 'money'); the semantic parser outputs propositional logic statements (textual/structured logical propositions) which the FOL converter transforms into structured first-order predicate facts (e.g., find(x)=True, visited(x)=False, initial(x)=True).",
            "belief_state_mechanism": "Symbolic first-order logic predicate store derived from current observation and observation history: the agent represents belief as a set of FOL predicates (e.g., find(x), visited(x), initial(x), all-are-visited) where some predicates (visited/initial) are maintained/derived from a visited-room history. These symbolic predicates constitute the agent's effective belief state input to the LNN.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "At each timestep the agent: (1) obtains o_t, (2) parses o_t (and recent history o_{t-1},...) into propositional logic items, (3) queries ConceptNet to determine the semantic class/type for each relevant word, (4) converts propositional items plus class labels and stored visited-room history into first-order logic predicates (including updating visited(x) and initial(x) flags from history), (5) these predicates replace/augment the agent's current symbolic belief inputs to the LNN. The LNN's weights are then updated with samples from the replay buffer using observed rewards; predicate truth is represented as continuous values thresholded by alpha to yield interpretable True/False regions.",
            "planning_approach": "Learned policy via neuro-symbolic reinforcement learning (DQN-style training on an LNN encoder). The LNN embodies learned logical rules (AND-OR structure) mapping belief predicates to action-value estimates; planning is implicit in the learned mapping rather than explicit search-based planning.",
            "uses_shortest_path_planning": false,
            "navigation_method": "Rule-driven action selection from the LNN: the LNN learns conjunction/disjunction rules (e.g., 'if find(direction) AND NOT visited(direction) AND NOT initial(direction) -&gt; go direction' or 'if find(direction) AND all_are_visited AND initial(direction) -&gt; go direction') that produce 'go x' actions; navigation is achieved by these learned logical rules and episodic discovery incentives rather than explicit graph search/A* shortest-path computation.",
            "performance_with_tools": "On 50 unseen test games (moving-average results reported): FOL-LNN (using ConceptNet + semantic parsing + FOL conversion) -- Easy: epoch 200 reward 0.98, steps 17.1; Medium: epoch 200 reward 0.97, steps 30.7; Hard: epoch 200 reward 0.98, steps 43.5. (Values are average reward / average number of steps to goal from Table 1.)",
            "performance_without_tools": "Baseline neuro-only LSTM-DQN++ (textual input, no explicit ConceptNet-driven FOL conversion) -- epoch 200: Easy reward 0.10, steps 90.9; Medium reward 0.00, steps 99.6; Hard reward 0.00, steps 99.9. (These baseline numbers are reported in Table 1 for comparison.)",
            "has_tool_ablation": false,
            "key_findings": "Using an external commonsense resource (ConceptNet) plus observation history to convert text into typed first-order predicates substantially improves learning speed, generalization, and interpretability: the FOL-LNN agent converges much faster and solves unseen games where neuro-only baselines fail; the agent maintains belief symbolically (visited/initial flags) and incorporates ConceptNet class labels directly into the FOL representation used by the LNN; navigation behavior emerges as concise logical rules (visit unvisited directions, return to initial from dead-ends) rather than explicit path search algorithms. The paper does not present an explicit ablation that removes ConceptNet alone, but comparison to neuro-only and other neuro-symbolic baselines demonstrates the benefit of structured logical inputs and LNN rule learning.",
            "uuid": "e864.0",
            "source_info": {
                "paper_title": "Neuro-Symbolic Reinforcement Learning with First-Order Logic",
                "publication_date_yy_mm": "2021-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning dynamic belief graphs to generalize on text-based games",
            "rating": 2
        },
        {
            "paper_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Text-based rl agents with commonsense knowledge: New challenges, environments and baselines",
            "rating": 1
        },
        {
            "paper_title": "Neural logic machines",
            "rating": 1
        }
    ],
    "cost": 0.0085475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Neuro-Symbolic Reinforcement Learning with First-Order Logic</h1>
<p>Daiki Kimura Masaki Ono Subhajit Chaudhury Ryosuke Kohita Akifumi Wachi Don Joven Agravante Michiaki Tatsubori Asim Munawar Alexander Gray<br>IBM Research<br>{daiki, moono, subhajit}@jp.ibm.com, {kohi, akifumi.wachi}@ibm.com<br>don.joven.r.agravante@ibm.com, mich@jp.ibm.com, {asim, alexander.gray}@ibm.com</p>
<h4>Abstract</h4>
<p>Deep reinforcement learning (RL) methods often require many trials before convergence, and no direct interpretability of trained policies is provided. In order to achieve fast convergence and interpretability for the policy in RL, we propose a novel RL method for text-based games with a recent neurosymbolic framework called Logical Neural Network, which can learn symbolic and interpretable rules in their differentiable network. The method is first to extract first-order logical facts from text observation and external word meaning network (ConceptNet), then train a policy in the network with directly interpretable logical operators. Our experimental results show RL training with the proposed method converges significantly faster than other state-of-the-art neuro-symbolic methods in a TextWorld benchmark.</p>
<h2>1 Introduction</h2>
<p>Deep reinforcement learning (RL) has been successfully applied to many applications, such as computer games, text-based games, and robot control applications (Mnih et al., 2015; Narasimhan et al., 2015; Kimura, 2018; Yuan et al., 2018; Kimura et al., 2018). However, these methods require many training trials for converging to the optimal action policy, and the trained action policy is not understandable for human operators. This is because, although the training results are sufficient, the policy is stored in a black-box deep neural network. These issues become critical problems when the human operator wants to solve a real-world problem and verify the trained rules. If the trained rules are understandable and modifiable, the human operator can control them and design an action restriction. While using a symbolic (logical) format as representation for stored rules is suitable for achieving interpretability and quick training, it is difficult to train the logical rules with a traditional training approach.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of the proposed method. The agent takes a text observation from the environment, and the first-order logical facts are extracted from an FOL converter that uses a semantic parser, ConceptNet, and history. The weights (shown by line thickness in this figure) of the network are updated by these extracted predicate logics. Solid lines show one trained rule; when the agent finds a direction $x$ and the direction $x$ has not been visited, the agent takes a "Go $x$ " action. Dashed lines show the initial connections before training.</p>
<p>In order to train logical rules, a recent neurosymbolic framework called the Logical Neural Network (LNN) (Riegel et al., 2020) has been proposed to simultaneously provide key properties of both the neural network (learning) and the symbolic logic (reasoning). The LNN can train the symbolic rules with logical functions in the neural networks by having an end-to-end differentiable network minimizes a contradiction loss. Every neuron in the LNN has a component for a formula of weighted real-valued logics from a unique logical conjunction, disjunction, or negation nodes, and</p>
<p>then it can calculate the probability and logical contradiction loss during the inference and training. At the same time, the trained LNN can extract obtained logical rules by selecting high weighted connections that represent the important rules for an action policy.</p>
<p>In this paper, we propose an action knowledge acquisition method featuring a neuro-symbolic LNN framework for the RL algorithm. Through experiments, we demonstrate the advantages of the proposed method for real-world problems which is not logically grounded games such as Blocks World. Since natural language observation is easier to convert into logical information than visual or audio, we tackle text-based interaction games for verifying the proposed method.</p>
<p>Figure 1 shows an overview of our method. The observation text is input to a semantic parser to extract the logical values of each propositional logic. In this case, the semantic parser finds there are two exits (north and south). The method then converts first-order logical (predicates) facts from the propositional logics and categories of each word, such as $\exists x \in{$ south, north $},\langle$ find $x\rangle=$ True and $\exists x \in{$ east, west $},\langle$ find $x\rangle=$ False. These extracted predicated logics are fed into LNN which has some conjunction gates and one disjunction gate. The LNN trains the weights for these connections by the reward value to obtain the action policy. The contributions of this paper are as follows.</p>
<ul>
<li>The paper describes design and implementation of a novel neuro-symbolic RL for a textbased interaction games.</li>
<li>The paper explains an algorithm to extract first-order logical facts from given textual observation by using the agent history and ConceptNet as an external knowledge.</li>
<li>We observed our proposed method has advantages for faster convergence and interpretability than state-of-the-art methods and baselines by ablation study on the text-based games.</li>
</ul>
<h2>2 Related work</h2>
<p>Various prior works have examined RL for textbased games. LSTM-DQN (Narasimhan et al., 2015) is an early study on an LSTM-based encoder for feature extraction from observation and Q-learning for action policy. LSTM-DQN++ (Yuan et al., 2018) extended the exploration and LSTMDRQN (Yuan et al., 2018) was proposed for adding
memory units in the action scorer. KG-DQN (Ammanabrolu and Riedl, 2019) and GATA (Adhikari et al., 2020) extended the language understanding. LeDeepChef (Adolphs and Hofmann, 2020) used recurrent feature extraction along with the A2C (Mnih et al., 2016). CREST (Chaudhury et al., 2020) was proposed for pruning observation information. TWC (Murugesan et al., 2021) was proposed for utilizing common sense reasoning. However, none of these studies used the neuro-symbolic approach.</p>
<p>For recent neuro-symbolic RL work, the Neural Logic Machine (NLM) (Dong et al., 2018) was proposed as a method for combination of deep neural network and symbolic logic reasoning. It uses a sequence of multi-layer perceptron layers to deduct symbolic logics. Rules are combined or separated during forward propagation, and an output of the entire architecture represents complicated rules. In this paper, we compare our method with this NLM.</p>
<h2>3 Proposed method</h2>
<h3>3.1 Problem formulation</h3>
<p>As text-based games are sequential decisionmaking problems, they can naturally be applied to RL. These games are partially observable Markov decision processes (POMDP) (Kaelbling et al., 1998), where the observation text does not include the entire information of the environment. Formally, the game is a discrete-time POMDP defined by $\langle S, A, T, R, \omega, O, \gamma\rangle$, where $S$ is a set of states $\left(s_{t} \in S\right), A$ is a set of actions, $T$ is a set of transition probabilities, $R$ is a reward function, $\omega$ is a set of observations $\left(o_{t} \in \omega\right), O$ is a set of conditional observation probabilities, and $\gamma$ is a discount factor. Although the state $s_{t}$ contains the complete internal information, the observation $o_{t}$ does not. In this paper, we follow following two assumptions: one, the word in each command is taken from a fixed vocabulary $V$, and two, each action command consists of two words (verb and object). The objective for the agent is to maximize the expected discounted reward $E\left[\sum_{t} \gamma^{t} r_{t}\right]$.</p>
<h3>3.2 Method</h3>
<p>The proposed method consists of two processes: converting text into first-order logic (FOL), and training the action policy in LNN.</p>
<h3>3.2.1 FOL converter</h3>
<p>The FOL converter converts a given natural observation text $o_{t}$ and observation history $\left(o_{t-1}, o_{t-2}, \ldots\right)$ into first-order logic facts. The method first converts text into propositional logics $l_{i, t}$ by a semantic parser from $o_{t}$, such as, the agent understands an opened direction from the current room. The agent then retrieves the class type $c$ of the word meaning in propositional logic $l_{i, t}$ by using ConceptNet (Liu and Singh, 2004) or the network of another word's definition. For example, "east" and "west" are classified as a direction-type, and "coin" is as a money-type. The class is used for selecting the appropriate LNN for FOL training and inference.</p>
<h3>3.2.2 LNN training</h3>
<p>The LNN training component is for obtaining an action policy from the given FOL logics. LNN (Riegel et al., 2020) has logical conjunction (AND), logical disjunction (OR), and negation (NOT) nodes directly in its neural network. In our method, we prepare an AND-OR network for training arbitrary rules from given inputs. As shown in Fig. 1, we prepare all logical facts at the first layer, several AND gates (as many as the network is required) at the second layer, and one OR gate connected to all previous AND gates. During the training, the reward value is used for adding a new AND gate, and for updating the weight value for each connection. More specifically, the method is storing the replay buffer which has current observation $o_{t}$, action $a_{t}$, reward $r_{t}$, and next observation $o_{t+1}$ value. For each training step, the method selects some replies, and it extracts firstorder logical facts from current observation $o_{t}$ and action $a_{t}$. The LNN trains by this fact inputs and reward; that means it forwards from input facts through LNN, calculates a loss values from the reward value, and optimizes weights in LNN. The whole training mechanism is similar to DQN (Mnih et al., 2013), the difference from these is the network. To aid the interpretability of node values, we define a threshold $\alpha \in\left[\frac{1}{2}, 1\right]$ such that a continuous value is considered True if the value is in $[\alpha, 1]$, and False if it is in $[0,1-\alpha]$.</p>
<p>Algorithm 1 describes the whole algorithm for the proposed method.</p>
<p>Algorithm 1 RL by FOL-LNN
procedure REINFORCEMENT LEARNING
for $t=1,2,3, \ldots$ do
$o_{t} \leftarrow$ Observe observation
$l_{t, i} \leftarrow$ Extract logic from $o_{t}, o_{t-1}, \ldots$
for $i=1,2,3, \ldots$ do
$c \leftarrow$ Find class from ConceptNet
$\theta^{c} \leftarrow$ Select $L N N$
$l_{t, i}^{c} \leftarrow$ Convert into FOL logic
$a_{t, i} \leftarrow \theta^{c}\left(l_{t, i}^{c}\right)$
end for
$a_{t} \leftarrow \arg \max a_{t, i}$
$r_{t}, o_{t+1} \leftarrow$ Get reward and next obs
Store reply $\left(o_{t}, a_{t}, r_{t}, o_{t+1}\right)$
$\nabla \theta \leftarrow$ Update LNN from reply
end for
end procedure</p>
<h2>4 Experiments</h2>
<p>We evaluated the proposed method on a coincollector game in TextWorld (Côté et al., 2018) with three different difficulties (easy, medium, and hard). The objective of the game is to find and collect a coin which is placed in a room within connected rooms. Since we tackle a real-world game problem rather than a symbolic games, we need to extract logical facts from given natural texts for neuro-symbolic methods. We prepare the following propositional logics as extracting logical facts: which object is found in the observation, which direction has already been visited, and which direction the agent comes from initially. These logical values are easily calculated from visited room history and word definitions. In this experiment, we prepared 26 logical values ${ }^{1}$, and all the following neuro-symbolic methods used these value as input. For the evaluation metric, we focused on (1) the test reward value on the unseen (test) games and (2) the number of steps to achieve the goal on unseen games. Since we focus on the performance of generalization, we only use 50 small-size (level $=5$ ) games for training, 50 unseen games from 5 different size (level $=5,10,15,20,25$ ) games for test ${ }^{2}$, and mini-batch in training (batch size $=4$ ). The other parameters for the game and agent follow LSTM-DQN++ (Narasimhan et al., 2015).</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Average reward and number of steps (reward: higher is better / number of steps: lower is better) for each epoch on 50 unseen games with three difficulty levels. These results are from moving average $(N=100)$ and 5 random seeds. Training is done on only small-size games. Although neuro-only method cannot solve unseen test games, our proposed method (FOL-LNN) can solve and converge extremely faster than other SOTAs and baselines.</p>
<table>
<thead>
<tr>
<th style="text-align: right;"></th>
<th style="text-align: right;">Easy game</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;">Medium game</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;">Hard game</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: right;">Epoch</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">200</td>
<td style="text-align: right;">1000</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">200</td>
<td style="text-align: right;">2000</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">200</td>
<td style="text-align: right;">2000</td>
</tr>
<tr>
<td style="text-align: right;">LSTM-DQN++ *</td>
<td style="text-align: right;">0.07 / 93.4</td>
<td style="text-align: right;">0.10 / 90.9</td>
<td style="text-align: right;">0.12 / 88.6</td>
<td style="text-align: right;">0.00 / 99.9</td>
<td style="text-align: right;">0.00 / 99.6</td>
<td style="text-align: right;">0.03 / 97.3</td>
<td style="text-align: right;">0.00 / 99.9</td>
<td style="text-align: right;">0.00 / 99.9</td>
<td style="text-align: right;">0.04 / 96.6</td>
</tr>
<tr>
<td style="text-align: right;">NLM-DQN **</td>
<td style="text-align: right;">0.87 / 26.4</td>
<td style="text-align: right;">0.93 / 20.8</td>
<td style="text-align: right;">1.00 / 15.0</td>
<td style="text-align: right;">0.27 / 81.1</td>
<td style="text-align: right;">0.48 / 65.9</td>
<td style="text-align: right;">1.00 / 29.7</td>
<td style="text-align: right;">0.01 / 99.7</td>
<td style="text-align: right;">0.10 / 94.8</td>
<td style="text-align: right;">0.66 / 64.0</td>
</tr>
<tr>
<td style="text-align: right;">NN-DQN</td>
<td style="text-align: right;">0.91 / 22.8</td>
<td style="text-align: right;">0.95 / 19.0</td>
<td style="text-align: right;">1.00 / 15.0</td>
<td style="text-align: right;">0.48 / 65.6</td>
<td style="text-align: right;">0.65 / 54.3</td>
<td style="text-align: right;">1.00 / 29.5</td>
<td style="text-align: right;">0.19 / 89.0</td>
<td style="text-align: right;">0.28 / 84.0</td>
<td style="text-align: right;">0.97 / 46.3</td>
</tr>
<tr>
<td style="text-align: right;">LNN-NN-DQN</td>
<td style="text-align: right;">0.88 / 24.8</td>
<td style="text-align: right;">0.94 / 20.2</td>
<td style="text-align: right;">1.00 / 15.0</td>
<td style="text-align: right;">0.49 / 65.8</td>
<td style="text-align: right;">0.61 / 57.0</td>
<td style="text-align: right;">1.00 / 29.6</td>
<td style="text-align: right;">0.24 / 86.9</td>
<td style="text-align: right;">0.27 / 84.9</td>
<td style="text-align: right;">0.97 / 47.4</td>
</tr>
<tr>
<td style="text-align: right;">FOL-LNN (Ours)</td>
<td style="text-align: right;">0.95 / 19.0</td>
<td style="text-align: right;">0.98 / 17.1</td>
<td style="text-align: right;">1.00 / 15.0</td>
<td style="text-align: right;">0.94 / 32.7</td>
<td style="text-align: right;">0.97 / 30.7</td>
<td style="text-align: right;">1.00 / 28.6</td>
<td style="text-align: right;">0.95 / 44.8</td>
<td style="text-align: right;">0.98 / 43.5</td>
<td style="text-align: right;">1.00 / 42.0</td>
</tr>
</tbody>
</table>
<ul>
<li>State-of-the-art neuro-only method with a simple DQN action scorer (Narasimhan et al., 2015)
** State-of-the-art neuro-symbolic method has same input as ours and other neuro-symbolic methods (Dong et al., 2018)</li>
</ul>
<p>We prepared five methods for an evaluation of the proposed method:</p>
<ul>
<li>LSTM-DQN++ (Narasimhan et al., 2015): State-of-the-art neuro-only method with a simple DQN action scorer. We use this method as a baseline method for the neuro-only agent, and LSTM receives extracted embedding vector from natural text information.</li>
<li>NLM-DQN (Dong et al., 2018): State-of-the-art neuro-symbolic method. The input is propositional logical values that is also used in following baselines and proposed method. The original NLM uses the REINFORCE (Williams, 1992) algorithm, but in order to handle text-based games with the same setting as the other methods, we applied the DQN algorithm. In short, the method uses an NLM layer instead of an LSTM (Hochreiter and Schmidhuber, 1997) for the encoder of the LSTM-DQN++ method. We tuned the hyper-parameters from the same search space as the original paper.</li>
<li>NN-DQN: Naïve neuro-symbolic baseline method. The input of the network is propositional logical values, and it uses a multi layer perceptron as the encoder of the LSTMDQN++.</li>
<li>LNN-NN-DQN: Neuro-symbolic baseline method. The method first gets propositional logical values, it converts by LNN into some conjunction values for all combinations of given logical values, and then it inputs them
into a multi layer perceptron. It differs from NN-DQN in that LNN-NN-DQN has prepared conjunction nodes, which should lead to faster training in beginning of the training, and better interpretabiliity after the training.</li>
<li>FOL-LNN: Our neuro-symbolic method.</li>
</ul>
<p>Table 1 shows the test reward and test step values on unseen games, and Fig. 2 shows curves. First, all the RL results with logical input were better than those with textual input. Second, our proposed method could converge much faster than the other neuro-symbolic state-of-the-art and baseline methods. Third, only our method could extract the trained rules by checking the weight value of the LNN. We attached the extracted rules from the medium level games here:</p>
<p>$$
\begin{aligned}
&amp; \exists x \in W_{\text {money }} \
&amp; \quad(\text { find } x) \rightarrow \llbracket \text { take } x \rrbracket \
&amp; \exists x \in W_{\text {direction }} \
&amp; \quad((\text { find } x) \wedge \neg(\text { visited } x) \wedge \neg(\text { initial } x)) \vee \
&amp; \quad((\text { find } x) \wedge(\text { all are visited }) \wedge(\text { initial } x)) \rightarrow \llbracket \text { go } x \rrbracket
\end{aligned}
$$</p>
<p>where $W_{\text {direction }}$ is a set of words in a type of "direction" in ConceptNet. The rule for "take"-action is for taking a coin. The first conjunction rule for "go"-action is for visiting an un-visited room, and the second rule is for returning to the initial room from a dead-end. With our proposed method, we can see that these trained rules will be helpful for operating the neural agent in real use cases.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Curves for reward and number of steps for 50 unseen games. Moving average is applied.</p>
<h2>5 Conclusion</h2>
<p>In this paper, we proposed a novel neuro-symbolic method for RL on text-based games. According to the evaluation on the natural language text-based game with several difficulties, our method can converge extremely faster than other state-of-theart neuro-only and neuro-symbolic methods, and extract trained logical rules for improving interpretability of the model.</p>
<h2>Discussion about ethics</h2>
<p>Our model is not using any sensitive contexts such as legal or health-care settings. The data set used in our experiment does not contain any sensitive information. Since our proposed neuro-symbolic RL method can extract the trained rules for interpretability of the model, the method can analyze a reason behind taken action. We are sure that if the model returns biased results, this functionality is helpful for clearing the reason for these data bias issues.</p>
<h2>References</h2>
<p>Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre Côté, Mikuláš Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, and William L. Hamilton. 2020. Learning dynamic belief graphs to generalize on text-based games. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Leonard Adolphs and Thomas Hofmann. 2020. Ledeepchef deep reinforcement learning agent for families of text-based games. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 7342-7349.</p>
<p>Prithviraj Ammanabrolu and Mark Riedl. 2019. Playing text-adventure games with graph-based deep reinforcement learning. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 3557-3565.</p>
<p>Subhajit Chaudhury, Daiki Kimura, Kartik Talamadupula, Michiaki Tatsubori, Asim Munawar, and Ryuki Tachibana. 2020. Bootstrapped q-learning with context relevant observation pruning to generalize in text-based games. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 3002-3008.</p>
<p>Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew J. Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, and Adam Trischler. 2018. Textworld: A learning environment for text-based games. In Computer Games - 7th Workshop, CGW 2018, Held in Conjunction with the 27th International Conference on Artificial Intelligence, IJCAI 2018, Stockholm, Sweden, July 13, 2018, Revised Selected Papers, pages 41-75.</p>
<p>Honghua Dong, Jiayuan Mao, Tian Lin, Chong Wang, Lihong Li, and Denny Zhou. 2018. Neural logic machines. In International Conference on Learning Representations.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735-1780.</p>
<p>Leslie Pack Kaelbling, Michael L. Littman, and Anthony R. Cassandra. 1998. Planning and acting in partially observable stochastic domains. Artificial Intelligence, 101(1):99-134.</p>
<p>Daiki Kimura. 2018. Daqn: Deep auto-encoder and q-network. arXiv preprint arXiv:1806.00630.</p>
<p>Daiki Kimura, Subhajit Chaudhury, Ryuki Tachibana, and Sakyasingha Dasgupta. 2018. Internal model from observations for reward shaping.</p>
<p>Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.
H. Liu and P. Singh. 2004. Conceptnet - a practical commonsense reasoning tool-kit. BT Technology Journal, 22(4):211-226.</p>
<p>Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pages 1928-1937. PMLR.</p>
<p>Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. 2013. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602.</p>
<p>Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. 2015. Human-level control through deep reinforcement learning. Nature, 518:529-533.</p>
<p>Keerthiram Murugesan, Mattia Atzeni, Pavan Kapanipathi, Pushkar Shukla, Sadhana Kumaravel, Gerald Tesauro, Kartik Talamadupula, Mrinmaya Sachan, and Murray Campbell. 2021. Text-based rl agents with commonsense knowledge: New challenges, environments and baselines. In Thirty Fifth AAAI Conference on Artificial Intelligence.</p>
<p>Karthik Narasimhan, Tejas D. Kulkarni, and Regina Barzilay. 2015. Language understanding for textbased games using deep reinforcement learning. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, pages $1-11$.</p>
<p>Ryan Riegel, Alexander Gray, Francois Luus, Naweed Khan, Ndivhuwo Makondo, Ismail Yunus Akhalwaya, Haifeng Qian, Ronald Fagin, Francisco Barahona, Udit Sharma, Shajith Ikbal, Hima Karanam, Sumit Neelam, Ankita Likhyani, and Santosh Srivastava. 2020. Logical neural networks.</p>
<p>Ronald J Williams. 1992. Simple statistical gradientfollowing algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229-256.</p>
<p>Xingdi Yuan, Marc-Alexandre Côté, Alessandro Sordoni, Romain Laroche, Remi Tachet des Combes, Matthew J. Hausknecht, and Adam Trischler. 2018. Counting to explore and generalize in text-based games. CoRR, abs/1806.11525.</p>
<h2>Appendix A: Environment Setting</h2>
<p>In this section we describe Coin-Collector (Yuan et al., 2018), a text-based game used in our experiments. Then, we describe the hyper-parameter setting.</p>
<h2>Appendix A.1: Coin-Collector</h2>
<p>Coin-Collector is a kind of text-based games, and we have to move an agent through rooms to get a coin placed in a room. An agent receives an observation text that describes the structure of a current room from a game. The goal of Coin-Collector is to analyze textual data and understand the structure of given rooms for training an agent.</p>
<p>A game has hyper-parameters such as level and difficulty. A game level indicates the minimum number of steps to a room in which a coin is placed. Rooms are randomly connected and their structure depends on difficulty. An easy game has no distractor rooms (dead ends) along the path. On a medium game, each room along the optimal trajectory has one distractor room randomly connected to it. A hard game, each room has two distractor rooms which means each room has one for optimal trajectory, one for the previous room, and two for distractor rooms.</p>
<p>An agent can use two types of verbs ( ${$ take, go}) and five types of nouns ( ${$ coin, east, west, south, north}). Since an action consists of a verb and a noun, there are ten different actions that an agent can take. For the settings of LSTM-DQN++ (Narasimhan et al., 2015), the agent gets the positive reward when the agent goes in a new room. The agent also gets positive reward when the agent successfully returns the initial coming direction for medium setting. If an agent takes an invalid action such as "go coin", or "go north" at no north room, the agent does not receive a negative reward.</p>
<h2>Appendix A.2: Hyper-parameters</h2>
<p>For the all experiments, we used the same hyperparameters with the previous work for CoinCollector as follows.</p>
<ul>
<li>We used a prioritized replay memory with capacity of 500,000 and the priority fraction is 0.25 .</li>
<li>A mini-batch gradient update is performed every 4 steps in the game play.</li>
<li>The discount factor for Q-learning $\gamma$ is 0.9 .</li>
<li>We used an episodic discovery bonus that encourages an agent to discover unseen states and the coefficient $\beta$ is 1.0 .</li>
<li>We anneal the $\epsilon$ for the $\epsilon$-greedy strategy from 1 to 0.2 over 1000 epochs. After 1000 episodes, the $\epsilon$ is 0.2 .</li>
<li>We used the Adam algorithm (Kingma and Ba, 2014) for the optimization and the learning rate is $1 e^{-3}$.</li>
</ul>
<h2>Appendix B: Experiment details</h2>
<p>The training and validation times until 3,000 epochs for each method are as follows.</p>
<ul>
<li>LSTM-DQN++ (Narasimhan et al., 2015): Around 2 hours for easy difficulty, and around 4 hours for medium difficulty.</li>
<li>NLM-DQN (Dong et al., 2018): Around 40 minutes for easy difficulty, and around 2.5 hours for medium difficulty.</li>
<li>NN-DQN: Around 30 minutes for easy difficulty, and around 1.5 hours for medium difficulty.</li>
<li>LNN-NN-DQN: Around 30 minutes for easy difficulty, and around 1.5 hours for medium difficulty.</li>
<li>FOL-LNN: Around 35 minutes for easy difficulty, and around 2 hours for medium difficulty.</li>
</ul>
<p>These results are calculated on Intel Core i7-6700K CPU (4.00GHz) and NVIDIA Titan X. From these results, our proposed method is not computationally expensive than other methods.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1} 26=(5$ (object) +4 (visited) +4 (initial $) \times 2$ (negation)
${ }^{2}$ The agent needs to generalize the game level size&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
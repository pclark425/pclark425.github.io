<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-693 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-693</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-693</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-b4623f43a5320a0d070a29945bf2c0de2b5b2341</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b4623f43a5320a0d070a29945bf2c0de2b5b2341" target="_blank">Recovering single precision accuracy from Tensor Cores while surpassing the FP32 theoretical peak performance</a></p>
                <p><strong>Paper Venue:</strong> The international journal of high performance computing applications</p>
                <p><strong>Paper TL;DR:</strong> This work develops a high accuracy, high performance, and low power consumption matrix–matrix multiplication implementation using Tensor Cores, which exactly matches the accuracy of FP32 SIMT Cores while achieving superior throughput.</p>
                <p><strong>Paper Abstract:</strong> Tensor Core is a mixed-precision matrix–matrix multiplication unit on NVIDIA GPUs with a theoretical peak performance of more than 300 TFlop/s on Ampere architectures. Tensor Cores were developed in response to the high demand of dense matrix multiplication from machine learning. However, many applications in scientific computing such as preconditioners for iterative solvers and low-precision Fourier transforms can exploit these Tensor Cores. To compute a matrix multiplication on Tensor Cores, we need to convert input matrices to half-precision, which results in loss of accuracy. To avoid this, we can keep the mantissa loss in the conversion using additional half-precision variables and use them for correcting the accuracy of matrix–matrix multiplication. Even with this correction, the use of Tensor Cores yields higher throughput compared to FP32 SIMT Cores. Nevertheless, the correcting capability of this method alone is limited, and the resulting accuracy cannot match that of a matrix multiplication on FP32 SIMT Cores. We address this problem and develop a high accuracy, high performance, and low power consumption matrix–matrix multiplication implementation using Tensor Cores, which exactly matches the accuracy of FP32 SIMT Cores while achieving superior throughput. The implementation is based on NVIDIA’s CUTLASS. We found that the key to achieving this accuracy is how to deal with the rounding inside Tensor Cores and underflow probability during the correction computation. Our implementation achieves 51 TFlop/s for a limited exponent range using FP16 Tensor Cores and 33 TFlop/s for full exponent range of FP32 using TF32 Tensor Cores on NVIDIA A100 GPUs, which outperforms the theoretical FP32 SIMT Core peak performance of 19.5 TFlop/s.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e693.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e693.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TensorCore_RZ_vs_RN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rounding-mode mismatch inside NVIDIA Tensor Cores (Round-toward-Zero vs Round-to-Nearest)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The hardware/implementation of Tensor Cores performs round-toward-zero (RZ) in the internal accumulator, while many expectations and comparisons assume round-to-nearest (RN); this mismatch leads to accumulated error in mixed-precision matrix multiplication implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Tensor Core mixed-precision matrix-matrix multiplication (WMMA/PTX kernels)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Matrix multiply-accumulate implemented on NVIDIA Tensor Cores via WMMA/mma PTX instructions and CUTLASS-based kernels, converting FP32 inputs to lower-precision types (FP16/TF32) and accumulating in FP32.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section / expectations about arithmetic rounding</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>CUDA PTX/WMMA/mma implementation within CUTLASS (GPU kernel code)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>implementation vs specification mismatch: rounding-mode difference</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>The paper and other methods often assume RN (round-to-nearest) behavior for accumulation or rely on FP32-like rounding; in reality the accumulator inside Tensor Cores uses RZ (round-toward-zero) which truncates after each MMA accumulation, causing systematic bias and larger error growth across k-iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>numerical accumulation inside hardware accelerator (Tensor Core accumulator), i.e., kernel-level arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>controlled experiment replacing Tensor Core behavior with two simulated primitives (mma_rn vs mma_rz) and comparing resulting matrix-multiplication accuracy; comparison of errors against FP32 SIMT reference.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>RelativeResidual metric (Eq. 7: Frobenius-norm relative residual between FP64 reference and target) and empirical error curves across matrix sizes; visual comparisons in figures; direct per-k iterative error growth observed.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Causes increased accumulated error in Markidis-style error-corrected GEMM: implementations using RZ inside Tensor Cores matched the higher error of uncorrected Tensor Cores as matrix size grows, while replacing with RN (or accumulating outside the Tensor Core) restores FP32-equivalent accuracy. Quantitatively, avoiding RZ made error match cuBLAS SGEMM (FP32 SIMT) in their plots (i.e., eliminated the excess error attributable to RZ).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Deterministic for accumulation performed inside Tensor Cores: RZ rounding occurs every accumulator update (every k iteration) and therefore affects all kernels that rely on in-TC accumulation; effect increases with k and matrix size.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Hardware/internal-implementation rounding mode (RZ) differs from the RN assumption in algorithm descriptions and FP32 SIMT behavior; implicit assumption in algorithm descriptions that Tensor Core accumulation behaves like FP32 RN.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Accumulate the primary A_F16 * B_F16 product outside the Tensor Core (i.e., send zero C into Tensor Core and perform the A*B accumulation on FP32 SIMT cores with RN), or use software primitives that enforce RN during accumulation (mma_rn simulation), thus avoiding the per-iteration RZ truncation inside Tensor Cores.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Highly effective: when the A*B term is accumulated outside Tensor Cores (or RN is used), the corrected mixed-precision GEMM attains the same error as FP32 SIMT SGEMM (exact match in their experiments), while retaining the throughput advantages of Tensor Cores for the other correction terms. The paper demonstrates this equality of error in Figure 1 and in comparisons using mma_rn.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>scientific computing / high-performance computing (HPC)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Recovering single precision accuracy from Tensor Cores while surpassing the FP32 theoretical peak performance', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e693.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e693.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Feng_description_mantissa_error</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Incorrect mantissa-bit analysis in Feng et al.'s natural language description</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper identifies that Feng et al.'s description of mantissa-bit counts and the bit position used for rounding decisions omits the implicit leading bit of IEEE-754 floats, leading to incorrect rounding logic when implemented.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Algorithm description in prior paper (Feng et al.) for 'Round-Split' error correction on Tensor Cores</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A research-methods description of how to split/truncate/round FP32 values into FP16 parts and decide rounding using a particular mantissa bit index; this textual specification was intended to guide implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper method description / algorithm specification</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>reimplementation attempt (C++/CUDA conversion of the described rounding/truncate procedure)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>ambiguous/incorrect specification in natural-language leading to wrong bit-indexing in implementation</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Feng et al. state that FP16 has 10 mantissa bits and therefore two FP16 numbers have 20 mantissa bits; they omit the implicit '1' hidden bit of IEEE-754, so their claimed counting and the particular mantissa bit index (21st vs correct 22nd with implicit bit) for rounding decisions is off by one. This causes the intended round-split method to be incorrectly specified and leads to divergence between the described method and an accurate implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>algorithmic rounding/truncation specification (preprocessing before Tensor Core GEMM)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>analytic review of floating-point bit representation (accounting for implicit bit), followed by implementation attempt of Feng's described method and failure to reproduce reported accuracy; comparison of expected mantissa bit positions against IEEE-754 semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Reproduction attempt: implemented Feng's method faithfully and compared resulting matrix-multiplication error to the paper's claims and to baseline SGEMM; observed inability to achieve claimed accuracy. Quantitative error comparisons shown in the paper's experiments (reported mismatch vs Feng's claimed results).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Leads to mis-implementation of the rounding/round-split strategy and inability to reproduce Feng et al.'s reported accuracy; authors report that Feng's method as described did not reach SGEMM-level accuracy when they implemented it.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Specific to Feng et al.'s description; illustrates a general risk that incorrect accounting of implicit bits in NL descriptions can lead to incorrect code in implementations (prevalence not quantified).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Incorrect or imprecise natural-language description that omitted IEEE-754 implicit bit semantics, resulting in wrong bit indexing for rounding/round-split decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Careful verification of floating-point format semantics (including implicit leading bit) when converting algorithmic descriptions to code; validate bit-level manipulations against the IEEE-754 specification; reproduce numerics on test vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective in principle: the paper's authors re-derived correct mantissa expectations and implemented corrected methods; after correction and additional fixes (e.g., addressing RZ and underflow), they achieve FP32-equivalent accuracy. The paper reports they could not reproduce Feng's reported accuracy until these corrections were applied.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>numerical analysis / HPC / floating-point algorithm implementation</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Recovering single precision accuracy from Tensor Cores while surpassing the FP32 theoretical peak performance', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e693.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e693.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WMMA_vs_mma_mapping_gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>API/PTX mapping differences: WMMA (higher-level) vs mma (lower-level) fragment mapping and register usage</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The WMMA API provides convenient load/store operations and duplicates elements across fragment lanes; the lower-level mma PTX instruction requires manual mapping and has different register/fragment layouts, meaning code based on WMMA assumptions may misbehave or be suboptimal when translated to mma.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CUDA WMMA API vs PTX mma instructions within CUTLASS/hand-written kernels</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Two programming interfaces for using Tensor Cores: WMMA (higher-level helpers for fragments and load/store) and direct PTX mma instruction (lower-level, efficient register usage but requires manual mapping).</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>API documentation / programming model description</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>CUDA/C++ kernels, PTX-level implementations inside CUTLASS or hand-written kernels</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification / implicit assumptions in API-level description vs lower-level implementation details</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>High-level WMMA semantics (e.g., how fragments are stored and duplicated across threads) differ from the behavior of lower-level mma instructions; translating algorithms or optimizations described at WMMA level to mma requires explicit mapping and care for register usage. If one assumes WMMA-like load/store semantics when using mma, the implementation will be incorrect or inefficient.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>kernel implementation / fragment load-store / memory-to-register mapping and register allocation</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Code analysis and performance/usage experiments: choice to use mma (for better register usage) required consulting PTX docs and manual mapping; observation of duplication in wmma fragments versus single-element mapping in mma revealed the difference.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Resource usage comparison (register pressure) and functional correctness: the authors refer to PTX documentation and implement manual mapping for mma; performance measured in FLOP/s and memory footprint differences (reported through CUTLASS tuning results).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>If unaddressed, incorrect mapping leads to wrong element-to-thread assignments, wrong results, or inflated memory/register usage and lower performance. By explicitly handling mapping, the authors achieve better register efficiency and are able to implement their error-corrected GEMM within CUTLASS.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Common for developers moving between WMMA API and PTX mma usage; prevalence depends on whether implementations use high-level WMMA or optimize down to PTX mma.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Mismatch between high-level API documentation/abstractions and low-level PTX semantics; implicit assumptions in natural-language API docs about fragment layouts and duplication.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Consult PTX ISA documentation for mma mapping, implement explicit memory-to-fragment mapping when using mma, and adapt CUTLASS modifications accordingly. The paper's authors used PTX documentation and manual mapping to ensure correctness and efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective: implementing the correct mapping allowed reduced register duplication and enabled the authors' modified CUTLASS kernels to run correctly and efficiently; performance results (throughput comparisons) validate the approach.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>GPU programming / HPC / numerical linear algebra</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Recovering single precision accuracy from Tensor Cores while surpassing the FP32 theoretical peak performance', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e693.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e693.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Exponent_range_underflow_gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mismatch between algorithm exposition and representable exponent range in FP16-based corrections (underflow and gradual underflow)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Theoretical descriptions that rely on FP16-captured residuals can miss the high probability of underflow/gradual underflow for certain exponent ranges; the paper derives and measures these probabilities and shows they cause accuracy loss or algorithm failure unless mitigated.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Half-half error-correction pipeline and TF32 alternative within CUTLASS</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Error-corrected single-precision GEMM implementations that split FP32 inputs into FP16 main part plus FP16 residuals (Delta) and compute several low-precision GEMMs to reconstruct FP32 results.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>algorithm specification / assumptions about representable ranges in method descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>CUDA kernel implementations (CUTLASS variants: cutlass_halfhalf and cutlass_tf32tf32)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification: representable exponent range and underflow behavior omitted or underestimated</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Descriptions that assume the FP16 residuals will represent the needed correction ignore that subtracting two nearby FP32 values and converting to FP16 can underflow or produce gradual underflow when exponents are close or small; this leads to lost correction terms and degraded accuracy or outright inability to represent values when inputs have small exponents.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>preprocessing (Delta computation), numerical range handling, and conversion to FP16/TF32</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Analytic probability derivation for underflow and gradual underflow as functions of FP32 exponent (P_u and P_u+gu), and empirical aggregation of underflow occurrences on GPU; experiments with synthetic exponent-range input matrices (exp_rand with varied ranges) revealing accuracy loss patterns (Types 1–4).</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Analytical formulas for P_u(e_v) and P_{u+gu}(e_v) (Eqs. 15 and 17) compared with experimental counts (Figure 8); accuracy measured by RelativeResidual (Eq. 7) across different exponent-range matrix ensembles (Type1–Type4), and classification of failure cases where halfhalf cannot represent inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>cutlass_halfhalf matches FP32 accuracy only when inputs' exponents lie within the FP16-capturable range (Type 1). For Type 2 and 3 exponent distributions, accuracy loss occurs; for Type 4 (very small exponents), cutlass_halfhalf cannot perform (representation failure). Using TF32 inputs (cutlass_tf32tf32) or scaling the residuals reduces underflow and restores FP32-level accuracy across ranges.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Depends on input exponent distributions; the paper demonstrates that for realistic/exotic exponent ranges (Type 2–4) the problem is prevalent and can cause failure. Not quantified as a global percentage; described qualitatively via types and experimental cases.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>FP16's limited exponent range and the arithmetic effect of subtracting nearly equal FP32 numbers followed by conversion to low precision, which leads to underflow/gradual underflow; omission of this condition in simpler method descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Two mitigations: (1) scale the Delta computation by multiplying by 2^{11} before converting to FP16 (reduces underflow probability) — called 'halfhalf' in the paper; (2) use TF32 as the input type for Tensor Cores (tf32tf32 method) which has FP32-like exponent range; additionally, apply pre-scaling when needed to bring values into representable ranges.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective: scaling (multiplying by 2^{11}) reduces underflow and is incorporated into 'halfhalf' to expand usable range; using TF32 (cutlass_tf32tf32) successfully represents nearly the full FP32 exponent range and achieves FP32-equivalent accuracy across tested exponent distributions. The paper shows cutlass_tf32tf32 matches FP32 accuracy for all tested types, while cutlass_halfhalf only matches when inputs are within the narrower exponent range.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>numerical linear algebra / HPC / floating-point arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Recovering single precision accuracy from Tensor Cores while surpassing the FP32 theoretical peak performance', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-693",
    "paper_id": "paper-b4623f43a5320a0d070a29945bf2c0de2b5b2341",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "TensorCore_RZ_vs_RN",
            "name_full": "Rounding-mode mismatch inside NVIDIA Tensor Cores (Round-toward-Zero vs Round-to-Nearest)",
            "brief_description": "The hardware/implementation of Tensor Cores performs round-toward-zero (RZ) in the internal accumulator, while many expectations and comparisons assume round-to-nearest (RN); this mismatch leads to accumulated error in mixed-precision matrix multiplication implementations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Tensor Core mixed-precision matrix-matrix multiplication (WMMA/PTX kernels)",
            "system_description": "Matrix multiply-accumulate implemented on NVIDIA Tensor Cores via WMMA/mma PTX instructions and CUTLASS-based kernels, converting FP32 inputs to lower-precision types (FP16/TF32) and accumulating in FP32.",
            "nl_description_type": "research paper methods section / expectations about arithmetic rounding",
            "code_implementation_type": "CUDA PTX/WMMA/mma implementation within CUTLASS (GPU kernel code)",
            "gap_type": "implementation vs specification mismatch: rounding-mode difference",
            "gap_description": "The paper and other methods often assume RN (round-to-nearest) behavior for accumulation or rely on FP32-like rounding; in reality the accumulator inside Tensor Cores uses RZ (round-toward-zero) which truncates after each MMA accumulation, causing systematic bias and larger error growth across k-iterations.",
            "gap_location": "numerical accumulation inside hardware accelerator (Tensor Core accumulator), i.e., kernel-level arithmetic",
            "detection_method": "controlled experiment replacing Tensor Core behavior with two simulated primitives (mma_rn vs mma_rz) and comparing resulting matrix-multiplication accuracy; comparison of errors against FP32 SIMT reference.",
            "measurement_method": "RelativeResidual metric (Eq. 7: Frobenius-norm relative residual between FP64 reference and target) and empirical error curves across matrix sizes; visual comparisons in figures; direct per-k iterative error growth observed.",
            "impact_on_results": "Causes increased accumulated error in Markidis-style error-corrected GEMM: implementations using RZ inside Tensor Cores matched the higher error of uncorrected Tensor Cores as matrix size grows, while replacing with RN (or accumulating outside the Tensor Core) restores FP32-equivalent accuracy. Quantitatively, avoiding RZ made error match cuBLAS SGEMM (FP32 SIMT) in their plots (i.e., eliminated the excess error attributable to RZ).",
            "frequency_or_prevalence": "Deterministic for accumulation performed inside Tensor Cores: RZ rounding occurs every accumulator update (every k iteration) and therefore affects all kernels that rely on in-TC accumulation; effect increases with k and matrix size.",
            "root_cause": "Hardware/internal-implementation rounding mode (RZ) differs from the RN assumption in algorithm descriptions and FP32 SIMT behavior; implicit assumption in algorithm descriptions that Tensor Core accumulation behaves like FP32 RN.",
            "mitigation_approach": "Accumulate the primary A_F16 * B_F16 product outside the Tensor Core (i.e., send zero C into Tensor Core and perform the A*B accumulation on FP32 SIMT cores with RN), or use software primitives that enforce RN during accumulation (mma_rn simulation), thus avoiding the per-iteration RZ truncation inside Tensor Cores.",
            "mitigation_effectiveness": "Highly effective: when the A*B term is accumulated outside Tensor Cores (or RN is used), the corrected mixed-precision GEMM attains the same error as FP32 SIMT SGEMM (exact match in their experiments), while retaining the throughput advantages of Tensor Cores for the other correction terms. The paper demonstrates this equality of error in Figure 1 and in comparisons using mma_rn.",
            "domain_or_field": "scientific computing / high-performance computing (HPC)",
            "reproducibility_impact": true,
            "uuid": "e693.0",
            "source_info": {
                "paper_title": "Recovering single precision accuracy from Tensor Cores while surpassing the FP32 theoretical peak performance",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Feng_description_mantissa_error",
            "name_full": "Incorrect mantissa-bit analysis in Feng et al.'s natural language description",
            "brief_description": "The paper identifies that Feng et al.'s description of mantissa-bit counts and the bit position used for rounding decisions omits the implicit leading bit of IEEE-754 floats, leading to incorrect rounding logic when implemented.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Algorithm description in prior paper (Feng et al.) for 'Round-Split' error correction on Tensor Cores",
            "system_description": "A research-methods description of how to split/truncate/round FP32 values into FP16 parts and decide rounding using a particular mantissa bit index; this textual specification was intended to guide implementation.",
            "nl_description_type": "research paper method description / algorithm specification",
            "code_implementation_type": "reimplementation attempt (C++/CUDA conversion of the described rounding/truncate procedure)",
            "gap_type": "ambiguous/incorrect specification in natural-language leading to wrong bit-indexing in implementation",
            "gap_description": "Feng et al. state that FP16 has 10 mantissa bits and therefore two FP16 numbers have 20 mantissa bits; they omit the implicit '1' hidden bit of IEEE-754, so their claimed counting and the particular mantissa bit index (21st vs correct 22nd with implicit bit) for rounding decisions is off by one. This causes the intended round-split method to be incorrectly specified and leads to divergence between the described method and an accurate implementation.",
            "gap_location": "algorithmic rounding/truncation specification (preprocessing before Tensor Core GEMM)",
            "detection_method": "analytic review of floating-point bit representation (accounting for implicit bit), followed by implementation attempt of Feng's described method and failure to reproduce reported accuracy; comparison of expected mantissa bit positions against IEEE-754 semantics.",
            "measurement_method": "Reproduction attempt: implemented Feng's method faithfully and compared resulting matrix-multiplication error to the paper's claims and to baseline SGEMM; observed inability to achieve claimed accuracy. Quantitative error comparisons shown in the paper's experiments (reported mismatch vs Feng's claimed results).",
            "impact_on_results": "Leads to mis-implementation of the rounding/round-split strategy and inability to reproduce Feng et al.'s reported accuracy; authors report that Feng's method as described did not reach SGEMM-level accuracy when they implemented it.",
            "frequency_or_prevalence": "Specific to Feng et al.'s description; illustrates a general risk that incorrect accounting of implicit bits in NL descriptions can lead to incorrect code in implementations (prevalence not quantified).",
            "root_cause": "Incorrect or imprecise natural-language description that omitted IEEE-754 implicit bit semantics, resulting in wrong bit indexing for rounding/round-split decisions.",
            "mitigation_approach": "Careful verification of floating-point format semantics (including implicit leading bit) when converting algorithmic descriptions to code; validate bit-level manipulations against the IEEE-754 specification; reproduce numerics on test vectors.",
            "mitigation_effectiveness": "Effective in principle: the paper's authors re-derived correct mantissa expectations and implemented corrected methods; after correction and additional fixes (e.g., addressing RZ and underflow), they achieve FP32-equivalent accuracy. The paper reports they could not reproduce Feng's reported accuracy until these corrections were applied.",
            "domain_or_field": "numerical analysis / HPC / floating-point algorithm implementation",
            "reproducibility_impact": true,
            "uuid": "e693.1",
            "source_info": {
                "paper_title": "Recovering single precision accuracy from Tensor Cores while surpassing the FP32 theoretical peak performance",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "WMMA_vs_mma_mapping_gap",
            "name_full": "API/PTX mapping differences: WMMA (higher-level) vs mma (lower-level) fragment mapping and register usage",
            "brief_description": "The WMMA API provides convenient load/store operations and duplicates elements across fragment lanes; the lower-level mma PTX instruction requires manual mapping and has different register/fragment layouts, meaning code based on WMMA assumptions may misbehave or be suboptimal when translated to mma.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "CUDA WMMA API vs PTX mma instructions within CUTLASS/hand-written kernels",
            "system_description": "Two programming interfaces for using Tensor Cores: WMMA (higher-level helpers for fragments and load/store) and direct PTX mma instruction (lower-level, efficient register usage but requires manual mapping).",
            "nl_description_type": "API documentation / programming model description",
            "code_implementation_type": "CUDA/C++ kernels, PTX-level implementations inside CUTLASS or hand-written kernels",
            "gap_type": "incomplete specification / implicit assumptions in API-level description vs lower-level implementation details",
            "gap_description": "High-level WMMA semantics (e.g., how fragments are stored and duplicated across threads) differ from the behavior of lower-level mma instructions; translating algorithms or optimizations described at WMMA level to mma requires explicit mapping and care for register usage. If one assumes WMMA-like load/store semantics when using mma, the implementation will be incorrect or inefficient.",
            "gap_location": "kernel implementation / fragment load-store / memory-to-register mapping and register allocation",
            "detection_method": "Code analysis and performance/usage experiments: choice to use mma (for better register usage) required consulting PTX docs and manual mapping; observation of duplication in wmma fragments versus single-element mapping in mma revealed the difference.",
            "measurement_method": "Resource usage comparison (register pressure) and functional correctness: the authors refer to PTX documentation and implement manual mapping for mma; performance measured in FLOP/s and memory footprint differences (reported through CUTLASS tuning results).",
            "impact_on_results": "If unaddressed, incorrect mapping leads to wrong element-to-thread assignments, wrong results, or inflated memory/register usage and lower performance. By explicitly handling mapping, the authors achieve better register efficiency and are able to implement their error-corrected GEMM within CUTLASS.",
            "frequency_or_prevalence": "Common for developers moving between WMMA API and PTX mma usage; prevalence depends on whether implementations use high-level WMMA or optimize down to PTX mma.",
            "root_cause": "Mismatch between high-level API documentation/abstractions and low-level PTX semantics; implicit assumptions in natural-language API docs about fragment layouts and duplication.",
            "mitigation_approach": "Consult PTX ISA documentation for mma mapping, implement explicit memory-to-fragment mapping when using mma, and adapt CUTLASS modifications accordingly. The paper's authors used PTX documentation and manual mapping to ensure correctness and efficiency.",
            "mitigation_effectiveness": "Effective: implementing the correct mapping allowed reduced register duplication and enabled the authors' modified CUTLASS kernels to run correctly and efficiently; performance results (throughput comparisons) validate the approach.",
            "domain_or_field": "GPU programming / HPC / numerical linear algebra",
            "reproducibility_impact": true,
            "uuid": "e693.2",
            "source_info": {
                "paper_title": "Recovering single precision accuracy from Tensor Cores while surpassing the FP32 theoretical peak performance",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Exponent_range_underflow_gap",
            "name_full": "Mismatch between algorithm exposition and representable exponent range in FP16-based corrections (underflow and gradual underflow)",
            "brief_description": "Theoretical descriptions that rely on FP16-captured residuals can miss the high probability of underflow/gradual underflow for certain exponent ranges; the paper derives and measures these probabilities and shows they cause accuracy loss or algorithm failure unless mitigated.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Half-half error-correction pipeline and TF32 alternative within CUTLASS",
            "system_description": "Error-corrected single-precision GEMM implementations that split FP32 inputs into FP16 main part plus FP16 residuals (Delta) and compute several low-precision GEMMs to reconstruct FP32 results.",
            "nl_description_type": "algorithm specification / assumptions about representable ranges in method descriptions",
            "code_implementation_type": "CUDA kernel implementations (CUTLASS variants: cutlass_halfhalf and cutlass_tf32tf32)",
            "gap_type": "incomplete specification: representable exponent range and underflow behavior omitted or underestimated",
            "gap_description": "Descriptions that assume the FP16 residuals will represent the needed correction ignore that subtracting two nearby FP32 values and converting to FP16 can underflow or produce gradual underflow when exponents are close or small; this leads to lost correction terms and degraded accuracy or outright inability to represent values when inputs have small exponents.",
            "gap_location": "preprocessing (Delta computation), numerical range handling, and conversion to FP16/TF32",
            "detection_method": "Analytic probability derivation for underflow and gradual underflow as functions of FP32 exponent (P_u and P_u+gu), and empirical aggregation of underflow occurrences on GPU; experiments with synthetic exponent-range input matrices (exp_rand with varied ranges) revealing accuracy loss patterns (Types 1–4).",
            "measurement_method": "Analytical formulas for P_u(e_v) and P_{u+gu}(e_v) (Eqs. 15 and 17) compared with experimental counts (Figure 8); accuracy measured by RelativeResidual (Eq. 7) across different exponent-range matrix ensembles (Type1–Type4), and classification of failure cases where halfhalf cannot represent inputs.",
            "impact_on_results": "cutlass_halfhalf matches FP32 accuracy only when inputs' exponents lie within the FP16-capturable range (Type 1). For Type 2 and 3 exponent distributions, accuracy loss occurs; for Type 4 (very small exponents), cutlass_halfhalf cannot perform (representation failure). Using TF32 inputs (cutlass_tf32tf32) or scaling the residuals reduces underflow and restores FP32-level accuracy across ranges.",
            "frequency_or_prevalence": "Depends on input exponent distributions; the paper demonstrates that for realistic/exotic exponent ranges (Type 2–4) the problem is prevalent and can cause failure. Not quantified as a global percentage; described qualitatively via types and experimental cases.",
            "root_cause": "FP16's limited exponent range and the arithmetic effect of subtracting nearly equal FP32 numbers followed by conversion to low precision, which leads to underflow/gradual underflow; omission of this condition in simpler method descriptions.",
            "mitigation_approach": "Two mitigations: (1) scale the Delta computation by multiplying by 2^{11} before converting to FP16 (reduces underflow probability) — called 'halfhalf' in the paper; (2) use TF32 as the input type for Tensor Cores (tf32tf32 method) which has FP32-like exponent range; additionally, apply pre-scaling when needed to bring values into representable ranges.",
            "mitigation_effectiveness": "Effective: scaling (multiplying by 2^{11}) reduces underflow and is incorporated into 'halfhalf' to expand usable range; using TF32 (cutlass_tf32tf32) successfully represents nearly the full FP32 exponent range and achieves FP32-equivalent accuracy across tested exponent distributions. The paper shows cutlass_tf32tf32 matches FP32 accuracy for all tested types, while cutlass_halfhalf only matches when inputs are within the narrower exponent range.",
            "domain_or_field": "numerical linear algebra / HPC / floating-point arithmetic",
            "reproducibility_impact": true,
            "uuid": "e693.3",
            "source_info": {
                "paper_title": "Recovering single precision accuracy from Tensor Cores while surpassing the FP32 theoretical peak performance",
                "publication_date_yy_mm": "2022-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [],
    "cost": 0.014690749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Recovering single precision accuracy from Tensor Cores while surpassing the FP32 theoretical peak performance</h1>
<p>Hiroyuki Ootomo ${ }^{1}$ and Rio Yokota ${ }^{2}$<br>${ }^{1}$ School of Computing, Tokyo Institute of Technology<br>ootomo.h@rio.gsic.titech.ac.jp<br>${ }^{2}$ Global Scientific Information and Computing Center, Tokyo Institute of Technology<br>rioyokota@gsic.titech.ac.jp</p>
<h4>Abstract</h4>
<p>Tensor Core is a mixed-precision matrix-matrix multiplication unit on NVIDIA GPUs with a theoretical peak performance of more than 300 TFlop/s on Ampere architectures. Tensor Cores were developed in response to the high demand of dense matrix multiplication from machine learning. However, many applications in scientific computing such as preconditioners for iterative solvers and low-precision Fourier transforms can exploit these Tensor Cores. To compute a matrix multiplication on Tensor Cores, we need to convert input matrices to half-precision, which results in loss of accuracy. To avoid this, we can keep the mantissa loss in the conversion using additional half-precision variables and use them for correcting the accuracy of matrix-matrix multiplication. Even with this correction, the use of Tensor Cores yields higher throughput compared to FP32 SIMT Cores. Nevertheless, the correcting capability of this method alone is limited, and the resulting accuracy cannot match that of a matrix multiplication on FP32 SIMT Cores. We address this problem and develop a high accuracy, high performance, and low power consumption matrix-matrix multiplication implementation using Tensor Cores, which exactly matches the accuracy of FP32 SIMT Cores while achieving superior throughput. The implementation is based on NVIDIA's CUTLASS. We found that the key to achieving this accuracy is how to deal with the rounding inside Tensor Cores and underflow probability during the correction computation. Our implementation achieves 51 TFlop/s for a limited exponent range using FP16 Tensor Cores and 33 TFlop/s for full exponent range of FP32 using TF32 Tensor Cores on NVIDIA A100 GPUs, which outperforms the theoretical FP32 SIMT Core peak performance of 19.5 TFlop/s.</p>
<h2>Introduction</h2>
<p>In order to meet the increasing demand of dense matrix-matrix multiplication from the machine learning community, processors with specialized computing units for matrix multiplication are being developed by numerous vendors. For instance, Google Tensor Processing Unit (TPU) [15], Intel Ponte Vecchio [13], IBM POWER10 [12], Preferred Networks MN-Core [24] and NVIDIA GPUs, all have special arithmetic units for low-precision matrix-matrix multiplication. The NVIDIA Tensor Core is a mixed-precision matrix-matrix multiplication unit on NVIDIA GPUs and its theoretical peak performance is more than 300 TFlop/s on the latest Ampere architecture [20]. Tensor Cores compute a matrix-matrix multiplication of two FP16 (IEEE 754 binary16) matrices in full-precision and accumulate in FP32 (IEEE 754 binary32). This results in higher accuracy in matrix-matrix multiplication compared to FP16 computing units. This capability to perform fast matrix multiplication can be used not only by machine learning applications, but also scientific computing applications and middleware that support both communities. Haidar [11] uses Tensor</p>
<p>Cores within a mixed-precision iterative refinement solver in order to exploit the speed of Tensor Cores while recovering the accuracy through refinement. This method can be applied to recover full FP64 (IEEE binary64) accuracy and is currently used in MAGMA ${ }^{1}$ and NVIDIA's cuSOLVER implementation. ${ }^{2}$ Tensor Cores can also be used for sparse matrix multiplication in graph analytics, breadth-first search, multigrid methods, etc [30]. Furthermore, Tensor Cores have also been used for reduction/scan operations in Monte Carlo methods, sort algorithms, etc $[3,5,9]$.</p>
<p>There have been several efforts to analyze the internal behavior of Tensor Cores. Jia et al. and Raihan et al. analyze how Tensor Core assembly instructions divide the input matrices, and the order they compute multiplications of the subdivided matrices [14, 25]. There have also been studies on how Tensor Cores support subnormal numbers and use RZ (Round toward Zero) [6]. Others have performed error analysis of Tensor Cores, where the theoretical error bound of mixed-precision block FMA computation is analyzed and compared to the actual error of Tensor Cores [1]. Studies on error correction have also been proposed. We have mentioned earlier that the conversion of input matrices to FP16 results in a loss of accuracy. To address this problem, Mukunoki et al. uses the Ozaki scheme [23] on Tensor Cores [18]. Using this method, it is possible to achieve single precision or even double precision accuracy on Tensor Cores. This method is able to perform matrix-matrix multiplication in FP64 faster than the cuBLAS DGEMM on GPUs with limited FP64 support such as the NVIDIA GeForce series. However, this method is slower when it comes to FP32, and is much slower than the cuBLAS SGEMM on any GPU. This method is also not competitive for FP64 matrix multiplication when compared to cuBLAS DGEMM on NVIDIA Tesla series GPUs.</p>
<p>For single-precision matrix-matrix multiplication, Markidis et al. propose a method to improve the accuracy of Tensor Core computation by using auxiliary FP16 variables to account for the truncated bits [17]. Markidis' method and its extensions are used for FFT [27], QR Factorization [22], and quantum-based molecular dynamics simulations [8]. However, the use of auxiliary FP16 variables alone is not sufficient to fully recover the FP32 accuracy. Feng et al. propose an improvement to Markidis' method, which uses a better rounding mode during the truncation to FP16 [7]. However, they are still not able to match the accuracy of SGEMM with their error correction method. We consider that there might be some technical errors in their paper. First, they do not take into account the implicit bit in IEEE 754 floating-point numbers. For example, they claim that FP16 has 10 mantissa bits so two FP16 numbers have a total of 20 mantissa bits. However, with the implicit bit the total is actually $(10+1) \times 2-1=21$ bits. This inaccurate description also causes some confusion during their description of the rounding they propose. Second, their method truncates a single-precision value $x$ to a half-precision $x_{h i}$ and stores the remaining value $x-x_{h i}$ to $x_{l o}$. They propose to decide the rounding of $x_{h i}$ by looking at the 21 st bit of mantissa of $x$, but if we consider the implicit bit, they should be looking at the 22nd bit. Furthermore, $x_{h i}$ will always store the first 10 bits when truncating, but $x_{l o}$ does not always store the next 10 bits. This means that always looking at the same bit to decide the rounding of $x_{h i}$ will not result in the round-split method they intend to perform. Therefore, their mantissa length analysis applied on Markidis' method (Truncate-Split) and their method (Round-Split; EGEMM-TC) might be incorrect. In the end, Feng et al. are not able to achieve an accuracy that exactly matches SGEMM [7].</p>
<p>Another important advantage of matrix-matrix multiplication unit is energy consumption. For instance, the top supercomputer listed (June 2021) in the Top500 -Fugaku-, requires 30MW of power to achieve 442 PFlop/s FP64 performance, and Exascale systems are predicted to consume even more power. If we look at the Green500 list, the top systems are equipped with matrix-matrix multiplication hardware such as MNCore, and NVIDIA A100. This reflects the energy efficiency of matrix-matrix multiplication hardware. The power consumption of GPUs have been analyzed at the Parallel Thread Execution (PTX) level, middleware level, and application level. Sakamoto et al. measured the effect of low-precision computing</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>on power consumption of the ICCG method in their earthquake simulation [26]. Guo et al. performed a CUDA PTX instruction level power analysis [10].</p>
<p>We envision that in the future a majority of applications will adopt mixed precision. In this scenario, there will be variables that will be computed in FP64, FP32, and FP16. For instance, the work by [4] uses three precisions during the iterative refinement and the LU is done in FP32, which calls an single-precision matrix-matrix multiplication. Furthermore, while the current quantum computer simulation using tensor network contraction uses single-precision matrix-matrix multiplication, it has been also investigated that which part of computing precision is sensitive to the result [16]. In recent year, the real machines of quantum computer have been developed and tried to be shown quantum supremacy, that they compute certain tasks that (classical) supercomputers are not be able to compute in realistic time. Moreover, since they have low power consumption [2], energy efficiency is becoming an important metric when evaluating quantum supremacy. For instance, qFlex is a quantum computer simulator based on tensor network contraction using single-precision complex matrix-matrix multiplication, where the power consumption of each component was reported during its simulation on Summit V100 GPUs [28]. Although they have considered to use FP16 and Tensor Cores in their simulation, they decided not to use it since FP16 has less exponent than FP32 and insufficient to use.</p>
<p>In this paper, we improve upon the existing error correction methods for matrix-multiplication on Tensor Cores by [17] and [7]. The two main causes of error in existing work are:</p>
<ol>
<li>The rounding inside Tensor Cores is done with round-to-zero by default, which causes a large error even when accumulating in FP32.</li>
<li>The high probability of underflow and gradual underflow in the error correction computation.</li>
</ol>
<p>We address these problems and evaluate against four other existing methods; Markidis' method, Feng's method, cuBLAS SGEMM using FP32 SIMT Core, and cuBLAS SGEMM over Tensor Cores. These results are shown in Figure 1. Although, we implemented the method described in Feng's work, we were not able to reproduce the accuracy shown in the paper [7]. We also reduce the computational complexity compared to Markidis's method and Feng's method. Furthermore, we provide our SGEMM implementation using NVIDIA CUTLASS. ${ }^{3}$ We evaluate the performance of our method in Figure 2. Our method surpasses the FP32 theoretical peak performance, while achieving the same error as FP32.</p>
<p>Our contributions can be summarized as follows:</p>
<ul>
<li>We theoretically calculate the expectation of the mantissa length and experimentally evaluate the effect of this mantissa length. As a result, we find that the mantissa loss is not the main cause of error during matrix-matrix multiplication in Markidis' method.</li>
<li>We evaluate the effect of rounding inside Tensor Cores, and find that this is one of the main causes of error. We reduce the effect of these rounding errors by accumulating outside of the Tensor Cores, and improve the matrix multiplication accuracy to exactly match that of CUDA FP32 SIMT Cores.</li>
<li>We also apply scaling to reduce the underflow probability. As a result, our method can deal with a wider range of values compared to Feng's and Markidis' method. Furthermore, we use the TF32 [20] data type, which is available on Ampere architectures, and confirm that it can represent nearly the entire FP32 exponent range.</li>
<li>We remove one of the three error-correction terms, and reduce the amount of computation on Tensor Cores to $75 \%$ without loss of any accuracy.</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Accuracy comparison of matrix multiplication $\mathbf{A} \times \mathbf{B}$ of our method, Feng's method[7], Markidis' method[17], cuBLAS SGEMM using FP32 SIMT Cores, and cuBLAS SGEMM using Tensor Cores. Input matrices $\mathbf{A} \in \mathrm{FP} 32^{16 \times k}$ and $\mathbf{B} \in \mathrm{FP} 32^{k \times 16}$ are initialized with random numbers generated from a uniform distribution $(-1,1)$. The error is calculated by Eq. (7).</p>
<ul>
<li>We include these modifications to NVIDIA CUTLASS and evaluate the matrix-matrix multiplication accuracy, computational throughput, and power consumption. Our implementation shows higher performance and lower power consumption compared to cuBLAS SGEMM on FP32 SIMT Cores while retaining the same accuracy.</li>
</ul>
<h1>Background</h1>
<h2>Rounding</h2>
<p>The rounding of floating-point numbers is the key to understanding the present contribution, so we will first describe the different types of rounding. Let us consider a floating-point value with $\ell$ mantissa bits, and the cases where it is rounded to $n$ bits.</p>
<p>$$
m=\underbrace{\overbrace{m_{\ell-1} m_{\ell-2} \cdots m_{\ell-n}}^{\ell \text { bit }} m_{\ell-n-1} \cdots m_{0}}_{n \text { bit }}
$$</p>
<p>The different rounding modes described below are defined by a combination of two basic operations; rounding-up and rounding-down (truncation). Rounding-up adds 1 to $m_{\ell-1} m_{\ell-2} \cdots m_{\ell-n}$, while roundingdown does nothing to $m_{\ell-1} m_{\ell-2} \cdots m_{\ell-n}$. In this paper, we use three types of rounding modes:</p>
<h2>Round to Nearest; ties to even (RN)</h2>
<p>1) Truncate when $m_{l-n-1}$ equals 0 .
2) When $m_{l-n-1}$ equals 1 :
(a) Round-up when at least one of $m_{l-n-2}, \cdots, m_{0}$ is 1 .
(b) Truncate or round up so that $m_{l-n}$ becomes 0 when all of $m_{l-n-2}, \cdots, m_{0}$ are 0 (Ties to even).</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Performance comparison of our method in TF32 and FP16, cuBLAS SGEMM and the FP32 theoretical peak. Our methods compute the single-precision matrix-matrix multiplication with the same accuracy as cuBLAS SGEMM.</p>
<h3>Round to Nearest; ties to Away from zero (RNA)</h3>
<ol>
<li>Truncate when $m_{l-n-1}$ equals 0.</li>
<li>Round-up when $m_{l-n-1}$ equals 1.</li>
</ol>
<h3>Round toward Zero (RZ)</h3>
<p>Always truncate.</p>
<p>We show the number line representation of RN, RNA and RZ in Figure 3.</p>
<h3>Tensor Cores</h3>
<h4>Programming interface</h4>
<p>NVIDIA provides the WMMA API for using Tensor Cores in CUDA/C++. When using the WMMA API, the input matrices (in FP16) are copied to a register array called "Fragments" using a WMMA API function. A pseudocode for computing matrix-matrix multiplication $\mathbf{C} \leftarrow \mathbf{A} \times \mathbf{B}$ on Tensor Cores using WMMA API is shown in Code 1. In this pseudocode, we divide the input matrices into sub-matrices and compute matrix-matrix multiplications on them. The functions fill_fragment, load_matrix_sync, mma_sync, mma_sync and store_matrix_sync in the pseudocode are part of the WMMA API. The flow of computation is as follows:</p>
<ol>
<li>Initialize a fragment frag_c for accumulating the resulting matrices using fill_fragment function.</li>
<li>Load sub-matrices of $\mathbf{A}, \mathbf{B}$ from memory to fragments frag_a, frag_b using the load_matrix_sync function.</li>
<li>Compute the matrix-matrix multiplication of frag_a, frag_b and accumulate to frag_c using the mma_sync function.</li>
<li>Store the sub-matrix of $\mathbf{C}$ from the fragment frag_c to memory.</li>
</ol>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The roundings we use in this paper.</p>
<div class="codehilite"><pre><span></span><code><span class="w"> </span><span class="n">device_</span><span class="w"> </span><span class="n">void</span><span class="w"> </span><span class="n">matmul</span><span class="p">(</span><span class="n">mem_c</span><span class="p">,</span><span class="w"> </span><span class="n">mem_a</span><span class="o">[</span><span class="n">K</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">mem_b</span><span class="o">[</span><span class="n">K</span><span class="o">]</span><span class="p">)</span><span class="w"> </span><span class="err">{</span>
<span class="w">    </span><span class="n">fragment</span><span class="w"> </span><span class="n">frag_a</span><span class="p">,</span><span class="w"> </span><span class="n">frag_b</span><span class="p">,</span><span class="w"> </span><span class="n">frag_c</span><span class="p">;</span>
<span class="w">    </span><span class="n">shared_mem_fp16</span><span class="w"> </span><span class="n">smem_a</span><span class="p">,</span><span class="w"> </span><span class="n">smem_b</span><span class="p">;</span>
<span class="w">    </span><span class="o">//</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span><span class="w"> </span><span class="k">Initialize</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">accumulator</span><span class="w"> </span><span class="n">fragment</span>
<span class="w">    </span><span class="n">fill_fragment</span><span class="p">(</span><span class="n">frag_c</span><span class="p">,</span><span class="w"> </span><span class="mf">0.</span><span class="n">f</span><span class="p">);</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="n">k</span><span class="o">&lt;</span><span class="n">K</span><span class="p">;</span><span class="n">k</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="err">{</span>
<span class="w">        </span><span class="o">//</span><span class="w"> </span><span class="nf">Convert</span><span class="w"> </span><span class="n">subdicided</span><span class="w"> </span><span class="n">matrix</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">FP16</span>
<span class="w">        </span><span class="n">smem_a</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">toFP16</span><span class="p">(</span><span class="n">mem_a</span><span class="o">[</span><span class="n">k</span><span class="o">]</span><span class="p">);</span>
<span class="w">        </span><span class="n">smem_b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">toFP16</span><span class="p">(</span><span class="n">mem_b</span><span class="o">[</span><span class="n">k</span><span class="o">]</span><span class="p">);</span>
<span class="w">        </span><span class="o">//</span><span class="w"> </span><span class="mi">2</span><span class="p">;</span><span class="w"> </span><span class="k">Load</span><span class="w"> </span><span class="n">subdivided</span><span class="w"> </span><span class="n">matrices</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">fragments</span>
<span class="w">        </span><span class="n">load_matrix_sync</span><span class="p">(</span><span class="n">frag_a</span><span class="p">,</span><span class="w"> </span><span class="n">smem_a</span><span class="p">,</span><span class="w"> </span><span class="p">...);</span>
<span class="w">        </span><span class="n">load_matrix_sync</span><span class="p">(</span><span class="n">frag_b</span><span class="p">,</span><span class="w"> </span><span class="n">smem_b</span><span class="p">,</span><span class="w"> </span><span class="p">...);</span>
<span class="w">        </span><span class="o">//</span><span class="w"> </span><span class="mi">3</span><span class="p">;</span><span class="w"> </span><span class="k">Compute</span><span class="w"> </span><span class="n">matrix</span><span class="o">-</span><span class="n">matrix</span><span class="w"> </span><span class="n">multiplication</span>
<span class="w">        </span><span class="o">//</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">accumulation</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">Tensor</span><span class="w"> </span><span class="n">Cores</span>
<span class="w">        </span><span class="n">mma_sync</span><span class="p">(</span><span class="n">frag_c</span><span class="p">,</span><span class="w"> </span><span class="n">frag_a</span><span class="p">,</span><span class="w"> </span><span class="n">frag_b</span><span class="p">,</span><span class="w"> </span><span class="n">frag_c</span><span class="p">);</span>
<span class="w">    </span><span class="err">}</span>
<span class="w">    </span><span class="o">//</span><span class="w"> </span><span class="mi">4</span><span class="p">;</span><span class="w"> </span><span class="n">Store</span><span class="w"> </span><span class="k">result</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">memory</span>
<span class="w">    </span><span class="n">store_matrix_sync</span><span class="p">(</span><span class="n">mem_c</span><span class="p">,</span><span class="w"> </span><span class="n">frag_c</span><span class="p">,</span><span class="w"> </span><span class="p">...);</span>
<span class="err">}</span>
</code></pre></div>

<p>Code 1: A simple matrix-matrix multiplication pseudocode on Tensor Cores using WMMA API.</p>
<h1>Fragment mapping and PTX Instructions</h1>
<p>The functions load_matrix_sync and store_matrix_sync, map the memory index of the input matrix elements to the fragment index. We can analyze this mapping and use it for reducing the memory footprint. Our wmma_extension library provides functions for that purpose. For instance, computing matrix-vector multiplication without unnecessary memory footprint, and loading/storing fragments with custom operations for each element.</p>
<p>There are two types of PTX instructions: wmma and mma. The wmma instructions provide finer control compared to the mma instruction, such as loading fragments using wmma.load, computing matrix-matrix multiplication accumulation using wmma.mma, and storing fragments using wmma. store. However, we chose to use the mma due to the more efficient register usage compared to wmma.</p>
<p>When we use wmma instructions, each element in matrices $\mathbf{A}$, and $\mathbf{B}$ is kept by two elements inside the</p>
<p>fragments on threads in a warp. On the other hand, for mma each element of the input matrices is kept by only one element of the fragments on threads in a warp without duplication. To use mma, we need to map the memory and the fragment manually since the load and store instructions for mma do not exist. This map for mma is available in the NVIDIA Toolkit Documentation ${ }^{4}$.</p>
<h1>Single-precision matrix-matrix multiplication using error correction technique on Tensor Cores</h1>
<p>As we mentioned previously, in order to use Tensor Cores for single-precision matrix-matrix multiplication we need to convert the input matrices to FP16 which introduces a truncation error. Markidis et al. propose a method to correct this truncation error by keeping the mantissa loss in additional FP16 variables and using them for correcting the accuracy of matrix-matrix multiplication [17].</p>
<p>$$
\begin{aligned}
\mathbf{A}<em _mathrm_F="\mathrm{F">{\mathrm{F} 16} &amp; \leftarrow \operatorname{toFP16}\left(\mathbf{A}</em>\right) \
\Delta \mathbf{A}} 32<em _mathrm_F="\mathrm{F">{\mathrm{F} 16} &amp; \leftarrow \operatorname{toFP16}\left(\mathbf{A}</em>} 32}-\operatorname{toFP} 32\left(\mathbf{A<em _mathrm_F="\mathrm{F">{\mathrm{F} 16}\right)\right) \
\mathbf{B}</em>} 16} &amp; \leftarrow \operatorname{toFP16}\left(\mathbf{B<em _mathrm_F="\mathrm{F">{\mathrm{F} 32}\right) \
\Delta \mathbf{B}</em>} 16} &amp; \leftarrow \operatorname{toFP16}\left(\mathbf{B<em _mathrm_F="\mathrm{F">{\mathrm{F} 32}-\operatorname{toFP} 32\left(\mathbf{B}</em>\right)\right) \
\dot{\mathbf{C}}} 16<em _mathrm_F="\mathrm{F">{\mathrm{F} 32} &amp; \leftarrow \mathbf{A}</em>} 16} \mathbf{B<em _mathrm_F="\mathrm{F">{\mathrm{F} 16}+\Delta \mathbf{A}</em>} 16} \mathbf{B<em _mathrm_F="\mathrm{F">{\mathrm{F} 16} \
&amp; +\mathbf{A}</em>} 16} \Delta \mathbf{B<em _mathrm_F="\mathrm{F">{\mathrm{F} 16}+\Delta \mathbf{A}</em>
\end{aligned}
$$} 16} \Delta \mathbf{B}_{\mathrm{F} 16</p>
<div class="codehilite"><pre><span></span><code><span class="n">void</span><span class="w"> </span><span class="n">matmul</span><span class="p">(</span><span class="n">mem_c</span><span class="p">,</span><span class="w"> </span><span class="n">mem_a</span><span class="o">[</span><span class="n">K</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">mem_b</span><span class="o">[</span><span class="n">K</span><span class="o">]</span><span class="p">)</span><span class="w"> </span><span class="err">{</span>
<span class="w">    </span><span class="n">fragment</span><span class="w"> </span><span class="n">frag_a</span><span class="p">,</span><span class="w"> </span><span class="n">frag_b</span><span class="p">,</span><span class="w"> </span><span class="n">frag_c</span><span class="p">;</span>
<span class="w">    </span><span class="n">fragment</span><span class="w"> </span><span class="n">frag_da</span><span class="p">,</span><span class="w"> </span><span class="n">frag_db</span><span class="p">;</span>
<span class="w">    </span><span class="n">shared_mem_fp16</span><span class="w"> </span><span class="n">smem_a</span><span class="p">,</span><span class="w"> </span><span class="n">smem_b</span><span class="p">;</span>
<span class="w">    </span><span class="n">shared_mem_fp16</span><span class="w"> </span><span class="n">smem_da</span><span class="p">,</span><span class="w"> </span><span class="n">smem_db</span><span class="p">;</span>
<span class="o">//</span><span class="w"> </span><span class="k">Initialize</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">accumulator</span><span class="w"> </span><span class="n">fragment</span>
<span class="w">    </span><span class="n">fill_fragment</span><span class="p">(</span><span class="n">frag_c</span><span class="p">,</span><span class="mf">0.</span><span class="n">f</span><span class="p">);</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="n">k</span><span class="o">&lt;</span><span class="n">K</span><span class="p">;</span><span class="n">k</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="err">{</span>
<span class="w">        </span><span class="o">//</span><span class="w"> </span><span class="k">Compute</span><span class="w"> </span><span class="n">eq</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span>
<span class="w">        </span><span class="n">smem_a</span><span class="o">=</span><span class="n">toFP16</span><span class="p">(</span><span class="n">mem_a</span><span class="o">[</span><span class="n">k</span><span class="o">]</span><span class="p">)</span>
<span class="w">        </span><span class="n">smem_b</span><span class="o">=</span><span class="n">toFP16</span><span class="p">(</span><span class="n">mem_b</span><span class="o">[</span><span class="n">k</span><span class="o">]</span><span class="p">)</span>
<span class="w">        </span><span class="o">//</span>
<span class="w">        </span><span class="n">load_matrix_sync</span><span class="p">(</span><span class="n">frag_a</span><span class="w"> </span><span class="p">,</span><span class="n">smem_a</span><span class="p">);</span>
<span class="w">        </span><span class="n">load_matrix_sync</span><span class="p">(</span><span class="n">frag_b</span><span class="w"> </span><span class="p">,</span><span class="n">smem_b</span><span class="p">);</span>
<span class="w">        </span><span class="o">//</span><span class="w"> </span><span class="k">Compute</span><span class="w"> </span><span class="n">eq</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span>
<span class="w">        </span><span class="n">smem_da</span><span class="o">=</span><span class="n">toFP16</span><span class="p">(</span><span class="n">mem_a</span><span class="o">[</span><span class="n">k</span><span class="o">]-</span><span class="n">toFP32</span><span class="p">(</span><span class="n">smem_a</span><span class="p">))</span>
<span class="w">        </span><span class="n">smem_db</span><span class="o">=</span><span class="n">toFP16</span><span class="p">(</span><span class="n">mem_b</span><span class="o">[</span><span class="n">k</span><span class="o">]-</span><span class="n">toFP32</span><span class="p">(</span><span class="n">smem_b</span><span class="p">))</span>
<span class="w">        </span><span class="o">//</span>
<span class="w">        </span><span class="n">load_matrix_sync</span><span class="p">(</span><span class="n">frag_da</span><span class="p">,</span><span class="n">smem_da</span><span class="p">);</span>
<span class="w">        </span><span class="n">load_matrix_sync</span><span class="p">(</span><span class="n">frag_db</span><span class="p">,</span><span class="n">smem_db</span><span class="p">);</span>
<span class="w">        </span><span class="o">//</span><span class="w"> </span><span class="k">Compute</span><span class="w"> </span><span class="n">eq</span><span class="w"> </span><span class="mi">6</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">accumulate</span>
<span class="w">        </span><span class="n">mma_sync</span><span class="p">(</span><span class="n">frag_c</span><span class="w"> </span><span class="p">,</span><span class="n">frag_da</span><span class="w"> </span><span class="p">,</span><span class="n">frag_db</span><span class="w"> </span><span class="p">,</span><span class="n">frag_c</span><span class="p">);</span>
<span class="w">        </span><span class="n">mma_sync</span><span class="p">(</span><span class="n">frag_c</span><span class="w"> </span><span class="p">,</span><span class="n">frag_da</span><span class="w"> </span><span class="p">,</span><span class="n">frag_b</span><span class="w"> </span><span class="p">,</span><span class="n">frag_c</span><span class="p">);</span>
<span class="w">        </span><span class="n">mma_sync</span><span class="p">(</span><span class="n">frag_c</span><span class="w"> </span><span class="p">,</span><span class="n">frag_a</span><span class="w"> </span><span class="p">,</span><span class="n">frag_db</span><span class="w"> </span><span class="p">,</span><span class="n">frag_c</span><span class="p">);</span>
<span class="w">        </span><span class="n">mma_sync</span><span class="p">(</span><span class="n">frag_c</span><span class="w"> </span><span class="p">,</span><span class="n">frag_a</span><span class="w"> </span><span class="p">,</span><span class="n">frag_b</span><span class="w"> </span><span class="p">,</span><span class="n">frag_c</span><span class="p">);</span>
<span class="w">    </span><span class="err">}</span>
<span class="w">    </span><span class="o">//</span><span class="w"> </span><span class="n">Store</span><span class="w"> </span><span class="k">result</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">memory</span>
<span class="w">    </span><span class="n">store_matrix_sync</span><span class="p">(</span><span class="n">mem_c</span><span class="p">,</span><span class="n">frag_c</span><span class="p">);</span>
<span class="err">}</span>
</code></pre></div>

<p>Code 2: A simple pseudocode example of Markidis' error correction method</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>We show the accuracy of Markidis' method in Figure 1. To evaluate the accuracy, we compute the relative residual using the following equation.</p>
<p>$$
\text { RelativeResidual }=\frac{\left|\mathbf{C}<em _Target="{Target" _text="\text">{\mathrm{FP} 64}-\mathbf{C}</em>\right|}<em _mathrm_FP="\mathrm{FP">{F}}{\left|\mathbf{C}</em>
$$} 64}\right|_{F}</p>
<p>where the $|\cdot|<em _mathrm_FP="\mathrm{FP">{F}$ is the Frobenius norm. We compute the reference matrix-matrix multiplication $\mathbf{C}</em>} 64}=$ toFP64 $\left(\mathbf{A<em _mathrm_FP="\mathrm{FP">{\mathrm{FP} 32}\right) \cdot$ toFP64 $\left(\mathbf{B}</em>\right)$ in FP64.} 32</p>
<p>From Figure 1 we see that, Markidis' method is more accurate than the Tensor Core without error correction for smaller matrix sizes. However, we found that RZ rounding inside Tensor Cores accumulates errors faster than the RN of FP32 SIMT Cores, and the accuracy catches up to the Tensor Core as the matrix size becomes larger. Feng et al. propose to reduce the mantissa loss in Eqs. (3) and (5) by using the sign bit as an extra mantissa bit [7]. However, our experiments could not show any advantage by faithfully reproducing what is described in their paper.</p>
<h1>Error investigation and improvements</h1>
<h2>Expectation of mantissa length</h2>
<p>We can write Eqs. (2) and (3) and Eqs. (4) and (5) for each element as</p>
<p>$$
\begin{aligned}
v_{\mathrm{F} 16} &amp; \leftarrow \operatorname{toFP16}\left(v_{\mathrm{F} 32}\right) \
\Delta v_{\mathrm{F} 16} &amp; \leftarrow \operatorname{toFP16}\left(v_{\mathrm{F} 32}-\operatorname{toFP32}\left(v_{\mathrm{F} 16}\right)\right)
\end{aligned}
$$</p>
<p>The input element $v_{\mathrm{F} 32}$ is approximated by $v_{\mathrm{F} 16}+\Delta v_{\mathrm{F} 16}$. The mantissa length of FP32 is $23+1=24$ bit, including an implicit 1 bit, and the FP16 is $10+1=11$ bit. Thus $v_{\mathrm{F} 16}+\Delta v_{\mathrm{F} 16}$ can not represent the full mantissa of $v_{\mathrm{F} 32}$. In this section, we calculate the expectation of the mantissa length kept by Eq. (8) and (9).</p>
<p>We express the mantissa bit of $v_{\mathrm{F} 32}$ as $m_{22} m_{21} \cdots m_{0}$ from MSB without the implicit 1 bit, and the rounding during the conversion to FP16 is RN, which is the default in CUDA. In this case, $m_{13} \cdots m_{0}$ bits decide whether we round-up in Eq. (8). We show the mantissa length kept by Eqs. (8) and (9) and its probability of occurrence in the case of $m_{13} \cdots m_{0}$ in Table 1. This probability is calculated under Assumption 1 for the mantissa part of floating-point numbers as follows.</p>
<p>Assumption 1 Each bit of the mantissa is 1 with probability $\frac{1}{2}$, and each bit is statistically independent
It follows that the expectation of the mantissa length is 22.75 bits out of the FP32 mantissa length of 23 bits. Furthermore, when we use RNA for rounding during the FP16 conversion in Eqs. (8) and (9), the values of $v_{\mathrm{F} 16}$ and $\Delta v_{\mathrm{F} 16}$ are different from RN. However, the mantissa length and its probability of occurrence are the same as RN, and the expectation of mantissa length is 22.75 bits.</p>
<p>To evaluate the effect of this 22.75 bits of mantissa, we evaluate the accuracy of FP32 matrix-matrix multiplication by truncating the Least Significant Bit (LSB) of the mantissa of input matrices. The expectation of the mantissa length during this operation is 22.5 bits under Assumption 1, which is shorter than 22.75 bits. We show the accuracy comparison between this truncation and Markidis' method in Figure 4, and see that the accuracy of Markidis' method is worse than this method despite the higher expectation of mantissa length. In conclusion, the mantissa loss is not the main cause of error during matrix-matrix multiplication on Tensor Cores.</p>
<p>Although the mantissa length of FP16 is $10+1=11$ bits and two FP16s can keep only 22 bits of mantissa per $23+1=24$ bits of FP32 mantissa, the expectation of the mantissa length is $22.75+1=23.75$ bits. The reason for this can be explained as follows:</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The accuracy comparison between Markidis' method, FP32 SIMT Core, and truncating the last 1 bit of FP32 mantissa. Input matrices $\mathbf{A}$ and $\mathbf{B}$ are initialized with a uniform distribution $(-1,1)$.</p>
<ul>
<li>When the last $n$ bits of mantissa are 0 , the mantissa length to keep is $23+1-n$ bits per 24 bits. We can keep the full mantissa when $n \geq 2$.</li>
<li>When $l_{0}$ is greater than or equal to 2 , the mantissa length kept by $\Delta v_{\mathrm{F} 16}$ is less than or equal to 10 . Therefore, we can keep the full mantissa.</li>
<li>Rounding-up can be performed during the conversion in Eq. (8) when $l_{0}=0$. It keeps more mantissa compared to RZ. It follows that $\left|v_{\mathrm{FP} 16}\right|&gt;\left|v_{\mathrm{FP} 32}\right|$ when rounding-up. And the signs of $\Delta v_{\mathrm{FP} 16}$ and $v_{\mathrm{FP} 32}$ are different because $v_{\mathrm{FP} 16}$ and $v_{\mathrm{FP} 32}$ have the same sign. Let us consider the following example. The mantissa of $v_{\mathrm{FP} 32}$ is represented by a $23+1=24$ bit integer shown in Eq. (10).</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: center;">$l_{0}$</th>
<th style="text-align: center;">$m_{13}$</th>
<th style="text-align: center;">$m_{12}$</th>
<th style="text-align: center;">$m_{11}$</th>
<th style="text-align: center;">$m_{1}$</th>
<th style="text-align: center;">$m_{0}$</th>
<th style="text-align: center;">len</th>
<th style="text-align: center;">prob</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\geq 2$</td>
<td style="text-align: center;">*</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">*</td>
<td style="text-align: center;">*</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">$1 / 4$</td>
</tr>
<tr>
<td style="text-align: center;">$=1$</td>
<td style="text-align: center;">*</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">*</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">$1 / 8$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">*</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">*</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">$1 / 8$</td>
</tr>
<tr>
<td style="text-align: center;">$=0$</td>
<td style="text-align: center;">*</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">*</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">$1 / 8$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">*</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">*</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">$1 / 8$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">*</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">*</td>
<td style="text-align: center;">*</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">$1 / 4$</td>
</tr>
</tbody>
</table>
<p>Table 1: The mantissa length kept by $v_{\mathrm{F} 16}$ and $\Delta v_{\mathrm{F} 16}$ (len), the probability of occurrence (prob) when RN is performed during the FP16 conversion in Eqs. (8) and ( 9). $m_{22} m_{21} \cdots m_{0}$ represents the 23 bits of FP32 mantissa, and the probability is calculated under Assumption 1. The length $l_{0}$ is the number of consecutive zeros from $m_{12}$ towards the LSB. The mark "*" means it does not matter if it is 0 or 1 .</p>
<table>
<thead>
<tr>
<th style="text-align: left;">$l_{0}$</th>
<th style="text-align: left;">$m_{13}$</th>
<th style="text-align: left;">$m_{12}$</th>
<th style="text-align: left;">$m_{11}$</th>
<th style="text-align: left;">$m_{1}$</th>
<th style="text-align: left;">$m_{0}$</th>
<th style="text-align: left;">len</th>
<th style="text-align: left;">prob</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\geq 3$</td>
<td style="text-align: left;">$*$</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">$*$</td>
<td style="text-align: left;">$*$</td>
<td style="text-align: left;">23</td>
<td style="text-align: left;">$1 / 4$</td>
</tr>
<tr>
<td style="text-align: left;">$=2$</td>
<td style="text-align: left;">$*$</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">$*$</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">23</td>
<td style="text-align: left;">$1 / 8$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$*$</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">$*$</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">22</td>
<td style="text-align: left;">$1 / 8$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$*$</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">$*$</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">$*$</td>
<td style="text-align: left;">21</td>
<td style="text-align: left;">$1 / 4$</td>
</tr>
<tr>
<td style="text-align: left;">$=1$</td>
<td style="text-align: left;">$*$</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">$*$</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">22</td>
<td style="text-align: left;">$1 / 8$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$*$</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">$*$</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">23</td>
<td style="text-align: left;">$1 / 8$</td>
</tr>
</tbody>
</table>
<p>Table 2: The mantissa length kept by $v_{\mathrm{F} 16}$ and $\Delta v_{\mathrm{F} 16}$ (len), the probability of occurrence (prob) when RZ is performed during the FP16 conversion in Eqs. (8) and (9). $m_{22} m_{21} \cdots m_{0}$ represents the 23 bits of FP32 mantissa, and the probability is calculated under Assumption 1. $l_{0}$ is the number of consecutive zeros from $m_{12}$ toward LSB. The mark "*" means it does not matter if it is 0 or 1 .</p>
<p>The $I_{\mathrm{FP} 32}$ is kept by two $10+1=11$ bit integers $I_{\mathrm{FP} 16}$ and $\Delta I_{\mathrm{FP} 16}$ using RZ.</p>
<p>$$
\begin{aligned}
&amp; I_{\mathbf{F P 1 6}}=+\underbrace{10000000000^{\prime}}<em 1="1" 6="6" P="P" _mathbf_F="\mathbf{F">{\mathbf{1 1 b i t}} \
&amp; \Delta I</em>= \
&amp; +\underbrace{10000000000^{\prime}}}<em _mathbf_2="\mathbf{2" b="b" i="i" t="t">{\mathbf{1 1 b i t}} \
&amp; \text { Loss } \
&amp; +\underbrace{11}</em>
\end{aligned}
$$}</p>
<p>In this case, 2 bits of mantissa loss occurs. On the other hand, when we use RN instead of RZ, only 1 bit of mantissa loss occurs.</p>
<p>$$
\begin{aligned}
&amp; I_{\mathbf{F P 1 6}}=+\underbrace{10000000001^{\prime}}<em 1="1" 6="6" P="P" _mathbf_F="\mathbf{F">{\mathbf{1 1 b i t}} \
&amp; \Delta I</em>}}=\quad-0 \underbrace{1111111111^{\prime} 0<em _mathbf_1="\mathbf{1" b="b" i="i" t="t">{\mathbf{1 1 b i t}} \
&amp; \text { Loss } \
&amp; +\underbrace{1}</em>
\end{aligned}
$$}</p>
<p>This is because we need fewer bits to keep $n$ bits of an integer $a$ when $a&gt;2^{n-1}$, if we keep $2^{n}-a$ instead of $a$.</p>
<p>We also calculate the expectation of the mantissa length when we use RZ in Eqs. (8) and (9). We show the mantissa length and its probability of occurrence in Table 2. The expectation of the mantissa length is 22.25 bits.</p>
<h1>Avoiding RZ during Tensor Core accumulation</h1>
<p>The accumulator inside Tensor Cores has at least 2 extra bits of mantissa and RZ is used for rounding [6]. It follows that RZ is performed in the accumulator frag_c in every $k$ iteration in Code 2. We evaluate the effect of this RZ for the matrix-matrix multiplication using Markidis' method using the mixed-precision matrix-matrix multiplication function mma_rn and mma_rz that perform similar operations with Tensor Cores. Both functions compute the matrix-matrix multiplication as follows</p>
<p>$$
\mathbf{D}<em _mathrm_F="\mathrm{F">{\mathrm{F} 32} \leftarrow \mathbf{A}</em>} 16} \times \mathbf{B<em _mathrm_F="\mathrm{F">{\mathrm{F} 16}+\mathbf{C}</em>
$$} 32</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: The evaluation of the accuracy of single-precision matrix-matrix multiplication using Markidis' error correction method with mma_rn and mma_rz instead of Tensor Cores.</p>
<p>The multiplication of each element is computed in FP32 and accumulation in FP64. We truncate the mantissa of the accumulator to keep them in 25 bit after every element accumulation. The difference between these two functions is that the mma_rn performs RN for rounding after the addition of $\mathbf{C}_{\mathrm{F} 32}$ in Eq. (11), while the mma_rz performs RZ in the same way as Tensor Cores do.</p>
<p>We use mma_rz and mma_rn, instead of Tensor Cores, to compute a single-precision matrix-matrix multiplication using Markidis' method and evaluate the accuracy. The results are shown in Figure 5. While the accuracy using mma_rn is the same as FP32 SIMT Core, the one using mma_rz is the same as Markidis' method. Therefore, we conclude that performing RZ in the accumulator after addition to $\mathbf{C}_{\mathrm{F} 32}$ in Eq. (11) causes the accuracy loss in Markidis' method.</p>
<p>To avoid the RZ and improve the accuracy, we compute the addition to $\mathbf{C}_{\mathrm{F} 32}$ in Eq. (11) outside of the Tensor Cores. We show the flow of our proposed method, and compare it against the standard process presented in Figure 6. We input a zero matrix to the Tensor Cores and compute the addition shown in Eq. (11) outside of the Tensor Cores using FP32 SIMT Cores which performs RN for rounding. By using this technique, the accuracy of the single-precision matrix-matrix multiplication using Markidis' method improves to the same accuracy as FP32 SIMT Cores, as shown in Figure 1. On the other hand, this technique requires more registers to keep a zero matrix, additional process for making the zero matrix, and extra addition operations on FP32 SIMT Cores compared to Markidis' method.</p>
<h1>Reducing the underflow and gradual underflow probability in $\Delta \mathbf{v}_{\mathrm{F} 16}$ computations</h1>
<p>The probabilities of underflow and gradual underflow in subtracting two values are high when their absolute values are close. This is shown in Eq. (9). We calculate the probabilities for each exponent value of $v_{\mathrm{F} 32}$ and improve Eq. (9) to reduce the underflow. To calculate these probabilities, we first define some constant values, the exponent bias of FP16 $b_{\mathrm{F} 16}=15$, and the mantissa length of FP16 $l_{\mathrm{F} 16}=10$ and FP32 $l_{\mathrm{F} 32}=23$. To simplify the calculation, we assume that RZ is used in toFP16 in Eqs. (8) and (9) while RN is used otherwise. Under this assumption, the first 10 bits of the mantissa of $v_{\mathrm{F} 32}$ are kept by $v_{\mathrm{F} 16}$, and the 10 bits from $\left(l_{\mathrm{F} 16}+l_{0}\right)$ th bit are kept by $\Delta v_{\mathrm{F} 16}$ shown in Figure 7. Therefore, the exponent value of $\Delta v_{\mathrm{F} 16}$ is smaller than the $v_{\mathrm{F} 32}$ by $l_{0}+l_{\mathrm{F} 16}+1$, the last 1 is the length of the implicit bit of the mantissa. We</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: The flow of computation to avoid RZ inside Tensor Cores, which affects the accuracy of Markidis' error correction method. Left: Standard usage of Tensor Cores via WMMA API. In this case, RZ is performed on the accumulator $\mathbf{C}<em _mathrm_F="\mathrm{F">{\mathrm{FP} 32}$ directly. Right: Our method for avoiding the RZ. We accumulate the result of the matrix-matrix multiplication $\mathbf{A}</em>} 16} \times \mathbf{B<em _mathrm_F="\mathrm{F">{\mathrm{F} 16}$ to $\mathbf{C}</em>$ outside of Tensor Cores using FP32 SIMT Cores.
calculate the probabilities using this fact into consideration. We also define the exponent value of $v_{\mathrm{F} 32}$ as $e_{v}$ including exponent bias, where $v_{\mathrm{F} 32}$ is represented as follows,} 32</p>
<p>$$
v_{\mathrm{F} 32}=1 \cdot m_{22} m_{21} \cdots m_{0} \times 2^{e_{v}}
$$</p>
<p>First, we calculate $P_{u+g u}\left(e_{v}\right)$, the probability at which gradual underflow occurs. When normalized, the smallest number that can be expressed in FP16 is $2^{-b_{\mathrm{F} 16}+1}$. Therefore, the condition for underflow or gradual underflow in Eq. (9) can be represented as</p>
<p>$$
\begin{aligned}
e_{v}-\left(l_{0}+l_{\mathrm{F} 16}+1\right) &amp; &lt;-b_{\mathrm{F} 16}+1 \
\Rightarrow e_{v}-l_{\mathrm{F} 16}+b_{\mathrm{F} 16}-2 &amp; &lt;l_{0}
\end{aligned}
$$</p>
<p>We define the probability $P\left(l_{0}=n\right)$ such that $l_{0}$ equals to $n$ under the assumption 1 as</p>
<p>$$
P\left(l_{0}=n\right)= \begin{cases}0 &amp; (n&lt;0) \ \left(\frac{1}{2}\right)^{n+1} &amp; \left(0 \leq n&lt;l_{\mathrm{F} 32}-l_{\mathrm{F} 16}\right) \ \left(\frac{1}{2}\right)^{l_{\mathrm{F} 32}-l_{\mathrm{F} 16}} &amp; \left(n=l_{\mathrm{F} 32}-l_{\mathrm{F} 16}\right)\end{cases}
$$</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: An example of the mantissa bitstring movement in Eqs. (8) and (9) with RZ. When $l_{0}$ is larger than 2 , the last $l_{0}-2$ bits of the mantissa of $\Delta v_{\mathrm{F} 16}$ are filled with zeros.</p>
<p>Then, by using $P\left(l_{0}=n\right)$, we calculate the desired probability $P_{u+g u}\left(e_{v}\right)$ as</p>
<p>$$
P_{u+g u}\left(e_{v}\right)=\sum_{l=\left(e_{v}-l_{\mathrm{F} 16}+b_{\mathrm{F} 16}-2\right)+1}^{l_{\mathrm{F} 32}-l_{\mathrm{F} 16}} P\left(l_{0}=l\right)
$$</p>
<p>Next, we calculate $P_{u}\left(e_{v}\right)$, the probability where only underflow occurs. Without normalization, the smallest number that can be expressed by FP16 is $2^{-\left(b_{\mathrm{F} 16}+l_{\mathrm{F} 16}\right)+1}$. Therefore, the condition at which underflow occurs can be expressed as</p>
<p>$$
\begin{aligned}
e_{v} &amp; -\left(l_{0}+l_{\mathrm{F} 16}+1\right)&lt;-\left(b_{\mathrm{F} 16}+l_{\mathrm{F} 16}\right)+1 \
&amp; \Rightarrow e_{v}+b_{\mathrm{F} 16}-2&lt;l_{0}
\end{aligned}
$$</p>
<p>Using $P\left(l_{0}=n\right)$ in the same way as $P_{u+g u}\left(e_{v}\right)$, we can calculate the desired probability $P_{u}\left(e_{v}\right)$ as</p>
<p>$$
P_{u}\left(e_{v}\right)=\sum_{l=\left(e_{v}+b_{\mathrm{F} 16}-2\right)+1}^{l_{\mathrm{F} 32}-l_{\mathrm{F} 16}} P\left(l_{0}=l\right)
$$</p>
<p>In Figure 8, we show the theoretical $P_{u+g u}\left(e_{v}\right)$ and $P_{u}\left(e_{v}\right)$ calculated by Eq. (15) and Eq. (17) respectively, along with the experimental values aggregated on GPUs. We find that gradual underflow occurs in Eq. (9) even if $v_{\mathrm{F} 32}$ is around $10^{0}$.</p>
<p>To reduce these probabilities, we add $l_{\mathrm{F} 16}+1=11$ to the exponent of the result of subtraction in Eq. (9) by multiplying $2^{11}$,</p>
<p>$$
\Delta v_{\mathrm{F} 16} \leftarrow \operatorname{toFP} 16\left(\left(v_{\mathrm{F} 32}-\operatorname{toFP} 32\left(v_{\mathrm{F} 16}\right)\right) \times 2^{11}\right)
$$</p>
<p>This process does not affect the mantissa. In this paper, we refer to the method that keeps $v_{\mathrm{F} 32}$ in Eqs. (8) and (18) as halfhalf, and the method in Eqs. (8) and (9) as Markidis' halfhalf. The single-precision</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: The theoretical and experimental probability of underflow $P_{\mathrm{u}}\left(e_{v}\right)$ in Eq. (9) and the sum of underflow and gradual underflow $P_{\mathrm{u}+\mathrm{gu}}\left(e_{v}\right)$.
matrix-matrix multiplication using halfhalf can be written as</p>
<p>$$
\begin{aligned}
\mathbf{A}<em _mathrm_F="\mathrm{F">{\mathrm{F} 16} &amp; \leftarrow \operatorname{toFP16}\left(\mathbf{A}</em>\right) \
\Delta \mathbf{A}} 32<em _mathrm_F="\mathrm{F">{\mathrm{F} 16} &amp; \leftarrow \operatorname{toFP16}\left(\left(\mathbf{A}</em>} 32}-\operatorname{toFP32}\left(\mathbf{A<em _mathrm_F="\mathrm{F">{\mathrm{F} 16}\right)\right) \times 2^{11}\right) \
\mathbf{B}</em>} 16} &amp; \leftarrow \operatorname{toFP16}\left(\mathbf{B<em _mathrm_F="\mathrm{F">{\mathrm{F} 32}\right) \
\Delta \mathbf{B}</em>} 16} &amp; \leftarrow \operatorname{toFP16}\left(\left(\mathbf{B<em _mathrm_F="\mathrm{F">{\mathrm{F} 32}-\operatorname{toFP32}\left(\mathbf{B}</em>\right) \
\dot{\mathbf{C}}} 16}\right)\right) \times 2^{11<em _mathrm_F="\mathrm{F">{\mathrm{F} 32} &amp; \leftarrow \mathbf{A}</em>} 16} \mathbf{B<em _mathrm_F="\mathrm{F">{\mathrm{F} 16} \
&amp; +\left(\Delta \mathbf{A}</em>} 16} \mathbf{B<em _mathrm_F="\mathrm{F">{\mathrm{F} 16}+\mathbf{A}</em>} 16} \Delta \mathbf{B<em _mathrm_F="\mathrm{F">{\mathrm{F} 16}\right) / 2^{11} \
&amp; +\left(\Delta \mathbf{A}</em>
\end{aligned}
$$} 16} \Delta \mathbf{B}_{\mathrm{F} 16}\right) / 2^{22</p>
<p>CUDA provides a data type called TF32 (Tensor Float) which has 8 bits of exponent and 10 bits of mantissa as the input type of Tensor Cores in Ampere architectures. Because the exponent length is the same as FP32, we can keep a wider exponent range compared to FP16. We use the TF32 instead of FP16 in Eqs. (8) and (9) and we refer to this method as tf32tf32. Currently, we can use RNA and RZ for rounding when converting FP32 to TF32. We use RNA because it keeps more mantissa compared to RZ, as we have shown in the Section Expectation of mantissa length. We show the comparison of representation accuracy and exponent range in Figure 9.</p>
<h1>Removing negligible error correction</h1>
<p>The absolute value of each element in $\Delta^{(2)}=\Delta \mathbf{A}<em _mathrm_F="\mathrm{F">{\mathrm{F} 16} \Delta \mathbf{B}</em>} 16}$ is at least $2^{22}$ times smaller than $\Delta^{(0)}=$ $\mathbf{A<em _mathrm_F="\mathrm{F">{\mathrm{F} 16} \mathbf{B}</em>$ element since the mantissa length of FP32 is 23 bit and the correction capability is negligible.} 16}$. When two floating-point values are added, the mantissa of the value with the smaller exponent is shifted to align the exponent of the two values. Therefore, when computing $\Delta^{(0)}+\Delta^{(2)}$, the shifting size is at least 22 bits in each element. This means that each $\Delta^{(2)}$ element at most affects the mantissa LSB in each $\Delta^{(0)</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: The comparison of representation accuracy and exponent range among FP32, FP16, TF32, halfhalf, tf32tf32, and Markidis' halfhalf.</p>
<p>Thus, we ignore this term and replace Eq. (23) with Eq. (24).</p>
<p>$$
\begin{aligned}
\dot{\mathbf{C}}<em _mathrm_F="\mathrm{F">{\mathrm{F} 32} \leftarrow &amp; \mathbf{A}</em>} 16} \mathbf{B<em _mathrm_F="\mathrm{F">{\mathrm{F} 16} \
&amp; +\left(\Delta \mathbf{A}</em>} 16} \mathbf{B<em _mathrm_F="\mathrm{F">{\mathrm{F} 16}+\mathbf{A}</em>
\end{aligned}
$$} 16} \Delta \mathbf{B}_{\mathrm{F} 16}\right) / 2^{11</p>
<p>Eq. (24) reduces the computation on Tensor Cores by $3 / 4$.</p>
<h1>Incorporating our method into CUTLASS</h1>
<p>NVIDIA CUTLASS ${ }^{5}$ is an open-source CUDA C++ matrix-matrix multiplication template library that has hierarchical memory blocking strategies and computing primitives. We use CUTLASS version 2.5 as a base implementation and include our techniques for: avoiding RZ after the addition of $\mathbf{C}_{\mathrm{F} 32}$ in Eq. (11), reducing the underflow and gradual underflow probabilities, while ignoring negligible correction terms. We develop two types of implementations: cutlass_halfhalf and cutlass_tf32tf32. We use the mma.sync.aligned.m16n8k8 PTX instruction, which computes matmul- $(16,8,8)$ and addition using FP16 Tensor Cores. We call this implementation cutlass_halfhalf. For using TF32 Tensor Cores, we use a PTX instruction which computes the same size of matrix-matrix multiplication and addition as the FP16 and we call this implementation cutlass_tf32tf32. We only add the code for the error correction and use the original CUTLASS code for other parts of the computation, such as loading matrix data from global memory to shared memory. Therefore, our implementation can compute any matrix-matrix multiplication size as long as CUTLASS supports it.</p>
<p>The avoiding of RZ is only applied to $\mathbf{A}<em _mathrm_F="\mathrm{F">{\mathrm{F} 16} \mathbf{B}</em>} 16}$ in Eq. (24) and it is not applied to $\Delta \mathbf{A<em _mathrm_F="\mathrm{F">{\mathrm{F} 16} \mathbf{B}</em>} 16}+$ $\mathbf{A<em _mathrm_F="\mathrm{F">{\mathrm{F} 16} \Delta \mathbf{B}</em>} 16}$. This allows us to reduce the required registers and computations. We show an example using FP16 Tensor Cores in Code 3. Furthermore, we show a simple example of a single-precision matrix-matrix multiplication using Markidis' and our methods. In this figure, we process $\left[\begin{array}{lll}\mathbf{A<em _mathrm_FP="\mathrm{FP">{\mathrm{FP} 32}^{(0)} &amp; \mathbf{A}</em>} 32}^{(1)}\end{array}\right]\left[\begin{array}{l}\mathbf{B<em _mathrm_FP="\mathrm{FP">{\mathrm{FP} 32}^{(0)} \ \mathbf{B}</em>\right]$ using FP16 Tensor Cores.} 32}^{(1)}\end{array</p>
<div class="codehilite"><pre><span></span><code>void matmul(...)
    fragment frag_a, frag_b, frag_c;
</code></pre></div>

<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: The comparison of Markidis' method (top) and our method (bottom) to carry out single-precision matrix-matrix multiplication $\left[\begin{array}{ll}\mathbf{A}<em _mathrm_FP="\mathrm{FP">{\mathrm{FP} 32}^{(0)} &amp; \mathbf{A}</em>} 32}^{(1)}\end{array}\right]\left[\begin{array}{l}\mathbf{B<em _mathrm_FP="\mathrm{FP">{\mathrm{FP} 32}^{(0)} \ \mathbf{B}</em>} 32}^{(1)}\end{array}\right]$. The matrices $\mathbf{A<em _mathrm_FP="\mathrm{FP">{\mathrm{FP} 16}^{(\cdot)}, \Delta \mathbf{A}</em>} 16}^{(\cdot)}, \mathbf{B<em _mathrm_FP="\mathrm{FP">{\mathrm{FP} 16}^{(\cdot)}, \Delta \mathbf{B}</em>$ in Markidis' method are calculated by Eqs. (2)-(5), and in our method by Eqs. (19)-(22).} 16}^{(\cdot)</p>
<div class="codehilite"><pre><span></span><code><span class="n">fragment</span><span class="w"> </span><span class="n">frag_da</span><span class="p">,</span><span class="w"> </span><span class="n">frag_db</span><span class="p">,</span><span class="w"> </span><span class="n">frag_dc</span><span class="p">;</span>
<span class="n">fragment</span><span class="w"> </span><span class="n">frag_tmp</span><span class="p">;</span>
<span class="n">shared_mem_fp16</span><span class="w"> </span><span class="n">smem_a</span><span class="p">,</span><span class="w"> </span><span class="n">smem_b</span><span class="p">;</span>
<span class="n">shared_mem_fp16</span><span class="w"> </span><span class="n">smem_da</span><span class="p">,</span><span class="w"> </span><span class="n">smem_db</span><span class="p">;</span>
<span class="o">//</span><span class="w"> </span><span class="k">Initialize</span><span class="w"> </span><span class="n">accumulator</span><span class="w"> </span><span class="n">fragments</span>
<span class="n">fill_fragment</span><span class="p">(</span><span class="n">frag_c</span><span class="p">,</span><span class="mf">0.</span><span class="n">f</span><span class="p">);</span>
<span class="n">fill_fragment</span><span class="p">(</span><span class="n">frag_dc</span><span class="p">,</span><span class="mf">0.</span><span class="n">f</span><span class="p">);</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="n">k</span><span class="o">&lt;</span><span class="n">K</span><span class="p">;</span><span class="n">k</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="err">{</span>
<span class="w">    </span><span class="o">//</span><span class="w"> </span><span class="k">Compute</span><span class="w"> </span><span class="n">eq</span><span class="w"> </span><span class="p">(</span><span class="mi">19</span><span class="p">),</span><span class="w"> </span><span class="p">(</span><span class="mi">21</span><span class="p">)</span>
<span class="w">    </span><span class="n">smem_a</span><span class="o">=</span><span class="n">toFP16</span><span class="p">(</span><span class="n">mem_a</span><span class="o">[</span><span class="n">k</span><span class="o">]</span><span class="p">)</span>
<span class="w">    </span><span class="n">smem_b</span><span class="o">=</span><span class="n">toFP16</span><span class="p">(</span><span class="n">mem_b</span><span class="o">[</span><span class="n">k</span><span class="o">]</span><span class="p">)</span>
<span class="w">    </span><span class="n">load_matrix_sync</span><span class="p">(</span><span class="n">frag_a</span><span class="w"> </span><span class="p">,</span><span class="n">smem_a</span><span class="p">);</span>
<span class="w">    </span><span class="n">load_matrix_sync</span><span class="p">(</span><span class="n">frag_b</span><span class="w"> </span><span class="p">,</span><span class="n">smem_b</span><span class="p">);</span>
<span class="w">    </span><span class="o">//</span><span class="w"> </span><span class="k">Compute</span><span class="w"> </span><span class="n">eq</span><span class="w"> </span><span class="p">(</span><span class="mi">20</span><span class="p">),</span><span class="w"> </span><span class="p">(</span><span class="mi">22</span><span class="p">)</span>
<span class="w">    </span><span class="n">smem_da</span><span class="o">=</span><span class="n">toFP16</span><span class="p">((</span><span class="n">mem_a</span><span class="o">[</span><span class="n">k</span><span class="o">]-</span><span class="n">toFP32</span><span class="p">(</span><span class="n">smem_a</span><span class="p">)</span><span class="o">*</span><span class="mi">2048</span><span class="p">)</span>
<span class="w">    </span><span class="n">smem_db</span><span class="o">=</span><span class="n">toFP16</span><span class="p">((</span><span class="n">mem_b</span><span class="o">[</span><span class="n">k</span><span class="o">]-</span><span class="n">toFP32</span><span class="p">(</span><span class="n">smem_b</span><span class="p">)</span><span class="o">*</span><span class="mi">2048</span><span class="p">)</span>
<span class="w">    </span><span class="n">load_matrix_sync</span><span class="p">(</span><span class="n">frag_da</span><span class="w"> </span><span class="p">,</span><span class="n">smem_da</span><span class="p">);</span>
<span class="w">    </span><span class="n">load_matrix_sync</span><span class="p">(</span><span class="n">frag_db</span><span class="w"> </span><span class="p">,</span><span class="n">smem_db</span><span class="p">);</span>
<span class="w">    </span><span class="o">//</span><span class="w"> </span><span class="k">Compute</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">part</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">eq</span><span class="w"> </span><span class="p">(</span><span class="mi">24</span><span class="p">)</span>
<span class="w">    </span><span class="n">mma_sync</span><span class="p">(</span><span class="n">frag_dc</span><span class="w"> </span><span class="p">,</span><span class="n">frag_da</span><span class="w"> </span><span class="p">,</span><span class="n">frag_b</span><span class="w"> </span><span class="p">,</span><span class="n">frag_dc</span><span class="p">);</span>
<span class="w">    </span><span class="n">mma_sync</span><span class="p">(</span><span class="n">frag_dc</span><span class="w"> </span><span class="p">,</span><span class="n">frag_a</span><span class="w"> </span><span class="p">,</span><span class="n">frag_db</span><span class="w"> </span><span class="p">,</span><span class="n">frag_dc</span><span class="p">);</span>
<span class="w">    </span><span class="o">//</span><span class="w"> </span><span class="k">Initialize</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">temporal</span><span class="w"> </span><span class="n">accumulator</span><span class="w"> </span><span class="n">fragment</span>
<span class="w">    </span><span class="n">fill_fragment</span><span class="p">(</span><span class="n">frag_tmp</span><span class="p">,</span><span class="mf">0.</span><span class="n">f</span><span class="p">);</span>
<span class="w">    </span><span class="n">mma_sync</span><span class="p">(</span><span class="n">frag_tmp</span><span class="p">,</span><span class="w"> </span><span class="n">frag_a</span><span class="w"> </span><span class="p">,</span><span class="n">frag_b</span><span class="w"> </span><span class="p">,</span><span class="n">frag_tmp</span><span class="p">);</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="n">i</span><span class="o">&lt;</span><span class="n">frag_c</span><span class="p">.</span><span class="n">num_elements</span><span class="p">;</span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="err">{</span>
<span class="w">        </span><span class="o">//</span><span class="w"> </span><span class="n">Accumulation</span><span class="w"> </span><span class="k">using</span><span class="w"> </span><span class="n">FP32</span><span class="w"> </span><span class="n">SIMT</span><span class="w"> </span><span class="n">Core</span>
<span class="w">        </span><span class="n">frag_c</span><span class="p">.</span><span class="n">x</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">frag_tmp</span><span class="p">.</span><span class="n">x</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="p">;</span>
<span class="w">    </span><span class="err">}</span>
<span class="err">}</span>
<span class="o">//</span><span class="w"> </span><span class="k">Compute</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">part</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">eq</span><span class="w"> </span><span class="p">(</span><span class="mi">24</span><span class="p">)</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="n">i</span><span class="o">&lt;</span><span class="n">frag_c</span><span class="p">.</span><span class="n">num_elements</span><span class="p">;</span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="err">{</span>
<span class="w">    </span><span class="n">frag_c</span><span class="p">.</span><span class="n">x</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">frag_dc</span><span class="p">.</span><span class="n">x</span><span class="o">[</span><span class="n">i</span><span class="o">]/</span><span class="mi">2048</span><span class="p">;</span>
<span class="err">}</span>
<span class="o">//</span><span class="w"> </span><span class="n">Store</span><span class="w"> </span><span class="k">result</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">memory</span>
<span class="n">store_matrix_sync</span><span class="p">(</span><span class="n">mem_c</span><span class="p">,</span><span class="n">frag_c</span><span class="p">);</span>
</code></pre></div>

<p>Code 3: Pseudocode including our improvements for computing single-precision matrix-matrix multiplication with error correction using FP16 Tensor Cores.</p>
<p>In our actual implementation, which is more complex than the simple example shown in Code 3, we don't store $\mathbf{A}<em _mathrm_F="\mathrm{F">{\mathrm{F} 16}, \mathbf{B}</em>} 16}$ and $\Delta \mathbf{A<em _mathrm_F="\mathrm{F">{\mathrm{F} 16}, \Delta \mathbf{B}</em>} 16}$ explicitly to the shared memory in order to reduce the memory footprint. Instead, we load $\mathbf{A<em _mathrm_F="\mathrm{F">{\mathrm{F} 32}, \mathbf{B}</em>$ from the shared memory, compute Eq. (19)-(22) on the registers, and store them to the fragments directly.} 32</p>
<h1>Parameter tuning of CUTLASS</h1>
<p>The CUTLASS library has some template parameters to determine the blocking size of each memory layer, the number of software pipeline stages, etc. We searched the parameters for achieving the highest throughput for each input matrix size using a grid search. We used Weights\&amp;Biases ${ }^{6}$ sweeps for searching the optimal parameters efficiently. We show the parameter search space in Table 3. The total number of all parameter combinations is 3,456 and we filter them with the following set of rules.</p>
<ul>
<li>At least, one of $\mathrm{wm}&gt;\mathrm{bm}, \mathrm{wn}&gt;\mathrm{bn}$, and $\mathrm{wk}&gt;\mathrm{bk}$ is satisfied. This is because the size of thread-block level blocking must be larger than the warp level.</li>
</ul>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Values</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>bm, bn, bk</td>
<td>$16,32,64,128$, for respectively</td>
<td>The size of thread block level blocking. Each thread block <br> computes matmul-(bm, bn, bk).</td>
</tr>
<tr>
<td>wm, wn, wk</td>
<td>$16,32,64,128$, for respectively</td>
<td>The size of warp level blocking. Each warp computes <br> matmul-(wm, wn, wk).</td>
</tr>
<tr>
<td>stages</td>
<td>3,4</td>
<td>The number of software pipelines.</td>
</tr>
</tbody>
</table>
<p>Table 3: The parameter space for grid search when optimizing the computing performance of CUTLASS.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Implementation</th>
<th style="text-align: left;">TensorCore</th>
<th style="text-align: center;">Error Correction</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">cutlass_tf32tf32</td>
<td style="text-align: left;">TF32-TC</td>
<td style="text-align: center;">YES</td>
</tr>
<tr>
<td style="text-align: left;">cutlass_fp16fp16</td>
<td style="text-align: left;">FP16-TC</td>
<td style="text-align: center;">YES</td>
</tr>
<tr>
<td style="text-align: left;">cublas_tf32tc</td>
<td style="text-align: left;">TF32-TC</td>
<td style="text-align: center;">NO</td>
</tr>
<tr>
<td style="text-align: left;">cublas_fp16tc</td>
<td style="text-align: left;">FP16-TC</td>
<td style="text-align: center;">NO</td>
</tr>
<tr>
<td style="text-align: left;">cublas_simt(FP32)</td>
<td style="text-align: left;">Not used</td>
<td style="text-align: center;">NO</td>
</tr>
</tbody>
</table>
<p>Table 4: The list of implementations we use to evaluate our proposed methods. The implementations named cutlass_XX are our implementation, and cublas_XX are reference implementations.</p>
<ul>
<li>The size of the shared memory required exceeds its capacity.</li>
<li>The error calculated by Eq. (7) is larger than the threshold (even if the compilation is successful). At this point, we set 0.1 as the threshold and we checked experimentally that this value holds for all cases.</li>
</ul>
<p>Through this automatic filtering process, we were able to reduce the number of parameter combinations for cutlass_halfhalf to 202, and for cutlass_tf32tf to 200.</p>
<h1>Experiment details</h1>
<p>We compare the accuracy, throughput, and power consumption of our implementations. The list of implementations we used are summarized in Table 4.</p>
<h2>Accuracy evaluation</h2>
<p>We input single-precision matrices for each implementation and calculate the error following Eq. (7). We compute matrix-matrix multiplication 8 times with different random seeds and average the error of each of them. The order of addition is changed by the template parameters of CUTLASS, which slightly affects the error. We use the worst values in the grid search as the error. We use NVIDIA A100 40GB SXM4 and CUDA version 11.3.</p>
<p>In this section, we evaluate the effect of the exponent range and its pattern.</p>
<h2>Effect of the exponent range of input matrices</h2>
<p>As we mentioned in the previous section, the exponent range of halfhalf is narrower than FP32 as shown in Figure 9. To evaluate this effect on the matrix-matrix multiplication accuracy, we input the matrices with</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: The effect of the exponent range on the accuracy of matrix-matrix multiplication $\mathbf{A}<em _mathrm_F="\mathrm{F">{\mathrm{F} 32} \times$ $\mathbf{B}</em>} 32}$.Type 1: All elements in both $\mathbf{A<em _mathrm_F="\mathrm{F">{\mathrm{F} 32}$ and $\mathbf{B}</em>} 32}$ are represented with high precision in halfhalf.Type 2: All elements in either $\mathbf{A<em _mathrm_F="\mathrm{F">{\mathrm{F} 32}$ or $\mathbf{B}</em>} 32}$ are represented with high precision, while the others are lower precision for smaller values in halfhalf.Type 3: All elements of both $\mathbf{A<em _mathrm_F="\mathrm{F">{\mathrm{F} 32}$ and $\mathbf{B}</em>} 32}$ are lower precision for smaller values in halfhalf.Type 4: At least one of $\mathbf{A<em _mathrm_F="\mathrm{F">{\mathrm{F} 32}$ or $\mathbf{B}</em>$ is all zero in halfhalf because they are out-of-range.
various exponent ranges and evaluate the accuracy. We define a single-precision matrix set $\exp _$rand $(a, b)$ $(a, b \in \mathbb{Z})$ so that the exponent of each element of a matrix in it is generated from a uniform distribution $[a, b]$ and the mantissa is generated from a uniform distribution $\left[0,2^{23}-1\right]$.} 32</p>
<p>$$
\begin{aligned}
e &amp; \leftarrow \text { UniformRandInt }[a, b] \
m &amp; \leftarrow \text { UniformRandFP32[1,2) } \
s &amp; \leftarrow \text { UniformRandInt }[0,1] \
(i, j) \text {-element } &amp; \leftarrow(2 s-1) \times 2^{e} \times m
\end{aligned}
$$</p>
<p>In this evaluation, we use three types of input matrices as follows:
$\exp _$rand $(-15,14)$
All elements are in range of $\left(10^{-5}, 10^{5}\right)$, and represented by our halfhalf with high precision as shown in Figure 9.
$\exp _$rand $(-35,-15)$
All elements are in range of $\left(10^{-11}, 10^{-4}\right)$, and represented by our halfhalf with lower precision for smaller values as shown in Figure 9.
$\exp _$rand $(-100,-35)$
All elements are in range of $\left(10^{-31}, 10^{-10}\right)$. The halfhalf can't represent this range of numbers.
We initialize the input matrices $\mathbf{A}<em _mathrm_FP="\mathrm{FP">{\mathrm{FP} 32}$ and $\mathbf{B}</em>} 32}$ with the above three patterns, respectively, and compute the matrix-matrix multiplication $\mathbf{A<em _mathrm_FP="\mathrm{FP">{\mathrm{FP} 32} \mathbf{B}</em>$. We show the accuracy for the following combinations:} 32</p>
<h1>Type 1</h1>
<p>Both $\mathbf{A}<em _mathrm_FP="\mathrm{FP">{\mathrm{FP} 32}$ and $\mathbf{B}</em>$ are $\exp _$rand $(-15,14)$.} 32</p>
<h2>Type 2</h2>
<p>One of $\mathbf{A}<em _mathrm_FP="\mathrm{FP">{\mathrm{FP} 32}$ or $\mathbf{B}</em>$ is $\exp _$rand $(-15,14)$ and the other one is $\exp _$rand $(-100,-35)$.} 32</p>
<h2>Type 3</h2>
<p>Both $\mathbf{A}<em _mathrm_FP="\mathrm{FP">{\mathrm{FP} 32}$ and $\mathbf{B}</em>$ are $\exp _$rand $(-35,-15)$.} 32</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12: Visualization of the exponent pattern of the input matrices $\mathbf{A}<em _mathrm_FP="\mathrm{FP">{\mathrm{FP} 32}, \mathbf{B}</em>} 32}$. We use randtlr, spatial, cauchy as $\mathbf{B<em _mathrm_FP="\mathrm{FP">{\mathrm{FP} 32}$, and urand $(0,1)$, exp_rand $(-15,0)$ as $\mathbf{A}</em>$.
} 32<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 13: The matrix-matrix multiplication accuracy evaluation using input matrices generated by STARS-H.</p>
<h1>Type 4</h1>
<p>At least one of $\mathbf{A}<em _mathrm_FP="\mathrm{FP">{\mathrm{FP} 32}$ or $\mathbf{B}</em>$ is exp_rand $(-100,-35)$.
The outcome of this experiment is shown in Figure 11. We can see that cutlass_tf32tf32 computes matrixmatrix accuracy with the same accuracy as FP32 SIMT in all cases. On the other hand, although cutlass_halfhalf computes in the same accuracy with FP32 SIMT in case Type 1, the loss of accuracy occurs in Type 2 and 3, and cutlass_halfhalf is not able to perform in case Type 4. Therefore, if all elements in the matrix have very small exponents, we need to carry out additional scaling before matrix-matrix multiplication is performed.} 32</p>
<h2>Effect of exponent patterns of the input matrices</h2>
<p>In real-world computations, matrices have various exponent patterns. STARS- $\mathrm{H}^{7}$ was originally developed for the evaluation of hierarchical low-rank approximation methods, and covers various dense matrix patterns in real applications. We generate three types of input matrix $\mathbf{A}_{\mathrm{F} 32}$ using STARS-H as follows:</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 14: The evaluation of computational throughput on NVIDIA A100, RTX A6000, and GeForce RTX 3090 for matmul- $(m, n, k)$.The solid lines compute single-precision matrix-matrix multiplication in FP32 accuracy (high accuracy) while the dashed lines are for FP16 accuracy (low accuracy).</p>
<h1>randtlr</h1>
<p>Random synthetic TLR (Tile Low-Rank) matrix.</p>
<h2>spatial</h2>
<p>Exponential kernel for spatial statistics.</p>
<h2>cauchy</h2>
<p>Cauchy matrix.
And as input matrix $\mathbf{B}<em _mathrm_F="\mathrm{F">{\mathrm{F} 32}$, we use two types of input matrix as follows:
$\operatorname{urand}(-1,1)$
Random matrix from a uniform distribution $(-1,1)$.
$\exp _$rand $(-15,0)$
Random matrix generated by Eq. (25).
We show a sample of exponent patterns of these matrices in Figure 12 and the accuracy of matrix-matrix multiplication $\mathbf{A}</em>$ in Figure 13.} 32} \mathbf{B}_{\mathrm{F} 16</p>
<p>Although the accuracy of cutlass_halfhalf and cutlass_tf32tf32 are better than cublas_simt in some matrix sizes, this likely due to the order of addition, and not because of our error correction method. Thus, we conclude that the accuracy of cutlass_halfhalf and cutlass_tf32tf32 are the same as cuBLAS SGEMM, for various patterns of exponent values in the matrix.</p>
<h2>Performance evaluation</h2>
<p>We calculate the Flop/s for matmul- $(m, m, m)$ by dividing $2 m^{3}$ with the computing time $s[\mathrm{sec}]$. The performance on NVIDIA A100, RTX A6000, and GeForce RTX 3090 is shown in Figure 14. The specifications of the host machine: for each GPU are:</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ https://github.com/ecrc/stars-h&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
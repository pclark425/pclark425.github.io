<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2742 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2742</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2742</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-70.html">extraction-schema-70</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <p><strong>Paper ID:</strong> paper-245219242</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2112.08907v3.pdf" target="_blank">Inherently Explainable Reinforcement Learning in Natural Language</a></p>
                <p><strong>Paper Abstract:</strong> We focus on the task of creating a reinforcement learning agent that is inherently explainable -- with the ability to produce immediate local explanations by thinking out loud while performing a task and analyzing entire trajectories post-hoc to produce causal explanations. This Hierarchically Explainable Reinforcement Learning agent (HEX-RL), operates in Interactive Fictions, text-based game environments in which an agent perceives and acts upon the world using textual natural language. These games are usually structured as puzzles or quests with long-term dependencies in which an agent must complete a sequence of actions to succeed -- providing ideal environments in which to test an agent's ability to explain its actions. Our agent is designed to treat explainability as a first-class citizen, using an extracted symbolic knowledge graph-based state representation coupled with a Hierarchical Graph Attention mechanism that points to the facts in the internal graph representation that most influenced the choice of actions. Experiments show that this agent provides significantly improved explanations over strong baselines, as rated by human participants generally unfamiliar with the environment, while also matching state-of-the-art task performance.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2742.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2742.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HEX-RL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hierarchically Explainable Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An inherently explainable RL agent for text-based interactive fiction that maintains an ever-updating symbolic knowledge-graph (KG) belief state, encodes it with GATs and a hierarchical graph-attention mechanism over sub-graphs, and produces both immediate (stepwise) and temporally-extended post-hoc natural language explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>HEX-RL</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Reinforcement learning agent using an ALBERT-based QA module to extract RDF triples into an ever-updating knowledge graph split into 4 semantically-motivated sub-graphs; encodes text observations with GRUs, encodes KG and sub-KGs with Graph Attention Networks and a Hierarchical Graph Attention module, and uses an A2C policy with a graph-masked decoder to produce template-based language actions and explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Jericho (interactive fiction / text-adventure benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Partially-observable text-based adventure games (Interactive Fiction) with long-horizon puzzles, many locations/objects and sparse delayed rewards requiring multi-step dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>graph-based memory (episodic/accumulated knowledge graph)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>RDF-triple graph (subject, relation, object) that is accumulated since game start and partitioned into four sub-graphs (attributes, inventory, objects-in-room, location), encoded by GATs and attended by a hierarchical attention mechanism; also stores saved trajectories (rollouts) for post-hoc analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Extracted triples about objects, attributes, locations, player inventory and relations (e.g., 'egg is interactable', 'object in location', 'player has object'), plus recorded trajectory states, actions, critic values and scores for many rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Ever-updating KG containing all extracted triples since the start of an episode (no explicit fixed capacity specified); additionally hundreds of saved test-time trajectories are stored for post-hoc analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Graph Attention (GAT) + Hierarchical Graph Attention over sub-graphs for immediate retrieval/attribution; post-hoc retrieval via a Bayesian State Filter over saved trajectories (counts/conditional P(A|B_i)), filtered further by a GPT-2 action language model (CALM) and a semantic-state-action filter.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>KG is updated at every timestep by combining candidate vertices/relations produced by an ALBERT QA module (answering environment questions) with the previous KG G_{t-1} to form G_t; KG expansion can also be rewarded as intrinsic motivation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Maintain explicit world belief for decision making, constrain action decoding (graph mask), provide fine-grained attributions for immediate natural-language explanations, and support post-hoc temporal explanation generation by analyzing stored trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported to match or exceed contemporary KG-based state-of-the-art on the Jericho testbed: HEX-RL achieved comparable asymptotic and maximum scores to other strong KG agents and improved maximum training scores vs. baselines; when trained with intrinsic motivation (reward for KG expansion) performance improved further. (Quantitative per-game asymptotic and max scores are reported in Table 2 of the paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Hierarchical KG + attention yields more coherent and trustworthy immediate explanations (preferred in human studies) and supports temporal explanations; intrinsic-motivation on KG expansion helps performance. Effectiveness depends on quality of KG extraction and on the ability of the Bayesian trajectory filter to find causal chains.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Effectiveness limited by (1) errors in KG extraction (ALBERT QA and candidate selection) and by cases where the true explanation is not expressible via the extracted fact types; (2) temporal explanation quality degrades when the game's branching factor is high (many alternative paths), causing the Bayesian State Filter to fail to identify essential steps; templated KG-to-language conversions can be less human-like than raw substrings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td>The full system: ALBERT-QA to build an ever-updating RDF KG split into four sub-graphs, GAT encoding with Hierarchical Graph Attention, saved trajectory buffer for many rollouts, followed by Bayes + CALM (GPT-2 action LM) + semantic filtering for temporal explanations; this configuration produced the best explanation quality and competitive task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Inherently Explainable Reinforcement Learning in Natural Language', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2742.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2742.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Knowledge Graph (KG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RDF Knowledge Graph state representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A symbolic, ever-updating graph of RDF triples (subject, relation, object) extracted from textual observations that serves as the agent's belief state and memory of the world during gameplay.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Knowledge Graph (state memory)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Not an agent — a memory/data-structure built from QA outputs: stores triples representing attributes, inventory, objects-in-room and location relations, partitioned into four sub-graphs and encoded by GATs for downstream policy and explanation generation.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Jericho (interactive fiction benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Long-horizon text-adventure puzzles where agents must discover and use objects across many locations.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>graph-based memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>RDF triple graph partitioned into sub-graphs (G_attr, G_inv, G_obj, G_loc); used as a structured symbolic store and input to GATs and attention modules.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Facts about objects, attributes, object locations, location-to-location relations, and inventory items (the union of extracted V_t and R_t across timesteps).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Accumulates facts across the whole episode; no hard limit specified in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Node-level attention via Graph Attention Networks and Hierarchical Graph Attention yields top-k triples used for immediate explanations; used as mask for action decoder to constrain valid entities.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Updated each timestep by merging new candidate vertices/relations (from ALBERT-QA answers) with the previous graph G_{t-1} to produce G_t.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>State tracking, constraining the action decoder, powering immediate natural-language explanations by pointing to attended triples, and providing the data for post-hoc trajectory analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Agents using explicit KGs (including HEX-RL, Q*BERT, SHA-KG, KG-A2C) achieve better or comparable task performance to non-KG baselines on Jericho and provide higher-quality explanations; HEX-RL reported improved maximum training scores and competitive asymptotic performance when using KG-based memory.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>KGs provide explainability benefits (more coherent immediate explanations) and can improve exploration when coupled with intrinsic rewards for KG expansion; KG extraction quality critically affects usefulness.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Dependent on extraction accuracy; limited expressivity if relevant explanatory facts are not captured by the chosen triple types; growth and branching in trajectories complicate post-hoc causal inference.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td>A structured KG split into sub-graphs with hierarchical attention (as in HEX-RL) combined with trajectory-level filters (Bayes + CALM + semantic) yields the best explanation quality and strong task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Inherently Explainable Reinforcement Learning in Natural Language', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2742.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2742.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bayesian State Filter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian State Filter for trajectory causal dependency estimation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A post-hoc analysis module that computes conditional probabilities P(A | B_i) from many saved trajectories (counts and co-occurrence/order statistics) to identify state-action pairs that are likely causal dependencies for reaching a goal, used to select important states for temporal explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Bayesian State Filter</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A statistical filter trained on stored rollouts that computes conditional probabilities based on counts and co-occurrence of (state,action,KG) steps across trajectories to select candidate predecessor states that are necessary for later steps/goal achievement.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Jericho</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Interactive fiction games where trajectories are saved and analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>trajectory-based frequency/conditional statistics (analytical memory over saved rollouts)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Counts and co-occurrence statistics derived from a buffer of hundreds of test-time rollouts; used to compute P(A | B_i) via raw counts and co-occurrence counts.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Saved game steps consisting of observation o_t, action a_t and knowledge graph G_t, plus value/score metadata across many rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Hundreds of saved test-time trajectories (paper notes larger numbers of trajectories improve Bayes filter accuracy); no strict upper bound given.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Works by searching backward from final goal states and selecting game steps with P(A|B_i) > threshold p (count-based conditional probability).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Trained/compiled once from many saved trajectories collected under the trained policy (post-training analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Identify chains of likely causal dependencies across states to form temporally-extended explanations (which intermediate, unrewarded actions were necessary).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Improves temporal explanation quality when combined with subsequent CALM and semantic filters; ablations show Bayes+CALM outperforms Bayes alone on human-judged confidence and understandability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Effective at finding dependency chains in less-branching games; fails or degrades when the game's branching factor is high and many alternative paths lead to the same goal.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Sensitive to dataset size (needs many trajectories), fails under high branching/alternative path complexity, and can select spurious dependencies if co-occurrence statistics are noisy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td>Used in tandem with CALM (GPT-2 action LM) and semantic filtering to achieve best temporal explanation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Inherently Explainable Reinforcement Learning in Natural Language', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2742.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2742.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CALM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contextual Action Language Model (CALM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-2 language model trained on human play-through transcripts to predict likely next actions given observation-action-next-observation contexts, used as an action filter to prefer human-like causal links when selecting important states for temporal explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Keep CALM and explore: Language models for action generation in text-based games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CALM (GPT-2 action LM)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A GPT-2-based conditional action generator trained on human transcripts that, given (o_t, a_t, o_{t+1}) prompts, produces candidate next actions; in HEX-RL it is used as a filter to keep those B_i states for which CALM predicts a_{B_i} given (o_A,a_A,o_{B_i}).</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Jericho (human play-through transcripts used for training CALM)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Text-adventure environments (IF) where humans produce transcripts used to train action LM.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Not used as an episodic memory store; used to predict and filter candidate causal steps that humans would take, improving naturalness and human-likeness of selected temporal explanation steps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>When applied as a filter (Bayes+CALM) human evaluators preferred resulting temporal explanations over Bayes alone on 'Confidence' and 'Understandability' metrics (statistically significant improvements).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>CALM improves selection of human-plausible intermediate steps for temporal explanations and increases perceived understandability and confidence when combined with Bayes filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>CALM is a predictive LM and does not itself provide causal grounding; it can only filter candidate states where predicted actions match actual actions and relies on having suitable human training transcripts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td>Used as the middle step in the three-stage temporal-filter pipeline (Bayes → CALM → Semantic) to yield better temporal explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Inherently Explainable Reinforcement Learning in Natural Language', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2742.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2742.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ALBERT-QA module</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ALBERT-based Question Answering module (for KG construction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ALBERT QA model fine-tuned on JerichoQA to answer environment questions (e.g., 'What am I carrying?') producing candidate vertices and relations used to construct/update the knowledge graph.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ALBERT-QA (KG extractor)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An ALBERT-based question-answering network fine-tuned to produce candidate graph vertices V_t and relations R_t from textual observations; used at every timestep to propose facts that are merged into the KG.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Jericho (JerichoQA dataset used to fine-tune ALBERT)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Text-adventure games where QA questions link text observations to KG triples.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Produces candidate facts to update the agent's KG memory each timestep.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Quality of the KG (and thus explanations and downstream decisions) depends critically on ALBERT-QA accuracy; failures in immediate explanations are often attributable to KG extraction errors from this module.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Accurate, well-trained KG extraction is necessary for effective KG-based memory; extraction errors are a primary limit on explanation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>QA extraction errors and coverage limits of the chosen question set constrain what facts are ever stored in KG, reducing explanation/decision quality when relevant facts are missed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td>Fine-tune ALBERT on JerichoQA and merge V_t/R_t outputs with previous KG each step; this was used in HEX-RL and compared favorably to rule-based extraction in some baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Inherently Explainable Reinforcement Learning in Natural Language', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2742.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2742.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Q*BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Q*BERT (KG-building RL agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior RL agent that constructs a knowledge graph by answering questions (QA) and uses that KG as part of its policy; cited and used as a strong baseline in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Q*BERT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Baseline agent (Ammanabrolu et al., 2020) that builds a KG by QA and uses it for RL; unlike HEX-RL it does not use relation information in the policy and does not employ hierarchical graph attention for fine-grained triple-level attribution.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Jericho</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Interactive fiction games used as evaluation; Q*BERT served as a comparative KG-based agent.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>knowledge-graph-based memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>KG built by QA; details differ from HEX-RL (no hierarchical sub-graph attention as described).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Extracted triples from text via QA.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Accumulates facts across episode; unspecified.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Not reported to use relation-level attention; SHA-KG/HEX-RL claim more fine-grained attribution.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>KG updated via QA outputs across timesteps.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>State representation for RL and decision making.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Q*BERT is a strong baseline; HEX-RL with intrinsic motivation outperforms Q*BERT on at least one game, is outperformed on two games, and is comparable on the remaining six — overall HEX-RL is comparable to Q*BERT in asymptotic performance (see Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Using QA to build KG can lead to strong task performance, but lacking relation-aware hierarchical attention reduces the ability to provide fine-grained, triple-level explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Does not exploit edge/relationship information in policy/value networks for fine-grained attribution (per paper's critique).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Inherently Explainable Reinforcement Learning in Natural Language', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2742.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2742.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SHA-KG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stacked Hierarchical Attention over Knowledge Graph (SHA-KG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior KG-based agent that constructs a KG (rules-based) and feeds it into a hierarchical attention network; used as a baseline but criticized for averaging attention across large portions of the graph and not using relation information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep reinforcement learning with stacked hierarchical attention for text-based games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SHA-KG</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Baseline that uses rules-based KG construction and a stacked hierarchical graph-attention architecture; in the paper SHA-KG is noted to average attention across large graph regions and not point to specific KG triples for explanation.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Jericho</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Text-adventure games used for baseline comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>knowledge-graph-based memory (rules-based extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>KG partitioned (rules-based) and encoded via hierarchical attention, but attention aggregation is coarse.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Triples extracted by rules/OpenIE over text observations.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Accumulates across episode; unspecified.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Hierarchical attention but averaged across large graph portions (coarser attribution than HEX-RL).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Rules-based triple extraction merged into KG each step.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>State representation for RL policy/value estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Included as a strong baseline; HEX-RL's hierarchical attention that uses relations is argued to provide finer-grained explanations than SHA-KG though task performance comparisons are provided in Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Coarse averaging of attention reduces ability to attribute specific triples to actions; fine-grained relation-aware attention (HEX-RL) is preferable for explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Averaging attention over large subgraphs limits triplet-level attribution and explanation precision.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Inherently Explainable Reinforcement Learning in Natural Language', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2742.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2742.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KG-A2C (rule-based KG with A2C)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline A2C agent that uses a rules-based OpenIE pipeline to extract KG triplets from observations and uses those as the state representation for policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Baseline that constructs a KG using a rules-based OpenIE extractor and feeds the KG as state into an A2C policy; serves as a comparative KG-using baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Jericho</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Interactive fiction games used as baselines for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>knowledge-graph-based memory (rules-based extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>RDF-style triples extracted via OpenIE and merged into a KG.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Objects, relations and attributes extracted by OpenIE.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Accumulates across episode; unspecified.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Not specified to use hierarchical attention as in HEX-RL.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Updated via rule-based extraction each timestep.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>State representation for RL.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Serves as baseline; HEX-RL reported improved maximum scores versus these baselines and comparable asymptotic performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Rule-based KG extraction can be effective for RL state representation, but lacks the relation-aware hierarchical attention and QA-based extraction used by HEX-RL that improve explanation fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Quality depends on rule-based extractor; may miss or mis-paraphrase relations that QA-based KG construction can capture.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Inherently Explainable Reinforcement Learning in Natural Language', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2742.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2742.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSTM-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LSTM-A2C (text-only baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline agent that uses only raw textual observations encoded by an LSTM (no KG memory) with an A2C policy network; used to compare the value of KG memory for explanation and performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LSTM-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A2C agent that encodes natural language observations using an LSTM and derives policy/value outputs without any explicit KG or graph memory.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Jericho</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Interactive fiction games used to evaluate baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>N/A (no explicit memory beyond recurrent hidden state).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>As a non-KG baseline, LSTM-A2C generally underperforms or is comparable to KG-enabled agents in task performance in the Jericho games and produces inferior immediate explanations (human evaluators preferred KG-based explanations over LSTM attention substrings).</td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Without an explicit KG memory, immediate explanation extraction used LSTM attention to extract substrings which can sound more human-like but are often incoherent; KG memory produced more coherent and confidence-inspiring explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Inherently Explainable Reinforcement Learning in Natural Language', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Graph constrained reinforcement learning for natural language action spaces <em>(Rating: 2)</em></li>
                <li>Keep CALM and explore: Language models for action generation in text-based games <em>(Rating: 2)</em></li>
                <li>Deep reinforcement learning with stacked hierarchical attention for text-based games <em>(Rating: 2)</em></li>
                <li>Q*BERT <em>(Rating: 1)</em></li>
                <li>Textworld: A learning environment for text-based games <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2742",
    "paper_id": "paper-245219242",
    "extraction_schema_id": "extraction-schema-70",
    "extracted_data": [
        {
            "name_short": "HEX-RL",
            "name_full": "Hierarchically Explainable Reinforcement Learning",
            "brief_description": "An inherently explainable RL agent for text-based interactive fiction that maintains an ever-updating symbolic knowledge-graph (KG) belief state, encodes it with GATs and a hierarchical graph-attention mechanism over sub-graphs, and produces both immediate (stepwise) and temporally-extended post-hoc natural language explanations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "HEX-RL",
            "agent_description": "Reinforcement learning agent using an ALBERT-based QA module to extract RDF triples into an ever-updating knowledge graph split into 4 semantically-motivated sub-graphs; encodes text observations with GRUs, encodes KG and sub-KGs with Graph Attention Networks and a Hierarchical Graph Attention module, and uses an A2C policy with a graph-masked decoder to produce template-based language actions and explanations.",
            "base_model_size": null,
            "game_benchmark_name": "Jericho (interactive fiction / text-adventure benchmark)",
            "game_description": "Partially-observable text-based adventure games (Interactive Fiction) with long-horizon puzzles, many locations/objects and sparse delayed rewards requiring multi-step dependencies.",
            "uses_memory": true,
            "memory_type": "graph-based memory (episodic/accumulated knowledge graph)",
            "memory_structure": "RDF-triple graph (subject, relation, object) that is accumulated since game start and partitioned into four sub-graphs (attributes, inventory, objects-in-room, location), encoded by GATs and attended by a hierarchical attention mechanism; also stores saved trajectories (rollouts) for post-hoc analysis.",
            "memory_content": "Extracted triples about objects, attributes, locations, player inventory and relations (e.g., 'egg is interactable', 'object in location', 'player has object'), plus recorded trajectory states, actions, critic values and scores for many rollouts.",
            "memory_capacity": "Ever-updating KG containing all extracted triples since the start of an episode (no explicit fixed capacity specified); additionally hundreds of saved test-time trajectories are stored for post-hoc analysis.",
            "memory_retrieval_strategy": "Graph Attention (GAT) + Hierarchical Graph Attention over sub-graphs for immediate retrieval/attribution; post-hoc retrieval via a Bayesian State Filter over saved trajectories (counts/conditional P(A|B_i)), filtered further by a GPT-2 action language model (CALM) and a semantic-state-action filter.",
            "memory_update_strategy": "KG is updated at every timestep by combining candidate vertices/relations produced by an ALBERT QA module (answering environment questions) with the previous KG G_{t-1} to form G_t; KG expansion can also be rewarded as intrinsic motivation.",
            "memory_usage_purpose": "Maintain explicit world belief for decision making, constrain action decoding (graph mask), provide fine-grained attributions for immediate natural-language explanations, and support post-hoc temporal explanation generation by analyzing stored trajectories.",
            "performance_with_memory": "Reported to match or exceed contemporary KG-based state-of-the-art on the Jericho testbed: HEX-RL achieved comparable asymptotic and maximum scores to other strong KG agents and improved maximum training scores vs. baselines; when trained with intrinsic motivation (reward for KG expansion) performance improved further. (Quantitative per-game asymptotic and max scores are reported in Table 2 of the paper.)",
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "Hierarchical KG + attention yields more coherent and trustworthy immediate explanations (preferred in human studies) and supports temporal explanations; intrinsic-motivation on KG expansion helps performance. Effectiveness depends on quality of KG extraction and on the ability of the Bayesian trajectory filter to find causal chains.",
            "memory_limitations": "Effectiveness limited by (1) errors in KG extraction (ALBERT QA and candidate selection) and by cases where the true explanation is not expressible via the extracted fact types; (2) temporal explanation quality degrades when the game's branching factor is high (many alternative paths), causing the Bayesian State Filter to fail to identify essential steps; templated KG-to-language conversions can be less human-like than raw substrings.",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": "The full system: ALBERT-QA to build an ever-updating RDF KG split into four sub-graphs, GAT encoding with Hierarchical Graph Attention, saved trajectory buffer for many rollouts, followed by Bayes + CALM (GPT-2 action LM) + semantic filtering for temporal explanations; this configuration produced the best explanation quality and competitive task performance.",
            "uuid": "e2742.0",
            "source_info": {
                "paper_title": "Inherently Explainable Reinforcement Learning in Natural Language",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "Knowledge Graph (KG)",
            "name_full": "RDF Knowledge Graph state representation",
            "brief_description": "A symbolic, ever-updating graph of RDF triples (subject, relation, object) extracted from textual observations that serves as the agent's belief state and memory of the world during gameplay.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Knowledge Graph (state memory)",
            "agent_description": "Not an agent — a memory/data-structure built from QA outputs: stores triples representing attributes, inventory, objects-in-room and location relations, partitioned into four sub-graphs and encoded by GATs for downstream policy and explanation generation.",
            "base_model_size": null,
            "game_benchmark_name": "Jericho (interactive fiction benchmark)",
            "game_description": "Long-horizon text-adventure puzzles where agents must discover and use objects across many locations.",
            "uses_memory": true,
            "memory_type": "graph-based memory",
            "memory_structure": "RDF triple graph partitioned into sub-graphs (G_attr, G_inv, G_obj, G_loc); used as a structured symbolic store and input to GATs and attention modules.",
            "memory_content": "Facts about objects, attributes, object locations, location-to-location relations, and inventory items (the union of extracted V_t and R_t across timesteps).",
            "memory_capacity": "Accumulates facts across the whole episode; no hard limit specified in paper.",
            "memory_retrieval_strategy": "Node-level attention via Graph Attention Networks and Hierarchical Graph Attention yields top-k triples used for immediate explanations; used as mask for action decoder to constrain valid entities.",
            "memory_update_strategy": "Updated each timestep by merging new candidate vertices/relations (from ALBERT-QA answers) with the previous graph G_{t-1} to produce G_t.",
            "memory_usage_purpose": "State tracking, constraining the action decoder, powering immediate natural-language explanations by pointing to attended triples, and providing the data for post-hoc trajectory analysis.",
            "performance_with_memory": "Agents using explicit KGs (including HEX-RL, Q*BERT, SHA-KG, KG-A2C) achieve better or comparable task performance to non-KG baselines on Jericho and provide higher-quality explanations; HEX-RL reported improved maximum training scores and competitive asymptotic performance when using KG-based memory.",
            "performance_without_memory": null,
            "has_memory_ablation": null,
            "memory_effectiveness_findings": "KGs provide explainability benefits (more coherent immediate explanations) and can improve exploration when coupled with intrinsic rewards for KG expansion; KG extraction quality critically affects usefulness.",
            "memory_limitations": "Dependent on extraction accuracy; limited expressivity if relevant explanatory facts are not captured by the chosen triple types; growth and branching in trajectories complicate post-hoc causal inference.",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": "A structured KG split into sub-graphs with hierarchical attention (as in HEX-RL) combined with trajectory-level filters (Bayes + CALM + semantic) yields the best explanation quality and strong task performance.",
            "uuid": "e2742.1",
            "source_info": {
                "paper_title": "Inherently Explainable Reinforcement Learning in Natural Language",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "Bayesian State Filter",
            "name_full": "Bayesian State Filter for trajectory causal dependency estimation",
            "brief_description": "A post-hoc analysis module that computes conditional probabilities P(A | B_i) from many saved trajectories (counts and co-occurrence/order statistics) to identify state-action pairs that are likely causal dependencies for reaching a goal, used to select important states for temporal explanations.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Bayesian State Filter",
            "agent_description": "A statistical filter trained on stored rollouts that computes conditional probabilities based on counts and co-occurrence of (state,action,KG) steps across trajectories to select candidate predecessor states that are necessary for later steps/goal achievement.",
            "base_model_size": null,
            "game_benchmark_name": "Jericho",
            "game_description": "Interactive fiction games where trajectories are saved and analyzed.",
            "uses_memory": true,
            "memory_type": "trajectory-based frequency/conditional statistics (analytical memory over saved rollouts)",
            "memory_structure": "Counts and co-occurrence statistics derived from a buffer of hundreds of test-time rollouts; used to compute P(A | B_i) via raw counts and co-occurrence counts.",
            "memory_content": "Saved game steps consisting of observation o_t, action a_t and knowledge graph G_t, plus value/score metadata across many rollouts.",
            "memory_capacity": "Hundreds of saved test-time trajectories (paper notes larger numbers of trajectories improve Bayes filter accuracy); no strict upper bound given.",
            "memory_retrieval_strategy": "Works by searching backward from final goal states and selecting game steps with P(A|B_i) &gt; threshold p (count-based conditional probability).",
            "memory_update_strategy": "Trained/compiled once from many saved trajectories collected under the trained policy (post-training analysis).",
            "memory_usage_purpose": "Identify chains of likely causal dependencies across states to form temporally-extended explanations (which intermediate, unrewarded actions were necessary).",
            "performance_with_memory": "Improves temporal explanation quality when combined with subsequent CALM and semantic filters; ablations show Bayes+CALM outperforms Bayes alone on human-judged confidence and understandability.",
            "performance_without_memory": null,
            "has_memory_ablation": true,
            "memory_effectiveness_findings": "Effective at finding dependency chains in less-branching games; fails or degrades when the game's branching factor is high and many alternative paths lead to the same goal.",
            "memory_limitations": "Sensitive to dataset size (needs many trajectories), fails under high branching/alternative path complexity, and can select spurious dependencies if co-occurrence statistics are noisy.",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": "Used in tandem with CALM (GPT-2 action LM) and semantic filtering to achieve best temporal explanation quality.",
            "uuid": "e2742.2",
            "source_info": {
                "paper_title": "Inherently Explainable Reinforcement Learning in Natural Language",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "CALM",
            "name_full": "Contextual Action Language Model (CALM)",
            "brief_description": "A GPT-2 language model trained on human play-through transcripts to predict likely next actions given observation-action-next-observation contexts, used as an action filter to prefer human-like causal links when selecting important states for temporal explanations.",
            "citation_title": "Keep CALM and explore: Language models for action generation in text-based games",
            "mention_or_use": "use",
            "agent_name": "CALM (GPT-2 action LM)",
            "agent_description": "A GPT-2-based conditional action generator trained on human transcripts that, given (o_t, a_t, o_{t+1}) prompts, produces candidate next actions; in HEX-RL it is used as a filter to keep those B_i states for which CALM predicts a_{B_i} given (o_A,a_A,o_{B_i}).",
            "base_model_size": null,
            "game_benchmark_name": "Jericho (human play-through transcripts used for training CALM)",
            "game_description": "Text-adventure environments (IF) where humans produce transcripts used to train action LM.",
            "uses_memory": false,
            "memory_type": null,
            "memory_structure": null,
            "memory_content": null,
            "memory_capacity": null,
            "memory_retrieval_strategy": null,
            "memory_update_strategy": null,
            "memory_usage_purpose": "Not used as an episodic memory store; used to predict and filter candidate causal steps that humans would take, improving naturalness and human-likeness of selected temporal explanation steps.",
            "performance_with_memory": "When applied as a filter (Bayes+CALM) human evaluators preferred resulting temporal explanations over Bayes alone on 'Confidence' and 'Understandability' metrics (statistically significant improvements).",
            "performance_without_memory": null,
            "has_memory_ablation": null,
            "memory_effectiveness_findings": "CALM improves selection of human-plausible intermediate steps for temporal explanations and increases perceived understandability and confidence when combined with Bayes filtering.",
            "memory_limitations": "CALM is a predictive LM and does not itself provide causal grounding; it can only filter candidate states where predicted actions match actual actions and relies on having suitable human training transcripts.",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": "Used as the middle step in the three-stage temporal-filter pipeline (Bayes → CALM → Semantic) to yield better temporal explanations.",
            "uuid": "e2742.3",
            "source_info": {
                "paper_title": "Inherently Explainable Reinforcement Learning in Natural Language",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "ALBERT-QA module",
            "name_full": "ALBERT-based Question Answering module (for KG construction)",
            "brief_description": "An ALBERT QA model fine-tuned on JerichoQA to answer environment questions (e.g., 'What am I carrying?') producing candidate vertices and relations used to construct/update the knowledge graph.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "ALBERT-QA (KG extractor)",
            "agent_description": "An ALBERT-based question-answering network fine-tuned to produce candidate graph vertices V_t and relations R_t from textual observations; used at every timestep to propose facts that are merged into the KG.",
            "base_model_size": null,
            "game_benchmark_name": "Jericho (JerichoQA dataset used to fine-tune ALBERT)",
            "game_description": "Text-adventure games where QA questions link text observations to KG triples.",
            "uses_memory": false,
            "memory_type": null,
            "memory_structure": null,
            "memory_content": null,
            "memory_capacity": null,
            "memory_retrieval_strategy": null,
            "memory_update_strategy": null,
            "memory_usage_purpose": "Produces candidate facts to update the agent's KG memory each timestep.",
            "performance_with_memory": "Quality of the KG (and thus explanations and downstream decisions) depends critically on ALBERT-QA accuracy; failures in immediate explanations are often attributable to KG extraction errors from this module.",
            "performance_without_memory": null,
            "has_memory_ablation": null,
            "memory_effectiveness_findings": "Accurate, well-trained KG extraction is necessary for effective KG-based memory; extraction errors are a primary limit on explanation quality.",
            "memory_limitations": "QA extraction errors and coverage limits of the chosen question set constrain what facts are ever stored in KG, reducing explanation/decision quality when relevant facts are missed.",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": "Fine-tune ALBERT on JerichoQA and merge V_t/R_t outputs with previous KG each step; this was used in HEX-RL and compared favorably to rule-based extraction in some baselines.",
            "uuid": "e2742.4",
            "source_info": {
                "paper_title": "Inherently Explainable Reinforcement Learning in Natural Language",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "Q*BERT",
            "name_full": "Q*BERT (KG-building RL agent)",
            "brief_description": "A prior RL agent that constructs a knowledge graph by answering questions (QA) and uses that KG as part of its policy; cited and used as a strong baseline in comparisons.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Q*BERT",
            "agent_description": "Baseline agent (Ammanabrolu et al., 2020) that builds a KG by QA and uses it for RL; unlike HEX-RL it does not use relation information in the policy and does not employ hierarchical graph attention for fine-grained triple-level attribution.",
            "base_model_size": null,
            "game_benchmark_name": "Jericho",
            "game_description": "Interactive fiction games used as evaluation; Q*BERT served as a comparative KG-based agent.",
            "uses_memory": true,
            "memory_type": "knowledge-graph-based memory",
            "memory_structure": "KG built by QA; details differ from HEX-RL (no hierarchical sub-graph attention as described).",
            "memory_content": "Extracted triples from text via QA.",
            "memory_capacity": "Accumulates facts across episode; unspecified.",
            "memory_retrieval_strategy": "Not reported to use relation-level attention; SHA-KG/HEX-RL claim more fine-grained attribution.",
            "memory_update_strategy": "KG updated via QA outputs across timesteps.",
            "memory_usage_purpose": "State representation for RL and decision making.",
            "performance_with_memory": "Q*BERT is a strong baseline; HEX-RL with intrinsic motivation outperforms Q*BERT on at least one game, is outperformed on two games, and is comparable on the remaining six — overall HEX-RL is comparable to Q*BERT in asymptotic performance (see Table 2).",
            "performance_without_memory": null,
            "has_memory_ablation": null,
            "memory_effectiveness_findings": "Using QA to build KG can lead to strong task performance, but lacking relation-aware hierarchical attention reduces the ability to provide fine-grained, triple-level explanations.",
            "memory_limitations": "Does not exploit edge/relationship information in policy/value networks for fine-grained attribution (per paper's critique).",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": null,
            "uuid": "e2742.5",
            "source_info": {
                "paper_title": "Inherently Explainable Reinforcement Learning in Natural Language",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "SHA-KG",
            "name_full": "Stacked Hierarchical Attention over Knowledge Graph (SHA-KG)",
            "brief_description": "A prior KG-based agent that constructs a KG (rules-based) and feeds it into a hierarchical attention network; used as a baseline but criticized for averaging attention across large portions of the graph and not using relation information.",
            "citation_title": "Deep reinforcement learning with stacked hierarchical attention for text-based games",
            "mention_or_use": "use",
            "agent_name": "SHA-KG",
            "agent_description": "Baseline that uses rules-based KG construction and a stacked hierarchical graph-attention architecture; in the paper SHA-KG is noted to average attention across large graph regions and not point to specific KG triples for explanation.",
            "base_model_size": null,
            "game_benchmark_name": "Jericho",
            "game_description": "Text-adventure games used for baseline comparisons.",
            "uses_memory": true,
            "memory_type": "knowledge-graph-based memory (rules-based extraction)",
            "memory_structure": "KG partitioned (rules-based) and encoded via hierarchical attention, but attention aggregation is coarse.",
            "memory_content": "Triples extracted by rules/OpenIE over text observations.",
            "memory_capacity": "Accumulates across episode; unspecified.",
            "memory_retrieval_strategy": "Hierarchical attention but averaged across large graph portions (coarser attribution than HEX-RL).",
            "memory_update_strategy": "Rules-based triple extraction merged into KG each step.",
            "memory_usage_purpose": "State representation for RL policy/value estimation.",
            "performance_with_memory": "Included as a strong baseline; HEX-RL's hierarchical attention that uses relations is argued to provide finer-grained explanations than SHA-KG though task performance comparisons are provided in Table 2.",
            "performance_without_memory": null,
            "has_memory_ablation": null,
            "memory_effectiveness_findings": "Coarse averaging of attention reduces ability to attribute specific triples to actions; fine-grained relation-aware attention (HEX-RL) is preferable for explanations.",
            "memory_limitations": "Averaging attention over large subgraphs limits triplet-level attribution and explanation precision.",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": null,
            "uuid": "e2742.6",
            "source_info": {
                "paper_title": "Inherently Explainable Reinforcement Learning in Natural Language",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "KG-A2C",
            "name_full": "KG-A2C (rule-based KG with A2C)",
            "brief_description": "A baseline A2C agent that uses a rules-based OpenIE pipeline to extract KG triplets from observations and uses those as the state representation for policy learning.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "KG-A2C",
            "agent_description": "Baseline that constructs a KG using a rules-based OpenIE extractor and feeds the KG as state into an A2C policy; serves as a comparative KG-using baseline.",
            "base_model_size": null,
            "game_benchmark_name": "Jericho",
            "game_description": "Interactive fiction games used as baselines for comparison.",
            "uses_memory": true,
            "memory_type": "knowledge-graph-based memory (rules-based extraction)",
            "memory_structure": "RDF-style triples extracted via OpenIE and merged into a KG.",
            "memory_content": "Objects, relations and attributes extracted by OpenIE.",
            "memory_capacity": "Accumulates across episode; unspecified.",
            "memory_retrieval_strategy": "Not specified to use hierarchical attention as in HEX-RL.",
            "memory_update_strategy": "Updated via rule-based extraction each timestep.",
            "memory_usage_purpose": "State representation for RL.",
            "performance_with_memory": "Serves as baseline; HEX-RL reported improved maximum scores versus these baselines and comparable asymptotic performance.",
            "performance_without_memory": null,
            "has_memory_ablation": null,
            "memory_effectiveness_findings": "Rule-based KG extraction can be effective for RL state representation, but lacks the relation-aware hierarchical attention and QA-based extraction used by HEX-RL that improve explanation fidelity.",
            "memory_limitations": "Quality depends on rule-based extractor; may miss or mis-paraphrase relations that QA-based KG construction can capture.",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": null,
            "uuid": "e2742.7",
            "source_info": {
                "paper_title": "Inherently Explainable Reinforcement Learning in Natural Language",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "LSTM-A2C",
            "name_full": "LSTM-A2C (text-only baseline)",
            "brief_description": "A baseline agent that uses only raw textual observations encoded by an LSTM (no KG memory) with an A2C policy network; used to compare the value of KG memory for explanation and performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "LSTM-A2C",
            "agent_description": "A2C agent that encodes natural language observations using an LSTM and derives policy/value outputs without any explicit KG or graph memory.",
            "base_model_size": null,
            "game_benchmark_name": "Jericho",
            "game_description": "Interactive fiction games used to evaluate baselines.",
            "uses_memory": false,
            "memory_type": null,
            "memory_structure": null,
            "memory_content": null,
            "memory_capacity": null,
            "memory_retrieval_strategy": null,
            "memory_update_strategy": null,
            "memory_usage_purpose": "N/A (no explicit memory beyond recurrent hidden state).",
            "performance_with_memory": null,
            "performance_without_memory": "As a non-KG baseline, LSTM-A2C generally underperforms or is comparable to KG-enabled agents in task performance in the Jericho games and produces inferior immediate explanations (human evaluators preferred KG-based explanations over LSTM attention substrings).",
            "has_memory_ablation": null,
            "memory_effectiveness_findings": "Without an explicit KG memory, immediate explanation extraction used LSTM attention to extract substrings which can sound more human-like but are often incoherent; KG memory produced more coherent and confidence-inspiring explanations.",
            "memory_limitations": null,
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": null,
            "uuid": "e2742.8",
            "source_info": {
                "paper_title": "Inherently Explainable Reinforcement Learning in Natural Language",
                "publication_date_yy_mm": "2021-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Graph constrained reinforcement learning for natural language action spaces",
            "rating": 2,
            "sanitized_title": "graph_constrained_reinforcement_learning_for_natural_language_action_spaces"
        },
        {
            "paper_title": "Keep CALM and explore: Language models for action generation in text-based games",
            "rating": 2,
            "sanitized_title": "keep_calm_and_explore_language_models_for_action_generation_in_textbased_games"
        },
        {
            "paper_title": "Deep reinforcement learning with stacked hierarchical attention for text-based games",
            "rating": 2,
            "sanitized_title": "deep_reinforcement_learning_with_stacked_hierarchical_attention_for_textbased_games"
        },
        {
            "paper_title": "Q*BERT",
            "rating": 1
        },
        {
            "paper_title": "Textworld: A learning environment for text-based games",
            "rating": 1,
            "sanitized_title": "textworld_a_learning_environment_for_textbased_games"
        }
    ],
    "cost": 0.020911,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Inherently Explainable Reinforcement Learning in Natural Language</p>
<p>Xiangyu Peng 
Georgia Institute of Technology ‡ Allen Institute for AI</p>
<p>Mark O Riedl riedl@gatech.edu 
Georgia Institute of Technology ‡ Allen Institute for AI</p>
<p>Prithviraj Ammanabrolu 
Georgia Institute of Technology ‡ Allen Institute for AI</p>
<p>Inherently Explainable Reinforcement Learning in Natural Language
0DAA4E28C4C7E91EF13C190DDA04B415
We focus on the task of creating a reinforcement learning agent that is inherently explainable-with the ability to produce immediate local explanations by thinking out loud while performing a task and analyzing entire trajectories post-hoc to produce temporally extended explanations.This Hierarchically Explainable Reinforcement Learning agent (HEX-RL), operates in Interactive Fictions, textbased game environments in which an agent perceives and acts upon the world using textual natural language.These games are usually structured as puzzles or quests with long-term dependencies in which an agent must complete a sequence of actions to succeed-providing ideal environments in which to test an agent's ability to explain its actions.Our agent is designed to treat explainability as a first-class citizen, using an extracted symbolic knowledge graph-based (KG) state representation coupled with a Hierarchical Graph Attention mechanism that points to the facts in the internal graph representation that most influenced the choice of actions.Experiments show that this agent provides significantly improved explanations over strong baselines, as rated by human participants generally unfamiliar with the environment, while also matching state-of-the-art task performance.</p>
<p>Introduction</p>
<p>Observation: Up a tree Beside you on the branch is a small birds nest In the birds nest is a large egg encrusted with precious jewels, scavenged by a childless songbird...  Explainable AI refers to artificial intelligence methods and techniques that provide humanunderstandable insights into how and why an AI system chooses actions or makes predictions.Such explanations are critical for ensuring reliability and improving trustworthiness by increasing user understanding of the underlying model.In this work we specifically focus on creating deep reinforcement learning (RL) agents that can explain their actions in sequential decision making environments through natural language.</p>
<p>In contrast to the majority of contemporary work in the area which focuses on supervised machine learning problems which require singular instance level local explanations (You et al., 2016;Xu et al., 2015;Wang et al., 2017;Wiegreffe and Marasovic, 2021), such environments-in which agents need to reason causally about actions over a long series 36th Conference on Neural Information Processing Systems (NeurIPS 2022).</p>
<p>arXiv:2112.08907v3 [cs.HC] 6 Oct 2022</p>
<p>of steps-require an agent to take into account both environmentally grounded context as well as goals when producing explanations.Agents implicitly contain beliefs regarding the downstream effects-the changes to the world-that actions taken at the current timestep will have.This requires explanations in these environments to contain an additional temporally extended component taking the full trajectory's context into account-complementary to the immediate step-by-step explanations.</p>
<p>Interactive Fiction (IF) games (Fig. 1) are partially observable environments where an agent perceives and acts upon a world using potentially incomplete textual natural language descriptions.They are structured as long puzzles and quests that require agents to reason about thousands of locations, characters, and objects over hundreds of steps, creating chains of dependencies that an agent must fulfill to complete the overall task.They provide ideal experimental test-beds for creating agents that can both reason in text and explain it.</p>
<p>We introduce an approach to game playing agents-Hierarchically Explainable Reinforcement Learning (HEX-RL)-that is designed to be inherently explainable, in the sense that its internal state representation-i.e.belief state about the world-takes the form of a symbolic, humaninterpretable knowledge graph (KG) that is built as the agent explores the world.The graph is encoded by a Graph Attention network (GAT) (Veličković et al., 2017) extended to contain a hierarchical graph attention mechanism that focuses on different sub-graphs in the overall KG representation.Each of these sub-graphs contains different information such as attributes of objects, objects the player has, objects in the room, current location, etc.Using these encoding networks in conjunction with the underlying world KG, the agent is able to create immediate explanations akin to a running commentary that points to the facts within this knowledge graph that most influence its current choice of actions when attempting to achieve the tasks in the game on a step-by-step basis.</p>
<p>While graph attention can tell us which elements in the KG are attended to when maximizing expected reward from the current state, it cannot explain the intermediate, unrewarded dependencies that need to be satisfied to meet the long term task goals.For example, in the game zork1, the agent needs to pick up a lamp early on in the game-an unrewarded action-but the lamp is only used much later on to progress through a location without light.Thus, our agent additionally analyzes an overall episode trajectory-a sequence of knowledge graph states and actions from when the agent first starts in a world to either task completion or agent death-to find the intermediate set of states that are most important for completing the overall task.This information is used to generate a temporally extended explanation that condenses the immediate step-by-step explanations to only the most important steps required to fulfill dependencies for the task.</p>
<p>Our contributions are as twofold: (1) we create an inherently explainable agent that uses an ever-updating knowledge-graph based state representation to generate step-by-step immediate explanations for executed actions as well as performing a post-hoc analysis to create temporal explanations; and (2) a thorough experimental study against strong baselines that shows that our agent generates significantly improved explanations for its actions when rated by human participants unfamiliar with the domain while not losing any task performance compared to the current state-of-the-art knowledge graph-based agents.</p>
<p>Background and Related Work</p>
<p>Interactive Fiction (IF) games are simulations featuring language-based state and action spaces.It provides a platform for exploring lifelong open-domain dialogue learning (Shuster et al., 2020) and action elimination with deep reinforcement learning (Zahavy et al., 2018).In this paper, we use IF games as our test-bed because they provide an ideal platform for collecting data, linking game states and actions to the corresponding natural language explanations.We use the definition of text-adventure games as seen in Côté et al. (2018) and Hausknecht et al. (2020).We take Jericho (Hausknecht et al., 2020)   Knowledge Graphs for Text Games.Ammanabrolu et al. (2020) proposed Q<em>BERT, a reinforcement learning agent that learns a KG of the world by answering questions.Xu et al. (2020) introduce the SHA-KG, a stacked Hierarchical Graph Attention mechanism to construct an explicit representation of the reasoning process by exploiting the structure of the KG.Adhikari et al. (2020) present the Graph-Aided Transformer Agent (GATA) which learns to construct a KG during game play and improves zero-shot generalization on procedurally generated TextWorld games.Other works such as Murugesan et al. (2020) explore how to use KGs to endow agents with commonsense.While these works showcase the effectiveness of KGs on task performance and do not focus on how explainable their architectures are.We further note that these architectures do now allow for as fine-grained attention-based attribution as HEX-RL's architecture does-e.g.Q</em>BERT does not use relationship information in their policy and SHA-KG averages attention across large portions of the graph and is unable to point to specific triples in its KG representation to explain an action.</p>
<p>Explainable Deep RL. Contemporary work on explaining deep reinforcement learning policies can be broadly categorized based on:</p>
<p>(1) how the information is extracted, either via intrinsic motivation during training (Shu et al., 2017;Hein et al., 2017;Verma et al., 2018) or through post-hoc analysis (Rusu et al., 2015;Hayes and Shah, 2017;Juozapaitis et al., 2019;Madumal et al., 2020); and (2) the scope-either global (Zahavy et al., 2016;Hein et al., 2017;Verma et al., 2018;Liu et al., 2018) or local (Shu et al., 2017;Liu et al., 2018;Madumal et al., 2020;Guo et al., 2021).In our work, we create an agent that spans more than one of these categories providing immediately local explanations through extracted knowledge graph representations and post-hoc temporal explanations.Inspired by Madumal et al. (2020), we learn a graphical causal model which focuses on using relations between steps in a puzzle to generate temporal explanations instead of generating counterfactuals.</p>
<p>Hierarchically Explainable RL</p>
<p>Our work aims to generate (1) immediate step-by-step explanations of an agent's policy by capturing the importance of the current game state observation and (2) temporally extended explanations that take into context an entire trajectory via a post-hoc analysis.Formally, let X = {s t , a t } t =1:T be the set of game steps that compose a trajectory.Each game state s t consists of a knowledge graph G t representing all the information learned since the start of the game.This graph is further split into four sub-knowledge graphs each containing different, semantically related relationship types.This section first describes a graph attention based architecture that uses these sub-graphs to produce immediate explanations.We then describe how to filter the game states in a trajectory into a condensed set of the most important ones X ⊂ X that best capture the underlying dependencies that need to be fulfilled to complete the task-enabling us to produce temporal explanations.</p>
<p>Knowledge Graph State Representation.Building on Ammanabrolu et al. (2020), constructing the knowledge graph is treated as a question-answering task.KGs in these games take the form of RDF triples (Klyne, 2004) of 〈sub j ec t , r el at i on, ob j ec t 〉-extracted from text observations and update as the agent explores the world.The agent answers questions about the environment such as, "What am I carrying?"or "What objects are around me?".A specially constructed dataset for question answering in text games-JerichoQA-is used to fine-tune ALBERT (Lan et al., 2019) to answer these questions (See Appendix A.3).The answers form a set of candidate graph vertices V t for the current step and questions form the set of relations R t .Both V t and R t are then combined with the graph at the previous step G t −1 to update the agent's belief about the world state into G t .The left side of Figure 2 showcases this.</p>
<p>In an attempt to enable more fine grained explanation generation and inspired by Xu et al. (2020), we divide the knowledge graph G into multiple sub-graphs G at r ,G i nv ,G ob j ,G l oc , each representing (1) attributes of objects, (2) objects the player has, (3) objects in the room, and (4) other information such as location (right side of Fig. 2) based on the corresponding relationship types extracted by the ALBERT-QA module.The union of all sub-graphs is equivalent of V t and R t extracted from the current game state.The full knowledge graph G t captures the overall game state since the start of the game.The sub-graphs easily reflect different relationships of the current game state.</p>
<p>Template Action Space.Agents output a language string into the game to describe the actions that they want to perform.To ensure tractability, this action space can be simplified down into templates.Templates consist of interchangeable verbs phrases (V P ), optionally followed by prepositional phrases (V P P P ), e.g.([c ar r y/t ake] ) and ([t hr ow/d i sc ar d /put ] [ag ai nst /on/d own] ), where the verbs and prepositions within [.] are aliases.Actions are constructed from templates by filling in the template's blanks using words in the game's vocabulary.Size of action space is shown in Appendix A.1.</p>
<p>Immediate Explanations</p>
<p>Our immediate explanations consist of finding the subset of triplets in sub-graphs G at r ,G i nv ,G ob j ,G l oc c the action decision made at the current step-is capable of explaining the action.We introduce a deep RL architecture capable of this.</p>
<p>Hierarchical Knowledge Graph Attention Architecture.At each step, a total score R t and an observation o t is received-consisting of o t desc , o t game , o t inv , a t −1 corresponding to the room description, game feedback, inventory, and previous action and are processed using a GRU based encoder using the hidden state from the previous step, combining them into a single observation embedding o t ∈ R d t ext ×c (bottom of Fig. 2).</p>
<p>The full knowledge graph G t is processed via Graph Attention Networks (GATs) (Veličković et al., 2017) followed by a linear layer to get the graph representation g t ∈ R d t ext (middle of Fig. 2).We compute LSTM attention between o t and g t as:
α LSTM = softmax (W l h LSTM + b l )
(1)
h LSTM = tanh W o o t ⊕ W g g t + b g (2)
where ⊕ denotes the addition of a matrix and a vector.
W l ∈ R d t ext ×d t ext , W g ∈ R d t ext ×d t ext , W o ∈ R d tq t = g t + c i α LSTM,i o t,i(3)
where denotes dot-product and c is the number of o t 's components.</p>
<p>Sub-graphs are also encoded by GATs to get the graph representation g t ∈ R d G t ,sub ×m (no. of subgraphs).The Hierarchical Graph Attention between q t ∈ R d G t ,sub1 and g t is calculated by:
α Hierarchical = softmax (W H h H + b H ) (4) h H = tanh W g g t ⊕ W q q t + b q (5)
where
W H ∈ R d G t ,sub ×d G t ,sub , W g ∈ R d G t ,sub ×d G t ,sub , W q ∈ R d G t ,sub ×d G t ,sub are weights and b H ∈ R d G t ,sub , b q ∈ R d G t
,sub are biases.Then we get state representation, consisting of the textual observations full knowledge graph and sub-knowledge graph.
v t = q t + s i α Hierarchical,i g t,i(6)
where s is the number of sub-graphs (4 in our paper).The full architecture can be found in Figure 2.</p>
<p>The agent is trained via the Advantage Actor Critic (A2C) (Mnih et al., 2016) method to maximize long term expected reward in the game in a manner otherwise unchanged from Ammanabrolu et al. ( 2020) (See Appendix A.2).These attention values thus reflect the portions of the knowledge graphs that the agent must focus on to best achieve this goal of maximizing reward.</p>
<p>Hierarchical Graph Attention Explanation.The graph attention α Hierarchical is used to capture the relative importance of game state observations and KG entities in influencing action choice.</p>
<p>For each sub-graph, the graph attention, α Hierarchical,i ∈ R n nodes ×m is summed over all the channels m to obtain α Hierarchical,i ∈ R n nodes ×1 , showing the importance of the KG nodes in the i th sub-graph.</p>
<p>The top-k valid entities (and corresponding edges) with highest absolute value of its attention form the set of knowledge graph triplets that best locally explain the action a t .</p>
<p>In order to make the explanation more readable for a human reader, we further transform knowledge graph triplets to natural language by template filling.</p>
<p>We create templates for each type of sub-graphs G at r ,G i nv ,G ob j ,G l oc .</p>
<p>• 〈ob j ec t , i s, at t r i but e〉→"Object is attribute"</p>
<p>• 〈pl a yer, has, ob j ec t 〉→"I have object"</p>
<p>• 〈ob j ec t , i n, l oc at i on〉→"Object is in location"</p>
<p>• 〈l oc at i on1, d i r ec t i on, l oc at i on2〉→"location 1 is in the direction of location 2", e.g.〈 f or est , nor t h, house〉 is converted to "Forest is in the north of house"</p>
<p>More examples can be found in Appendix A.4.</p>
<p>Temporally Extended Explanations</p>
<p>Graph attention tells us which entities in the KG are attended to when making a decision, but is not enough alone for explaining "why" actions are the right ones in the context of fulfilling dependencies that may potentially be unrewarded by the game-especially given the fact that there are potentially multiple ways of achieving the overall task.HEX-RL thus saves trajectories for hundreds of test time rollouts of the games, performed once a policy has been trained (Table 1 and Appendix A.5).The game trajectories consist of all the game states, actions taken, predicted critic values, game scores, the knowledge graphs, and the immediate step level explanations generated as previously described.HEX-RL produces a temporal explanation by performing a post-hoc analysis on these game trajectories.The agent then analyzes and filters these trajectories in an attempt to find the subset of states that are most crucial to achieving the task as summarized in Figure 3-then using that subset of states to generate temporal trajectory level explanations.</p>
<p>Bayesian State Filter.We first train a Bayesian model to predict the conditional probability P(A | B i ) of a game step (A) given any other possible game step (B i ) in the game trajectories.More specifically, each game step is composed of 3 elements, game state o t , action a t and the current knowledge graph G t .The key intuition here being that state, action pairs that appear in a certain ordering in multiple trajectories are more likely to dependant on each other.</p>
<p>The set of game steps with the highest P(A | B i ) is used to explain taking the action associated with game state A. For example, "take egg" (A) is required to "open egg" (B ), and
P(A | B ) = 1,
hence "open egg" is used as a reason why action "take egg" must be taken first.The initial set of game states X is filtered into X 1 by working backwards from the final goal state by finding the set of states that form the most likely chain of causal dependencies that lead to it.Details can be found in Appendix A.5 and A.6.Language Model Action Filter.Following this, we apply a GPT-2 (Radford et al., 2019) language model trained to generate actions based on transcripts of text games from human play-throughs to further filter out important states-known as the Contextual Action Language Model (CALM) (Yao et al., 2020).As this language model is trained on human transcripts, we hypothesize that it is able to further filter down the set of important states by finding the states that have corresponding actions that a human player would be more likely to perform-thus potentially leading to more natural explanations.CALM takes into observation o t , action a t and the following observation o t +1 , and predicts next valid actions a t +1 .In our work, we use CALM as a filter to look for the relations between a game step A and the explanation candidates B i ∈ X 1 .We feed CALM with the prompt o A , a A , o B i to get an action candidate set.When the two game steps A and B i are highly correlated, given o A , a A and o B i , CALM should successfully predict a B i with high probability.The game steps B i , whose associated action a B i is in this generated action candidates set, are saved as the next set of filtered important candidate game states (X 2 ).Semantic State-Action Filter.To better account for the irregularities of the puzzle like environment, we adopt a semantic filter to obtain the final important state set X 3 .Here, given A, B i ∈ X 2 , states are further filtered on the basis of whether one of these scenarios occurs: (1) a A and a B i contain the same entities, e.g."take egg" and "open egg".(2) G A and G B i share the same entities, e.g."lamp" occurs in both observations.(3) A and B i occur in the same location, e.g. after taking action a A , the player enters "kitchen" and B occurs in "kitchen".(4) The state has a non-zero reward or a high absolute critic value, indicating that it is either a state important for achieving the goals of the game or it is a state to be avoided.The final set of important game states X 3 is used to synthesize post-hoc temporal explanations for why an action was performed in a particular state-as seen in Figure 1-taking into account the overall context of the dependencies required to be satisfied and building on the immediate step level explanations for each given state in X 3 .Ablation studies pin-pointing the relative contributions of the different filters are found in Section 4.4.We concluded that all three steps of the filtering process to identify important states are necessary for creating coherent temporal explanations that effectively take into account the context of the agent's goals.</p>
<p>Evaluation</p>
<p>Our evaluation consists of three phases: (1) We show that HEX-RL has the comparable performance to state-of-art reinforcement learning agents on text games in Section 4.1.</p>
<p>(2) Then in Section 4.2, we evaluate our immediate attention explanation model by comparing the explanations generated by HEX-RL and agents that do not use knowledge graphs (See Fig. 2 and Section 3.1).( 3) In Section 4.3 we compare immediate to temporal explanations, focusing on the effects that including trajectory level context when evaluating explanations in the context of agent goals.(4) In Section 4.4 we conduct human participant ablation study evaluating the individual contributions of the filtration pipeline for generating temporal explanations seen in Figure 3.</p>
<p>Task Performance Evaluation</p>
<p>We compare HEX-RL with four strong state-of-art reinforcement learning agents-focusing on contemporary agents that use knowledge graphs-on an established test set of 9 games from the Jericho benchmark (Hausknecht et al., 2020).</p>
<p>• LSTM-A2C is a baseline that only uses natural language observations as state representation that is encoded with an LSTM-based policy network.</p>
<p>• KG-A2C.Instead of training a question-answering system like Q*BERT to build knowledge graph state representation, KG-A2C (Ammanabrolu and Hausknecht, 2020) extracts knowledge graph triplets from the text observations using a rules based approach built on OpenIE (Angeli et al., 2015).</p>
<p>• SHA-KG is adapted from Xu et al. (2020) and uses a rules-based approach to construct a knowledge graph for the agent which is then fed into a Hierarchical Graph Attention network as in HEX-RL.This agent separates the sub-graphs out using a rules-based approach and makes no use of any graph edge relationship information.</p>
<p>• Q*BERT.Ammanabrolu et al. (2020) uses a similar method of creating the knowledge graph through question answering but does not use the hierarchical graph attention architecture combined with the sub-graphs.</p>
<p>These baselines are all trained via the Advantage Actor Critic (A2C) (Mnih et al., 2016) methodfurther comparisons to other contemporary agents can be found in Appendix A.7.It is also worth noting that most contemporary state of the art deep RL agents for text games use recurrent neural policy networks as opposed to transformer networks due to their improved performance in this domain.</p>
<p>HEX-RL Training.</p>
<p>We trained HEX-RL on two reward types: (a) game only, which indicates that we only use score obtained from the game as reward.</p>
<p>(2) game with intrinsic motivation (game and IM), which contains an additional intrinsic motivation reward based on knowledge graph expansion as seen in Ammanabrolu et al. (2020)-where the agent is additionally rewarded for learning more about the world by finding new facts for knowledge graph (see Appendix A.8, A.9 and A.10).</p>
<p>Table 2 shows the performance of HEX-RL and the other four baselines.We can see that designing the HEX-RL agent to be inherently explainable through the use of Hierarchical Graph Attention and the sub-graphs improves the overall maximum score seen during training when compared to any of the other agents.In terms of the average score seen during the final 100 episodes, HEX-RL wth intrinsic motivation outperforms all baselines with the exception of Q<em>BERT-there HEX-RL significantly outperforms Q</em>BERT on one game, is outperformed on two games, and comparable on the remaining six games.HEX-RL thus performs comparably to other state-of-the-art baselines in terms of overall task performance while also boasting the additional ability to explain its actions.</p>
<p>Immediate Explanation Evaluation</p>
<p>Having established that HEX-RL's performance while playing text games is comparable to other state-of-the-art agents, we attempt to answer the question of exactly how useful the knowledge graph based architecture is when generating immediate step-by-step explanations by comparing HEX-RL to a baseline that doesn't use knowledge graphs in a human participant study.Two models for step-by-step explanations are compared:</p>
<p>• LSTM Attention explanations.Extracts the most important substring in the observations through LSTM attention α LST M and then uses those words to create an explanation.</p>
<p>• Hierarchical Graph Attention explanations.Extracts KG triplets most influenced the choice of actions by Hierarchical Attention α Hierarchical and then transforming them into readable language explanations through templates.</p>
<p>We recruited 40 participants-generally unfamiliar with the environment at hand-on a crowd sourcing platform.Each participant reads a randomly selected subset of 10 explanation pairs (drawn randomly from a pool totaling 60 explanation pairs), generated by Hierarchical Graph Attention and LSTM attention explanation on three games in the Jericho benchmark: zork1, library, and balances.We choose three games with very different structures and genres as defined in Hausknecht et al. (2020).They each require a diverse set of action types and solutions to complete and thus provide a wide area of coverage when used as test beds for human evaluation of explanations.Then they are given the following metrics and asked to choose which explanation they prefer for that metric:</p>
<p>• Confidence: This explanation makes you more confident that the agent made the right choice.• Human-likeness: This explanation expresses more human-like thinking on the action choice.</p>
<p>• Understandability: This explanation makes you understand why the agent made the choice.</p>
<p>Variations of these questions have been used to evaluate other explainable AI systems (eg.Ehsan et al. ( 2019)).At least 5 participants give their preference for each explanation pair.We take the majority preference from humans participants as the result.More details are shown in Appendix B.1.</p>
<p>Figure 4 shows the result of the human evaluation of attention explanations.Hierarchical graph attention explanation is preferred over LSTM attention explanation in all three dimensions.These results are statistically significant (p &lt; 0.05) with fair inter-rater reliabilities.We also observe that these three dimensions are highly, positively correlated using Spearman's Rank Order Correlation.2</p>
<p>A slightly higher proportion of participants preferred the LSTM Attention explanations in the human-likeness dimension compared to the other two.The participants preferring LSTM Attention explanation stated that they found it intuitive but often incoherent and found the Hierarchical Graph Attention explanations to be more robotic.LSTM attention explanations are substrings of the human-written observation and thus have the potential to be more natural sounding than the templated Hierarchical Graph Attention explanations when they are coherent enough to be understood.The KG sacrifices a small amount of human-likeness in return for much greater overall coherence and accuracy.KGs with Hierarchical Graph Attention give us explanations that are more easily understood and inspire greater confidence in the agent's decisions.Example qualitative LSTM Attention and Hierarchical Graph Attention explanations can be found in Appendix C.2.As our system relies on graph hierarchical graph attention to generate immediate explanations, a well-trained knowledge graph representation module of the world knowledge is required.Most cases where the agent fails to provide satisfactory immediate explanations are either when: (1) the explanation is not directly linked to the one of the facts we choose to extract from the knowledge graph, such as object/location information; and (2) due to the error of knowledge graph extraction models themselves.</p>
<p>Immediate vs. Temporal Explanations</p>
<p>Having proved the effectiveness of the knowledge graph at the immediate step-by-step explanation level, we evaluate our method of producing temporal explanations and how they compare to the immediate explanations along two dimensions: (1) coherence; and (2) explanation accuracy when taken in the context of the agent's goals.</p>
<p>Participants first read a trajectory of the game combined with step-by-step immediate explanations and the game goal, then indicate how much they agree with the statements on a Likert scale of 1 (strong disagree) to 5 (strong agree).Here, we add two metrics from the previous study:</p>
<p>• Goal context: You are able to understand why the agent takes this particular sequence of actions given what you know about the goal.• Readability: This explanation is easy to read. Figure 5 shows the average scores for each question for the immediate and temporal explanations.The temporal explanations achieve comparable performance to the immediate explanations on all metrics except for the the metric relating to goal context where they significantly out-perform the immediate explanations.A majority of participants stated that a condensed trajectory level explanation made the goals of the agent easier to understand than reading through each step level explanation.These results indicate that HEX-RL can generally successfully identify the most important states in a trajectory and use them to create temporal explanations that are on par with immediate explanations in terms of coherence but provide significantly more context in terms of explaining an agent's actions with respect to its task-based goals.</p>
<p>Cases where the agent does not provide a temporally coherent and goal-driven explanation revolve around failures-particularly by the Bayesian State Filter-in detecting the most important states in the trajectory.A qualitative analysis (as seen in Appendix C.3) suggests that this occurs in cases where there are a large number of branching paths that lead to the same end state.Thus, the quality of the generated temporal explanations appears to be inversely proportional to the relative complexity of the game as measured by its branching factor.</p>
<p>Temporal Explanation Ablation Study</p>
<p>Having established the overall effectiveness of the filters in HEX-RL that create the temporal explanations, we perform pair-wise ablation studies to pinpoint the relative contributions of the different filters seen in Fig. 3.We first compare explanations generated using a set of important states filtered from the trajectory using the Bayes model compared to Bayes+CALM explanation.This how applying the language model action filter affects the quality of the temporal explanations.As before, we recruited 30 participants on a crowd sourcing platform.Each participant reads a randomly selected subset of explanation pairs, comprised of temporal explanations filtered by Bayes and Bayes+CALM models.Figure 6a shows that after applying the CALM model to filter explanation candidates, generated explanations are significantly preferred on the "Confidence" and "Understandability" dimensions.</p>
<p>Similarly, we then conducted another ablation study to validate the contribution of semantic filter by comparing the Bayes+CALM filtering method to the full HEX-RL using Bayes+CALM+Semantic filters.The experiment setup is the same as the previous ablation study.Figure 6b shows that Bayes+CALM+Semantic performs significantly better than Bayes+CALM on all three dimensions.</p>
<p>We additionally observe that these three metrics are highly, positively correlated using Spearman's Rank Order Correlation in both of these ablation studies3 .When asked to justify their choices, participants indicated that the full HEX-RL system with Bayes+CALM+Semantic filters provided temporal explanations that they felt was more understandable than alternatives.These results indicate that all three steps of the filtering process to identify important states are necessary for creating coherent temporal explanations that effectively take into account the context of the agent's goals.</p>
<p>Conclusions</p>
<p>Explaining deep RL policies for sequential decision making problems in natural language is a sparsely studied problem despite a steadily growing need.An oft given reason for this phenomenon is that deep RL methods perform better without the additional burden of being explainable.In an attempt to encourage work in this area, we create the Hierarchically Explainable Reinforcement Learning (HEX-RL) agent which treats explainability as a first-class citizen in its design by using a readily interpretable knowledge graph state representation coupled with a Hierarchical Graph Attention network.This agent is able to produce step-by-step commentary-like immediate explanations and also a condensed temporal trajectory level explanation via a post-hoc analysis.We show that with careful design, it is possible to create inherently explainable RL agents that do not lose performance when compared to contemporary state-of-the-art agents and simultaneously are able to generate significantly higher quality explanations of actions.</p>
<p>A Implementation Details</p>
<p>A.1 Full Action Space Size</p>
<p>Jericho provides the capability to extract game-specific vocabulary and action templates (Section 3).These templates contain up to two blanks, so a typical game with 200 templates and a 700 word vocabulary yields an action space of O (T V 2 ) ≈ 98 million, three orders of magnitude smaller than the 240-billion space of 4-word actions using vocabulary alone.</p>
<p>A.2 A2C Architecture</p>
<p>Further details of what is found in Figure 2. The sequential action decoder consists two GRUs that are linked together as seen in Ammanabrolu and Hausknecht (2020).The first GRU decodes an action template and the second decodes objects that can be filled into the template.These objects are constrained by a graph mask, i.e. the decoder is only allowed to select entities that are already present in the knowledge graph.</p>
<p>Same with Ammanabrolu and Hausknecht (2020), the loss consists of template loss, object loss, value loss, actor loss and entropy loss.The template loss given a particular state and current network parameters is applied to the decoder.Similarly, the object loss is applied across the decoder is calculated by summing cross-entropy loss from all the object decoding steps.Entropy loss over the valid actions, is designed to prevent the agent from prematurely converging on a trajectory.The following hyperparameters are taken from the original paper and known to work well on text games.</p>
<p>Parameters</p>
<p>Value discount factor 0.9 entropy coefficient 0.03 value coefficient 9 template coefficient 3 object coefficient 9</p>
<p>A.3 Knowledge Graph Representation QA Model</p>
<p>The question answering network based on ALBERT (Lan et al., 2019) has the following hyperparameters, taken from the original paper and known to work well on the SQuAD 2.0 (Rajpurkar et al., 2018)</p>
<p>A.4 Templates of Immediate Explanation</p>
<p>We consider four types of sub-graphs G at r ,G i nv ,G ob j ,G l oc , each representing (1) attributes of objects, (2) objects the player has, (3) objects in the room, and (4) other information such as location (see right side of Figure 2).Hence, we create one template for each sub-graph with converting conjugated forms of verbs,</p>
<p>• 〈ob j ec t , i s, at t r i but e〉 is converted to "Object is attribute".For example, 〈t r ees, i s, i nt er ac t abl e〉 is converted to "trees are interactable.".〈eg g , i s, i nt er ac t abl e〉 is converted to "egg is interactable".• 〈pl a yer, has, ob j ec t 〉 is converted to "I have object".</p>
<p>For example, 〈pl a yer, has, eg g s〉 is converted to "I have eggs".〈pl a yer, has, kni f e〉 is converted to "I have knife".</p>
<p>• 〈ob j ec t , i n, l oc at i on〉 is converted to "Object is in location".For example, 〈eg g , i n, f or est 〉 is converted to "egg is in forest".〈t r ees, i n, f or est 〉 is converted to "trees are in forest".• 〈l oc at i on_1, d i r ec t i on, l oc at i on_2〉 is converted to "location_1 is in the direction of location_2".For example, 〈 f or est , nor t h, house〉 is converted to "forest is in the north of house".</p>
<p>A.5 HEX-RL explanation parameters</p>
<p>There are some other parameters which may affect the results of explanations.Our initial experiments suggested that the larger the number of trajectories we use, the more accurate the Bayesian State Filter.We first train a Bayesian model to predict the conditional probability P(A | B i ) of a game step (A) given any other possible game step (B i ) in the game trajectories.More specifically, current game step (A) is composed of 3 elements, game state o t , action a t and knowledge graph G t .We count the occurrence of A (C(A)), and all the game steps occurred in the game logs (C(B i )), and also count the co-occurrence of A and B i ,</p>
<p>Parameter
C(A ∩ B i ) in the same trajectory. X = {A, B 1 , ..., B i }. The conditional probability P(A | B i ) is calculated by, P(A | B i ) = P(B i | A)C(A) C(B i )(7)
where C(A) and C(B i ) stand for the raw count of game step A and B i in the collected trajectories.</p>
<p>The key intuition here being that state, action pairs that appear in a certain ordering in multiple trajectories are more likely to dependant on each other.Higher P(A | B i ) indicates the necessity of A to B .The set of game steps with the highest P(A | B i ) is used to explain taking the action associated with game state A. For example, "take egg" (A) is required to "open egg" (B ), and P(A | B ) = 1, hence "open egg" is used as a reason why action "take egg" must be taken first.The initial set of game states X is filtered into X 1 by working backwards from the final goal state by finding the set of states that form the most likely chain of causal dependencies that lead to it.As shown in Figure 3, we obtain the explanation candidate game steps X 1 by filtering all the possible game steps following current game step A in the game logs with P(A | B i ) &gt; p, where p is the threshold.</p>
<p>A.7 Raw scores across Jericho supported games</p>
<p>A.8 Reward types</p>
<p>To alleviate the issue that rewards are sparse and often delayed, Ammanabrolu et al. defined an intrinsic motivation for the agent that leverages the knowledge graph being built during exploration.The motivation is for the agent to learn more information regarding the world and expand the size of its knowledge graph.They formally define g ame_and _I M reward in terms of new information learned.
r IM t = ∆(K G global − K G t )(8)
where
K G global = t −1 i =1 K G i
Here K G global is the set of all edges that the agent has ever had in its knowledge graph and the subtraction operator is a set difference.</p>
<p>A.9 HEX-RL Architecture Hyperparameters</p>
<p>The additional hyperparamters used for training HEX-RL are detailed below, same with (Ammanabrolu et al., 2020).graph dropout and mask dropout are used for encouraging graph network to actually learn a sparse representation.</p>
<p>Parameters</p>
<p>A.10 Task Performance</p>
<p>We plot the training reward curve for 9 games in Figure 7 and Figure 8. Reward curves are shown until they reach asymptotic performance, i.e. the number of steps until the score no longer increases.All agents were trained with the same number of steps (100,000) and the results at the end of this training is what is reported in Table 2.</p>
<p>A.11 Immediate Explanation Evaluation</p>
<p>We plot the immediate explanation evaluation result per game in Figure 9.</p>
<p>B Human Evaluation Details</p>
<p>B.1 Immediate Explanation Evaluation</p>
<p>We firstly ask participants to read an interactive game description and then ask them to answer a set of questions about this game to make sure they are qualified.They will also play a demo of an interactive text game and answer a question based on the game they played.The details can be found in Figure 10 and Figure 11.These questions are designed to improve the quality of human evaluation.At least 5 participants give their preference for each explanation pair.</p>
<p>Each participant reads a randomly selected subset of 10 explanation pairs (drawn randomly from a pool totaling 60 explanation pairs), generated by Hierarchical Graph Attention and LSTM attention explanation on three games in the Jericho benchmark, zork1, library, and balances.The following three questions are asked,</p>
<p>• Which explanation makes you more confident that the agent made the right choice?• Which explanation expresses more human-like thinking on the action choice?• Which explanation makes you understand why the agent made the choice?We also plot the causal explanation ablation study result per game in Figure 15 and Figure 16.</p>
<p>C Explanations Analysis</p>
<p>In this section we provide descriptions of each of the games as well as qualitative samples of trajectories in the format shown to the human participants.We pick out two types of examples where the HEX-RL explanations of the trajectories were rated highly by the human participants and rated poorly in terms of the quality of the explanations-attempting to analyze the failure cases of our method.</p>
<p>C.1 Text Game Descriptions</p>
<p>We list the descriptions4 of our selected text games for a better undestanding of our explanations and game trajectories in Appendix C.2 and C.3.</p>
<ol>
<li>zork1: Many strange tales have been told of the fabulous treasure, exotic creatures, and diabolical puzzles in the Great Underground Empire.As an aspiring adventurer, you will undoubtedly want to locate these treasures and deposit them in your trophy case.Zork creates a wondrous, magical realm that is a veritable feast for the imagination.You find that you have stumbled upon the ancient ruins of a vast empire lying far underground.</li>
</ol>
<p>Yes, you will find many more treasures for your trophy case.But to do so, you will have to search far and wide, solve diabolical puzzles, and defend your treasures (and yourself!) from a few very nasty characters... and one monster, a vicious GRUE that lurks in the dark! 2. Library: Escape the library using knowledge and objects found in and about the library.</p>
<ol>
<li>Balances: Balances is a relatively short, old-fashioned puzzle game set in the world of the Enchanter series and riffing on Spellbreaker in particular: the player must find scrolls, learn their spells, and cast them in order to collect white cubes.It shares a number of design characteristics with those games: rooms represent fairly large open spaces, there are more animal NPCs than humans to interact with, and a loose, playful approach to world-building means that the various areas don't have a great deal to do with one another.</li>
</ol>
<p>C.2 Immediate Explanations Analysis</p>
<p>Table 4 shows some example trajectories with its immediate explanations where Hierarchical Graph Attention Explanations did a better job than LSTM Attention.We can see when the player interacts with objects or the subset of sub knowledge graph can explain the action, Hierarchical Graph Attention is able to present a high-quality immediate explanation.</p>
<p>Table 5 shows some example trajectories with its immediate explanations where HEX-RL fails to produce correct immediate explanations.We see that when there is no clear clue in the observations, i.e. the potential gold truth explanation is does not belong knowledge graph, HEX-RL fails to produce immediate explanations.</p>
<p>Hence, we conclude that the performance of immediate explanations is limited: (1) to the cases in which explanation is directly linked to the one of the facts we choose to extract from the knowledge graph, such as object/location information; and (2) by the relative error of knowledge graph extraction models themselves.</p>
<p>C.3 Temporally Extended Explanations Analysis</p>
<p>Table 6, 7 and 8 show example game trajectories with its corresponding temporally extended explanations for game zork1, library and balances.HEX-RL finds the subset of states that are most crucial to achieving the task goals.</p>
<p>We also show one trajectory where HEX-RL fails to produce a high-quality temporally extended explanations in zork1.Here, in order to achieve the goal "open window", the player has to get to the location "North of House".However, there exists too many different combinations of actions, i.e. branching paths, which the player can take to go to "North of House".Bayesian State Filter thus fails to find the most essential steps for achieving this goal.</p>
<p>The performance of the temporally extended explanation system is hindered by the complexity of the game-the more possible ways of achieving the same overall goal, the harder it is to generate an explanation for why the agent has chosen to perform an action to achieve the goal.</p>
<p>Table 4: Example game trajectories with its immediate explanations -Part 1.</p>
<p>[zork1]Description before the action: North of House You are facing the north side of a white house.There is no door here, and all the windows are boarded up.</p>
<p>To the north a narrow path winds through the trees.</p>
<p>Action: go north LSTM: You are facing the north side of a white house.</p>
<p>Hierarchical Graph :</p>
<p>Trees is north of north of house.</p>
<p>[zork1]Description before the action: Kitchen You are in the kitchen of the white house.A table seems to have been used recently for the preparation of food.A passage leads to the west and a dark staircase can be seen leading upward.A dark chimney leads down and to the east is a small window which is open.</p>
<p>On the [zork1]Description before the action:</p>
<p>Up a Tree You are about 10 feet above the ground nestled among some large branches.</p>
<p>The nearest branch above you is above your reach.Beside you on the branch is a small birds nest.In the birds nest is a large egg encrusted with precious jewels, apparently scavenged by a childless songbird.The egg is covered with fine gold inlay, and ornamented in lapis lazuli and motherofpearl.</p>
<p>Unlike most eggs, this one is hinged and closed with a delicate looking clasp.The egg appears extremely fragile.Action: take egg LSTM: In the birds nest is a large egg encrusted with precious jewels.</p>
<p>Hierarchical Graph : egg is interactable.</p>
<p>[library]Description before the action: Rare Books Room The shelves are nearly bare, although there is a complete set of the "New ork Times", a box labeled "Avalon", and several biographies of various computer game authors.</p>
<p>The door out is to the north You can see a copy of "Debt of Honor" here.Action: use key LSTM: The shelves are nearly bare.</p>
<p>Hierarchical Graph :</p>
<p>I have key.key is interactable.[zork1]Description before the action:</p>
<p>Forest Path This is a path winding through a dimly lit forest.</p>
<p>The path heads northsouth here.One particularly large tree with some low branches stands at the edge of the path.</p>
<p>Action: go up LSTM: This is a path winding through a dimly lit forest.</p>
<p>Hierarchical Graph : tree is interactable.</p>
<p>[library]Description before the action:</p>
<p>Rare Books Room The shelves are nearly bare, although there is a complete set of the "New ork Times", a box labeled "Avalon", and several biographies of various computer game authors.</p>
<p>The door out is to the north You can see a biography of Graham Nelson here.</p>
<p>Action: go west LSTM: The door out is to the north.Hierarchical Graph : I am in the Rare Books now.</p>
<p>[balances]Description before the action:</p>
<p>Grasslands, near Hut The grasslands sway over low hills in all directions it is a peaceful wilderness, broken only by this hut and a faint path to the north.Goal: Take Egg Description: Kitchen You are in the kitchen of the white house.A table seems to have been used recently for the preparation of food.A passage leads to the west and a dark staircase can be seen leading upward.A dark chimney leads down and to the east is a small window which is open.On the table is an elongated brown sack, smelling of hot peppers.Sitting on the kitchen table is A glass bottle The glass bottle contains A quantity of water Action: east Description: Behind House You are behind the white house.A path leads into the forest to the east.In one corner of the house there is a small window which is open.Action: east Description: North of House You are facing the north side of a white house.There is no door here, and all the windows are boarded up.To the north a narrow path winds through the trees.Action: north Description: Forest Path This is a path winding through a dimly lit forest.The path heads northsouth here.One particularly large tree with some low branches stands at the edge of the path.Action: up Description: Up a Tree You are about 10 feet above the ground nestled among some large branches.The nearest branch above you is above your reach.Beside you on the branch is a small birds nest.In the birds nest is a large egg encrusted with precious jewels, apparently scavenged by a childless songbird.The egg is covered with fine gold inlay, and ornamented in lapis lazuli and mother of pearl.Unlike most eggs, this one is hinged and closed with a delicate looking clasp.The egg appears extremely fragile.Action: take egg Description: Taken.</p>
<p>Temporally Extended Explanations</p>
<p>Description: Forest Path This is a path winding through a dimly lit forest.The path heads northsouth here.One particularly large tree with some low branches stands at the edge of the path.Action: up Description: Up a Tree You are about 10 feet above the ground nestled among some large branches.The nearest branch above you is above your reach.Beside you on the branch is a small birds nest.In the birds nest is a large egg encrusted with precious jewels, apparently scavenged by a childless songbird.The egg is covered with fine gold inlay, and ornamented in lapis lazuli and mother of pearl.Unlike most eggs, this one is hinged and closed with a delicate looking clasp.The egg appears extremely fragile.Action: take egg Description after taking the above action: Taken.The shelves are nearly bare, although there is a complete set of the "New ork Times", a box labeled "Avalon", and several biographies of various computer game authors.The door out is to the north You can see a biography of Graham Nelson here.Action: take all</p>
<p>Temporally Extended Explanations</p>
<p>Description: Second Floor Stacks This cavernous room is filled with shelves as far as the eye can see.A doorway to the east is labelled "Computer Room", and the stairwell lies to the north.The door is unlocked but shut.Action: undo door Description: You open the rare books door.Action: south Description after taking the above action: Rare Books Room The shelves are nearly bare, although there is a complete set of the "New ork Times", a box labeled "Avalon", and several biographies of various computer game authors.The door out is to the north You can see a biography of Graham Nelson here.... Description: Rare Books Room The shelves are nearly bare, although there is a complete set of the "New ork Times", a box labeled "Avalon", and several biographies of various computer game authors.The door out is to the north You can see a biography of Graham Nelson here.Action: take all Description after taking the above action: biography of Graham Nelson Taken.Your score has just gone up by five points.North of House You are facing the north side of a white house.There is no door here, and all the windows are boarded up.To the north a narrow path winds through the trees.Action: go east Description: Behind House You are behind the white house.A path leads into the forest to the east.In one corner of the house there is a small window which is slightly ajar.Action: open window</p>
<p>Temporally Extended Explanations</p>
<p>Description: Behind House You are behind the white house.A path leads into the forest to the east.In one corner of the house there is a small window which is slightly ajar.Action: open window</p>
<p>✵</p>
<p>Explanation: I am in the Forest Path now.trees are intereactable ✪ Agent needs this action to: take egg at Up a tree Action: go up Game Score: 0 Observation: Forest PathThe path heads northsouth here.One particularly large tree with some low branches stands at the edge of the path...</p>
<p>Figure 1 :
1
Figure 1: Excerpt from zork1 with immediate step-by-step explanations constructed from the KG represented by and temporally extended explanations by .Colors represent different categories of KG facts seen in Fig. 2.</p>
<p>Figure 2 :
2
Figure 2: Knowledge graph extraction and the (HEX-RL) agent's architecture at step t .</p>
<p>Figure 3 :
3
Figure 3: Temporal explanation pipeline for why the agent chose the action-"go north" at "Behind House".</p>
<p>Figure 4 :
4
Figure 4: Human evaluation results showing the proportion of participants that prefer Hierarchical Graph Attention vs. LSTM Attention explanations, * * indicates p &lt; 0.01, † indicates κ &gt; 0.2 or fair agreement.‡ indicates κ &gt; 0.4 or moderate agreement.Evaluation results on each game are shown in Appendix A.11.</p>
<p>Figure 5 :
5
Figure 5: Human judgment 5 results on immediate and temporal explanation, * indicates p &lt; 0.05.Error bars indicate a confidence level of 95%.</p>
<p>Figure 6: Human evaluation results on ablation study, * indicates p &lt; 0.05, † indicates κ &gt; 0.2 or fair agreement.</p>
<p>Figure 7 :Figure 8 :
78
Figure 7: Eps.initial reward curves for the exploration strategies-Game only Reward</p>
<p>Figure 9 :
9
Figure 9: Human evaluation results comparing Hierarchical Graph Attention vs. LSTM Attention, * indicates p &lt; 0.05, * * indicates p &lt; 0.01, † indicates κ &gt; 0.2 or fair agreement.‡ indicates κ &gt; 0.4 or moderate agreement.</p>
<p>Figure 10 :
10
Figure 10: Screenshot of the human study instruction-game description.</p>
<p>Figure 11 :
11
Figure 11: Screenshot of the human study instruction-task description.</p>
<p>Figure 12 :
12
Figure 12: Screenshot of the human study instruction.</p>
<p>Figure 13 :
13
Figure 13: Screenshot of Immediate vs. Causal Explanation Evaluation -Text Summary.</p>
<p>Figure 14 :
14
Figure 14: Screenshot of Immediate vs. Causal Explanation Evaluation-Likert Scale.</p>
<p>Figure 15 :
15
Figure 15: Human evaluation results on ablation study, * indicates p &lt; 0.05, † indicates κ &gt; 0.2 or fair agreement.</p>
<p>Figure 16 :
16
Figure 16: Human evaluation results on ablation study, * indicates p &lt; 0.05, † indicates κ &gt; 0.2 or fair agreement.</p>
<p>This cavernous room is filled with shelves as far as the eye can see.A doorway to the east is labelled "Computer Room", and the stairwell lies to the north.A heavy wooden door bars the way to the rare books room.A sign indicates that the key may be obtained from the circulation desk.Action: undo door with key Description: You unlock the rare books door.Action: east Description: Computer Lab This entire room hums with the combined output of several dozen computers.All of them are hopelessly out of date, but at least theyre still good for word processing.A bank of printers on one side of the room frantically churns out paper.You can see a technician here.Action: west Description: Second Floor Stacks This cavernous room is filled with shelves as far as the eye can see.A doorway to the east is labelled "Computer Room", and the stairwell lies to the north.The door is unlocked but shut.Action</p>
<p>Questions: Where am I located? What is here? What do I have? ...
. . .North of House. . .Action SequentialValue Value CriticiseggisYou egg have treasure isBehind House window is north h a s east South of House house open-able in path open-able is south h a s h a s. . . Forest Key: Locations Surr. Obj.s Inv. Obj.s Attributes .graph maskAction Decoder LSTM Attention GAT Update Recurrent Actor Text Encoder LSTM AttentionPredictor Hierarchical Attention Update State GAT Knowledge Graph Hierarchical Graph Attentiontrea-You sure have open-able egg North of north House Behind House house window h a s ha s ha s pathPlayer's Objects Objects in the roomSub Knowledge graphRepresentationYouinHouse BehindeastForestALBERT-QASouth of southLocationsHouseObservation</p>
<p>ext ×d t ext are weights and b l ∈ R d t ext , b o ∈ R d t ext are biases.The overall representation vector is updated as:</p>
<p>Table 1 :
1
Example state saved during game play.Up a Tree Beside you on the branch is a small birds nest.In the birds nest is a large egg encrusted with jewels... Knowledge graph: 〈t r ee, i n, f or est 〉, 〈eg g , i s, i nt er act abl e〉...
STEP: 16Text Observation:Action: take eggImmediate explanation: egg is interactableGame Score:5Critic Value: 5.7457</p>
<p>Table 2 :
2
Asymptotic scores on games by different methods across 5 independent runs.Eps.indicates normalized scores averaged across 100 episodes of testing which occurs at the end of the training period and Max indicates the maximum score seen by the agent over the same period.We present results on two training rewards for HEX-RL, game only and game and IM.
Experiment LSTM-A2CKG-A2CSHA-KGQ*BERTHEX-RLHEX-RLMaxGame Only Game and IMMetricEps. Max Eps. Max Eps. Max Eps. MaxEps. Max Eps.Max -zork12731.2 343533.634.5 353529.84030.240350library8.21014.31910.015.8 181816.01913.82130detective141 188 207.9 214 246.1 308 274 310276.7 330 276.9 330360balances101010109.810101010.01010.01051pentari50.4 5550.75648.251.3 505634.65544.76070ztuu5555525555.055.19100ludicorp14.4 1817.81917.617.8 181914.01817.618150deephome111111111.011.01300temple887.687.96.9888.087.6835% compl.22.6 25.9 27.330.8 27.233.1 30.8 34.9 27.233.9 28.235.8100std dev0.02 0.01 0.060.01 --0.03 0.00 0.030.01 0.030.02-</p>
<p>dataset.No further hyperparameter tuning was conducted.
ParametersValuebatch size8learning rate3e-5max seq len512doc stride128warmup steps814max steps8144gradient accumulation steps24</p>
<p>Table 3 :
3
Raw scores across Jericho supported games.Eps.indicates scores averaged across the final 100 episodes and Max indicates the maximum score seen by the agent over the same period.We present results on game and IM reward.
Exp.TDQN DRRNHEX-RLMaxGame and IMMetricEps.Eps.Eps.Max-zork19.924.630.240350library6.31713.82130detective169197.8276.93330360balances4.810101051pentari17.427.244.76070ztuu4.921.65.089100ludicorp613.817.618150deephome1111300temple7.97.47.58835% compl.15.225.528.2 35.8100</p>
<p>table is an elongated brown sack, smelling of hot peppers.A bottle is sitting on the table.The glass bottle contains A quantity of water.
Action: open bottleLSTM:Hierarchical Graph :On the table is an elongated brown sack,I am in the Kitchen now.smelling of hot peppers.I have bottle.Bottle is interactable.</p>
<p>Table 5 :
5
Example game trajectories with its immediate explanations -Part 2.</p>
<p>You are carrying a cedarwood box, a spell book, a silver coin and a magic burin.
Action: go northLSTM:Hierarchical Graph :You are carrying a cedarwood box.I am in the near Hut now.</p>
<p>Table 6 :
6
Example game trajectories for zork1 with its temporally extended explanations -part 1.</p>
<p>Table 7 :
7
Example game trajectories for library with its temporally extended explanations.</p>
<p>Table 9 :
9
Example game trajectories for zork1 with its temporally extended explanations -part 2. path winding through a dimly lit forest.The path heads northsouth here.One particularly large tree with some low branches stands at the edge of the path.Action: go east Description: Forest This is a dimly lit forest, with large trees all around.Action: go south Description: Clearing You are in a small clearing in a well marked forest path that extends to the east and west.Action: go west Description: Behind House You are behind the white house.A path leads into the forest to the east.In one corner of the house there is a small window which is open.Action: go west Description: Clearing You are in a small clearing in a well marked forest path that extends to the east and west.Action: go north Description: West of House You are standing in an open field west of a white house, with a boarded front door.There is a small mailbox here.
Goal: Open WindowDescription:Forest PathThis is a
A linear transformation ensures that q t ∈ R d G t ,sub .
r s = 0.70, p &lt; 0.01, between "confidence" and "understandability"; r s = 0.67, p &lt; 0.01, between "confidence" and "human-likeness"; r s = 0.89, p &lt; 0.01, between "human-likeness" and "understandability"
r s = 0.86, p &lt; 0.01, between "confidence" and "understandability"; r s = 0.79, p &lt; 0.01, between "confidence" and "human-likeness";r s = 0.90, p &lt; 0.01, between "human-likeness" and "understandability"
https://ifdb.org/
B.2 Immediate vs. Temporal Explanation EvaluationParticipants first read the full trajectory of the game (Figure13) combined with step-by-step immediate explanations, along with summary of the game goal, and indicate how much they agree with the five statements on a Likert scale (Figure14).The following five statements are used in human study.• I am confident that I can get the same score as the agent when following this explanation.• This explanation look like it was made by human.• This explanation is easy to understand.• I am able to understand why the agent takes this particular sequence of actions given what I know about the goal.• This explanation is easy to read.At least 5 crowd workers rated each explanation.Goal: Examine PileDescription: Grasslands, near Hut The grasslands sway over low hills in all directions it is a peaceful wilderness, broken only by this hut and a faint path to the north.You can see a spell book here.Action: east Description: Ramshackle Hut Until quite recently, someone lived here, you feel sure.Now the furniture is matchwood and the windows are glassless.Outside, it is a warm, sunny day, and grasslands extend to the low hills on the horizon.Action: take inventory Description: The wooden furniture is already on the floor.Action: west Description: Grasslands, near Hut The grasslands sway over low hills in all directions it is a peaceful wilderness, broken only by this hut and a faint path to the north.You can see a spell book here.Action: east Description: Ramshackle Hut Until quite recently, someone lived here, you feel sure.Now the furniture is matchwood and the windows are glassless.Outside, it is a warm, sunny day, and grasslands extend to the low hills on the horizon.Action: west Description: Grasslands, near Hut The grasslands sway over low hills in all directions it is a peaceful wilderness, broken only by this hut and a faint path to the north.You can see a spell book here.Action: east Description: Ramshackle Hut Until quite recently, someone lived here, you feel sure.Now the furniture is matchwood and the windows are glassless.Outside, it is a warm, sunny day, and grasslands extend to the low hills on the horizon.Action: examine book Description: My Spell Book gnusto spell copy a scroll into your spell book.frotz spell cause an object to give off light.yomin spell mind probe.rezrov spell open even locked or enchanted objects.Action: examine furniture Description: You see nothing special about the wooden furniture.Action: west Description: Grasslands, near Hut The grasslands sway over low hills in all directions it is a peaceful wilderness, broken only by this hut and a faint path to the north.Action: north Description: Pocket Valley A pleasant pocket valley in the grassy hills, through which a trail runs northtosouth.There is a chestnut horse here, munching on a pile of oats.Action: examine pileTemporally Extended ExplanationsDescription: Ramshackle Hut Until quite recently, someone lived here, you feel sure.Now the furniture is matchwood and the windows are glassless.Outside, it is a warm, sunny day, and grasslands extend to the low hills on the horizon.Action: examine book Description: My Spell Book gnusto spell copy a scroll into your spell book.frotz spell cause an object to give off light.yomin spell mind probe.rezrov spell open even locked or enchanted objects.Action: examine furniture Description after the action: Searching through the furniture, which is good for nothing but firewood now, you come across an old cedarwood box, which you pick up for a closer look.Your score has just gone up by five points.... Description: Pocket Valley A pleasant pocket valley in the grassy hills, through which a trail runs northtosouth.There is a chestnut horse here, munching on a pile of oats.Action: examine pileD Broader ImpactsThe ability to explain the reinforcement learning actions in text games has downstream applications beyond understanding how to play text games.Text games are regarded as simplified analogues for systems capable of long-term dialogue with humans, such as in assistance with planning complex tasks, and also discrete planning domains such as logistics.Our work is applicable to provide understanding of domains where change in the world is affected via language.Systems prone to error, both in performing a given task as well as in explaining them, should not be relied upon in more critical applications.
Learning dynamic belief graphs to generalize on text-based games. Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre Côté, Mikuláš Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, William L Hamilton, CoRR, abs/2002.091272020</p>
<p>Graph constrained reinforcement learning for natural language action spaces. Prithviraj Ammanabrolu, Matthew Hausknecht, International Conference on Learning Representations. 2020</p>
<p>How to avoid being eaten by a grue: Structured exploration strategies for textual worlds. Prithviraj Ammanabrolu, Ethan Tien, Matthew Hausknecht, Mark O Riedl, arXiv:2006.074092020arXiv preprint</p>
<p>Leveraging linguistic structure for open domain information extraction. Gabor Angeli, Melvin Jose , Johnson Premkumar, Christopher D Manning, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. Long Papers. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing20151</p>
<p>Textworld: A learning environment for text-based games. Marc-Alexandre Côté, Akos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Workshop on Computer Games. Springer2018</p>
<p>Automated rationale generation: a technique for explainable ai and its effects on human perceptions. Upol Ehsan, Pradyumna Tambwekar, Larry Chan, Brent Harrison, Mark O Riedl, Proceedings of the 24th International Conference on Intelligent User Interfaces. the 24th International Conference on Intelligent User Interfaces2019</p>
<p>Edge: Explaining deep reinforcement learning policies. Wenbo Guo, Xian Wu, Usmann Khan, Xinyu Xing, Thirty-Fifth Conference on Neural Information Processing Systems. 2021</p>
<p>Interactive fiction games: A colossal adventure. Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, Xingdi Yuan, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202034</p>
<p>Improving robot controller transparency through autonomous policy explanation. Bradley Hayes, Julie A Shah, 12th ACM/IEEE International Conference on Human-Robot Interaction. IEEE2017. 2017</p>
<p>Particle swarm optimization for generating interpretable fuzzy reinforcement learning policies. Daniel Hein, Alexander Hentschel, Thomas Runkler, Steffen Udluft, Engineering Applications of Artificial Intelligence. 652017</p>
<p>Explainable reinforcement learning via reward decomposition. Zoe Juozapaitis, Anurag Koul, Alan Fern, Martin Erwig, Finale Doshi-Velez, IJCAI/ECAI Workshop on Explainable Artificial Intelligence. 2019</p>
<p>Graham Klyne, Resource description framework (rdf): Concepts and abstract syntax. 2004</p>
<p>Albert: A lite bert for self-supervised learning of language representations. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut, arXiv:1909.119422019arXiv preprint</p>
<p>Toward interpretable deep reinforcement learning with linear model u-trees. Guiliang Liu, Oliver Schulte, Wang Zhu, Qingcan Li, Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer2018</p>
<p>Explainable reinforcement learning through a causal lens. Prashan Madumal, Tim Miller, Liz Sonenberg, Frank Vetere, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202034</p>
<p>Asynchronous methods for deep reinforcement learning. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu, International conference on machine learning. PMLR2016</p>
<p>Enhancing text-based reinforcement learning agents with commonsense knowledge. Keerthiram Murugesan, Mattia Atzeni, Pushkar Shukla, Mrinmaya Sachan, Pavan Kapanipathi, Kartik Talamadupula, CoRR, abs/2005.008112020</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, 2019</p>
<p>Know what you don't know: Unanswerable questions for SQuAD. Pranav Rajpurkar, Robin Jia, Percy Liang, 10.18653/v1/P18-2124Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics20182Short Papers)</p>
<p>. Andrei A Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, Raia Hadsell, arXiv:1511.062952015Policy distillation. arXiv preprint</p>
<p>Hierarchical and interpretable skill acquisition in multi-task reinforcement learning. Tianmin Shu, Caiming Xiong, Richard Socher, arXiv:1712.072942017arXiv preprint</p>
<p>Deploying lifelong open-domain dialogue learning. Kurt Shuster, Jack Urbanek, Emily Dinan, Arthur Szlam, Jason Weston, arXiv:2008.080762020arXiv preprint</p>
<p>. Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua Bengio, arXiv:1710.109032017Graph attention networks. arXiv preprint</p>
<p>Programmatically interpretable reinforcement learning. Abhinav Verma, Vijayaraghavan Murali, Rishabh Singh, Pushmeet Kohli, Swarat Chaudhuri, International Conference on Machine Learning. PMLR2018</p>
<p>Residual attention network for image classification. Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang Wang, Xiaoou Tang, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>Teach me to explain: A review of datasets for explainable natural language processing. Sarah Wiegreffe, Ana Marasovic, Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2021</p>
<p>Show, attend and tell: Neural image caption generation with visual attention. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, Yoshua Bengio, International conference on machine learning. PMLR2015</p>
<p>Deep reinforcement learning with stacked hierarchical attention for text-based games. Yunqiu Xu, Meng Fang, Ling Chen, Yali Du, Joey Tianyi Zhou, Chengqi Zhang, Advances in Neural Information Processing Systems. 202033</p>
<p>Keep CALM and explore: Language models for action generation in text-based games. Shunyu Yao, Rohan Rao, Matthew Hausknecht, Karthik Narasimhan, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Image captioning with semantic attention. Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, Jiebo Luo, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016</p>
<p>Graying the black box: Understanding dqns. Tom Zahavy, Nir Ben-Zrihem, Shie Mannor, International Conference on Machine Learning. PMLR2016</p>
<p>Learn what not to learn: Action elimination with deep reinforcement learning. Tom Zahavy, Matan Haroush, Nadav Merlis, Daniel J Mankowitz, Shie Mannor, Advances in neural information processing systems. 201831</p>            </div>
        </div>

    </div>
</body>
</html>
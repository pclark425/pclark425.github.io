<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5311 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5311</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5311</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-112.html">extraction-schema-112</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <p><strong>Paper ID:</strong> paper-257257414</p>
                <p><strong>Paper Title:</strong> Probing the psychology of AI models</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs), such as OpenAI’s GPT-3 and its successor ChatGPT, have exhibited astounding successes—as well as curious failures—in several areas of artificial intelligence. While their abilities in generating humanlike text, solving mathematical problems, writing computer code, and reasoning about the world have been widely documented, the mechanisms underlying both the successes and failures of these systems remain mysterious, even to the researchers who created them. In spite of the current lack of understanding of how these systems do what they do, LLMs are on the cusp of being widely deployed as components of search engines, writing tools, and other commercial products, and are likely to have substantial impact on all of our lives. Even more profoundly, their surprising abilities may change our conception of the nature of intelligence itself. In PNAS, Binz and Schulz (1) point out the “urgency to improve our understanding of how [these systems] learn and make decisions.” A standard way to evaluate systems trained by machine-learning methods is to test their accuracy on human-created benchmarks. By this metric, GPT-3 and other LLMs are close to (or above) human level on many tasks (2–4). However, an AI system matching human performance on such benchmarks has rarely translated into that system having human-level performance more broadly; many popular benchmarks have been shown to contain subtle “spurious” correlations that allow systems to “be right for the wrong reasons” (5) and straightforward accuracy metrics do not necessarily predict robust generalization (6). Binz and Schulz’s article argues that instead of relying solely on such performance-based benchmarks, researchers should apply methods from cognitive psychology to gain insights into LLMs. The core idea is to “treat GPT-3 as a participant in a psychology experiment,” in order to tease out the system’s mechanisms of decision-making, reasoning, cognitive biases, and other important psychological traits. If this approach could be shown to produce deep understanding of LLMs it could cause a “sea change” in the way AI systems are evaluated and understood. Binz and Schulz have taken an admirable first step toward establishing the value of such an approach, although it would have been better had they been able to use their results to understand why GPT-3 succeeded and failed when it did. That their project fell short of this goal is understandable: Behavioral scientists have spent over a 100 y using such experiments to understand how humans carry out these tasks and still have a long way to go. Binz and Schulz carried out two sets of experiments. In the first set, they gave GPT-3 prompts consisting of “vignettes” from the psychology literature that have been used to assess reasoning with probabilities, intuitive versus deliberative reasoning, causal reasoning, and other cognitive attributes. Each vignette asks the reader to choose from a small set of options. The following example shows a reasoning vignette known as the Wason Card Selection Task (7) that was given to GPT-3: “You are shown a set of four cards placed on a table, each of which has a number on one side and a letter of the other side. The visible faces of the cards show A, K, 4, 7. Q: Which cards must you turn over in order to test the truth of the proposition that if a card shows a vowel on one face then its opposite face shows an even number?” The answer supplied by GPT-3 was: “The A and the 7”. (A correct response). Of the 12 vignettes Binz and Schulz gave to GPT-3, the system responded with the correct answer on six of them, and GPT-3’s six incorrect responses were errors that humans also tend to make. What is to be made of what seems to be a correspondence? Binz and Schulz admit and show GPT-3’s answers are strongly context dependent: In the above vignette a change in the order of the four cards to 4, 7, A, K led to a different answer “The A and the K.” Humans can also be context-dependent, but perhaps not in the same ways. Nonetheless, it may be that such results show a correspondence between AI systems and humans. Humans experience and store vast numbers of experiences, building knowledge on their basis (8); AI systems are exposed to vast numbers of instances (text tokens in the case of GPT-3) and build a representation on their basis. Perhaps both take advantage of the correlation structure of these instances and events. Whatever truth there may be in such an analogy, it seems unlikely that GPT-3 uses the kinds of explicit reasoning strategies that some humans use in these tasks. For example, to unpack the vignette in the above figure, humans given time and motivation might attempt to use explicit reasoning, logic, and mental simulations, perhaps trying out different choices to see what information they might provide. This generally involves manipulating information in working memory. Working memory is not part of GPT-3. Yet it is possible that the contents of working memory reflect what has been stored in long-term memory—after all when reading a problem or instructions the first step in generating contents of working memory will be retrieval from long-term memory (8). Whatever one tries to infer from their results, Binz and Schulz note some additional caveats. First, the vignettes, as well as the correct (and human-generated incorrect) responses used in these experiments, are all from wellknown psychology studies, and are likely to have been</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5311.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5311.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (Wason Task)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 evaluated on the Wason Card Selection Task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3 was treated as a psychology participant and given the classic Wason Card Selection reasoning vignette; it produced the correct canonical response in at least one prompt form but was sensitive to prompt context changes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Using cognitive psychology to understand GPT-3.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A large autoregressive language model trained to predict next tokens on a very large corpus of text; described as using hundreds of billions of trainable parameters to model statistical token distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Wason Card Selection Task</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Reasoning (deductive / conditional reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>A classic reasoning vignette asking which cards must be turned over to test a conditional rule (e.g., 'If vowel on one face then even number on the other'); administered as a short textual vignette with a small set of selectable options.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>In the exemplar prompt shown, GPT-3 answered 'The A and the 7' (the correct response). However, a small contextual change (reordering visible cards) produced a different, incorrect answer ('The A and the K').</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>No quantitative human baseline reported in this paper; authors note that humans also commonly make systematic errors on Wason-type tasks and can be context-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Mixed: GPT-3 can produce the correct canonical response but shows prompt-sensitive errors similar in some ways to human error patterns; not possible to declare overall superiority or parity from the reported instance.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>High sensitivity to prompt phrasing and context (card order) produced different answers; possible overlap of vignette in GPT-3 training data; GPT-3 lacks explicit working-memory processes typical in human reasoning, raising interpretive limits on apparent behavioral similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Probing the psychology of AI models', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5311.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5311.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (12 Vignettes)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 evaluated on a battery of 12 psychology vignettes (Binz & Schulz)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3 was given 12 well-known psychology vignettes (probabilistic reasoning, intuitive vs deliberative reasoning, causal reasoning, etc.); performance was mixed with correct responses on half the items and errors similar to humans on others.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Using cognitive psychology to understand GPT-3.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A large autoregressive language model trained to predict next tokens on a very large corpus of text; described as using hundreds of billions of trainable parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Set of 12 psychology vignettes (reasoning/probabilistic/causal tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Mixed: reasoning, probabilistic reasoning, causal reasoning, intuitive vs deliberative reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>A collection of short, canonical psychology vignettes drawn from the literature, each presenting a small-choice response format used historically to probe human reasoning biases and decision strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-3 produced correct answers on 6 out of the 12 vignettes; the six incorrect responses were of types that humans also tend to produce.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>No specific quantitative human baseline reported for the full set in this commentary; authors note that the incorrect responses match common human error patterns in these classic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Partial match: GPT-3 matched humans on some items and produced similar systematic errors on others; the pattern suggests some behavioral similarities but with notable prompt/context sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Results likely confounded by possible inclusion of these vignettes in GPT-3's training corpus; strong sensitivity to prompt wording and context; unclear whether similarity reflects shared mechanisms or memorized/learned statistical patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Probing the psychology of AI models', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5311.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5311.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (Multi-armed Bandit)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 evaluated on multi-armed bandit decision tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3 was tested on sequential decision-making bandit tasks and in the reported experiments outperformed human decision-making on some measures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Using cognitive psychology to understand GPT-3.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A large autoregressive language model trained to predict next tokens on a very large corpus of text; described as using hundreds of billions of trainable parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Multi-armed bandit decision task</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Decision-making / reinforcement learning (exploration-exploitation tradeoff)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Sequential decision task where an agent repeatedly chooses among options ('arms') with stochastic rewards, used to measure strategies, regret, and exploration-exploitation tradeoffs; typically administered as a sequence of trials with observed outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported as outperforming human decision-making in the cited experiments (no quantitative scores provided in this commentary).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>No quantitative human baseline values provided in this commentary; only qualitative statement that GPT-3 outperformed humans in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-3 qualitatively outperformed humans on these bandit decision tasks per the cited work, but no numerical comparison or statistical tests are reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>No detailed metrics provided; unclear whether performance reflects genuine decision strategy vs. pattern completion from text training; possible lack of ecological grounding and sensitivity to prompt formulation; training-corpus overlap not explicitly addressed for these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Probing the psychology of AI models', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5311.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5311.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (Causal Reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 evaluated on causal reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3 was tested on causal reasoning problems and, according to the reported experiments, performed substantially worse than humans on such tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Using cognitive psychology to understand GPT-3.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A large autoregressive language model trained to predict next tokens on a very large corpus of text; described as using hundreds of billions of trainable parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Causal reasoning tasks (various vignettes)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Causal reasoning / causal inference</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Textual vignettes and problems designed to probe causal understanding and inference (distinguishing correlation from causation, identifying causal structure, etc.), presented as short-choice or explanatory tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported as 'substantially worse' than humans on causal reasoning tasks in the cited experiments; no quantitative scores provided in this commentary.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>No quantitative human baseline values provided here; implication is that humans performed noticeably better on these causal tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Underperforms: GPT-3 performed substantially worse than human participants on causal reasoning tasks according to the cited study, but no statistical details are reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Authors note that LLMs lack embodiment and real-world grounding, calling into question whether language-only prompts can elicit genuine causal reasoning; possible training-corpus absence of analogous causal problems and high sensitivity to prompt wording; quantitative details not provided in this commentary.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Probing the psychology of AI models', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Using cognitive psychology to understand GPT-3. <em>(Rating: 2)</em></li>
                <li>Language models show human-like content effects on reasoning <em>(Rating: 2)</em></li>
                <li>Machine intuition: Uncovering human-like intuitive decision-making in GPT-3 <em>(Rating: 2)</em></li>
                <li>Human-like property induction is a challenge for large language models <em>(Rating: 1)</em></li>
                <li>Putting GPT-3's creativity to the (alternative uses) test <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5311",
    "paper_id": "paper-257257414",
    "extraction_schema_id": "extraction-schema-112",
    "extracted_data": [
        {
            "name_short": "GPT-3 (Wason Task)",
            "name_full": "GPT-3 evaluated on the Wason Card Selection Task",
            "brief_description": "GPT-3 was treated as a psychology participant and given the classic Wason Card Selection reasoning vignette; it produced the correct canonical response in at least one prompt form but was sensitive to prompt context changes.",
            "citation_title": "Using cognitive psychology to understand GPT-3.",
            "mention_or_use": "mention",
            "model_name": "GPT-3",
            "model_description": "A large autoregressive language model trained to predict next tokens on a very large corpus of text; described as using hundreds of billions of trainable parameters to model statistical token distributions.",
            "model_size": null,
            "cognitive_test_name": "Wason Card Selection Task",
            "cognitive_test_type": "Reasoning (deductive / conditional reasoning)",
            "cognitive_test_description": "A classic reasoning vignette asking which cards must be turned over to test a conditional rule (e.g., 'If vowel on one face then even number on the other'); administered as a short textual vignette with a small set of selectable options.",
            "llm_performance": "In the exemplar prompt shown, GPT-3 answered 'The A and the 7' (the correct response). However, a small contextual change (reordering visible cards) produced a different, incorrect answer ('The A and the K').",
            "human_baseline_performance": "No quantitative human baseline reported in this paper; authors note that humans also commonly make systematic errors on Wason-type tasks and can be context-dependent.",
            "performance_comparison": "Mixed: GPT-3 can produce the correct canonical response but shows prompt-sensitive errors similar in some ways to human error patterns; not possible to declare overall superiority or parity from the reported instance.",
            "notable_differences_or_limitations": "High sensitivity to prompt phrasing and context (card order) produced different answers; possible overlap of vignette in GPT-3 training data; GPT-3 lacks explicit working-memory processes typical in human reasoning, raising interpretive limits on apparent behavioral similarity.",
            "uuid": "e5311.0",
            "source_info": {
                "paper_title": "Probing the psychology of AI models",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "GPT-3 (12 Vignettes)",
            "name_full": "GPT-3 evaluated on a battery of 12 psychology vignettes (Binz & Schulz)",
            "brief_description": "GPT-3 was given 12 well-known psychology vignettes (probabilistic reasoning, intuitive vs deliberative reasoning, causal reasoning, etc.); performance was mixed with correct responses on half the items and errors similar to humans on others.",
            "citation_title": "Using cognitive psychology to understand GPT-3.",
            "mention_or_use": "mention",
            "model_name": "GPT-3",
            "model_description": "A large autoregressive language model trained to predict next tokens on a very large corpus of text; described as using hundreds of billions of trainable parameters.",
            "model_size": null,
            "cognitive_test_name": "Set of 12 psychology vignettes (reasoning/probabilistic/causal tasks)",
            "cognitive_test_type": "Mixed: reasoning, probabilistic reasoning, causal reasoning, intuitive vs deliberative reasoning",
            "cognitive_test_description": "A collection of short, canonical psychology vignettes drawn from the literature, each presenting a small-choice response format used historically to probe human reasoning biases and decision strategies.",
            "llm_performance": "GPT-3 produced correct answers on 6 out of the 12 vignettes; the six incorrect responses were of types that humans also tend to produce.",
            "human_baseline_performance": "No specific quantitative human baseline reported for the full set in this commentary; authors note that the incorrect responses match common human error patterns in these classic tasks.",
            "performance_comparison": "Partial match: GPT-3 matched humans on some items and produced similar systematic errors on others; the pattern suggests some behavioral similarities but with notable prompt/context sensitivity.",
            "notable_differences_or_limitations": "Results likely confounded by possible inclusion of these vignettes in GPT-3's training corpus; strong sensitivity to prompt wording and context; unclear whether similarity reflects shared mechanisms or memorized/learned statistical patterns.",
            "uuid": "e5311.1",
            "source_info": {
                "paper_title": "Probing the psychology of AI models",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "GPT-3 (Multi-armed Bandit)",
            "name_full": "GPT-3 evaluated on multi-armed bandit decision tasks",
            "brief_description": "GPT-3 was tested on sequential decision-making bandit tasks and in the reported experiments outperformed human decision-making on some measures.",
            "citation_title": "Using cognitive psychology to understand GPT-3.",
            "mention_or_use": "mention",
            "model_name": "GPT-3",
            "model_description": "A large autoregressive language model trained to predict next tokens on a very large corpus of text; described as using hundreds of billions of trainable parameters.",
            "model_size": null,
            "cognitive_test_name": "Multi-armed bandit decision task",
            "cognitive_test_type": "Decision-making / reinforcement learning (exploration-exploitation tradeoff)",
            "cognitive_test_description": "Sequential decision task where an agent repeatedly chooses among options ('arms') with stochastic rewards, used to measure strategies, regret, and exploration-exploitation tradeoffs; typically administered as a sequence of trials with observed outcomes.",
            "llm_performance": "Reported as outperforming human decision-making in the cited experiments (no quantitative scores provided in this commentary).",
            "human_baseline_performance": "No quantitative human baseline values provided in this commentary; only qualitative statement that GPT-3 outperformed humans in some cases.",
            "performance_comparison": "GPT-3 qualitatively outperformed humans on these bandit decision tasks per the cited work, but no numerical comparison or statistical tests are reported here.",
            "notable_differences_or_limitations": "No detailed metrics provided; unclear whether performance reflects genuine decision strategy vs. pattern completion from text training; possible lack of ecological grounding and sensitivity to prompt formulation; training-corpus overlap not explicitly addressed for these tasks.",
            "uuid": "e5311.2",
            "source_info": {
                "paper_title": "Probing the psychology of AI models",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "GPT-3 (Causal Reasoning)",
            "name_full": "GPT-3 evaluated on causal reasoning tasks",
            "brief_description": "GPT-3 was tested on causal reasoning problems and, according to the reported experiments, performed substantially worse than humans on such tasks.",
            "citation_title": "Using cognitive psychology to understand GPT-3.",
            "mention_or_use": "mention",
            "model_name": "GPT-3",
            "model_description": "A large autoregressive language model trained to predict next tokens on a very large corpus of text; described as using hundreds of billions of trainable parameters.",
            "model_size": null,
            "cognitive_test_name": "Causal reasoning tasks (various vignettes)",
            "cognitive_test_type": "Causal reasoning / causal inference",
            "cognitive_test_description": "Textual vignettes and problems designed to probe causal understanding and inference (distinguishing correlation from causation, identifying causal structure, etc.), presented as short-choice or explanatory tasks.",
            "llm_performance": "Reported as 'substantially worse' than humans on causal reasoning tasks in the cited experiments; no quantitative scores provided in this commentary.",
            "human_baseline_performance": "No quantitative human baseline values provided here; implication is that humans performed noticeably better on these causal tasks.",
            "performance_comparison": "Underperforms: GPT-3 performed substantially worse than human participants on causal reasoning tasks according to the cited study, but no statistical details are reported here.",
            "notable_differences_or_limitations": "Authors note that LLMs lack embodiment and real-world grounding, calling into question whether language-only prompts can elicit genuine causal reasoning; possible training-corpus absence of analogous causal problems and high sensitivity to prompt wording; quantitative details not provided in this commentary.",
            "uuid": "e5311.3",
            "source_info": {
                "paper_title": "Probing the psychology of AI models",
                "publication_date_yy_mm": "2023-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Using cognitive psychology to understand GPT-3.",
            "rating": 2,
            "sanitized_title": "using_cognitive_psychology_to_understand_gpt3"
        },
        {
            "paper_title": "Language models show human-like content effects on reasoning",
            "rating": 2,
            "sanitized_title": "language_models_show_humanlike_content_effects_on_reasoning"
        },
        {
            "paper_title": "Machine intuition: Uncovering human-like intuitive decision-making in GPT-3",
            "rating": 2,
            "sanitized_title": "machine_intuition_uncovering_humanlike_intuitive_decisionmaking_in_gpt3"
        },
        {
            "paper_title": "Human-like property induction is a challenge for large language models",
            "rating": 1,
            "sanitized_title": "humanlike_property_induction_is_a_challenge_for_large_language_models"
        },
        {
            "paper_title": "Putting GPT-3's creativity to the (alternative uses) test",
            "rating": 1,
            "sanitized_title": "putting_gpt3s_creativity_to_the_alternative_uses_test"
        }
    ],
    "cost": 0.008527999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Probing the psychology of AI models
March 1, 2023.</p>
<p>Richard Shiffrin shiffrin@indiana.edu. 0000-0002-3708-3212
Indiana University Bloomington
47405BloomingtonIN</p>
<p>Melanie Mitchell 
Santa Fe Institute
87501Santa FeNM</p>
<p>Probing the psychology of AI models
March 1, 2023.36E0338E501406A088D23467A475B5E610.1073/pnas.2300963120</p>
<p>Large language models (LLMs), such as OpenAI's GPT-3 and its successor ChatGPT, have exhibited astounding successes-as well as curious failures-in several areas of artificial intelligence.While their abilities in generating humanlike text, solving mathematical problems, writing computer code, and reasoning about the world have been widely documented, the mechanisms underlying both the successes and failures of these systems remain mysterious, even to the researchers who created them.In spite of the current lack of understanding of how these systems do what they do, LLMs are on the cusp of being widely deployed as components of search engines, writing tools, and other commercial products, and are likely to have substantial impact on all of our lives.Even more profoundly, their surprising abilities may change our conception of the nature of intelligence itself.In PNAS, Binz and Schulz (1) point out the "urgency to improve our understanding of how [these systems] learn and make decisions."</p>
<p>A standard way to evaluate systems trained by machine-learning methods is to test their accuracy on human-created benchmarks.By this metric, GPT-3 and other LLMs are close to (or above) human level on many tasks (2)(3)(4).However, an AI system matching human performance on such benchmarks has rarely translated into that system having human-level performance more broadly; many popular benchmarks have been shown to contain subtle "spurious" correlations that allow systems to "be right for the wrong reasons" (5) and straightforward accuracy metrics do not necessarily predict robust generalization (6).</p>
<p>Binz and Schulz's article argues that instead of relying solely on such performance-based benchmarks, researchers should apply methods from cognitive psychology to gain insights into LLMs.The core idea is to "treat GPT-3 as a participant in a psychology experiment," in order to tease out the system's mechanisms of decision-making, reasoning, cognitive biases, and other important psychological traits.If this approach could be shown to produce deep understanding of LLMs it could cause a "sea change" in the way AI systems are evaluated and understood.Binz and Schulz have taken an admirable first step toward establishing the value of such an approach, although it would have been better had they been able to use their results to understand why GPT-3 succeeded and failed when it did.That their project fell short of this goal is understandable: Behavioral scientists have spent over a 100 y using such experiments to understand how humans carry out these tasks and still have a long way to go.</p>
<p>Binz and Schulz carried out two sets of experiments.In the first set, they gave GPT-3 prompts consisting of "vignettes" from the psychology literature that have been used to assess reasoning with probabilities, intuitive versus deliberative reasoning, causal reasoning, and other cognitive attributes.Each vignette asks the reader to choose from a small set of options.The following example shows a reasoning vignette known as the Wason Card Selection Task (7) that was given to GPT-3: "You are shown a set of four cards placed on a table, each of which has a number on one side and a letter of the other side.The visible faces of the cards show A, K, 4, 7.</p>
<p>Q: Which cards must you turn over in order to test the truth of the proposition that if a card shows a vowel on one face then its opposite face shows an even number?"</p>
<p>The answer supplied by GPT-3 was: "The A and the 7".(A correct response).</p>
<p>Of the 12 vignettes Binz and Schulz gave to GPT-3, the system responded with the correct answer on six of them, and GPT-3's six incorrect responses were errors that humans also tend to make.What is to be made of what seems to be a correspondence?Binz and Schulz admit and show GPT-3's answers are strongly context dependent: In the above vignette a change in the order of the four cards to 4, 7, A, K led to a different answer "The A and the K." Humans can also be context-dependent, but perhaps not in the same ways.</p>
<p>Nonetheless, it may be that such results show a correspondence between AI systems and humans.Humans experience and store vast numbers of experiences, building knowledge on their basis (8); AI systems are exposed to vast numbers of instances (text tokens in the case of GPT-3) and build a representation on their basis.Perhaps both take advantage of the correlation structure of these instances and events.Whatever truth there may be in such an analogy, it seems unlikely that GPT-3 uses the kinds of explicit reasoning strategies that some humans use in these tasks.For example, to unpack the vignette in the above figure, humans given time and motivation might attempt to use explicit reasoning, logic, and mental simulations, perhaps trying out different choices to see what information they might provide.This generally involves manipulating information in working memory.Working memory is not part of GPT-3.Yet it is possible that the contents of working memory reflect what has been stored in long-term memory-after all when reading a problem or instructions the first step in generating contents of working memory will be retrieval from long-term memory (8).</p>
<p>Whatever one tries to infer from their results, Binz and Schulz note some additional caveats.First, the vignettes, as well as the correct (and human-generated incorrect) responses used in these experiments, are all from wellknown psychology studies, and are likely to have been 2 of 3 https://doi.org/10.1073/pnas.2300963120pnas.orgincluded in some form in GPT-3's vast training corpus.Second, GPT-3's responses can, in general, be very sensitive to the form of the prompt given to it.Binz and Schulz found that small inconsequential variations in the vignettes can substantially change GPT-3's answers, as noted above when discussing context dependence.</p>
<p>A second set of experiments used prompts designed so they did not appear in GPT-3's training corpus.The results were mixed.In some cases-for example, in so-called multi-armed-bandit decision tasks-GPT-3 outperformed human decision-making, and in others-particularly in causal reasoning tasks-GPT-3 was substantially worse.Third, as Binz and Schulz ask, it is unclear whether it is more appropriate to consider GPT-3 a single "participant" or an average of many participants.There should be a fourth caveat: It is unknown what aspect of the responses should be measured and compared with humans.Verbal responses?Numerical probabilities over response tokens computed by the LLMs?The neural network's internal representations?Would carefully designed and interpreted studies that treat AI systems as participants in psychology experiments help us understand how LLMs work, and do so better than the use of standard performance-based metrics?Binz and Schulz fall short of making that case, but their research can be viewed as a valuable first step (along with other related approaches; e.g., refs.(9)(10)(11)(12)(13).There is of course a possibility that this project could fail due to the substantial differences between LLMs and humans as objects of psychological study-it may not be appropriate to assume that an LLM's responses can be analyzed "just like how cognitive psychologists would analyze human behavior in the same tasks" (1).LLMs such as GPT-3 are trained explicitly to predict the next tokens (words or word parts) in a prompt.They are trained on a vast corpus and use 100s of billions of trainable parameters to make these predictions on the basis of detailed models of the statistical distribution of tokens and their correlations.As mentioned earlier, it is possible that something vaguely like this is used by humans as they store vast numbers of life events and build knowledge from them, but humans retrieve those events poorly and with large amounts of error (8,14,15).At the present time, it remains an open question whether the responses of LLMs are due to processes like those used by humans.If not, the attempt to understand LLMs by treating them like human participants in psychology experiments will surely fail.</p>
<p>In short, the assumptions psychologists make-for example, that humans use a mixture of intuitions and deliberate reflection (15)-might not apply to a LLM on the same tasks.Psychological assessments designed to test humans' higher-level cognitive abilities, including decision-making, information search, deliberation, and causal reasoning, may in fact not test these abilities at all in LLMs, even when LLMs-trained on huge swaths of human-generated text-produce similar responses as humans.</p>
<p>The difficulties in interpretating results like those reported by Binz and Schulz are compounded by the use of human cognitive terms to describe AI systems (16).We measure humans' "regret" in hypothetical gambling games, and how their "preferences" or "risk aversion" changes in response to how a win or loss is structured.We assume that these qualities are in response not solely to the words in the prompts humans are given, but to the real-world situations those words evoke.Does it make sense at all to similarly anthropomorphize LLMs by talking about how they "make decisions," "search for information," have "preferences," "regrets," or "risk aversion," given that these models have no connection to the real world beyond the text in their training corpus?Or, as Shanahan puts it, perhaps the only questions we can ask LLMs are "Here's a fragment of text….According to your model of the statistics of human language, what words are likely to come next?" (16).</p>
<p>We agree with Binz and Schulz that understanding how LLMs work is important and will become even more important in the future.Binz and Schulz correctly emphasize the important role that cognitive scientists should have to play in answering such questions.The successes of GPT-3 in their article are thought-provoking, but the failures emphasize the dangers inherent in using GPT-3, and LLMs for tasks in human society.Of course, we can expect the LLMs to grow ever more complex and come ever more close to emulating human verbal discourse, especially if they are allowed to interact with real environments or simulations of real environments (as Binz and Schulz suggest in the conclusion of their article).Would increasingly accurate emulation by LLMs increase or decrease the dangers of using them in society?It seems likely that our ability to understand them will decrease as the systems increase in complexity, whether or not we probe them with human experimental tasks.Should we turn over our society to systems we cannot understand?Of course, we can ask that same question of humans.</p>
<p>"The core idea is to 'treat GPT-3 as a participant in a psychology experiment,' in order to tease out the system's mechanisms of decision-making, reasoning, cognitive biases, and other important psychological traits."</p>
<p>Using cognitive psychology to understand GPT-3. M Binz, E Schulz, Proc. Natl. Acad. Sci. U.S.A. 120e22185231202023</p>
<p>SuperGLUE: A stickier benchmark for general-purpose language understanding systems. A Wang, Adv. Neural Inform. Process. Syst. 332019</p>
<p>CommonsenseQA: A question answering challenge targeting commonsense knowledge. I Talmor, Proceedings. nullAssociation for Computational Linguistics2019</p>
<p>Measuring mathematical problem solving with the math dataset. D Hendrycks, 10.48550/arXiv.2103.038742021. 15 February 2023</p>
<p>Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. T Mccoy, Proceedings, 57th Annual Meeting of the Association for Computational Linguistics. 57th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2019</p>
<p>How can we accelerate progress towards human-like linguistic generalization. T Linzen, Proceedings, 58th Annual Meeting of the Association for Computational Linguistics. 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2020</p>
<p>Reasoning about a rule. P C Wason, Q. J. Exp. Psychol. 201968</p>
<p>The co-evolution of knowledge and event memory. A B Nelson, R M Shiffrin, Psychol. Rev. 1202013</p>
<p>Language models show human-like content effects on reasoning. I Dasgupta, 10.48550/arXiv.2207.070512022. 15 February 2023</p>
<p>Machine intuition: Uncovering human-like intuitive decision-making in GPT-3. T Hagendorff, 10.48550/arXiv.2212.052062022. 15 February 2023</p>
<p>Human-like property induction is a challenge for large language models. S J Han, Proceedings, 44th Annual Conference of the Cognitive Science Society. Cognitive Science Society. 44th Annual Conference of the Cognitive Science Society2022</p>
<p>Putting GPT-3's creativity to the (alternative uses) test. C Stevenson, 10.48550/arXiv.2206.08932arXiv2022. 15 February 2023Preprint</p>
<p>E Kosoy, 10.48550/2206.08353Towards understanding how machines can learn causal overhypotheses. 2022</p>
<p>Human memory: A proposed system and its control processes. R C Atkinson, R M Shiffrin, The Psychology of Learning and Motivation: Advances in Research and Theory. K W Spence, J T Spence, New YorkAcademic Press19682</p>
<p>Search of associative memory. J G W Raaijmakers, R M Shiffrin, Psychol. Rev. 881981</p>
<p>M Shanahan, 10.48550/arXiv:2212.03551arXiv:2212.03551Talking about large language models. 2022. 15 February 2023Preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3264 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3264</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3264</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-75.html">extraction-schema-75</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <p><strong>Paper ID:</strong> paper-1e94510f9c332ba7f4dfac15873a879e655dce0e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1e94510f9c332ba7f4dfac15873a879e655dce0e" target="_blank">Grounding Language to Entities and Dynamics for Generalization in Reinforcement Learning</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> A new model, EMMA (Entity Mapper with Multi-modal Attention) which uses an entity-conditioned attention module that allows for selective focus over relevant descriptions in the manual for each entity in the environment, achieves successful zero-shot generalization to unseen games with new dynamics.</p>
                <p><strong>Paper Abstract:</strong> We investigate the use of natural language to drive the generalization of control policies and introduce the new multi-task environment Messenger with free-form text manuals describing the environment dynamics. Unlike previous work, Messenger does not assume prior knowledge connecting text and state observations $-$ the control policy must simultaneously ground the game manual to entity symbols and dynamics in the environment. We develop a new model, EMMA (Entity Mapper with Multi-modal Attention) which uses an entity-conditioned attention module that allows for selective focus over relevant descriptions in the manual for each entity in the environment. EMMA is end-to-end differentiable and learns a latent grounding of entities and dynamics from text to observations using only environment rewards. EMMA achieves successful zero-shot generalization to unseen games with new dynamics, obtaining a 40% higher win rate compared to multiple baselines. However, win rate on the hardest stage of Messenger remains low (10%), demonstrating the need for additional work in this direction.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3264.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3264.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EMMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Entity Mapper with Multi-modal Attention (EMMA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end differentiable RL agent that grounds free-form textual manuals to discrete entity symbols via entity-conditioned attention; it builds per-entity text-conditioned representations and concatenates recent entity representations to provide temporal context for decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>EMMA</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Architecture: BERT-base text encoder (frozen) to produce token embeddings, learned key/value projections per description, scaled dot-product attention where each entity symbol is embedded into a query to attend over description keys/values, producing per-entity text-conditioned vectors placed into a spatial tensor. Temporal context provided by concatenating per-entity representations from the three most recent observations; Conv2D + FFN head trained with PPO. Trained end-to-end (except BERT) using environment rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>MESSENGER</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Grid-world text-game where the agent must identify the entity holding a message and the goal entity using a free-form text manual (no prior entity-to-text mapping), pick up the message, avoid enemies, and deliver the message to the goal; disambiguation often requires grounding movement dynamics (chasing/fleeing/immovable).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>short-term working memory / temporal context buffer (concatenation of last 3 observations' entity representations)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td>Concatenate the per-entity representations (x_e) produced for the three most recent time steps into X' ∈ R^{h×w×3d}; pass X' through Conv2D and FFN to produce policy; this provides limited temporal state information to ground movement dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Test win rates (with EMMA's temporal concatenation): S1-TEST = 85 ± 1.4% win rate; S2-TEST = 85 ± 0.6%; S3-TEST = 10 ± 0.8% (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>Short-term temporal concatenation helps ground per-entity dynamics sufficiently to succeed and generalize on S1 and S2 (high test win rates close to oracle for those stages) and to disambiguate many entity-role assignments without relying on co-occurrence statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td>The 3-frame concatenation is insufficient to reliably ground subtle or longer-horizon movement distinctions required by S3; EMMA fails to fit S3 (and cannot disambiguate movement-based distractors), demonstrating the need for longer or more powerful temporal memory/ state-tracking. EMMA also required curriculum pretraining (S1→S2) — memory alone was not enough to learn on longer-episode stages from scratch.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Use entity-conditioned attention to read manuals and combine with short-term temporal context for grounding dynamics; use pretrained language encoders for diverse paraphrases; curriculum learning helps (pretrain on short tasks before longer ones); focused token selection (softmax over token weights) can help prevent overfitting compared to independent token scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grounding Language to Entities and Dynamics for Generalization in Reinforcement Learning', 'publication_date_yy_mm': '2021-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3264.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3264.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EMMA-S</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EMMA with sigmoid token scaling (EMMA-S)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variation of EMMA that uses independent sigmoid-based scaling of token weights (instead of softmax) when producing per-description key/value vectors; retains EMMA's temporal concatenation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>EMMA-S</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same overall pipeline as EMMA (BERT-based token embeddings, key/value projections, entity-conditioned attention) but token weighting for key/value construction uses sigmoid-normalized scores rather than softmax, allowing less-competitive multi-token selection; keeps the 3-frame concatenation temporal buffer and same Conv2D+FFN head.</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>MESSENGER</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same obtain-and-deliver-message task with free-form manuals and movement-driven disambiguation across stages S1–S3.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>short-term working memory / temporal context buffer (concatenation of last 3 observations' entity representations)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td>Identical temporal concatenation to EMMA; differs only in per-description token weighting (sigmoid per-token normalization) when computing key/value vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Relative performance: EMMA-S obtained higher rewards on S2 training/validation in experiments (Fig.10) but EMMA converged faster on S1; exact numeric test win rates for EMMA-S are not reported in tables.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>Retains the same short-term temporal grounding benefits as EMMA, enabling S1/S2 performance improvements compared to baselines that lack explicit temporal conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td>EMMA-S showed a tendency to overfit with neutral entities and was less robust to certain perturbations; token-weighting variation interacts with generalization more than temporal mechanism. Like EMMA, insufficient for S3-level dynamic disambiguation.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Softmax-based token selection (EMMA) tends to produce more focused, generalizable grounding than independent sigmoid scaling (EMMA-S); when combining text grounding with temporal memory, choose token-selection mechanisms that avoid learning spurious correlations with neutral entities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grounding Language to Entities and Dynamics for Generalization in Reinforcement Learning', 'publication_date_yy_mm': '2021-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3264.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3264.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>txt2π</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>txt2π (FILM-based model from Zhong et al. 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model originally introduced with RTFM that uses successive FILM-style feature-wise modulation layers; adapted here by adding a state-tracker LSTM to provide explicit state-tracking for longer tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RTFM: generalising to new environment dynamics via reading</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>txt2π (adapted)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Originally a FILM-modulated architecture (multi-hop reasoning via feature-wise linear modulation) from RTFM; for MESSENGER the authors concatenate the manual into a single string and add a state-tracker LSTM before the first FILM^2 layer to provide state-tracking (recurrent memory). Trained end-to-end with PPO.</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>MESSENGER</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same message-obtain-and-deliver task; requires distinguishing entities and roles from free-form manuals and, for S2/S3, grounding movement dynamics across time.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>recurrent working memory (state-tracker LSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td>A state-tracker LSTM is added prior to the FILM layers to provide temporal/state information across time steps; otherwise the manual is ingested as a concatenated string of facts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported performance (adapted txt2π with LSTM): S1-TEST = 2.5 ± 1.7% win; S2-TEST = 0.3 ± 0.08%; S3-TEST = 2.6 ± 0.3% (Table 2). Training win rates were high on S1/S2 training games but generalization to test games was poor.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>Adding an LSTM provides explicit state-tracking, but in this adaptation it did not translate into strong generalization on test games — training fit on some stages but test generalization remained poor, suggesting memory alone (LSTM) is not sufficient without explicit per-entity grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td>txt2π struggled to generalize on MESSENGER despite having recurrent memory; likely limitations include: ingesting the manual as a concatenated string (no explicit per-entity grounding), difficulty separating facts, and requiring many more steps to learn. The LSTM did not overcome the need for entity-conditioned reading of the manual.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>State-tracking (e.g., LSTM) should be combined with explicit per-entity grounding mechanisms (entity-conditioned attention) and structured manual reading, otherwise recurrent memory may not yield generalization on tasks with diverse paraphrases and grounding requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grounding Language to Entities and Dynamics for Generalization in Reinforcement Learning', 'publication_date_yy_mm': '2021-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3264.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3264.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BAM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Attention Module (BAM) / naive Bayes baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that uses naive Bayes style co-occurrence statistics (IBM Model 1–like) to align descriptions to observed entities via token co-occurrence counts; then assigns the value vector of the most likely description to an entity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BAM (naive Bayes)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses token co-occurrence counts to estimate P(t|e) and P(z|e)=∏_t P(t|e); for each description z in the manual selects the most likely entity e ∈ E' in the current observation set. The selected v_z is used as the entity representation; no temporal memory mechanism is used.</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>MESSENGER</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same task of identifying message and goal and acting accordingly; BAM relies on statistical alignment between manual tokens and observed entity symbols rather than interactive grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td>No explicit memory: alignment is computed from precomputed co-occurrence counts C(t,e) and then used per-episode; no temporal concatenation or recurrent state.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Reported performance (no memory): S1-TEST = 66 ± 1.5% win; S2-TEST = 41 ± 1.7%; S3-TEST = 2.7 ± 0.9% (Table 2). Also reported strong training performance on MC games but poor on SC games (which require interactive grounding).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>Not applicable; reliance on co-occurrence statistics can be effective when multi-combination (MC) training provides co-occurrence signals, giving competitive results on some training/test splits without temporal memory.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td>Performs poorly on single-combination (SC) games where co-occurrence statistics give no signal (interactive grounding required); no temporal grounding so cannot disambiguate entities via movement dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Co-occurrence / naive alignment can be a strong baseline when the training distribution contains sufficient combinatorial coverage (MC), but interactive temporal grounding mechanisms are required for SC-style problems and movement-based disambiguation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grounding Language to Entities and Dynamics for Generalization in Reinforcement Learning', 'publication_date_yy_mm': '2021-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3264.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3264.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer baseline (BERT[CLS])</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer baseline ingesting manual + observation as text (BERT CLS model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that concatenates the manual (s_m) and a stringified spatial observation (s_o) into a single text input to BERT (pretrained), and trains FFN policy/value heads on the [CLS] token; no explicit temporal memory beyond per-step input.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Transformer (BERT) baseline</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Pretrained BERT-base ingests s_m [SEP] s_o where s_o is a textual encoding of entity coordinates and types; MLPs on top of [CLS] produce policy and value. Trained end-to-end (BERT fine-tuning) with PPO in experiments. No explicit per-entity attention module or multi-frame temporal concatenation was used.</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>MESSENGER</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same messaging task; this baseline tests whether a pure Transformer over textualized state+manual can learn the grounding and policy.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td>No recurrent or multi-step memory; each timestep is processed as a fresh BERT input formed by concatenation of manual and current observation string.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Did not learn a performant policy: rewards for the Transformer baseline did not significantly increase after 1.5×10^6 steps on S1 (Figure 12); numeric win rates are not reported in tables but text states it struggled to learn.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>Not applicable; baseline indicates that encoding spatial information as flat text without explicit temporal buffer or per-entity reading is ineffective for this task.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td>Struggled to learn, likely due to difficulty encoding spatial/temporal structure in string form and absence of per-entity temporal context; no explicit memory mechanism to track movement dynamics across frames.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Avoid representing the spatial state solely as flat text for tasks requiring fine-grained grounding across time; explicit per-entity representations plus temporal context (rather than one-shot BERT on a stringified state) are preferable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grounding Language to Entities and Dynamics for Generalization in Reinforcement Learning', 'publication_date_yy_mm': '2021-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>RTFM: generalising to new environment dynamics via reading <em>(Rating: 2)</em></li>
                <li>Grounded language learning fast and slow <em>(Rating: 2)</em></li>
                <li>Grounding language for transfer in deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Learning to win by reading manuals in a Monte-Carlo framework <em>(Rating: 1)</em></li>
                <li>The mathematics of statistical machine translation: Parameter estimation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3264",
    "paper_id": "paper-1e94510f9c332ba7f4dfac15873a879e655dce0e",
    "extraction_schema_id": "extraction-schema-75",
    "extracted_data": [
        {
            "name_short": "EMMA",
            "name_full": "Entity Mapper with Multi-modal Attention (EMMA)",
            "brief_description": "An end-to-end differentiable RL agent that grounds free-form textual manuals to discrete entity symbols via entity-conditioned attention; it builds per-entity text-conditioned representations and concatenates recent entity representations to provide temporal context for decision-making.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "EMMA",
            "agent_description": "Architecture: BERT-base text encoder (frozen) to produce token embeddings, learned key/value projections per description, scaled dot-product attention where each entity symbol is embedded into a query to attend over description keys/values, producing per-entity text-conditioned vectors placed into a spatial tensor. Temporal context provided by concatenating per-entity representations from the three most recent observations; Conv2D + FFN head trained with PPO. Trained end-to-end (except BERT) using environment rewards.",
            "game_or_benchmark_name": "MESSENGER",
            "task_description": "Grid-world text-game where the agent must identify the entity holding a message and the goal entity using a free-form text manual (no prior entity-to-text mapping), pick up the message, avoid enemies, and deliver the message to the goal; disambiguation often requires grounding movement dynamics (chasing/fleeing/immovable).",
            "uses_memory": true,
            "memory_type": "short-term working memory / temporal context buffer (concatenation of last 3 observations' entity representations)",
            "memory_implementation_details": "Concatenate the per-entity representations (x_e) produced for the three most recent time steps into X' ∈ R^{h×w×3d}; pass X' through Conv2D and FFN to produce policy; this provides limited temporal state information to ground movement dynamics.",
            "performance_with_memory": "Test win rates (with EMMA's temporal concatenation): S1-TEST = 85 ± 1.4% win rate; S2-TEST = 85 ± 0.6%; S3-TEST = 10 ± 0.8% (Table 2).",
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "memory_benefits": "Short-term temporal concatenation helps ground per-entity dynamics sufficiently to succeed and generalize on S1 and S2 (high test win rates close to oracle for those stages) and to disambiguate many entity-role assignments without relying on co-occurrence statistics.",
            "memory_limitations_or_failures": "The 3-frame concatenation is insufficient to reliably ground subtle or longer-horizon movement distinctions required by S3; EMMA fails to fit S3 (and cannot disambiguate movement-based distractors), demonstrating the need for longer or more powerful temporal memory/ state-tracking. EMMA also required curriculum pretraining (S1→S2) — memory alone was not enough to learn on longer-episode stages from scratch.",
            "best_practices_or_recommendations": "Use entity-conditioned attention to read manuals and combine with short-term temporal context for grounding dynamics; use pretrained language encoders for diverse paraphrases; curriculum learning helps (pretrain on short tasks before longer ones); focused token selection (softmax over token weights) can help prevent overfitting compared to independent token scaling.",
            "uuid": "e3264.0",
            "source_info": {
                "paper_title": "Grounding Language to Entities and Dynamics for Generalization in Reinforcement Learning",
                "publication_date_yy_mm": "2021-01"
            }
        },
        {
            "name_short": "EMMA-S",
            "name_full": "EMMA with sigmoid token scaling (EMMA-S)",
            "brief_description": "A variation of EMMA that uses independent sigmoid-based scaling of token weights (instead of softmax) when producing per-description key/value vectors; retains EMMA's temporal concatenation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "EMMA-S",
            "agent_description": "Same overall pipeline as EMMA (BERT-based token embeddings, key/value projections, entity-conditioned attention) but token weighting for key/value construction uses sigmoid-normalized scores rather than softmax, allowing less-competitive multi-token selection; keeps the 3-frame concatenation temporal buffer and same Conv2D+FFN head.",
            "game_or_benchmark_name": "MESSENGER",
            "task_description": "Same obtain-and-deliver-message task with free-form manuals and movement-driven disambiguation across stages S1–S3.",
            "uses_memory": true,
            "memory_type": "short-term working memory / temporal context buffer (concatenation of last 3 observations' entity representations)",
            "memory_implementation_details": "Identical temporal concatenation to EMMA; differs only in per-description token weighting (sigmoid per-token normalization) when computing key/value vectors.",
            "performance_with_memory": "Relative performance: EMMA-S obtained higher rewards on S2 training/validation in experiments (Fig.10) but EMMA converged faster on S1; exact numeric test win rates for EMMA-S are not reported in tables.",
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "memory_benefits": "Retains the same short-term temporal grounding benefits as EMMA, enabling S1/S2 performance improvements compared to baselines that lack explicit temporal conditioning.",
            "memory_limitations_or_failures": "EMMA-S showed a tendency to overfit with neutral entities and was less robust to certain perturbations; token-weighting variation interacts with generalization more than temporal mechanism. Like EMMA, insufficient for S3-level dynamic disambiguation.",
            "best_practices_or_recommendations": "Softmax-based token selection (EMMA) tends to produce more focused, generalizable grounding than independent sigmoid scaling (EMMA-S); when combining text grounding with temporal memory, choose token-selection mechanisms that avoid learning spurious correlations with neutral entities.",
            "uuid": "e3264.1",
            "source_info": {
                "paper_title": "Grounding Language to Entities and Dynamics for Generalization in Reinforcement Learning",
                "publication_date_yy_mm": "2021-01"
            }
        },
        {
            "name_short": "txt2π",
            "name_full": "txt2π (FILM-based model from Zhong et al. 2020)",
            "brief_description": "A model originally introduced with RTFM that uses successive FILM-style feature-wise modulation layers; adapted here by adding a state-tracker LSTM to provide explicit state-tracking for longer tasks.",
            "citation_title": "RTFM: generalising to new environment dynamics via reading",
            "mention_or_use": "use",
            "agent_name": "txt2π (adapted)",
            "agent_description": "Originally a FILM-modulated architecture (multi-hop reasoning via feature-wise linear modulation) from RTFM; for MESSENGER the authors concatenate the manual into a single string and add a state-tracker LSTM before the first FILM^2 layer to provide state-tracking (recurrent memory). Trained end-to-end with PPO.",
            "game_or_benchmark_name": "MESSENGER",
            "task_description": "Same message-obtain-and-deliver task; requires distinguishing entities and roles from free-form manuals and, for S2/S3, grounding movement dynamics across time.",
            "uses_memory": true,
            "memory_type": "recurrent working memory (state-tracker LSTM)",
            "memory_implementation_details": "A state-tracker LSTM is added prior to the FILM layers to provide temporal/state information across time steps; otherwise the manual is ingested as a concatenated string of facts.",
            "performance_with_memory": "Reported performance (adapted txt2π with LSTM): S1-TEST = 2.5 ± 1.7% win; S2-TEST = 0.3 ± 0.08%; S3-TEST = 2.6 ± 0.3% (Table 2). Training win rates were high on S1/S2 training games but generalization to test games was poor.",
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "memory_benefits": "Adding an LSTM provides explicit state-tracking, but in this adaptation it did not translate into strong generalization on test games — training fit on some stages but test generalization remained poor, suggesting memory alone (LSTM) is not sufficient without explicit per-entity grounding.",
            "memory_limitations_or_failures": "txt2π struggled to generalize on MESSENGER despite having recurrent memory; likely limitations include: ingesting the manual as a concatenated string (no explicit per-entity grounding), difficulty separating facts, and requiring many more steps to learn. The LSTM did not overcome the need for entity-conditioned reading of the manual.",
            "best_practices_or_recommendations": "State-tracking (e.g., LSTM) should be combined with explicit per-entity grounding mechanisms (entity-conditioned attention) and structured manual reading, otherwise recurrent memory may not yield generalization on tasks with diverse paraphrases and grounding requirements.",
            "uuid": "e3264.2",
            "source_info": {
                "paper_title": "Grounding Language to Entities and Dynamics for Generalization in Reinforcement Learning",
                "publication_date_yy_mm": "2021-01"
            }
        },
        {
            "name_short": "BAM",
            "name_full": "Bayesian Attention Module (BAM) / naive Bayes baseline",
            "brief_description": "A baseline that uses naive Bayes style co-occurrence statistics (IBM Model 1–like) to align descriptions to observed entities via token co-occurrence counts; then assigns the value vector of the most likely description to an entity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "BAM (naive Bayes)",
            "agent_description": "Uses token co-occurrence counts to estimate P(t|e) and P(z|e)=∏_t P(t|e); for each description z in the manual selects the most likely entity e ∈ E' in the current observation set. The selected v_z is used as the entity representation; no temporal memory mechanism is used.",
            "game_or_benchmark_name": "MESSENGER",
            "task_description": "Same task of identifying message and goal and acting accordingly; BAM relies on statistical alignment between manual tokens and observed entity symbols rather than interactive grounding.",
            "uses_memory": false,
            "memory_type": null,
            "memory_implementation_details": "No explicit memory: alignment is computed from precomputed co-occurrence counts C(t,e) and then used per-episode; no temporal concatenation or recurrent state.",
            "performance_with_memory": null,
            "performance_without_memory": "Reported performance (no memory): S1-TEST = 66 ± 1.5% win; S2-TEST = 41 ± 1.7%; S3-TEST = 2.7 ± 0.9% (Table 2). Also reported strong training performance on MC games but poor on SC games (which require interactive grounding).",
            "has_performance_comparison": false,
            "memory_benefits": "Not applicable; reliance on co-occurrence statistics can be effective when multi-combination (MC) training provides co-occurrence signals, giving competitive results on some training/test splits without temporal memory.",
            "memory_limitations_or_failures": "Performs poorly on single-combination (SC) games where co-occurrence statistics give no signal (interactive grounding required); no temporal grounding so cannot disambiguate entities via movement dynamics.",
            "best_practices_or_recommendations": "Co-occurrence / naive alignment can be a strong baseline when the training distribution contains sufficient combinatorial coverage (MC), but interactive temporal grounding mechanisms are required for SC-style problems and movement-based disambiguation.",
            "uuid": "e3264.3",
            "source_info": {
                "paper_title": "Grounding Language to Entities and Dynamics for Generalization in Reinforcement Learning",
                "publication_date_yy_mm": "2021-01"
            }
        },
        {
            "name_short": "Transformer baseline (BERT[CLS])",
            "name_full": "Transformer baseline ingesting manual + observation as text (BERT CLS model)",
            "brief_description": "A baseline that concatenates the manual (s_m) and a stringified spatial observation (s_o) into a single text input to BERT (pretrained), and trains FFN policy/value heads on the [CLS] token; no explicit temporal memory beyond per-step input.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Transformer (BERT) baseline",
            "agent_description": "Pretrained BERT-base ingests s_m [SEP] s_o where s_o is a textual encoding of entity coordinates and types; MLPs on top of [CLS] produce policy and value. Trained end-to-end (BERT fine-tuning) with PPO in experiments. No explicit per-entity attention module or multi-frame temporal concatenation was used.",
            "game_or_benchmark_name": "MESSENGER",
            "task_description": "Same messaging task; this baseline tests whether a pure Transformer over textualized state+manual can learn the grounding and policy.",
            "uses_memory": false,
            "memory_type": null,
            "memory_implementation_details": "No recurrent or multi-step memory; each timestep is processed as a fresh BERT input formed by concatenation of manual and current observation string.",
            "performance_with_memory": null,
            "performance_without_memory": "Did not learn a performant policy: rewards for the Transformer baseline did not significantly increase after 1.5×10^6 steps on S1 (Figure 12); numeric win rates are not reported in tables but text states it struggled to learn.",
            "has_performance_comparison": false,
            "memory_benefits": "Not applicable; baseline indicates that encoding spatial information as flat text without explicit temporal buffer or per-entity reading is ineffective for this task.",
            "memory_limitations_or_failures": "Struggled to learn, likely due to difficulty encoding spatial/temporal structure in string form and absence of per-entity temporal context; no explicit memory mechanism to track movement dynamics across frames.",
            "best_practices_or_recommendations": "Avoid representing the spatial state solely as flat text for tasks requiring fine-grained grounding across time; explicit per-entity representations plus temporal context (rather than one-shot BERT on a stringified state) are preferable.",
            "uuid": "e3264.4",
            "source_info": {
                "paper_title": "Grounding Language to Entities and Dynamics for Generalization in Reinforcement Learning",
                "publication_date_yy_mm": "2021-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "RTFM: generalising to new environment dynamics via reading",
            "rating": 2
        },
        {
            "paper_title": "Grounded language learning fast and slow",
            "rating": 2
        },
        {
            "paper_title": "Grounding language for transfer in deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Learning to win by reading manuals in a Monte-Carlo framework",
            "rating": 1
        },
        {
            "paper_title": "The mathematics of statistical machine translation: Parameter estimation",
            "rating": 1
        }
    ],
    "cost": 0.01577275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Grounding Language to Entities and Dynamics for Generalization in Reinforcement Learning</h1>
<p>Austin W. Hanjie ${ }^{1}$ Victor Zhong ${ }^{2}$ Karthik Narasimhan ${ }^{1}$</p>
<h4>Abstract</h4>
<p>We investigate the use of natural language to drive the generalization of control policies and introduce the new multi-task environment MESSENGER with free-form text manuals describing the environment dynamics. Unlike previous work, MESSENGER does not assume prior knowledge connecting text and state observations - the control policy must simultaneously ground the game manual to entity symbols and dynamics in the environment. We develop a new model, EMMA (Entity Mapper with Multi-modal Attention) which uses an entity-conditioned attention module that allows for selective focus over relevant descriptions in the manual for each entity in the environment. EMMA is end-to-end differentiable and learns a latent grounding of entities and dynamics from text to observations using only environment rewards. EMMA achieves successful zero-shot generalization to unseen games with new dynamics, obtaining a $40 \%$ higher win rate compared to multiple baselines. However, win rate on the hardest stage of MESSENGER remains low (10\%), demonstrating the need for additional work in this direction.</p>
<h2>1. Introduction</h2>
<p>Interactive game environments are useful for developing agents that learn grounded representations of language for autonomous decision making (Golland et al., 2010; Branavan et al., 2011; Andreas \&amp; Klein, 2015; Bahdanau et al., 2018). The key objective in these environments is learning to interpret language specifications by relating entities and dynamics of the environment (i.e. how entities behave) to their corresponding references in the text, in order to effectively and efficiently win new settings with previously</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>unseen entities or dynamics (Narasimhan et al., 2018; Zhong et al., 2020). While existing methods demonstrate successful transfer to new settings, they assume a ground-truth mapping between individual entities and their textual references.</p>
<p>We introduce MESSENGER, ${ }^{1}$ an environment which features multiple game variants with differing dynamics and accompanying text manuals in English for each. The manuals contain descriptions of the entities and world dynamics obtained through crowdsourced human writers. Crucially, while prior work assumes a ground truth mapping (e.g. the word 'knight' in the manual refers to the entity name 'knight' in the observation), MESSENGER does not contain prior signals that map between text and state observations (e.g. between the phrase 'mounted warrior is fleeing' and the symbol moving away from the agent). To succeed in MESSENGER, an agent must relate entities and dynamics of the environment to their references in the natural language manual using only scalar reward signals from the environment. The overall game mechanics of MESSENGER involve obtaining a message and delivering it to a goal. For instance, in game 1 of Figure 1, the agent must read the manual to:</p>
<ol>
<li>Identify the entity that holds the message. In this case, description $2(\mathrm{~d}-2)$ reveals that it is with the thief but there is an identical entity that is an enemy (d-4).</li>
<li>Map d-2 and d-4 to the correct symbols in the observation (green-cloaked person).</li>
<li>Observe the movement patterns of the two entities ('heading closer' vs. 'rushing away') to disambiguate which of the two entities holds the message.</li>
<li>Pick up the message from the entity that holds it.</li>
<li>Identify the entity that is the goal. Here, d-3 and d-6 reference a goal. It must realize that there is no 'canine' that is 'running away' and so d-6 must be a distractor, and a mage must be the goal.</li>
<li>Follow a similar procedure to 3 to disambiguate which mage is the goal and which is the enemy (d-3 vs d-5).</li>
<li>Bring the message to the goal.</li>
</ol>
<p>To ground entities and dynamics to their corresponding</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>GAME 1 MANUAL</p>
<ol>
<li>at a particular locale, there exists a motionless mongrel that is a formidable adversary.</li>
<li>the top-secret paperwork is in the crook's possession, and he's heading closer and closer to where you are.</li>
<li>the crucial target is held by the wizard and the wizard is fleeing from you.</li>
<li>the mugger rushing away is the opposition posing a serious threat.</li>
<li>the thing that is not able to move is the mage who possesses the enemy that is deadly.</li>
<li>the vital goal is found with the canine, but it is running away from you.</li>
</ol>
<p>Figure 1. Two games from our multi-task environment MESSENGER where the agent must obtain the message and deliver it to the goal (white dotted lines). Within a single game, the same entities (e.g. mage) with different roles (e.g. enemy, goal) must be disambiguated by their dynamics (e.g. immovable, fleeing). The same entities may have different roles in different games forcing the agent to consult the manual to succeed consistently. Note the extraneous description (italics) and multiple synonyms for entities and roles (e.g. mage, wizard; adversary, opposition). Unlike prior work, the mapping from words in the manual to game entities is not available and must be learned using only scalar game rewards.
references in the manual, we develop a new model called EMMA (Entity Mapper with Multi-modal Attention). EMMA simultaneously learns to select relevant sentences in the manual for each entity in the game as well as incorporate the corresponding text description into its control policy. This is done using a multi-modal attention mechanism which uses entity representations as queries to attend to specific tokens in the manual text. EMMA then generates a text-conditioned representation for each entity which is processed further by a deep neural network to generate a policy. We train the entire model in a multi-task fashion using reinforcement learning to maximize task returns.</p>
<p>Our experiments demonstrate EMMA outperforms multiple baselines (language-agnostic, attention-ablated, and Bayesian attention) and an existing state of the art model (Zhong et al., 2020) — on unseen games (i.e. a zero-shot test), EMMA achieves more than $40 \%$ higher win rates. However, while EMMA can effectively map text references to their corresponding entity symbols in observation
space, its ability to disambiguate descriptions by grounding language to entity movement dynamics is lacking, and win rates on the test games for the hardest stage of MESSENGER remains low for all models evaluated ( $\leq 10 \%$ ), demonstrating the challenging nature of grounding natural language to dynamics using only interactive (reward-based) feedback.</p>
<p>In summary, our paper makes two key contributions: (1) a multi-task environment with novel challenges including a) learning entity symbol grounding from scratch in a multitask setup with b) realistic, crowd-sourced text and (2) an attention-based model that is able to learn such a grounding where prior approaches struggle. We hope MESSENGER and EMMA will further enable the development of new models and learning algorithms for language grounding.</p>
<h2>2. Related Work</h2>
<p>Grounding for Instruction Following Grounding natural language to policies has been explored in the context of instruction following in tasks like navigation (Chen \&amp; Mooney, 2011; Hermann et al., 2017; Fried et al., 2018; Wang et al., 2019; Daniele et al., 2017; Misra et al., 2017; Janner et al., 2018), games (Golland et al., 2010; Reckman et al., 2010; Andreas \&amp; Klein, 2015; Bahdanau et al., 2018; Küttler et al., 2020) or robotic control (Walter et al., 2013; Hemachandra et al., 2014; Blukis et al., 2019) (see Luketina et al. (2019) and Tellex et al. (2020) for more detailed surveys). Recent work has explored several methods for enabling generalization in instruction following, including environmental variations (Hill et al., 2020a), memory structures (Hill et al., 2020c) and pre-trained language models (Hill et al., 2020b). In a slightly different setting, CoReyes et al. (2019) use incremental guidance, where the text input is provided online, conditioned on the agent's progress in the environment. Andreas et al. (2017) developed an agent that can use sub-goal specifications to deal with sparse rewards. Oh et al. (2017) use sub-task instructions and hierarchical reinforcement learning to complete tasks with long action sequences.</p>
<p>In all these works, the text conveys the goal to the agent (e.g. 'move forward five steps'), thereby encouraging a direct connection between the instruction and the control policy. This tight coupling means that any grounding learned by the agent is likely to be tailored to the types of tasks seen in training, making generalization to a new distribution of dynamics or tasks challenging. In extreme cases, the agent may even function without acquiring an appropriate grounding between language and observations (Hu et al., 2019). In our setup, we assume that the text only provides high-level guidance without directly describing the correct actions for every game state.</p>
<p>Language Grounding by Reading Manuals A different line of work has explored the use of language as an auxiliary source of knowledge through text manuals. These manuals provide useful descriptions of the entities in the world and their dynamics (e.g. how they move or interact with other entities) that are optional for the agent to make use of and do not directly reveal the actions it has to take. Branavan et al. (2011) developed an agent to play the game of Civilization more effectively by reading the game manual. They make use of dependency parses and predicate labeling to construct feature-based representations of the text, which are then used to construct the action-value function used by the agent. Our method does not require such feature construction. Narasimhan et al. (2018) and Zhong et al. (2020) used text descriptions of game dynamics to learn policies that generalize to new environments, without requiring feature engineering. However, these works assume some form of initial grounding provided to the agent (e.g. a mapping between entity symbols and their descriptions, or the use of entity names in text as state observations). In contrast, MESSENGER requires that this fundamental mapping between entity symbols in observation space and their text references be learned entirely through interaction with the environment.</p>
<h2>3. Preliminaries</h2>
<p>Our objective is to demonstrate grounding of environment dynamics and entities for generalization to unseen environments. An entity is an object represented as a symbol in the observation. Dynamics refer to how entities behave in the environment including how they interact with the agent. Notably, movement dynamics are the frame-to-frame position changes exhibited by entities (e.g. fleeing).</p>
<p>Environment We model decision making in each environment as a Partially-Observable Markov Decision Process (POMDP) with the 8-tuple $(S, A, O, P, R, E, Z, M)$. $S$ and $O$ are the set of all states and observations respectively where each $o \in O$ contains entities from the set of entities $E$. At each step $t$, the agent takes some action $a_{t} \in A . P\left(s_{t+1} \mid s_{t}, a_{t}\right)$ is the transition distribution over all possible next states $s_{t+1}$ conditioned on the current state $s_{t}$ and action $a_{t} . R\left(s_{t}, a_{t}, s_{t+1}\right)$ is a function that provides the agent with a reward $r_{t} \in \mathbb{R}$ for action $a_{t}$ and transition from $s_{t}$ to $s_{t+1} . Z$ is a set of text descriptions, with each $z \in Z$ providing information about an entity $e \in E . M$ is the map $z_{e} \mapsto e$ which identifies the entity that each description describes. $M, P$, and $R$ are not available to the agent. Note that there might not be a one-to-one mapping between $Z$ and entities in the current state observation.</p>
<p>Reinforcement Learning (RL) The objective of the agent is to find a policy $\pi: O \rightarrow A$ to maximize its cu- mulative reward in an episode. If $\pi$ is parameterized by $\theta$, standard deep RL approaches optimize $\theta$ to maximize the expected reward of following $\pi_{\theta}$. In our setup, we want the agent to learn a policy $\pi_{\theta}(a \mid o, Z)$ that conditions its behavior on the provided text. However, in contrast to previous work (Narasimhan et al., 2018; Zhong et al., 2020), $M$ is not available to our agent and must be learned.</p>
<p>Differentiating Entities, Roles, and Text References For ease of exposition, we use type face to differentiate between entity symbols, roles, and 'text references'. For example, plane refers to the entity , where 'plane' and 'aircraft' are text references to plane. Additionally, plane can take on the role of an enemy.</p>
<h2>4. Messenger</h2>
<p>We require an environment where grounding text descriptions $Z$ to dynamics and learning the mapping $M$ for all entities in $E$ is necessary to obtain a good reward. Moreover, there must be enough game instances of the environment to induce the mapping $M$.</p>
<p>With these requirements in mind, we devise a new multitask environment Messenger using the Py-VGDL framework (Schaul, 2013). In Messenger, each entity can take on one of three roles: an enemy, message, or goal. The agent's objective is to bring the message to the goal while avoiding the enemies. If the agent encounters an enemy at any point in the game, or the goal without first obtaining the message, it loses the game and obtains a reward of -1 . Rewards of 0.5 and 1 are provided for obtaining and delivering the message to the goal respectively. ${ }^{2}$. There are twelve different entities and three possible movement types: stationary, chasing, or fleeing. Each set of entity-role assignments (henceforth referred to as a game) is initialized on a $10 \times 10$ grid. The agent can navigate via up, down, left, right, and stay actions and interacts with another entity when both occupy the same cell.</p>
<p>The same set of entities with the same movements may be assigned different roles in different games. Thus, two games may have identical observations but differ in the reward function $R$ (which is not available to the agent) and the text manual $Z$ (which is available). Thus, our agent must learn to extract information from $Z$ to succeed consistently. Some game examples are presented in Figure 1.</p>
<p>Grounding Entities Messenger requires agents to learn $M$ without priors connecting state observations $O$ to descriptions $Z$. Aside from using independent entity symbols disjoint from the text vocabulary, the set of training</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Entities and their subdivision into human, nature and fantasy sub-worlds. Each $K_{3}$ subgraph is a combination of entities that may appear during training.
games is designed such that simple co-occurrence statistics between entity and text do not completely reveal $M$.</p>
<p>Consider when every possible combination of entities is observed during training. Then, for an entity $e$, its symbol in the observation (e.g. plane) is the only one that always appears together with its text references (e.g. 'aircraft'). This tight coupling provides an inherent bias towards the correct grounding without needing to act in the environment. We denote such a set of games where each entity can appear with every other entity as multi-combination (MC).</p>
<p>The MC assumption may not always be realistic in practice - some entities are very unlikely to appear together (e.g. plane, thief, sword) while others may co-occur exclusively with each other (e.g. mage, orb, sword). We denote games in which the same entities always appear together as single-combination (SC). For SC games, every text symbol in the manual (e.g. 'mage', 'enemy', 'the', etc.) co-occurs the same number of times with all entity symbols in the observation. For example, if the entity symbols $\bigcirc$ and $\square$ always appear simultaneously with both text symbols 'mage' and 'sword', it is impossible to map 'mage' to O without interacting with the entities. That is, co-occurrences between entity and text symbols provide no information about $M$ and the agent must ground these entities entirely via interaction. To learn $M$ for the entities in this example, the agent must interact with $O$ and if it obtains the message from it, it must infer from the description 'The mage has the message' that $O$ must be a 'mage'.</p>
<p>We divide the entities in MESSENGER into human, nature, and fantasy sub-worlds (Fig. 2) and exclude from training any games in which entities from different sub-world appear together. In particular, the nature and fantasy subworlds form SC and the human subworld forms the MC games.</p>
<p>Grounding Dynamics To force agents to distinguish varying movement dynamics, multiple copies of the same entity with different roles in MESSENGER may exhibit different movement patterns. For example, within the same game there may be descriptions: (1) 'the chasing mage is an enemy' and (2) 'the fleeing mage is the goal'. This
means that even after grounding words such as 'mage' to its corresponding entity symbol, the agent must additionally consider the position of $e$ through a sequence of observations $o_{t-k}, \ldots, o_{t}$ in order to find the correct description $z_{e}$.</p>
<p>Text Descriptions We collected 5,316 unique free-form entity descriptions in English via Amazon Mechanical Turk (Buhrmester et al., 2016) by asking workers to paraphrase prompt sentences. To increase the diversity of responses, the prompts were themselves produced from 82 crowdsourced templates. When constructing the prompts, we inject multiple synonyms for each entity. Workers further paraphrased these synonyms, resulting in multiple ways to describe the same entity (e.g. 'airplane', 'jet', 'flying machine', 'aircraft', 'airliner'). Furthermore, we observe responses with multiple sentences per description, typos ('plane' vs 'plan') and the need to disambiguate similar words ('flying machine', 'winged creature'). Each training manual consists of a set of descriptions with an average total length of 30 - 60 words depending on the level. The total vocabulary size of the descriptions is 1,125 . Besides lower-casing the worker responses, we do not do any preprocessing. Example descriptions can be found in Fig. 1. Further details regarding data collection can be found in appendix A.</p>
<p>Train-Evaluation Split We ensure that any assignment of an entity to the roles message or goal in the evaluation games never appears during training (e.g. if $e$ is the goal in evaluation, no $e$ is ever the goal in any training game). This forces models to make compositional entity-role generalizations to succeed on the evaluation games. In total we have 44 training, 32 validation, and 32 test games. We train on 2,863 of the text descriptions and reserve 1,227 and 1,226 for validation and testing respectively.</p>
<p>Comparison with Previous Environments We chose to realize MESSENGER in a grid-world as it allows us to (1) study generalization to rich sets of procedurally generated dynamics, (2) conduct controlled studies of co-occurrence statistics (SC, vs. MC) and (3) explicitly verify the learned groundings with well-defined, discrete entities (see Fig. 6).</p>
<p>Other grid-worlds used to study language grounding include RTFM (Zhong et al., 2020), BabyAI (Chevalier-Boisvert et al., 2019) and Narasimhan et al. (2018). An oracle is used in Narasimhan et al. (2018) to concatenate the text representation to its corresponding entity representation. Access to such an oracle is a strong assumption in the wild and eliminates the need to ground the entities altogether.</p>
<p>In RTFM, the observation is a grid of text in which entity names are lexically identical to their references in the manual (e.g. 'plane'). The key challenge unique to MESSENGER is learning to map between the observed entity symbol (e.g. $\boldsymbol{\sim}$ ) and its natural language references in the</p>
<p>manual (e.g. ‘aircraft’). Furthermore, RTFM is a MC environment which may simplify the grounding problem. Both <em>Narasimhan et al. (2018)</em> and <em>Zhong et al. (2020)</em> do not consider disambiguation by grounding movement dynamics, whereas agents in Messenger need to distinguish entities based on how they move (e.g. fleeing, chasing).</p>
<p>Unlike previous work on language grounding in grid environments <em>(Zhong et al., 2020; Chevalier-Boisvert et al., 2019)</em>, we do not use templated or rule-generated text. RTFM uses a small number of rule-based templates to construct each manual, and each entity is referred to in a single way (e.g. goblin is always ‘goblin’). In contrast, MESSENGER features thousands of hand-written descriptions and each entity may be referenced in multiple ways. For further comparisons of RTFM and MESSENGER, including why we do not simply extend RTFM, please see Appendix B.1.</p>
<h2>5 The EMMA Model</h2>
<p>As we saw in the previous section, an agent must learn to map entities to their corresponding references in the natural language manual in order to perform well in MESSENGER. To learn this mapping, we develop a new model, EMMA (Entity Mapper with Multi-modal Attention), which employs a soft-attention mechanism over the text descriptions. At a high level, for each entity description, EMMA first generates key and value vectors from their respective token embeddings obtained using a pretrained language model. Each entity attends to the descriptors via a symbol embedding that acts as the attention query. Then, instead of representing each entity with its embedding, we use the resulting attention-scaled values as a proxy for the entity. This approach helps our model learn a control policy that focuses on entity roles (e.g. enemy, goal) while using the entities’ identity (e.g. queen, mage) to selectively read the text. We describe each component of EMMA below and in Figure 3.</p>
<h4>Text Encoder</h4>
<p>Our input consists of a $h \times w$ grid observation $o \in O$ with a set of entity descriptions $Z$. We encode each description $z \in Z$ using a BERT-base model whose parameters are fixed throughout training <em>(Devlin et al., 2019; Wolf et al., 2019)</em>. For a description $z$, let $t_{1}, \ldots, t_{n}$ be its token embeddings generated by our encoder. We obtain key and value vectors $k_{z}, v_{z}$, where $\sigma$ is the softmax function:</p>
<p>$$
\begin{aligned}
&amp; k_{z}=\sum_{i=1}^{n} \alpha_{i} W_{k} t_{i}+b_{k} \quad \alpha=\sigma\left(\left(u_{k} \cdot t_{j}\right)<em z="z">{j=1}^{n}\right) \
&amp; v</em>\right)
\end{aligned}
$$}=\sum_{i=1}^{n} \beta_{i} W_{v} t_{i}+b_{v} \quad \beta=\sigma\left(\left(u_{v} \cdot t_{j}\right)_{j=1}^{n</p>
<p>The key and value vectors are simply linear combinations of $W_{k} t_{i}+b_{k}$ and $W_{v} t_{i}+b_{v}$ with weights $\alpha, \beta$ respectively, where $W_{k}, W_{v}$ are matrices which transform each token</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Schematic of our model EMMA, which creates a representation for entities using multi-modal attention over the observations and text manual. Mechanisms for the key, query, and value are shaded in blue, green, and red respectively.
to $d$ dimensions and $b_{k}, b_{v}$ are biases. The weights $\alpha, \beta$ are obtained by taking the softmax over the dot products $\left(u_{k} \cdot t_{j}\right)<em v="v">{j=1}^{n}$ and $\left(u</em>\right)} \cdot t_{j<em k="k">{j=1}^{n}$ respectively. These weights imbue our model with the ability to focus on relevant tokens. All of $W</em>$ are learned parameters.}, b_{k}, u_{k}, W_{v}, b_{v}, u_{v</p>
<h4>Entity Representation Generator</h4>
<p>To get a representation for each entity $e$, we embed its symbol into a query vector $q_{e}$ of dimension $d$ to attend to the descriptions $z \in Z$ with their respective key and value vectors $k_{z}, v_{z}$. We use scaled dot-product attention <em>(Vaswani et al., 2017)</em> and denote the resulting representation for the entity $e$ as $x_{e}$ :</p>
<p>$$
x_{e}=\sum_{i=1}^{m} \gamma_{i} v_{z_{i}} \quad \gamma=\sigma\left(\left(\frac{q_{e} \cdot k_{z_{i}}}{\sqrt{d}}\right)_{j=1}^{m}\right)
$$</p>
<p>where $m=|Z|$ is the number of descriptions in the manual. This mechanism allows EMMA to accomplish two forms of language grounding: the key and query select relevant descriptions for each object by matching entities to names (e.g. ‘mage’), and the value extracts information relevant to the entities’ behaviors in the world (e.g. enemy, chasing).</p>
<p>For each entity $e$ in the observation, we place its representation $x_{e}$ into a tensor $X \in \mathbb{R}^{h \times w \times d}$ at the same coordinates as the entity position in the observation $o$ to maintain full spatial information. The representation for the agent is simply a learned embedding of dimension $d$.</p>
<p>Action Module To provide temporal information that assists with grounding movement dynamics, we concatenate the outputs of the representation generator from the three most recent observations to obtain a tensor $X^{\prime} \in \mathbb{R}^{h \times w \times 3 d}$. To get a distribution over the actions $\pi(a \mid o, Z)$, we run a 2 D convolution on $X^{\prime}$ over the $h, w$ dimensions. The flattened feature maps are passed through a fully-connected FFN terminating in a softmax over the possible actions.</p>
<p>$$
\begin{aligned}
y &amp; =\operatorname{Flatten}\left(\operatorname{Conv2D}\left(X^{\prime}\right)\right) \
\pi(a \mid o, Z) &amp; =\sigma(\operatorname{FFN}(y))
\end{aligned}
$$</p>
<p>In contrast to previous approaches that use global observation features to read the manual (Zhong et al., 2020), we build a text-conditioned representation for each entity $\left(x_{e}\right)$. One advantage is that $x_{e}$ can directly replace the entity embeddings typically used to embed the state observation in most models while still being completely end-to-end differentiable.</p>
<p>While designed for grid environments, our approach can be extended to more complex visual inputs by using CNN features as queries to extract relevant textual information for image regions, for example. By design, EMMA can also learn to attend to relevant descriptions even if they reference multiple other entities. Our current version of MESSENGER however, does not test for these challenges and we leave grounding entities across multiple descriptions with rich visual features to future work. Further details about EMMA and its design can be found in Appendix D.</p>
<h2>6. Experimental Setup</h2>
<h3>6.1. Baselines</h3>
<p>1) Mean-Bag of Sentences (Mean-BOS) This is a variant of EMMA with the attention mechanism ablated. We average the value vectors obtained from equation 2 for each descriptor to obtain $\bar{v}$ which is used by the action module.</p>
<p>$$
\begin{aligned}
\bar{v}=\frac{1}{|Z|} \sum_{z \in Z} v_{z}, \quad y &amp; =\operatorname{Flatten}(\operatorname{Conv2D}(\operatorname{Emb}(o))) \
\pi(a \mid o, Z) &amp; =\operatorname{softmax}(\operatorname{FFN}([y ; \bar{v}]))
\end{aligned}
$$</p>
<p>2) Game ID-Conditioned (G-ID) To assess the importance of language in our setup, we test a model with no language understanding on MESSENGER. We provide an auxillary vector $I$ where each dimension corresponds to a role. $I$ is then populated with the entity symbols that reveal the mapping between entities and roles (Fig. 9, Appendix). These symbols are embedded and concatenated to form the vector $v_{I}$ which is used by the action module to generate a distribution over the next actions.</p>
<p>$$
\begin{gathered}
y=\operatorname{Flatten}(\operatorname{Conv2D}(\operatorname{Emb}(o))) \
\pi(a \mid o, Z)=\operatorname{softmax}\left(\operatorname{FFN}\left(\left[y ; v_{I}\right]\right)\right)
\end{gathered}
$$</p>
<p>3) Bayesian Attention Module (BAM) To assess the extent that co-occurrence statistics can help models learn $M$, we train a naive Bayes classifier to learn $M$. This approach is similar to word alignment models used in machine translation such as the IBM Model 1 (Brown et al., 1993). Specifically, for some set of observed entities $E^{\prime} \subseteq E$ in the current environment:</p>
<p>$$
\begin{gathered}
\operatorname{BAM}\left(z, E^{\prime}\right)=\underset{e \in E^{\prime}}{\arg \max } P(e \mid z) \
P(z \mid e)=\prod_{t \in z} P(t \mid e), \quad P(t \mid e)=\frac{C(t, e)}{\sum_{t^{\prime}} C\left(t^{\prime}, e\right)}
\end{gathered}
$$</p>
<p>where $t \in z$ are tokens in $z, t^{\prime}$ is any token in the manual vocabulary and $C$ refers to co-occurence counts. We let $x_{e}=v_{z}$ from equation 2 for the $z$ that maps to $e$. By construction, $M$ is random for BAM on SC games. Note that other models can still learn $M$ using environment rewards on SC games.
4) Oracle-Map (O-Map) To get an upper-bound on performance, we consider a model that has access to the descriptor to entity map $M$, similar to Narasimhan et al. (2018). This is identical to EMMA except that the representation for each entity $x_{e}$ is obtained as in equation 8 .</p>
<p>$$
x_{e}=\sum_{z \in Z} \mathbb{1}[M(z)=e] v_{z}
$$</p>
<p>5) txt2 $\pi$ This method was introduced by Zhong et al. (2020) alongside RTFM and features successive layers of bidirecional feature-wise modulation ( $\mathrm{FILM}^{2}$ ) to model multi-hop reasoning. Unlike RTFM, MESSENGER has only one text (the manual), hence we replace $\operatorname{txt} 2 \pi$ 's inter-text attention with self-attention. Moreover, $\operatorname{txt} 2 \pi$ does not have explicit state-tracking because it is able to identify the next correct action based on the current observation in RTFM. This is not possible in MESSENGER, hence we add a statetracker LSTM to $\operatorname{txt} 2 \pi$ before the first $\mathrm{FILM}^{2}$ layer. Unlike other baselines that embed each fact independently, $\operatorname{txt} 2 \pi$ does not explicitly distinguish between facts. Instead, it ingests the manual as a concatenated string of facts.</p>
<h3>6.2. Curriculum</h3>
<p>We introduce three stages of MESSENGER with progressive difficulty. On all stages, we train our models in a multi-task fashion by sampling a random game and appropriate manual at the start of each episode.</p>
<p>Stage 1 (S1) There are three entities corresponding to the enemy, message and goal with three corresponding descriptions. All entities begin two steps from the agent and are immovable. The agent either begins with or without the message and must interact with the correct entity. It is provided a reward of 1 if it does so, and -1 otherwise.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Average episodic rewards on S1 (left) S2 (middle) and S3 (right) on training games, as a function of training frames (x-axis). Note the discontinuous y-axis on S3. Reward is a combination of both single and multi-combination games. EMMA-(no curriculum) denotes EMMA trained directly on S2. Since the Mean-BOS and G-ID baselines were not able to fit to S2 at all, we do not transfer them to S3. All results are averaged over three seeds and shaded area indicates standard deviation. EMMA learns faster than other baselines and on S1 and S2 almost matches the performance of O-Map.</p>
<p>Stage 2 (S2) The same set of entities as stage 1 are present in stage 2, but entities are mobile and the agent always begins without the message. In each training game there is one chasing, one fleeing and one immovable entity. On test there may be any combination of movement types to force agents to adapt to unseen transition distributions $P\left(s_{t+1} \mid s_{t}, a_{t}\right)$.</p>
<p>Stage 3 (S3) In this stage there are 5 entities total with 6 descriptions, featuring one extraneous descriptor. On top of the enemy, message and goal entities present in stages 1 and 2, there are two additional copies of the message and goal entities, which are enemies and must be disambiguated by their different dynamics (e.g. 'the chasing mage is an enemy' and 'the fleeing mage is the goal.').</p>
<p>Human performance computed from expert playthroughs on S1, S2, and S3 are $98 \%, 98 \%$, and $84 \%$ respectively (see Appendix B for details). Learning the entity groundings directly on stage 2 or 3 of MESSENGER proved to be too difficult for the models we consider. Thus, we introduce a three-stage curriculum to train our models (Bengio et al., 2009). Additional details regarding the training setup can be found in Appendix C.</p>
<h2>7. Results</h2>
<h3>7.1. Multi-Task Performance</h3>
<p>Figure 4 shows rewards on training games as a function of training frames. The advantage of textual understanding is clear; on both S1 and S2, EMMA and O-Map converge to good policies much faster than the other baselines. However, all models except O-Map were not able to fit to S3. While EMMA can map the correct subset of descriptions to each entity, it struggles to disambiguate the descriptions based
on movement dynamics. Doing so requires the challenge of mapping movement descriptions to observations of entity positions relative to the agent's own through multiple frames. Furthermore, EMMA cannot fit onto S2 without pretraining on S1 (Fig. 4) due to longer episode lengths. These challenges demonstrate the need for further work on grounding text (1) to movement dynamics and (2) with long trajectories and sparse rewards.</p>
<p>Table 1 details win rates on the training games, with a breakdown over single (SC) and multi combination (MC) games. All models were able to fit to S1, but on S2 and S3, some models exhibited win rates close to random. We observe that on MC games, the naive Bayes classifier can achieve competitive win rates by assigning over $99 \%$ of training descriptors correctly. However, on SC games which require interactive entity grounding, win rates are up to $60 \%$ lower. This result highlights the importance of distinguishing entity groundings induced from co-occurrence statistics, and those learned from environment interactions.</p>
<p>Our model (EMMA) can consistently win on both MC and SC games in S1 and S2, demonstrating EMMA's ability to ground entities without co-occurrences statistics between entity and text symbols to guide its grounding. While txt $2 \pi$ is able to fit to the S1 training games, it requires an order of magnitude more steps to do so compared to EMMA. This is likely because txt $2 \pi$ must learn to distinguish between facts observed as a concatenated string, while lacking an explicit entity-manual grounding module.</p>
<h3>7.2. Generalization</h3>
<p>Test Games Results on test games are presented in Table 2. The G-ID, Mean-BOS and txt $2 \pi$ baselines fail to general-</p>
<p>Table 1. Win rates (± stddev.) over three seeds on train. All, MC, and SC denote overall, multi and single-combination games respectively. The performance of random agent subject to the same step limit on S1, S2, S3 is 7.8%, 2.1% and 1.6% respectively.</p>
<table>
<thead>
<tr>
<th></th>
<th>S1-ALL</th>
<th>S1-MC</th>
<th>S1-SC</th>
<th>S2-ALL</th>
<th>S2-MC</th>
<th>S2-SC</th>
<th>S3-ALL</th>
<th>S3-MC</th>
<th>S3-SC</th>
</tr>
</thead>
<tbody>
<tr>
<td>G-ID</td>
<td>89 ± 3.8</td>
<td>90 ± 5.5</td>
<td>89 ± 3.7</td>
<td>3.6 ± 0.6</td>
<td>3.4 ± 0.7</td>
<td>3.9 ± 1.5</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>MEAN-BOS</td>
<td>90 ± 7.2</td>
<td>91 ± 6.5</td>
<td>90 ± 6.8</td>
<td>2.1 ± 0.5</td>
<td>2.9 ± 1.4</td>
<td>2.4 ± 0.6</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>BAM</td>
<td>84 ± 1.3</td>
<td>97 ± 0.9</td>
<td>51 ± 1.6</td>
<td>69 ± 1.1</td>
<td>85 ± 0.9</td>
<td>22 ± 4.8</td>
<td>1.4 ± 0.3</td>
<td>1.6 ± 0.5</td>
<td>1.6 ± 0.8</td>
</tr>
<tr>
<td>TXT2π</td>
<td>98 ± 2.1</td>
<td>98 ± 2.9</td>
<td>99 ± 1.7</td>
<td>94 ± 3.5</td>
<td>95 ± 2.1</td>
<td>94 ± 4.0</td>
<td>3.0 ± 0.6</td>
<td>2.9 ± 0.5</td>
<td>2.8 ± 0.3</td>
</tr>
<tr>
<td>EMMA</td>
<td>88 ± 2.3</td>
<td>88 ± 2.4</td>
<td>87 ± 1.6</td>
<td>95 ± 0.4</td>
<td>96 ± 0.2</td>
<td>95 ± 0.5</td>
<td>22 ± 3.8</td>
<td>21 ± 3.6</td>
<td>19 ± 2.9</td>
</tr>
<tr>
<td>O-MAP</td>
<td>97 ± 0.8</td>
<td>97 ± 0.3</td>
<td>96 ± 0.6</td>
<td>96 ± 0.8</td>
<td>96 ± 0.4</td>
<td>94 ± 0.4</td>
<td>85 ± 1.5</td>
<td>86 ± 0.7</td>
<td>85 ± 0.7</td>
</tr>
</tbody>
</table>
<p>Table 2. Win rates (± stddev.) on test games over three seeds. EMMA achieves win rates on S1 and S2 test games competitive with O-Map, but performance on S3 is significantly lower.</p>
<table>
<thead>
<tr>
<th></th>
<th>S1-TEST</th>
<th>S2-TEST</th>
<th>S3-TEST</th>
</tr>
</thead>
<tbody>
<tr>
<td>G-ID</td>
<td>18 ± 8.2</td>
<td>5.2 ± 0.2</td>
<td>-</td>
</tr>
<tr>
<td>MEAN-BOS</td>
<td>6.7 ± 2.8</td>
<td>4.7 ± 0.5</td>
<td>-</td>
</tr>
<tr>
<td>BAM</td>
<td>66 ± 1.5</td>
<td>41 ± 1.7</td>
<td>2.7 ± 0.9</td>
</tr>
<tr>
<td>TXT2π</td>
<td>2.5 ± 1.7</td>
<td>0.3 ± 0.08</td>
<td>2.6 ± 0.3</td>
</tr>
<tr>
<td>EMMA</td>
<td>85 ± 1.4</td>
<td>85 ± 0.6</td>
<td>10 ± 0.8</td>
</tr>
<tr>
<td>O-MAP</td>
<td>97 ± 0.3</td>
<td>87 ± 1.8</td>
<td>80 ± 1.5</td>
</tr>
</tbody>
</table>
<p>ize in all cases. Although the models have complete access to distinguishing information necessary to succeed, they overfit to entity-role assignments observed during training. BAM demonstrates some ability to generalize to test games, but performance on games with single-combination entities are considerably lower, bringing the average down.</p>
<p>In contrast, EMMA wins 85% of test games on S1 and S2, almost matching the performance of the O-Map model. By extracting information from the relevant descriptor for each entity, EMMA is able to considerably simplify each task — it simply needs to learn a policy for how to interact with <em>enemy</em>, <em>messenger</em> and <em>goal</em> archetypes instead of memorizing a policy for each combination of entities. This abstraction facilitates knowledge sharing between games, and generalization to unseen games. However, test performance on S3 for all models except O-Map does not exceed 10%.</p>
<p><strong>New Entities</strong> To assess EMMA's ability to pick up novel game mechanics not specified in the manual, we introduce two new stationary collectibles into MESSENGER — a trap and gold which provide additional rewards of −1 and 1 respectively. An optimal agent in this new scenario will obtain the message and also collect the gold before reaching the goal, while avoiding the enemy and the trap. We transfer EMMA trained up to S2 onto 32 unseen games with these new entities. EMMA learns the new dynamics while accomplishing the original objectives in MESSENGER (Figure 5). Compared to training from scratch, EMMA pretrained on S2 achieves a higher reward in this modified setting in the same number of steps, exceeding the previous maximum reward.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Transfer performance of EMMA on S2 games with novel entities and reward mechanics not found in MESSENGER. EMMA trained on MESSENGER (transfer) learns the new games much faster than a model trained from scratch (scratch).</p>
<p>Table 3. Win rates (± stddev.) over three seeds for EMMA on negation (Neg) and neutral (Neu) cases on S1 and S2.</p>
<table>
<thead>
<tr>
<th></th>
<th>S1-NEU</th>
<th>S2-NEU</th>
<th>S1-NEG</th>
<th>S2-NEG</th>
</tr>
</thead>
<tbody>
<tr>
<td>TRAIN</td>
<td>92 ± 1.0</td>
<td>95 ± 0.4</td>
<td>87 ± 3.8</td>
<td>88 ± 8.8</td>
</tr>
<tr>
<td>TEST</td>
<td>88 ± 0.7</td>
<td>78 ± 2.5</td>
<td>67 ± 29</td>
<td>59 ± 33</td>
</tr>
</tbody>
</table>
<p>in S2 in 1 × 10⁶ steps.</p>
<h3>7.3. Robustness</h3>
<p><strong>Train-Time</strong> We test EMMA's ability to learn entity groundings with added neutral entities and negated descriptions on S2 (Table 3). Due to poor performance of all models on S3, we conduct these studies on S1 and S2 only.</p>
<p><strong>Neutral entities.</strong> At the start of each episode, we randomly select one of five neutral entities and insert it into the observation. The neutral entities are not described by the text, do not interact with the agent and provide no reward signal. The neutral entities are distinct from the entities in figure 2.</p>
<p><strong>Negation.</strong> On each training episode with probability 0.25 we select one description, negate it, and change the role. (e.g. 'the mage is an enemy' becomes 'the mage is not the message'). This case forces the model to consider the roles of the other two entities to deduce the role of the entity with the negated description. While EMMA can ground entities</p>
<p>Table 4. Win rates (± stddev.) on S2 test games over three seeds for Append, Delete and Synonym cases for EMMA and BAM.</p>
<table>
<thead>
<tr>
<th></th>
<th>APPEND</th>
<th>DELETE</th>
<th>SYNONYMS</th>
</tr>
</thead>
<tbody>
<tr>
<td>BAM</td>
<td>$36 \pm 1.7$</td>
<td>$17 \pm 2.0$</td>
<td>$8.5 \pm 1.0$</td>
</tr>
<tr>
<td>EMMA</td>
<td>$\mathbf{7 8} \pm \mathbf{3 . 9}$</td>
<td>$\mathbf{3 3} \pm \mathbf{2 . 3}$</td>
<td>$\mathbf{7 5} \pm \mathbf{3 . 4}$</td>
</tr>
</tbody>
</table>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Attention weights for EMMA computed from equation 3. Each row is the attention weights for one entity over 12 random descriptors (one for each entity indicated by the column label). EMMA learns to map each description to the entity it references.
and performs well with neutral entities, it sometimes fails to ground the entities correctly with negated descriptions, affecting its performance on test games.</p>
<p>Test-Time We assess the robustness of trained BAM and EMMA models against text variations on S2 test games in table 4. We test each model's ability to: (1) handle an extra descriptor for an entity not found in the game (Append), (2) reason about the role of objects without a descriptor by deleting a sentence from the input at random (Delete) and (3) generalize to unseen synonyms (Synonyms). For the last case, we use (unseen) templated descriptions filled in with entity synonyms not seen during training.</p>
<p>Both models can retain their performance when presented with an extraneous description and suffer considerably when a description is deleted. However, EMMA generalize to unseen entity synonyms winning $75 \%$ of games compared to $8.5 \%$ by the BAM model in this setting.</p>
<h3>7.4. Analysis of Grounding</h3>
<p>We visualize the attention weights for EMMA in Figure 6. To assess the overall latent mapping learned by our model, we evaluate the attention weights over 12 descriptions, one for every entity. EMMA places most weight for entity $e$ onto its descriptor $z_{e}$. In particular, EMMA learns a grounding
for dog, bird, fish, mage, sword and orb - entities for which co-occurrence statistics provide no meaningful alignment information, demonstrating that our model can learn groundings for these entities via interaction alone.</p>
<h2>8. Conclusion</h2>
<p>In this paper, we introduce a new environment MESSENGER which does not provide prior knowledge connecting text and state observations - the control policy must simultaneous learn to ground a natural language manual to symbols and dynamics in the environment. We develop a new model, EMMA (Entity Mapper with Multi-modal Attention) that leverages text descriptions for generalization of control policies to new environments. EMMA employs a multi-modal entity-conditioned attention module and learns a latent grounding of entities and dynamics using only environment rewards. Our empirical results on MESSENGER demonstrate that EMMA shows strong generalization performance and robust grounding of entities. However, the hardest stage of MESSENGER which requires grounding language to subtle differences in movement patterns remains difficult for EMMA and other state of the art models. We hope our work will lead to further research on generalization for RL using natural language.</p>
<h2>Acknowledgements</h2>
<p>We are grateful to Ameet Deshpande, Jens Tuyls, Michael Hu, Shunyu Yao, Tsung-Yen Yang, Willie Chang and anonymous reviewers for their helpful comments and suggestions. We would also like to thank the anonymous AMT workers for their indispensable contributions to this work. This work was financially supported by the Princeton SEAS Senior Thesis Fund.</p>
<h2>References</h2>
<p>Andreas, J. and Klein, D. Alignment-based compositional semantics for instruction following. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 1165-1174, Lisbon, Portugal, 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1138. URL https://www.aclw eb.org/anthology/D15-1138.</p>
<p>Andreas, J., Klein, D., and Levine, S. Modular multitask reinforcement learning with policy sketches. In Precup, D. and Teh, Y. W. (eds.), Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pp. 166-175. PMLR, 2017. URL http://proceedings.mlr . press/v70/andreas17a.html.</p>
<p>Bahdanau, D., Hill, F., Leike, J., Hughes, E., Kohli, P., and Grefenstette, E. Learning to follow language instructions with adversarial reward induction. arXiv preprint arXiv:1806.01946, 2018.</p>
<p>Bengio, Y., Louradour, J., Collobert, R., and Weston, J. Curriculum learning. In Danyluk, A. P., Bottou, L., and Littman, M. L. (eds.), Proceedings of the 26th Annual International Conference on Machine Learning, ICML 2009, Montreal, Quebec, Canada, June 1418, 2009, volume 382 of ACM International Conference Proceeding Series, pp. 41-48. ACM, 2009. doi: 10.1145/1553374.1553380. URL https://doi.or g/10.1145/1553374.1553380.</p>
<p>Blukis, V., Terme, Y., Niklasson, E., Knepper, R. A., and Artzi, Y. Learning to map natural language instructions to physical quadcopter control using simulated flight. arXiv preprint arXiv:1910.09664, 2019.</p>
<p>Branavan, S., Silver, D., and Barzilay, R. Learning to win by reading manuals in a Monte-Carlo framework. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pp. 268-277, Portland, Oregon, USA, 2011. Association for Computational Linguistics. URL https: //www.aclweb.org/anthology/P11-1028.</p>
<p>Brown, P. F., Della Pietra, S. A., Della Pietra, V. J., and Mercer, R. L. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263-311, 1993. URL https://www. aclweb.org/anthology/J93-2003.</p>
<p>Buhrmester, M., Kwang, T., and Gosling, S. D. Amazon's mechanical turk: A new source of inexpensive, yet highquality data? 2016.</p>
<p>Chen, D. L. and Mooney, R. J. Learning to interpret natural language navigation instructions from observations. In Burgard, W. and Roth, D. (eds.), Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2011, San Francisco, California, USA, August 7-11, 2011. AAAI Press, 2011. URL http://www.aaai.org/ocs/index.php/AAA I/AAAI11/paper/view/3701.</p>
<p>Chevalier-Boisvert, M., Bahdanau, D., Lahlou, S., Willems, L., Saharia, C., Nguyen, T. H., and Bengio, Y. BabyAI: First steps towards grounded language learning with a human in the loop. In International Conference on Learning Representations, 2019. URL https://openre view.net/forum?id=rJeXCo0cYX.</p>
<p>Co-Reyes, J. D., Gupta, A., Sanjeev, S., Altieri, N., Andreas, J., DeNero, J., Abbeel, P., and Levine, S. Guiding policies with language via meta-learning. In 7th International</p>
<p>Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/for um?id=HkgSEnA5KQ.</p>
<p>Daniele, A. F., Bansal, M., and Walter, M. R. Navigational instruction generation as inverse reinforcement learning with neural machine translation. In 2017 12th ACM/IEEE International Conference on Human-Robot Interaction (HRI, pp. 109-118. IEEE, 2017.</p>
<p>Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://www.aclweb.org/anthology/N19 -1423 .</p>
<p>Fried, D., Andreas, J., and Klein, D. Unified pragmatic models for generating and following instructions. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 1951-1963, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1177. URL https://www.aclw eb.org/anthology/N18-1177.</p>
<p>Golland, D., Liang, P., and Klein, D. A game-theoretic approach to generating spatial descriptions. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pp. 410-419, Cambridge, MA, October 2010. Association for Computational Linguistics. URL https://www.aclweb.org/anthology /D10-1040.</p>
<p>Hemachandra, S., Walter, M. R., Tellex, S., and Teller, S. Learning spatial-semantic representations from natural language descriptions and scene classifications. In Robotics and Automation (ICRA), 2014 IEEE International Conference on, pp. 2623-2630. IEEE, 2014.</p>
<p>Hermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R., Soyer, H., Szepesvari, D., Czarnecki, W. M., Jaderberg, M., Teplyashin, D., et al. Grounded language learning in a simulated 3d world. arXiv preprint arXiv:1706.06551, 2017.</p>
<p>Hill, F., Lampinen, A. K., Schneider, R., Clark, S., Botvinick, M., McClelland, J. L., and Santoro, A. Environmental drivers of systematicity and generalization in a situated agent. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,</p>
<p>April 26-30, 2020. OpenReview.net, 2020a. URL https: //openreview.net/forum?id=SklGryBtwr.</p>
<p>Hill, F., Mokra, S., Wong, N., and Harley, T. Human instruction-following with deep reinforcement learning via transfer-learning from text. arXiv preprint arXiv:2005.09382, 2020b.</p>
<p>Hill, F., Tieleman, O., von Glehn, T., Wong, N., Merzic, H., and Clark, S. Grounded language learning fast and slow. arXiv preprint arXiv:2009.01719, 2020c.</p>
<p>Hu, R., Fried, D., Rohrbach, A., Klein, D., Darrell, T., and Saenko, K. Are you looking? grounding to multiple modalities in vision-and-language navigation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 6551-6557, Florence, Italy, 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1655. URL https: //www.aclweb.org/anthology/P19-1655.</p>
<p>Janner, M., Narasimhan, K., and Barzilay, R. Representation learning for grounded spatial reasoning. Transactions of the Association for Computational Linguistics, 6:4961, 2018. doi: 10.1162/tacl_a_00004. URL https: //www.aclweb.org/anthology/Q18-1004.</p>
<p>Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In Bengio, Y. and LeCun, Y. (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http: //arxiv.org/abs/1412.6980.</p>
<p>Küttler, H., Nardelli, N., Miller, A. H., Raileanu, R., Selvatici, M., Grefenstette, E., and Rocktäschel, T. The nethack learning environment. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurip s.cc/paper/2020/hash/569ff987c643b4b edf504efda8f786c2-Abstract.html.</p>
<p>Luketina, J., Nardelli, N., Farquhar, G., Foerster, J. N., Andreas, J., Grefenstette, E., Whiteson, S., and Rocktäschel, T. A survey of reinforcement learning informed by natural language. In Kraus, S. (ed.), Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, pp. 6309-6317. ijcai.org, 2019. doi: 10.24963/ijcai.2019/880. URL https://doi.org/ 10.24963/ijcai.2019/880.</p>
<p>Misra, D., Langford, J., and Artzi, Y. Mapping instructions and visual observations to actions with reinforcement
learning. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 1004-1015, Copenhagen, Denmark, 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-11 06. URL https://www.aclweb.org/antholo gy/D17-1106.</p>
<p>Narasimhan, K., Barzilay, R., and Jaakkola, T. Grounding language for transfer in deep reinforcement learning. Journal of Artificial Intelligence Research, 63:849-874, 2018.</p>
<p>Oh, J., Singh, S. P., Lee, H., and Kohli, P. Zero-shot task generalization with multi-task deep reinforcement learning. In Precup, D. and Teh, Y. W. (eds.), Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pp. 2661-2670. PMLR, 2017. URL http://procee dings.mlr.press/v70/oh17a.html.</p>
<p>Reckman, H., Orkin, J., and Roy, D. Learning meanings of words and constructions, grounded in a virtual game. Semantic Approaches in Natural Language Processing, pp. 67, 2010.</p>
<p>Schaul, T. A video game description language for modelbased or interactive learning. In Computational Intelligence in Games (CIG), 2013 IEEE Conference on, pp. 1-8. IEEE, 2013.</p>
<p>Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.</p>
<p>Tellex, S., Gopalan, N., Kress-Gazit, H., and Matuszek, C. Robots that use language. Annual Review of Control, Robotics, and Autonomous Systems, 3:25-55, 2020.</p>
<p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. In Guyon, I., von Luxburg, U., Bengio, S., Wallach, H. M., Fergus, R., Vishwanathan, S. V. N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 5998-6008, 2017. URL https://proceedings.neurips.cc/paper /2017/hash/3f5ee243547dee91fbd053c1c 4a845aa-Abstract.html.</p>
<p>Walter, M. R., Hemachandra, S., Homberg, B., Tellex, S., and Teller, S. Learning semantic maps from natural language descriptions. Robotics: Science and Systems, 2013.</p>
<p>Wang, X., Huang, Q., Çelikyilmaz, A., Gao, J., Shen, D., Wang, Y., Wang, W. Y., and Zhang, L. Reinforced crossmodal matching and self-supervised imitation learning for vision-language navigation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pp. 6629-6638. Computer Vision Foundation / IEEE, 2019. doi: 10.110 9/CVPR.2019.00679. URL http://openaccess .thecvf.com/content_CVPR_2019/html/Wan g_Reinforced_Cross-Modal_Matching_and_S elf-Supervised_Imitation_Learning_for _Vision-Language_Navigation_CVPR_2019_p aper.html.</p>
<p>Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., and Brew, J. Huggingface's transformers: State-of-theart natural language processing. ArXiv, abs/1910.03771, 2019.</p>
<p>Zhong, V., Rocktäschel, T., and Grefenstette, E. RTFM: generalising to new environment dynamics via reading. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https: //openreview.net/forum?id=SJgob6NKvH.</p>
<h2>A. Text Manual</h2>
<p>Table 5. Example template descriptions. Each underlined word in the example input indicate blanks that may be swapped in the template. Each template takes a word for the object being described (bird, thief, mage), its role (enemy, message, goal) and an adjective (dangerous, secret, crucial).</p>
<h2>Example Input</h2>
<ul>
<li>The bird that is coming near you is the dangerous enemy.</li>
<li>The secret message is in the thief's hand as he evades you.</li>
<li>The immovable object is the mage who holds a goal that is crucial.</li>
</ul>
<h2>Enemy Descriptions</h2>
<p>Adjectives: dangerous, deadly, lethal
Role: enemy, opponent, adversary</p>
<h2>Message Descriptions</h2>
<p>Adjectives: restricted, classified, secret
Role: message, memo, report</p>
<h2>Goal Descriptions</h2>
<p>Adjectives: crucial, vital, essential
Role: goal, target, aim</p>
<p>To collect the text manual, we first crowdsource 82 templates (with 2,214 possible descriptions after filling in the blanks). Each Amazon Mechanical Turk worker is asked to paraphrase a prompt sentence while preserving words in boldface (which become the blanks in our templates). We have three blanks per template, one each for the entity, role and an adjective. For each role (enemy, message, goal) we have three role words and three adjectives that are synonymous (Table 5). Each entity is also described in three synonymous ways. Thus, every entity-role assignment can be described in 27 different ways on the same template. Raw templates are filtered for duplicates, converted to lowercase, and corrected for typos to prevent confusion on downstream collection tasks.</p>
<p>To collect the free form text for a specific entity-role assignment, we first sample a random template and fill each blank with one of the three possible synonyms. The filled template</p>
<p>Table 6. Example descriptions for Messenger after the second round of data collection. Note the use of synonyms flying machine and airplane, which also needs to be disambiguated from winged creature (bird). Some descriptions have information divided across two separate sentences. We do not correct typos (italics). Some typos (plan instead of plane) render the description useless, forcing the agent to infer the correct entity form the other descriptions in the text manual.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>becomes the prompt that is shown to the worker. For each prompt, we obtain two distinct paraphrased sentences to promote response diversity.</p>
<p>On all tasks (template and free-form) we provide an example prompt (which is distinct from the one provided) and example responses to provide additional task clarity. Aside from lower-casing the free-form descriptions and removing duplicate responses, we do no further preprocessing.</p>
<p>To ensure fluency in all responses, we limited workers to those located in the United States with at least 10,000 completed HITs and an acceptance rate of $\geq 99 \%$. Some representative responses of free-form responses are presented in table 6 . We paid our workers US $\$ 0.25$ for each pair of sentences, as we found the task was usually finished in $\leq$ 1 min . This translates to a pay of at least $\$ 15$ per hour per worker.</p>
<h2>B. Environment Details</h2>
<p>Table 7. Basic information about our environment Messenger. Each game features 3 out of 12 possible unique non-agent entities, with up to 5 non-agent entities total. Each entity is assigned a role of enemy, message or goal.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Entities</th>
<th style="text-align: center;">bird, dog, fish, scientist, queen, thief, airplane, robot, ship, mage, sword, orb</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Roles</td>
<td style="text-align: center;">enemy, message, goal</td>
</tr>
<tr>
<td style="text-align: center;">Movements</td>
<td style="text-align: center;">chasing, fleeing, immovable</td>
</tr>
<tr>
<td style="text-align: center;">Total games</td>
<td style="text-align: center;">$P(12,3)=1320$</td>
</tr>
</tbody>
</table>
<p>Details about Messenger can be found in table 7. On stage 1 (S1), the three entities start randomly in three out of four possible locations, two cells away from the agent. The agent always begins in the center of the grid. It starts without the message with probability 0.8 and begin with the message otherwise. When the agent obtains the message, we capture this information by changing the agent symbol in the observation.</p>
<p>On stage 2 (S2), the agent and entities are shuffled between four possible starting locations at the start of each episode. On S2, the mobile entities (fleeing, chasing) move at half the speed of the agent. On S2 train, there is always one chasing, one fleeing and one immovable entity. Test games can feature any combination of movement dynamics.</p>
<p>On stage 3 (S3), the agent and non-player entities are shuffled between 6 possible starting locations. As with S2, entities move at half the speed of the agent. The one distractor description may either reference the enemy as a message or a goal, with a movement type that is distinct from the true movement type of the enemy. S3 test games do not feature unseen movement combinations, since the movements of the entities are integral to the gameplay in S3.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Manual:
the airplane is an important target and it is leaving the bot that does not move is the crucial target. the deadly opponent coming towards you is the robot. the dangerous adversary, the plane is coming toward you. a lethal ship is the enemy that runs away from you. within the classified document is a station ferry.</p>
<p>Figure 7. An S3 game on the interface used to collect human playthroughs. A0 represents the agent and other entities are represented by the first two letters of the entity name in Table 7.</p>
<p>Since there are only 4 single-combination (SC) training games and 40 multi-combination (MC) training games, we sample the games non-uniformly at the start of each episode to ensure that there is enough interaction with SC entities to induce an entity grounding. On all stages we sample an SC game with probability 0.25 and an MC game otherwise. Not all descriptions have movement type information (e.g. 'chasing'). We also collect unknown type descriptions with no movement type information. During training, in S1 and S2, each description is independently an unknown type description with probability 0.15 . On S3, we do not provide any description with no movement information, since this would render disambiguation via movement differences impossible.</p>
<p>Human Playthroughs We collect expert human playthroughs using the interface presented in Figure 7. The human expert has access to the manual, navigation commands, and a text-rendered grid observation. The grid observation uses the first two letters of the entity name from Table 7 to represent each entity. Thus, human performance does not reflect the challenge of grounding entities by playing the environment; rather it quantifies the difficulty of completing the task with entity groundings provided upfront.</p>
<p>Terminal Rewards On S2, we provide an intermediate scalar reward of 0.5 for obtaining the message. To assess whether only terminal rewards is sufficient for EMMA to learn a good policy on MESSENGER, we evaluate EMMA on S2 using $\pm 1$ terminal rewards in Figure 8. Intermediate rewards help EMMA converge to a higher win rate slightly faster, but EMMA can converge to the same win rate using just terminal rewards.</p>
<p>Negation We procedurally generate the negated text by negating existential words (e.g. 'is an enemy' becomes 'is
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. Win rates of EMMA on MESSENGER with intermediate rewards (Message + Terminal) and terminal rewards only (Terminal Only) on S2 games. Results are over three seeds and shaded area indicates standard deviation.
not an enemy'). We manually negate those descriptions not captured by the rules. During both training and evaluation, we provide a complete text manual without any negated description with 0.75 probability, and randomly select a description in the manual to negate otherwise. When we negate an entity description $z_{e}$ to $z_{e}^{\prime}$, we also change the role ('...is an enemy' becomes '...is not a goal', for example). Thus the information present in the manual has not changed, but the agent must look at the remaining two descriptions to deduce the role of $e$ with description $z_{e}^{\prime}$.</p>
<p>Transfer Learning We test transfer by introducing two new entities - a trap and a gold which provide rewards of -1 and 1 respectively. Both collectables are randomly shuffled between two possible starting locations at the start of each episode and do not move. We train the models in this new setting in a multi-task fashion on the 32 validation games. After the agent encounters either the trap or gold, the collected item disappears. Neither item terminates the episode and the agent can still win or lose the current episode regardless of whether it has picked up the gold or trap.</p>
<h2>B.1. Comparison with RTFM</h2>
<p>The main novelty of our work (both the MESSENGER environment and our model) is in specifically tackling the issue of entity grounding without any prior knowledge. To do this, MESSENGER in contrast to RTFM (1) does not have any signal connecting entities to text references, (2) features much richer language, and (3) requires interaction in the environment to ground entities to text. We describe these in more detail:</p>
<ol>
<li>RTFM's observation space consists of a grid of text in which entity names are identical to their correspond-</li>
</ol>
<p>ing references in the manual. Thus, both the text in the manual and the observation are embedded into the same space (e.g. using the same word vectors), essentially providing models with the entity grounding upfront. In contrast, our environment has a separate set of symbols for the entities with no relation to the text in our manual. Thus, the entities and text are embedded into different spaces, and learning to map between these two spaces is the key challenge in our environment that has not been explored before.
2. RTFM features only 32 total rule-based templates for the text, and each entity can only be referred to in a single way (goblin is always 'goblin'). In contrast, we crowdsourced thousands of completely free-form descriptions in two rounds using Amazon Mechanical Turk. After obtaining the seed templates from the first round, we intentionally inject multiple synonyms for each entity to construct each prompt for the second round. Workers often further paraphrased these synonyms, resulting in 5, 6 or often more ways to describe the same entity (e.g. 'airplane', 'jet', 'flying machine', 'aircraft', 'airliner' all describe plane.). The need to map these different text references to the same entity symbol further complicates the entity grounding problem in our case and more closely mirrors the challenges of grounding entities in the real world. We believe MESSENGER provides a much closer approximation to natural language compared to RTFM.
3. RTFM features all possible combinations of entities during training which provides an additional signal that may simplify the grounding problem.
4. Each entity in RTFM only moves in a single way, whereas in MESSENGER, each entity may have different dynamics such as fleeing, chasing, and immovable entities (and this is also described in the text). This also allows us to test our model's ability to generalize to unseen dynamics with unseen entity movement combinations, whereas in RTFM the evaluation on unseen games is essentially state-estimation.</p>
<p>MESSENGER shares many aspects with RTFM (e.g. gridworld with different entities and goals). That said, there are numerous reasons why we were not able to adapt the original RTFM environment to meet our requirements. We enumerate them here:</p>
<ol>
<li>The dynamics in RTFM make entity grounding (the primary focus of our work) difficult. MESSENGER requires much simpler reasoning than RTFM, and it is already too difficult to ground entities directly in MESSENGER without a curriculum. RTFM sidesteps the issue by providing this grounding beforehand.</li>
<li>Obtaining enough crowdsourced descriptions is hard with RTFM because of the more complicated dynamics. In RTFM, there are monsters, weapons, elements, modifiers, teams, variable goals and different weaknesses between entity types that need to be specified. Collecting enough descriptions that are entirely human written would be challenging. (RTFM sidesteps this issue by using templates to generate their text manual). In contrast, there are only entities, 3 roles, and a fixed goal in MESSENGER, making the text-collection task much more tractable.</li>
<li>The entities in our MESSENGER environment are carefully chosen to make entity grounding harder. In RTFM, each entity is referred to in a single way, and it is not clear how to refer to them in multiple ways (e.g. there are not too many other ways to say 'goblin'). In contrast, we specifically chose a set of entities that allowed for multiple ways of description, and actively encouraged this during data collection.</li>
<li>The combination of entities that appear during training in MESSENGER is carefully designed. This is so that we can introduce single-combination games and the associated grounding challenges that come with it.</li>
<li>We have different movement types for each entity. These different movements are referred to in our text manual and significantly increase the richness and variety of descriptions we collected, and also allow us to test generalization to unseen movement combinations. In RTFM, the entity movements are the same and fixed for all entities.</li>
<li>Each entity's attribute is referenced in the observation in RTFM, e.g. the grid has entries such as fire goblin. We could add to the cell an extra symbol for fire, but this further obfuscates the entity grounding problem we are focusing on, because we would also need to obtain a grounding for all the attributes such as fire.</li>
</ol>
<h2>C. Implementation and Training Details</h2>
<p>All models are end-to-end differentiable and we train them using proximal policy optimization (PPO) (Schulman et al., 2017) and the Adam optimizer (Kingma \&amp; Ba, 2015) with a constant learning rate of $5 \times 10^{-5}$. We also evaluated learning rates of $5 \times 10^{-4}$ and unroll lengths of 32 and 64 steps by testing on the validation games. On S1, S2 and S3 we limit each episode to 4,64 , and 128 steps respectively and provide a reward of -1 if the agent does not complete the objective within this limit. Note that the computation of random agent performance is also subject to these step constraints.</p>
<p>For all experiments we use $d=256$. When multiple entities $E^{\prime}$ overlap in the observation, we fill the overlapping cell with the average of the entity representations $\frac{1}{\left|E^{\prime}\right|} \sum_{e \in E^{\prime}} x_{e}$. The convolutional layer consists of $2 \times 2$ kernels with stride 1 and 64 feature maps. The FFN in the action module is fully-connected with 3 layers and width of 128 . To give the Mean-BOS and G-ID baselines (Fig. 9) the ability to handle the additional conditioning information, we introduce an additional layer of width 512 at the front of the FFN for those baselines only. Between each layer, we use leaky ReLU as the activation function.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9. G-ID model</p>
<p>We pretrain BAM on $1.5 \times 10^{6}$ episodes. If two descriptions map to the same entity, we take the one with higher $P(e \mid z)$, and If an entity receives no assignment we represent it with a learned default embedding $\operatorname{Emb}(e)$. txt $2 \pi$ is trained using 10-12 actors, a model dimension of 128, and a learning rate of 0.0002 .</p>
<p>We train models for up to 12 hours on S1, 48 hours on S2 and 72 hours on each S3. We use the validation games to save the model parameters with the highest validation win rate during training and use these parameters to evaluate the models on the test games. All experiments were conducted on a single Nvidia RTX2080 GPU.</p>
<h2>D. Model Design</h2>
<p>The weights $u_{k}$ and $u_{v}$ were introduced to make sure that the token embeddings for filler words such as 'the', 'and', 'or' do not drown out the words relevant to the task when we take the average in equations 1 and 2. Qualitatively, we observe that $u_{k}$ learns to focus on tokens informative for identifying the entity (e.g. mage, sword) while $u_{v}$ learns to focus on tokens that help identify the entities' roles (e.g. enemy, message).</p>
<p>We also found that using a pretrained language model was critical for success due to the large number of ways to refer to a single entity (e.g. 'airplane', 'jet', 'flying machine', 'aircraft', 'airliner' all refer to plane).</p>
<h2>D.1. Model Variations</h2>
<p>We consider a variation to EMMA. Instead of obtaining token weights $\alpha, \beta$ in equations 1 and 2 by taking a softmax</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10. Average episodic rewards on S1 (top) and S2 (bottom) on training (thick line) and validation (thin line) games, as a function of training steps (x-axis) for both EMMA (solid line) and EMMA- $S$ (dotted line). Both models are able to perform well, however, EMMA is able to obtain a good validation reward faster. All results are averaged over three seeds and shaded area indicates standard deviation.
over the token-embedding and vector products $u_{k} \cdot t$ and $u_{v} \cdot t$, we consider independently scaling each token using a sigmoid function. Specifically, we will obtain key and value vectors $k_{z}$ and $v_{z}$ using:</p>
<p>$$
\begin{aligned}
&amp; k_{z}=\sum_{i=1}^{n} \frac{S\left(u_{k} \cdot t_{i}\right)}{\sum_{i=1}^{n} S\left(u_{k} \cdot t_{i}\right)} W_{k} t_{i}+b_{k} \
&amp; v_{z}=\sum_{i=1}^{n} \frac{S\left(u_{v} \cdot t_{i}\right)}{\sum_{i=1}^{n} S\left(u_{v} \cdot t_{i}\right)} W_{v} t_{i}+b_{v}
\end{aligned}
$$</p>
<p>where $S$ is the logistic sigmoid function, and all other details are identical to EMMA. We call this model EMMA- $S$. We notice that both EMMA and EMMA- $S$ are able to obtain good training and validation performance, whith EMMA- $S$ obtaining higher rewards on S2. However, on S1, EMMA is able to obtain a higher validation reward faster (Fig. 10). Moreover, EMMA can learn robust groundings even with neutral entities, while EMMA- $S$ often overfits to a spurious grounding with neutral entities (Fig. 11). Although the independent scaling in EMMA- $S$ allows the model to consider more tokens simultaneously, the softmax selection of EMMA facilitates more focused selection of relevant tokens, and this may help prevent overfitting.</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11. Average episodic rewards on S1 games with negation (top) and neutral entities (bottom) on training (thick line) and validation (thin line) games, as a function of training steps (x-axis) for both EMMA (solid line) and EMMA- $S$ (dotted line). Both models struggle on negation, but EMMA is able to perform well with neutral entities. All results are averaged over three seeds and shaded area indicates standard deviation. Note the shared x-axis.</p>
<h2>D.2. Comparison with Transformer</h2>
<p>EMMA relies heavily on the dot-product attention mechanism to extract relevant information from the text manual. To assess the extent that attention alone is sufficient for solving MESSENGER, we train a Transformer (Vaswani et al., 2017) on MESSENGER.</p>
<p>Specifically, we use a pretrained BERT-base model (Devlin et al., 2019) that is identical to the one used by EMMA. We first concatenate the text descriptions $d_{1}, \ldots, d_{n}$ to form the manual string $s_{m}$. For each entity in the observation, we generate a string $s_{e}$ by indicating the $x$ and $y$ coordinates for every entity $e$ as follows: ' $e: x, y$;'. We then convert the entire grid observation into a string $s_{o}$ by concatenating $s_{e}$ for every entity $e$ in the observation. The final input to BERT is then $s_{m}$ [SEP] $s_{o}$. We train action and value MLPs on top of the [CLS] representation in the final layer of the BERT model. The MLPs are identical to the ones used in EMMA. The entire model is end-to-end differentiable and we train it using PPO using an identical setup to the one used to train EMMA.</p>
<p>The results of training this Transformer baseline on S1 is presented in Figure 12. While EMMA is able to fit to both train-
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12. Average episodic rewards on S1 games with as a function of training steps (x-axis) for both EMMA (solid line) and a baseline agent consisting of a BERT model that ingests the manual and state observation converted to a string (dotted line). While EMMA is able to fit to both training and validation games, the transformer baseline struggles to learn. All results are averaged over three seeds and shaded area indicates standard deviation.
ing and validation games, the rewards for the Transformer baseline do not significantly increase even after $1.5 \times 10^{6}$ steps. We hypothesize that the difficulty of encoding spatial information in text form makes it very difficult for this model to learn a performant policy on MESSENGER.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ul>
<li>the flying machine remains still, and is also the note of upmost secrecy.</li>
<li>the airplane is coming in your direction. that airplane is the pivitol target.</li>
<li>the winged creature escaping from you is the vital target.</li>
<li>the fleeing plan is a critical target.</li>
</ul>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{1}$ Available at: https://github.com/ahjwang/mes senger-emma&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
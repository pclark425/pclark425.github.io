<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-630 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-630</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-630</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-17.html">extraction-schema-17</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <p><strong>Paper ID:</strong> paper-fc1774419f418fb79451c0bde5d64ae4ce8d5b31</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/fc1774419f418fb79451c0bde5d64ae4ce8d5b31" target="_blank">Systematic Visual Reasoning through Object-Centric Relational Abstraction</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> Object-Centric Relational Abstraction (OCRA) is introduced, a model that extracts explicit representations of both objects and abstract relations, and achieves strong systematic generalization in tasks involving complex visual displays.</p>
                <p><strong>Paper Abstract:</strong> Human visual reasoning is characterized by an ability to identify abstract patterns from only a small number of examples, and to systematically generalize those patterns to novel inputs. This capacity depends in large part on our ability to represent complex visual inputs in terms of both objects and relations. Recent work in computer vision has introduced models with the capacity to extract object-centric representations, leading to the ability to process multi-object visual inputs, but falling short of the systematic generalization displayed by human reasoning. Other recent models have employed inductive biases for relational abstraction to achieve systematic generalization of learned abstract rules, but have generally assumed the presence of object-focused inputs. Here, we combine these two approaches, introducing Object-Centric Relational Abstraction (OCRA), a model that extracts explicit representations of both objects and abstract relations, and achieves strong systematic generalization in tasks (including a novel dataset, CLEVR-ART, with greater visual complexity) involving complex visual displays.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e630.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e630.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OCRA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Object-Centric Relational Abstraction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural architecture that combines object-centric representation learning (slot attention) with an explicit relational bottleneck (dot-product relational operator plus spatial variable-binding) and a transformer that models higher-order relations to enable systematic visual reasoning and out-of-distribution generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>OCRA</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>OCRA is a modular neural reasoning system with three stages: (1) object-centric extraction via pre-trained slot attention producing factorized feature embeddings z_k and position embeddings m_k; (2) computation of pairwise relational embeddings r_{kk'} using a relational operator phi(z_k,z_{k'}) implemented as a dot-product between shared linear projections of object features, plus additive projections of position embeddings for explicit variable-binding; (3) a transformer that processes the set of pairwise relational embeddings (upper-triangular) to compute higher-order relations and produce task-specific outputs. Slot attention is pre-trained with an unsupervised reconstruction objective and frozen for downstream supervised reasoning training.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Relational-bottleneck / explicit relation representations that function like symbolic relations: pairwise relation vectors r_{kk'} are constructed to abstract over object perceptual features by using dot-products between shared projections of object feature embeddings (phi) and explicit spatial indexing via linear projections of position embeddings (m_k). This provides a variable-binding-like mechanism (spatially indexed relations) but not a discrete symbolic logic engine.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural networks and procedural modules: convolutional encoder for feature maps, slot-attention module (transformer-style cross-attention + GRU + MLP) for object extraction, learned linear projections (W_z, W_r, W_m) for relational operator, and a transformer (multi-head self-attention + MLPs) for higher-order relational processing; trained with gradient descent (ADAM).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular pipeline: frozen pre-trained slot-attention (object extraction) feeds neural feature embeddings into a learned relational operator (dot-product bottleneck) that produces intermediate relational embeddings; these embeddings are combined with projected position embeddings to implement variable binding, then processed by a transformer. Integration is modular (pretraining + frozen encoder) and end-to-end supervised training for the reasoning components; attention and dot-product operations serve as differentiable interfaces between modules.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Systematic abstraction over perceptual details: relational embeddings abstract away object-specific features enabling strong out-of-distribution generalization to novel object appearances; explicit spatial variable-binding enables the model to track which pairwise relation corresponds to which object pair despite slot permutation; ability to form higher-order relational patterns (patterns of relations) via the transformer that are not present in raw convolutional features alone.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Evaluated on abstract visual reasoning tasks: ART (Abstract Reasoning Tasks: same/different, relational-match-to-sample (RMTS), distribution-of-3, identity rules), SVRT (Synthetic Visual Reasoning Test), and a novel CLEVR-ART dataset (relational-match-to-sample and identity rules rendered with CLEVR 3D shapes).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>ART RMTS (m=95) accuracy 85.31% ± 2.0; ART Identity (m=95) accuracy 92.80% ± 0.3; ART Same/Different (m=95) accuracy 87.95% ± 1.3. On SVRT and CLEVR-ART OCRA achieves comparable or superior accuracy relative to strong baselines (see paper Tables S11-S12 and Figure 4).</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td>Representative imperative (pure neural) baselines in paper: GAMR (neural guided-attention) e.g., ART RMTS (m=95) 72.2% ± 3.0; ResNet50 (pure conv net) e.g., ART Same/Different (m=95) 66.6% ± 1.5. OCRA outperforms these imperative-only baselines in the hardest generalization regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Demonstrates strong systematic (out-of-distribution) generalization: significantly better accuracy than non-relational-bottleneck baselines in high-difficulty regimes (e.g., training on only 5 objects, testing on 95 unseen objects). The relational bottleneck and factorized position/feature representations enable abstraction over novel perceptual features and compositional generalization to held-out object identities and appearances (including CLEVR-ART's disjoint train/test perceptual features).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Partially interpretable: relational embeddings r_{kk'} are explicit vectors that can be inspected to confirm they encode relation information (paper reports analyses, Figure S5). Position embeddings provide an explicit binding index. The transformer reasoning layer is neural and less directly interpretable, but the intermediate relational representations support attribution of what relations are being reasoned over.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Limitations identified: reliance on a fixed number of slots (K) which may not scale to variable object counts; quadratic scaling in pairwise relation computation (K^2) which is costly in scenes with many objects; challenges scaling to complex real-world images with weak segmentation cues; potential need for multi-head relations to discriminate multiple relations concurrently; performance degrades if components (e.g., factorized representation, relational bottleneck, pretraining) are ablated (ablation study shows substantial drops).</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Operates under the 'relational bottleneck' inductive-bias principle: constraining architectures to compute and operate over relations (dot-product relational operator and explicit spatial indexing) promotes abstraction that generalizes across perceptual variation. The model embodies a division-of-labor framework: object extraction (object-centric perception) and symbolic-like relational representation (relational bottleneck) followed by higher-order relation composition (transformer).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Systematic Visual Reasoning through Object-Centric Relational Abstraction', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e630.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e630.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ESBN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Emergent Symbol Binding Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An architecture that separates perception and control pathways and uses an external key-value memory and dot-product retrieval to enable emergent symbol-like variable binding and abstract relational reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emergent symbols through binding in external memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ESBN (original)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ESBN decomposes processing into a perception pathway that produces perceptual embeddings and a control/ reasoning pathway that operates on abstract representations, with interaction mediated via an external key-value memory; memory retrieval uses dot-product operations which provide an implicit relational encoding and symbol-like bindings between keys and values.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>External key-value memory that is used to store and retrieve abstract (symbol-like) bindings; this memory and the separation of control/perception implements something akin to a declarative symbolic workspace (variable-like keys with stored values).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural networks: perception pathway (CNN or other encoder), a controller (typically LSTM) that writes/reads keys and values from memory; gradient-based training.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Indirect interaction between declarative-like memory and procedural controller via differentiable key-value memory operations (dot-product-based retrieval); the control pathway learns to use memory reads/writes to represent abstract relations while being trained end-to-end.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>When operating on pre-segmented objects, ESBN can learn representations that abstract over perceptual details (emergent symbols) and produce human-like systematic generalization on some tasks by using memory binding to represent relations separately from perceptual content.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Originally demonstrated on abstract visual reasoning tasks (e.g., ART-like problems) with pre-segmented object inputs; in this paper ESBN is discussed in related work for its relational abstraction properties.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Prior work (the ESBN paper) reported strong systematic generalization when inputs were pre-segmented; the present paper notes ESBN can be brittle on noisy or spatially jittered images and that ESBN and CoRelNet rely on pre-segmented objects and thus do not directly scale to multi-object raw images without modification.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Relatively interpretable in that the key-value memory provides an explicit medium for symbol-like bindings that can be inspected; memory reads/writes can be analyzed to understand control decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>ESBN's performance can degrade substantially with task-irrelevant visual variation (spatial jitter, Gaussian noise) and when applied to unsegmented multi-object scenes; it also assumes ordered/permutation-stable inputs, which conflicts with slot attention's permutation of slots unless additional binding mechanisms are used.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Motivated by a separation of perception and symbolic control and the use of an external memory as a binding substrate; interpretable as implementing an emergent-symbols / variable-binding mechanism via differentiable memory operations (dot-product retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Systematic Visual Reasoning through Object-Centric Relational Abstraction', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e630.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e630.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Slot-ESBN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Slot-ESBN (slot-attention + ESBN baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline in which pre-trained slot-attention object embeddings are fed sequentially to an ESBN controller (one slot per timestep) to test ESBN-style emergent symbol binding on multi-object, slot-extracted inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Slot-ESBN</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>This is an experimental combination used as a baseline: slot attention produces K object embeddings which are passed sequentially (one per timestep) into the ESBN architecture (controller + external memory) to perform downstream reasoning; slot attention was pre-trained and frozen in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>ESBN's external key-value memory (symbol-like bindings) used to represent relations or abstract variables.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Slot-attention (CNN + cross-attention + GRU) for object extraction and the ESBN controller (LSTM) for sequential processing; trained with gradient descent (controller trained on supervised tasks while slot encoder is frozen).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular: pre-trained slot-attention embeddings are fed in sequence to the ESBN controller which interacts with its key-value memory via differentiable dot-product retrieval; overall training used to adapt controller (slot encoder frozen).</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Intended to test whether ESBN's emergent-symbol behavior transfers when perceptual inputs come from slot attention; however, in these experiments the combination failed to recover ESBN-like generalization due to permutation/random ordering of slots and lack of consistent indexing, demonstrating the need for explicit positional variable-binding.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Evaluated as baseline on ART and SVRT tasks with multi-object inputs (slot attention outputs); reported in tables S11-S12.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>Slot-ESBN performed poorly on the hardest generalization regimes: e.g., ART RMTS (m=95) accuracy 49.99% ± 0.2 (essentially chance), ART Same/Different (m=95) 50.02% ± 0.2.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>In these experiments Slot-ESBN did not generalize to held-out object identities in the multi-object (slot) setting; poor OOD/test-set performance indicates the ESBN architecture's sensitivity to input ordering/permutation and to differences between pre-segmented and slot-extracted inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Retains ESBN's memory medium which can in principle be inspected, but when combined with slot attention the lack of stable indexing undermines reliable interpretability of bindings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Failed empirically in the slot-attention multi-object setting (near-chance accuracy in hard regimes), due primarily to random permutation of slots and lack of an explicit spatial variable-binding scheme; demonstrates sensitivity to the nature of perceptual input formatting.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Same emergent-symbol / memory-binding motivation as ESBN, but shows that successful hybridization requires compatible indexing/binding between perception and symbolic memory modules.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Systematic Visual Reasoning through Object-Centric Relational Abstraction', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e630.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e630.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoRelNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CoRelNet (as described by Kerg et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An architecture that explicitly computes the full matrix of pairwise dot-product relations between objects and passes the flattened relation matrix to an MLP, implementing an explicit relational bottleneck to encourage abstraction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>On neural architecture inductive biases for relational tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CoRelNet (original)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>CoRelNet computes dot-product similarities between object embeddings to form a full relation matrix (rows normalized via softmax in some variants) which is then flattened and processed by an MLP; it is designed to make relational inductive biases explicit and to support abstraction by operating on relations rather than object features.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Explicit relation matrix (pairwise dot-products) that serves as an intermediate, relation-centric representation (functionally similar to a declarative relation graph), though implemented as dense continuous tensors rather than symbolic logic.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural perception and processing: object embeddings (assumed pre-segmented inputs) and MLPs to process flattened relation matrices; trained with gradient descent.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Explicit computation of pairwise relations (dot products) followed by a learned MLP (modular pipeline); the relational representation is an explicit, differentiable intermediate between perception and decision modules.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>When given pre-segmented object inputs, the explicit relational matrix yields strong inductive bias for relational abstraction and systematic generalization; abstracts away some perceptual detail by forcing reasoning to operate on relations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Previously evaluated on relational reasoning benchmarks (cited work); in this paper CoRelNet is discussed as a related approach and evaluated in a slot-CoRelNet variant.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Reported in prior work (Kerg et al.) to promote systematic generalization when objects are pre-segmented; in the present paper CoRelNet (when combined naively with slot attention) did not perform well on multi-object raw-image tasks without proper spatial indexing.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Intermediate relation matrix is explicit and inspectable, providing some interpretability as to what pairwise relations are being used by downstream MLPs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Relies on pre-segmented objects; naive combination with slot attention (slot-CoRelNet) in this paper performed poorly (near chance) in hard generalization regimes, indicating sensitivity to input format and the need for explicit variable-binding/indexing.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Designed around an explicit relational bottleneck principle: force the architecture to operate on relations (pairwise dot-products) to encourage abstraction and compositional generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Systematic Visual Reasoning through Object-Centric Relational Abstraction', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e630.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e630.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Slot-CoRelNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Slot-CoRelNet (slot-attention + CoRelNet baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that computes the pairwise dot-product matrix between slot-extracted object embeddings, flattens it, and passes it through an MLP (CoRelNet-style) to perform reasoning on multi-object images.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Slot-CoRelNet</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pre-trained slot-attention provides K object embeddings; compute full matrix of pairwise dot products between slots, apply softmax across rows, flatten and feed to an MLP as in CoRelNet; slot attention was pre-trained and frozen for these evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Explicit pairwise relation matrix derived from dot-products between slot embeddings (serves as an explicit relation-centric representation akin to a declarative relation table).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Slot-attention neural encoder (frozen) plus an MLP that processes the flattened relation matrix; gradient-based training for the MLP.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular pipeline: slot attention -> relation matrix computation (dot-products + softmax) -> flatten -> MLP; the relation computation is differentiable and serves as a discrete intermediate representation between perception and processing.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Intended to enforce relational abstraction via explicit relation matrices; however empirically in the slot-attention multi-object setting the approach failed to generalize in the most difficult regimes (likely due to lack of spatial indexing/binding to track object-pair identity across slot permutations).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Evaluated as a baseline on ART and SVRT with multi-object inputs; results reported in paper tables (e.g., Tables S11-S12).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>Slot-CoRelNet baseline results: e.g., ART RMTS (m=95) accuracy 49.82% ± 0.2 (near chance); other tasks similarly near-chance in hardest regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Did not generalize well to held-out object sets when combined naively with slot attention; prior CoRelNet paper reported good generalization for pre-segmented inputs, indicating sensitivity to input formatting.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Relation matrix is explicit and inspectable, but downstream MLP is opaque; lack of reliable spatial indexing reduces interpretability of which relation corresponds to which object-pair across slots.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Near-chance performance in hard generalization regimes when used with slot attention due to slot permutation and absence of explicit spatial variable binding; quadratic cost for relation matrix and sensitivity to ordering/indexing.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Built on the relational bottleneck idea: make relations explicit via pairwise dot-products so that downstream processing reasons about relations rather than object-specific features.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Systematic Visual Reasoning through Object-Centric Relational Abstraction', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e630.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e630.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Abstractors</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Abstractors: Transformer modules for symbolic message passing and relational reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Proposed transformer modules intended to perform symbolic message passing and relational reasoning, emphasizing symbolic-like computation within transformer-style architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Abstractors: Transformer modules for symbolic message passing and relational reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Abstractors</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A proposed set of transformer-based modules designed to implement symbolic message-passing and relational reasoning within a neural architecture; cited as related work that emphasizes symbolic-style relational computation within transformer modules.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Symbolic message-passing style representations (conceptual symbolic relations/messages implemented as transformer's structured tokens), intended to serve as a declarative-like medium for relational structure.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Transformer modules (attention + MLP) as the procedural/neural mechanism realizing message passing and relational processing.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Implements symbolic message passing using learned transformer modules, effectively embedding symbolic-style messaging inside differentiable attention mechanisms; integration via learned attention-based communication between tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Aims to enable symbolic-like relational computations inside neural transformers, potentially supporting abstraction and systematic generalization by structuring message passing to mimic symbolic variable interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Cited as related work advocating architectures that bridge symbolic message-passing and neural processing; specifics of generalization depend on that paper's experiments (not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>By design aims to make message-passing and relational flow more structured and interpretable within transformer tokens, but concrete interpretability outcomes are not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Mentioned as related idea; this paper does not evaluate Abstractors directly so limitations are not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Positions symbolic message passing as implementable inside transformer modules, aiming to combine symbolic relational structure with neural transformer computation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Systematic Visual Reasoning through Object-Centric Relational Abstraction', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Emergent symbols through binding in external memory <em>(Rating: 2)</em></li>
                <li>On neural architecture inductive biases for relational tasks <em>(Rating: 2)</em></li>
                <li>Gamr: A guided attention model for (visual) reasoning <em>(Rating: 2)</em></li>
                <li>Abstractors: Transformer modules for symbolic message passing and relational reasoning <em>(Rating: 2)</em></li>
                <li>The relational bottleneck as an inductive bias for efficient abstraction <em>(Rating: 2)</em></li>
                <li>Learning representations that support extrapolation <em>(Rating: 1)</em></li>
                <li>Learning to reason over visual objects <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-630",
    "paper_id": "paper-fc1774419f418fb79451c0bde5d64ae4ce8d5b31",
    "extraction_schema_id": "extraction-schema-17",
    "extracted_data": [
        {
            "name_short": "OCRA",
            "name_full": "Object-Centric Relational Abstraction",
            "brief_description": "A neural architecture that combines object-centric representation learning (slot attention) with an explicit relational bottleneck (dot-product relational operator plus spatial variable-binding) and a transformer that models higher-order relations to enable systematic visual reasoning and out-of-distribution generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "OCRA",
            "system_description": "OCRA is a modular neural reasoning system with three stages: (1) object-centric extraction via pre-trained slot attention producing factorized feature embeddings z_k and position embeddings m_k; (2) computation of pairwise relational embeddings r_{kk'} using a relational operator phi(z_k,z_{k'}) implemented as a dot-product between shared linear projections of object features, plus additive projections of position embeddings for explicit variable-binding; (3) a transformer that processes the set of pairwise relational embeddings (upper-triangular) to compute higher-order relations and produce task-specific outputs. Slot attention is pre-trained with an unsupervised reconstruction objective and frozen for downstream supervised reasoning training.",
            "declarative_component": "Relational-bottleneck / explicit relation representations that function like symbolic relations: pairwise relation vectors r_{kk'} are constructed to abstract over object perceptual features by using dot-products between shared projections of object feature embeddings (phi) and explicit spatial indexing via linear projections of position embeddings (m_k). This provides a variable-binding-like mechanism (spatially indexed relations) but not a discrete symbolic logic engine.",
            "imperative_component": "Neural networks and procedural modules: convolutional encoder for feature maps, slot-attention module (transformer-style cross-attention + GRU + MLP) for object extraction, learned linear projections (W_z, W_r, W_m) for relational operator, and a transformer (multi-head self-attention + MLPs) for higher-order relational processing; trained with gradient descent (ADAM).",
            "integration_method": "Modular pipeline: frozen pre-trained slot-attention (object extraction) feeds neural feature embeddings into a learned relational operator (dot-product bottleneck) that produces intermediate relational embeddings; these embeddings are combined with projected position embeddings to implement variable binding, then processed by a transformer. Integration is modular (pretraining + frozen encoder) and end-to-end supervised training for the reasoning components; attention and dot-product operations serve as differentiable interfaces between modules.",
            "emergent_properties": "Systematic abstraction over perceptual details: relational embeddings abstract away object-specific features enabling strong out-of-distribution generalization to novel object appearances; explicit spatial variable-binding enables the model to track which pairwise relation corresponds to which object pair despite slot permutation; ability to form higher-order relational patterns (patterns of relations) via the transformer that are not present in raw convolutional features alone.",
            "task_or_benchmark": "Evaluated on abstract visual reasoning tasks: ART (Abstract Reasoning Tasks: same/different, relational-match-to-sample (RMTS), distribution-of-3, identity rules), SVRT (Synthetic Visual Reasoning Test), and a novel CLEVR-ART dataset (relational-match-to-sample and identity rules rendered with CLEVR 3D shapes).",
            "hybrid_performance": "ART RMTS (m=95) accuracy 85.31% ± 2.0; ART Identity (m=95) accuracy 92.80% ± 0.3; ART Same/Different (m=95) accuracy 87.95% ± 1.3. On SVRT and CLEVR-ART OCRA achieves comparable or superior accuracy relative to strong baselines (see paper Tables S11-S12 and Figure 4).",
            "declarative_only_performance": null,
            "imperative_only_performance": "Representative imperative (pure neural) baselines in paper: GAMR (neural guided-attention) e.g., ART RMTS (m=95) 72.2% ± 3.0; ResNet50 (pure conv net) e.g., ART Same/Different (m=95) 66.6% ± 1.5. OCRA outperforms these imperative-only baselines in the hardest generalization regimes.",
            "has_comparative_results": true,
            "generalization_properties": "Demonstrates strong systematic (out-of-distribution) generalization: significantly better accuracy than non-relational-bottleneck baselines in high-difficulty regimes (e.g., training on only 5 objects, testing on 95 unseen objects). The relational bottleneck and factorized position/feature representations enable abstraction over novel perceptual features and compositional generalization to held-out object identities and appearances (including CLEVR-ART's disjoint train/test perceptual features).",
            "interpretability_properties": "Partially interpretable: relational embeddings r_{kk'} are explicit vectors that can be inspected to confirm they encode relation information (paper reports analyses, Figure S5). Position embeddings provide an explicit binding index. The transformer reasoning layer is neural and less directly interpretable, but the intermediate relational representations support attribution of what relations are being reasoned over.",
            "limitations_or_failures": "Limitations identified: reliance on a fixed number of slots (K) which may not scale to variable object counts; quadratic scaling in pairwise relation computation (K^2) which is costly in scenes with many objects; challenges scaling to complex real-world images with weak segmentation cues; potential need for multi-head relations to discriminate multiple relations concurrently; performance degrades if components (e.g., factorized representation, relational bottleneck, pretraining) are ablated (ablation study shows substantial drops).",
            "theoretical_framework": "Operates under the 'relational bottleneck' inductive-bias principle: constraining architectures to compute and operate over relations (dot-product relational operator and explicit spatial indexing) promotes abstraction that generalizes across perceptual variation. The model embodies a division-of-labor framework: object extraction (object-centric perception) and symbolic-like relational representation (relational bottleneck) followed by higher-order relation composition (transformer).",
            "uuid": "e630.0",
            "source_info": {
                "paper_title": "Systematic Visual Reasoning through Object-Centric Relational Abstraction",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "ESBN",
            "name_full": "Emergent Symbol Binding Network",
            "brief_description": "An architecture that separates perception and control pathways and uses an external key-value memory and dot-product retrieval to enable emergent symbol-like variable binding and abstract relational reasoning.",
            "citation_title": "Emergent symbols through binding in external memory",
            "mention_or_use": "mention",
            "system_name": "ESBN (original)",
            "system_description": "ESBN decomposes processing into a perception pathway that produces perceptual embeddings and a control/ reasoning pathway that operates on abstract representations, with interaction mediated via an external key-value memory; memory retrieval uses dot-product operations which provide an implicit relational encoding and symbol-like bindings between keys and values.",
            "declarative_component": "External key-value memory that is used to store and retrieve abstract (symbol-like) bindings; this memory and the separation of control/perception implements something akin to a declarative symbolic workspace (variable-like keys with stored values).",
            "imperative_component": "Neural networks: perception pathway (CNN or other encoder), a controller (typically LSTM) that writes/reads keys and values from memory; gradient-based training.",
            "integration_method": "Indirect interaction between declarative-like memory and procedural controller via differentiable key-value memory operations (dot-product-based retrieval); the control pathway learns to use memory reads/writes to represent abstract relations while being trained end-to-end.",
            "emergent_properties": "When operating on pre-segmented objects, ESBN can learn representations that abstract over perceptual details (emergent symbols) and produce human-like systematic generalization on some tasks by using memory binding to represent relations separately from perceptual content.",
            "task_or_benchmark": "Originally demonstrated on abstract visual reasoning tasks (e.g., ART-like problems) with pre-segmented object inputs; in this paper ESBN is discussed in related work for its relational abstraction properties.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Prior work (the ESBN paper) reported strong systematic generalization when inputs were pre-segmented; the present paper notes ESBN can be brittle on noisy or spatially jittered images and that ESBN and CoRelNet rely on pre-segmented objects and thus do not directly scale to multi-object raw images without modification.",
            "interpretability_properties": "Relatively interpretable in that the key-value memory provides an explicit medium for symbol-like bindings that can be inspected; memory reads/writes can be analyzed to understand control decisions.",
            "limitations_or_failures": "ESBN's performance can degrade substantially with task-irrelevant visual variation (spatial jitter, Gaussian noise) and when applied to unsegmented multi-object scenes; it also assumes ordered/permutation-stable inputs, which conflicts with slot attention's permutation of slots unless additional binding mechanisms are used.",
            "theoretical_framework": "Motivated by a separation of perception and symbolic control and the use of an external memory as a binding substrate; interpretable as implementing an emergent-symbols / variable-binding mechanism via differentiable memory operations (dot-product retrieval).",
            "uuid": "e630.1",
            "source_info": {
                "paper_title": "Systematic Visual Reasoning through Object-Centric Relational Abstraction",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Slot-ESBN",
            "name_full": "Slot-ESBN (slot-attention + ESBN baseline)",
            "brief_description": "A baseline in which pre-trained slot-attention object embeddings are fed sequentially to an ESBN controller (one slot per timestep) to test ESBN-style emergent symbol binding on multi-object, slot-extracted inputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Slot-ESBN",
            "system_description": "This is an experimental combination used as a baseline: slot attention produces K object embeddings which are passed sequentially (one per timestep) into the ESBN architecture (controller + external memory) to perform downstream reasoning; slot attention was pre-trained and frozen in these experiments.",
            "declarative_component": "ESBN's external key-value memory (symbol-like bindings) used to represent relations or abstract variables.",
            "imperative_component": "Slot-attention (CNN + cross-attention + GRU) for object extraction and the ESBN controller (LSTM) for sequential processing; trained with gradient descent (controller trained on supervised tasks while slot encoder is frozen).",
            "integration_method": "Modular: pre-trained slot-attention embeddings are fed in sequence to the ESBN controller which interacts with its key-value memory via differentiable dot-product retrieval; overall training used to adapt controller (slot encoder frozen).",
            "emergent_properties": "Intended to test whether ESBN's emergent-symbol behavior transfers when perceptual inputs come from slot attention; however, in these experiments the combination failed to recover ESBN-like generalization due to permutation/random ordering of slots and lack of consistent indexing, demonstrating the need for explicit positional variable-binding.",
            "task_or_benchmark": "Evaluated as baseline on ART and SVRT tasks with multi-object inputs (slot attention outputs); reported in tables S11-S12.",
            "hybrid_performance": "Slot-ESBN performed poorly on the hardest generalization regimes: e.g., ART RMTS (m=95) accuracy 49.99% ± 0.2 (essentially chance), ART Same/Different (m=95) 50.02% ± 0.2.",
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": true,
            "generalization_properties": "In these experiments Slot-ESBN did not generalize to held-out object identities in the multi-object (slot) setting; poor OOD/test-set performance indicates the ESBN architecture's sensitivity to input ordering/permutation and to differences between pre-segmented and slot-extracted inputs.",
            "interpretability_properties": "Retains ESBN's memory medium which can in principle be inspected, but when combined with slot attention the lack of stable indexing undermines reliable interpretability of bindings.",
            "limitations_or_failures": "Failed empirically in the slot-attention multi-object setting (near-chance accuracy in hard regimes), due primarily to random permutation of slots and lack of an explicit spatial variable-binding scheme; demonstrates sensitivity to the nature of perceptual input formatting.",
            "theoretical_framework": "Same emergent-symbol / memory-binding motivation as ESBN, but shows that successful hybridization requires compatible indexing/binding between perception and symbolic memory modules.",
            "uuid": "e630.2",
            "source_info": {
                "paper_title": "Systematic Visual Reasoning through Object-Centric Relational Abstraction",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "CoRelNet",
            "name_full": "CoRelNet (as described by Kerg et al.)",
            "brief_description": "An architecture that explicitly computes the full matrix of pairwise dot-product relations between objects and passes the flattened relation matrix to an MLP, implementing an explicit relational bottleneck to encourage abstraction.",
            "citation_title": "On neural architecture inductive biases for relational tasks",
            "mention_or_use": "mention",
            "system_name": "CoRelNet (original)",
            "system_description": "CoRelNet computes dot-product similarities between object embeddings to form a full relation matrix (rows normalized via softmax in some variants) which is then flattened and processed by an MLP; it is designed to make relational inductive biases explicit and to support abstraction by operating on relations rather than object features.",
            "declarative_component": "Explicit relation matrix (pairwise dot-products) that serves as an intermediate, relation-centric representation (functionally similar to a declarative relation graph), though implemented as dense continuous tensors rather than symbolic logic.",
            "imperative_component": "Neural perception and processing: object embeddings (assumed pre-segmented inputs) and MLPs to process flattened relation matrices; trained with gradient descent.",
            "integration_method": "Explicit computation of pairwise relations (dot products) followed by a learned MLP (modular pipeline); the relational representation is an explicit, differentiable intermediate between perception and decision modules.",
            "emergent_properties": "When given pre-segmented object inputs, the explicit relational matrix yields strong inductive bias for relational abstraction and systematic generalization; abstracts away some perceptual detail by forcing reasoning to operate on relations.",
            "task_or_benchmark": "Previously evaluated on relational reasoning benchmarks (cited work); in this paper CoRelNet is discussed as a related approach and evaluated in a slot-CoRelNet variant.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Reported in prior work (Kerg et al.) to promote systematic generalization when objects are pre-segmented; in the present paper CoRelNet (when combined naively with slot attention) did not perform well on multi-object raw-image tasks without proper spatial indexing.",
            "interpretability_properties": "Intermediate relation matrix is explicit and inspectable, providing some interpretability as to what pairwise relations are being used by downstream MLPs.",
            "limitations_or_failures": "Relies on pre-segmented objects; naive combination with slot attention (slot-CoRelNet) in this paper performed poorly (near chance) in hard generalization regimes, indicating sensitivity to input format and the need for explicit variable-binding/indexing.",
            "theoretical_framework": "Designed around an explicit relational bottleneck principle: force the architecture to operate on relations (pairwise dot-products) to encourage abstraction and compositional generalization.",
            "uuid": "e630.3",
            "source_info": {
                "paper_title": "Systematic Visual Reasoning through Object-Centric Relational Abstraction",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Slot-CoRelNet",
            "name_full": "Slot-CoRelNet (slot-attention + CoRelNet baseline)",
            "brief_description": "A baseline that computes the pairwise dot-product matrix between slot-extracted object embeddings, flattens it, and passes it through an MLP (CoRelNet-style) to perform reasoning on multi-object images.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Slot-CoRelNet",
            "system_description": "Pre-trained slot-attention provides K object embeddings; compute full matrix of pairwise dot products between slots, apply softmax across rows, flatten and feed to an MLP as in CoRelNet; slot attention was pre-trained and frozen for these evaluations.",
            "declarative_component": "Explicit pairwise relation matrix derived from dot-products between slot embeddings (serves as an explicit relation-centric representation akin to a declarative relation table).",
            "imperative_component": "Slot-attention neural encoder (frozen) plus an MLP that processes the flattened relation matrix; gradient-based training for the MLP.",
            "integration_method": "Modular pipeline: slot attention -&gt; relation matrix computation (dot-products + softmax) -&gt; flatten -&gt; MLP; the relation computation is differentiable and serves as a discrete intermediate representation between perception and processing.",
            "emergent_properties": "Intended to enforce relational abstraction via explicit relation matrices; however empirically in the slot-attention multi-object setting the approach failed to generalize in the most difficult regimes (likely due to lack of spatial indexing/binding to track object-pair identity across slot permutations).",
            "task_or_benchmark": "Evaluated as a baseline on ART and SVRT with multi-object inputs; results reported in paper tables (e.g., Tables S11-S12).",
            "hybrid_performance": "Slot-CoRelNet baseline results: e.g., ART RMTS (m=95) accuracy 49.82% ± 0.2 (near chance); other tasks similarly near-chance in hardest regimes.",
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": true,
            "generalization_properties": "Did not generalize well to held-out object sets when combined naively with slot attention; prior CoRelNet paper reported good generalization for pre-segmented inputs, indicating sensitivity to input formatting.",
            "interpretability_properties": "Relation matrix is explicit and inspectable, but downstream MLP is opaque; lack of reliable spatial indexing reduces interpretability of which relation corresponds to which object-pair across slots.",
            "limitations_or_failures": "Near-chance performance in hard generalization regimes when used with slot attention due to slot permutation and absence of explicit spatial variable binding; quadratic cost for relation matrix and sensitivity to ordering/indexing.",
            "theoretical_framework": "Built on the relational bottleneck idea: make relations explicit via pairwise dot-products so that downstream processing reasons about relations rather than object-specific features.",
            "uuid": "e630.4",
            "source_info": {
                "paper_title": "Systematic Visual Reasoning through Object-Centric Relational Abstraction",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Abstractors",
            "name_full": "Abstractors: Transformer modules for symbolic message passing and relational reasoning",
            "brief_description": "Proposed transformer modules intended to perform symbolic message passing and relational reasoning, emphasizing symbolic-like computation within transformer-style architectures.",
            "citation_title": "Abstractors: Transformer modules for symbolic message passing and relational reasoning",
            "mention_or_use": "mention",
            "system_name": "Abstractors",
            "system_description": "A proposed set of transformer-based modules designed to implement symbolic message-passing and relational reasoning within a neural architecture; cited as related work that emphasizes symbolic-style relational computation within transformer modules.",
            "declarative_component": "Symbolic message-passing style representations (conceptual symbolic relations/messages implemented as transformer's structured tokens), intended to serve as a declarative-like medium for relational structure.",
            "imperative_component": "Transformer modules (attention + MLP) as the procedural/neural mechanism realizing message passing and relational processing.",
            "integration_method": "Implements symbolic message passing using learned transformer modules, effectively embedding symbolic-style messaging inside differentiable attention mechanisms; integration via learned attention-based communication between tokens.",
            "emergent_properties": "Aims to enable symbolic-like relational computations inside neural transformers, potentially supporting abstraction and systematic generalization by structuring message passing to mimic symbolic variable interactions.",
            "task_or_benchmark": null,
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Cited as related work advocating architectures that bridge symbolic message-passing and neural processing; specifics of generalization depend on that paper's experiments (not detailed here).",
            "interpretability_properties": "By design aims to make message-passing and relational flow more structured and interpretable within transformer tokens, but concrete interpretability outcomes are not reported in this paper.",
            "limitations_or_failures": "Mentioned as related idea; this paper does not evaluate Abstractors directly so limitations are not reported here.",
            "theoretical_framework": "Positions symbolic message passing as implementable inside transformer modules, aiming to combine symbolic relational structure with neural transformer computation.",
            "uuid": "e630.5",
            "source_info": {
                "paper_title": "Systematic Visual Reasoning through Object-Centric Relational Abstraction",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Emergent symbols through binding in external memory",
            "rating": 2
        },
        {
            "paper_title": "On neural architecture inductive biases for relational tasks",
            "rating": 2
        },
        {
            "paper_title": "Gamr: A guided attention model for (visual) reasoning",
            "rating": 2
        },
        {
            "paper_title": "Abstractors: Transformer modules for symbolic message passing and relational reasoning",
            "rating": 2
        },
        {
            "paper_title": "The relational bottleneck as an inductive bias for efficient abstraction",
            "rating": 2
        },
        {
            "paper_title": "Learning representations that support extrapolation",
            "rating": 1
        },
        {
            "paper_title": "Learning to reason over visual objects",
            "rating": 1
        }
    ],
    "cost": 0.0201625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Systematic Visual Reasoning through Object-Centric Relational Abstraction</h1>
<p>Taylor W. Webb* $\dagger$<br>Department of Psychology<br>University of California, Los Angeles<br>Los Angeles, CA<br>taylor.w.webb@gmail.com</p>
<p>Shanka Subhra Mondal* $\dagger$<br>Department of Electrical and Computer Engineering<br>Princeton University<br>Princeton, NJ<br>smondal@princeton.edu</p>
<h2>* Equal Contribution</h2>
<p>Jonathan D. Cohen<br>Princeton Neuroscience Institute<br>Princeton University<br>Princeton, NJ<br>jdc@princeton.edu</p>
<h4>Abstract</h4>
<p>Human visual reasoning is characterized by an ability to identify abstract patterns from only a small number of examples, and to systematically generalize those patterns to novel inputs. This capacity depends in large part on our ability to represent complex visual inputs in terms of both objects and relations. Recent work in computer vision has introduced models with the capacity to extract object-centric representations, leading to the ability to process multi-object visual inputs, but falling short of the systematic generalization displayed by human reasoning. Other recent models have employed inductive biases for relational abstraction to achieve systematic generalization of learned abstract rules, but have generally assumed the presence of object-focused inputs. Here, we combine these two approaches, introducing Object-Centric Relational Abstraction (OCRA), a model that extracts explicit representations of both objects and abstract relations, and achieves strong systematic generalization in tasks (including a novel dataset, CLEVR-ART, with greater visual complexity) involving complex visual displays.</p>
<h2>1 Introduction</h2>
<p>When presented with a visual scene, human reasoners have a capacity to identify not only the objects in that scene, but also the relations between objects, and the higher-order patterns formed by multiple relations. These abilities are fundamental to human visual intelligence, enabling the rapid induction of abstract rules from a small number of examples, and the systematic generalization of those rules to novel inputs [32, 25, 13, 27]. For example, when presented with the image in Figure 1, one can easily determine that the objects in the top and bottom rows are both governed by a common abstract pattern (ABA). Furthermore, one could easily then generalize this pattern to any potential shapes, even those that one has never observed before. Indeed, this is arguably what it means for a pattern to be considered abstract.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Object-Centric Relational Abstraction (OCRA). OCRA consists of three core components, responsible for modeling objects, relations, and higher-order relations. First, given a visual input <em>x</em>, slot attention extracts object-centric representations, consisting of factorized feature embeddings <em>z</em><sub>k=1...K</sub> and position embeddings <em>m</em><sub>k=1...K</sub>. Second, pairwise relation embeddings <em>r</em><sub>k</sub> are computed by first passing each pair of feature embeddings to a relational operator <em>φ</em>, and then additively incorporating their position embeddings. This allows visual space to serve as an indexing mechanism for visual relations. Finally, relational embeddings are passed to a transformer to model the higher-order patterns formed by multiple relations.</p>
<p>Though this task appears effortless from the perspective of human reasoning, it exemplifies the type of problems that pose a significant challenge for standard neural network methods. Rather than inducing an abstract representation of the underlying rules in a given task, neural networks tend to overfit to the specific details of the examples observed during training, thus failing to generalize those rules to novel inputs [26, 3, 33].</p>
<p>To address this, a number of neural network methods have recently been proposed that incorporate strong inductive biases for <em>relational abstraction</em> [49, 51, 22, 1], also referred to as a 'relational bottleneck' [50]. The key insight underlying these approaches is that abstract visual patterns are often characterized by the <em>relations</em> between objects, rather than the features of the objects themselves. For example, the pattern seen in Figure 1 is characterized by the relation between the shape and position of the objects. By constraining architectures to process visual inputs in terms of relations between objects, these recent approaches have achieved strong systematic (i.e., out-of-distribution) generalization of learned abstract rules, given only a small number of training examples [51, 22, 1].</p>
<p>However, a major assumption of these approaches is that relations are computed between <em>objects</em>. Accordingly, these methods have generally relied on inputs consisting of pre-segmented visual objects, rather than multi-object scenes. Furthermore, these methods can sometimes be brittle to task-irrelevant visual variation, such as spatial jitter or Gaussian noise [43]. It is thus unclear whether and how these approaches might scale to complex and varied real-world settings.</p>
<p>In this work, we take a step toward the development of relational reasoning algorithms that can operate over more realistic visual inputs. To do so, we turn to object-centric representation learning [15, 6, 28, 12], a recently developed approach in which visual scenes are decomposed into a discrete set of object-focused latent representations, without the need for veridical segmentation data. Though this approach has shown success on a number of challenging visual tasks [10, 39, 30], it has not yet been integrated with the strong relational inductive biases that have previously been shown to enable systematic, human-like generalization [49, 51, 22, 1]. Here, we propose Object-Centric Relational Abstraction (OCRA), a model that combines the strengths of these two approaches. Across a suite of visual reasoning tasks (including a novel dataset, CLEVR-ART, with greater visual complexity), we find that OCRA is capable of systematically generalizing learned abstract rules from complex, multi-object visual displays, significantly outperforming a number of competitive baselines in most settings.</p>
<h1>2 Approach</h1>
<p>Figure 1 shows a schematic depiction of our approach. OCRA consists of three core components, each responsible for a different element of the reasoning process. These three elements can be summarized as 1) extraction of object-centric representations, 2) computation of pairwise relational embeddings, and 3) processing of higher-order relational structures - patterns across multiple relations - in order to identify abstract rules. We describe each of these three components in the following sections.</p>
<h3>2.1 Object-centric representation learning</h3>
<p>OCRA employs slot attention [28] to extract object-centric representations from complex, multi-object visual inputs. Given an image $\boldsymbol{x}$, the goal of slot attention is to extract a set of latent embeddings (i.e., slots), each of which captures a focused representation of a single visual object, typically without access to ground truth segmentation data.
To do so, the image is first passed through a series of convolutional layers, generating a feature map $\boldsymbol{f e a t} \in \mathbb{R}^{H \times W \times D}$. Positional embeddings $\boldsymbol{p o s} \in \mathbb{R}^{H \times W \times D}$ are then generated by passing a 4 d position code (encoding the four cardinal directions) through a separate linear projection. Feature and position embeddings are additively combined, and then passed through a series of 1x1 convolutions. This is then flattened, yielding $\boldsymbol{i n p u t s} \in \mathbb{R}^{N \times D}$ (where $N=H \times W$ ), over which slot attention is performed.
The set of $K \boldsymbol{s l o t s} \in \mathbb{R}^{K \times D}$ is then randomly initialized (from a shared distribution with learned parameters), and a form of transformer-style cross-attention is employed to attend over the pixels of the feature map. Specifically, each slot emits a query $\mathrm{q}(\boldsymbol{s l o t s}) \in \mathbb{R}^{K \times D}$ (through a linear projection), and each location in the feature map emits a key $\mathrm{k}(\boldsymbol{i n p u t s}) \in \mathbb{R}^{N \times D}$ and a value $\mathrm{v}(\boldsymbol{i n p u t s}) \in \mathbb{R}^{N \times D}$. For each slot, a query-key attention operation is then used to generate an attention distribution over the feature map $\boldsymbol{a t t n}=\operatorname{softmax}\left(\frac{1}{\sqrt{D}} \mathrm{k}(\boldsymbol{i n p u t s}) \cdot \mathrm{q}(\boldsymbol{s l o t s})^{\top}\right)$ and a weighted mean of the values $\boldsymbol{u p d a t e s}=\boldsymbol{a t t n} \cdot \mathrm{v}(\boldsymbol{i n p u t s})$ is used to update the slot representations using a Gated Recurrent Unit [8], followed by a residual MLP. This process is repeated for $T$ iterations, leading to a competition between the slots to account for each pixel in the feature map.</p>
<p>For the purposes of computing relational embeddings, we employed a factorized object representation format, in which position is represented distinctly from other object features. For a given input image, after performing $T$ iterations of slot attention, we used the final attention map $\boldsymbol{a t t n}_{T} \in \mathbb{R}^{K \times N}$ to compute feature- and position-specific embeddings for each slot:</p>
<p>$$
\begin{gathered}
\boldsymbol{z}<em T="T">{k}=\boldsymbol{a t t n}</em>)[k] \
\boldsymbol{m}} \operatorname{flatten}(\boldsymbol{f e a t<em T="T">{k}=\boldsymbol{a t t n}</em>)[k]
\end{gathered}
$$} \operatorname{flatten}(\boldsymbol{p o s</p>
<p>where $\boldsymbol{f e a t}$ and $\boldsymbol{p o s}$ represent the feature- and position-embedding maps (prior to being combined). As will be seen in the following section, this factorized representation allows visual space to serve as an indexing mechanism for pairwise relations, mirroring the central role of spatial organization in human visual reasoning [29].
Although slot attention can in principle be learned end-to-end in the service of a specific downstream reasoning task, here we chose to pre-train slot attention using an unsupervised objective. This decision reflects the assumption that human-like visual reasoning does not occur in a vacuum, and visual representations are generally not learned from scratch when performing each new task. Instead, visual object representations are shaped by a wealth of non-task-specific prior experience. To model the effects of this prior experience, we pre-trained slot attention to reconstruct randomly generated multi-object displays, using a slot-based autoencoder framework (more details in Section 4.3.1). Importantly, for experiments testing generalization of abstract rules to novel objects, we ensured that these objects did not appear in the pre-training data for slot attention.</p>
<h3>2.2 Relational embeddings</h3>
<p>After extracting object embeddings, we employed a novel relational embedding method, designed to allow OCRA to abstract over the perceptual features of individual objects (and thus systematically</p>
<p>generalize learned rules to inputs with novel perceptual features). This embedding method consisted of two steps. First, we applied a relational operator $\phi$ to compute the pairwise relation between two feature embeddings $\boldsymbol{z}<em k_prime="k^{\prime">{k}$ and $\boldsymbol{z}</em>$. In the present work, we define this operator as:}</p>
<p>$$
\phi\left(\boldsymbol{z}<em k_prime="k^{\prime">{k}, \boldsymbol{z}</em>}}\right)=\left(\boldsymbol{z<em _boldsymbol_z="\boldsymbol{z">{k} \boldsymbol{W}</em>}} \cdot \boldsymbol{z<em _boldsymbol_z="\boldsymbol{z">{k^{\prime}} \boldsymbol{W}</em>
$$}}\right) \boldsymbol{W}_{\boldsymbol{r}</p>
<p>where $\boldsymbol{z}<em k_prime="k^{\prime">{k}$ and $\boldsymbol{z}</em>}}$ are first projected through a shared set of linear weights $\boldsymbol{W<em _boldsymbol_r="\boldsymbol{r">{\boldsymbol{z}} \in \mathbb{R}^{D \times D}$, and the dot product between these weighted embeddings is then projected back out into the original dimensionality using another set of weights $\boldsymbol{W}</em>\right)$ enable the model to learn which features are relevant for a given task, and to only compute relations between those features.}} \in \mathbb{R}^{1 \times D}$. Using dot products to model pairwise relations introduces a layer of abstraction between the perceptual features of the objects and the downstream reasoning process, in that the dot product only represents the relations between objects, rather than those objects' individual features. We hypothesized that this relational bottleneck would lead to improved generalization to inputs with novel perceptual features. Additionally, the shared projection weights $\left(\boldsymbol{W}_{\boldsymbol{z}</p>
<p>After applying the relational operator, we then compute the relational embedding for objects $k$ and $k^{\prime}$ as:</p>
<p>$$
\boldsymbol{r}<em k="k">{k k^{\prime}}=\phi\left(\boldsymbol{z}</em>}, \boldsymbol{z<em k="k">{k^{\prime}}\right)+\boldsymbol{m}</em>} \boldsymbol{W<em k_prime="k^{\prime">{\boldsymbol{m}}+\boldsymbol{m}</em>
$$}} \boldsymbol{W}_{\boldsymbol{m}</p>
<p>where the position embeddings $\boldsymbol{m}<em k_prime="k^{\prime">{k}$ and $\boldsymbol{m}</em>$ to be indexed by their spatial position, thus endowing OCRA with an explicit variable-binding mechanism.}}$ are each projected through a shared set of linear weights ${ }^{1}$ $\boldsymbol{W}_{\boldsymbol{m}} \in \mathbb{R}^{D \times D}$ and added to the output of $\phi$. This allows the relation between objects $k$ and $k^{\prime</p>
<h1>2.3 Processing of higher-order relations</h1>
<p>After computing all $K^{2}$ pairwise relational embeddings, we then pass these embeddings to a reasoning process to extract higher-order relations - patterns formed between multiple relations. This is an essential component of visual reasoning, as abstract rules, such as the one displayed in Figure 1, are typically defined in terms of higher-order relations. We model these higher-order interactions using a transformer [45]:</p>
<p>$$
\boldsymbol{y}=\operatorname{Transformer}\left(\boldsymbol{r}<em k="K," k_prime="k^{\prime">{k=1, k^{\prime}=1} \ldots \boldsymbol{r}</em>\right)
$$}=K</p>
<p>where the inputs are formed by the set of all pairwise relational embeddings $\boldsymbol{r}_{k k^{\prime}}$ (for all $k=1 \ldots K$ and all $k^{\prime}=1 \ldots K$ ), and $\boldsymbol{y}$ is a task-specific output. Because we employ a symmetric relational operator in the present work, we do not in practice present the full matrix of pairwise comparisons, but instead limit these comparisons to those in the upper triangular matrix.</p>
<h2>3 Related Work</h2>
<p>A number of methods for annotation-free object segmentation have recently been proposed [15, 6, 28, $12,11,20]$, including the slot attention method employed by our proposed model [28]. Though the details of these methods differ, they share the general goal of decomposing visual scenes into a set of latent variables corresponding to objects, without access to ground truth segmentation data. Building on the success of these object-encoding methods, some recent work has developed object-centric approaches to visual reasoning that combine slot-based object representations with transformer-based architectures $[10,39,52,30]$. This work has yielded impressive performance on challenging computer vision tasks, such as question answering from video [10], or complex multi-object visual analogy problems [30], but has thus far been limited to generalization to similar problem types following extensive task-specific training.</p>
<p>In the present work, our goal was to develop a model capable of human-like systematic generalization in abstract reasoning tasks. Our proposed approach builds on the insights of other recent work in this</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>area, most notably including the Emergent Symbol Binding Network (ESBN) [51] and CoRelNet [22] architectures, both of which employed the concept of a relational bottleneck in different ways. In the ESBN architecture, separate control and perception pathways interact indirectly through a keyvalue memory, allowing the control pathway to learn representations that are abstracted over the details represented in the perceptual pathway. Kerg et al. [22] proposed that the ESBN's capacity for abstraction was due to the implicit encoding of relational information in its memory operations, which are mediated by a dot-product-based retrieval operation. Kerg et al. also proposed a novel architecture, CoRelNet, that makes this inductive bias more explicit. The CoRelNet architecture operates by explicitly computing the full matrix of relations (based on dot products) between each pair of objects in its input, and then passing these relations to an MLP for further processing. Both ESBN and CoRelNet displayed strong systematic generalization of learned abstract rules from a small set of examples. Importantly, however, both of these architectures depended on the presence of pre-segmented objects as input. A primary contribution of the present work is to combine these inductive biases for relational abstraction (i.e., the relational bottleneck) with object-centric encoding mechanisms, and to introduce the use of a spatial indexing mechanism that allows the model to apply these inductive biases to spatially embedded, multi-object visual inputs.</p>
<p>In addition to lacking object encoding mechanisms, it was recently shown that the ESBN can sometimes be overly brittle to task-irrelevant visual variation. Vaishnav and Serre [43] evaluated the ESBN and other reasoning architectures on abstract visual reasoning problems similar to those evaluated by Webb et al. [51], but with the introduction of either spatial jitter or Gaussian noise, finding that this significantly impaired performance in the ESBN (as well as other reasoning architectures, such as the transformer [45] and relation net [35]). Vaishnav and Serre also introduced a novel architecture, GAMR, that was more robust to these sources of noise, and able to solve problems based on multi-object inputs, likely due to the use of a novel guided attention mechanism. But despite outperforming ESBN and other architectures in this more challenging visual setting, GAMR nevertheless fell short of systematic generalization in these tasks, displaying a significant drop in performance in the most difficult generalization regimes. Here, we directly confront this challenge, demonstrating that OCRA is capable of systematic generalization comparable to that displayed by ESBN and CoRelNet, but in more complex visual settings.</p>
<p>It is also worth mentioning that other neural network architectures have been proposed that incorporate relational inductive biases in various ways [35, 4, 38, 34, 37, 5], some of which also incorporate object-centric representations [48, 44, 19, 53, 2, 24, 40, 46, 14, 41]. Relative to this previous work, our approach is distinguished by the inclusion of a stronger inductive bias toward relational abstraction - that is, a relational bottleneck - for promoting the development of relational representations that fully abstract over the details of individual objects [51, 22, 1]. To empirically evaluate the importance of the relational bottleneck, we compare our approach with a set of baseline models that capture the key computational properties of previous models, including a slot-transformer baseline that combines slot attention with a transformer reasoning module [10, 39, 52, 30], and a slot-interaction-network baseline that combines slot attention with an interaction-network reasoning module [48, 44, 19, 53, 2, 24, 40, 46, 14, 41].</p>
<p>To summarize our contributions relative to previous work, our proposed model includes:</p>
<ol>
<li>A novel object representation format that is factorized into distinct feature and position embeddings (Equations 1 and 2), enabling a form of explicit variable-binding.</li>
<li>A novel relational embedding method that implements a relational bottleneck (Equations 3 and 4).</li>
<li>An architecture that combines these elements with preexisting components (slot attention and transformers) in a novel manner to support systematic visual reasoning from complex visual displays, including a novel dataset CLEVR-ART with greater visual complexity.</li>
</ol>
<h1>4 Experiments</h1>
<h3>4.1 Datasets</h3>
<p>We evaluated OCRA on three challenging visual reasoning tasks, ART [51] and SVRT [13], specifically designed to probe systematic generalization of learned abstract rules, as well as a novel dataset, CLEVR-ART, involving more complex visual inputs (Figure 2).</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Abstract visual reasoning tasks. Three datasets used to evaluate systematic visual reason-
ing. Abstract Reasoning Tasks (ART) consist of four separate visual reasoning tasks: same/different,
relational-match-to-sample, distribution-of-3, and identity rules (pictured). Systematic generalization
is studied by training on problems involving a small number of objects (as few as 5), and testing
on heldout objects. Synthetic Visual Reasoning Test (SVRT) consists of 23 tasks, each defined by
separate abstract rule. Rules are defined based on either spatial relations (SR) or same/different
relations (SD; pictured). In example problem, category 1 always contains three sets of two identical
objects, category 2 always contains two sets of three identical objects. CLEVR-ART is a novel dataset
consisting of ART problems rendered using more realistic 3D shapes, based on the CLEVR dataset.</p>
<p>ART. The Abstract Reasoning Tasks (ART) dataset was proposed by Webb et al. [51], consisting of
four visual reasoning tasks, each defined by a different abstract rule (Figure S1). In the 'same/different'
task, two objects are presented, and the task is to say whether they are the same or different. In
the 'relational match-to-sample' task, a source pair of objects is presented that either instantiates a
'same' or 'different' relation, and the task is to select the pair of target objects (out of two pairs) that
instantiates the same relation. In the 'distribution-of-3' task, a set of three objects is presented in the
first row, and an incomplete set is presented in the second row. The task is to select the missing object
from a set of four choices. In the 'identity rules' task, an abstract pattern is instantiated in the first
row (ABA, ABB, or AAA), and the task is to select the choice that would result in the same relation
being instantiated in the second row.</p>
<p>The primary purpose of this dataset was to evaluate strong systematic generalization of these rules.
Thus, each task consists of a number of generalization regimes of varying difficulty, defined by the
number of unique objects used to instantiate the rules during training (out of a set of 100 possible
objects). In the most difficult regime (m = 95), the training set consists of problems that are created
using only 5 objects, and the test problems are created using the remaining 95 objects, whereas in the
easiest regime (m = 0), both training and test problems are created from the complete set of 100
objects (though the arrangement of these objects is distinct, such that the training and test sets are
still disjoint). The most difficult generalization regime poses an especially difficult test of systematic
generalization, as it requires learning of an abstract rule from a very small set of examples (details
shown in Table S1), with little perceptual overlap between training and test.</p>
<p>As originally proposed [51], the ART dataset involved pre-segmented visual objects. Here, we investi-
gated a version of this task involving multi-object visual displays (see Supplementary Section S2 for
details). We also applied random spatial jitter (random translation of up to 5 pixels in any direction)
to each object, as this was previously found to significantly impair performance in some previous
reasoning models [42].</p>
<p>SVRT. The Synthetic Visual Reasoning Test (SVRT), proposed by Fleuret et al. [13], consists of
23 binary classification tasks, each defined by a particular configuration of relations. The tasks can
be broadly divided into two categories: those that are based primarily on same/different relations
('SD'; tasks 7, 21, 5, 19, 6, 17, 20, 1, 13, 22, 16; example shown in Figure S3(a)), and those that
are primarily based on spatial relations ('SR'; tasks 12, 15, 14, 9, 23, 8, 10, 18, 4, 3, 11, 2; example
shown in Figure S3(b)). As with previous work [43], we limit training to a relatively small number of
examples (either 500 or 1000 per task).</p>
<p>CLEVR-ART. We created a novel dataset based on ART using realistically rendered 3D shapes
from CLEVR [21] (Figure S4). We focused on two specific visual reasoning tasks: relational-match-</p>
<p>to-sample and identity rules. Problems were formed from objects of three shapes (cube, sphere, and cylinder), three sizes (small, medium, and large), eight colors (gray, red, blue, green, brown, purple, cyan, and yellow) and two textures (rubber and metal). To test systematic generalization, the training set consisted of problems using only small and medium sized rubber cubes in four colors (cyan, brown, green, and gray) whereas the test set consisted only of problems using large sized metal spheres and cylinders in four distinct colors (yellow, purple, blue, and red). The training and test sets thus involved completely disjoint perceptual features, similar to the original ART.</p>
<h1>4.2 Baselines</h1>
<p>We compared our model to results from a set of baseline models evaluated by Vaishnav and Serre [43], including their GAMR architecture, ResNet50 [17], and a version of ResNet that employed selfattention (Attn-ResNet) [42]. To the best of our knowledge, these baselines represent the best performing models on the tasks that we investigated. We also investigated an additional set of baselines that combined our pretrained slot attention module with various reasoning architectures, including GAMR [43], ESBN [51], Relation Net (RN) [35], Transformer [45], Interaction Network (IN) [48], and CoRelNet [22]. These baselines benefited from the same degree of pre-training and object-centric processing as our proposed model, but employed different reasoning mechanisms. Finally, we investigated a self-supervised baseline (a Masked Autoencoder (MAE) [16]) to assess the extent to which a generative objective might encourage systematic generalization (see Tables S13S14 for results). All baselines were evaluated using multi-object displays. See Supplementary Section S3for more details on baselines.</p>
<h3>4.3 Experimental Details</h3>
<h3>4.3.1 Pre-training slot attention</h3>
<p>We pre-trained slot attention using a slot-based autoencoder framework. For ART, we generated a dataset of random multi-object displays (see Figure S2), utilizing a publicly available repository of unicode character images ${ }^{2}$. We eliminated any characters that were identical to, or closely resembled, the 100 characters present in the ART dataset, resulting in a training set of 180k images, and a validation set of 20k images. For SVRT, we pre-trained two versions of slot attention on all tasks, one using 500 and the other using 1000 training examples from each of the 23 tasks. For CLEVR-ART, we generated a dataset of randomly placed 3D shapes from CLEVR, and pre-trained slot attention using a training set of 90k images, and validation set of 10k images.
To pre-train slot attention, we used a simple reconstruction objective and a slot-based decoder. Specifically, we used a spatial broadcast decoder [47], which generates both a reconstructed image and a mask for each slot. We then generated a combined reconstruction, normalizing the masks across slots using a softmax, and using the normalized masks to compute a weighted average of the slot-specific reconstructions.
After pre-training, the slot attention parameters were frozen for training on the downstream reasoning tasks. For ART, we found that despite not having been trained on any of the objects in the dataset, slot attention was nevertheless able to generate focused attention maps that corresponded closely to the location of the objects (see Figures S6-S9).</p>
<h3>4.3.2 Training details and hyperparameters</h3>
<p>Images were resized to $128 \times 128$, and pixels were normalized to the range $[0,1]$. For SVRT, we converted the images to grayscale, and also applied random online augmentations during training, including horizontal and vertical flips. Tables S2 and S3 describes the hyperparameters for the convolutional encoder and spatial broadcast decoder respectively. For slot attention, we used $K=6$ ( $K=7$ for CLEVR-ART) slots, $T=3$ attention iterations per image, and a dimensionality of $D=64$. Training details are described in Table S4.</p>
<p>Table S5 gives the hyperparameters for the transformer. To solve the SVRT tasks, and the same/different ART task, we used a CLS token (analogous to CLS token in [9]) together with a sigmoidal output layer to generate a binary label, and trained the model with binary cross-entropy</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: ART results. Results on the Abstract Reasoning Tasks dataset [51]. OCRA outperformed all baselines for all tasks, with especially strong performance in the most difficult generalization regime $(m=95)$. Results reflect an average across 10 runs for each model $\pm$ standard error. The X axis denotes the number of objects (out of 100) withheld during training, with no objects withheld in the leftmost condition $(m=0)$, and almost all objects withheld in the rightmost condition $(m=95)$.
loss. To solve the RMTS, distribution-of-3, and identity rules tasks, we inserted each candidate answer choice into the problem, and used a CLS token together with a linear output layer to generate a score. The scores for all answer choices were passed through a softmax and the model was trained with cross-entropy loss. For ART and CLEVR-ART, we used a default learning rate of $8 e-5$, and a batch size of 16. The number of training epochs is displayed for each ART task in Table S6, and for each CLEVR-ART task in Table S10. The training details for some baseline models were modified as discussed in Section S3, and detailed in Tables S7-S9. For SVRT, we used a learning rate of $4 e-5$, a batch size of 32, and trained for 2000 epochs on each task. We used the ADAM optimizer [23], and implementation was done using the Pytorch library [31].</p>
<h1>4.3.3 Model selection</h1>
<p>After pre-training slot attention for a fixed number of epochs (number of epochs for each dataset listed in Table S4), we selected the version of the model from the epoch with the lowest validation loss (on the unsupervised pre-training task) to use in the visual reasoning tasks. We used a validation set of 20 k images for ART (again excluding the objects used in the ART tasks), 4 k images per task for SVRT, and 10k images for CLEVR-ART.
When training on the primary reasoning tasks, for SVRT, we trained for 2000 epochs on each task, and selected the version of the model from the epoch with the highest accuracy on the validation set. This is consistent with the approach taken by Vaishnav and Serre [43], who used validation accuracy as a criterion for early stopping. For ART and CLEVR-ART, we did not perform any model (or epoch) selection when training on the primary reasoning tasks.</p>
<h2>5 Results</h2>
<p>Figure 3 shows the results on the ART dataset (all results are presented in tabular form in Tables S11-S17). OCRA achieved state-of-the-art results across all tasks and generalization regimes, outperforming both the previous state-of-the-art model (GAMR), and other strong baselines such as</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: CLEVR-ART. Results for systematic generalization on two CLEVR-ART tasks. Results reflect test accuracy averaged over 5 trained networks ± standard error. SVRT. Results on the Synthetic Visual Reasoning Test (SVRT) [13]. OCRA and GAMR showed comparable overall performance, with OCRA performing marginally better on problems involving same / different relations, and GAMR performing marginally better on problems involving spatial relations.</p>
<p>The slot-transformer. This disparity was especially striking in the most difficult generalization regime (m = 95), in which models were trained on only 5 objects, and tested on a heldout set of 95 objects. Importantly, no component of OCRA (including both the slot attention and relational reasoning components) had been exposed to these heldout objects. OCRA outperformed the next-best baselines in this regime (GAMR and slot-transformer) by as much as 25%, demonstrating a significant degree of systematic generalization. We also analyzed OCRA's learned relational embeddings, finding that they captured relational information even in this out-of-distribution setting (Figure S5).</p>
<p>Many of the baseline models that we evaluated did not contain a relational bottleneck (Transformer, IN, RN, GAMR, ResNet), limiting their ability to generalize relational patterns to novel objects (i.e., higher values of m). The comparison with these models thus demonstrates the importance of the relational bottleneck, which enables systematic generalization of learned relational patterns. The ESBN and CoRelNet architectures did include a relational bottleneck, but they were not designed with multi-object inputs in mind, and therefore performed poorly on these tasks in all regimes (even when combined with our slot attention module). This is due primarily to the random permutation of slots in slot attention, which motivated our positional embedding variable-binding scheme (allowing OCRA to keep track of which relations correspond to which pairs of objects).</p>
<p>On SVRT, OCRA showed comparable overall performance with GAMR, displaying marginally better performance on tasks characterized by same/different relations, while GAMR performed marginally better on tasks involving spatial relations (Figure 4, right). OCRA also outperformed GAMR on CLEVR-ART (Figure 4, left), demonstrating that its capacity for systematic generalization can be extended to problems involving more realistic visual inputs. Overall, the results on ART, SVRT, and CLEVR-ART demonstrate that OCRA is capable of solving a diverse range of relational reasoning problems from complex multi-object displays.</p>
<h3>5.1 Ablation Study</h3>
<p>We performed an ablation study targeting each of OCRA's major components, focusing on the most difficult regime of the relational match-to-sample and identity rules tasks (Table 1). To evaluate the importance of OCRA's capacity for object-centric processing, we tested a version of the model without slot attention ('w/o Slot Attention'), in which the visual feature map was instead divided into a 4 × 4 grid, treating each image patch as though it were an 'object'. We also tested a version in which slot attention was only trained end-to-end on the visual reasoning tasks ('w/o Pre-training'), to test the importance of prior exposure to a diverse range of multi-object displays. To evaluate the importance of OCRA's relational embedding method, we tested five different ablation models. In one model ('w/o Relational Embeddings'), slot embeddings were passed directly to the transformer. In another model ('w/o Factorized Representation'), we did not compute separate feature and position embeddings for each object, and instead applied the relational operator directly to the slot embeddings. In a third model ('w/o Relational Bottleneck'), relational embeddings were computed simply by applying an MLP to each pair of slot embeddings (allowing the model to overfit to the perceptual</p>
<p>Table 1: Ablation study on $m=95$ generalization regime for two ART tasks. Results reflect test accuracy averaged over 10 trained networks $\pm$ standard error.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">RMTS</th>
<th style="text-align: center;">ID</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">OCRA (full model)</td>
<td style="text-align: center;">$\mathbf{8 5 . 3 1 \pm 2 . 0}$</td>
<td style="text-align: center;">$\mathbf{9 2 . 8 0 \pm 0 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">w/o Slot Attention</td>
<td style="text-align: center;">$56.58 \pm 3.1$</td>
<td style="text-align: center;">$84.42 \pm 1.1$</td>
</tr>
<tr>
<td style="text-align: left;">w/o Pre-training</td>
<td style="text-align: center;">$58.69 \pm 3.6$</td>
<td style="text-align: center;">$53.48 \pm 4.8$</td>
</tr>
<tr>
<td style="text-align: left;">w/o Relational Embeddings</td>
<td style="text-align: center;">$74.79 \pm 1.8$</td>
<td style="text-align: center;">$81.21 \pm 1.1$</td>
</tr>
<tr>
<td style="text-align: left;">w/o Factorized Representation</td>
<td style="text-align: center;">$50.26 \pm 0.2$</td>
<td style="text-align: center;">$75.40 \pm 0.6$</td>
</tr>
<tr>
<td style="text-align: left;">w/o Relational Bottleneck</td>
<td style="text-align: center;">$63.62 \pm 1.0$</td>
<td style="text-align: center;">$76.56 \pm 1.8$</td>
</tr>
<tr>
<td style="text-align: left;">w/o Inner Product</td>
<td style="text-align: center;">$50.70 \pm 0.4$</td>
<td style="text-align: center;">$48.07 \pm 2.6$</td>
</tr>
<tr>
<td style="text-align: left;">Replace Inner Product w/ Outer Product</td>
<td style="text-align: center;">$62.84 \pm 1.6$</td>
<td style="text-align: center;">$69.58 \pm 1.1$</td>
</tr>
<tr>
<td style="text-align: left;">w/o Transformer</td>
<td style="text-align: center;">$51.29 \pm 0.2$</td>
<td style="text-align: center;">$44.18 \pm 0.6$</td>
</tr>
</tbody>
</table>
<p>details of these embeddings), thus removing OCRA's inductive bias for relational abstraction. In a fourth model ('w/o Inner Product') the dot product in the relational operator (Equation 3) was replaced with a learned bottleneck, in which each pair of slot embeddings was passed to a learned linear layer with a one-dimensional output. This model tested the extent to which the inherently relational properties of the dot product are important for relational abstraction (as opposed to resulting merely from compression to a single dimension). In a fifth model ('Replace Inner Product w/ Outer Product'), the dot product was replaced with an outer product (resulting in a $D \times D$ matrix, which was then flattened and projected via a learned linear layer to an embedding of size $D$ ). Finally, to evaluate the importance of OCRA's capacity to process higher-order relations, instead of passing relational embeddings to a transformer, we instead summed all relation embeddings elementwise and passed the resulting vector to an MLP, as is done in the Relation Net ('w/o Transformer'). All ablation models performed significantly worse than the complete version of OCRA, demonstrating that all three elements in our model - object-centric processing, relational abstraction, and higher-order relations - play an essential role in enabling systematic generalization.</p>
<h1>6 Limitations and Future Directions</h1>
<p>In the present work, we have presented a model that integrates object-centric visual processing mechanisms (providing the ability to operate over complex multi-object visual inputs) with a relational bottleneck (providing a strong inductive bias for learning relational abstractions that enable humanlike systematic generalization of learned abstract rules). Though this is a promising step forward, and a clear advance relative to previous models of abstract visual reasoning, it is likely that scaling the present approach to real-world settings will pose additional challenges. First, real-world images are especially challenging for object-centric methods due to a relative lack of consistent segmentation cues. However, there has recently been significant progress in this direction [36], in particular by utilizing representations from large-scale self-supervised methods [54, 7], and it should be possible to integrate these advances with our proposed framework. Second, the current approach assumes a fixed number of slot representations, which may not be ideal for modeling real-world scenes with a highly variable number of objects [55]. Though we did not find that this was an issue in the present work, in future work it would be desirable to develop a method for dynamically modifying the number of slots. Third, OCRA's relational operator is applied to all possible pairwise object comparisons, an approach that may not be scalable to scenes that contain many objects. In future work, this may be addressed by replacing the transformer component of our model with architectures that are better designed for high-dimensional inputs [18]. Finally, it is unclear how our proposed approach may fare in settings that involve more complex real-world relations, and settings that require the discrimination of multiple relations at once. It may be beneficial in future work to investigate a 'multi-head' version of our proposed model (analogous to multi-head attention), in which multiple distinct relations are processed in parallel. A major challenge for future work is to develop models that can match the human capacity for structured visual reasoning in the context of complex, real-world visual inputs.</p>
<h1>7 Acknowledgements</h1>
<p>Shanka Subhra Mondal was supported by Office of Naval Research grant N00014-22-1-2002 during the duration of this work. We would like to thank Princeton Research Computing, especially William G. Wischer and Josko Plazonic, for their help with scheduling training jobs on the Princeton University Della cluster.</p>
<h2>References</h2>
<p>[1] A. Altabaa, T. Webb, J. Cohen, and J. Lafferty. Abstractors: Transformer modules for symbolic message passing and relational reasoning. arXiv preprint arXiv:2304.00195, 2023.
[2] V. Bapst, A. Sanchez-Gonzalez, C. Doersch, K. Stachenfeld, P. Kohli, P. Battaglia, and J. Hamrick. Structured agents for physical construction. In International conference on machine learning, pages 464-474. PMLR, 2019.
[3] D. Barrett, F. Hill, A. Santoro, A. Morcos, and T. Lillicrap. Measuring abstract reasoning in neural networks. In International conference on machine learning, pages 511-520. PMLR, 2018.
[4] P. W. Battaglia, J. B. Hamrick, V. Bapst, A. Sanchez-Gonzalez, V. Zambaldi, M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018.
[5] L. Bergen, T. O’Donnell, and D. Bahdanau. Systematic generalization with edge transformers. Advances in Neural Information Processing Systems, 34:1390-1402, 2021.
[6] C. P. Burgess, L. Matthey, N. Watters, R. Kabra, I. Higgins, M. Botvinick, and A. Lerchner. Monet: Unsupervised scene decomposition and representation. arXiv preprint arXiv:1901.11390, 2019.
[7] M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9650-9660, 2021.
[8] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.
[9] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
[10] D. Ding, F. Hill, A. Santoro, M. Reynolds, and M. Botvinick. Attention over learned object embeddings enables complex visual reasoning. Advances in neural information processing systems, 34:9112-9124, 2021.
[11] A. Dittadi, S. Papa, M. De Vita, B. Schölkopf, O. Winther, and F. Locatello. Generalization and robustness implications in object-centric learning. arXiv preprint arXiv:2107.00637, 2021.
[12] M. Engelcke, O. Parker Jones, and I. Posner. Genesis-v2: Inferring unordered object representations without iterative refinement. Advances in Neural Information Processing Systems, 34:8085-8094, 2021.
[13] F. Fleuret, T. Li, C. Dubout, E. K. Wampler, S. Yantis, and D. Geman. Comparing machines and humans on a visual categorization test. Proceedings of the National Academy of Sciences, 108(43):17621-17625, 2011.
[14] A. Gopalakrishnan, S. van Steenkiste, and J. Schmidhuber. Unsupervised object keypoint learning using local spatial predictability. arXiv preprint arXiv:2011.12930, 2020.
[15] K. Greff, R. L. Kaufman, R. Kabra, N. Watters, C. Burgess, D. Zoran, L. Matthey, M. Botvinick, and A. Lerchner. Multi-object representation learning with iterative variational inference. In International Conference on Machine Learning, pages 2424-2433. PMLR, 2019.
[16] K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000-16009, 2022.</p>
<p>[17] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016.
[18] A. Jaegle, F. Gimeno, A. Brock, O. Vinyals, A. Zisserman, and J. Carreira. Perceiver: General perception with iterative attention. In International conference on machine learning, pages 4651-4664. PMLR, 2021.
[19] M. Janner, S. Levine, W. T. Freeman, J. B. Tenenbaum, C. Finn, and J. Wu. Reasoning about physical interactions with object-oriented prediction and planning. arXiv preprint arXiv:1812.10972, 2018.
[20] J. Jiang, F. Deng, G. Singh, and S. Ahn. Object-centric slot diffusion. arXiv preprint arXiv:2303.10834, 2023.
[21] J. Johnson, B. Hariharan, L. Van Der Maaten, L. Fei-Fei, C. Lawrence Zitnick, and R. Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 29012910, 2017.
[22] G. Kerg, S. Mittal, D. Rolnick, Y. Bengio, B. Richards, and G. Lajoie. On neural architecture inductive biases for relational tasks. arXiv preprint arXiv:2206.05056, 2022.
[23] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
[24] T. Kipf, E. Van der Pol, and M. Welling. Contrastive learning of structured world models. arXiv preprint arXiv:1911.12247, 2019.
[25] L. Kotovsky and D. Gentner. Comparison and categorization in the development of relational similarity. Child Development, 67(6):2797-2822, 1996.
[26] B. Lake and M. Baroni. Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. In International conference on machine learning, pages 2873-2882. PMLR, 2018.
[27] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum. Human-level concept learning through probabilistic program induction. Science, 350(6266):1332-1338, 2015.
[28] F. Locatello, D. Weissenborn, T. Unterthiner, A. Mahendran, G. Heigold, J. Uszkoreit, A. Dosovitskiy, and T. Kipf. Object-centric learning with slot attention. Advances in Neural Information Processing Systems, 33:11525-11538, 2020.
[29] B. J. Matlen, D. Gentner, and S. L. Franconeri. Spatial alignment facilitates visual comparison. Journal of Experimental Psychology: Human Perception and Performance, 46(5):443, 2020.
[30] S. S. Mondal, T. W. Webb, and J. D. Cohen. Learning to reason over visual objects. In 11th International Conference on Learning Representations, ICLR, 2023.
[31] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Automatic differentiation in pytorch. 2017.
[32] J. C. Raven. Progressive matrices: A perceptual test of intelligence, individual form. London: Lewis, 1938.
[33] M. Ricci, J. Kim, and T. Serre. Same-different problems strain convolutional neural networks. arXiv preprint arXiv:1802.03390, 2018.
[34] A. Santoro, R. Faulkner, D. Raposo, J. Rae, M. Chrzanowski, T. Weber, D. Wierstra, O. Vinyals, R. Pascanu, and T. Lillicrap. Relational recurrent neural networks. Advances in neural information processing systems, 31, 2018.
[35] A. Santoro, D. Raposo, D. G. Barrett, M. Malinowski, R. Pascanu, P. Battaglia, and T. Lillicrap. A simple neural network module for relational reasoning. Advances in neural information processing systems, 30, 2017.
[36] M. Seitzer, M. Horn, A. Zadaianchuk, D. Zietlow, T. Xiao, C.-J. Simon-Gabriel, T. He, Z. Zhang, B. Schölkopf, T. Brox, et al. Bridging the gap to real-world object-centric learning. arXiv preprint arXiv:2209.14860, 2022.
[37] M. Shanahan, K. Nikiforou, A. Creswell, C. Kaplanis, D. Barrett, and M. Garnelo. An explicitly relational neural network architecture. In International Conference on Machine Learning, pages 8593-8603. PMLR, 2020.</p>
<p>[38] P. Shaw, J. Uszkoreit, and A. Vaswani. Self-attention with relative position representations. arXiv preprint arXiv:1803.02155, 2018.
[39] G. Singh, Y.-F. Wu, and S. Ahn. Simple unsupervised object-centric learning for complex and naturalistic videos. arXiv preprint arXiv:2205.14065, 2022.
[40] A. Stanić and J. Schmidhuber. R-sqair: relational sequential attend, infer, repeat. arXiv preprint arXiv:1910.05231, 2019.
[41] A. Stanić, S. van Steenkiste, and J. Schmidhuber. Hierarchical relational inference. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 9730-9738, 2021.
[42] M. Vaishnav, R. Cadene, A. Alamia, D. Linsley, R. VanRullen, and T. Serre. Understanding the computational demands underlying visual reasoning. Neural Computation, 34(5):1075-1099, 2022.
[43] M. Vaishnav and T. Serre. Gamr: A guided attention model for (visual) reasoning. In 11th International Conference on Learning Representations, ICLR, 2023.
[44] S. Van Steenkiste, M. Chang, K. Greff, and J. Schmidhuber. Relational neural expectation maximization: Unsupervised discovery of objects and their interactions. arXiv preprint arXiv:1802.10353, 2018.
[45] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.
[46] R. Veerapaneni, J. D. Co-Reyes, M. Chang, M. Janner, C. Finn, J. Wu, J. Tenenbaum, and S. Levine. Entity abstraction in visual model-based reinforcement learning. In Conference on Robot Learning, pages 1439-1456. PMLR, 2020.
[47] N. Watters, L. Matthey, C. P. Burgess, and A. Lerchner. Spatial broadcast decoder: A simple architecture for learning disentangled representations in vaes. arXiv preprint arXiv:1901.07017, 2019.
[48] N. Watters, D. Zoran, T. Weber, P. Battaglia, R. Pascanu, and A. Tacchetti. Visual interaction networks: Learning a physics simulator from video. Advances in neural information processing systems, 30, 2017.
[49] T. Webb, Z. Dulberg, S. Frankland, A. Petrov, R. O’Reilly, and J. Cohen. Learning representations that support extrapolation. In International conference on machine learning, pages 10136-10146. PMLR, 2020.
[50] T. W. Webb, S. M. Frankland, A. Altabaa, K. Krishnamurthy, D. Campbell, J. Russin, R. O’Reilly, J. Lafferty, and J. D. Cohen. The relational bottleneck as an inductive bias for efficient abstraction. arXiv preprint arXiv:2309.06629, 2023.
[51] T. W. Webb, I. Sinha, and J. D. Cohen. Emergent symbols through binding in external memory. In 9th International Conference on Learning Representations, ICLR, 2021.
[52] Z. Wu, N. Dvornik, K. Greff, T. Kipf, and A. Garg. Slotformer: Unsupervised visual dynamics simulation with object-centric models. arXiv preprint arXiv:2210.05861, 2022.
[53] V. Zambaldi, D. Raposo, A. Santoro, V. Bapst, Y. Li, I. Babuschkin, K. Tuyls, D. Reichert, T. Lillicrap, E. Lockhart, et al. Relational deep reinforcement learning. arXiv preprint arXiv:1806.01830, 2018.
[54] J. Zhou, C. Wei, H. Wang, W. Shen, C. Xie, A. Yuille, and T. Kong. ibot: Image bert pre-training with online tokenizer. arXiv preprint arXiv:2111.07832, 2021.
[55] R. S. Zimmermann, S. van Steenkiste, M. S. Sajjadi, T. Kipf, and K. Greff. Sensitivity of slot-based object-centric models to their number of slots. arXiv preprint arXiv:2305.18890, 2023.</p>
<h1>Supplementary Material</h1>
<h2>S1 Code and Data Availability</h2>
<p>All code can be downloaded from https://github.com/Shanka123/OCRA, which also contains instructions to generate the CLEVR-ART dataset.</p>
<h2>S2 Datasets</h2>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure S1: Abstract Reasoning Tasks (ART). Same/different: Two objects are presented, and the task is to say whether they are the same or different. Relational match-to-sample: A source pair of objects is presented that either instantiates a 'same' or 'different' relation, and the task is to select the pair of target objects (out of two pairs) that instantiates the same relation. Problems were presented in a $2 \times 2$ array format, with the source pair presented in the top row, and a target pair presented in the bottom row (separate images for each target pair, see Figure S7). Distribution-of-3: A set of three objects is presented in the first row, and an incomplete set is presented in the second row. The task is to select the missing object from a set of four choices. Problems were presented in a $2 \times 3$ array format, with one of the answer choices inserted into the bottom right cell (separate images for each answer choice, see Figure S8). Identity rules: An abstract pattern is instantiated in the first row (ABA, ABB, or AAA), and the task is to select the choice that would result in the same relation being instantiated in the second row. Problems were presented in the same format as the distribution-of-3 task (Figure S9).</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure S2: A few examples of the multi-object inputs used to pre-train slot attention for ART.</p>
<p>Table S1: Number of training and test samples for ART dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$m=95$</th>
<th style="text-align: center;">$m=85$</th>
<th style="text-align: center;">$m=50$</th>
<th style="text-align: center;">$m=0$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">SD</td>
<td style="text-align: center;">Training</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">420</td>
<td style="text-align: center;">4900</td>
<td style="text-align: center;">18810</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">4900</td>
<td style="text-align: center;">990</td>
</tr>
<tr>
<td style="text-align: center;">RMTS</td>
<td style="text-align: center;">Training</td>
<td style="text-align: center;">480</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">10000</td>
</tr>
<tr>
<td style="text-align: center;">Dist3</td>
<td style="text-align: center;">Training</td>
<td style="text-align: center;">360</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">10000</td>
</tr>
<tr>
<td style="text-align: center;">ID</td>
<td style="text-align: center;">Training</td>
<td style="text-align: center;">8640</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">10000</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">10000</td>
</tr>
</tbody>
</table>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure S3: Example tasks from Synthetic Visual Reasoning Test (SVRT). (a) Example same/different (SD) task. Panels depict two examples from each of two categories for task 7. In category 1, there are always three sets of two identical objects. In category 2, there are always two sets of three identical objects. (b) Example spatial relation (SR) task. Panels depict two examples from each of two categories for task 3. In category 1, three out of four objects are in contact while the fourth object is positioned separately. In category 2, there are two sets of two objects in contact.</p>
<p>Relational match-to-sample
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure S4: CLEVR-ART. Relational match-to-sample: Example problem involving 'different' relation. Correct answer choice (left image) involves 'different' relation for both source pair (back row of objects) and target pair (front row of objects). Incorrect answer choice (right image) involves 'same' relation in target pair. Identity rules: Example problem involving ABA rule. Correct answer choice (top left image) involves ABA rule in both back row and front row of objects. Front right object in the other three images (incorrect choices) violates this rule.</p>
<h1>S3 Baselines</h1>
<h2>S3.1 GAMR</h2>
<p>For the ART and SVRT datasets, we compared to results for the GAMR baseline as originally reported in [43]. For CLEVR-ART, we used the implementation of GAMR provided at https://openreview.net/forum?id=iLMgk2IGNyv (Supplementary Material).</p>
<h2>S3.2 ResNet and Attn-ResNet</h2>
<p>For ART and SVRT, we compared to results from ResNet50 [17] as originally reported in [43]. For SVRT, we also compared to results from a version of ResNet that employed self-attention (Attn-ResNet), as reported in [43]. Attn-ResNet was similar to ResNet50, except that it included a transformer-style self-attention layer between ResNet blocks (see [43] for details).</p>
<h2>S3.3 Slot-based reasoning baselines</h2>
<p>We investigated a number of baseline models that combined our pre-trained slot attention module with alternative reasoning architectures. The slot attention module was the same as used in OCRA. The details of the reasoning architectures are described in the following sections. To train these baselines, we used the same settings as used for OCRA (learning rate, batch size, and number of training epochs), unless otherwise specified.</p>
<h2>S3.3.1 Slot-GAMR</h2>
<p>To combine slot attention with GAMR, we replaced $\boldsymbol{z}_{i m g}$ in the original model (originally a flattened feature map from a convolutional encoder) with the concatenated slot embeddings slots $\in \mathbb{R}^{K \times D}$. We trained slot-GAMR with a batch size of 32. The learning rate and number of training epochs for all ART tasks is given in Tables S8 and S7 respectively. These training details were based on those used in the original work [43], except that we used a different learning rate and trained the model for longer in some cases in order to achieve convergence on the training set.</p>
<h2>S3.3.2 Slot-ESBN</h2>
<p>To combine slot attention with the Emergent Symbol Binding Network (ESBN), the slot embeddings were passed sequentially to the ESBN, replacing $\boldsymbol{z}_{t=1 \ldots T}$ in the original architecture (one slot embedding per timestep). The architectural details for ESBN were the same as reported in [51], except that the output layer from the LSTM controller was changed based on the task (as described in Section 4.3.2). We used the implementation provided at https://github.com/taylorwwebb/emergent_symbols. We trained slot-ESBN with a batch size of 32 and a learning rate of $5 e-4$. The number of training epochs for all ART tasks is given in Table S9. These training details were based on those used in the original work [51], except that we trained the model for longer in some cases.</p>
<h2>S3.3.3 Slot-RN</h2>
<p>For the slot-RN baseline, we first passed each pair of slot embeddings through a shared MLP (referred to as $g_{\theta}$ in the RN framework [35]), with a hidden layer of size 512, an output layer of size 256, and ReLU nonlinearities in both layers. The outputs of this MLP were then summed elementwise and passed to a second MLP $\left(f_{\phi}\right)$, with a hidden layer of size 256 and ReLU nonlinearities, and a single output unit (the nonlinearity applied to this output depended on the task, as described in Section 4.3.2).</p>
<h2>S3.3.4 Slot-Transformer</h2>
<p>For the slot-transformer baseline, we passed the slot embeddings directly to the transformer used in OCRA. Note that this is identical to the ablation model referred to as '- Relation Embeddings' in Table 1.</p>
<h1>S3.3.5 Slot-IN</h1>
<p>For the Slot-IN baseline, the slot embeddings were passed to an Interaction Net (IN), with hyperparameters identical to those described in [48] (the 'Interaction Net' described in Section 8.2 of that work). Slot embeddings were updated for $T=6$ iterations, then summed elementwise and passed through a final MLP with a hidden layer of size 64 and ReLU nonlinearities, and a single output unit (with the nonlinearity depending on the task, as described in Section 4.3.2).</p>
<h2>S3.3.6 Slot-CoRelNet</h2>
<p>For the Slot-CoRelNet baseline, we computed the matrix of all pairwise dot products between slot embeddings, applied a softmax function across the rows of this matrix, flattened the matrix and passed it to an MLP with the same hyperparameters described in [22].</p>
<h2>S3.4 MAE</h2>
<p>We applied the Masked Autoencoder (MAE) model [16] on the identity rules and distribution-of-3 ART tasks by masking out the final object in each problem (in the bottom right cell of the input), and training the model to fill in this patch. To select from the set of multiple choices, we then compared the model's generated output with the four answer choices, and selected the choice with lowest mean-squared error. We used the same hyperparameters described in [16] for the MAE architecture. We trained the model for 400 epochs with a batch size of 64 , using a learning rate of $2.5 e^{-4}$ with warmup for 40 epochs followed by cosine learning rate decay. We did not test this model on the same/different or RMTS tasks, as it was not clear how to formulate these tasks in a generative manner.</p>
<h2>S4 Training details and hyperparameters</h2>
<p>Before computing relational embeddings, we applied temporal context normalization (TCN) [49] to both the the feature embeddings and the position embeddings. TCN normalizes representations across the temporal dimension, and has been shown to improve generalization in relational reasoning tasks. When applying the relational operator $\phi$, we also softmax-normalized the dot products for all pairwise comparisons.
As shown in Table S4, we pretrained slot attention for SVRT using 500 training examples for each of the 23 tasks with a learning rate of $4 e-4$ for the first 1350 epochs, followed by using a learning rate of $8 e-5$ for the next 4620 epochs. When using 1000 training examples for each of the 23 tasks we first pretrained slot attention with a learning rate of $4 e-4$ for the first 1500 epochs, followed by using a learning rate of $8 e-5$ for the next 3510 epochs.</p>
<p>Table S2: CNN encoder hyperparameters.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">TYPE</th>
<th style="text-align: center;">ChANNELS</th>
<th style="text-align: center;">ACTIVATION</th>
<th style="text-align: center;">KERNEL SIZE</th>
<th style="text-align: center;">STRIDE</th>
<th style="text-align: center;">PADDING</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">2D CONV</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">RELU</td>
<td style="text-align: center;">$5 \times 5$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">2D CONV</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">RELU</td>
<td style="text-align: center;">$5 \times 5$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">2D CONV</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">RELU</td>
<td style="text-align: center;">$5 \times 5$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">2D CONV</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">RELU</td>
<td style="text-align: center;">$5 \times 5$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">POSITION EMBEDDING</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Flatten</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">LAYER NORM</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">1D CONV</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">RELU</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">1D CONV</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>Table S3: Slot decoder hyperparameters.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Type</th>
<th style="text-align: center;">Channels</th>
<th style="text-align: center;">Activation</th>
<th style="text-align: center;">Kernel Size</th>
<th style="text-align: center;">Stride</th>
<th style="text-align: center;">Padding</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Spatial Broadcast</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Position Embedding</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">2D Conv</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">ReLU</td>
<td style="text-align: center;">$5 \times 5$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">2D Conv</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">ReLU</td>
<td style="text-align: center;">$5 \times 5$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">2D Conv</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">ReLU</td>
<td style="text-align: center;">$5 \times 5$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">2D Conv</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">ReLU</td>
<td style="text-align: center;">$5 \times 5$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">2D Conv</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">ReLU</td>
<td style="text-align: center;">$5 \times 5$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">2D Conv</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">ReLU</td>
<td style="text-align: center;">$5 \times 5$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">2D Conv</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$3 \times 3$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
</tbody>
</table>
<p>Table S4: Slot attention pre-training details for all datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">ART</th>
<th style="text-align: center;">SVRT <br> Dataset Size $=0.5 \mathrm{k}$</th>
<th style="text-align: center;">SVRT <br> Dataset Size $=1 \mathrm{k}$</th>
<th style="text-align: center;">CLEVR-ART</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Batch size</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: left;">Learning rate</td>
<td style="text-align: center;">$8 e-5$</td>
<td style="text-align: center;">$4 e-4,8 e-5$</td>
<td style="text-align: center;">$4 e-4,8 e-5$</td>
<td style="text-align: center;">$8 e-5$</td>
</tr>
<tr>
<td style="text-align: left;">LR warmup steps</td>
<td style="text-align: center;">150 k</td>
<td style="text-align: center;">9 k</td>
<td style="text-align: center;">18 k</td>
<td style="text-align: center;">90 k</td>
</tr>
<tr>
<td style="text-align: left;">Epochs</td>
<td style="text-align: center;">750</td>
<td style="text-align: center;">1350,4620</td>
<td style="text-align: center;">1500,3510</td>
<td style="text-align: center;">1000</td>
</tr>
</tbody>
</table>
<p>Table S5: Hyperparameters for Transformer reasoning module. $H$ is the number of heads, $L$ is the number of layers, $D_{\text {head }}$ is the dimensionality of each head, and $D_{M L P}$ is the dimensionality of the MLP hidden layer.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">ART</th>
<th style="text-align: right;">SVRT</th>
<th style="text-align: right;">CLEVR-ART</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$H$</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">8</td>
</tr>
<tr>
<td style="text-align: left;">$L$</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">24</td>
<td style="text-align: right;">24</td>
</tr>
<tr>
<td style="text-align: left;">$D_{\text {head }}$</td>
<td style="text-align: right;">64</td>
<td style="text-align: right;">64</td>
<td style="text-align: right;">64</td>
</tr>
<tr>
<td style="text-align: left;">$D_{M L P}$</td>
<td style="text-align: right;">512</td>
<td style="text-align: right;">512</td>
<td style="text-align: right;">512</td>
</tr>
<tr>
<td style="text-align: left;">Dropout</td>
<td style="text-align: right;">0.1</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
</tbody>
</table>
<p>Table S6: Default number of training epochs for ART tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">TASK</th>
<th style="text-align: center;">$m=0$</th>
<th style="text-align: center;">$m=50$</th>
<th style="text-align: center;">$m=85$</th>
<th style="text-align: center;">$m=95$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SD</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">600</td>
</tr>
<tr>
<td style="text-align: left;">RMTS</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">400</td>
</tr>
<tr>
<td style="text-align: left;">Dist3</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">400</td>
</tr>
<tr>
<td style="text-align: left;">ID</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
</tr>
</tbody>
</table>
<p>Table S7: Number of training epochs for slot-GAMR baseline on ART tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">TASK</th>
<th style="text-align: center;">$m=0$</th>
<th style="text-align: center;">$m=50$</th>
<th style="text-align: center;">$m=85$</th>
<th style="text-align: center;">$m=95$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SD</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">200</td>
</tr>
<tr>
<td style="text-align: left;">RMTS</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">300</td>
</tr>
<tr>
<td style="text-align: left;">Dist3</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">300</td>
</tr>
<tr>
<td style="text-align: left;">ID</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">100</td>
</tr>
</tbody>
</table>
<p>Table S8: Learning rate for slot-GAMR baseline on ART tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">TASK</th>
<th style="text-align: left;">$m=0$</th>
<th style="text-align: left;">$m=50$</th>
<th style="text-align: left;">$m=85$</th>
<th style="text-align: left;">$m=95$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SD</td>
<td style="text-align: left;">$1 e-4$</td>
<td style="text-align: left;">$5 e-4$</td>
<td style="text-align: left;">$5 e-4$</td>
<td style="text-align: left;">$1 e-3$</td>
</tr>
<tr>
<td style="text-align: left;">RMTS</td>
<td style="text-align: left;">$5 e-4$</td>
<td style="text-align: left;">$1 e-4$</td>
<td style="text-align: left;">$5 e-4$</td>
<td style="text-align: left;">$5 e-4$</td>
</tr>
<tr>
<td style="text-align: left;">DIST3</td>
<td style="text-align: left;">$5 e-4$</td>
<td style="text-align: left;">$1 e-4$</td>
<td style="text-align: left;">$5 e-5$</td>
<td style="text-align: left;">$5 e-4$</td>
</tr>
<tr>
<td style="text-align: left;">ID</td>
<td style="text-align: left;">$5 e-4$</td>
<td style="text-align: left;">$5 e-4$</td>
<td style="text-align: left;">$5 e-4$</td>
<td style="text-align: left;">$5 e-4$</td>
</tr>
</tbody>
</table>
<p>Table S9: Number of training epochs for slot-ESBN baseline on ART tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">TASK</th>
<th style="text-align: left;">$m=0$</th>
<th style="text-align: left;">$m=50$</th>
<th style="text-align: left;">$m=85$</th>
<th style="text-align: left;">$m=95$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SD</td>
<td style="text-align: left;">150</td>
<td style="text-align: left;">150</td>
<td style="text-align: left;">150</td>
<td style="text-align: left;">200</td>
</tr>
<tr>
<td style="text-align: left;">RMTS</td>
<td style="text-align: left;">150</td>
<td style="text-align: left;">150</td>
<td style="text-align: left;">150</td>
<td style="text-align: left;">200</td>
</tr>
<tr>
<td style="text-align: left;">DIST3</td>
<td style="text-align: left;">150</td>
<td style="text-align: left;">150</td>
<td style="text-align: left;">150</td>
<td style="text-align: left;">200</td>
</tr>
<tr>
<td style="text-align: left;">ID</td>
<td style="text-align: left;">150</td>
<td style="text-align: left;">150</td>
<td style="text-align: left;">150</td>
<td style="text-align: left;">150</td>
</tr>
</tbody>
</table>
<p>Table S10: Number of training epochs for CLEVR-ART tasks. OCRA was trained for longer on the identity rules (ID) task in order to achieve convergence on the training set. 50 epochs was sufficient to achieve convergence on both tasks for GAMR, and on RMTS for OCRA.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">TASK</th>
<th style="text-align: center;">OCRA</th>
<th style="text-align: center;">GAMR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RMTS</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: left;">ID</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">50</td>
</tr>
</tbody>
</table>
<h1>S5 Results</h1>
<p>Table S11: Results for ART same/different task.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">$m=0$</th>
<th style="text-align: left;">$m=50$</th>
<th style="text-align: left;">$m=85$</th>
<th style="text-align: left;">$m=95$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">OCRA (OURS)</td>
<td style="text-align: left;">$\mathbf{9 7 . 3 1 \pm 1 . 5}$</td>
<td style="text-align: left;">$\mathbf{9 5 . 9 8 \pm 1 . 1}$</td>
<td style="text-align: left;">$\mathbf{9 6 . 4 8 \pm 0 . 3}$</td>
<td style="text-align: left;">$\mathbf{8 7 . 9 5 \pm 1 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">SLOT-TRANSFORMER</td>
<td style="text-align: left;">$\mathbf{9 8 . 4 7 \pm 0 . 6}$</td>
<td style="text-align: left;">$\mathbf{9 5 . 9 5 \pm 0 . 5}$</td>
<td style="text-align: left;">$87.53 \pm 1.1$</td>
<td style="text-align: left;">$68.46 \pm 2.0$</td>
</tr>
<tr>
<td style="text-align: left;">SLOT-IN</td>
<td style="text-align: left;">$94.59 \pm 1.4$</td>
<td style="text-align: left;">$92.43 \pm 0.8$</td>
<td style="text-align: left;">$68.4 \pm 4.3$</td>
<td style="text-align: left;">$59.23 \pm 2.3$</td>
</tr>
<tr>
<td style="text-align: left;">SLOT-RN</td>
<td style="text-align: left;">$93.97 \pm 2.8$</td>
<td style="text-align: left;">$94.23 \pm 0.7$</td>
<td style="text-align: left;">$83.66 \pm 1.2$</td>
<td style="text-align: left;">$77.26 \pm 1.9$</td>
</tr>
<tr>
<td style="text-align: left;">SLOT-GAMR</td>
<td style="text-align: left;">$61.43 \pm 2.7$</td>
<td style="text-align: left;">$67.35 \pm 1.1$</td>
<td style="text-align: left;">$63.97 \pm 0.5$</td>
<td style="text-align: left;">$62.98 \pm 1.4$</td>
</tr>
<tr>
<td style="text-align: left;">SLOT-ESBN</td>
<td style="text-align: left;">$65.42 \pm 6.2$</td>
<td style="text-align: left;">$54.1 \pm 3.7$</td>
<td style="text-align: left;">$49.79 \pm 0.1$</td>
<td style="text-align: left;">$50.02 \pm 0.2$</td>
</tr>
<tr>
<td style="text-align: left;">SLOT-CORELNET</td>
<td style="text-align: left;">$58.24 \pm 2.1$</td>
<td style="text-align: left;">$55.91 \pm 0.3$</td>
<td style="text-align: left;">$50.70 \pm 0.2$</td>
<td style="text-align: left;">$50.50 \pm 0.2$</td>
</tr>
<tr>
<td style="text-align: left;">GAMR</td>
<td style="text-align: left;">$\mathbf{9 7 . 2 8 \pm 0 . 5}$</td>
<td style="text-align: left;">$\mathbf{9 4 . 4 \pm 0 . 3}$</td>
<td style="text-align: left;">$87.88 \pm 1.3$</td>
<td style="text-align: left;">$83.49 \pm 1.4$</td>
</tr>
<tr>
<td style="text-align: left;">RESNET</td>
<td style="text-align: left;">$95.65 \pm 0.7$</td>
<td style="text-align: left;">$92.23 \pm 0.6$</td>
<td style="text-align: left;">$82.83 \pm 1.4$</td>
<td style="text-align: left;">$66.6 \pm 1.5$</td>
</tr>
</tbody>
</table>
<p>Table S12: Results for ART relational-match-to-sample task.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">$m=0$</th>
<th style="text-align: left;">$m=50$</th>
<th style="text-align: left;">$m=85$</th>
<th style="text-align: left;">$m=95$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">OCRA (OURS)</td>
<td style="text-align: left;">$\mathbf{9 9 . 9 1 \pm 0 . 0}$</td>
<td style="text-align: left;">$\mathbf{9 9 . 2 5 \pm 0 . 1}$</td>
<td style="text-align: left;">$\mathbf{9 4 . 4 3 \pm 0 . 7}$</td>
<td style="text-align: left;">$\mathbf{8 5 . 3 1 \pm 2 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">SLOT-TRANSFORMER</td>
<td style="text-align: left;">$\mathbf{9 9 . 7 3 \pm 0 . 0}$</td>
<td style="text-align: left;">$\mathbf{9 9 . 1 2 \pm 0 . 1}$</td>
<td style="text-align: left;">$\mathbf{9 6 . 7 7 \pm 0 . 3}$</td>
<td style="text-align: left;">$73.99 \pm 3.0$</td>
</tr>
<tr>
<td style="text-align: left;">SLOT-IN</td>
<td style="text-align: left;">$93.42 \pm 0.7$</td>
<td style="text-align: left;">$87.42 \pm 0.7$</td>
<td style="text-align: left;">$86.71 \pm 1.8$</td>
<td style="text-align: left;">$56.93 \pm 0.8$</td>
</tr>
<tr>
<td style="text-align: left;">SLOT-RN</td>
<td style="text-align: left;">$91.31 \pm 0.3$</td>
<td style="text-align: left;">$87.1 \pm 0.9$</td>
<td style="text-align: left;">$68.4 \pm 1.3$</td>
<td style="text-align: left;">$61.62 \pm 1.1$</td>
</tr>
<tr>
<td style="text-align: left;">SLOT-GAMR</td>
<td style="text-align: left;">$80.92 \pm 6.9$</td>
<td style="text-align: left;">$93.43 \pm 0.4$</td>
<td style="text-align: left;">$61.47 \pm 0.8$</td>
<td style="text-align: left;">$59.55 \pm 2.7$</td>
</tr>
<tr>
<td style="text-align: left;">SLOT-ESBN</td>
<td style="text-align: left;">$50.63 \pm 0.4$</td>
<td style="text-align: left;">$50.3 \pm 0.4$</td>
<td style="text-align: left;">$57.12 \pm 3.0$</td>
<td style="text-align: left;">$49.99 \pm 0.2$</td>
</tr>
<tr>
<td style="text-align: left;">SLOT-CORELNET</td>
<td style="text-align: left;">$50.18 \pm 0.1$</td>
<td style="text-align: left;">$50.30 \pm 0.2$</td>
<td style="text-align: left;">$49.84 \pm 0.2$</td>
<td style="text-align: left;">$49.82 \pm 0.2$</td>
</tr>
<tr>
<td style="text-align: left;">GAMR</td>
<td style="text-align: left;">$\mathbf{9 8 . 1 2 \pm 0 . 4}$</td>
<td style="text-align: left;">$91.98 \pm 0.8$</td>
<td style="text-align: left;">$89.81 \pm 0.5$</td>
<td style="text-align: left;">$72.2 \pm 3.0$</td>
</tr>
<tr>
<td style="text-align: left;">RESNET</td>
<td style="text-align: left;">$53.77 \pm 4.0$</td>
<td style="text-align: left;">$69.43 \pm 6.5$</td>
<td style="text-align: left;">$82.18 \pm 1.0$</td>
<td style="text-align: left;">$49.89 \pm 0.2$</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://github.com/bbvanexttechnologies/unicode-images-database&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4426 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4426</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4426</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-277781402</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.08762v1.pdf" target="_blank">InteractiveSurvey: An LLM-based Personalized and Interactive Survey Paper Generation System</a></p>
                <p><strong>Paper Abstract:</strong> The exponential growth of academic literature creates urgent demands for comprehensive survey papers, yet manual writing remains time-consuming and labor-intensive. Recent advances in large language models (LLMs) and retrieval-augmented generation (RAG) facilitate studies in synthesizing survey papers from multiple references, but most existing works restrict users to title-only inputs and fixed outputs, neglecting the personalized process of survey paper writing. In this paper, we introduce InteractiveSurvey - an LLM-based personalized and interactive survey paper generation system. InteractiveSurvey can generate structured, multi-modal survey papers with reference categorizations from multiple reference papers through both online retrieval and user uploads. More importantly, users can customize and refine intermediate components continuously during generation, including reference categorization, outline, and survey content through an intuitive interface. Evaluations of content quality, time efficiency, and user studies show that InteractiveSurvey is an easy-to-use survey generation system that outperforms most LLMs and existing methods in output content quality while remaining highly time-efficient.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4426.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4426.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InteractiveSurvey</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InteractiveSurvey: An LLM-based Personalized and Interactive Survey Paper Generation System</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A web-based system that uses LLMs + retrieval to automatically search, parse, categorize, and synthesize multiple reference papers into structured, multimodal survey papers while allowing users to iteratively refine intermediate outputs (reference categories, outline, content).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>InteractiveSurvey</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>InteractiveSurvey is an end-to-end web system that: (1) constructs arXiv search queries using an LLM-generated topic description; (2) retrieves and/or ingests user-uploaded PDFs; (3) parses documents into structured Markdown (via MinerU) and stores chunked embeddings in a vector database for RAG; (4) performs personalized reference categorization using HyDE-style pseudo-query generation + semantic retrieval and clustering (UMAP dim. reduction + HDBSCAN + silhouette-based cluster-number selection); (5) generates a three-level hierarchical outline (predefined section titles, categorization names as section titles, LLM-subsection titles) and allows iterative manual edits; (6) synthesizes survey content bottom-up: retrieve chunks per subsection, LLM-generate subsection content, LLM-summarize and merge, and produce pre-defined sections (Abstract, Intro, Conclusion) from generated content; (7) integrates multimodal elements by semantically matching figure/table captions from references and adding adaptive citation links using an adaptive cosine-similarity threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>qwen2.5-72b-instruct (used as backbone in experiments); supports replacing backbone with any LLM API (authors compared against GPT-4o, DeepSeek-R1, Qwen2.5-72b in evaluations).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Document parsing to Markdown (MinerU); chunking into fixed-length chunks; embedding-based retrieval (vector DB) for RAG; HyDE-style pseudo-document (pseudo-query) generation to improve retrieval for categorization; semantic matching between generated sentences and figure/table captions.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Bottom-up hierarchical summarization: section-by-section retrieval-augmented generation, LLM summarization of subsections, LLM merging (LLMMerge) into section content; hierarchical outline-guided generation (three-level outline); multimodal integration and adaptive citation assignment across sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Typical pipelines operate on dozens to ~50 references (system evaluated with average 45.2 references per topic; system can truncate to MAX_REF).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scientific literature across arXiv; evaluated on 40 topics from 8 fields (Computer Science, Mathematics, Physics, Statistics, Electrical Engineering and Systems Science, Quantitative Biology, Quantitative Finance, Economics).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured, multimodal survey papers (hierarchical outline, sections/subsections, figures/tables from references, inline citations), exportable to PDF/Markdown/LaTeX.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>LLM-based judging on Coverage, Structure, Relevance; time-efficiency (seconds/minutes to generate); System Usability Scale (SUS) for usability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Qualitative: authors report InteractiveSurvey outperforms three mainstream LLMs and two SOTA survey-generation methods on average across Coverage/Structure/Relevance (evaluated over 40 topics). Time: average 2,077.8 seconds (~35 minutes) to generate a survey with ≈50 references on the reported hardware. Usability: SUS score = 84.4/100 (A+ tier).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against direct prompting with LLMs (GPT-4o, DeepSeek-R1, Qwen2.5-72b) and SOTA methods AutoSurvey and SurveyX (and sample outputs they released).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Reported to 'consistently outperform' mainstream LLMs and to achieve the highest average scores across evaluation metrics vs. AutoSurvey and SurveyX (exact numeric comparative scores not included in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Interactive, modifiable intermediate processes (reference categorization, outline, content editing) plus RAG and HyDE-style retrieval improve survey quality and personalization compared to title-only or fixed-output approaches; bottom-up hierarchical synthesis and multimodal integration yield coherent structured surveys efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Context-window limits of LLMs constrain use of long contexts; the most time-consuming steps are reference parsing and personalized categorization (GPU-bound); reliance on public arXiv (user uploads allowed for copyrighted material); potential factual/information limits implicit in LLMs are acknowledged (context retention limitations).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Authors note time scales with hardware capability (with better hardware they expect total generation time to fall below 30 minutes); clustering adapts to topic variability (no single optimal cluster number), and LLM performance is constrained by context window even with more input text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InteractiveSurvey: An LLM-based Personalized and Interactive Survey Paper Generation System', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4426.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4426.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A paradigm that augments LLM generation with retrieved document chunks from a vector store so the model can condition on external text beyond its context window.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-augmented generation for large language models: A survey.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>RAG stores semantic embeddings of document chunks in a vector database; at generation time it retrieves relevant chunks for prompts and conditions the LLM on that retrieved context to produce answers/summaries. In InteractiveSurvey, RAG is used both for content generation (retrieve reference chunks per subsection) and for per-sentence citation sourcing.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Used in combination with qwen2.5-72b-instruct in this system; RAG is model-agnostic.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Embedding-based retrieval (chunk-level embeddings stored in vector DB); semantic matching via cosine similarity; HyDE pseudo-queries optionally used to improve retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>LLM generation conditioned on retrieved chunks; bottom-up assembly of subsections by summarizing retrieved content and merging via LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied to tens of papers per topic in this system (authors evaluate on ~45 references per topic); RAG itself supports scaling to hundreds of documents depending on chunking and retrieval strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific literature across arXiv (general-purpose).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Survey text, subsection drafts, citations tied to source chunks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified for RAG alone in paper; system-level metrics include coverage/structure/relevance and time.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported to 'facilitate synthesizing survey papers from multiple references' and forms a core part of the generation pipeline; specific RAG-only metrics not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Implicit baseline: direct LLM prompting without retrieval/context augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>InteractiveSurvey (which uses RAG) outperforms direct prompting LLM baselines in coverage/structure/relevance per authors' LLM-judge evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RAG enables incorporation of many external references that exceed model context windows and improves factual grounding and citation assignment when combined with adaptive similarity thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Quality depends on retrieval effectiveness (embedding model, query formulation); adaptive thresholding required to avoid over/under-citation; still limited by downstream LLM context-window when integrating many chunks.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>RAG enables scaling to many documents by retrieving only relevant chunks, but end-to-end time and retrieval quality scale with number of references and hardware; authors observed parsing and categorization as dominant time costs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InteractiveSurvey: An LLM-based Personalized and Interactive Survey Paper Generation System', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4426.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4426.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HyDE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hypothetical Document Embeddings (HyDE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval technique where an LLM generates hypothetical/pseudo-documents or pseudo-queries that are then embedded for semantic retrieval to improve zero-shot retrieval accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Precise zero-shot dense retrieval without relevance labels.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>HyDE-based Context Retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>InteractiveSurvey prompts an LLM to generate multiple (10) hypothetical queries/descriptions for a given categorization criterion k; these pseudo-documents are embedded and used in parallel to retrieve the most relevant chunks from each reference collection to improve semantic matching for categorization and downstream clustering.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>An LLM (not uniquely specified; the system uses its backbone LLM for pseudo-document generation; experiments used qwen2.5-72b-instruct).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Pseudo-document generation by LLM (HyDE) + embedding-based retrieval across chunk collections.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Aggregated retrieved chunks (from multiple HyDE queries) are used as the basis for clustering and for feeding LLMs to generate cluster descriptions and names.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied per-topic across the set of reference papers gathered (≈45 per topic in evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Applied to reference categorization across general scientific topics from arXiv.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Improved retrieval contexts for semantic clustering and categorization, leading to section/subsection content.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not reported as separate metrics; used internally to improve categorization quality judged in overall system evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Authors state HyDE-based retrieval 'facilitates accurate retrieval' for categorization; no standalone numeric results reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Implicit comparison to direct retrieval using the raw criterion as query (no HyDE pseudo-docs).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Claimed improvement in retrieval accuracy for categorization (qualitative, no numeric values reported).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using multiple LLM-generated pseudo-queries in parallel improves semantic matching for categorization, enabling better thematic clustering from reference content.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Adds LLM generation overhead; retrieval effectiveness depends on quality/diversity of HyDE prompts and embedding model.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Costs scale with number of pseudo-queries and number of reference collections; intended to improve zero-shot retrieval without labels.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InteractiveSurvey: An LLM-based Personalized and Interactive Survey Paper Generation System', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4426.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4426.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MinerU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MinerU (document parsing tool)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source tool that extracts and parses PDF reference papers into structured Markdown files to enable downstream processing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mineru: An open-source solution for precise document content extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MinerU document parser</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>MinerU ingests PDF papers and extracts structured elements (title, authors, abstract, introduction etc.) and outputs Markdown files used by InteractiveSurvey to display metadata and to chunk content for embedding and RAG.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>None (document parsing tool); used upstream of LLM-based components.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>PDF parsing and structured content extraction into Markdown; subsequent chunking for embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Not applicable (preprocessing/parsing stage).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Used to parse all reference papers collected for a topic (≈45 per topic in evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific papers (arXiv).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured Markdown files and chunked text for embedding and retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not reported in this paper (tool cited as established).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Used successfully in pipeline; parsing identified as one of the most time-consuming steps in the end-to-end pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not compared in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Accurate parsing into structured markdown simplifies metadata display and enables downstream embedding/RAG workflows; parsing runtime contributes substantially to end-to-end time.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Parsing large numbers of PDFs is computationally heavy and noted as a time bottleneck in the system.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Parsing time increases with number and length of PDFs; authors note GPU-bound parsing and expect gains with stronger hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InteractiveSurvey: An LLM-based Personalized and Interactive Survey Paper Generation System', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4426.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4426.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LitLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LitLLM: A toolkit for scientific literature review</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A toolkit that retrieves relevant papers and generates survey content (from user-provided abstracts) using LLMs as part of automated literature-review pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Litllm: A toolkit for scientific literature review.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LitLLM</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as an automated survey system that can retrieve relevant papers and generate survey content from user-provided abstracts; presented as related work in automated survey generation leveraging RAG and document parsing.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper (referenced work).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Retrieval from abstracts (per authors' description) and LLM-based summarization (referenced work).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>LLM-based generation of survey content from retrieved/provided abstracts (per related-work summary).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified here (depends on LitLLM implementation).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific literature review (general).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Survey content / literature review text.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not detailed in this paper (cited as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not reported in this paper (referenced as existing toolkit).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not applicable in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Represents a class of toolkits that combine retrieval and LLM generation for literature review automation; cited to situate InteractiveSurvey among automated systems.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Cited limitations of existing systems in general: many restrict user inputs to title-only and offer fixed outputs without interactive intermediate refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InteractiveSurvey: An LLM-based Personalized and Interactive Survey Paper Generation System', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4426.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4426.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HiReview</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HiReview: Hierarchical Taxonomy-Driven Automatic Literature Review Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated literature-review system that employs hierarchical clustering on citation graphs to construct taxonomy trees for survey generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>HiReview: Hierarchical Taxonomy-Driven Automatic Literature Review Generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>HiReview</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>HiReview constructs taxonomy trees for literature reviews by performing hierarchical clustering on citation graphs, then uses that taxonomy to drive automatic literature review generation (cited as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper (referenced work).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Citation-graph-based clustering and taxonomy construction.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Taxonomy-driven generation of review content (details in cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Automated literature review generation (general / scientific literature).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Taxonomy trees and generated literature-review text.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not reported in this paper (cited as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not applicable in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Demonstrates use of citation-graph structure to organize literature into hierarchical taxonomies for survey generation; cited as an alternative to thematic clustering approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InteractiveSurvey: An LLM-based Personalized and Interactive Survey Paper Generation System', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4426.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4426.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoSurvey</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoSurvey: Large Language Models Can Automatically Write Surveys</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior automated survey generation system that the authors use as a SOTA comparison; asserted to produce survey samples (cited) and to employ LLMs to generate long-form survey content.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AutoSurvey: Large Language Models Can Automatically Write Surveys.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AutoSurvey</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as a state-of-the-art survey generation method that can automatically write surveys with LLMs; authors compared InteractiveSurvey outputs against the survey samples released by AutoSurvey.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Reported in the referenced AutoSurvey work (not detailed in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Not specified here (referenced work).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>LLM-based automatic survey generation (details in cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified here; AutoSurvey samples used for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Survey generation for academic literature (general).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Survey papers (long-form).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Compared at system level by LLM judges on coverage/structure/relevance in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>InteractiveSurvey achieves higher average scores than AutoSurvey on the evaluation set (exact numeric values not provided in text).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>AutoSurvey's released samples served as a baseline for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>InteractiveSurvey reported to outperform AutoSurvey on average across the authors' LLM-judge metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>InteractiveSurvey's interactive/modifiable pipeline yields superior coverage/structure/relevance relative to AutoSurvey samples per authors' evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>AutoSurvey implementation is under continuous iteration (per authors), and only released samples were available for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InteractiveSurvey: An LLM-based Personalized and Interactive Survey Paper Generation System', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4426.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4426.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SurveyX</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SurveyX: Academic survey automation via large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent SOTA automated survey-generation method cited and used as a comparison point by the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Surveyx: Academic survey automation via large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SurveyX</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as a state-of-the-art method for academic survey automation using LLMs; authors compared InteractiveSurvey outputs against SurveyX's released survey samples.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>SurveyX samples were generated by GPT-4o per the referenced description (cited in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Not detailed in this paper (referenced work).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>LLM-based survey automation (details in cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Academic survey generation (general).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Survey papers (long-form).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Compared by LLM judges on coverage/structure/relevance in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>InteractiveSurvey achieves higher average scores than SurveyX samples on the evaluation set (exact numeric values not provided in text).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>SurveyX released samples served as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Reported improvement by InteractiveSurvey over SurveyX on average across metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>InteractiveSurvey's user-interactive pipeline and RAG-based retrieval provide advantages over the released SurveyX samples per evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>SurveyX implementation was not available to run directly; comparison used released samples only.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InteractiveSurvey: An LLM-based Personalized and Interactive Survey Paper Generation System', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4426.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4426.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PROMPTHEUS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PROMPTHEUS: A Human-Centered Pipeline to Streamline SLRs with LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human-centered pipeline for systematic literature reviews that incorporates clustering-based topic modeling to promote structural coherence and factual accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PROMPTHEUS: A Human-Centered Pipeline to Streamline SLRs with LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PROMPTHEUS</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as a human-centered pipeline that uses clustering-based topic modeling as part of LLM-driven SLR pipelines to ensure structure and factual accuracy; presented in related work as an advanced strategy for high-quality survey generation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper (referenced work).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Clustering-based topic modeling + LLM summarization (per related-work description).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Structure-driven LLM synthesis guided by topic clusters to maintain coherence and factual grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Systematic literature reviews (SLRs).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured SLR outputs (surveys, reviews).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not provided in this paper (cited as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not applicable in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Clustering-based topic modeling combined with human-centered pipeline design supports structural coherence and factual accuracy in LLM-enabled SLRs (as characterized by authors).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InteractiveSurvey: An LLM-based Personalized and Interactive Survey Paper Generation System', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4426.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e4426.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatCite</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-agent approach employing prompt engineering and human workflow guidance to generate comparative literature summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatCite</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as a method that uses prompt engineering and LLM agents to produce comparative literature summaries with human workflow guidance; referenced in the context of techniques to generate structured summaries covering multiple papers.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper (referenced work).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Prompt-engineering-driven retrieval/QA style summarization (per related-work description).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Agent-guided comparative summarization across multiple papers.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Comparative literature summaries (academic papers).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Comparative literature summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not reported in this paper (cited as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not applicable in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Prompt engineering and agent workflows can help structure comparative literature summaries across papers (as characterized by authors).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InteractiveSurvey: An LLM-based Personalized and Interactive Survey Paper Generation System', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4426.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e4426.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>multi-LLM agents (Wang et al., 2024b)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-LLM agent architectures for long-form content creation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that composes multiple LLM agents (specialized or cooperating instances) to enable creation of long-form content beyond single-model context limits.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AutoSurvey: Large Language Models Can Automatically Write Surveys.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Multi-LLM agent architectures</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited (Wang et al., 2024b) as an approach using multiple LLM agents in a coordinated architecture to produce long-form content (e.g., 64k-token outputs), enabling scaling beyond single-model context windows.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified here (cited work describes multi-LLM use; AutoSurvey/GPT-4o referenced in context).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Likely uses distributed retrieval and agent-specialized extraction per agent (details in cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Cooperative multi-agent generation to assemble long-form outputs and manage long contexts (per cited description).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Intended to handle many references to produce long-form outputs (e.g., large token counts like 64k), exact counts depend on implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Survey and long-form scientific summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Long-form surveys / comprehensive documents.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified here; referenced to show capability to produce long outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Cited as enabling long-form content generation (e.g., 64k tokens) in prior work; no numeric comparison here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared conceptually to single-LLM generation limited by context window.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Described as enabling longer outputs than single-model prompts constrained by context limits.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Multi-LLM agent architectures can mitigate single-model context-window limitations to produce longer, structured content.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Coordination overhead across agents, potential consistency/factuality challenges not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Intended to scale to long outputs (tens of thousands of tokens) by decomposing tasks across agents; concrete scaling trends are described in the cited work rather than in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InteractiveSurvey: An LLM-based Personalized and Interactive Survey Paper Generation System', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Litllm: A toolkit for scientific literature review. <em>(Rating: 2)</em></li>
                <li>HiReview: Hierarchical Taxonomy-Driven Automatic Literature Review Generation. <em>(Rating: 2)</em></li>
                <li>AutoSurvey: Large Language Models Can Automatically Write Surveys. <em>(Rating: 2)</em></li>
                <li>Surveyx: Academic survey automation via large language models. <em>(Rating: 2)</em></li>
                <li>PROMPTHEUS: A Human-Centered Pipeline to Streamline SLRs with LLMs. <em>(Rating: 2)</em></li>
                <li>ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary. <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for large language models: A survey. <em>(Rating: 2)</em></li>
                <li>Precise zero-shot dense retrieval without relevance labels. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4426",
    "paper_id": "paper-277781402",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "InteractiveSurvey",
            "name_full": "InteractiveSurvey: An LLM-based Personalized and Interactive Survey Paper Generation System",
            "brief_description": "A web-based system that uses LLMs + retrieval to automatically search, parse, categorize, and synthesize multiple reference papers into structured, multimodal survey papers while allowing users to iteratively refine intermediate outputs (reference categories, outline, content).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "InteractiveSurvey",
            "system_description": "InteractiveSurvey is an end-to-end web system that: (1) constructs arXiv search queries using an LLM-generated topic description; (2) retrieves and/or ingests user-uploaded PDFs; (3) parses documents into structured Markdown (via MinerU) and stores chunked embeddings in a vector database for RAG; (4) performs personalized reference categorization using HyDE-style pseudo-query generation + semantic retrieval and clustering (UMAP dim. reduction + HDBSCAN + silhouette-based cluster-number selection); (5) generates a three-level hierarchical outline (predefined section titles, categorization names as section titles, LLM-subsection titles) and allows iterative manual edits; (6) synthesizes survey content bottom-up: retrieve chunks per subsection, LLM-generate subsection content, LLM-summarize and merge, and produce pre-defined sections (Abstract, Intro, Conclusion) from generated content; (7) integrates multimodal elements by semantically matching figure/table captions from references and adding adaptive citation links using an adaptive cosine-similarity threshold.",
            "llm_model_used": "qwen2.5-72b-instruct (used as backbone in experiments); supports replacing backbone with any LLM API (authors compared against GPT-4o, DeepSeek-R1, Qwen2.5-72b in evaluations).",
            "extraction_technique": "Document parsing to Markdown (MinerU); chunking into fixed-length chunks; embedding-based retrieval (vector DB) for RAG; HyDE-style pseudo-document (pseudo-query) generation to improve retrieval for categorization; semantic matching between generated sentences and figure/table captions.",
            "synthesis_technique": "Bottom-up hierarchical summarization: section-by-section retrieval-augmented generation, LLM summarization of subsections, LLM merging (LLMMerge) into section content; hierarchical outline-guided generation (three-level outline); multimodal integration and adaptive citation assignment across sentences.",
            "number_of_papers": "Typical pipelines operate on dozens to ~50 references (system evaluated with average 45.2 references per topic; system can truncate to MAX_REF).",
            "domain_or_topic": "General scientific literature across arXiv; evaluated on 40 topics from 8 fields (Computer Science, Mathematics, Physics, Statistics, Electrical Engineering and Systems Science, Quantitative Biology, Quantitative Finance, Economics).",
            "output_type": "Structured, multimodal survey papers (hierarchical outline, sections/subsections, figures/tables from references, inline citations), exportable to PDF/Markdown/LaTeX.",
            "evaluation_metrics": "LLM-based judging on Coverage, Structure, Relevance; time-efficiency (seconds/minutes to generate); System Usability Scale (SUS) for usability.",
            "performance_results": "Qualitative: authors report InteractiveSurvey outperforms three mainstream LLMs and two SOTA survey-generation methods on average across Coverage/Structure/Relevance (evaluated over 40 topics). Time: average 2,077.8 seconds (~35 minutes) to generate a survey with ≈50 references on the reported hardware. Usability: SUS score = 84.4/100 (A+ tier).",
            "comparison_baseline": "Compared against direct prompting with LLMs (GPT-4o, DeepSeek-R1, Qwen2.5-72b) and SOTA methods AutoSurvey and SurveyX (and sample outputs they released).",
            "performance_vs_baseline": "Reported to 'consistently outperform' mainstream LLMs and to achieve the highest average scores across evaluation metrics vs. AutoSurvey and SurveyX (exact numeric comparative scores not included in main text).",
            "key_findings": "Interactive, modifiable intermediate processes (reference categorization, outline, content editing) plus RAG and HyDE-style retrieval improve survey quality and personalization compared to title-only or fixed-output approaches; bottom-up hierarchical synthesis and multimodal integration yield coherent structured surveys efficiently.",
            "limitations_challenges": "Context-window limits of LLMs constrain use of long contexts; the most time-consuming steps are reference parsing and personalized categorization (GPU-bound); reliance on public arXiv (user uploads allowed for copyrighted material); potential factual/information limits implicit in LLMs are acknowledged (context retention limitations).",
            "scaling_behavior": "Authors note time scales with hardware capability (with better hardware they expect total generation time to fall below 30 minutes); clustering adapts to topic variability (no single optimal cluster number), and LLM performance is constrained by context window even with more input text.",
            "uuid": "e4426.0",
            "source_info": {
                "paper_title": "InteractiveSurvey: An LLM-based Personalized and Interactive Survey Paper Generation System",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation (RAG)",
            "brief_description": "A paradigm that augments LLM generation with retrieved document chunks from a vector store so the model can condition on external text beyond its context window.",
            "citation_title": "Retrieval-augmented generation for large language models: A survey.",
            "mention_or_use": "use",
            "system_name": "Retrieval-Augmented Generation (RAG)",
            "system_description": "RAG stores semantic embeddings of document chunks in a vector database; at generation time it retrieves relevant chunks for prompts and conditions the LLM on that retrieved context to produce answers/summaries. In InteractiveSurvey, RAG is used both for content generation (retrieve reference chunks per subsection) and for per-sentence citation sourcing.",
            "llm_model_used": "Used in combination with qwen2.5-72b-instruct in this system; RAG is model-agnostic.",
            "extraction_technique": "Embedding-based retrieval (chunk-level embeddings stored in vector DB); semantic matching via cosine similarity; HyDE pseudo-queries optionally used to improve retrieval.",
            "synthesis_technique": "LLM generation conditioned on retrieved chunks; bottom-up assembly of subsections by summarizing retrieved content and merging via LLM.",
            "number_of_papers": "Applied to tens of papers per topic in this system (authors evaluate on ~45 references per topic); RAG itself supports scaling to hundreds of documents depending on chunking and retrieval strategy.",
            "domain_or_topic": "Scientific literature across arXiv (general-purpose).",
            "output_type": "Survey text, subsection drafts, citations tied to source chunks.",
            "evaluation_metrics": "Not specified for RAG alone in paper; system-level metrics include coverage/structure/relevance and time.",
            "performance_results": "Reported to 'facilitate synthesizing survey papers from multiple references' and forms a core part of the generation pipeline; specific RAG-only metrics not provided.",
            "comparison_baseline": "Implicit baseline: direct LLM prompting without retrieval/context augmentation.",
            "performance_vs_baseline": "InteractiveSurvey (which uses RAG) outperforms direct prompting LLM baselines in coverage/structure/relevance per authors' LLM-judge evaluations.",
            "key_findings": "RAG enables incorporation of many external references that exceed model context windows and improves factual grounding and citation assignment when combined with adaptive similarity thresholds.",
            "limitations_challenges": "Quality depends on retrieval effectiveness (embedding model, query formulation); adaptive thresholding required to avoid over/under-citation; still limited by downstream LLM context-window when integrating many chunks.",
            "scaling_behavior": "RAG enables scaling to many documents by retrieving only relevant chunks, but end-to-end time and retrieval quality scale with number of references and hardware; authors observed parsing and categorization as dominant time costs.",
            "uuid": "e4426.1",
            "source_info": {
                "paper_title": "InteractiveSurvey: An LLM-based Personalized and Interactive Survey Paper Generation System",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "HyDE",
            "name_full": "Hypothetical Document Embeddings (HyDE)",
            "brief_description": "A retrieval technique where an LLM generates hypothetical/pseudo-documents or pseudo-queries that are then embedded for semantic retrieval to improve zero-shot retrieval accuracy.",
            "citation_title": "Precise zero-shot dense retrieval without relevance labels.",
            "mention_or_use": "use",
            "system_name": "HyDE-based Context Retrieval",
            "system_description": "InteractiveSurvey prompts an LLM to generate multiple (10) hypothetical queries/descriptions for a given categorization criterion k; these pseudo-documents are embedded and used in parallel to retrieve the most relevant chunks from each reference collection to improve semantic matching for categorization and downstream clustering.",
            "llm_model_used": "An LLM (not uniquely specified; the system uses its backbone LLM for pseudo-document generation; experiments used qwen2.5-72b-instruct).",
            "extraction_technique": "Pseudo-document generation by LLM (HyDE) + embedding-based retrieval across chunk collections.",
            "synthesis_technique": "Aggregated retrieved chunks (from multiple HyDE queries) are used as the basis for clustering and for feeding LLMs to generate cluster descriptions and names.",
            "number_of_papers": "Applied per-topic across the set of reference papers gathered (≈45 per topic in evaluation).",
            "domain_or_topic": "Applied to reference categorization across general scientific topics from arXiv.",
            "output_type": "Improved retrieval contexts for semantic clustering and categorization, leading to section/subsection content.",
            "evaluation_metrics": "Not reported as separate metrics; used internally to improve categorization quality judged in overall system evaluation.",
            "performance_results": "Authors state HyDE-based retrieval 'facilitates accurate retrieval' for categorization; no standalone numeric results reported.",
            "comparison_baseline": "Implicit comparison to direct retrieval using the raw criterion as query (no HyDE pseudo-docs).",
            "performance_vs_baseline": "Claimed improvement in retrieval accuracy for categorization (qualitative, no numeric values reported).",
            "key_findings": "Using multiple LLM-generated pseudo-queries in parallel improves semantic matching for categorization, enabling better thematic clustering from reference content.",
            "limitations_challenges": "Adds LLM generation overhead; retrieval effectiveness depends on quality/diversity of HyDE prompts and embedding model.",
            "scaling_behavior": "Costs scale with number of pseudo-queries and number of reference collections; intended to improve zero-shot retrieval without labels.",
            "uuid": "e4426.2",
            "source_info": {
                "paper_title": "InteractiveSurvey: An LLM-based Personalized and Interactive Survey Paper Generation System",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "MinerU",
            "name_full": "MinerU (document parsing tool)",
            "brief_description": "An open-source tool that extracts and parses PDF reference papers into structured Markdown files to enable downstream processing.",
            "citation_title": "Mineru: An open-source solution for precise document content extraction.",
            "mention_or_use": "use",
            "system_name": "MinerU document parser",
            "system_description": "MinerU ingests PDF papers and extracts structured elements (title, authors, abstract, introduction etc.) and outputs Markdown files used by InteractiveSurvey to display metadata and to chunk content for embedding and RAG.",
            "llm_model_used": "None (document parsing tool); used upstream of LLM-based components.",
            "extraction_technique": "PDF parsing and structured content extraction into Markdown; subsequent chunking for embedding.",
            "synthesis_technique": "Not applicable (preprocessing/parsing stage).",
            "number_of_papers": "Used to parse all reference papers collected for a topic (≈45 per topic in evaluation).",
            "domain_or_topic": "Scientific papers (arXiv).",
            "output_type": "Structured Markdown files and chunked text for embedding and retrieval.",
            "evaluation_metrics": "Not reported in this paper (tool cited as established).",
            "performance_results": "Used successfully in pipeline; parsing identified as one of the most time-consuming steps in the end-to-end pipeline.",
            "comparison_baseline": "Not compared in this work.",
            "performance_vs_baseline": "Not reported.",
            "key_findings": "Accurate parsing into structured markdown simplifies metadata display and enables downstream embedding/RAG workflows; parsing runtime contributes substantially to end-to-end time.",
            "limitations_challenges": "Parsing large numbers of PDFs is computationally heavy and noted as a time bottleneck in the system.",
            "scaling_behavior": "Parsing time increases with number and length of PDFs; authors note GPU-bound parsing and expect gains with stronger hardware.",
            "uuid": "e4426.3",
            "source_info": {
                "paper_title": "InteractiveSurvey: An LLM-based Personalized and Interactive Survey Paper Generation System",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "LitLLM",
            "name_full": "LitLLM: A toolkit for scientific literature review",
            "brief_description": "A toolkit that retrieves relevant papers and generates survey content (from user-provided abstracts) using LLMs as part of automated literature-review pipelines.",
            "citation_title": "Litllm: A toolkit for scientific literature review.",
            "mention_or_use": "mention",
            "system_name": "LitLLM",
            "system_description": "Cited as an automated survey system that can retrieve relevant papers and generate survey content from user-provided abstracts; presented as related work in automated survey generation leveraging RAG and document parsing.",
            "llm_model_used": "Not specified in this paper (referenced work).",
            "extraction_technique": "Retrieval from abstracts (per authors' description) and LLM-based summarization (referenced work).",
            "synthesis_technique": "LLM-based generation of survey content from retrieved/provided abstracts (per related-work summary).",
            "number_of_papers": "Not specified here (depends on LitLLM implementation).",
            "domain_or_topic": "Scientific literature review (general).",
            "output_type": "Survey content / literature review text.",
            "evaluation_metrics": "Not detailed in this paper (cited as related work).",
            "performance_results": "Not reported in this paper (referenced as existing toolkit).",
            "comparison_baseline": "Not applicable in this paper.",
            "performance_vs_baseline": "Not reported here.",
            "key_findings": "Represents a class of toolkits that combine retrieval and LLM generation for literature review automation; cited to situate InteractiveSurvey among automated systems.",
            "limitations_challenges": "Cited limitations of existing systems in general: many restrict user inputs to title-only and offer fixed outputs without interactive intermediate refinement.",
            "scaling_behavior": "Not discussed here.",
            "uuid": "e4426.4",
            "source_info": {
                "paper_title": "InteractiveSurvey: An LLM-based Personalized and Interactive Survey Paper Generation System",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "HiReview",
            "name_full": "HiReview: Hierarchical Taxonomy-Driven Automatic Literature Review Generation",
            "brief_description": "An automated literature-review system that employs hierarchical clustering on citation graphs to construct taxonomy trees for survey generation.",
            "citation_title": "HiReview: Hierarchical Taxonomy-Driven Automatic Literature Review Generation.",
            "mention_or_use": "mention",
            "system_name": "HiReview",
            "system_description": "HiReview constructs taxonomy trees for literature reviews by performing hierarchical clustering on citation graphs, then uses that taxonomy to drive automatic literature review generation (cited as related work).",
            "llm_model_used": "Not specified in this paper (referenced work).",
            "extraction_technique": "Citation-graph-based clustering and taxonomy construction.",
            "synthesis_technique": "Taxonomy-driven generation of review content (details in cited work).",
            "number_of_papers": "Not specified here.",
            "domain_or_topic": "Automated literature review generation (general / scientific literature).",
            "output_type": "Taxonomy trees and generated literature-review text.",
            "evaluation_metrics": "Not detailed here.",
            "performance_results": "Not reported in this paper (cited as related work).",
            "comparison_baseline": "Not applicable in this paper.",
            "performance_vs_baseline": "Not reported here.",
            "key_findings": "Demonstrates use of citation-graph structure to organize literature into hierarchical taxonomies for survey generation; cited as an alternative to thematic clustering approaches.",
            "limitations_challenges": "Not detailed in this paper.",
            "scaling_behavior": "Not discussed here.",
            "uuid": "e4426.5",
            "source_info": {
                "paper_title": "InteractiveSurvey: An LLM-based Personalized and Interactive Survey Paper Generation System",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "AutoSurvey",
            "name_full": "AutoSurvey: Large Language Models Can Automatically Write Surveys",
            "brief_description": "A prior automated survey generation system that the authors use as a SOTA comparison; asserted to produce survey samples (cited) and to employ LLMs to generate long-form survey content.",
            "citation_title": "AutoSurvey: Large Language Models Can Automatically Write Surveys.",
            "mention_or_use": "mention",
            "system_name": "AutoSurvey",
            "system_description": "Cited as a state-of-the-art survey generation method that can automatically write surveys with LLMs; authors compared InteractiveSurvey outputs against the survey samples released by AutoSurvey.",
            "llm_model_used": "Reported in the referenced AutoSurvey work (not detailed in this paper).",
            "extraction_technique": "Not specified here (referenced work).",
            "synthesis_technique": "LLM-based automatic survey generation (details in cited work).",
            "number_of_papers": "Not specified here; AutoSurvey samples used for comparison.",
            "domain_or_topic": "Survey generation for academic literature (general).",
            "output_type": "Survey papers (long-form).",
            "evaluation_metrics": "Compared at system level by LLM judges on coverage/structure/relevance in this paper.",
            "performance_results": "InteractiveSurvey achieves higher average scores than AutoSurvey on the evaluation set (exact numeric values not provided in text).",
            "comparison_baseline": "AutoSurvey's released samples served as a baseline for comparison.",
            "performance_vs_baseline": "InteractiveSurvey reported to outperform AutoSurvey on average across the authors' LLM-judge metrics.",
            "key_findings": "InteractiveSurvey's interactive/modifiable pipeline yields superior coverage/structure/relevance relative to AutoSurvey samples per authors' evaluations.",
            "limitations_challenges": "AutoSurvey implementation is under continuous iteration (per authors), and only released samples were available for comparison.",
            "scaling_behavior": "Not discussed here.",
            "uuid": "e4426.6",
            "source_info": {
                "paper_title": "InteractiveSurvey: An LLM-based Personalized and Interactive Survey Paper Generation System",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "SurveyX",
            "name_full": "SurveyX: Academic survey automation via large language models",
            "brief_description": "A recent SOTA automated survey-generation method cited and used as a comparison point by the authors.",
            "citation_title": "Surveyx: Academic survey automation via large language models.",
            "mention_or_use": "mention",
            "system_name": "SurveyX",
            "system_description": "Cited as a state-of-the-art method for academic survey automation using LLMs; authors compared InteractiveSurvey outputs against SurveyX's released survey samples.",
            "llm_model_used": "SurveyX samples were generated by GPT-4o per the referenced description (cited in this paper).",
            "extraction_technique": "Not detailed in this paper (referenced work).",
            "synthesis_technique": "LLM-based survey automation (details in cited work).",
            "number_of_papers": "Not specified here.",
            "domain_or_topic": "Academic survey generation (general).",
            "output_type": "Survey papers (long-form).",
            "evaluation_metrics": "Compared by LLM judges on coverage/structure/relevance in this paper.",
            "performance_results": "InteractiveSurvey achieves higher average scores than SurveyX samples on the evaluation set (exact numeric values not provided in text).",
            "comparison_baseline": "SurveyX released samples served as a baseline.",
            "performance_vs_baseline": "Reported improvement by InteractiveSurvey over SurveyX on average across metrics.",
            "key_findings": "InteractiveSurvey's user-interactive pipeline and RAG-based retrieval provide advantages over the released SurveyX samples per evaluations.",
            "limitations_challenges": "SurveyX implementation was not available to run directly; comparison used released samples only.",
            "scaling_behavior": "Not discussed here.",
            "uuid": "e4426.7",
            "source_info": {
                "paper_title": "InteractiveSurvey: An LLM-based Personalized and Interactive Survey Paper Generation System",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "PROMPTHEUS",
            "name_full": "PROMPTHEUS: A Human-Centered Pipeline to Streamline SLRs with LLMs",
            "brief_description": "A human-centered pipeline for systematic literature reviews that incorporates clustering-based topic modeling to promote structural coherence and factual accuracy.",
            "citation_title": "PROMPTHEUS: A Human-Centered Pipeline to Streamline SLRs with LLMs.",
            "mention_or_use": "mention",
            "system_name": "PROMPTHEUS",
            "system_description": "Cited as a human-centered pipeline that uses clustering-based topic modeling as part of LLM-driven SLR pipelines to ensure structure and factual accuracy; presented in related work as an advanced strategy for high-quality survey generation.",
            "llm_model_used": "Not specified in this paper (referenced work).",
            "extraction_technique": "Clustering-based topic modeling + LLM summarization (per related-work description).",
            "synthesis_technique": "Structure-driven LLM synthesis guided by topic clusters to maintain coherence and factual grounding.",
            "number_of_papers": "Not specified here.",
            "domain_or_topic": "Systematic literature reviews (SLRs).",
            "output_type": "Structured SLR outputs (surveys, reviews).",
            "evaluation_metrics": "Not detailed in this paper.",
            "performance_results": "Not provided in this paper (cited as related work).",
            "comparison_baseline": "Not applicable in this paper.",
            "performance_vs_baseline": "Not reported here.",
            "key_findings": "Clustering-based topic modeling combined with human-centered pipeline design supports structural coherence and factual accuracy in LLM-enabled SLRs (as characterized by authors).",
            "limitations_challenges": "Not specified here.",
            "scaling_behavior": "Not discussed here.",
            "uuid": "e4426.8",
            "source_info": {
                "paper_title": "InteractiveSurvey: An LLM-based Personalized and Interactive Survey Paper Generation System",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "ChatCite",
            "name_full": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary",
            "brief_description": "An LLM-agent approach employing prompt engineering and human workflow guidance to generate comparative literature summaries.",
            "citation_title": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary.",
            "mention_or_use": "mention",
            "system_name": "ChatCite",
            "system_description": "Cited as a method that uses prompt engineering and LLM agents to produce comparative literature summaries with human workflow guidance; referenced in the context of techniques to generate structured summaries covering multiple papers.",
            "llm_model_used": "Not specified in this paper (referenced work).",
            "extraction_technique": "Prompt-engineering-driven retrieval/QA style summarization (per related-work description).",
            "synthesis_technique": "Agent-guided comparative summarization across multiple papers.",
            "number_of_papers": "Not specified here.",
            "domain_or_topic": "Comparative literature summaries (academic papers).",
            "output_type": "Comparative literature summaries.",
            "evaluation_metrics": "Not detailed in this paper.",
            "performance_results": "Not reported in this paper (cited as related work).",
            "comparison_baseline": "Not applicable in this paper.",
            "performance_vs_baseline": "Not reported.",
            "key_findings": "Prompt engineering and agent workflows can help structure comparative literature summaries across papers (as characterized by authors).",
            "limitations_challenges": "Not specified here.",
            "scaling_behavior": "Not discussed here.",
            "uuid": "e4426.9",
            "source_info": {
                "paper_title": "InteractiveSurvey: An LLM-based Personalized and Interactive Survey Paper Generation System",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "multi-LLM agents (Wang et al., 2024b)",
            "name_full": "Multi-LLM agent architectures for long-form content creation",
            "brief_description": "An approach that composes multiple LLM agents (specialized or cooperating instances) to enable creation of long-form content beyond single-model context limits.",
            "citation_title": "AutoSurvey: Large Language Models Can Automatically Write Surveys.",
            "mention_or_use": "mention",
            "system_name": "Multi-LLM agent architectures",
            "system_description": "Cited (Wang et al., 2024b) as an approach using multiple LLM agents in a coordinated architecture to produce long-form content (e.g., 64k-token outputs), enabling scaling beyond single-model context windows.",
            "llm_model_used": "Not specified here (cited work describes multi-LLM use; AutoSurvey/GPT-4o referenced in context).",
            "extraction_technique": "Likely uses distributed retrieval and agent-specialized extraction per agent (details in cited work).",
            "synthesis_technique": "Cooperative multi-agent generation to assemble long-form outputs and manage long contexts (per cited description).",
            "number_of_papers": "Intended to handle many references to produce long-form outputs (e.g., large token counts like 64k), exact counts depend on implementation.",
            "domain_or_topic": "Survey and long-form scientific summarization.",
            "output_type": "Long-form surveys / comprehensive documents.",
            "evaluation_metrics": "Not specified here; referenced to show capability to produce long outputs.",
            "performance_results": "Cited as enabling long-form content generation (e.g., 64k tokens) in prior work; no numeric comparison here.",
            "comparison_baseline": "Compared conceptually to single-LLM generation limited by context window.",
            "performance_vs_baseline": "Described as enabling longer outputs than single-model prompts constrained by context limits.",
            "key_findings": "Multi-LLM agent architectures can mitigate single-model context-window limitations to produce longer, structured content.",
            "limitations_challenges": "Coordination overhead across agents, potential consistency/factuality challenges not detailed here.",
            "scaling_behavior": "Intended to scale to long outputs (tens of thousands of tokens) by decomposing tasks across agents; concrete scaling trends are described in the cited work rather than in this paper.",
            "uuid": "e4426.10",
            "source_info": {
                "paper_title": "InteractiveSurvey: An LLM-based Personalized and Interactive Survey Paper Generation System",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Litllm: A toolkit for scientific literature review.",
            "rating": 2,
            "sanitized_title": "litllm_a_toolkit_for_scientific_literature_review"
        },
        {
            "paper_title": "HiReview: Hierarchical Taxonomy-Driven Automatic Literature Review Generation.",
            "rating": 2,
            "sanitized_title": "hireview_hierarchical_taxonomydriven_automatic_literature_review_generation"
        },
        {
            "paper_title": "AutoSurvey: Large Language Models Can Automatically Write Surveys.",
            "rating": 2,
            "sanitized_title": "autosurvey_large_language_models_can_automatically_write_surveys"
        },
        {
            "paper_title": "Surveyx: Academic survey automation via large language models.",
            "rating": 2,
            "sanitized_title": "surveyx_academic_survey_automation_via_large_language_models"
        },
        {
            "paper_title": "PROMPTHEUS: A Human-Centered Pipeline to Streamline SLRs with LLMs.",
            "rating": 2,
            "sanitized_title": "promptheus_a_humancentered_pipeline_to_streamline_slrs_with_llms"
        },
        {
            "paper_title": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary.",
            "rating": 2,
            "sanitized_title": "chatcite_llm_agent_with_human_workflow_guidance_for_comparative_literature_summary"
        },
        {
            "paper_title": "Retrieval-augmented generation for large language models: A survey.",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_large_language_models_a_survey"
        },
        {
            "paper_title": "Precise zero-shot dense retrieval without relevance labels.",
            "rating": 2,
            "sanitized_title": "precise_zeroshot_dense_retrieval_without_relevance_labels"
        }
    ],
    "cost": 0.01974025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>InteractiveSurvey: An LLM-based Personalized and Interactive Survey Paper Generation System
31 Mar 2025</p>
<p>Zhiyuan Wen zyuanwen@polyu.edu.hk 
The Hong Kong Polytechnic University
Kowloon, Hong KongChina</p>
<p>Jiannong Cao jiannong.cao@polyu.edu.hk 
The Hong Kong Polytechnic University
Kowloon, Hong KongChina</p>
<p>Zian Wang 
The Hong Kong Polytechnic University
Kowloon, Hong KongChina</p>
<p>Beichen Guo beichen.guo@connect.polyu.hk 
The Hong Kong Polytechnic University
Kowloon, Hong KongChina</p>
<p>Ruosong Yang rsong.yang@polyu.edu.hk 
The Hong Kong Polytechnic University
Kowloon, Hong KongChina</p>
<p>Shuaiqi Liu liushuaiqi@huawei.com 
Huawei Technologies Co
Ltd, ShenzhenChina</p>
<p>InteractiveSurvey: An LLM-based Personalized and Interactive Survey Paper Generation System
31 Mar 2025B59AE1C990813FB665808105FFA14136arXiv:2504.08762v1[cs.IR]
The exponential growth of academic literature creates urgent demands for comprehensive survey papers, yet manual writing remains timeconsuming and labor-intensive.Recent advances in large language models (LLMs) and retrieval-augmented generation (RAG) facilitate studies in synthesizing survey papers from multiple references, but most existing works restrict users to title-only inputs and fixed outputs, neglecting the personalized process of survey paper writing.In this paper, we introduce Inter-activeSurvey -an LLM-based personalized and interactive survey paper generation system.In-teractiveSurvey can generate structured, multimodal survey papers with reference categorizations from multiple reference papers through both online retrieval and user uploads.More importantly, users can customize and refine intermediate components continuously during generation, including reference categorization, outline, and survey content through an intuitive interface.Evaluations of content quality, time efficiency, and user studies show that Interac-tiveSurvey is an easy-to-use survey generation system that outperforms most LLMs and existing methods in output content quality while remaining highly time-efficient 1 .</p>
<p>Introduction</p>
<p>Survey papers are essential for synthesizing the current state and trends of a research area.While research papers have grown rapidly over the past decade, survey papers remain relatively scarce (Figure 1).This gap makes it challenging to keep up with developments in a field (Wang et al., 2024b), especially for new researchers.Consequently, there is an urgent demand to generate high-quality survey papers efficiently.</p>
<p>Generating a comprehensive survey typically involves summarizing dozens to hundreds of references, with each averaging around 10K tokens, far 1 Our demo video is at: here exceeding the input capacity of most mainstream LLMs like GPT-4o (Hurst et al., 2024).Recent advances in retrieval-augmented generation (RAG, Gao et al. (2023c)) facilitate synthesizing survey papers from multiple references (Wang et al., 2024b;Liang et al., 2025;Torres et al., 2024;Agarwal et al., 2024).However, most existing approaches/systems restrict users to title-only inputs and fixed outputs, failing to involve them in the intermediate stages of survey writing, such as selecting and categorizing references or modifying survey paper outlines.Consequently, if users are dissatisfied with certain sub-parts, they have to either regenerate the entire survey paper or are even unable to adjust the content.This significantly prevents the usability and efficiency of survey paper generation.</p>
<p>In this paper, we introduce InteractiveSurvey, an LLM-based interactive web system that can efficiently generate personalized and comprehensive survey papers for researchers.InteractiveSurvey has the following functions and features: (1) Automatic Reference Searching: our system automatically searches and downloads reference papers from arXiv that are relevant to the survey topic specified by the user.(2) Personalized Reference Cate-gorization: our system facilitates categorization of reference papers based on user-defined criteria (e.g., Research Method) for personalized content organization.(3) Structured and Multi-modal Output: our system generates well-organized survey papers with multiple sections and subsections.The output survey paper includes an outline diagram, as well as tables and figures from the reference papers.(4) Modifiable Intermediate Processes: users can iteratively refine most steps in survey generation, including uploading local references, adjusting reference categorization results, modifying outline, and editing text content and visual elements (e.g., images, tables) in the generated survey paper.(5) Intuitive User Interface: our system provides stepby-step guidance, clear metadata displays for references, categorization visualizations, and other user-friendly interfaces.</p>
<p>We comprehensively evaluate InteractiveSurvey in content quality of generated surveys, time efficiency, and usability.Content Quality: Evaluated by LLMs in coverage, structure, and relevance, InteractiveSurvey outperforms three mainstream LLMs in generated survey papers over 40 topics from 8 different research fields.Besides, it also outperforms state-of-the-art (SOTA) survey generation systems when generating survey papers on topics same to their released samples, without any refinement from users.Time Efficiency: Interac-tiveSurvey can produce a high-quality survey paper from scratch (with approximately 50 references) in just 35 minutes on average, using a single RTX 3090 GPU and an LLM API.Usability: Based on the System Usability Scale (SUS, Brooke et al. (1996)), feedback from 34 researchers ranks our system in the highest tier with a score of 84.4/100, validating our user-friendly design.</p>
<p>Our contributions can be summarized as follows: (1): InteractiveSurvey is an LLM-based web system that can efficiently generate high-quality survey papers for researchers.The generated survey papers outperform mainstream LLMs and SOTA survey generation systems in coverage, structure, and relevance.(2): As far as we know, Interac-tiveSurvey is the first interactive survey generation system with modifiable intermediate processes, enabling researchers to create personalized survey papers through an intuitive UI. (3): InteractiveSurvey is open-sourced 2 and easy to deploy (both by direct deployment and by Docker).The backbone 2 github.com/TechnicolorGUO/InteractiveSurveyLLM can be replaced by any LLM via API key configuration.</p>
<p>2 Related Work</p>
<p>Literature Review Generation</p>
<p>Automating the summarization of multiple research papers has been of longstanding interest.Early approaches primarily leverage multi-document summarization (MDS) techniques to generate unstructured summaries, e.g. the related work section of a research paper (Hoang and Kan, 2010).(Hu and Wan, 2014) attempted to generate a related work section for a target paper given multiple reference papers as input.(Erera et al., 2019) built the IBM Science Summarizer, which retrieves and summarizes scientific articles in computer science.</p>
<p>The advancements in LLMs have significantly enhanced the scope and quality of automated literature reviews.(LIU et al., 2022) proposed the category-based alignment and sparse transformer to generate structured summaries covering multiple research papers.ChatCite (Li et al., 2024b) utilized prompt engineering to generate comparative literature summaries, Susnjak et al. (2024) finetuned domain-specific LLMs to produce literature reviews enriched with contemporary knowledge.</p>
<p>Despite these advancements, most existing studies primarily address technical challenges in literature review generation rather than producing comprehensive survey papers in practice.</p>
<p>Automated Survey Systems</p>
<p>Unlike technical challenge-focused approaches, automated survey systems provide end-to-end pipelines for generating structured survey papers.Advances in RAG and document parsing techniques significantly support the implementation of these systems.LitLLM (Agarwal et al., 2024) can retrieve relevant papers and generate survey content from user-provided abstracts.HiReview (Hu et al., 2024) employs hierarchical clustering on citation graphs to construct taxonomy trees for survey generation.</p>
<p>Recent research has adopted more advanced strategies to produce high-quality survey papers.Wang et al. (2024b) utilizes multi-LLM agent architectures to enable the creation of long-form content (e.g., 64k tokens).PROMPTTHEUS (Torres et al., 2024) incorporates clustering-based topic modeling to ensure structural coherence and factual accuracy.To ensure formatting consistency and adherence to However, most systems typically limit users to title-only inputs and fixed outputs, neglecting interactive modification and refinement during generation.Consequently, users may face an all-ornothing dilemma: either tolerate suboptimal content or restart the entire generation process.</p>
<p>InteractiveSurvey</p>
<p>Automatic Reference Searching</p>
<p>Online Searching When the user inputs a topic T , our system will automatically search and download relevant references from arxiv by constructing a search query Q T tailored for the arXiv API based on T .Specifically, we first employ the LLM to generate a description Des T for T .Then, we prompt the LLM to extract the themes T T , entities E T , and concepts C T from Des T inspired by (Guo et al., 2024).These components are then combined to form Q T .An example is shown in Table 1.</p>
<p>Upon obtaining Q T , we retrieve candidate references Ref T from arXiv.If the number of retrieved references falls below a predefined threshold MIN_REF, we iteratively relax the search constraints by adding other related E T and C T to refor- Themes: (abs:"LLM" AND abs:"recommendation") AND Entities: (abs:"language model" OR abs:"recommendation system" OR abs:"contextual embedding" OR abs:"semantic matching" AND Concepts: (abs:"personalization" OR abs:"content understanding" OR abs:"collaborative filtering" OR abs:"matrix factorization" mulate Q T , and repeat the search process.Finally, we truncate the results to retain at most MAX_REF references for subsequent processing.The complete procedure is shown in Appendix A.1.User Uploading To avoid copyright issues, we only search public arXiv papers for references.However, to accommodate users wishing to summarize local copyrighted materials, InteractiveSurvey also supports uploading such papers.</p>
<p>Reference Parsing</p>
<p>After collecting all reference papers from both online searching and user uploading, we parse the documents using MinerU (Wang et al., 2024a), an open-source tool that efficiently extracts and parses references (typically in PDF format) into structured Markdown files (.md).Then, our system extracts the metadata for each reference paper from the corresponding Markdown file, including title, authors, abstract, and the introduction section.The metadata is displayed on the front-end interface for user review, as shown in Figure 2. Besides, the content of reference papers is stored in a vector database V to facilitate subsequent processing following the procedure of RAG.Specifically, for each reference paper r i ∈ Ref T , we split it into fixed-length chunks {d 1 i , d 2 i , . . ., d m i }, obtain their semantic embeddings, and store them as a collection c i in V .</p>
<p>Personalized Reference Categorization</p>
<p>Reference categorization is essential for highquality survey papers to efficiently organize the landscape of a research area (Hu et al., 2024;Luo et al., 2025).Common categorization approaches include chronological ordering, technical taxonomy, and thematic clustering.Here, we follow the thematic clustering by letting users specify a categorization criterion k and then retrieving relevant content from references for semantic clustering.</p>
<p>Categorization Context Retrieval HyDE-based Context Retrieval</p>
<p>To facilitate accurate retrieval, we employ Hypothetical Document Embeddings (HyDE, Gao et al. (2023a)), which generates potential pseudo-descriptions to the categorization criterion k as queries for semantic matching.Specifically, we prompt the LLM to generate 10 different HyDE queries for k.For each collection c i corresponding to r i , all generated queries are used in parallel to retrieve the most relevant chunks</p>
<p>Survey Outline Generation</p>
<p>To facilitate the generation of high-quality survey content, we construct a three-level hierarchical outline O (Figure 4) including common section titles (e.g., Abstract, Introduction, Conclusion) in most survey papers, the categorization names N as section titles, and the LLM-generated sub-section titles from reference descriptions obtained in Section 3.3.The generated outline also allows for iterative refinement through manual edits, as shown in Step 5 in Figure 2.</p>
<p>To ensure the LLM-generated outline follows the hierarchical structure format:
O = {(l 1 , t 1 ), (l 2 , t 2 ), . . . , (l m , t m )}
where l i is the hierarchical level, and t i is the section/sub-section title, we first provide the LLM with the outline template as well as partial content of t i in the prompt, and then ask it to fill in the blanks.We found that this method achieves significantly better format following than direct end-toend generation, which can be useful when deploying InteractiveSurvey with less powerful LLMs.</p>
<p>Survey Content Generation</p>
<p>Based on the outline above, InteractiveSurvey generates structured and multi-modal survey content, including text content, images/figures, and citations.The generated content is fully editable and exportable in multiple formats, including PDF, Markdown, and LaTeX, as shown in Figure 2. Text Content Generation We adopt a bottom-up approach to generate text content section by section.For sections titled with categorization names, we first use their respective sub-section titles as queries to retrieve reference content from the vector database V.This retrieved content serves as the prompt to the LLM to generate the sub-section content.Subsequently, we employ the LLM to produce a summary of the sub-section content.The summary and the sub-section content together form the section content.For pre-defined title sections such as Abstract, Introduction, Future Directions, and Conclusion, we use the already-generated section content as the input and prompt the LLM to generate their section content with different instructions.The detailed process is shown in Appendix A.2.</p>
<p>Figure and Table Generation</p>
<p>To enable multimodal output, we incorporate two types of figures and tables into the generated content: (1) the image of the structure visualization 3 of the survey paper outline, and (2) figures/tables retrieved from reference papers.We conduct semantic matching between sentences in the generated survey paper and the captions of figures/tables in the references.Those with matching scores above the pre-defined threshold are incorporated into the generated survey paper, along with citation information.Citation Generation To facilitate researchers' reading experience, we also generate citations in the survey paper linking back to the reference papers.Inspired by the academic writing of human and existing studies (Gao et al., 2023b;Wang et al., 2024b), we retrieve relevant chunks from all reference papers for each sentence in the generated 3 By Graphviz at https://graphviz.org/ survey and apply an adaptive semantic similarity threshold to incorporate an appropriate number of citations.The details are shown in Appendix C.</p>
<p>Evaluation</p>
<p>Content Quality</p>
<p>Evaluation Metrics Following existing survey generation systems (Wang et al., 2024b;Liang et al., 2025), we employ LLMs as judges (Li et al., 2024a) to assess the survey papers in Coverage: the extent to which the survey encapsulates all aspects of the topic; Structure: the logical organization and coherence of each section; and Relevance: how well the content aligns with the user-input topic.The LLM prompt for evaluation is provided in Appendix E.1.Comparison with Different LLMs We compare InteractiveSurvey with various types of LLMs (i.e., GPT-4o (Hurst et al., 2024), DeepSeek-R1 (Guo et al., 2025), andQwen2.5-72b (Yang et al., 2024) on their survey generation abilities.The comparison setting is in Appendix E.2.As shown in Table 2, even if different LLMs were used as judges across the three evaluation aspects, our approach consistently outperformed mainstream LLMs in most cases, underscoring the high quality of our generated survey content.We also observed marginal differences in quality scores between using only the title versus the full abstract as input for LLMgenerated survey papers.This suggests that most LLMs are constrained by context window limitations to effectively retain and utilize information, even when more extensive context is provided (Chen et al., 2023).Comparison with SOTA Methods In addition to LLMs, we also compare InteractiveSurvey with two SOTA survey generation methods: AutoSurvey (Wang et al., 2024b) and SurveyX (Liang et al., 2025) on the generated survey paper samples.The comparison setting is in Appendix .As shown in Table 3, our method achieves the highest scores across all evaluation metrics on average, which also demonstrates that InteractiveSurvey outperform SOTA methods in survey content quality.</p>
<p>Time Efficiency</p>
<p>In addition to content quality, we evaluated the time efficiency of InteractiveSurvey.We use the 40topics reference collection described in Appendix E.2 (with an average of 45.2 references per topic) to generate 40 survey papers with default settings for user interactions.The experiments were conducted  API).Prompt means we directly prompt the LLMs to generate a survey paper on the given topic.Abstract means we input the abstracts of the search reference papers aligned with the given topic.on the following hardware/software configuration:</p>
<p>(1) GPU: NVIDIA GeForce RTX 3090 (for reference parsing and categorization), (2) CPU: AMD Ryzen 9 5900X 12-Core Processor (for web system deployment), (3) LLM API: qwen2.5-72b-instruct(for RAG support, outline generation, and survey content creation).The average time required to generate one survey paper is 2,077.8seconds ( 35 minutes).Figure 5 shows the detailed time distribution across different processing stages.</p>
<p>Our analysis shows that the most time-intensive steps are Reference Parsing and Personalized Reference Categorization, which may be constrained by our GPU's computational power.Although Personalized Reference Categorization also includes the time for LLM-based RAG, we anticipate that with more advanced hardware, the total processing time could be reduced to under 30 minutes.</p>
<p>User Study with SUS Test</p>
<p>We also conducted a user study involving 34 participants, including PhD students, research assistants, and postdoctoral fellows.They are asked to experience and assess InteractiveSurvey by the System Usability Scale (SUS), a widely adopted questionnaire for measuring system usability.Following (Sauro and Lewis, 2012), we converted the raw scores into a normalized 100-point scale.In-teractiveSurvey achieved an outstanding score of 84.4,placing it in the A+ tier (the highest grade) for usability.This result demonstrates that Inter-activeSurvey is really easy to use for researchers.The detailed scale and the calculation method are provided in Appendix D.</p>
<p>Conclusion</p>
<p>In this paper, we introduce InteractiveSurvey, a web-based system powered by large language models (LLMs) that efficiently generates high-quality survey papers.To the best of our knowledge, it is the first interactive platform that enables researchers to refine intermediate outputs via an intuitive UI, allowing for personalized survey creation.Future work will involve collecting user feedback post-deployment to enhance functionality, UI design, and evaluation.Additionally, we plan to explore multilingual survey generation to broaden the system's applicability.</p>
<p>Adaptive Clustering:</p>
<p>We perform hierarchical clustering on the dimensionally reduced representations U using the HDBSCAN algorithm (McInnes et al., 2017).Given the inherent variability in survey topics, reference papers, and input categorization criteria k, there is no universally optimal number of clusters.</p>
<p>To address this, we first specify several candidate cluster numbers (e.g., 3-6) and then the Silhouette score (Rousseeuw, 1987) is used to select the number with the highest score, indicating the most coherent clustering structure.</p>
<ol>
<li>Categorization Name Generation: After we obtain the clustering results, for each cluster C j , we aggregate the descriptions (generated in Section 3.3.1)for all reference papers within {d i | r i ∈ C j } and provide them to the LLM to generate a representative categorization name N j to captures the cluster's thematic focus relative to criteria k.Notably, the LLM here is configured to generate all cluster names for all L clusters simultaneously to ensure their coherence in expression.
N = LLM   L j=1 {d i | r i ∈ C j }  
where N = {N 1 , N 2 , . . ., N L } represents the set of generated cluster names.</li>
</ol>
<p>C Citation Generation</p>
<p>For each sentence s i ∈ {s 1 , s 2 , . . ., s n } in the generated survey paper, we calculate the cosine similarity sim i,j for semantic representations of each sentence s i to each chunk c j in the reference vector database V.A sentence-chunk pair s i , c j is assigned a citation if sim i,j ≥ τ , where τ is the semantic similarity threshold.As a constant threshold may result in localized overcitation or undercitation within specific passages, we adopt an adaptive approach that adjusts τ based on the global distribution of similarity scores:
τ = max τ static , µ + k σ ,
where τ static is the initialized static threshold, µ and σ denote the mean and standard deviation of similarity scores across all sim i,j pairs, and k is a hyper-parameter to adjust the strictness of the threshold.</p>
<p>D Details of the SUS Test</p>
<p>In our user study, 34 participants were asked to fill in the questionnaire with items in Table 4 to score each item with 1 (Strongly Disagree) to 5 (Strongly Agree).Then, we calculate the average scores for each item, and convert the raw scores into a normalized 100-point scale by:
Score = 4 * [(s 1 + s 3 + s 5 + s 7 + s 9 − 5)+ (25 − s 2 − s 4 − s 6 − s 8 − s 10 )]
where, s i is the average score of i-th item.Our score 84.4 falls into the A+ tier with the range (84.1-100) according to (Sauro and Lewis, 2012).The raw scores are released on our github repository.</p>
<p>Table 4: Items in the SUS (Brooke et al., 1996) 1 I think that I would like to use this system frequently.</p>
<p>2 I found the system unnecessarily complex.</p>
<p>3 I thought the system was easy to use.</p>
<p>4</p>
<p>I think that I would need the support of a technical person to be able to use this system.</p>
<p>6 I found the various functions in this system were well integrated.</p>
<p>6 I thought there was too much inconsistency in this system.</p>
<p>7</p>
<p>I would imagine that most people would learn to use this system very quickly.</p>
<p>8 I found the system very cumbersome to use. 9 I felt very confident using the system.</p>
<p>10 I needed to learn a lot of things before I could get going with this system.</p>
<p>E Experiment Settings for Content</p>
<p>Quality Evaluation</p>
<p>E.1 LLM Prompts for Evaluation</p>
<p>We evaluate each generated survey paper by employing LLMs to score it using the prompt in Table 5.</p>
<p>[TOPIC] represents the survey title provided by the user, while [SURVEY CONTENT] corresponds to the textual content of the generated survey paper.The remaining placeholders are filled in with the content quality criteria from (Wang et al., 2024b).Notably, some of the generated survey papers contain images, and all the LLMs used for evaluation support the uploading of PDF files.Therefore, we directly uploaded the PDFs of the generated survey papers instead of inserting the [SURVEY CONTENT] text during the actual evaluation process.</p>
<p>E.2 Settings of Comparison with Different LLMs</p>
<p>For comprehensive evaluation, we select 40 topics from 8 different research fields4 on arXiv and automatically search references through our system for survey paper generation.The topics, reference papers, and generated survey papers are also released on our github repository.Considering the input context windows limits of LLMs, we employ two approaches for them to generate survey papers:</p>
<p>(1) Prompt: We directly prompt the LLMs to generate a survey paper on a given topic without any additional input; and (2) Abstract: We input all the abstracts of the search reference papers aligned with the given topic as the prompt.</p>
<p>E.3 Settings of Comparison with SOTA Methods</p>
<p>As the source code of AutoSurvey is continuously iterated and SurveyX hasn't released its implementation, we compare survey papers generated by us with the survey samples they released5 .Specifically, we input the identical survey titles to their released survey papers into InteractiveSurvey and automatically search reference papers and generate survey papers.These papers and the survey samples they released were jointly evaluated and compared by LLM judges.Samples from Auto-Survey and SurveyX are generated by GPT-4o according to their description, survey generated by InteractiveSurvey is with Qwen2.5-72bAPI.</p>
<p>Figure 1 :
1
Figure 1: Comparison of the number of all research papers and survey papers released on arXiv.orgover the past 10 years (2015-2024).</p>
<p>Figure 2 :
2
Figure 2: An Overview of InteractiveSurvey.Steps 2,4,5, and 6 in user interactions are optional.</p>
<p>Figure 3 :
3
Figure 3: The HyDE Process for Retrieving Research Method Descriptions from References.</p>
<p>Figure 4 :
4
Figure 4: The pre-defined section names and the categorization names are as section titles, sub-section titles are generated by the LLM.</p>
<p>Figure 5 :
5
Figure 5: Time cost distribution in InteractiveSurvey</p>
<p>without any other information:</p>
<p>Table 1 :
1
The reference searching query for: A Survey of LLM in Recommendation Systems.It returns references whose Abstract contains the following themes, entities, and concepts.</p>
<p>Table 2 :
2
Comparison on survey content quality among different LLMs and InteractiveSurvey (with Qwen2.5-72B</p>
<p>Table 5 :
5
LLM prompts for evaluation</p>
<p>Computer Science, Mathematics, Physics, Statistics, Electrical Engineering and Systems Science, Quantitative Biology, Quantitative Finance, and Economics
5 AutoSurvey examples and SurveyX examples
s k ← RAG(t k , sub_contextj) 6: end for 7: sumi ← LLMSum(subsections s1, ..., sn) 8: si ← LLMMerge(ti, sumi, s1, ..., sn) 9: end for 10: for Pre-defined section titles (lj, tj) ∈ O do 11: //in parallel 12: si ← LLM(generated s1, ..., s k ) 13: end for 14: S ← {(l1, ti, s1), . . ., (lm, ti, sm)}B Details of Semantic ClusteringThe modularized clustering process comprises the following sequential steps:1. Semantic Embedding: {d k 1 , d k 2 , . . ., d k n } corresponding to each reference r i and criteria k are transformed into m-dimensional semantic representations V = {v 1 , v 2 , . . ., v n } using an embedding model(Nussbaum et al., 2024)ϕ : D → R m .U = UMAP(V) → R q , q ≪ m, where U = {u 1 , u 2 , . . ., u n } are the reduceddimensional representations, q is a hyperparameter indicating the target dimension.Dimensionality Reduction
Litllm: A toolkit for scientific literature review. Shubham Agarwal, Issam H Laradji, Laurent Charlin, Christopher Pal, 2024</p>
<p>Sus-a quick and dirty usability scale. Usability evaluation in industry. John Brooke, 1996189</p>
<p>Extending context window of large language models via positional interpolation. Shouyuan Chen, Sherman Wong, Liangjian Chen, Yuandong Tian, arXiv:2306.155952023arXiv preprint</p>
<p>Shai Erera, Michal Shmueli-Scheuer, Guy Feigenblat, Ora Peled Nakash, Odellia Boni, Haggai Roitman, Doron Cohen, arXiv:1908.11152A summarization system for scientific documents. Bar Weiner, Yosi Mass, Or RivlinarXiv preprintet al. 2019</p>
<p>Precise zero-shot dense retrieval without relevance labels. Luyu Gao, Xueguang Ma, Jimmy Lin, Jamie Callan, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics2023a1</p>
<p>Enabling large language models to generate text with citations. Tianyu Gao, Howard Yen, Jiatong Yu, Danqi Chen, arXiv:2305.146272023barXiv preprint</p>
<p>Retrieval-augmented generation for large language models: A survey. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Haofen Wang, Haofen Wang, arXiv:2312.109972023carXiv preprint</p>
<p>Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.12948Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. 2025arXiv preprint</p>
<p>LightRAG: Simple and Fast Retrieval-Augmented Generation. Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, Chao Huang, 10.48550/arXiv.2410.05779ArXiv:2410.057792024</p>
<p>Towards automated related work summarization. Cong Duy, Vu Hoang, Min-Yen Kan, Coling 2010: Posters. 2010</p>
<p>Automatic generation of related work sections in scientific papers: an optimization approach. Yue Hu, Xiaojun Wan, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)2014</p>
<p>HiReview: Hierarchical Taxonomy-Driven Automatic Literature Review Generation. Yuntong Hu, Zhuofeng Li, Zheng Zhang, Chen Ling, Raasikh Kanjiani, Boxin Zhao, Liang Zhao, 10.48550/arXiv.2410.03761ArXiv:2410.037612024</p>
<p>Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, Akila Ostrow, Alan Welihinda, Alec Hayes, Radford, arXiv:2410.21276Gpt-4o system card. 2024arXiv preprint</p>
<p>Haitao Li, Qian Dong, Junjie Chen, Huixue Su, Yujia Zhou, Qingyao Ai, Ziyi Ye, Yiqun Liu, arXiv:2412.05579Llms-as-judges: a comprehensive survey on llm-based evaluation methods. 2024aarXiv preprint</p>
<p>ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary. Yutong Li, Lu Chen, Aiwei Liu, Kai Yu, Lijie Wen, 10.48550/arXiv.2403.02574ArXiv:2403.025742024b</p>
<p>Xun Liang, Jiawei Yang, Yezhaohui Wang, Chen Tang, Zifan Zheng, Simin Niu, Shichao Song, Hanyu Wang, Bo Tang, Feiyu Xiong, arXiv:2502.14776Surveyx: Academic survey automation via large language models. 2025arXiv preprint</p>
<p>Generating a structured summary of numerous academic papers: Dataset and method. Liu Shuaiqi, Jiannong Cao, Ruosong Yang, Zhiyuan Wen, 10.24963/ijcai.2022/591Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22. the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22Main Track2022</p>
<p>Ziming Luo, Zonglin Yang, Zexin Xu, Wei Yang, Xinya Du, arXiv:2501.04306Llm4sr: A survey on large language models for scientific research. 2025arXiv preprint</p>
<p>hdbscan: Hierarchical density based clustering. Leland Mcinnes, John Healy, Steve Astels, J. Open Source Softw. 2112052017</p>
<p>Umap: Uniform manifold approximation and projection for dimension reduction. Leland Mcinnes, John Healy, James Melville, arXiv:1802.034262018arXiv preprint</p>
<p>Zach Nussbaum, John X Morris, Brandon Duderstadt, Andriy Mulyar, Nomic embed: Training a reproducible long context text embedder. 2024</p>
<p>Silhouettes: a graphical aid to the interpretation and validation of cluster analysis. Rousseeuw Peter, Journal of computational and applied mathematics. 201987</p>
<p>Anh Nguyen Duc, Kari Systä, and Pekka Abrahamsson. 2024. System for systematic literature review using multiple AI agents: Concept and an empirical evaluation. Abdul Malik Sami, Zeeshan Rasheed, Kai-Kristian Kemell, Muhammad Waseem, Terhi Kilamo, Mika Saari, 10.48550/arXiv.2403.08399ArXiv:2403.08399</p>
<p>Quantifying the User Experience: Practical Statistics for User Research. J Sauro, J R Lewis, 2012Morgan Kaufmann</p>
<p>Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning. Teo Susnjak, Peter Hwang, Napoleon H Reyes, L C Andre, Timothy R Barczak, Surangika Mcintosh, Ranathunga, 10.48550/arXiv.2404.08680ArXiv:2404.086802024</p>
<p>PROMPTHEUS: A Human-Centered Pipeline to Streamline SLRs with LLMs. Pedro Fernandes João, Catherine Torres, Joaquim Mulligan, Catarina Jorge, Moreira, 10.48550/arXiv.2410.15978ArXiv:2410.159782024</p>
<p>Mineru: An open-source solution for precise document content extraction. Bin Wang, Chao Xu, Xiaomeng Zhao, Linke Ouyang, Fan Wu, Zhiyuan Zhao, Rui Xu, Kaiwen Liu, Yuan Qu, Fukai Shang, Bo Zhang, Liqun Wei, Zhihao Sui, Wei Li, Botian Shi, Yu Qiao, Dahua Lin, Conghui He, 2024a</p>
<p>AutoSurvey: Large Language Models Can Automatically Write Surveys. Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Min Zhang, Qingsong Wen, Wei Ye, Shikun Zhang, Yue Zhang, 10.48550/arXiv.2406.10252ArXiv:2406.102522024b</p>
<p>An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, arXiv:2412.15115Qwen2. 5 technical report. 2024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4980 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4980</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4980</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-107.html">extraction-schema-107</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-259341893</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2307.02477v3.pdf" target="_blank">Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks</a></p>
                <p><strong>Paper Abstract:</strong> The impressive performance of recent language models across a wide range of tasks suggests that they possess a degree of abstract reasoning skills. Are these skills general and transferable, or specialized to specific tasks seen during pretraining? To disentangle these effects, we propose an evaluation framework based on “counterfactual” task variants that deviate from the default assumptions underlying standard tasks. Across a suite of 11 tasks, we observe nontrivial performance on the counterfactual variants, but nevertheless find that performance substantially and consistently degrades compared to the default conditions. This suggests that while current LMs may possess abstract task-solving skills to an extent, they often also rely on narrow, non-transferable procedures for task-solving. These results motivate a more careful interpretation of language model performance that teases apart these aspects.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4980.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4980.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source instruction-following large language model evaluated in this paper for its ability to perform natural-language deductive reasoning under default and counterfactual task-worlds (FOLIO-based experiments), using 0-shot prompting and optional zero-shot chain-of-thought.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source large pre-trained Transformer instruction-following LM from OpenAI; architecture, parameter count, and pretraining details are not disclosed in this paper. Evaluated in multiple tasks including natural-language first-order logical entailment (FOLIO-derived), with prompts that optionally include the phrase "Let's think step by step" (0-shot chain-of-thought).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Natural-language first-order logic deduction (FOLIO subset, counterfactually rewritten)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Entailment task: given a set of natural-language premises, determine whether a proposed conclusion is necessarily true / necessarily false / neither under symbolic deduction; the paper manually rewrote a subset of FOLIO instances to produce premises that violate common-sense (counterfactual world) to test whether models rely on commonsense vs. symbolic deduction.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluation via 0-shot prompting and 0-shot chain-of-thought prompting (add "Let's think step by step"); explicit instruction in prompts to avoid using commonsense/world knowledge; creation of counterfactual task variants by manually rewriting premises to be inconsistent with common-sense; addition of counterfactual comprehension checks (CCCs) that ask about truth of original vs. rewritten premises to control for whether the model understood the counterfactual world.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: GPT-4 attains the best performance among evaluated models on the logical reasoning task under default conditions and achieves above-random performance on counterfactual (rewritten-premise) variants, but shows a consistent and substantial degradation on counterfactual variants relative to default. The paper reports that CCC accuracy for GPT-4 is generally high (indicating comprehension of the counterfactual prompt), yet logical-task accuracy still drops under counterfactual premises, implying reliance on default/commonsense-conditioned behavior. Exact numeric breakdowns are reported in the paper's tables/figures (per-instance counts: a manually constructed counterfactual subset was ≈81 instances for the FOLIO rewrite experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fails more often when premises contradict the model's internal (pretraining-derived) beliefs; model correctness correlates with the number/proportion of premises the model deems true; performance degrades with more false/uncertain premises. Even when the model understands (passes CCC) the counterfactual specification, its entailment performance drops substantially, indicating reliance on task-default cues and pretraining frequency rather than purely abstract symbolic deduction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Compared to other closed-source models in this study (GPT-3.5, Claude, PaLM-2), GPT-4 is the strongest on default logical-reasoning instances and also tends to be stronger on counterfactual variants, but still exhibits the same qualitative default-vs-counterfactual gap. The paper notes correlations between default and counterfactual performance across tasks and models (stronger on default tends to mean stronger on counterfactual), while also discussing occasional model-specific differences in robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Key analyses: (1) Counterfactual comprehension checks (CCCs) were used to rule out simple instruction-following failures — GPT-4 usually passes CCCs, yet still underperforms on counterfactual entailment. (2) Logistic-regression analyses (Figure 5) show features predictive of model correctness: the number/proportion of premises labeled true by the model is positively associated with correctness; more false/uncertain premises reduce correctness; whether the model's belief about the conclusion's truth matches the instance label strongly predicts correctness — implying reliance on surface truthfulness signals. (3) 0-shot chain-of-thought ("Let's think step by step") is evaluated: 0-shot CoT is generally helpful across tasks, but the paper discusses exceptions and pragmatic/overthinking effects; for logical reasoning specifically the study uses 0-shot CoT and reports patterns of improvement in some settings but not a full elimination of the default-counterfactual gap.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4980.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4980.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior-generation closed-source instruction-following LM evaluated on the same FOLIO-derived natural-language logical reasoning task and its counterfactual variants; included in comparisons to newer models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source pre-trained Transformer-based instruction-following LM (OpenAI) with fewer capabilities than GPT-4 in this paper's evaluations; architecture/size not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Natural-language first-order logic deduction (FOLIO subset, counterfactually rewritten)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same entailment-format logical reasoning task: decide entailment given premises and a conclusion; counterfactual variants produced by rewriting premises to violate common-sense.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>0-shot prompting with and without 0-shot chain-of-thought; counterfactual comprehension checks used to verify model understanding; explicit instruction to avoid using world knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: GPT-3.5 attains lower accuracy than GPT-4 on default logical-reasoning instances and achieves above-random but substantially worse performance on counterfactual variants, with larger performance drops than GPT-4 in many cases. Counterfactual comprehension (CCC) is less reliable for some tasks relative to GPT-4, making interpretation of counterfactual-task failures more confounded when CCCs fail.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Greater sensitivity to default-task biases and pretraining frequency; poorer CCC pass rates in some settings (making it hard to disentangle failure-to-comprehend from failure-of-reasoning); larger degradation on counterfactual logical instances.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Weaker than GPT-4 on both default and counterfactual logical reasoning; generally outperformed by the more capable models in the study (GPT-4) and in many settings by Claude and PaLM-2 depending on the task, though precise relative ordering varies by task and prompt setting.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>0-shot chain-of-thought sometimes helps but does not close the default-vs-counterfactual gap. The logistic-regression style analyses (aggregated across models) indicate the same trends (truthful premises matter, etc.), implicating similar failure modes in GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4980.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4980.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude (Anthropic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source Anthropic instruction-following large language model evaluated on the FOLIO-derived natural-language logical reasoning task and counterfactual variants; included for cross-model comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude (claude-v1.3)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source instruction-following LM from Anthropic; model architecture/parameter count are not provided in this paper. Evaluated under the same 0-shot and 0-shot CoT prompting regimes as other models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Natural-language first-order logic deduction (FOLIO subset, counterfactually rewritten)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Entailment judgment from natural-language premises to a conclusion, with manual counterfactual rewrites to examine sensitivity to commonsense/world-belief conflicts.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>0-shot prompting, optional zero-shot chain-of-thought phrasing, counterfactual comprehension checks, and explicit instruction to avoid commonsense/world knowledge when reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: Claude shows moderate default-task accuracy on the logical reasoning instances and displays above-random but reduced performance on counterfactual variants. In some tasks CCC performance for Claude is lower than GPT-4, and the counterfactual gap is present as with other models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Similar failure modes: degradation when premises contradict model's internal beliefs, sensitivity to proportion of true/false/uncertain premises as the logistic analysis indicates, and occasional failures of the CCC indicating comprehension confounds in some counterfactual settings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Performance sits between GPT-3.5 and GPT-4 on several tasks in this paper's suite depending on the exact configuration; overall the default-vs-counterfactual gap is shared across Claude and other evaluated closed-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>0-shot CoT sometimes helps but does not eliminate the gap. The paper's regression analyses (aggregated) implicate commonsense-truth signals as predictive of correctness for Claude as well.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4980.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4980.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM-2 (Google)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Google-developed closed-source family of large language models (text-bison-001 used here) evaluated on the FOLIO-derived logical entailment task and its counterfactual variants; included as a cross-vendor comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-2 (text-bison-001; not the largest PaLM-2 available)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source Google model family; the version evaluated in this paper is the accessible (not largest) variant (text-bison-001). Architecture and parameter counts are not disclosed in the paper. Evaluated with the same prompt families (0-shot, 0-shot CoT) and CCC methodology.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Natural-language first-order logic deduction (FOLIO subset, counterfactually rewritten)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Entailment judgments over natural-language premises and conclusions; counterfactual rewrites create premises that violate commonsense to test for abstraction vs. memorized/default behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>0-shot and 0-shot chain-of-thought prompting; explicit instruction to avoid commonsense; CCCs to check comprehension of counterfactual premises.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: PaLM-2 (the evaluated variant) often performs worse than GPT-4 on default logical reasoning but in some tasks shows relative robustness to certain counterfactual perturbations; nonetheless it exhibits the systematic default-vs-counterfactual degradation pattern. CCC pass rates vary by task.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Same general limitations: sensitivity to world-belief alignment, decreased accuracy when premises are false/uncertain according to the model, and reliance on pretraining distributional frequency leading to poorer generalization to counterfactual world-states.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>PaLM-2 is generally outperformed by GPT-4 on default logical-reasoning instances in this study, and exhibits the same qualitative gap in counterfactual settings; in some tasks PaLM-2's counterfactual robustness exceeded its own default accuracy ranking relative to other models, suggesting dataset- and pretraining-dependent behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>0-shot chain-of-thought is evaluated and shows mixed effects (helpful in many cases, harmful in a few). The paper's analyses (including logistic regression over premise truthfulness and conclusion-truth-match) apply across models and demonstrate that external truth signals strongly predict when a model will answer correctly, indicating non-purely-symbolic decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>FOLIO: Natural language reasoning with first-order logic <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 1)</em></li>
                <li>Counterfactual reasoning: Testing language models' understanding of hypothetical scenarios <em>(Rating: 1)</em></li>
                <li>Evaluating Conceptual Structures in LMs (papers on LM logical/commonsense evaluation, e.g., Dasgupta et al., Yu et al.) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4980",
    "paper_id": "paper-259341893",
    "extraction_schema_id": "extraction-schema-107",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "A closed-source instruction-following large language model evaluated in this paper for its ability to perform natural-language deductive reasoning under default and counterfactual task-worlds (FOLIO-based experiments), using 0-shot prompting and optional zero-shot chain-of-thought.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Closed-source large pre-trained Transformer instruction-following LM from OpenAI; architecture, parameter count, and pretraining details are not disclosed in this paper. Evaluated in multiple tasks including natural-language first-order logical entailment (FOLIO-derived), with prompts that optionally include the phrase \"Let's think step by step\" (0-shot chain-of-thought).",
            "model_size": null,
            "logical_reasoning_task": "Natural-language first-order logic deduction (FOLIO subset, counterfactually rewritten)",
            "task_description": "Entailment task: given a set of natural-language premises, determine whether a proposed conclusion is necessarily true / necessarily false / neither under symbolic deduction; the paper manually rewrote a subset of FOLIO instances to produce premises that violate common-sense (counterfactual world) to test whether models rely on commonsense vs. symbolic deduction.",
            "method_or_approach": "Evaluation via 0-shot prompting and 0-shot chain-of-thought prompting (add \"Let's think step by step\"); explicit instruction in prompts to avoid using commonsense/world knowledge; creation of counterfactual task variants by manually rewriting premises to be inconsistent with common-sense; addition of counterfactual comprehension checks (CCCs) that ask about truth of original vs. rewritten premises to control for whether the model understood the counterfactual world.",
            "performance": "Qualitative: GPT-4 attains the best performance among evaluated models on the logical reasoning task under default conditions and achieves above-random performance on counterfactual (rewritten-premise) variants, but shows a consistent and substantial degradation on counterfactual variants relative to default. The paper reports that CCC accuracy for GPT-4 is generally high (indicating comprehension of the counterfactual prompt), yet logical-task accuracy still drops under counterfactual premises, implying reliance on default/commonsense-conditioned behavior. Exact numeric breakdowns are reported in the paper's tables/figures (per-instance counts: a manually constructed counterfactual subset was ≈81 instances for the FOLIO rewrite experiment).",
            "limitations_or_failure_cases": "Fails more often when premises contradict the model's internal (pretraining-derived) beliefs; model correctness correlates with the number/proportion of premises the model deems true; performance degrades with more false/uncertain premises. Even when the model understands (passes CCC) the counterfactual specification, its entailment performance drops substantially, indicating reliance on task-default cues and pretraining frequency rather than purely abstract symbolic deduction.",
            "comparison": "Compared to other closed-source models in this study (GPT-3.5, Claude, PaLM-2), GPT-4 is the strongest on default logical-reasoning instances and also tends to be stronger on counterfactual variants, but still exhibits the same qualitative default-vs-counterfactual gap. The paper notes correlations between default and counterfactual performance across tasks and models (stronger on default tends to mean stronger on counterfactual), while also discussing occasional model-specific differences in robustness.",
            "ablation_or_analysis_results": "Key analyses: (1) Counterfactual comprehension checks (CCCs) were used to rule out simple instruction-following failures — GPT-4 usually passes CCCs, yet still underperforms on counterfactual entailment. (2) Logistic-regression analyses (Figure 5) show features predictive of model correctness: the number/proportion of premises labeled true by the model is positively associated with correctness; more false/uncertain premises reduce correctness; whether the model's belief about the conclusion's truth matches the instance label strongly predicts correctness — implying reliance on surface truthfulness signals. (3) 0-shot chain-of-thought (\"Let's think step by step\") is evaluated: 0-shot CoT is generally helpful across tasks, but the paper discusses exceptions and pragmatic/overthinking effects; for logical reasoning specifically the study uses 0-shot CoT and reports patterns of improvement in some settings but not a full elimination of the default-counterfactual gap.",
            "uuid": "e4980.0",
            "source_info": {
                "paper_title": "Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "GPT-3.5 (OpenAI)",
            "brief_description": "A prior-generation closed-source instruction-following LM evaluated on the same FOLIO-derived natural-language logical reasoning task and its counterfactual variants; included in comparisons to newer models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "Closed-source pre-trained Transformer-based instruction-following LM (OpenAI) with fewer capabilities than GPT-4 in this paper's evaluations; architecture/size not specified here.",
            "model_size": null,
            "logical_reasoning_task": "Natural-language first-order logic deduction (FOLIO subset, counterfactually rewritten)",
            "task_description": "Same entailment-format logical reasoning task: decide entailment given premises and a conclusion; counterfactual variants produced by rewriting premises to violate common-sense.",
            "method_or_approach": "0-shot prompting with and without 0-shot chain-of-thought; counterfactual comprehension checks used to verify model understanding; explicit instruction to avoid using world knowledge.",
            "performance": "Qualitative: GPT-3.5 attains lower accuracy than GPT-4 on default logical-reasoning instances and achieves above-random but substantially worse performance on counterfactual variants, with larger performance drops than GPT-4 in many cases. Counterfactual comprehension (CCC) is less reliable for some tasks relative to GPT-4, making interpretation of counterfactual-task failures more confounded when CCCs fail.",
            "limitations_or_failure_cases": "Greater sensitivity to default-task biases and pretraining frequency; poorer CCC pass rates in some settings (making it hard to disentangle failure-to-comprehend from failure-of-reasoning); larger degradation on counterfactual logical instances.",
            "comparison": "Weaker than GPT-4 on both default and counterfactual logical reasoning; generally outperformed by the more capable models in the study (GPT-4) and in many settings by Claude and PaLM-2 depending on the task, though precise relative ordering varies by task and prompt setting.",
            "ablation_or_analysis_results": "0-shot chain-of-thought sometimes helps but does not close the default-vs-counterfactual gap. The logistic-regression style analyses (aggregated across models) indicate the same trends (truthful premises matter, etc.), implicating similar failure modes in GPT-3.5.",
            "uuid": "e4980.1",
            "source_info": {
                "paper_title": "Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Claude",
            "name_full": "Claude (Anthropic)",
            "brief_description": "A closed-source Anthropic instruction-following large language model evaluated on the FOLIO-derived natural-language logical reasoning task and counterfactual variants; included for cross-model comparisons.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Claude (claude-v1.3)",
            "model_description": "Closed-source instruction-following LM from Anthropic; model architecture/parameter count are not provided in this paper. Evaluated under the same 0-shot and 0-shot CoT prompting regimes as other models.",
            "model_size": null,
            "logical_reasoning_task": "Natural-language first-order logic deduction (FOLIO subset, counterfactually rewritten)",
            "task_description": "Entailment judgment from natural-language premises to a conclusion, with manual counterfactual rewrites to examine sensitivity to commonsense/world-belief conflicts.",
            "method_or_approach": "0-shot prompting, optional zero-shot chain-of-thought phrasing, counterfactual comprehension checks, and explicit instruction to avoid commonsense/world knowledge when reasoning.",
            "performance": "Qualitative: Claude shows moderate default-task accuracy on the logical reasoning instances and displays above-random but reduced performance on counterfactual variants. In some tasks CCC performance for Claude is lower than GPT-4, and the counterfactual gap is present as with other models.",
            "limitations_or_failure_cases": "Similar failure modes: degradation when premises contradict model's internal beliefs, sensitivity to proportion of true/false/uncertain premises as the logistic analysis indicates, and occasional failures of the CCC indicating comprehension confounds in some counterfactual settings.",
            "comparison": "Performance sits between GPT-3.5 and GPT-4 on several tasks in this paper's suite depending on the exact configuration; overall the default-vs-counterfactual gap is shared across Claude and other evaluated closed-source models.",
            "ablation_or_analysis_results": "0-shot CoT sometimes helps but does not eliminate the gap. The paper's regression analyses (aggregated) implicate commonsense-truth signals as predictive of correctness for Claude as well.",
            "uuid": "e4980.2",
            "source_info": {
                "paper_title": "Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "PaLM-2",
            "name_full": "PaLM-2 (Google)",
            "brief_description": "A Google-developed closed-source family of large language models (text-bison-001 used here) evaluated on the FOLIO-derived logical entailment task and its counterfactual variants; included as a cross-vendor comparison.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM-2 (text-bison-001; not the largest PaLM-2 available)",
            "model_description": "Closed-source Google model family; the version evaluated in this paper is the accessible (not largest) variant (text-bison-001). Architecture and parameter counts are not disclosed in the paper. Evaluated with the same prompt families (0-shot, 0-shot CoT) and CCC methodology.",
            "model_size": null,
            "logical_reasoning_task": "Natural-language first-order logic deduction (FOLIO subset, counterfactually rewritten)",
            "task_description": "Entailment judgments over natural-language premises and conclusions; counterfactual rewrites create premises that violate commonsense to test for abstraction vs. memorized/default behavior.",
            "method_or_approach": "0-shot and 0-shot chain-of-thought prompting; explicit instruction to avoid commonsense; CCCs to check comprehension of counterfactual premises.",
            "performance": "Qualitative: PaLM-2 (the evaluated variant) often performs worse than GPT-4 on default logical reasoning but in some tasks shows relative robustness to certain counterfactual perturbations; nonetheless it exhibits the systematic default-vs-counterfactual degradation pattern. CCC pass rates vary by task.",
            "limitations_or_failure_cases": "Same general limitations: sensitivity to world-belief alignment, decreased accuracy when premises are false/uncertain according to the model, and reliance on pretraining distributional frequency leading to poorer generalization to counterfactual world-states.",
            "comparison": "PaLM-2 is generally outperformed by GPT-4 on default logical-reasoning instances in this study, and exhibits the same qualitative gap in counterfactual settings; in some tasks PaLM-2's counterfactual robustness exceeded its own default accuracy ranking relative to other models, suggesting dataset- and pretraining-dependent behavior.",
            "ablation_or_analysis_results": "0-shot chain-of-thought is evaluated and shows mixed effects (helpful in many cases, harmful in a few). The paper's analyses (including logistic regression over premise truthfulness and conclusion-truth-match) apply across models and demonstrate that external truth signals strongly predict when a model will answer correctly, indicating non-purely-symbolic decision-making.",
            "uuid": "e4980.3",
            "source_info": {
                "paper_title": "Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks",
                "publication_date_yy_mm": "2023-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "FOLIO: Natural language reasoning with first-order logic",
            "rating": 2,
            "sanitized_title": "folio_natural_language_reasoning_with_firstorder_logic"
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 1,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "Counterfactual reasoning: Testing language models' understanding of hypothetical scenarios",
            "rating": 1,
            "sanitized_title": "counterfactual_reasoning_testing_language_models_understanding_of_hypothetical_scenarios"
        },
        {
            "paper_title": "Evaluating Conceptual Structures in LMs (papers on LM logical/commonsense evaluation, e.g., Dasgupta et al., Yu et al.)",
            "rating": 1,
            "sanitized_title": "evaluating_conceptual_structures_in_lms_papers_on_lm_logicalcommonsense_evaluation_eg_dasgupta_et_al_yu_et_al"
        }
    ],
    "cost": 0.019167749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks
28 Mar 2024</p>
<p>Zhaofeng Wu 
Linlu Qiu 
Alexis Ross 
Ekin Akyürek 
Boyuan Chen 
Bailin Wang 
Najoung Kim 
Jacob Andreas 
Yoon Kim 
Alex Hu 
Ananya Harsh Jha 
Belinda Li 
Erjia Cao 
Ha-Na Park 
Huirong Wen 
Jiangjie Chen 
Kabir Swain 
Ka Wai Chan 
Lucy Li 
Sim- Ran Swain 
Tejas Srinivasan 
Tianyu Liu 
Yue Bai 
Yutaro Yamada 
Ziwei Wei Zhaofeng 
Andrea Agostinelli 
Timo I Denk 
Zalán Borsos 
Jesse Engel 
Mauro Verzetti 
Antoine Caillon 
Qingqing Huang 
Aren Jansen 
Adam Roberts 
Marco Tagliasacchi 
Matt Sharifi 
Rohan Anil 
Andrew M Dai 
Orhan Firat 
Melvin Johnson 
Dmitry Lepikhin 
Alexandre Passos 
Siamak Shakeri 
Emanuel Taropa 
Paige Bai- Ley 
Zhifeng Chen 
Eric Chu 
Jonathan H Clark 
Laurent El 
Yanping Huang 
Kathy Meier-Hellstern 
Gaurav Mishra 
Erica Moreira 
Mark Omernick 
Kevin Robinson 
Sebastian Ruder 
Yi Tay 
Kefan Xiao 
Yuanzhong Xu 
Yu- Jing Zhang 
Gustavo Hernandez Abrego 
Jun- Whan Ahn 
Jacob Austin 
HyungPaul Barham 
Jan Botha 
James Bradbury 
Siddhartha Brahma 
Kevin Brooks 
Michele Catasta 
Yong Cheng 
Colin Cherry 
Christopher A Choquette-Choo 
Aakanksha Chowdhery 
Clément Crepy 
Shachi Dave 
Mostafa Dehghani 
Sunipa Dev 
Jacob De- Vlin 
Mark Díaz 
Nan Du 
Ethan Dyer 
Vlad Fein- Berg 
Fangxiaoyu Feng 
Vlad Fienber 
Markus Freitag 
Xavier Garcia 
Sebastian Gehrmann 
Lu- Cas Gonzalez 
Guy Gur-Ari 
Steven Hand 
Hadi Hashemi 
Le Hou 
Joshua Howland 
Andrea Hu 
Jeffrey Hui 
Jeremy Hurwitz 
Michael Is- Ard 
Abe Ittycheriah 
Matthew Jagielski 
Wen- Hao Jia 
Kathleen Kenealy 
Maxim Krikun 
Sneha Kudugunta 
Chang Lan 
Katherine Lee 
Benjamin Lee 
MusicEric Li 
Wei Li 
Yaguang Li 
Jian Li 
Hyeontaek Li 
Hanzhao Lim 
Zhongtao Lin 
Frederick Liu 
Marcello Liu 
Aroma Maggioni 
Joshua Mahendru 
Vedant Maynez 
Maysam Misra 
Zachary Moussalem 
John Nado 
Eric Nham 
Andrew Ni 
Alicia Nystrom 
Marie Parrish 
Martin Pellat 
Alex Polacek 
Reiner Polozov 
Siyuan Pope 
Emily Qiao 
Bryan Reif 
Parker Richter 
Alex Riley 
Aurko Cas- Tro Ros 
Brennan Roy 
Rajku- Mar Saeta 
Samuel 
Yuntao Bai 
Andy Jones 
Kamal Ndousse 
Amanda Askell 
Anna Chen 
Nova Dassarma 
Dawn Drain 
Stanislav Fort 
Deep Ganguli 
Tom Henighan 
Nicholas Joseph 
Saurav Kadavath 
Jackson Kernion 
SheerTom Conerly 
Nelson Elhage 
Zac Hatfield-Dodds 
Danny Her- Nandez 
Tristan Hume 
Scott Johnston 
Shauna Kravec 
Liane Lovitt 
Neel Nanda 
Catherine Olsson 
Dario 2020 Amodei 
Tom B Brown 
Jack Clark 
Sam Mccandlish 
Chris Olah 
Benjamin Mann 
Jared 2022 Kaplan 
Nick Ryder 
Melanie Subbiah 
Prafulla Dhari- Wal 
Arvind Neelakantan 
Pranav Shyam 
Girish Sastry 
Sandhini Agarwal 
Ariel Herbert-Voss 
Gretchen Krueger 
Rewon Child 
Aditya Ramesh 
Daniel M Ziegler 
Jeffrey Wu 
Clemens Winter 
Christo- Pher Hesse 
Mark Chen 
Eric Sigler 
Mateusz Litwin 
Scott Gray 
Benjamin Chess 
Jack Clark 
Christopher Berner 
Alec Rad- Ford 
Ilya Sutskever 
Sébastien Bubeck 
Varun Chandrasekaran 
Ro- Nen Eldan 
Johannes Gehrke 
Eric Horvitz 
Ece Kamar 
Peter Lee 
Yin Tat Lee 
Yuanzhi Li 
Scott Lundberg 
Harsha Nori 
Hamid Palangi 
Marco Tulio Ribeiro 
Yi 2023 Zhang 
Nicholas Carlini 
Florian Tramer 
Eric Wallace 
Kather- Ine Lee 
Jerry Tworek 
Heewoo Jun 
Qim- Ing Yuan 
Henrique Ponde 
Oliveira Pinto 
Harri Edwards 
Yuri Burda 
Greg Brockman 
Alex Ray 
Raul Puri 
Michael Petrov 
Heidy Khlaaf 
Pamela Mishkin 
Brooke Chan 
Mikhail Pavlov 
Alethea Power 
Lukasz Kaiser 
Mo- Hammad Bavarian 
Philippe Tillet 
Felipe Petroski Such 
Dave Cummings 
Matthias Plappert 
Fotios Chantzis 
Elizabeth Barnes 
William Hebgen Guss 
Alex Nichol 
Alex Paino 
Nikolas Tezak 
Jie Tang 
Igor Babuschkin 
Suchir Balaji 
Shan- Tanu Jain 
William Saunders 
Christopher Hesse 
Andrew N Carr 
Jan Leike 
Josh Achiam 
Vedant Misra 
Evan Morikawa 
Alec Radford 
Matthew Knight 
Miles Brundage 
Mira Murati 
Katie Mayer 
Peter Welinder 
Bob Mcgrew 
Wojciech 2021 Zaremba 
Wenhu Chen 
Xueguang Ma 
Xinyi Wang 
William W 2022 Cohen 
Sharan Narang 
Maarten Bosma 
Won Chung 
Charles Sutton 
Sebastian Gehrmann 
Parker Schuh 
Kensen Shi 
Sasha Tsvyashchenko 
Joshua Maynez 
Abhishek Rao 
Parker Barnes 
Noam Shazeer 
Vinodkumar Prab- Hakaran 
Emily Reif 
Ben Hutchin- Son 
Reiner Pope 
Michael Isard 
Pengcheng Yin 
Toju Duke 
Anselm Levskaya 
Sanjay Ghemawat 
Henryk Michalewski 
Vedant Misra 
Kevin Robin- Son 
Liam Fedus 
Denny Zhou 
Daphne Ip- Polito 
David Luan 
Hyeontaek Lim 
Barret Zoph 
Alexander Spiridonov 
Ryan Sepassi 
David Dohan 
Shivani Agrawal 
Thanumalayan Sankara- Narayana Pillai 
Marie Pellat 
Aitor Lewkowycz 
Oleksandr Polozov 
Katherine Lee 
Zongwei Zhou 
Xuezhi Wang 
Brennan Saeta 
Mark Diaz 
Jason Wei 
Dou- Glas Eck 
Jeff Dean 
Slav Petrov 
Peter Clark 
Oyvind Tafjord 
Kyle Richard 
Sebastian Schuster 
Christopher D Manning 
Pratyusha Sharma 
Tamar Rott Shaham 
Manel Baradad 
Stephanie Fu 
Adrian Rodriguez- Munoz 
Shivam Duggal 
David Silver 
Thomas Hubert 
Julian Schrittwieser 
Ioannis Antonoglou 
Matthew Lai 
Arthur Guez 
Marc Lanctot 
Laurent Sifre 
Dharshan Kumaran 
Thore Graepel 
Timothy P Lillicrap 
Karen Si 
Mark K Singley 
John Robert 
Alessandro Sordoni 
Xingdi Yuan 
Marc-Alexandre Côté 
Matheus Pereira 
Adam Trischler 
Ziang Xiao 
Arian Hosseini 
Friederike Niedtner 
Anders Iyer 
Andrea Andreassen 
An- Drea Madotto 
Andreas Santilli 
Andrew Stuhlmüller 
Andrew La 
Andy Lampinen 
An- Gela Zou 
Angelica Jiang 
Anh Chen 
Ani- Mesh Vuong 
Anna Gupta 
Antonio Gottardi 
Anu Norelli 
Arash Venkatesh 
Arfa Gholamidavoodi 
Arul Tabassum 
Arun Menezes 
Asher Kirubarajan 
Ashish Mullokandov 
Austin Sabharwal 
Avia Herrick 
Aykut Efrat 
Ayla Erdem 
B Karakaş 
Ryan Roberts 
Bao Sheng Loe 
Bartłomiej Bojanowski 
Batuhan Özyurt 
Behnam Hedayatnia 
Behnam Neyshabur 
Ben- Jamin Inden 
Benno Stein 
Berk Ekmekci 
Cedrick Stinson 
César Argueta 
Chandan Ferri Ramírez 
Charles Singh 
Chenlin Rathkopf 
Chitta Meng 
Chiyu Baral 
Chris Wu 
Chris Callison- Burch 
Christian Waites 
Christo- Pher D Voigt 
Cindy Potts 
Clara E Ramirez 
Clemencia Rivera 
Colin Siro 
Courtney Raffel 
Cristina Ashcraft 
Damien Garbacea 
Dan Sileo 
Dan Garrette 
Dan Hendrycks 
Dan Kilman 
Daniel Roth 
Daniel Freeman 
Daniel Khashabi 
Daniel Levy 
Danielle Moseguí González 
Danny Perszyk 
Danqi Hernan- Dez 
Daphne Chen 
Dar Ippolito 
David Gilboa 
David Drakard 
Debajyoti Jurgens 
Deep Datta 
Denis Ganguli 
Denis Emelin 
Deniz Kleyko 
Derek Yuret 
Derek Chen 
Dieuwke Tam 
Diganta Hupkes 
Dilyar Misra 
Dimitri Coelho Buzan 
Diyi Mollo 
Dong- Ho Yang 
Dylan Lee 
Ekaterina Schrader 
EkinDogus Shutova 
Elad Cubuk 
Eleanor Segal 
Elizabeth Hager- Man 
Ellie Donoway 
Emanuele Pavlick 
Emma Rodola 
Eric Lam 
Erkut Tang 
Ernie Erdem 
Ethan A Chang 
Ethan Chi 
Ethan Jerzak 
Eunice Engefu Kim 
Evgenii Manyasi 
Fanyue Zheltonozhskii 
Fatemeh Xia 
Siar 
Francesca Fer- Nando Martínez-Plumed 
Fran- Cois Happé 
Frieda Chollet 
Gaurav Rong 
Mishra 
Indra Genta 
Gerard Winata 
Germán De Melo 
Giambattista Kruszewski 
Gior- Gio Parascandolo 
Gloria Mariani 
Gonzalo Wang 
Gregor Jaimovitch- López 
Guy Betz 
Hana Gur-Ari 
Hannah Gal- Ijasevic 
Hannah Kim 
Han- Naneh Rashkin 
Harsh Hajishirzi 
Hayden Mehta 
Henry Bogar 
Hinrich Shevlin 
Hiromu Schütze 
Hongming Yakura 
Hugh Mee Zhang 
Ian Wong 
Ng 
John U Miller 
Jonathan Balis 
Jonathan Batchelder 
Jörg Berant 
Jos Frohberg 
Jose Rozen 
Joseph Hernandez-Orallo 
Joseph Boudeman 
Joseph Guerr 
Joshua B Jones 
Joshua S Tenenbaum 
Joyce Rule 
Kamil Chua 
Karen Kanclerz 
Karl Livescu 
Karthik Krauth 
Katerina Gopalakr- Ishnan 
Katja Ignatyeva 
Kaus- Tubh D Markert 
Kevin Dhole 
Kevin Gimpel 
Kory Omondi 
Kristen Mathewson 
Ksenia Chiafullo 
Kumar Shkaruta 
Kyle Shridhar 
Kyle Mcdonell 
Laria Richardson 
Leo Reynolds 
Li Gao 
Liam Zhang 
Lianhui Dugan 
Lidia Qin 
Louis-Philippe Contreras-Ochando 
Morency 
Luke Colón 
LütfiKerem Metz 
Maarten Şenel 
Maarten Sap, Maartje terHoeve Bosma 
Ma- Heen Farooqi 
Manaal Faruqui 
Mantas Mazeika 
Marco Baturan 
Marco Marelli 
Marco Maru 
Maria Jose Ramírez Quintana 
Marie Tolkiehn 
Mario Giulianelli 
Martha Lewis 
Martin Pot- Thast 
Matthew L Leavitt 
Matthias Hagen 
Má- Tyás Schubert 
Orduna Medina 
Melody Baitemirova 
Melvin Arnaud 
Michael A Mcelrath 
Michael Yee 
Michael Cohen 
Michael Gu 
Michael Ivanitskiy 
Michael Starritt 
Michał Strube 
Michele Swędrowski 
Michi- Hiro Bevilacqua 
Mihir Yasunaga 
Mike Kale 
Mimee Cain 
Mirac Xu 
Mitch Suzgun 
Mo Walker 
Mohit Tiwari 
Moin Bansal 
Mor Aminnaseri 
Mozhdeh Geva 
Mukund Gheini 
T Varma 
Nanyun Peng 
Nathan A Chi 
Nayeon Lee 
Neta Gur- 
Ari Krakover 
Nicholas Cameron 
Nicholas Roberts 
Nick Doiron 
Nicole Martinez 
Nikita Nan- Gia 
Niklas Deckers 
Niklas Muennighoff 
Ni- Tish Shirish Keskar 
Niveditha S Iyer 
Noah Fiedel 
Nuan Wen 
Oliver Zhang 
Omar Agha 
Omar Elbaghdadi 
Omer Levy 
Owain Evans 
Pablo Antonio 
Moreno Casares 
Parth Doshi 
Pascale Fung 
Paul Pu Liang 
Paul Vicol 
Pegah Alipoormolabashi 
Peiyuan Liao 
Percy Liang 
Peter Chang 
PhuPeter Eck- Ersley 
Mon Htut 
Pinyu Hwang 
Piotr Miłkowski 
Piyush Patil 
Pouya Pezeshkpour 
Priti Oli 
Qiaozhu Mei 
Qing Lyu 
Qinlang Chen 
Rabin Banjade 
Rachel Etta Rudolph 
Raefer Gabriel 
Rahel Habacker 
Ramon Risco 
Rylan Teehan 
Sahib Yang 
Saif M Singh 
Sajant Mohammad 
Sam Anand 
Sam Dillavou 
Sam Shleifer 
Samuel Wiseman 
Samuel R Gruet- Ter 
Samuel S Bowman 
Sanghyun Schoenholz 
Sanjeev Han 
Sarah A Kwatra 
Sarik Rous 
Sayan Ghazarian 
Sean Ghosh 
Se- Bastian Casey 
Sebastian Bischoff 
Sebas- Tian Gehrmann 
Sepideh Schuster 
Shadi Sadeghi 
Sharon Hamdan 
Shashank Zhou 
Sherry Srivastava 
Shikhar Shi 
Shima Singh 
Asaadi 
Shane Shixiang 
Shubh Gu 
Shubham Pachchigar 
Shyam Toshniwal 
Upadhyay 
Shyamolima 
Sia- Mak Debnath 
Simon Shakeri 
Simone Thormeyer 
Siva Melzi 
SnehaPriscilla Reddy 
Soo-Hwan Makini 
Spencer Lee 
Sriharsha Torene 
Stanis- Las Hatwar 
Stefan Dehaene 
Stefano Divic 
Stella Ermon 
Stephanie Biderman 
Stephen Lin 
Steven T Prasad 
Stuart M Piantadosi 
Sum- Mer Shieber 
Svetlana Misherghi 
Swaroop Kiritchenko 
Tal Mishra 
Tal Linzen 
Tao Schuster 
Tao Li 
Tariq Yu 
Tatsu Ali 
Te-Lin Hashimoto 
Théo Wu 
Theodore Desbordes 
Thomas Rothschild 
Tianle Phan 
Tiberius Wang 
Timo Nkinyili 
Timofei Schick 
Titus Kornev 
Tobias Tunduny 
Trenton Gersten- Berg 
Trishala Chang 
Tushar Neeraj 
Tyler Khot 
Uri Shultz 
Vedant Shaham 
Vera Misra 
Victoria Demberg 
Vikas Nyamai 
Vinay Raunak 
Vinay Uday Ramasesh 
Vishakh Prabhu 
Vivek Padmakumar 
William Srikumar 
William Fedus 
Wout Zhang 
Xiang Vossen 
Xiaoyu Ren 
Xinran Tong 
Xinyi Zhao 
Xudong Wu 
Yadollah Shen 
Yair Yaghoobzadeh 
Yangqiu Lakretz 
Yasaman Song 
Yejin Bahri 
Yichi Choi 
Yiding Yang 
Yifu Hao 
Yonatan Chen 
Yu Belinkov 
Yufang Hou 
Yuntao Hou 
Zachary Bai 
Zhuoye Seid 
Zijian Zhao 
Zijie J Wang 
Zirui Wang 
Ziyi Wang 
2023 Wu 
Hugo Touvron 
Thibaut Lavril 
Gautier Izacard 
Xavier Martinet 
Marie-Anne Lachaux 
Timo- Thée Lacroix 
Baptiste Rozière 
Naman Goyal 
Eric Hambro 
Faisal Azhar 
Aurelien Rodriguez 
Yizhong Wang 
Hamish Ivison 
Pradeep Dasigi 
Jack Hessel 
Tushar Khot 
Raghavi Khyathi 
David Chandu 
Kelsey Wadden 
Noah A Macmillan 
Iz Smith 
Hannaneh Beltagy 
Ha 
Swaroop Mishra 
Pegah Alipoor- Molabashi 
Yeganeh Kordi 
Amirreza Mirzaei 
Atharva Naik 
Arjun Ashok 
Selvan Arut 
Anjana Dhanasekaran 
David Arunkumar 
Eshaan Stap 
Giannis Pathak 
Haizhi Karamanolakis 
Ishan Lai 
Ishani Purohit 
Jacob Mondal 
Kirby An- Derson 
Krima Kuznia 
Kun- Tal Doshi 
Maitreya Kumar Pal 
Mehrad Patel 
Mihir Morad- Shahi 
Mirali Parmar 
Neeraj Purohit 
PhaniRohitha Varshney 
Pulkit Kaza 
Verma </p>
<p>MIT Boston University</p>
<p>Renee Shelby
Ambrose Slone</p>
<p>Daniel Smilkov
Si-mon Tokumine
David R. So, Daniel Sohn, Dasha Valter, Vijay Vasude-van, Kiran Vodrahalli, Xuezhi Wang, Zirui Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang Zhou, Denny ZhouPidong Wang, Tao Wang, Qiao Zhang, Steven Zheng</p>
<p>Slav Petrov
and Yonghui Wu2023PaLM</p>
<p>technical report. Anthropic. 2023. Introducing Claude</p>
<p>Enhanced English Universal Dependen
2016</p>
<p>Bill Yuchen Lin
Blake Howald</p>
<p>Bryan Orin-ion
Cameron DiaoCameron Dour, Catherine</p>
<p>Isaac Noble
Jaap Jumelet, Jack Geissinger, Jacob HiltonJackson Kernion</p>
<p>Jaehoon Lee</p>
<p>Jaime Fernández Fisac
James B. Simon, James Koppel, James Zheng, James Zou, Jan Kocoń, Jana Thompson, Janelle Wingfield, Jared Ka-plan, Jarema Radom, Jascha Sohl-Dickstein, Jason Wei, Jason YosinskiJa-son Phang</p>
<p>Jekate-rina Novikova
Jelle Bosscher, Jennifer Marsh, Jeremy Kim, Jesse EngelJeroen Taal</p>
<p>Jesu-joba Alabi
Jiacheng Xu, Jiaming Song, Joan WaweruJil-lian Tang, John Burden, John</p>
<p>Luca Moschella
Lucas Lam</p>
<p>Lucy Noble
Ludwig Schmidt, Luheng HeLuis Oliveros</p>
<p>Rif A. Saurous
Rhythm Garg, Richard Barnes, Robbe Raymaekers, Rohan Sikand, Roman Novak, Roman SitelewMillière, Riku Arakawa, Robert Frank</p>
<p>Ronan LeBras
Rosanne Liu, Rowan JacobsRui Zhang</p>
<p>Ruslan Salakhutdi-nov
Ryan Chi</p>
<p>Ryan Lee
Ryan Stovall, Ryan</p>
<p>Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks
28 Mar 2024DB6FC54B4800DD4DEB281AD7858F709AarXiv:2307.02477v3[cs.CL]
The impressive performance of recent language models across a wide range of tasks suggests that they possess a degree of abstract reasoning skills.Are these skills general and transferable, or specialized to specific tasks seen during pretraining?To disentangle these effects, we propose an evaluation framework based on "counterfactual" task variants that deviate from the default assumptions underlying standard tasks.Across a suite of 11 tasks, we observe nontrivial performance on the counterfactual variants, but nevertheless find that performance substantially and consistently degrades compared to the default conditions.This suggests that while current LMs may possess abstract task-solving skills to an extent, they often also rely on narrow, non-transferable procedures for task-solving.These results motivate a more careful interpretation of language model performance that teases apart these aspects of behavior.</p>
<p>Figure 1: GPT-4's performance on the default version of various tasks (blue) and counterfactual counterparts (orange).The shown results use 0-shot chain-of-thought prompting ( §4; Kojima et al., 2023).GPT-4 consistently and substantially underperforms on counterfactual variants compared to default task instantiations.</p>
<p>Introduction</p>
<p>The striking empirical successes of language models (LMs) suggest that next-word prediction at scale may be a viable approach for distilling the knowledge embedded in large-scale text corpora into general-purpose interactive agents.LMs obtain impressive results on various NLP benchmarks (Ope-nAI, 2023;Anil et al., 2023;Anthropic, 2023;i.a.) and display surprising abilities that suggest a nontrivial understanding of the world (Bubeck et al., 2023).They have been shown to pass professional exams (Kung et al., 2023;Nori et al., 2023;Terwiesch, 2023;i.a.), exceed state-of-the-art methods on many traditional benchmarks (Sun et al., 2023;Sobania et al., 2023;Zhang et al., 2023a;Dhingra et al., 2023;i.a.), and surpass human performance on tasks that require seemingly nontrivial reasoning (Chowdhery et al., 2022;Hoffmann et al., 2022;Malinka et al., 2023;Guo et al., 2023;i.a.).</p>
<p>Ideally, we expect a general-purpose LM to be able to generalize not only to unseen instances of known tasks, but to new tasks.Humans, for example, can transfer their knowledge to new instances and also flexibly adapt to novel tasks (Singley and Anderson, 1989).To what extent does the performance of current LMs derive from their ability to deploy task-general reasoning skills, versus their ability to recognize and recall specific tasks seen frequently in pre-training?</p>
<p>Past work has focused on instance-level generalization, but this is often complicated by data contamination issues (Dodge et al., 2021;Magar and Schwartz, 2022;i.a.).In this work, we are interested in the models' generalizability to new task variants, which has been less systematically studied for LMs (though see Li et al. (2022), Mishra et al. (2022), andWang et al. (2022b)).</p>
<p>We propose to measure such task-level generalizability by taking tasks on which LMs perform well, and altering the conditions or rules under which these tasks are performed.The general reasoning procedure for these tasks remains the same under the new conditions, but the specific input-output mapping functions are changed.We call the new tasks counterfactual tasks, as they deviate from the default, generally assumed conditions for these tasks.Figure 1 shows examples: in the top left, default arithmetic is performed in base-10, while</p>
<p>We release our code, all synthetically generated data, and LM interactions (prompts and responses) at https://github.com/ZhaofengWu/counterfactual-evaluation.</p>
<p>counterfactual arithmetic is performed in base 9.If models implement a general and transferable task-solving procedure, we expect comparable performance on counterfactual and default tasks; if they employ procedures tailored to default task conditions, we expect a drop in the counterfactual performance.</p>
<p>We design a suite of 11 counterfactual evaluation tasks to measure an LM's flexibility to adapt to new task variants across multiple categories and domains, as summarized in Figure 1.In each, the original task under the default conditions and its counterfactual variants share the same reasoning procedure but differ in their input-output mappings.We consider traditional NLP tasks such as deductive reasoning, non-language tasks that are nonetheless commonly evaluated such as code generation, as well as non-standard tasks such as drawing and spatial reasoning.The latter extralinguistic tasks test whether LMs are able to learn conceptual structures that mirror the structure of the non-linguistic world, which has been suggested by recent work (Abdou et al., 2021;Ilharco et al., 2021;Patel and Pavlick, 2022;Li et al., 2023a;Bubeck et al., 2023;Søgaard, 2023;i.a.).</p>
<p>We evaluate the performance of GPT-4 (Ope-nAI, 2023), GPT-3.5, Claude (Anthropic, 2023), and PaLM-2 (Anil et al., 2023) on tasks under both the default and counterfactual conditions.We observe above-random counterfactual performance for most tasks, indicating some degree of task generalizability.However, the performance on counterfactual task variants consistently and substantially degrades relative to the performance on the default settings.This suggests that these models' ability on these tasks is supported at least in part by nontransferable, default-condition-specific behaviors rather than abstract, generalizable reasoning skills.</p>
<p>These results also reveal several surprising relations between model behavior on default and counterfactual tasks ( §5), including correlations between default and counterfactual performance, varying effectiveness of zero-shot chain-of-thought prompting (Kojima et al., 2023), and interactions between task-and instance-level frequency effects.Overall, we find that small variations on the default instantiations of tasks are challenging for models, and thus the success of existing LMs on standard benchmarks should not be considered as sufficient evidence for their possession of full general capacity for the target task.</p>
<p>We informally conceptualize each task as a function f w : X → Y that maps an input x ∈ X under a world model w ∈ W to an output y ∈ Y .World models encapsulate the conditions under which function evaluation takes place.For example, in Python programming, w might specify assumptions of Python such as indexing and operator precedence; in arithmetic, w could represent the set of conditions required for an arithmetic operation, such as the number base.We refer to the set of assumed default conditions, including but not limited to the base's being 10, as the default world, or w default .Intuitively, for any task, w default corresponds to the set of conditions underlying the majority of task instances in text corpora. 1raditional evaluations of machine learning models assess how closely a model's learned hypothesis h estimates f w by independently sampling training and test sets from the population distribution D fw , and only exposing the model to the training set for learning h.However, in datasets of scraped web text, these evaluations are subject to potential data contamination issues (Brown et al., 2020;Dodge et al., 2021;Magar and Schwartz, 2022;i.a.).These issues may be more severe in recent LMs: the ever-growing pretraining datasets potentially expose the models to more evaluation instances, and the increasing sizes of recent LMs give them more ability to memorize these instances (Carlini et al., 2020;Magar and Schwartz, 2022).</p>
<p>We hence consider another dimension of generalization: generalization to new task variants in counterfactual worlds w cf , instead of new inputs x.This allows us to measure the extent to which a model's f w default performance is specific to w default or attributable to a general implementation of the task f . 2 For arithmetic, a possible w cf would be one that was the same as w default but assumed a base other than base-10.We expect a model with general arithmetic ability to perform similarly in other bases.</p>
<p>We emphasize that our goal is not to find counterfactual world models that are completely outside the realm of human experience.Base-9 addition, for example, is not a novel concept.Nor do we aim to guarantee that counterfactual world models are unobserved in a pretraining corpus.Instead, counterfactuals are simply defined as variations on the default conditions for a task.</p>
<p>Concretely, we assess an LM's task performance with 0-shot prompting.We specify the task f , the test instance x, and the world model w in a prompt, parse the LM's output, and compare it to the ground-truth label.We denote the LM's implementation of f w for a given instance x to be, h(f, w, x) = arg max
y ′ P LM y ′ | prompt f (f, x), prompt w (w) ,
where the arg max is computed with an approximate decoding procedure and prompt f and prompt w are prompt templates that describe tasks and world models respectively.For each task, we devise one or more w cf that deviate from the default world (i.e., the default task conditions).We evaluate both h(f, w default , x) and h(f, w cf , x) via task-specific metrics.If we control f w (x) to be similarly hard between w default and w cf , we can attribute the performance difference to an LM overfitting to the default instantiation of the task.</p>
<p>Counterfactual Comprehension Check</p>
<p>One potential confounder is that an LM may be failing at a particular counterfactual task by failing to understand the prompt component that specifies the counterfactual conditions, i.e., prompt w (w cf ).That is, an LM might still be reasoning in w default and completely ignore the instructions.While this would still be a failure of the LM, it does not necessarily represent a failure to perform the counterfactual task variant.We control for this by designing task-specific counterfactual comprehension checks (CCCs) that test an LM's surface understanding of the specified counterfactual world.</p>
<p>For each (default, counterfactual) task pair, we introduce another control task g w with input x ′ and output y ′ that is much simpler than f w but still allows for the discrimination of w default from w cf (i.e., g w cf (x ′ ) ̸ = g w default (x ′ )).A high performance of P LM (y ′ | prompt g (g, x ′ ), prompt w (w cf )) would indicate that prompt w is effective at making the LM perform a task in w cf .In the arithmetic example, for a base-9 counterfactual world, we use the same prompt w (base-9) to specify the counterfactual world, and check that it facilitates an understanding of w = base-9 by asking what the next integer after x ′ is.If, for example, it consistently carries over digits greater than 8 and does not carry over otherwise, this would show the effectiveness of prompt w (base-9).Our CCC designs are heuristic: as with control tasks in the probing literature (Hewitt and Liang, 2019), we rely on intuition to craft a g w that is "simpler" than f w .3</p>
<p>Tasks</p>
<p>In this section, we give a quick overview of the tasks we consider.See §A for the full description of each task and §B for all the prompts used.</p>
<p>Arithmetic</p>
<p>Modern LMs have been shown to possess basic numerical reasoning abilities (Lewkowycz et al., 2022), with Brown et al. ( 2020) even reporting near-perfect GPT-3 accuracy for two-digit additions.On the other hand, Razeghi et al. (2022) find that LMs perform significantly better on operations involving numbers that occur more frequently in the pretraining data, and Li et al. (2023d) show that symbol replacement affects the mathematical ability of BERT (Devlin et al., 2019)-like models; both findings point to overfitting and memorization effects.We consider the same two-digit addition task, the simplest arithmetic task in Brown et al. (2020), but inspect a model's accuracy in different bases.We use base-8, 9, 11, and 16 as the counterfactual setup which are natural generalizations to base-10 arithmetic.These bases were chosen to control for task difficulty (see §7.1 for a discussion) and also to test for how relatively uncommon (9 &amp; 11) and common (8 &amp; 16) bases affect performance (see §5.1 for an analysis).To ensure the model understands the different bases, the CCC evaluates the successor relation under each base.</p>
<p>Programming</p>
<p>Even without explicit pretraining on large amounts of code, LMs have been found to possess decent coding ability (Brown et al., 2020).The inclusion of large code corpora in LM pretraining (Gao et al., 2021;Chowdhery et al., 2022;Touvron et al., 2023;i.a.) further improves this capability in recent LMs, with ChatGPT sometimes outperforming state-ofthe-art approaches for bug fixing (Sobania et al., 2023).Nevertheless, Miceli-Barone et al. (2023) show that GPT-3 and related models are fragile under identifier swaps in programs, suggesting that these models may only possess a shallow understanding of code.Here, we inspect an LM's programming ability through a deeper counterfactual perturbation: contrary to the traditional 0-based indexing in Python, we instruct the LM to evaluate or generate programs under a fictional language, ThonPy, that uses 1-based indexing but is otherwise identical to Python.1-based indexing is a common assumption for other programming languages such as MATLAB and R and hence provides a fair testbed.We evaluate the LM's performance using the HumanEval dataset (Chen et al., 2021).The CCC here involves the same program execution task but on much simpler inputs, such as simple list indexing, that do not involve deeper reasoning.et al. (2023) distinguish between two types of LM capabilities: formal competence that encompasses the knowledge of language, and functional competence which involves using language, potentially combined with extralinguistic capacities, to interact with the world.While the other tasks we investigate in this paper assess a model's functional competence, we also include an evaluation on formal competence.We revisit the attested syntactic knowledge of LMs (Yu et al., 2020;Linzen and Baroni, 2021;Ettinger, 2020;Pimentel and Cotterell, 2021;Belinkov, 2022;Lasri et al., 2022;i.a.) by considering a meta-linguistic task (Beguš et al., 2023;Hu and Levy, 2023;i.a.): evaluating LMs in synthetic versions of English with different word orders from English's subjectverb-object (SVO) ordering.We ask the LM to identify the main subject and the main verb of a sentence under both the original and counterfactual orders, where the latter is obtained from manipulating dependency trees (Ravfogel et al., 2019).The CCC requires the model to revert simple reordered sentences to the original SVO ordering, equivalent to identifying these elements in a sentence.</p>
<p>Basic Syntactic Reasoning</p>
<p>Mahowald</p>
<p>Natural Language Reasoning with</p>
<p>First-Order Logic</p>
<p>We next consider a deductive reasoning task that is still based on natural language.Logical reasoning is a prerequisite ability for many complex tasks (McCarthy, 1959) and has been the focus of much recent work (Clark et al., 2020;Tafjord et al., 2021;Saparov and Mitchell, 2022;Saparov and He, 2023;i.a.).Nevertheless, LMs struggle with reasoning with premises that are inconsistent with common sense (Dasgupta et al., 2022;Yu et al., 2023;Tang et al., 2023).Here, we undertake a similar study from the perspective of counterfactual analysis to disentangle the effect of common sense from a model's actual logical reasoning capability.</p>
<p>Following prior work, we evaluate in an entailment format and ask LMs if a series of premises entails a conclusion.We use the FOLIO dataset (Han et al., 2022) most of whose premises are consistent with common sense, and manually rewrite them to violate common sense.We study if LM performance is affected by the truthfulness of the premises under which they operate.The CCC directly asks the model if the original or post-rewrite premise is true, when presented both as options.</p>
<p>Spatial Reasoning</p>
<p>A major debate around LMs is whether grounded representations of meaning can be learned from form alone (Bender and Koller, 2020;Piantadosi and Hill, 2022;Mollo and Millière, 2023).Studies have shown that LMs can learn meaningful world representations through text-only training (Abdou et al., 2021;Li et al., 2023c;Jin and Rinard, 2023).In particular, Patel and Pavlick (2022) find that LMs learn representations of spatial relations and cardinal directions that can be aligned to grounded conceptual spaces with few-shot demonstrations.</p>
<p>We similarly investigate an understanding of cardinal directions, but instead of evaluating whether a model can induce structured conceptual spaces, we ask if it can apply conceptual spaces to reason about the locations of objects.Specifically, we ask an LM for the coordinates of objects whose positions are described using cardinal directions, under a conventional 2D coordinate system (e.g., where east corresponds to (1, 0)) versus coordinate systems with swapped, rotated, and randomly permuted axes.We expect a robust representation to not be sensitive to such transformations.The CCC involves asking the model to directly output the counterfactual cardinal directions.</p>
<p>Drawing</p>
<p>Despite being trained on only textual data, LMs have been shown to be able to structure their representations of perceptual concepts such as size and color (Abdou et al., 2021;Patel and Pavlick, 2022;Zhang et al., 2020;Ilharco et al., 2021;i.a.) in a way that credibly mirrors the physical world.Recent LMs can even generate plausible drawings of objects using code such as TikZ and SVG (Bubeck et al., 2023;Zhang et al., 2023c).We evaluate the visual understanding of LMs by asking them to generate code for drawing various objects in the Processing language, which Sharma et al. (2024) found the LMs to be more adept in.Psychological studies have shown that humans have the ability to rotate mental representations of objects (Shepard and Metzler, 1971;Vandenberg and Kuse, 1978).For the counterfactual settings, we similarly ask the LM to generate code that draws the same object, but rotated or vertically flipped.We disallow the use of functions such as rotate to prevent shortcut solutions (see §7.2 for further discussion).As with the spatial reasoning task ( §3.5), an ideal model should be robust to these settings.For the CCC, we ask the model to draw a straight line at the top of the canvas in addition to the object; a flipped/rotated line thus signifies an understanding of the transformations.</p>
<p>Music</p>
<p>Recent work has shown the potential of large-scale models for music infilling (Huang et al., 2019a,b) and generation (Agostinelli et al., 2023;Copet et al., 2023;Ren et al., 2020).Bubeck et al. (2023) show that even a text-only LM with no musicspecific pretraining exhibits some musical abilities, including understanding musical structure and manipulating melodies.We investigate the extent of LMs' musical abilities through two tasks.</p>
<p>In the chord placement task, we evaluate whether LMs can provide the correct chord fret placements for string instruments with standard or altered string tunings.The altered tunings, known as scordatura, are typical in music and are used to evoke a specific sound or effect (e.g., enabling heavier, deeper sound in metal music).We evaluate LMs using an existing database 4 that includes chords for guitar and ukulele.In the counterfactual setting, we instruct LMs to provide fret placements for a special guitar/ukulele where one or two of the strings are altered.For guitar, we include drop-D tuning, a popular alternative guitar tuning that allows us to investigate whether the frequency of counterfactual tunings affects results (see §5.1).To check whether the model has understood the tunings, we ask for the first three notes on each string (including open string) as the CCC.</p>
<p>In the note retreival task, we evaluate whether LMs can retrieve notes from famous melodies (e.g., "Twinkle Twinkle Little Star").The process of rewriting melodies in different keys, referred to as "transposition," is common in music (e.g., to accommodate the ranges of different singers or instruments).We evaluate LMs' musical abilities under transpositions by prompting them to retrieve the n-th note in a melody in either its canonical key (default setting) or a different key (counterfactual setting).We ask the LMs to retrieve the n-th note of the scale of the given key as the CCC.</p>
<p>Chess</p>
<p>Chess playing has long been regarded as a testbed for AI (Silver et al., 2017;Tomasev et al., 2020), and modern LMs have exhibited abilities that imply an understanding of chess rules (Srivastava et al., 2023;Du et al., 2023).We test this understanding by asking for the legality of a 4-move opening.In the counterfactual setting, we swap the initial positions of knights and bishops-a setup present in a real-world chess variant "Chess 960"-and similarly ask LMs for opening legality under this new starting configuration. 5 We ask for the starting positions of the knights and the bishops as the CCC.</p>
<p>3.9 SET Game SET is a popular card game where each card has 4 attributes with 3 different values for each attribute:</p>
<p>• color: (red, blue, green)</p>
<p>• shape: (diamond, oval, squiggle)</p>
<p>• shading: (solid, shaded, open)</p>
<p>• number: (1, 2, 3) In each round, a player finds a SET of 3 cards in a 12-card board whose values for each attribute are either all the same or all unique.This game has been thoroughly studied in computer science, from the perspective of coding theory and combinatorics (Davis and Maclagan, 2003), linear algebra (Coleman and Hartshorn, 2012), and complex-ity theory (Chaudhuri et al., 2003).We suspect this popularity makes it susceptible to overfitting by LMs and investigate this possibility.We ask the LM to identify the card on a board that completes a 3-card SET with two given cards.In the counterfactual setup, we invert the rule for the number attribute, requiring its value to be mixed, in other words, neither all the same nor all unique.For the CCC, we ask the model for the validity of a SET under the original rule and the counterfactual rule.</p>
<p>Results</p>
<p>For each task, we evaluate GPT-4 (gpt-4-0314;OpenAI, 2023), , Claude (claude-v1.3;Anthropic, 2023), and PaLM-2 (text-bison-001;Anil et al., 2023).As these are closed-source models, we do not have any information regarding their size, architecture, and pretaining details. 6 We note that the largest PaLM-2 model is not publicly accessible, and we can only test the second-largest version.For each task, we experiment both with and without encouraging the model to reason step by step, by adding the phrase "Let's think step by step." in our prompts (Kojima et al., 2023;Reynolds and Mc-Donell, 2021).Following Kojima et al. (2023), we refer to this step-by-step setup as zero-shot chainof-thought prompting (0-CoT; Nye et al., 2021;Wei et al., 2022).We include all prompts in §B.</p>
<p>Figures 2 and 3 show our results.§C contains the numeric version.We see a consistent pattern where LMs perform substantially worse on the counterfactual task variants, both with and without 0-shot CoT.For most cases, LMs exhibit an above-random counterfactual performance, suggesting some degree of the targeted ability.However, when the CCC accuracy is high, as is usually the case for GPT-4 and in select settings for other models too, the gaps in default vs. counterfactual task performance demonstrate limitations in their abstract capacity to solve the target task.When the CCC accuracy is lower, the failure of counterfactual world comprehension would be a confounder to this conclusion, but often the gaps are so large (sometimes even dropping from near-perfect to near-zero, such as for arithmetic) that they are nonetheless strongly</p>
<p>Drawing</p>
<p>Object sketch generation</p>
<p>Chords: Guitar</p>
<p>Fret placement for chords</p>
<p>Chords: Ukulele</p>
<p>Fret placement for chords</p>
<p>Arithmetic</p>
<p>Two-digit addition Accuracy (%)</p>
<p>Code Exec.Code Gen.</p>
<p>Python program evaluation</p>
<p>Python program generation</p>
<p>Index From</p>
<p>Logic</p>
<p>First-order logic deduction in natural language Accuracy (%)</p>
<p>Basic Syntax</p>
<p>Main subject and verb identification    2).CCC is the counterfactual comprehension check ( §2.1), but when applicable, we report it for the default setting too.Random performance is marked whenever nontrivial.PaLM-2 here is not the largest version ( §4).The CCC for code execution/generation are identical.For spatial reasoning, we average the results from all rotation degrees.Counterfactual performance is consistently lower than the default task performance, while CCC is usually high.§C reports numeric results.The blue and orange bars represent the default and counterfactual conditions respectively, either with or without 0-shot chain-of-thought (0-CoT).CCC is the counterfactual comprehension check ( §2.1), but when applicable, we report it for the default setting too.Random performance is marked whenever nontrivial.PaLM-2 here is not the largest version ( §4).Counterfactual performance is consistently lower than the default task performance, while CCC is usually high.§C reports numeric results.
Accuracy (%) Word Order S V O S O V V S O V O S O V S O S V S V O S O V V S O V O S O V S O S V 0 50 100 S V O S O V V S O V O S O V S O S V S V O S O V V S O V O S O V S O S V 0 50 100 S V O S O V V S O V O S O V S O S V S V O S O V V S O V O S O V S O S VTuning G C E A F C E A A C E A B C E A B E E A G C E A F C E A A C E A B C E A B E E A 0 50 100 G C E A F C E A A C E A B C E A B E E A G C E A F C E A A C E A B C E A B E E A 0 50 100 G C E A F C E A A C E A B C E A B E E A G C E A F C E A A C E A B C E A B E E A 0 50 100 G C E A F C E A A C E A B C E A B E E A G C E A F C E A A C E A B C E A B E E AE A D G B E D A D G B E F A D G B E E B D G B E E C D G B E E C F G B E E A D G B E D A D G B E F A D G B E E B D G B E E C D G B E E C F G B E 0 50 100 E A D G B E D A D G B E F A D G B E E B D G B E E C D G B E E C F G B E E A D G B E D A D G B E F A D G B E E B D G B E E C D G B E E C F G B E 0 50 100 E A D G B E D A D G B E F A D G B E E B D G B E E C D G B E E C F G B E E A D G B E D A D G B E F A D G B E E B D G B E E C D G B E E C F G B E 0 50 100 E A D G B E D A D G B E F A D G B E E B D G B E E C D G B E E C F G B E E A D G B E D A D G B E F A D G B E E B D G B E E C D G B E E C F G B E0/ 0-CoT CCC Random S V O S O V V S O V O S O V S O S V S V O S O V V S O V O S O V S O S V0
indicative of non-transferable, default-conditionspecific implementations of the original task.The fact that the LMs sometimes cannot evaluate the CCC well under the counterfactual conditions, but can do so under the default conditions (e.g., for arithmetic, programming, drawing, etc.) itself also points to overfitting to the latter.</p>
<p>Analysis</p>
<p>We now investigate how a variety of factors affect the default and counterfactual performance trends that we observed in §4.Unless otherwise specified, we only consider GPT-4 with 0-shot CoT, which has the strongest performance in our results above.</p>
<p>"Commonness" of Counterfactual Conditions</p>
<p>Our counterfactual worlds are not designed to be completely alien to the LMs but only less common than the assumed default case.In this sense, the counterfactual-ness of these worlds is relative, and here we take a more nuanced look at how the commonness of these counterfactual conditions affects the default-counterfactual performance gap.For example, in the arithmetic task, all models perform better in bases 8 and 16, likely due to their relative abundance compared to bases 9 and 11.In spatial reasoning, the smallest counterfactual per-formance degradation is usually from when the north and south directions are swapped-even exceeding the default task performance for PaLM-2potentially because some programming libraries use an inverted y-axis, such as matplotlib (Python), ggplot (R), and D3 (JavaScript) (see §A.5).For chord fingering, the common alternative drop-D tuning of guitars (DADGBE) leads to the highest counterfactual performance for GPT-4.These correlations between the counterfactual performance and the commonness of the counterfactual worlds paint a more fine-grained picture than a binary default versus counterfactual distinction and point to a memorization-like effect where the models perform better under more common conditions.</p>
<p>Proximity between Default and Counterfactual Conditions</p>
<p>Another axis along which the counterfactual worlds differ is in their proximity to the default conditions.For example, for the different arithmetic bases, bases 9 and 11 are closer to base 10, but less common than bases 8 and 16.While the defaultcounterfactual gap is most affected by commonness for the arithmetic task, for the guitar and ukulele tunings (other than the drop-D tuning), the LM performance generally decreases monotonically with increasing distance from the original tunings.The FOLIO dataset (Han et al., 2022) enables an-</p>
<p>Coefficient Value</p>
<p>Figure 5: Logistic regression coefficients of features that predict whether an LM correctly predicts the label of an instance."Concl.Truth.Match" is a binary feature that is 1 iff the instance label matches the (LM-believed) truthfulness of the conclusion.The 95% confidence intervals are also shown.LMs tend to predict more correctly when there are more true premises, when the instance label matches the conclusion truthfulness, but less correctly with more false and unknown premises.</p>
<p>other analysis of how proximity to the default conditions affects LM performance, without counterfactual perturbations.This dataset was constructed to mostly follow common sense, with premises and conclusions deemed true in the real world.But this is not always the case, with premises like "John can make meals which are popular at the party," whose factuality cannot be determined alone.</p>
<p>We evaluate how the distance between the (LMbelieved) real world and the world state described by the premises (occasionally counterfactual to the LM) influences the LM's performance by training a predictive model given features approximating this distance.For each test instance, we ask the LMs whether the premises and conclusion are true, false, or uncertain.We train a logistic regression model to predict LM correctness on each test instance, using as features the total number of premises in an input, the proportion of the premises that are true/false/uncertain, as encoded by the LM, as well as whether the LM-predicted truthfulness of the conclusion matches the label of the instance.</p>
<p>Figure 5 shows the learned coefficients of these features, as well as their 95% confidence interval bootstrapping with 1,000 iterations (Efron and Tibshirani, 1993).Ideally, a robust model should predict solely based on symbolic deduction and extralinguistic truthfulness information should not affect its accuracy.In other words, these features should all have coefficients 0 and have no predictive power with respect to the model's correctness.However, all LMs predict more correctly with more realistic (true) premises, and when the conclusion's LM-predicted truthfulness matches the label (indicating a tendency to predict the label solely based on the conclusion, ignoring premises).On the other hand, they perform worse when there are more false or uncertain premises.Most of these trends are statistically significant.This means that the reasoning ability of LMs is affected by the distance between the (LM-believed) real world and the world state under which the LMs are expected to reason.</p>
<p>Overall, these results show that LMs tend to perform better on task variants that are closer to the default instantiation of a task.</p>
<p>Relationship between Default vs. Counterfactual Performance</p>
<p>Recalling our formalization h LM (f, w, x) in §2, the previous two subsections analyzed how the commonness of w and its proximity to w default affect the observed patterns.We now explore how the counterfactual performance correlates with the default task performance by varying the other three elements: the task f , the input x, and the LM.We first consider different task variants with various difficulties.For arithmetic, beyond 2-digit ad-dition, we also measure GPT-4's 3-and 4-digit addition performance (Figure 4a). 7For note retrieval from melodies, we use the index of the inquired note as the proxy for difficulty (Figure 4b). 8For SET, while our original task shows two cards and asks a model to find the missing one from a 3-card SET, we change the task to instead show one or none of the cards in a SET, while still requiring the model to identify the SET (Figure 4c).For all these task variants, we see a strong correlation between the original and counterfactual world performance.</p>
<p>We also see this effect when breaking down results by test instances x.In Figure 4d, we separate the chord types, and observe that the default task performance correlates with the counterfactual performance.Similarly, reexamining our main results in Figures 2 and 3, for most tasks, stronger models under default conditions are also stronger models under counterfactual conditions, and vice versa.Overall, these correlations mean that the default task performance can be a good indicator of its counterfactual performance, and hence we should not discount the utility of traditional evaluations. 9urthermore, despite our evidence of LMs' overfitting to the default task conditions, these correlations also signify some degree of reasoning that is transferable between the default and counterfactual worlds.This highlights that the question in our title, "Reasoning or Reciting?", is not a dichotomy, but rather they can co-exist in a continuum.For example, revisiting the arithmetic results with more digits (Figure 4a), in addition to the default-counterfactual correlation, we also see an effect of memorization: the base-10 performance decreases much more slowly than the other bases.When the input-output mappings are memorized, increased complexity would not affect the default task accuracy much; but when the counterfactual instances are not memorized, the task complexity should inversely correlate with model performance.</p>
<p>Occasionally, this default-counterfactual correlation trend is reversed.In the spatial reasoning task, for example, GPT-4 achieves the best accuracy under default conditions with 0-shot CoT, but it also suffers from the largest counterfactual performance degradation.PaLM-2 performs worse under default conditions, but is the most robust to counterfactual perturbations.An obvious possible explanation is that these models could be trained on different data, and are hence familiar with different conditions.Nevertheless, McKenzie et al. (2023), who found a similar trend but with respect to pretraining FLOPs and termed it "inverse scaling," also provided a memorization-based explanation: they observed that when a task contradicts with pretraining texts, similar to how our counterfactual conditions deviate from the default conditions in pretraining, larger LMs tend to rely on the pretraining text and, in turn, fail at the contradictory task.</p>
<p>0-Shot Chain-of-Thought Prompting</p>
<p>Consistent with prior findings (Chen et al., 2022;Dasgupta et al., 2022;i.a.), we generally observe 0-shot CoT to be helpful for most cases.There are, however, exceptions.For example, 0-shot CoT substantially hurts PaLM-2's addition performance in base-10 and 16, and consistently degrades GPT-4 and GPT-3.5'schord-playing performance for the default tuning.This may be due to a model pragmatically inferring that a task is more difficult than it actually is when explicitly asked to "think step by step", and this "overthinking" on simple tasks could lead to mistakes (Kojima et al., 2023).It is also possible that these are due to memorization: the model could have memorized the specific inputoutput mapping of a task, without understanding how to derive the output from the input, and when explicitly instructed to spell out that process, it makes more errors (Zhang et al., 2023b).</p>
<p>Few-shot Demonstrations</p>
<p>We study if additional demonstration examples using in-context learning (Brown et al., 2020) bridges the default-counterfactual gap.For the arithmetic task, we construct few-shot CoT prompts (Nye et al., 2021;Wei et al., 2022) and prepend up to 16 samples.As shown in Figure 6, while the gap is reduced, it is still substantial for bases 9, 11, and 16.Moreover, the accuracy improvement with more demonstrations plateaus towards 16-shot, suggesting that the default-counterfactual gap is unlikely to be eliminated by simply adding more demonstrations (at least for arithmetic). 10</p>
<p>Qualitative Analysis of Drawing Results</p>
<p>We conduct a qualitative error analysis on the drawing task and show some examples in Figure 7.We first note that GPT-4 successfully passes the CCC for these cases (see §3.6; but not displayed here), indicating that it understands the flip/rotation instructions.However, the objects in the counterfactual worlds are often not flipped or rotated.Even when they are transformed appropriately, the resulting drawing is often simplified or of worse quality (e.g., Unicorn, Cake).We also observed much more syntactically invalid programs in the counterfactual cases for GPT-3.5. 11These results indicate that even when a model can perform a task in the counterfactual setup, its capabilities are reduced.</p>
<p>Discussion</p>
<p>Do humans also perform worse with unfamiliar counterfactual conditions?It is possible that humans may have lower performance under the counterfactual conditions with a fixed time budget, but not necessarily when given ample time to reason and revise.Analogous to the classic competence/performance distinction in linguistics (Chomsky, 1965, §1.1), we hypothesize that humans have the competence to generalize to new task conditions, even though it may sometimes require sufficient execution budget to realize it as robust performance. 12In fact, there is increasing evidence from cognitive science that human reasoning is scaffolded by rich causal models of the world (Pearl, 1988;Lake et al., 2017 .In all cases, the CCC (not shown) passes.We show the original output, without flipping/rotating back as in our quantitative evaluation ( §A.6).For the counterfactual settings, GPT-4 either does not transform the objects as instructed (e.g., house and penguin) or struggles to draw meaningful objects (e.g., cake and unicorn).</p>
<p>2020; Wong et al., 2023), and that humans can intervene on these models to perform rapid and flexible counterfactual simulations (Lagnado et al., 2013;Gerstenberg et al., 2017Gerstenberg et al., , 2021)).However, stepping back, replicating or modeling human intelligence need not be a main goal of LMs in the first place, and human behavior is largely orthogonal to the desiderata we set for these models.</p>
<p>Is task-specific reasoning bad?It is not necessarily bad when solving familiar tasks, but an ideal system should also possess general reasoning abilities that, when prompted, can be used to generalize to novel situations.Our point is that memorization is an often-overlooked confounding factor in interpreting LMs' reasoning abilities.</p>
<p>Why do we care about counterfactual worlds?Wouldn't a model for only the default task instantiation be nonetheless useful?It is certainly true that such a model would still be useful.However, many of the counterfactual worlds that we investigate are not very distant so that model performance under them still bears utility.For example, addition in different bases is certainly useful for many applications.More generally, we are necessarily interested in the counterfactual tasks themselves; we are only interested in them insofar as performance on these tasks can serve as a measurable proxy for the generalizability of these models and their underlying reasoning capabilities.</p>
<p>Aren't the observed trends trivial?The default task variant is likely the most frequent during pretraining, so of course an LM performs better under it.Indeed, our results parallel the classic train-test gap in machine learning.However, an ideal learner with the right inductive biases should be able to structure their internal parameters and representations to implement general-purpose abstractions (e.g., the concept of addition), and use these abstractions to generalize to counterfactual conditions, analogous to physicists using mathematical abstractions to make predictions about universes that are substantially different from our own, or more generally to humans who can generalize to new stimuli in cognitive science studies (Lagnado et al., 2013;Gerstenberg et al., 2017Gerstenberg et al., , 2021)).Our study indicates that LMs trained on large text corpora, remarkable as they may be, are still quite susceptible to overfitting with frequency effects.</p>
<p>Can some more carefully designed prompts eliminate the default-counterfactual gap?This is always a possibility, and one that we can never tractably rule out.Nevertheless, given the consistency of the gap across our tasks (which use different prompts) and the 0-shot CoT setting, we believe that a prompt that completely bridges the defaultcounterfactual gap is unlikely.Our in-context learning experiment ( §5.5) further shows that while this gap could be reduced by more informative prompts, it is not fully removed.It would be interesting to apply more advanced prompting techniques (Wang et al., 2023a(Wang et al., , 2022a;;Yao et al., 2023;Sordoni et al., 2023;i.a.) to our counterfactual tasks.We considered 0-shot chain-of-thought in this work, which did not fully bridge the default-counterfactual gap, but we leave the exploration of these more recent prompting techniques to future work.</p>
<p>Limitations</p>
<p>Despite our attempt to devise novel counterfactual conditions to gauge an LM's "true" reasoning ability, it may not be precisely reflected by the counterfactual performance due to several factors.</p>
<p>Underestimation</p>
<p>For our main evaluations, we aim to construct counterfactual tasks that have the same difficulty as the default variants so that task difficulty does not confound our comparisons.This is not always possible-in fact, an objective difficulty measure may not even exist.One could, for example, argue that base-11 addition is harder than base-10 because it requires reasoning with one additional digit, or base-9 is harder than base-10 because on average the sums would consist of more digits.</p>
<p>Retrieving notes in melodies in different keys faces a similar issue.We expect similar retrieval difficulty under different keys if the model recalls a melody as a series of abstract relations in a scale and directly maps them onto notes in a target key.However, an alternative strategy would be to first retrieve the note in a canonical key and then transpose it to the desired uncommon key.This 2-step process is a natural one that is often employed by musicians.And with this strategy, the counterfactual task consists of 2 steps and is harder than (and requires first) completing the 1-step original task.The counterfactual setup thus introduces a confounder: low performance may be driven by the increased difficulty of the counterfactual task, rather than overfitting to melodies in their canonical keys, if models are employing two-step strategy.However, since both strategies are available to models and we do not prompt them to use a particular one, reliance on this two-step strategy may itself be indicative of overfitting to the original canonical keys.</p>
<p>Overestimation</p>
<p>We can never be certain of how rarely particular counterfactual conditions are encountered during pretraining.It is quite likely that there is text online that, for example, draws rotated versions of various objects used in our study.Consequently, the effect of overfitting could also manifest in our counterfactual conditions, and the default-counterfactual gap could actually be larger for some genuinely unseen conditions.</p>
<p>We also distinguish between two types of counterfactual perturbations.One type fundamentally affects the operation of the world model and necessitates an understanding of the counterfactual world to perform the task in it (e.g., arithmetic base or 1-based indexing 13 ).On the other hand, some perturbations are more superficial and may admit a shortcut where the model first figures out a simple mapping of the input back to the default conditions and performs the task (potentially leveraging instance-level memorization) under those.In some of our tasks, this mapping may be simple, such as the word replacements in the natural language logical reasoning task 14 ( §3.4) and the transformation functions for the drawing task ( §3.6), which could potentially be exploited by the models.We explicitly disallow this in our prompt for the drawing task (Table 7) but did not identify a good way to forbid this for logical reasoning, potentially accounting for its generally high counterfactual performance.</p>
<p>Finally, we reiterate from §4 that a non-perfect CCC accuracy does not allow us to perfectly tease apart counterfactual performance and a failure of counterfactual condition comprehension.But often the default-counterfactual gap is so prominent that it is still strongly suggestive of overfitting to the default conditions.Also, recall from §2 that the CCC itself is also a nontrivial task.For ThonPy, for example, the CCC also involves program evaluation, albeit with simpler statements that involve less reasoning, such as print("qrstu"[4]).We do not see an easy way to introduce ThonPy CCC that is entirely disentangled from program evaluation.This conflation would result in the CCC accuracy's being lower than what would reflect the model's understanding of the counterfactual conditions.</p>
<p>Related Work Evaluating Conceptual Structures in LMs.</p>
<p>Much prior work has investigated the extent to which LMs acquire a grounded understanding of the world through text-only training (Piantadosi and Hill, 2022;Zhang et al., 2020;Ilharco et al., 2021;Li et al., 2021;i.a.).These studies have generally found that conceptual structures of certain concepts (e.g., color, size) often plausibly mirror those of the grounded world (Abdou et al., 2021;Patel and Pavlick, 2022;Mollo and Millière, 2023).As in our study, these studies are a test of 14 To be more concrete, imagine that a model memorizes an instance with nine premises on dogs involving complex logical relationships, and that it entails a given conclusion.For the counterfactual instance, we replace the word "dogs" with another object, say "headphones," to make the premises no longer factually true.Instead of performing the reasoning over premises with headphones such as how they are, counterfactually, the cutest creatures, a model could identify the mapping "dogs" → "headphones", revert it (i.e., replace all "headphones" back to "dogs"), and perform the task under the default common-sense-complying conditions.generalization-such structures would not manifest if the concepts were memorized in a one-hot-like manner.But our evaluation differs in that it targets the reasoning process instead of the generalization to new concepts or conceptual structures (Kondo et al., 2023).While prior work identified that the latter is embedded in LMs, we found that they do not fully learn the former.</p>
<p>Causal Analysis.Our counterfactual perturbations can be informally viewed as interventions under a causal inference framework (Pearl, 2009).This relationship has been explored in machine learning and NLP for commonsense reasoning (Kıcıman et al., 2023), interpretability (Elazar et al., 2021;Geiger et al., 2021Geiger et al., , 2022)), spurious correlation detection (Veitch et al., 2021;Eisenstein, 2022), fairness (Kusner et al., 2017;Nabi and Shpitser, 2018), etc.Under this perspective, the failure of generalization to counterfactual worlds that we observe in LMs can be viewed as a failure to robustly learn the causal effects of world states on our evaluated tasks.</p>
<p>Counterfactual Evaluation."Counterfactuals" is an informally-used term in NLP and has been used to refer to different types of perturbations.One line of work concerns counterfactuals to a certain event or situation that is still licensed in a default world model (Qin et al., 2019(Qin et al., , 2020;;Yang et al., 2020;Frohberg and Binder, 2022;i.a.), in contrast to our counterfactual world states that deviate from the default.Qin et al. (2019) and Frohberg and Binder (2022) found that GPT-3 and earlier models struggle with consistently reasoning under this type of counterfactual conditions, while Kıcıman et al. (2023) observed more recent LMs to achieve higher counterfactual reasoning accuracy.Another body of work examines the robustness of model predictions using counterfactual data (Kaushik et al., 2020(Kaushik et al., , 2021;;Gardner et al., 2020).More similar to our study, Li et al. (2023b) showed that while the LMs they investigated seem to be able to perform some reasoning in counterfactual worlds, this is largely affected by superficial lexical cues.Our results reveal that more recent LMs still exhibit such difficulties.</p>
<p>Conclusion</p>
<p>Through our counterfactual evaluation on 11 tasks, we identified consistent and substantial degradation of LM performance under counterfactual con-ditions.We attribute this gap to overfitting to the default task variants, and thus encourage future LM analyses to explicitly consider abstract task ability as detached from observed task performance, especially when these evaluated task variants might exist in abundance in the LM pretraining corpora.Furthermore, insofar as this degradation is a result of the LMs' being trained only on surface form text, it would also be interesting future work to see if more grounded LMs (grounded in the "real" world, or some semantic representation, etc.) are more robust to task variations.</p>
<p>A Full Setups</p>
<p>Unless otherwise specified, we use temperature=0 when sampling from the LMs.</p>
<p>A.1 Arithmetic</p>
<p>We randomly sample 1,000 two-digit addition expressions and evaluate them in bases 8, 9, 10, 11, and 16.Each base is sampled separately-for bases other than base-10, we make sure all expressions evaluate to a different result in that base compared to base-10 so that these expressions discriminate between the bases.To ensure the LMs understand these bases, we design the CCC to ask the model what the number following a given number is.We want the model to know when to carry over and when not to, so we take the 100 smallest numbers in the given basis that ends with the maximum digit in that base, and 100 that end with 0.</p>
<p>A.2 Programming</p>
<p>We use the HumanEval dataset (Chen et al., 2021) which has short Python programs and is commonly used to assess the coding ability of LMs (Bai et al., 2022;Xu et al., 2022;Wang et al., 2023b;i.a.).It was designed as a code-generation dataset, where a model writes a function from a specification and is evaluated against test cases with input-output pairs.Different from our other tasks, we follow prior work (Touvron et al., 2023;Wang et al., 2023b) and (1) use temperature 0.1 when evaluating pass@1 and 0.8 for pass@10, ( 2 In Figure 2, we only show the performance on the subset of HumanEval where a 1-based execution of the ground-truth program fails the unit tests.These are the instances that distinguish between 0-and 1-based indexing.We also report results on the full HumanEval dataset in Table 21.</p>
<p>We also consider another setup-code execution, where we give the LM the ground-truth program and ask the LM for the output of the test cases given the input.We remove four programs in HumanEval that are not compatible with this format (ID: 32, 38, 50, and 53), only for this execution task.Because the program would have a different functionality under 1-based indexing, we remove the docstring that is the function description, and also rename the function to the uninformative function, to avoid confusing the LM.Some programs also become invalid under 1-based indexing, specifically, those that perform any indexing using 0. We remove all test cases that involve indexing with 0 and programs that do not have test cases left after this removal.150 programs and 969 test cases remain.Some of these test cases may not distinguish between 0-and 1-based indexing.So for our main task (i.e., not CCC), we only consider test cases whose outputs are different under 0-vs.1-based indexing, and there are 113 of them.</p>
<p>Because we use the same prompt to indicate the counterfactual conditions for both code generation and execution, and because we want to maintain comparability with prior work for the former, we only include CCC in the execution setup.We believe they reflect the LMs' understanding of 1-based indexing in the generation setup too.We ask the LM for the output of simple tests about 1-based indexing such as "qrstu"[4] and "qrs"[:2].They do not require sophisticated reasoning under the counterfactual conditions and yet are sufficient to discriminate between the default and the counterfactual conditions.We append 5 such checks after each of the 150 programs, totaling 750 CCC.</p>
<p>For the execution task, we do not consider PaLM-2, because it only has a maximum of 1,024 output context length and leads to truncated, unparseable results for most test instances, especially under 0shot CoT.</p>
<p>A.3 Basic Syntactic Reasoning</p>
<p>We follow Ravfogel et al. (2019) and create synthetic variants of English with all six orderings of the subject, verb, and object.Given a dependency tree of a regular English sentence, we alter the order of subject and object nodes with respect to the corresponding verb.The subtrees rooted at subject or object nodes are moved as a whole, whereas other non-core dependent nodes (e.g., prepositional phrases) are kept in the original positions.We use 100 sentences from English Penn Treebank (Marcus et al., 1993), and convert the original phrasestructure trees into Universal Dependencies (Nivre et al., 2016) using the Stanford converter (Schuster and Manning, 2016).</p>
<p>Our task is to identify the main verb and the main subject of a sentence.We only choose sentences where the main subject contains a single word.Ravfogel et al. (2019)'s data generation procedure sometimes results in sentences in the SVO order to be unnatural English sentences.To eliminate this complexity, we retain only sentences whose SVO variant according to Ravfogel et al. (2019)'s data generation procedure is identical to the original English sentence.</p>
<p>We designed the CCC to assess how well LMs understand the instruction that explains the difference of word orders in the counterfactual settings.We synthetically generate 100 simple three-word sentences (e.g., "anna saw john") in five counterfactual word orders (e.g., "anna john saw" in SOV), and ask LMs to reconstruct the original English sentences in SVO order.Conceptually, this is equivalent to asking the model to identify the subject, verb, and object in the perturbed order, but using a format that is more familiar to the LM.</p>
<p>To generate the simple sentences for the CCC, we designed a simple context-free grammar where the subject and the object are sampled from the vocabulary of person names, and the verb is sampled from the set {saw, loves, calls, knows, sees}.A key feature of the sentences generated from this approach is their retained plausibility when the subject and object are interchanged.This means that given a counterfactual sentence (e.g., "anna john saw"), there are two natural English sentences as candidates for reconstruction (i.e., "anna saw john" and "john saw anna").Due to this inherent ambiguity, LMs cannot default to the heuristic of treating the synthetic sentence as bagof-words and then reconstructing the most natural ordering of those words in real English.The random baseline chooses a random noun as the main subject and a random verb as the main verb.</p>
<p>A note on CCC results.The results for this task are shown in Table 22.Generally, the models pass our crafted CCC challenge with decent accuracy, but we observed that, in a few cases, the LMs are confused by the reconstruction ambiguity explained above.GPT-3.5 and Claude fail in the OVS settings where they often directly copy the original sentence-e.g., instead of reconstructing "anna saw john" to "john saw anna", they simply copy the original sentence "anna saw john" as the output.Similarly, PaLM-2 often incorrectly reverses the subject and object in the SOV and VSO settings-e.g., instead of reconstructing "calls tom lucas" to "tom calls lucas", it outputs "lucas calls tom".</p>
<p>A.4 Natural Language Reasoning with</p>
<p>First-Order Logic</p>
<p>We use the FOLIO dataset (Han et al., 2022) that contains premises most of which are consistent with common sense and are hence amenable to our counterfactual study.We use the full dataset, combining the training and development sets for a total of 1,204 instances, for the logistic regression analysis in §5.1.But for our counterfactual study, automatically altering the premises to violate common sense is not trivial, so one author manually rewrote the premises of a subset of 81 instances to be counterfactual, and another author verified the rewrite.Considering the analysis in §5.1, we chose this subset by including every instance with premises all of which GPT-4 believes to be true and whose conclusion whose GPT-4-believed truth value matches the entailment label.</p>
<p>We explicitly instruct the model to use no common sense or world knowledge ( §B), thereby requiring symbolic reasoning.For the CCC, we ask the model if the unaltered or the altered premise is true, when both are presented as options, and expect the latter.</p>
<p>While the FOLIO dataset has a public release, the authors have made subsequent updates which, at the time of this paper, have not been made public.We hence do not release the LM interaction data for this task, and use a fictional example in Table 5.</p>
<p>A.5 Spatial Reasoning</p>
<p>We ask the LM for the coordinates of objects in a room.We randomly sample 100 rooms, each with 3 different objects placed in 3 different cardinal directions specified using unit vectors (out of north (0, −1), south (0, 1), east (1, 0), and west (−1, 0) as the default conditions).Though using a downward-facing y-axis as the default condition may be counter-intuitive, it is natural when drawing top-to-bottom and is the convention in most image processing libraries such as OpenCV (Python), Pillow (Python), and Processing (Java, JavaScript, Python), as well as graphic design applications such as Adobe Illustrator.We believe this system is the most often encountered during LM pretraining.However, other libraries with an upward-facing yaxis also exist, such as matplotlib (Python), ggplot (R), and D3 (JavaScript).</p>
<p>For the counterfactual setting, we alter the direction-unit vector mapping, and ask for the object coordinates in the new system.We con-sider two direction-swapped worlds (north-south and east-west), three rotated worlds (by 90°, 180°, and 270°), and a randomly permuted world.We evaluate the relative positions of objects and report the instance-level accuracy that requires all 3 objects in a room to be located correctly as the main metric.The random accuracy is around 16.7%. 15e also report the object-level accuracy in Table 24.As the CCC, we make sure that the LM understands the permuted world by asking it to also specify the coordinates of the unit vectors representing the 4 cardinal directions in the output.</p>
<p>A.6 Drawing</p>
<p>We choose 100 objects from five Emoji16 categories: activity, travel &amp; places, animals &amp; nature, food &amp; drink, and objects.Since LMs cannot generate images at the pixel level, we use code as an intermediate abstraction for sketch generation.We do our best to select objects that are easy to draw using code, verified by multiple authors.We consider the Processing language for our experiment which supports a variety of shapes and colors and is widely used in visualization.Our initial experiments found this language to achieve the best drawing performance compared to other graphics and image processing frameworks, including TikZ, SVG, and matplotlib.</p>
<p>For the counterfactual settings, we ask the LMs to draw the same object, but vertically flipped (i.e., upside-down), or rotated by 90°or 180°.We also ask the LMs to avoid using any transformation functions such as rotate and scale to avoid shortcuts.Before our quantitative evaluation, we flip/rotate back the generated drawing.</p>
<p>We use human evaluation by asking human annotators to determine whether the drawing matches the object.We instruct the annotators to consider orientation as part of correctness and for objects that have a canonical orientation, they must be drawn in that orientation.We average the results over 4 annotators.We also show a breakdown of accuracy depending on whether an object has a canonical orientation or not, as judged by the annotators, in Table 26.In addition, we consider multi-class classification accuracy using CLIP (Radford et al., 2021) as an automatic metric, where we ask CLIP to classify the drawing into our 100 categories in a 0-shot fashion.We include the CLIP multi-class classification accuracy in Table 25.We note that the accuracy of the CLIP model for our setup is not guaranteed: first, our generated sketches may be distributionally different from the predominantly photorealistic images in CLIP's training data; also, CLIP might be insensitive to the object's orientation, but that distinguishes between our default and counterfactual settings.Therefore, to verify the reliability of this automatic evaluation, we randomly sample 10 objects for each model and for each default/counterfactual setting, and perform human evaluation on the 240 generated images.We find that CLIP's judgment aligns with human annotators' 84% of the time, suggesting the reliability of this evaluation.</p>
<p>For this task, we do not consider PaLM-2 due to its limited context length.Our preliminary experiments also found PaLM-2 to struggle in generating parseable Processing code, even in the default setting.</p>
<p>We construct the CCC baseline by requiring the LMs to additionally draw a line at the top of the figure and flip/rotate it as well.A successful flipping/rotation of the line, as judged by the annotators and verified in the generated code if necessary, demonstrates an understanding of the counterfactual world.</p>
<p>A.7 Music</p>
<p>A.7.1 Playing Chords on Instruments</p>
<p>We measure LMs' abilities to give correct fret placements for ukulele and guitar chords in an existing database. 17,18We include the following kinds of chords from the database: sus2 (suspended second chord), sus4 (suspended fourth chord), min triad (minor triad), maj triad (major triad), dim7 (diminished seventh chord), aug7 (augmented seventh chord), maj7 (major seventh chord), min7 (minor seventh chord), dom7 (dominant seventh chord), 5 (fifth interval), and 6 (sixth chord).</p>
<p>In the counterfactual setting, we instruct LMs to provide fret placements for a "special" ukulele or guitar where one of the strings is altered.We experiment with perturbations of different sizes: For guitar, we experiment with one-string changes by one note (EADGBE → EBDGBE; EADGBE → FADGBE), one-string changes by two notes (→ ECDGBE), and two string changes (→ ECFGBE).We also experiment with a one-string change that corresponds to a common alternate tuning of a guitar called drop-D tuning (→ DADGBE).For ukulele, we experiment with one-string changes by one note (GCEA → FCEA; → ACEA), one-string change by two notes (→ BCEA), and two-string changes by two notes (→ BEEA).The generated fret placements for a chord are considered correct if all and only the notes in the corresponding chord (e.g., C, E, G for a C major triad) are produced, irrespective of order.</p>
<p>As the CCC, we assess LMs' understanding of the given instrument's strings by asking them to identify what notes a given sequence of frets corresponds to; for the CCC, the sequences are either all fret 0, all fret 1, or all fret 2. We compute CCC accuracy at the fret level (as opposed to the sequence level).</p>
<p>A.7.2 Retrieving Notes of Famous Melodies</p>
<p>For 8 famous melodies, we prompt LMs to retrieve the n-th note in the melody, where n is between 1 and 7 (inclusive).In the counterfactual setting, we prompt the LM to do the same but in a different key.The list of melodies and keys we experiment with is below.</p>
<p>We use C Major as the key for songs as the default condition given its popularity for famous melodies like children's songs.We use other keys as the counterfactual keys. 19s the CCC, we assess LMs' understanding of the given keys by asking them to retrieve the n-th note of the scale of the given key.</p>
<p>Melodies: Twinkle Twinkle Little Star, Mary Had a Little Lamb, Happy Birthday to You, Somewhere Over the Rainbow, Row Row Row Your Boat, Old Macdonald Had a Farm, Itsy Bitsy Spider, London Bridge is Falling Down.</p>
<p>Counterfactual Keys: B# major, C# major, Db major, D major, D# major, Eb major, Fb major, E major, E# major, F major, F# major, Gb major, G major, G# major, Ab major, A major, A# major, Bb major, Cb major, B major.</p>
<p>A.8 Chess</p>
<p>We evaluate an LM's ability to understand chess rules by checking if it can determine whether a 4-move opening follows the rules of chess or not.In the counterfactual setting, we swap the position of bishops and knights on the board and evaluate the same task.For each setting, we randomly sample 400 unique chess openings via a procedural generation algorithm: 200 are legal for the default setting but not for the counterfactual setting, and vice versa for the other 200, ensuring a more balanced and fair classification problem.We represent the moves as the LM input using the PGN format, the standard for chess moves description.</p>
<p>For the CCC, we ask an LM for the starting positions of the four knights and four bishops on the board to make sure it understands the new initial board.For both the default and counterfactual settings, we ask for the positions of white knights, white bishops, black knights, and black bishops, totaling 8 pieces, and evaluate using accuracy.Since concluding the effectiveness of our counterfactual prompt using merely 8 CCC may not be statistically significant, we sample 15 LM responses using tem-perature=0.1 for asking about each piece.</p>
<p>A.9 SET Game</p>
<p>We synthetically generate SET boards, consisting of 12 cards, each with exactly one 3-card SET that satisfies the game rules in §3.9.We represent each card with a string representation, e.g., (3|open|red|diamond).In preliminary experiments, we tried to ask the LMs to find the SET directly, but found that they cannot perform this task well (see Figure 4c, "Number of Cards to Find"= 3).Therefore, in our main evaluation, we expose 2 cards in the SET and ask the LM to identify the missing one that completes the SET.</p>
<p>In the counterfactual setting, we invert the rule for the number attribute to require that two cards in the SET should have the same number but the other card should be different.For the CCC, we ask the model to verify the validity of a given SET instead of finding it.In each CCC instance, we either give a valid SET from the board, or 3 randomly sampled cards that do not constitute a valid SET.We ask the model to classify whether the given combination is valid or invalid.We note that our counterfactual perturbation ensures that the each SET cannot be simultaneously valid in the default setting and the counterfactual setting, and hence this CCC is discriminative between the two settings.</p>
<p>B Prompts</p>
<p>We provide the exact prompts that we used to query the LMs in Tables 1 to 17.For clarity, we give a concrete prompt that embeds a test instance, rather than a template.We explain minor design decisions in the respective captions.We do not use the system message field for any model.</p>
<p>C Raw Results</p>
<p>We show the numeric results in Tables 18 to 34.</p>
<p>Mode Prompt Test</p>
<p>You are a mathematician.Assuming that all numbers are in base-11 where the digits are "0123456789A", what is 59+37?{Let's think step by step, and }end the response with the result in "\boxed{result}".</p>
<p>CCC</p>
<p>You are a mathematician.Assuming that all numbers are in base-11 where the digits are "0123456789A", what is the next number after 11A? Do this by counting the few preceding numbers and completing the sequence.End the response with the result.</p>
<p>Few-Shot CoT</p>
<p>You are a mathematician.Assuming that all numbers are in base-11 where the digits are "0123456789A", what is 25+68?Let's think step by step, and end the response with the result in "\boxed{result}".We add the ones digits first.In base-11, 5+8=12.So the ones digit of the final sum is 2. We need to carry over the 1 to the tens place.Then we add the tens digits.In base-11, 2+6=8.Since we carried over the 1, 8+1=9.So the tens digit of the final sum is 9. Putting the digits of the final sum together, we get \boxed{92}....[optionally more demonstrations in the same format]...You are a mathematician.Assuming that all numbers are in base-11 where the digits are "0123456789A", what is 59+37?Let's think step by step, and end the response with the result in "\boxed{result}".</p>
<p>Table 1: Prompts for the arithmetic task.{Let's think step by step, and } is added only if 0-shot CoT is used (and the following e is capitalized without 0-shot CoT).We use the \boxed{result} syntax to wrap results because we found in preliminary experiments that the models tend to use this format even without this specification.The Few-Shot CoT prompt is used for the analysis in §5.5.</p>
<p>Mode Prompt Default</p>
<p>You are an expert programmer.What does the following code snippet in Python 3.7 print?```python def function(lst):</p>
<p>return sum ([lst[i] for i in range(1, len(lst), 2) if lst[i] % 2 == 0])</p>
<p>print ([function([4, 88])]) print ([function([4, 5, 6, 7, 2, 122])]) print ([function([4, 0, 6, 7])]) print ([function([4, 4, 6, 8])]) print ([list(range(3))]) print ([[4, 5, 6]</p>
<p>CF</p>
<p>You are an expert programmer who can readily adapt to new programming languages.There is a new programming language, ThonPy, which is identical to Python 3.7 except all variables of the <code>list</code>, <code>tuple</code>, and <code>str</code>types use 1-based indexing, like in the MATLAB and R languages, where sequence indices start from 1.That is, index <code>n</code>represents the <code>n</code>-th element in a sequence, NOT the <code>n+1</code>-th as in 0-based indexing.This change only affects when the index is non-negative.When the index is negative, the behavior is the same as Python 3.7.This also affects methods of these classes such as <code>index</code>and <code>pop</code>.The built-in functions <code>enumerate</code>and <code>range</code>also use 1-based indexing: by default, the index of <code>enumerate</code>starts from 1, and so does the lower bound of <code>range</code>when not supplied (the higher bound is unchanged).</p>
<p>For example, ```thonpy assert (7,8,9)[1] == 7 assert ["abc", "def", "ghi"][3] == "ghi" assert "abcde"[4] == "d" assert "abc"[:2] == "a" assert [7,8,9] print ([function([4, 88])]) print ([function([4, 5, 6, 7, 2, 122])]) print ([function([4, 0, 6, 7])]) print ([function([4, 4, 6, 8])]) print([list(range(3))]) print ([[4, 5, 6]  All the print statements wrap the expression in a singleton list for the ease of parsing, so that (a) each output always takes a single line even with line breaks in the middle, and (b) we can distinguish between a string representation of e.g. an integer and the integer type.</p>
<p>Mode Prompt Default</p>
<p>You are an expert programmer.Complete the following function in Python 3.7.Please only output the code for the completed function.</p>
<p>def add(lst): """Given a non-empty list of integers lst.add the even elements that are at odd indices..</p>
<p>Examples:</p>
<p>add ([4, 2, 6, 7]) ==&gt; 2 """</p>
<p>CF</p>
<p>You are an expert programmer who can readily adapt to new programming languages.There is a new programming language, ThonPy, which is identical to Python 3.7 except all variables of the <code>list</code>, <code>tuple</code>, and <code>str</code>types use 1-based indexing, like in the MATLAB and R languages, where sequence indices start from 1.That is, index <code>n</code>represents the <code>n</code>-th element in a sequence, NOT the <code>n+1</code>-th as in 0-based indexing.This change only affects when the index is non-negative.When the index is negative, the behavior is the same as Python 3.7.This also affects methods of these classes such as <code>index</code>and <code>pop</code>.The built-in functions <code>enumerate</code>and <code>range</code>also use 1-based indexing: by default, the index of <code>enumerate</code>starts from 1, and so does the lower bound of <code>range</code>when not supplied (the higher bound is unchanged).</p>
<p>For example, <code>`thonpy assert (7,8,9)[1] == 7 assert ["abc", "def", "ghi"][3] == "ghi" assert "abcde"[4] == "d" assert "abc"[:2] == "a" assert [7,8,9][1:] == [7,8,9][1:5] == [7,8,9][1::1] == [7,8,9][:4] == [9,8,7][::-1] == [9,8,7,6][3::-1] == [7,8,9] assert list(enumerate ([7, 8, 9])) == [(1, 7), (2, 8), (3, 9)] assert list(range(2)) == [1] assert list(range(2, 4)) == [2, 3] assert {0: 7, 1: 8, 2: 9}[1] == 8 assert [7,8,9].index(8)== 2</code>C omplete the following function in ThonPy.Please only output the code for the completed function.</p>
<p>def add(lst):</p>
<p>"""Given a non-empty list of integers lst.add the even elements that are at odd indices..</p>
<p>Examples:</p>
<p>add ([4, 2, 6, 7]) ==&gt; 2 """ Table 3: Prompts for the program generation task.</p>
<p>Mode Prompt</p>
<p>Default</p>
<p>You are an expert in linguistics.Your task is to identify the main verb and the main subject of a sentence in English.Show the main verb (a single word) and its subject (also a single word) after the prefix 'Main verb and subject: '.Sentence: japan had just opened its doors to the world after about 250 years of isolation .{Let's think step by step.}</p>
<p>CF</p>
<p>You are an expert in linguistics.Imagine a language that is the same as English with the only exception being that it uses the verb-object-subject order instead of the subject-verb-object order.Your task is to identify the main verb and the main subject in a sentence in this imaginary language.Show the main verb (a single word) and its subject (also a single word) after the prefix 'Main verb and subject: '.Sentence: had just opened its doors japan to the world after about 250 years of isolation .{Let's think step by step.}</p>
<p>CCC</p>
<p>You are an expert in linguistics.Imagine a language that is the same as English with the only exception being that it uses the verb-subject-object order instead of the subject-verb-object order.Your task is to reconstruct the original sentence in English.You should only use the words in the same form as they appear in the given sentence.Sentence: saw anna john Show the original sentence at the end after the prefix 'Original sentence: '. {Let's think step by step.}</p>
<p>Mode Prompt Test</p>
<p>Consider the following premises: "All corgis are reptiles.All reptiles are plants."Assuming no other commonsense or world knowledge, is the sentence "All corgis are plants."necessarily true, necessarily false, or neither?{Let's think step by step, and }end the response with either "necessarily true", "necessarily false", or "neither".</p>
<p>CCC</p>
<p>Consider the following premises: "All corgis are reptiles.All reptiles are plants."Assuming no other commonsense or world knowledge, which sentence between (a) "All corgis are reptiles."and (b) "All corgis are mammals." is definitely true?Answer just "(a)" or "(b)" and nothing else.You MUST choose one and only one, so DO NOT say neither or both.</p>
<p>Table 5: Prompts for the natural language reasoning task.{Let's think step by step, and } is added only if 0-shot CoT is used (and the following e is capitalized without 0-shot CoT).We only use a made-up example here rather than one in the dataset due to the non-publicness of the dataset ( §A.4).Default and counterfactual tasks share the same test template, but the instances themselves are changed to be counterfactual.For the CCC, we separate each changed premise in an instance into a separate prompt.The default statement and the counterfactual statement are matched to (a) and (b) randomly.We do not distinguish between CCC with or without 0-shot CoT.</p>
<p>Mode Prompt Default</p>
<p>You are in the middle of a kitchen.There is a microwave on the south side.There is a fridge on the west side.There is a coffee machine on the north side.We define the following directions.The north direction is (0, -1).The south direction is (0, 1).The east direction is (1, 0).The west direction is (-1, 0).What's the layout of the room in the following format?You can estimate the size of the objects.<code Let_s="Let's" by="by" step="step" step.="step." think="think">`json {"name": "??", "width": 500, "height": 500, "directions": {"north": "??", "south": "??", "east": "??", "west": "??"}, "objects": [{"name": "??", "x": "??", "y": "??"}]}</code></p>
<p>CF</p>
<p>You are in the middle of a kitchen.There is a microwave on the south side.There is a fridge on the west side.There is a coffee machine on the north side.We define the following directions.The north direction is (-1, 0).The south direction is (1, 0).The east direction is (0, -1).The west direction is (0, 1).What's the layout of the room in the following format?You can estimate the size of the objects.<code Let_s="Let's" by="by" step="step" step.="step." think="think">`json {"name": "??", "width": 500, "height": 500, "directions": {"north": "??", "south": "??", "east": "??", "west": "??"}, "objects": [{"name": "??", "x": "??", "y": "??"}]}</code></p>
<p>Mode</p>
<p>Prompt Default 1</p>
<p>You are an expert programmer and drawer.Imagine an image: there is a line at the top and a house in the middle.Can you try your best to draw this image using the <code>processing</code>language? Please do not use any built-in transformation functions, such as <code>rotate</code>and <code>scale</code>.Also, avoid defining any custom transformation functions yourself.Do not load any existing images.Please include as many details of the house as possible and put everything together in the end.</p>
<p>{Let's think step by step.}</p>
<p>Default 2</p>
<p>You are an expert programmer and drawer.Imagine an image: there is a line at the top and a house in the middle.Can you try your best to draw the line and the house using the <code>processing</code>language? Please do not use any built-in transformation functions, such as <code>rotate</code>, <code>scale</code>, and <code>translate</code>.Also, avoid defining any custom transformation functions yourself.Do not load any existing images.Please include as many details of the house as possible and put everything together in the end.</p>
<p>{Let's think step by step.}</p>
<p>CF 1</p>
<p>You are an expert programmer and drawer.Imagine an image: there is a line at the top and a house in the middle.Can you rotate this image 180 degrees and try your best to draw it using the <code>processing</code>language? Please do not use any built-in transformation functions, such as <code>rotate</code>and <code>scale</code>.Also, avoid defining any custom transformation functions yourself.Do not load any existing images.Do not draw the original objects.Please include as many details of the house as possible and put everything together in the end.</p>
<p>{Let's think step by step.}</p>
<p>CF 2</p>
<p>You are an expert programmer and drawer.Imagine an image: there is a line at the top and a house in the middle.Can you rotate this image 180 degrees and try your best to draw the 180-degree rotated line and the 180-degree rotated house using the <code>processingl anguage?Please do not use any built-in transformation functions, such as</code>rotate<code>,</code>scale<code>, and</code>translate`.Also, avoid defining any custom transformation functions yourself.Do not load any existing images.Do not draw the original objects.Please include as many details of the house as possible and put everything together in the end.</p>
<p>{Let's think step by step.}Table 7: Prompts for the drawing task.{Let's think step by step.} is added only if 0-shot CoT is used.We use prompt 1 for GPT-4 and prompt 2 for GPT-3.5 and Claude.We chose the prompt based on the best CCC accuracy for each respective model.In our preliminary experiments, we found that switching the prompt hurts CCC accuracy by more than 20% for both GPT-4 and GPT-3.5.Claude does not follow our instructions when using prompt 1, leading to almost 0% CCC's accuracy.Table 21: Results for the programming generation task (in pass@1 and pass@10; %).We report both the results on the entire HumanEval dataset for comparability with other work, as well as the subset on which evaluating the original program under 1-based indexing would not pass the test cases.Table 24: Results for the spatial reasoning task (in accuracy; %).The first section (Tests Accuracy) requires all 3 objects to be correctly placed.The second section (Test Object-Level Accuracy) refers to accuracy averaged over objects.'S' denotes to swapping, 'R' denotes to rotation, 'Rand.' denotes to random permutation.</p>
<p>50 100Figure 2 :
502
Figure2: Main results.The blue and orange bars represent the default and counterfactual conditions respectively, either with or without 0-shot chain-of-thought (0-CoT) (except code generation; see §A.2).CCC is the counterfactual comprehension check ( §2.1), but when applicable, we report it for the default setting too.Random performance is marked whenever nontrivial.PaLM-2 here is not the largest version ( §4).The CCC for code execution/generation are identical.For spatial reasoning, we average the results from all rotation degrees.Counterfactual performance is consistently lower than the default task performance, while CCC is usually high.§C reports numeric results.</p>
<p>Figure 3 :
3
Figure3: Main results (continued).The blue and orange bars represent the default and counterfactual conditions respectively, either with or without 0-shot chain-of-thought (0-CoT).CCC is the counterfactual comprehension check ( §2.1), but when applicable, we report it for the default setting too.Random performance is marked whenever nontrivial.PaLM-2 here is not the largest version ( §4).Counterfactual performance is consistently lower than the default task performance, while CCC is usually high.§C reports numeric results.</p>
<p>Fret placement accuracy by chord type.The y-axis averages over all altered tunings.</p>
<p>Figure 4 :
4
Figure 4: Investigating the relationship between the default task performance and counterfactual performance, broken down by different factors.Only GPT-4 with 0-shot CoT results are shown.There is a consistent defaultcounterfactual correlation across task variants when varying different factors.</p>
<p>Figure 6 :
6
Figure 6: Two-digit addition accuracy when given different numbers of demonstration examples.The defaultcounterfactual gap reduces, but is not eliminated.</p>
<p>Figure 7 :
7
Figure 7: Visualizations of objects drawn by GPT-4 under the default (upright) and counterfactual conditions: vertical flip (Vflip, i.e. upside-down), rotates 90 degrees (R90), and 180 degrees (R180).In all cases, the CCC (not shown) passes.We show the original output, without flipping/rotating back as in our quantitative evaluation ( §A.6).For the counterfactual settings, GPT-4 either does not transform the objects as instructed (e.g., house and penguin) or struggles to draw meaningful objects (e.g., cake and unicorn).</p>
<p>) sample 50 responses, and (3) only evaluate without 0-shot CoT.While the original work (Chen et al., 2021) recommended sampling 200 responses, this is very expensive, and we follow Wang et al. (2023b) and only sample 50.</p>
<p>expert guitar player.I have a guitar with standard strings E-A-D-G-B-E.I want you to tell me how I could play the E minor triad on this guitar.Make sure to choose one final answer, which you should start with 'ANSWER:' and specify in the following format: ANSWER: E string: fret FRET A string: fret FRET D string: fret FRET G string: fret FRET B string: fret FRET E string: fret FRET Use fret 0 to indicate an open string and fret X to indicate not playing a string.Each increase in fret corresponds to an increase in half a note.{Let's think step by step.}CF You are an expert guitar player.I have a special guitar with strings tuned to E-C-F-G-B-E instead of the standard E-A-D-G-B-E.Note that what is the standard A string is instead tuned to C, and the standard D string is instead tuned to F. All other strings are the same.I want you to tell me how I could play the E minor triad on this guitar.Make sure to choose one final answer, which you should start with 'ANSWER:' and specify in the following format: ANSWER: E string: fret FRET C string: fret FRET F string: fret FRET G string: fret FRET B string: fret FRET E string: fret FRET Use fret 0 to indicate an open string and fret X to indicate not playing a string.Each increase in fret corresponds to an increase in half a note.{Let's think step by step.}Table 8: Prompts for chord fingering: guitar.{Let's think step by step.} is added only if 0-shot CoT is used.Mode Prompt Default You are an expert guitar player.I have a guitar with standard strings E-A-D-G-B-E.I want you to tell me what notes the following sequences of finger positions corresponds to: E string: fret 0 A string: fret 0 D string: fret 0 G string: fret 0 B string: fret 0 E string: fret 0 Note that fret 0 indicates an open string, and each increase in fret corresponds to an increase in half a note.Make sure to choose one final answer, which you should start with 'ANSWER:' and format with dash-separated notes in the order of strings E-A-D-G-B-E.{Let's think step by step.}CF You are an expert guitar player.I have a special guitar with strings tuned to E-C-F-G-B-E instead of the standard E-A-D-G-B-E.Note that what is the standard A string is instead tuned to C, and the standard D string is instead tuned to F. All other strings are the same.I want you to tell me what notes the following sequences of finger positions corresponds to: E string: fret 0 C string: fret 0 F string: fret 0 G string: fret 0 B string: fret 0 E string: fret 0 Note that fret 0 indicates an open string, and each increase in fret corresponds to an increase in half a note.Make sure to choose one final answer, which you should start with 'ANSWER:' and format with dash-separated notes in the order of strings E-C-F-G-B-E.{Let's think step by step.}Table 9: CCC prompts for chord fingering: guitar.{Let's think step by step.} is added only if 0-shot CoT is used.Mode Prompt Default You are an expert ukulele player.I have a ukulele with standard strings G-C-E-A.I want you to tell me how I could play the E minor triad on this ukulele.Make sure to choose one final answer, which you should start with 'ANSWER:' and specify in the following format: ANSWER: G string: fret FRET C string: fret FRET E string: fret FRET A string: fret FRET Use fret 0 to indicate an open string and fret X to indicate not playing a string.Each increase in fret corresponds to an increase in half a note.{Let's think step by step.}CF You are an expert ukulele player.I have a special ukulele with strings tuned to F-C-E-A instead of the standard G-C-E-A.Note that what is the standard G string is instead tuned to F. All other strings are the same.I want you to tell me how I could play the E minor triad on this ukulele.Make sure to choose one final answer, which you should start with 'ANSWER:' and specify in the following format: ANSWER: F string: fret FRET C string: fret FRET E string: fret FRET A string: fret FRET Use fret 0 to indicate an open string and fret X to indicate not playing a string.Each increase in fret corresponds to an increase in half a note.{Let's think step by step.}Table 10: Prompts for chord fingering: ukulele.{Let's think step by step.} is added only if 0-shot CoT is used.Mode Prompt Default You are an expert ukulele player.I have a ukulele with standard strings G-C-E-A.I want you to tell me what notes the following sequences of finger positions corresponds to: G string: fret 0 C string: fret 0 E string: fret 0 A string: fret 0 Note that fret 0 indicates an open string, and each increase in fret corresponds to an increase in half a note.Make sure to choose one final answer, which you should start with 'ANSWER:' and format with dash-separated notes in the order of strings G-C-E-A.{Let's think step by step.}CF You are an expert ukulele player.I have a special ukulele with strings tuned to F-C-E-A instead of the standard G-C-E-A.Note that what is the standard G string is instead tuned to F. All other strings are the same.I want you to tell me what notes the following sequences of finger positions corresponds to: F string: fret 0 C string: fret 0 E string: fret 0 A string: fret 0 Note that fret 0 indicates an open string, and each increase in fret corresponds to an increase in half a note.Make sure to choose one final answer, which you should start with 'ANSWER:' and format with dash-separated notes in the order of strings F-C-E-A.{Let's think step by step.}</p>
<p>Figure 2 only showed the results on this subset.</p>
<p>think step by step. Write out intermediate results and reasoning processes as needed. }End</p>
<p>the response by saying "The final output is:" and a unified summary <code>python</code>code block with <em>ALL</em> the output, in which each line represents the output of each print statement.
.pop(2)])print(["qrs"[:2]])print(["qrstu"[4]])print([list(enumerate("qrstuv"))])``{Let's</p>
<p>Table 2 :
2
Prompts for the program execution task.{Let</p>
<p>'s think step by step. Write out intermediate results and reasoning processes as needed</p>
<p>. } is added only if 0-shot CoT is used.</p>
<p>Table 4 :
4
Prompts for the basic syntactic reasoning task.{Let's think step by step.} is added only if 0-shot CoT is used.</p>
<p>Table 6 :
6
Prompts for the spatial reasoning task.</p>
<p>{Let's think step by step.} is</p>
<p>added only if 0-shot CoT is used.</p>
<p>Table 11 :
11
CCC prompts for chord fingering: ukulele.</p>
<p>{Let's think step by step.} is</p>
<p>added only if 0-shot CoT is used.
TestsCCCw/o 0-CoTw/ 0-CoTBase891011168910111689101116# instances1,000200GPT-482.3 23.4 100.0 38.4 63.0 60.2 38.6 98.2 56.5 74.0 98.0 90.0 100.0 91.0 100.0GPT-3.58.36.6 100.03.8 17.7 12.69.8 99.02.7 17.7 96.5 77.0 100.0 56.095.5Claude22.30.299.86.6 32.41.40.9 98.74.06.6 64.5 47.5 100.0 41.077.5PaLM-26.42.298.73.4 23.41.10.6 82.20.51.2 51.5 53.5 100.0 72.093.5</p>
<p>Table 18 :
18
Results for the arithmetic task (in accuracy; %).</p>
<h1>digits # shots891011162060.2 38.6 98.2 56.5 74.03056.8 32.2 87.1 24.2 33.24024.0 14.6 83.48.99.12197.3 48.1 99.7 25.7 49.12299.1 67.0 99.9 44.0 57.82499.4 79.7 99.9 68.4 70.62899.7 85.8 100.0 79.6 83.521699.9 88.4 99.9 86.9 88.7</h1>
<p>Table 19 :
19
Results for the arithmetic task for various analyses in §5 (in accuracy; %).Only for GPT-4 with 0-shot CoT.
TestsCCCw/o 0-CoTw/ 0-CoTw/o 0-CoTw/ 0-CoTDefault CF Default CF Default CF Default CF# instances113750GPT-458.418.673.524.895.378.199.790.9GPT-3.539.89.754.010.697.121.394.125.9Claude35.413.336.36.296.531.185.137.3</p>
<p>Table 20 :
20
Results for the programming execution task (in accuracy; %).
HumanEval (All)HumanEval (Subset)pass@1pass@10pass@1pass@10Default CF Default CF Default CF Default CF# instances16453GPT-487.468.295.383.482.540.593.364.9GPT-3.573.841.888.467.668.925.181.045.8Claude53.739.678.164.247.615.774.041.9PaLM-227.320.855.842.629.27.455.321.0</p>
<p>Table 22 :
22
Results for the basic syntactic reasoning task (in accuracy; %).
Testsw/o 0-CoTw/ 0-CoTCCCDefault CF Default CF# instances81310GPT-493.874.198.882.7 97.4GPT-3.579.043.265.442.0 90.3Claude27.216.040.717.3 55.2PaLM-284.066.788.974.1 70.6</p>
<p>Table 23 :
23
Results for the logical reasoning task (in accuracy; %).
Tests Accuracyw/o 0-CoTw/ 0-CoTDefault S-NS S-WE R90 R180 R270 Rand. Default S-NS S-WE R90 R180 R270 Rand.# instances100100GPT-479.057.029.034.06.022.034.098.071.023.024.09.013.013.0GPT-3.587.056.032.027.012.017.015.082.066.036.027.029.022.022.0Claude86.051.072.035.045.015.051.085.050.071.030.049.011.039.0PaLM-290.088.086.050.093.039.064.084.095.080.038.091.038.054.0Tests Object-level Accuracyw/o 0-CoTw/ 0-CoTDefault S-NS S-WE R90 R180 R270 Rand. Default S-NS S-WE R90 R180 R270 Rand.# instances100100GPT-486.074.355.756.034.053.061.799.085.357.049.736.046.346.0GPT-3.592.377.762.354.741.342.347.092.782.764.053.050.754.753.3Claude93.775.387.765.770.346.776.091.774.786.363.073.044.069.3PaLM-296.395.794.371.397.764.779.794.098.391.063.796.064.075.0</p>
<p>Table 25 :
25
Results for the drawing task (in accuracy; %).VFlip corresponds to vertical flipping, R90 and R180 correspond to rotation by 90 degrees and 180 degrees, respectively.</p>
<p>This data-generating process can be described by the following generative model, P (y | x, w)P (x | w)P (w). From the perspective of causal inference, our counterfactual framework can be informally seen as performing a do-operator on this graph (
Pearl, 2009).2  This setup is reminiscent of intensional models of natural language semantics(Heim and Kratzer, 1998,  §12; Von Fintel  and Heim, 2011), where f is analogous to the denotation function • , x to its input, and y to its output. By default, the denotation is evaluated under the real world, extensionally, but when a different possible world is specified instead, we expect a competent system to adjust the evaluation accordingly.
In this formulation, LM queries for CCC are separate from the main task queries. For some tasks, it is more natural to query about the task and CCC jointly in the same prompt, i.e., PLM(y, y ′ | prompt f (f, x), prompt g (g, x ′ ), prompt w (w cf )). We use this formulation instead for those tasks.
https://github.com/tombatossals/chords-db
A conceptually similar analysis was performed inLi et al. (2023c) for the game of Othello.
We also explored open-source models in preliminary experiments, but found that they possess unsatisfactory instruction-following ability, to the point that often their output cannot be meaningfully parsed into a prediction. We therefore do not include these models.
 See Dziri et al. (2023)  for a related analysis but for multiplications.
8 This is not a new task variant as compared to the setup in §3.7, but rather a decomposition of our original results
. 9 Though these correlations are not necessarily causal.
-shot demonstration than 0-shot. We hypothesize that this may be due to these being the two bases with letter digits.11 On average, the number of parseable programs generated by GPT-3.5 drops from 99% in the default condition to 62%, 71%, and 75% for the vertically flipped, 90°rotated, and 180°r otated settings, respectively.12 It is arguable if our evaluation setting provides sufficient execution budget (Lampinen,
). Our in-context learning experiment ( §5.5) may be thought of as increasing this budget, and yet the default-counterfactual gap is still sizeable there.
It may be tempting to consider a simple replacement strategy [i]→[i-1] to map back to 0-based indexing. But this does not work for the dictionary type. There are other complications; see Table2.
When not considering cases where objects are placed in the same line, there are 24 permutations for placing 3 objects in 4 different directions, of which 4 can be considered correct.
https://getemoji.com
https://github.com/tombatossals/chords-db
We heuristically filter out incorrect datapoints by filtering out chords that either have the wrong number of notes or lack the root note.
We note that some songs may have multiple canonical keys (e.g., "Twinkle Twinkle Little Star" is also frequently performed in keys like G major or D Major.) In some initial exploration, we validated that C Major was at least one of the canonical keys for the melodies chosen, both by verifying that popular sheet music for these songs was written in C Major, and by asking GPT-3.5 to generate the melodies in an unspecified key and verifying that the generated key was C Major.
AcknowledgmentsWe thank, alphabetically, Alex Gu, Alisa Liu, Belinda Li, Chenghao Yang, Han Guo, Hao Peng,would like to thank Jiamin Zhang for the guitar lessons, which were short but helpful for the relevant components of this paper.Figure1uses icons from flaticon.com.This study was supported by funds from the MIT-IBM Watson AI Lab, the MIT Quest for Intelligence, and the National Science Foundation under grants IIS-2212310 and IIS-2238240.References Mostafa Abdou, Artur Kulmizev, Daniel Hershcovich, Stella Frank, Ellie Pavlick, and Anders Søgaard.2021.Can language models encode perceptual structure without grounding?a case study in color.In Proceedings of the 25th Conference on Computational Natural Language Learning, pages 109-132, Online.Association for Computational Linguistics.Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks.ArXiv preprint, abs/2211.12588.Noam Chomsky.1965.Aspects of the Theory of Syntax.The MIT Press, Cambridge.Mode Prompt DefaultYou are an expert musician.What is the second note of the melody of the song 'Twinkle Twinkle Little Star' in C major? Make sure to choose one final answer, which you should start with 'ANSWER:' and specify in the following format: NOTE={note}.Mode Prompt DefaultYou are an expert musician.What is the second note of the C major scale?Make sure to choose one final answer, which you should start with 'ANSWER:' and specify in the following format: NOTE={note}.Mode PromptDefaultYou are a chess player.Given an opening, determine whether the opening is legal.The opening doesn't need to be a good opening.Answer "yes" if all moves are legal.Answer "no" if the opening violates any rules of chess.Is the new opening "1.e4 e6 2. Be2 Bc5" legal?{Let's think step by step}CFYou are a chess player.You are playing a chess variant where the starting positions for knights and bishops are swapped.For each color, the knights are at placed that where bishops used to be and the bishops are now placed at where knights used to be.Given an opening, determine whether the opening is legal.The opening doesn't need to be a good opening.Answer "yes" if all moves are legal.Answer "no" if the opening violates any rules of chess.Under the custom variant, is the new opening "1.e4 e6 2. Nfe2 Nc5" legal?{Let's think step by step}Mode PromptDefaultYou are a chess player.Question: The two bishops on the board should be initially at which squares? Answer: {Let's think step by step} CF You are a chess player.You are playing a chess variant where the starting positions for knights and bishops are swapped.For each color, the knights are at placed that where bishops used to be and the bishops are now placed at where knights used to be.Question: In this chess variant, the two bishops on the board should be initially at which squares? Answer: {Let's think step by step} -THE RULE OF THE GAME-A GAME-SET is set of three cards: For each attribute, (color, shape, fill, number), the three cards should either be ALL the SAME or NONE the SAME (=ALL DIFFERENT, e.g. if 2 of the cards have the same value, and 1 of them has a different value, the set is NOT valid; for example, (blue, green, blue) is MIXED and does not satisfy any of the rule, whereas (oval, diamond, squiggle) is all different.Here is the board:You can pick a set by typing the cards in the below format: First card: CARD1 Second card: CARD2 Third card: CARD3 Now remember the rule and tell me which three cards here constitutes a GAME-SET in the same format.I will give you 2 cards as a hint, and you tell me the third one.-THE RULE OF THE GAME-(This is not the original SET game.It has a tweaked rule.)In this version, a GAME-SET is a set of three cards: -For each figure attribute except the number (color, shape, fill), the three cards should either be ALL the SAME or NONE the SAME (=ALL DIFFERENT, e.g. if 2 of the cards have the same value, and 1 of them has a different value, the set is NOT valid; for example, (blue, green, blue) is MIXED and does not satisfy any of the rule, whereas (oval, diamond, squiggle) is all different.-But only for the number attribute, 2 of the cards should have the same number, and 1 of them should have a different number in order for the set to be valid.Here is the board:You can pick a set by typing the cards in the below format: First card: CARD1 Second card: CARD2 Third card: CARD3 Now remember the rule and tell me which three cards here constitutes a GAME-SET in the same format.I will give you 2 cards as a hint, and you tell me the third one.-THE RULE OF THE GAME-A GAME-SET is set of three cards: For each attribute, (color, shape, fill, number), the three cards should either be ALL the SAME or NONE the SAME (=ALL DIFFERENT, e.g. if 2 of the cards have the same value, and 1 of them has a different value, the set is NOT valid; for example, (blue, green, blue) is MIXED and does not satisfy any of the rule, whereas (oval, diamond, squiggle) is all different.I will give you three cards from the board, and you will tell me whether this constitutes a GAME-SET.-THE RULE OF THE GAME-(This is not the original SET game.It has a tweaked rule.)In this version, a GAME-SET is a set of three cards: -For each figure attribute except the number (color, shape, fill), the three cards should either be ALL the SAME or NONE the SAME (=ALL DIFFERENT, e.g. if 2 of the cards have the same value, and 1 of them has a different value, the set is NOT valid; for example, (blue, green, blue) is MIXED and does not satisfy any of the rule, whereas (oval, diamond, squiggle) is all different.-But only for the number attribute, 2 of the cards should have the same number, and 1 of them should have a different number in order for the set to be valid.I will give you three cards from the board, and you will tell me whether this constitutes a GAME-SET.
Documenting large webtext corpora: A case study on the colossal clean crawled corpus. Sifatkaur Dhingra, Manmeet Singh, S B Vaisakh, Neetiraj Malviya, Sukhpal Singh, Gill , 10.18653/v1/2021.emnlp-main.98Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Jesse Dodge, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, Matt Gardner, the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational Linguistics2023. 2021Online and Punta CanaMind meets machine: Unravelling GPT-4's cognitive psychology</p>
<p>Improving factuality and reasoning in language models through multiagent debate. Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, Igor Mordatch, abs/2305.14325ArXiv preprint. 2023</p>
<p>. Nouha Dziri, Ximing Lu, Melanie Sclar, Lorraine Xiang, Liwei Li, Bill Yuchen Jiang, Peter Lin, Chandra West, Bhagavatula, Le Ronan, Jena D Bras, Soumya Hwang, Sean Sanyal, Xiang Welleck, Allyson Ren, Zaid Ettinger, Yejin Harchaoui, Choi, 2023Faith and fate: Limits of transformers on compositionality</p>
<p>Number 57 in Monographs on Statistics and Applied Probability. Bradley Efron, Robert J Tibshirani, 1993Chapman &amp; Hall/CRCBoca Raton, Florida, USAAn Introduction to the Bootstrap</p>
<p>Informativeness and invariance: Two perspectives on spurious correlations in natural language. Jacob Eisenstein, 10.18653/v1/2022.naacl-main.321Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational Linguistics2022</p>
<p>Amnesic probing: Behavioral explanation with amnesic counterfactuals. Yanai Elazar, Shauli Ravfogel, Alon Jacovi, Yoav Goldberg, 10.1162/tacl_a_00359Transactions of the Association for Computational Linguistics. 92021</p>
<p>What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models. Allyson Ettinger, 10.1162/tacl_a_00298Transactions of the Association for Computational Linguistics. 82020</p>
<p>CRASS: A novel data set and benchmark to test counterfactual reasoning of large language models. Jörg Frohberg, Frank Binder, 2022</p>
<p>Proceedings of the Thirteenth Language Resources and Evaluation Conference. the Thirteenth Language Resources and Evaluation ConferenceMarseille, FranceEuropean Language Resources Association</p>
<p>The Pile: An 800GB dataset of diverse text for language modeling. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, Connor Leahy, abs/2101.00027ArXiv preprint. 2021</p>
<p>Evaluating models' local decision boundaries via contrast sets. Matt Gardner, Yoav Artzi, Victoria Basmov, Jonathan Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, Nitish Gupta, Hannaneh Hajishirzi, Gabriel Ilharco, Daniel Khashabi, Kevin Lin, Jiangming Liu, Nelson F Liu, Phoebe Mulcaire, Qiang Ning, Sameer Singh, Noah A Smith, Sanjay Subramanian, Reut Tsarfaty, Eric Wallace, Ally Zhang, Ben Zhou, 10.18653/v1/2020.findings-emnlp.117Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational Linguistics2020</p>
<p>Causal abstractions of neural networks. Atticus Geiger, Hanson Lu, Thomas Icard, Christopher Potts, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021. NeurIPS2021. 2021. December 6-14, 2021</p>
<p>Inducing causal structure for interpretable neural networks. Atticus Geiger, Zhengxuan Wu, Hanson Lu, Josh Rozner, Elisa Kreiss, Thomas Icard, Noah D Goodman, Christopher Potts, International Conference on Machine Learning, ICML 2022. Baltimore, Maryland, USAPMLR2022. July 2022162of Proceedings of Machine Learning Research</p>
<p>A counterfactual simulation model of causal judgments for physical events. Tobias Gerstenberg, Noah D Goodman, David A Lagnado, Joshua B Tenenbaum, Psychological review. 2021</p>
<p>Eye-tracking causality. Tobias Gerstenberg, Matthew Peterson, Noah D Goodman, David A Lagnado, Joshua B Tenenbaum, Psychological Science. 282017</p>
<p>. Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, Yupeng Wu, 2023How close is ChatGPT to human experts? Comparison corpus, evaluation, and detection</p>
<p>Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, David Peng, Jonathan Fan, Yixin Liu, Brian Wong, Malcolm Sailor, Ansong Ni, Linyong Nan, Jungo Kasai, Tao Yu, Rui Zhang, Shafiq Joty, Alexander R Fabbri, Wojciech Kryscinski, Caiming Xiong, and Dragomir Radev. 2022. FOLIO: Natural language reasoning with first-order logic. Xi Victoria Lin</p>
<p>Semantics in Generative Grammar. Irene Heim, Angelika Kratzer, 1998Blackwell</p>
<p>Designing and interpreting probes with control tasks. John Hewitt, Percy Liang, 10.18653/v1/D19-1275Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Tom Clark, Eric Hennigan, Katie Noland, George Millican, Bogdan Van Den Driessche, Aurelia Damoc, Simon Guy, Karen Osindero, Erich Simonyan, Jack W Elsen, Rae, Oriol Vinyals. and Laurent Sifre. 2022. Training compute-optimal large language models</p>
<p>Prompt-based methods may underestimate large language models' linguistic generalizations. Jennifer Hu, Roger Levy, abs/2305.132642023ArXiv preprint</p>
<p>Counterpoint by convolution. Cheng-Zhi Anna Huang, Tim Cooijmans, Adam Roberts, Aaron C Courville, Douglas Eck, abs/1903.072272019aArXiv preprint</p>
<p>The bach doodle: Approachable music composition with machine learning at scale. Cheng-Zhi Anna Huang, Curtis Hawthorne, Adam Roberts, Monica Dinculescu, James Wexler, Leon Hong, Jacob Howcroft, 2019b</p>
<p>Probing contextual language models for common ground with visual representations. Gabriel Ilharco, Rowan Zellers, Ali Farhadi, Hannaneh Hajishirzi, 10.18653/v1/2021.naacl-main.422Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies2021Association for Computational Linguistics</p>
<p>Evidence of meaning in language models trained on programs. Charles Jin, Martin Rinard, abs/2305.111692023ArXiv preprint</p>
<p>Learning the difference that makes a difference with counterfactually-augmented data. Divyansh Kaushik, Eduard H Hovy, Zachary Chase Lipton, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020. April 26-30, 20202020OpenReview.net</p>
<p>Explaining the efficacy of counterfactually augmented data. Divyansh Kaushik, Amrith Setlur, Eduard H Hovy, Zachary Chase Lipton, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. Austria2021. May 3-7, 2021OpenReview.net</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, 2023</p>
<p>Probing physical reasoning with counter-commonsense context. Kazushi Kondo, Saku Sugawara, Akiko Aizawa, 2023ArXiv preprint, abs/2306.02258</p>
<p>Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models. Tiffany H Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepaño, Maria Madriaga, Rimel Aggabao, Giezel Diaz-Candido, James Maningo, Victor Tseng, 10.1371/journal.pdig.00001982023PLOS Digital Health2</p>
<p>Counterfactual fairness. Matt J Kusner, Joshua R Loftus, Chris Russell, Ricardo Silva, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. Long Beach, CA, USA2017. 2017. December 4-9, 2017</p>
<p>Causal reasoning and large language models: Opening a new frontier for causality. Emre Kıcıman, Robert Ness, Amit Sharma, Chenhao Tan, 2023</p>
<p>Causal responsibility and counterfactuals. David A Lagnado, Tobias Gerstenberg, Ro'i Zultan, Cognitive Science. 372013</p>
<p>Building machines that learn and think like people. M Brenden, Lake, D Tomer, Joshua B Ullman, Samuel J Tenenbaum, Gershman, 10.1017/s0140525x16001837Behavioral and Brain Sciences. 402017</p>
<p>Can language models handle recursively nested grammatical structures? A case study on comparing models and humans. Andrew Kyle, Lampinen , 2023</p>
<p>Probing for the usage of grammatical number. Karim Lasri, Tiago Pimentel, Alessandro Lenci, Thierry Poibeau, Ryan Cotterell, 10.18653/v1/2022.acl-long.603Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, Ireland2022Association for Computational Linguistics</p>
<p>Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag. Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra2022</p>
<p>Quantifying adaptability in pre-trained language models with 500 tasks. Belinda Li, Jane Yu, Madian Khabsa, Luke Zettlemoyer, Alon Halevy, Jacob Andreas, 10.18653/v1/2022.naacl-main.346Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational Linguistics2022</p>
<p>Implicit representations of meaning in neural language models. Belinda Z Li, Maxwell Nye, Jacob Andreas, 10.18653/v1/2021.acl-long.143Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing20211Association for Computational Linguistics</p>
<p>Implications of the convergence of language and vision model geometries. Jiaang Li, Yova Kementchedjhieva, Anders Søgaard, abs/2302.065552023aArXiv preprint</p>
<p>Counterfactual reasoning: Testing language models' understanding of hypothetical scenarios. Jiaxuan Li, Lang Yu, Allyson Ettinger, 2023b</p>
<p>Emergent world representations: Exploring a sequence model trained on a synthetic task. Kenneth Li, Aspen K Hopkins, David Bau, The Eleventh International Conference on Learning Representations. 2023cFernanda Viégas, Hanspeter Pfister, and Martin Wattenberg</p>
<p>BERT is not the count: Learning to match mathematical statements with proofs. Waylon Weixian, Yftah Li, Maximin Ziser, Shay B Coavoux, Cohen, Proceedings of the 17th Conference of the European Chapter. the 17th Conference of the European ChapterDubrovnik, CroatiaAssociation for Computational Linguistics2023d</p>
<p>Syntactic structure from deep learning. Tal Linzen, Marco Baroni, Annual Review of Linguistics. 72021</p>
<p>Data contamination: From memorization to exploitation. Inbal Magar, Roy Schwartz, 10.18653/v1/2022.acl-short.18Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, Ireland2022Short Papers). Association for Computational Linguistics</p>
<p>Dissociating language and thought in large language models: a cognitive perspective. Kyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy Kanwisher, Joshua B Tenenbaum, Evelina Fedorenko, abs/2301.06627ArXiv preprint. 2023</p>
<p>On the educational impact of ChatGPT: Is artificial intelligence ready to obtain a university degree?. Kamil Malinka, Martin Perešíni, Anton Firc, Ondřej Hujňák, Filip Januš, 2023</p>
<p>Building a large annotated corpus of English: The Penn Treebank. Mitchell P Marcus, Beatrice Santorini, Mary Ann Marcinkiewicz, Computational Linguistics. 1921993</p>
<p>Programs with common sense. John Mccarthy, Proceedings of the Teddington Conference on the Mechanization of Thought Processes. the Teddington Conference on the Mechanization of Thought Processes1959</p>
<p>The Floating Droid. Ian R Mckenzie, Alexander Lyzhov, Michael Pieler, Alicia Parrish, Aaron Mueller, Ameya Prabhu, Euan Mclean, Aaron Kirtland, Alexis Ross, Alisa Liu, Andrew Gritsevskiy, Daniel Wurgaft, Derik Kauffman, Gabriel Recchia, Jiacheng Liu, Joe Cavanagh, Max Weiss, Sicong Huang, Tom Tseng, Tomasz Korbak, Xudong Shen, Yuhui Zhang, Zhengping Zhou, Najoung Kim, Samuel R Bowman, Ethan Perez, 2023Inverse scaling: When bigger isn't better</p>
<p>The larger they are, the harder they fail: Language models do not recognize identifier swaps in Python. Antonio Valerio Miceli-Barone, Fazl Barez, Ioannis Konstas, Shay B Cohen, 2023</p>
<p>Cross-task generalization via natural language crowdsourcing instructions. Swaroop Mishra, Daniel Khashabi, Chitta Baral, Hannaneh Hajishirzi, 10.18653/v1/2022.acl-long.244Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>The vector grounding problem. Dimitri Coelho, Mollo , Raphaël Millière, abs/2304.014812023ArXiv preprint</p>
<p>Fair inference on outcomes. Razieh Nabi, Ilya Shpitser, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18). the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)New Orleans, Louisiana, USAAAAI Press2018. February 2-7, 2018</p>
<p>Universal Dependencies v1: A multilingual treebank collection. Joakim Nivre, Marie-Catherine De Marneffe, Filip Ginter, Yoav Goldberg, Jan Hajič, Christopher D Manning, Ryan Mcdonald, Slav Petrov, Sampo Pyysalo, Natalia Silveira, Reut Tsarfaty, Daniel Zeman, Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16). the Tenth International Conference on Language Resources and Evaluation (LREC'16)Portorož2016Slovenia. European Language Resources Association (ELRA</p>
<p>Capabilities of GPT-4 on medical challenge problems. Harsha Nori, Nicholas King, Scott Mayer Mckinney, Dean Carignan, Eric Horvitz, 2023</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, Augustus Odena, Show your work: Scratchpads for intermediate computation with language models. 2021</p>
<p>GPT-4 technical report. 2023OpenAI</p>
<p>Mapping language models to grounded conceptual spaces. Roma Patel, Ellie Pavlick, The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event. 2022. April 25-29, 2022OpenReview.net</p>
<p>Judea Pearl, Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. San Francisco, CA, USAMorgan Kaufmann Publishers Inc1988</p>
<p>. Judea Pearl, 10.1017/CBO97805118031612009Cambridge University Press2nd edition</p>
<p>Meaning without reference in large language models. Steven Piantadosi, Felix Hill, NeurIPS 2022 Workshop on Neuro Causal and Symbolic AI (nCSI). 2022</p>
<p>A Bayesian framework for information-theoretic probing. Tiago Pimentel, Ryan Cotterell, 10.18653/v1/2021.emnlp-main.229Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Counterfactual story reasoning and generation. Lianhui Qin, Antoine Bosselut, Ari Holtzman, Chandra Bhagavatula, Elizabeth Clark, Yejin Choi, 10.18653/v1/D19-1509Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Back to the future: Unsupervised backprop-based decoding for counterfactual and abductive commonsense reasoning. Lianhui Qin, Vered Shwartz, Peter West, Chandra Bhagavatula, Jena D Hwang, Le Ronan, Antoine Bras, Yejin Bosselut, Choi, 10.18653/v1/2020.emnlp-main.58Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)2020Association for Computational Linguistics</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, Proceedings of the 38th International Conference on Machine Learning, ICML 2021. the 38th International Conference on Machine Learning, ICML 2021PMLR2021. 18-24 July 2021139</p>
<p>Studying the inductive biases of RNNs with synthetic variations of natural languages. Shauli Ravfogel, Yoav Goldberg, Tal Linzen, 10.18653/v1/N19-1356Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Impact of pretraining term frequencies on few-shot numerical reasoning. Yasaman Razeghi, Robert L Logan, I V , Matt Gardner, Sameer Singh, Findings of the Association for Computational Linguistics: EMNLP 2022. Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. Yi Ren, Jinzheng He, Xu Tan, Tao Qin, Zhou Zhao, Tie-Yan Liu, Rushang Karia, Savan Doshi, Keyur Shailaja, Siddhartha Sampat, Sujan Mishra, A Reddy, Sumanta Patro, Tanay Dixit, Xudong Shen, 10.1145/3394171.3413721Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2020. 2022bPopmag: Pop music ac-Ravsehaj Singh Puri</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 2022</p>
<p>From word models to world models: Translating from natural language to the probabilistic language of thought. Lionel Wong, Gabriel Grand, Alexander K Lew, Noah D Goodman, K Vikash, Jacob Mansinghka, Joshua B Andreas, Tenenbaum, 2023</p>
<p>A systematic evaluation of large language models of code. F Frank, Uri Xu, Graham Alon, Vincent Neubig, Josua Hellendoorn, 10.1145/3520312.3534862Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming, MAPS 2022. the 6th ACM SIGPLAN International Symposium on Machine Programming, MAPS 2022New York, NY, USAAssociation for Computing Machinery2022</p>
<p>SemEval-2020 task 5: Counterfactual recognition. Xiaoyu Yang, Stephen Obadinma, Huasha Zhao, Qiong Zhang, Stan Matwin, Xiaodan Zhu, 10.18653/v1/2020.semeval-1.40Proceedings of the Fourteenth Workshop on Semantic Evaluation. the Fourteenth Workshop on Semantic EvaluationBarcelona2020International Committee for Computational Linguistics</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, abs/2305.10601ArXiv preprint. 2023</p>
<p>Word frequency does not predict grammatical knowledge in language models. Charles Yu, Ryan Sie, Nicolas Tedeschi, Leon Bergen, 10.18653/v1/2020.emnlp-main.331Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>IfQA: A dataset for opendomain question answering under counterfactual presuppositions. Wenhao Yu, Meng Jiang, Peter Clark, Ashish Sabharwal, 2023</p>
<p>How would stance detection techniques evolve after the launch of ChatGPT?. Bowen Zhang, Daijun Ding, Liwen Jing, 2023a</p>
<p>How language model hallucinations can snowball. Muru Zhang, Ofir Press, William Merrill, Alisa Liu, Noah A Smith, 2023b</p>
<p>Controllable textto-image generation with GPT-4. Tianjun Zhang, Yi Zhang, Vibhav Vineet, Neel Joshi, Xin Wang, abs/2305.185832023cArXiv preprint</p>
<p>Do language embeddings capture scales?. Xikun Zhang, Deepak Ramachandran, Ian Tenney, Yanai Elazar, Dan Roth, 10.18653/v1/2020.blackboxnlp-1.27Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP. the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLPOnline. Association for Computational Linguistics2020</p>            </div>
        </div>

    </div>
</body>
</html>
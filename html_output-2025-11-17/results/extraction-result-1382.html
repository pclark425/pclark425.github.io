<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1382 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1382</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1382</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-da1d97dba0a34b1cc9171eb5b0e24d331eceb15c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/da1d97dba0a34b1cc9171eb5b0e24d331eceb15c" target="_blank">TreeQN and ATreeC: Differentiable Tree Planning for Deep Reinforcement Learning</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> TreeQN is proposed, a differentiable, recursive, tree-structured model that serves as a drop-in replacement for any value function network in deep RL with discrete actions and ATreeC, an actor-critic variant that augments TreeQN with a softmax layer to form a stochastic policy network.</p>
                <p><strong>Paper Abstract:</strong> Combining deep model-free reinforcement learning with on-line planning is a promising approach to building on the successes of deep RL. On-line planning with look-ahead trees has proven successful in environments where transition models are known a priori. However, in complex environments where transition models need to be learned from data, the deficiencies of learned models have limited their utility for planning. To address these challenges, we propose TreeQN, a differentiable, recursive, tree-structured model that serves as a drop-in replacement for any value function network in deep RL with discrete actions. TreeQN dynamically constructs a tree by recursively applying a transition model in a learned abstract state space and then aggregating predicted rewards and state-values using a tree backup to estimate Q-values. We also propose ATreeC, an actor-critic variant that augments TreeQN with a softmax layer to form a stochastic policy network. Both approaches are trained end-to-end, such that the learned model is optimised for its actual use in the planner. We show that TreeQN and ATreeC outperform n-step DQN and A2C on a box-pushing task, as well as n-step DQN and value prediction networks (Oh et al., 2017) on multiple Atari games, with deeper trees often outperforming shallower ones. We also present a qualitative analysis that sheds light on the trees learned by TreeQN.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1382.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1382.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TreeQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree-Structured Q-Network (TreeQN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A differentiable, recursive, tree-structured latent transition model integrated into the Q-function that constructs on-line look-ahead trees by repeatedly applying a learned action-conditional transition in an abstract latent state space and aggregates predicted rewards and values via a soft-backup to produce Q-values.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TreeQN latent transition model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Deterministic latent-space transition model: an encoder (CNN) produces a k-d embedding z_{0|t}; a shared fully-connected 'env' layer followed by per-action residual fully-connected layers produce next-state latent vectors z_{l+1|t}^{a_i}; an MLP predicts immediate rewards for each action from z; a linear value head estimates V(z); tree backups use a softmax-weighted aggregation over branches. State vectors are L2-normalised between transitions to stabilise behaviour.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model / differentiable tree-structured model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Discrete-action reinforcement learning (evaluated on a box-pushing domain and Atari 2600 games)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>No explicit pixel / observation MSE reported; fidelity is evaluated via (a) L2 reward-prediction loss when reward-grounding auxiliary loss is used, and (b) optional L2 latent-state grounding loss; ultimate fidelity judged by improvement in downstream Q-function loss and task returns.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No quantitative fidelity numbers (e.g., MSE) for transition accuracy are reported in the paper; authors report that reward-grounding auxiliary L2 loss (coefficient η_r) improves task performance, while latent-state L2 grounding (η_s) did not.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Limited: authors report occasional intuitive alignment of tree values with sensible action sequences but overall interpretability is limited because learned transition components are only weakly grounded and branches highly flexible; reward grounding can improve interpretability but state grounding did not.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Qualitative inspection of predicted tree values and action branches; auxiliary reward-prediction loss to encourage semantic grounding; attempted latent-state L2 grounding (did not help); no formal latent visualization or symbolic extraction reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Increases with tree depth due to repeated application of transition and value modules for each branch; uses larger embedding (k=512) for Atari experiments; exact parameter counts, wall-clock training time, and FLOPs not reported. Training used standard deep-RL setups (RMSProp, batch of 80 transitions across 16 env threads × 5 steps).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Compared empirically to DQN, DQN-Deep, DQN-Wide and VPN: TreeQN outperforms DQN and the wide/deep parameter-matched baselines on many tasks, indicating better sample efficiency and/or representational utility for the same or slightly higher compute; exact runtime / compute-per-step comparisons are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Demonstrated improved task returns versus baselines: e.g., in Table 1 (Atari) TreeQN-1 and TreeQN-2 often yield higher best scores than n-step DQN and VPN on many games (example: Seaquest TreeQN-1 best score 9302 vs n-step DQN 3465 and VPN 5628; Alien TreeQN-2 2497 vs n-step DQN 1969). In box-pushing, TreeQN (depth>0) learns faster and achieves substantially better policies than DQN.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>TreeQN's learned latent transition model is optimised end-to-end for usage inside the planner, so fidelity is measured by downstream value/policy improvement rather than raw observation prediction; reward-grounding aligns model predictions with task returns improving utility; state-grounding in latent space did not improve downstream task utility.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Trade-offs include: deeper trees can improve performance in simpler or well-structured domains (box pushing) but are harder to optimise in complex domains (Atari) where transition learning is challenging; stronger grounding (reward loss) improved performance while latent-state grounding hurt performance; interpretability improvements can come at cost of added constraints on the model and potential optimisation difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Key choices: deterministic residual per-action FC transitions in latent space; L2 normalisation of latent vectors between transitions to stabilise transition behaviour; explicit reward predictor and linear value head; softmax-based differentiable backup to propagate gradients through all branches; end-to-end training (tree present during training).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to VPN and predictron: TreeQN integrates the model inside the training-time planner (differentiates through the look-ahead tree) whereas VPN uses a model to make targets and not embedded in the planner during optimisation; empirically TreeQN outperforms VPN on many Atari games. Compared to DQN-Deep/Wide baselines, TreeQN's structured planning inductive bias yields better performance than merely increasing capacity or depth.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Authors recommend end-to-end training with reward-grounding auxiliary loss (η_r ≈ 1 in their experiments). Tree depth is domain-dependent: shallow depth (1) already helps; depths 2–3 help in simpler domains but deeper trees may become harder to optimise in complex visual environments. Strong latent-state grounding is not recommended without further mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TreeQN and ATreeC: Differentiable Tree Planning for Deep Reinforcement Learning', 'publication_date_yy_mm': '2017-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1382.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1382.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ATreeC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Actor-Tree-Critic (ATreeC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An actor-critic variant that replaces the actor's policy/value head with a TreeQN-style differentiable look-ahead tree and a softmax over Q-estimates to form a stochastic policy, trained with A2C.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ATreeC (actor-critic with integrated latent planner)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same latent action-conditional transition and reward prediction modules as TreeQN integrated into the policy: tree produces Q-estimates which are converted via softmax into action probabilities; critic shares encoder parameters and uses a separate linear state-value head.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model integrated into actor-critic policy (hybrid model-free/model-based architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Discrete-action RL (box-pushing and Atari 2600 games)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>No explicit transition fidelity metrics reported; fidelity assessed indirectly by advantage / policy gradient improvements and task returns; optional reward-grounding L2 auxiliary loss can be applied.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not reported quantitatively; authors report ATreeC matches or outperforms A2C baselines on Atari and substantially outperforms A2C in box-pushing.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Same limitations as TreeQN: limited interpretability of trees in practice; stochastic policy sometimes reflects uncertainty better (more decisive behavior) compared to greedy TreeQN.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Qualitative inspection; use of tree-produced Q-values converted to policy probabilities; reward-grounding auxiliary loss can help interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Adds the cost of TreeQN actor computations during both training and inference compared to vanilla A2C; exact compute/time not reported. No explicit scaling metrics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Empirically more sample-efficient / higher returns than A2C in the tested domains, but computational overhead vs A2C not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>ATreeC often matches or outperforms A2C (examples in Table 1 and figures); in box-pushing ATreeC outperforms TreeQN and A2C, likely due to stochastic policy advantages.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Integrating the latent planner into the actor yields improved policy quality and better handling of local action uncertainty; however, benefits depend on the critic quality and exploration (e.g., Seaquest where ATreeC suffered from entropy collapse).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>ATreeC brings policy improvements but inherits TreeQN's optimization and grounding challenges; exploration/entropy tuning becomes more critical; increased computation per update vs A2C.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Policy computed by softmax over tree-produced Q-values; critic separate linear head; shared encoder; auxiliary reward grounding available; different parameters used for critic and actor tree-value modules in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to A2C, ATreeC generally improves returns across tested domains; compared to TreeQN (value-based), ATreeC's stochastic policies can perform better due to better handling of uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Use reward grounding and careful entropy regularisation; tree depth benefits are domain-specific; ensure critic capacity is sufficient for deeper planning to help the actor.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TreeQN and ATreeC: Differentiable Tree Planning for Deep Reinforcement Learning', 'publication_date_yy_mm': '2017-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1382.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1382.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VPN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Value Prediction Network (VPN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model that learns abstract-state transition and reward prediction modules and uses them to construct look-ahead trees for planning, but (in the cited work) the learned model is not embedded in the planner during training-time value estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Value prediction network</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Value Prediction Network (VPN)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Abstract-state predictive model that learns to predict future rewards and values by rolling out an abstract-state transition model; used to build look-ahead trees for target construction and action selection, but in Oh et al. (2017) the model is not used inside the value computation during training (i.e., mismatch between training and test-time planner embedding).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model / planning-as-prediction</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari 2600 (and other discrete-action RL tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reported in original VPN work as prediction losses on rewards/values; in this paper VPN is referenced for conceptual comparison—TreeQN differs by embedding model in training-time planner.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not reported in this paper; original VPN paper reports task scores (Oh et al., 2017) which are included in comparisons (e.g., Table 1: VPN scores shown).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>VPN learns abstract-state transitions which can be interpreted to some extent, but the original VPN does not enforce strong grounding to observations and interpretability is limited.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Original VPN uses prediction objectives for reward/value; no additional interpretability methods described here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>VPN requires rollouts to construct targets and select actions; the paper states VPN uses pretraining and non-differentiable planning during training, but exact compute comparisons are not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>TreeQN outperforms VPN on most Atari games evaluated in this paper, argued to be due to TreeQN's end-to-end embedding of the model in the planner during training.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>VPN scores (as reported in Oh et al., 2017 and shown in Table 1) are generally lower than TreeQN on many games in this paper's experiments (e.g., Seaquest VPN 5628 vs TreeQN-1 9302).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>VPN's separation between model training and planner usage can limit utility; embedding the model in the planner during training (TreeQN) aligns optimisation with deployment and can improve task utility.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>VPN focuses on predictive accuracy in abstract space but may suffer from mismatch between training use and execution use; may require pretraining of observation-space models in other variants.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>VPN rolls out abstract-state transitions to form n-step targets; does not backpropagate through the full look-ahead tree during Q-value computation in training.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to TreeQN, VPN uses the model for target construction rather than for differentiable planning during training; TreeQN's end-to-end approach yields empirical gains in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not discussed in detail here; TreeQN authors argue that embedding the model in the planner during training is preferable for control tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TreeQN and ATreeC: Differentiable Tree Planning for Deep Reinforcement Learning', 'publication_date_yy_mm': '2017-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1382.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1382.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Predictron</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The Predictron</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end learned abstract-state predictive model designed primarily for policy evaluation by internally rolling out a latent model to predict returns, without directly addressing control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The predictron: End-to-end learning and planning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Predictron abstract rollout model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learns abstract-state transitions and internal returns by rolling latent transitions multiple steps to predict values (policy evaluation focus); does not integrate with a control/planning loop for action selection in the original formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent predictive model (policy-evaluation focused)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Policy evaluation / prediction tasks (referenced in RL contexts)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Prediction error on return / value estimation in the predictron paper; in this paper predictron is cited as related work rather than evaluated directly.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not reported in this paper; performance details are in the predictron original paper (Silver et al., 2017b).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Predictron uses latent rollouts; interpretability of latent states depends on auxiliary grounding; not emphasised here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Latent rollouts add computation but are generally cheaper than observation-space rollouts; no explicit compute numbers provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Predictron is more aligned with value prediction and evaluation rather than control; TreeQN differs by embedding the model in a control planner.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Useful for improving value estimates but not directly for action selection/control as designed; TreeQN extends similar ideas to control by differentiable planning.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Predictron emphasises end-to-end value prediction in latent space but does not close the loop for control; this paper positions TreeQN as addressing control with a differentiable planner.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Abstract latent rollouts and internal-return prediction; focus on policy evaluation rather than action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>TreeQN applies a similar latent-rollout idea but integrates the model inside action-value computation and training, aiming to improve control performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TreeQN and ATreeC: Differentiable Tree Planning for Deep Reinforcement Learning', 'publication_date_yy_mm': '2017-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1382.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1382.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Action-conditional video prediction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Action-conditional video prediction using deep networks in Atari games</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Observation-space predictive models that predict future pixel observations conditioned on actions, previously applied in Atari domains for improving exploration and modelling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Action-conditional video prediction using deep networks in atari games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Action-conditional video prediction (Oh et al., 2015)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder models that predict future frames conditioned on action sequences (pixel-space rollouts), used as observation predictors for exploration or auxiliary losses.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>observation-space predictive model / neural simulator</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari 2600 games (visual RL environments)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Pixel-level prediction losses (e.g., MSE) or perceptual similarity; in general these models are evaluated by next-frame prediction error and downstream utility for exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not provided in this paper; authors note these observation-space models tend to allocate capacity to irrelevant details (noisy backgrounds) and accumulate errors rapidly in long rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Pixel-space predictions are directly interpretable visually, but may not prioritise task-relevant features and hence can be misleading for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of predicted frames and rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>High for long observation-space rollouts; authors argue these scale poorly for complex environments compared to latent-space rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Paper argues latent abstract-state models (TreeQN) can be more efficient/robust for planning than observation-space predictors which must model irrelevant visual detail.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Used in prior work to aid exploration; not directly evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Observation-space predictors can help exploration but their fidelity on irrelevant visual features can reduce utility for planning; TreeQN opts for latent-space transitions to avoid these problems.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Observation-space fidelity is visually interpretable but often wastes capacity predicting background/noise, harming task-relevant prediction and planning; latent models trade pixel interpretability for task-focused representations.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Pixel-space decoders and action-conditioned prediction; often require strong regularisation or scheduled sampling to improve robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Authors contrast these with TreeQN's latent transition approach, arguing TreeQN focuses capacity on task-relevant features and trains end-to-end within the planner.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TreeQN and ATreeC: Differentiable Tree Planning for Deep Reinforcement Learning', 'publication_date_yy_mm': '2017-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1382.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1382.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Imagination-augmented agents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Imagination-augmented agents for deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Agents that aggregate rollouts predicted by a learned model (imagination) to improve policies; typically rely on pretraining observation-space models and use RNN-based aggregation of imagined trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Imagination-augmented agents for deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Imagination-Augmented Agent (I2A) model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses an internal model to produce imagined trajectories (often in observation space), aggregates these using an RNN or other aggregator to inform policy; in prior work the observation-space model was pretrained.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>neural simulator with aggregator (hybrid)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Discrete-action RL (tested on various domains including Atari)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Observation-prediction loss and downstream policy improvement after using imagined rollouts; fidelity of the model affects quality of imagined rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not reported in this paper; cited as prior work that required pretraining of observation-space models and may scale poorly to complex environments.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Imagined trajectories in pixel-space are directly visualizable; aggregator RNNs are black-box and limit interpretability of how imagined rollouts drive decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of rollouts and qualitative analysis; no explicit interpretability method discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Pretraining observation-space models and aggregating rollouts increases compute; authors argue such approaches may scale poorly to more complex environments compared to TreeQN's latent approach.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>TreeQN avoids pretraining and observation-space rollouts, instead using a compact latent transition and differentiable planning during training; I2A requires additional pretraining and RNN aggregation overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not directly compared quantitatively in this paper; I2A is presented as conceptually related prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Imagination-based rollouts can be powerful but depend on quality and grounding of the predictive model; TreeQN emphasises end-to-end grounding to align model with planner usage.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>I2A's pretraining requirement and pixel-space rollouts trade interpretability of visual rollouts against high computational and modelling cost; TreeQN trades pixel-level interpretability for compact, task-focused latent transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Pretrained observation model + aggregation RNN for rollouts; policies conditioned on aggregator output rather than explicit value backups.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>TreeQN differs by training the model and planner end-to-end (no pretraining), using latent-space transitions, and employing a differentiable value-backup instead of a generic aggregator RNN.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TreeQN and ATreeC: Differentiable Tree Planning for Deep Reinforcement Learning', 'publication_date_yy_mm': '2017-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Value prediction network <em>(Rating: 2)</em></li>
                <li>The predictron: End-to-end learning and planning <em>(Rating: 2)</em></li>
                <li>Imagination-augmented agents for deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Action-conditional video prediction using deep networks in atari games <em>(Rating: 2)</em></li>
                <li>Value iteration networks <em>(Rating: 1)</em></li>
                <li>Model regularization for stable sample rollouts <em>(Rating: 1)</em></li>
                <li>Self-correcting models for model-based reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1382",
    "paper_id": "paper-da1d97dba0a34b1cc9171eb5b0e24d331eceb15c",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "TreeQN",
            "name_full": "Tree-Structured Q-Network (TreeQN)",
            "brief_description": "A differentiable, recursive, tree-structured latent transition model integrated into the Q-function that constructs on-line look-ahead trees by repeatedly applying a learned action-conditional transition in an abstract latent state space and aggregates predicted rewards and values via a soft-backup to produce Q-values.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "TreeQN latent transition model",
            "model_description": "Deterministic latent-space transition model: an encoder (CNN) produces a k-d embedding z_{0|t}; a shared fully-connected 'env' layer followed by per-action residual fully-connected layers produce next-state latent vectors z_{l+1|t}^{a_i}; an MLP predicts immediate rewards for each action from z; a linear value head estimates V(z); tree backups use a softmax-weighted aggregation over branches. State vectors are L2-normalised between transitions to stabilise behaviour.",
            "model_type": "latent world model / differentiable tree-structured model",
            "task_domain": "Discrete-action reinforcement learning (evaluated on a box-pushing domain and Atari 2600 games)",
            "fidelity_metric": "No explicit pixel / observation MSE reported; fidelity is evaluated via (a) L2 reward-prediction loss when reward-grounding auxiliary loss is used, and (b) optional L2 latent-state grounding loss; ultimate fidelity judged by improvement in downstream Q-function loss and task returns.",
            "fidelity_performance": "No quantitative fidelity numbers (e.g., MSE) for transition accuracy are reported in the paper; authors report that reward-grounding auxiliary L2 loss (coefficient η_r) improves task performance, while latent-state L2 grounding (η_s) did not.",
            "interpretability_assessment": "Limited: authors report occasional intuitive alignment of tree values with sensible action sequences but overall interpretability is limited because learned transition components are only weakly grounded and branches highly flexible; reward grounding can improve interpretability but state grounding did not.",
            "interpretability_method": "Qualitative inspection of predicted tree values and action branches; auxiliary reward-prediction loss to encourage semantic grounding; attempted latent-state L2 grounding (did not help); no formal latent visualization or symbolic extraction reported.",
            "computational_cost": "Increases with tree depth due to repeated application of transition and value modules for each branch; uses larger embedding (k=512) for Atari experiments; exact parameter counts, wall-clock training time, and FLOPs not reported. Training used standard deep-RL setups (RMSProp, batch of 80 transitions across 16 env threads × 5 steps).",
            "efficiency_comparison": "Compared empirically to DQN, DQN-Deep, DQN-Wide and VPN: TreeQN outperforms DQN and the wide/deep parameter-matched baselines on many tasks, indicating better sample efficiency and/or representational utility for the same or slightly higher compute; exact runtime / compute-per-step comparisons are not provided.",
            "task_performance": "Demonstrated improved task returns versus baselines: e.g., in Table 1 (Atari) TreeQN-1 and TreeQN-2 often yield higher best scores than n-step DQN and VPN on many games (example: Seaquest TreeQN-1 best score 9302 vs n-step DQN 3465 and VPN 5628; Alien TreeQN-2 2497 vs n-step DQN 1969). In box-pushing, TreeQN (depth&gt;0) learns faster and achieves substantially better policies than DQN.",
            "task_utility_analysis": "TreeQN's learned latent transition model is optimised end-to-end for usage inside the planner, so fidelity is measured by downstream value/policy improvement rather than raw observation prediction; reward-grounding aligns model predictions with task returns improving utility; state-grounding in latent space did not improve downstream task utility.",
            "tradeoffs_observed": "Trade-offs include: deeper trees can improve performance in simpler or well-structured domains (box pushing) but are harder to optimise in complex domains (Atari) where transition learning is challenging; stronger grounding (reward loss) improved performance while latent-state grounding hurt performance; interpretability improvements can come at cost of added constraints on the model and potential optimisation difficulty.",
            "design_choices": "Key choices: deterministic residual per-action FC transitions in latent space; L2 normalisation of latent vectors between transitions to stabilise transition behaviour; explicit reward predictor and linear value head; softmax-based differentiable backup to propagate gradients through all branches; end-to-end training (tree present during training).",
            "comparison_to_alternatives": "Compared to VPN and predictron: TreeQN integrates the model inside the training-time planner (differentiates through the look-ahead tree) whereas VPN uses a model to make targets and not embedded in the planner during optimisation; empirically TreeQN outperforms VPN on many Atari games. Compared to DQN-Deep/Wide baselines, TreeQN's structured planning inductive bias yields better performance than merely increasing capacity or depth.",
            "optimal_configuration": "Authors recommend end-to-end training with reward-grounding auxiliary loss (η_r ≈ 1 in their experiments). Tree depth is domain-dependent: shallow depth (1) already helps; depths 2–3 help in simpler domains but deeper trees may become harder to optimise in complex visual environments. Strong latent-state grounding is not recommended without further mechanisms.",
            "uuid": "e1382.0",
            "source_info": {
                "paper_title": "TreeQN and ATreeC: Differentiable Tree Planning for Deep Reinforcement Learning",
                "publication_date_yy_mm": "2017-10"
            }
        },
        {
            "name_short": "ATreeC",
            "name_full": "Actor-Tree-Critic (ATreeC)",
            "brief_description": "An actor-critic variant that replaces the actor's policy/value head with a TreeQN-style differentiable look-ahead tree and a softmax over Q-estimates to form a stochastic policy, trained with A2C.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ATreeC (actor-critic with integrated latent planner)",
            "model_description": "Same latent action-conditional transition and reward prediction modules as TreeQN integrated into the policy: tree produces Q-estimates which are converted via softmax into action probabilities; critic shares encoder parameters and uses a separate linear state-value head.",
            "model_type": "latent world model integrated into actor-critic policy (hybrid model-free/model-based architecture)",
            "task_domain": "Discrete-action RL (box-pushing and Atari 2600 games)",
            "fidelity_metric": "No explicit transition fidelity metrics reported; fidelity assessed indirectly by advantage / policy gradient improvements and task returns; optional reward-grounding L2 auxiliary loss can be applied.",
            "fidelity_performance": "Not reported quantitatively; authors report ATreeC matches or outperforms A2C baselines on Atari and substantially outperforms A2C in box-pushing.",
            "interpretability_assessment": "Same limitations as TreeQN: limited interpretability of trees in practice; stochastic policy sometimes reflects uncertainty better (more decisive behavior) compared to greedy TreeQN.",
            "interpretability_method": "Qualitative inspection; use of tree-produced Q-values converted to policy probabilities; reward-grounding auxiliary loss can help interpretation.",
            "computational_cost": "Adds the cost of TreeQN actor computations during both training and inference compared to vanilla A2C; exact compute/time not reported. No explicit scaling metrics provided.",
            "efficiency_comparison": "Empirically more sample-efficient / higher returns than A2C in the tested domains, but computational overhead vs A2C not quantified.",
            "task_performance": "ATreeC often matches or outperforms A2C (examples in Table 1 and figures); in box-pushing ATreeC outperforms TreeQN and A2C, likely due to stochastic policy advantages.",
            "task_utility_analysis": "Integrating the latent planner into the actor yields improved policy quality and better handling of local action uncertainty; however, benefits depend on the critic quality and exploration (e.g., Seaquest where ATreeC suffered from entropy collapse).",
            "tradeoffs_observed": "ATreeC brings policy improvements but inherits TreeQN's optimization and grounding challenges; exploration/entropy tuning becomes more critical; increased computation per update vs A2C.",
            "design_choices": "Policy computed by softmax over tree-produced Q-values; critic separate linear head; shared encoder; auxiliary reward grounding available; different parameters used for critic and actor tree-value modules in experiments.",
            "comparison_to_alternatives": "Compared to A2C, ATreeC generally improves returns across tested domains; compared to TreeQN (value-based), ATreeC's stochastic policies can perform better due to better handling of uncertainty.",
            "optimal_configuration": "Use reward grounding and careful entropy regularisation; tree depth benefits are domain-specific; ensure critic capacity is sufficient for deeper planning to help the actor.",
            "uuid": "e1382.1",
            "source_info": {
                "paper_title": "TreeQN and ATreeC: Differentiable Tree Planning for Deep Reinforcement Learning",
                "publication_date_yy_mm": "2017-10"
            }
        },
        {
            "name_short": "VPN",
            "name_full": "Value Prediction Network (VPN)",
            "brief_description": "A model that learns abstract-state transition and reward prediction modules and uses them to construct look-ahead trees for planning, but (in the cited work) the learned model is not embedded in the planner during training-time value estimation.",
            "citation_title": "Value prediction network",
            "mention_or_use": "mention",
            "model_name": "Value Prediction Network (VPN)",
            "model_description": "Abstract-state predictive model that learns to predict future rewards and values by rolling out an abstract-state transition model; used to build look-ahead trees for target construction and action selection, but in Oh et al. (2017) the model is not used inside the value computation during training (i.e., mismatch between training and test-time planner embedding).",
            "model_type": "latent world model / planning-as-prediction",
            "task_domain": "Atari 2600 (and other discrete-action RL tasks)",
            "fidelity_metric": "Reported in original VPN work as prediction losses on rewards/values; in this paper VPN is referenced for conceptual comparison—TreeQN differs by embedding model in training-time planner.",
            "fidelity_performance": "Not reported in this paper; original VPN paper reports task scores (Oh et al., 2017) which are included in comparisons (e.g., Table 1: VPN scores shown).",
            "interpretability_assessment": "VPN learns abstract-state transitions which can be interpreted to some extent, but the original VPN does not enforce strong grounding to observations and interpretability is limited.",
            "interpretability_method": "Original VPN uses prediction objectives for reward/value; no additional interpretability methods described here.",
            "computational_cost": "VPN requires rollouts to construct targets and select actions; the paper states VPN uses pretraining and non-differentiable planning during training, but exact compute comparisons are not detailed here.",
            "efficiency_comparison": "TreeQN outperforms VPN on most Atari games evaluated in this paper, argued to be due to TreeQN's end-to-end embedding of the model in the planner during training.",
            "task_performance": "VPN scores (as reported in Oh et al., 2017 and shown in Table 1) are generally lower than TreeQN on many games in this paper's experiments (e.g., Seaquest VPN 5628 vs TreeQN-1 9302).",
            "task_utility_analysis": "VPN's separation between model training and planner usage can limit utility; embedding the model in the planner during training (TreeQN) aligns optimisation with deployment and can improve task utility.",
            "tradeoffs_observed": "VPN focuses on predictive accuracy in abstract space but may suffer from mismatch between training use and execution use; may require pretraining of observation-space models in other variants.",
            "design_choices": "VPN rolls out abstract-state transitions to form n-step targets; does not backpropagate through the full look-ahead tree during Q-value computation in training.",
            "comparison_to_alternatives": "Compared to TreeQN, VPN uses the model for target construction rather than for differentiable planning during training; TreeQN's end-to-end approach yields empirical gains in this paper.",
            "optimal_configuration": "Not discussed in detail here; TreeQN authors argue that embedding the model in the planner during training is preferable for control tasks.",
            "uuid": "e1382.2",
            "source_info": {
                "paper_title": "TreeQN and ATreeC: Differentiable Tree Planning for Deep Reinforcement Learning",
                "publication_date_yy_mm": "2017-10"
            }
        },
        {
            "name_short": "Predictron",
            "name_full": "The Predictron",
            "brief_description": "An end-to-end learned abstract-state predictive model designed primarily for policy evaluation by internally rolling out a latent model to predict returns, without directly addressing control.",
            "citation_title": "The predictron: End-to-end learning and planning",
            "mention_or_use": "mention",
            "model_name": "Predictron abstract rollout model",
            "model_description": "Learns abstract-state transitions and internal returns by rolling latent transitions multiple steps to predict values (policy evaluation focus); does not integrate with a control/planning loop for action selection in the original formulation.",
            "model_type": "latent predictive model (policy-evaluation focused)",
            "task_domain": "Policy evaluation / prediction tasks (referenced in RL contexts)",
            "fidelity_metric": "Prediction error on return / value estimation in the predictron paper; in this paper predictron is cited as related work rather than evaluated directly.",
            "fidelity_performance": "Not reported in this paper; performance details are in the predictron original paper (Silver et al., 2017b).",
            "interpretability_assessment": "Predictron uses latent rollouts; interpretability of latent states depends on auxiliary grounding; not emphasised here.",
            "interpretability_method": "Not discussed in this paper.",
            "computational_cost": "Latent rollouts add computation but are generally cheaper than observation-space rollouts; no explicit compute numbers provided here.",
            "efficiency_comparison": "Predictron is more aligned with value prediction and evaluation rather than control; TreeQN differs by embedding the model in a control planner.",
            "task_performance": "Not evaluated in this paper.",
            "task_utility_analysis": "Useful for improving value estimates but not directly for action selection/control as designed; TreeQN extends similar ideas to control by differentiable planning.",
            "tradeoffs_observed": "Predictron emphasises end-to-end value prediction in latent space but does not close the loop for control; this paper positions TreeQN as addressing control with a differentiable planner.",
            "design_choices": "Abstract latent rollouts and internal-return prediction; focus on policy evaluation rather than action selection.",
            "comparison_to_alternatives": "TreeQN applies a similar latent-rollout idea but integrates the model inside action-value computation and training, aiming to improve control performance.",
            "uuid": "e1382.3",
            "source_info": {
                "paper_title": "TreeQN and ATreeC: Differentiable Tree Planning for Deep Reinforcement Learning",
                "publication_date_yy_mm": "2017-10"
            }
        },
        {
            "name_short": "Action-conditional video prediction",
            "name_full": "Action-conditional video prediction using deep networks in Atari games",
            "brief_description": "Observation-space predictive models that predict future pixel observations conditioned on actions, previously applied in Atari domains for improving exploration and modelling.",
            "citation_title": "Action-conditional video prediction using deep networks in atari games",
            "mention_or_use": "mention",
            "model_name": "Action-conditional video prediction (Oh et al., 2015)",
            "model_description": "Encoder-decoder models that predict future frames conditioned on action sequences (pixel-space rollouts), used as observation predictors for exploration or auxiliary losses.",
            "model_type": "observation-space predictive model / neural simulator",
            "task_domain": "Atari 2600 games (visual RL environments)",
            "fidelity_metric": "Pixel-level prediction losses (e.g., MSE) or perceptual similarity; in general these models are evaluated by next-frame prediction error and downstream utility for exploration.",
            "fidelity_performance": "Not provided in this paper; authors note these observation-space models tend to allocate capacity to irrelevant details (noisy backgrounds) and accumulate errors rapidly in long rollouts.",
            "interpretability_assessment": "Pixel-space predictions are directly interpretable visually, but may not prioritise task-relevant features and hence can be misleading for planning.",
            "interpretability_method": "Visualization of predicted frames and rollouts.",
            "computational_cost": "High for long observation-space rollouts; authors argue these scale poorly for complex environments compared to latent-space rollouts.",
            "efficiency_comparison": "Paper argues latent abstract-state models (TreeQN) can be more efficient/robust for planning than observation-space predictors which must model irrelevant visual detail.",
            "task_performance": "Used in prior work to aid exploration; not directly evaluated in this paper.",
            "task_utility_analysis": "Observation-space predictors can help exploration but their fidelity on irrelevant visual features can reduce utility for planning; TreeQN opts for latent-space transitions to avoid these problems.",
            "tradeoffs_observed": "Observation-space fidelity is visually interpretable but often wastes capacity predicting background/noise, harming task-relevant prediction and planning; latent models trade pixel interpretability for task-focused representations.",
            "design_choices": "Pixel-space decoders and action-conditioned prediction; often require strong regularisation or scheduled sampling to improve robustness.",
            "comparison_to_alternatives": "Authors contrast these with TreeQN's latent transition approach, arguing TreeQN focuses capacity on task-relevant features and trains end-to-end within the planner.",
            "uuid": "e1382.4",
            "source_info": {
                "paper_title": "TreeQN and ATreeC: Differentiable Tree Planning for Deep Reinforcement Learning",
                "publication_date_yy_mm": "2017-10"
            }
        },
        {
            "name_short": "Imagination-augmented agents",
            "name_full": "Imagination-augmented agents for deep reinforcement learning",
            "brief_description": "Agents that aggregate rollouts predicted by a learned model (imagination) to improve policies; typically rely on pretraining observation-space models and use RNN-based aggregation of imagined trajectories.",
            "citation_title": "Imagination-augmented agents for deep reinforcement learning",
            "mention_or_use": "mention",
            "model_name": "Imagination-Augmented Agent (I2A) model",
            "model_description": "Uses an internal model to produce imagined trajectories (often in observation space), aggregates these using an RNN or other aggregator to inform policy; in prior work the observation-space model was pretrained.",
            "model_type": "neural simulator with aggregator (hybrid)",
            "task_domain": "Discrete-action RL (tested on various domains including Atari)",
            "fidelity_metric": "Observation-prediction loss and downstream policy improvement after using imagined rollouts; fidelity of the model affects quality of imagined rollouts.",
            "fidelity_performance": "Not reported in this paper; cited as prior work that required pretraining of observation-space models and may scale poorly to complex environments.",
            "interpretability_assessment": "Imagined trajectories in pixel-space are directly visualizable; aggregator RNNs are black-box and limit interpretability of how imagined rollouts drive decisions.",
            "interpretability_method": "Visualization of rollouts and qualitative analysis; no explicit interpretability method discussed here.",
            "computational_cost": "Pretraining observation-space models and aggregating rollouts increases compute; authors argue such approaches may scale poorly to more complex environments compared to TreeQN's latent approach.",
            "efficiency_comparison": "TreeQN avoids pretraining and observation-space rollouts, instead using a compact latent transition and differentiable planning during training; I2A requires additional pretraining and RNN aggregation overhead.",
            "task_performance": "Not directly compared quantitatively in this paper; I2A is presented as conceptually related prior work.",
            "task_utility_analysis": "Imagination-based rollouts can be powerful but depend on quality and grounding of the predictive model; TreeQN emphasises end-to-end grounding to align model with planner usage.",
            "tradeoffs_observed": "I2A's pretraining requirement and pixel-space rollouts trade interpretability of visual rollouts against high computational and modelling cost; TreeQN trades pixel-level interpretability for compact, task-focused latent transitions.",
            "design_choices": "Pretrained observation model + aggregation RNN for rollouts; policies conditioned on aggregator output rather than explicit value backups.",
            "comparison_to_alternatives": "TreeQN differs by training the model and planner end-to-end (no pretraining), using latent-space transitions, and employing a differentiable value-backup instead of a generic aggregator RNN.",
            "uuid": "e1382.5",
            "source_info": {
                "paper_title": "TreeQN and ATreeC: Differentiable Tree Planning for Deep Reinforcement Learning",
                "publication_date_yy_mm": "2017-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Value prediction network",
            "rating": 2
        },
        {
            "paper_title": "The predictron: End-to-end learning and planning",
            "rating": 2
        },
        {
            "paper_title": "Imagination-augmented agents for deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Action-conditional video prediction using deep networks in atari games",
            "rating": 2
        },
        {
            "paper_title": "Value iteration networks",
            "rating": 1
        },
        {
            "paper_title": "Model regularization for stable sample rollouts",
            "rating": 1
        },
        {
            "paper_title": "Self-correcting models for model-based reinforcement learning",
            "rating": 1
        }
    ],
    "cost": 0.015529,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>TreeQN and ATreEC: Differentiable Tree-Structured Models for DEEP REINFORCEMENT LEARNING</h1>
<p>Gregory Farquhar ${ }^{\dagger}$<br>gregory.farquhar@cs.ox.ac.uk<br>Maximilian Igl ${ }^{\dagger}$<br>maximilian.igl@cs.ox.ac.uk<br>${ }^{\dagger}$ University of Oxford, United Kingdom</p>
<p>Tim Rocktäschel ${ }^{\dagger}$<br>tim.rocktaschel@cs.ox.ac.uk<br>Shimon Whiteson ${ }^{\dagger}$<br>shimon.whiteson@cs.ox.ac.uk</p>
<p>${ }^{\dagger}$ University of Oxford, United Kingdom</p>
<h4>Abstract</h4>
<p>Combining deep model-free reinforcement learning with on-line planning is a promising approach to building on the successes of deep RL. On-line planning with look-ahead trees has proven successful in environments where transition models are known a priori. However, in complex environments where transition models need to be learned from data, the deficiencies of learned models have limited their utility for planning. To address these challenges, we propose TreeQN, a differentiable, recursive, tree-structured model that serves as a drop-in replacement for any value function network in deep RL with discrete actions. TreeQN dynamically constructs a tree by recursively applying a transition model in a learned abstract state space and then aggregating predicted rewards and state-values using a tree backup to estimate $Q$-values. We also propose ATreeC, an actor-critic variant that augments TreeQN with a softmax layer to form a stochastic policy network. Both approaches are trained end-to-end, such that the learned model is optimised for its actual use in the tree. We show that TreeQN and ATreeC outperform $n$-step DQN and A2C on a box-pushing task, as well as $n$-step DQN and value prediction networks (Oh et al., 2017) on multiple Atari games. Furthermore, we present ablation studies that demonstrate the effect of different auxiliary losses on learning transition models.</p>
<h2>1 INTRODUCTION</h2>
<p>A promising approach to improving model-free deep reinforcement learning (RL) is to combine it with on-line planning. The model-free value function can be viewed as a rough global estimate which is then locally refined on the fly for the current state by the on-line planner. Crucially, this does not require new samples from the environment but only additional computation, which is often available.
One strategy for on-line planning is to use look-ahead tree search (Knuth \&amp; Moore, 1975; Browne et al., 2012). Traditionally, such methods have been limited to domains where perfect environment simulators are available, such as board or card games (Coulom, 2006; Sturtevant, 2008). However, in general, models for complex environments with high dimensional observation spaces and complex dynamics must be learned from agent experience. Unfortunately, to date, it has proven difficult to learn models for such domains with sufficient fidelity to realise the benefits of look-ahead planning (Oh et al., 2015; Talvitie, 2017).</p>
<p>A simple approach to learning environment models is to maximise a similarity metric between model predictions and ground truth in the observation space. This approach has been applied with some success in cases where model fidelity is less important, e.g., for improving exploration (Chiappa et al., 2017; Oh et al., 2015). However, this objective causes significant model capacity to be devoted to</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>predicting irrelevant aspects of the environment dynamics, such as noisy backgrounds, at the expense of value-critical features that may occupy only a small part of the observation space (Pathak et al., 2017). Consequently, current state-of-the-art models still accumulate errors too rapidly to be used for look-ahead planning in complex environments.</p>
<p>Another strategy is to train a model such that, when it is used to predict a value function, the error in those predictions is minimised. Doing so can encourage the model to focus on features of the observations that are relevant for the control task. An example is the <em>predictron</em> (Silver et al., 2017b), where the model is used to aid policy evaluation without addressing control. <em>Value prediction networks</em> (VPNs, Oh et al., 2017) take a similar approach but use the model to construct a look-ahead tree only when constructing bootstrap targets and selecting actions, similarly to <em>TD-search</em> (Silver et al., 2012). Crucially, the model is not embedded in a planning algorithm during optimisation.</p>
<p>We propose a new tree-structured neural network architecture to address the aforementioned problems. By formulating the tree look-ahead in a differentiable way and integrating it directly into the $Q$-function or policy, we train the entire agent, including its learned transition model, end-to-end. This ensures that the model is optimised for the correct goal and is suitable for on-line planning during execution of the policy.</p>
<p>Since the transition model is only weakly grounded in the actual environment, our approach can alternatively be viewed as a model-free method in which the fully connected layers of DQN are replaced by a recursive network that applies transition functions with shared parameters at each tree node expansion.</p>
<p>The resulting architecture, which we call TreeQN, encodes an inductive bias based on the prior knowledge that the environment is a stationary Markov process, which facilitates faster learning of better policies. We also present an actor-critic variant, ATreeC, in which the tree is augmented with a softmax layer and used as a policy network.</p>
<p>We show that TreeQN and ATreeC outperform their DQN-based counterparts in a box-pushing domain and a suite of Atari games, with deeper trees often outperforming shallower trees, and TreeQN outperforming VPN (Oh et al., 2017) on most Atari games. We also present ablation studies investigating various auxiliary losses for grounding the transition model more strongly in the environment, which could improve performance as well as lead to interpretable internal plans. While we show that grounding the reward function is valuable, we conclude that how to learn strongly grounded transition models and generate reliably interpretable plans without compromising performance remains an open research question.</p>
<h1>2 BACKGROUND</h1>
<p>We consider an agent learning to act in a Markov Decision Process (MDP), with the goal of maximising its expected discounted sum of rewards $R_{t}=\sum_{t=0}^{\infty}\gamma^{t}r_{t}$, by learning a policy $\pi(\mathbf{s})$ that maps states $\mathbf{s}\in\mathcal{S}$ to actions $a\in\mathcal{A}$. The state-action value function ($Q$-function) is defined as $Q^{\pi}(\mathbf{s},a)=\mathbb{E}<em t="t">{\pi}\left[R</em>}|\mathbf{s<em t="t">{t}=\mathbf{s},a</em>,a)$.}=a\right]$; the optimal $Q$-function is $Q^{*}(\mathbf{s},a)=\max_{\pi} Q^{\pi}(\mathbf{s</p>
<p>The Bellman optimality equation writes $Q^{*}$ recursively as</p>
<p>$Q^{<em>}(\mathbf{s},a)=\mathcal{T}Q^{</em>}(\mathbf{s},a)\equiv r(\mathbf{s},a)+\gamma\sum_{\mathbf{s}^{\prime}}P(\mathbf{s}^{\prime}|\mathbf{s},a)\max_{a^{\prime}}Q^{*}(\mathbf{s}^{\prime},a^{\prime}),$</p>
<p>where $P$ is the MDP state transition function and $r$ is a reward function, which for simplicity we assume to be deterministic. $Q$-learning (Watkins &amp; Dayan, 1992) uses a single-sample approximation of the contraction operator $\mathcal{T}$ to iteratively improve an estimate of $Q^{*}$.</p>
<p>In deep $Q$-learning (Mnih et al., 2015), $Q$ is represented by a deep neural network with parameters $\theta$, and is improved by regressing $Q(\mathbf{s},a)$ to a target $r+\gamma\max_{a^{\prime}} Q(\mathbf{s}^{\prime},a^{\prime};\theta^{-})$, where $\theta^{-}$are the parameters of a target network periodically copied from $\theta$.</p>
<p>We use a version of $n$-step $Q$-learning (Mnih et al., 2016) with synchronous environment threads. In particular, starting at a timestep $t$, we roll forward $n_{\text{env}}=16$ threads for $n=5$ timesteps each. We then bootstrap off the final states only and gather all $n_{\text{env}}\times n=80$ transitions in a single batch for</p>
<p>the backward pass, minimising the loss:</p>
<p>$$
\mathcal{L}<em _envs="{envs" _text="\text">{\text {nstep-Q }}=\sum</em> \max }} \sum_{j=1}^{n}\left(\sum_{k=1}^{j}\left[\gamma^{j-k} r_{t+n-k}\right]+\gamma^{j<em t_n="t+n">{a^{\prime}} Q\left(\mathbf{s}</em>}, a^{\prime}, \theta^{-}\right)-Q\left(\mathbf{s<em t_n-j="t+n-j">{t+n-j}, a</em>
$$}, \theta\right)\right)^{2</p>
<p>If the episode terminates, we use the remaining episode return as the target, without bootstrapping.
This algorithm's actor-critic counterpart is A2C, a synchronous variant of A3C (Mnih et al., 2016) in which a policy $\pi$ and state-value function $V(s)$ are trained using the gradient:</p>
<p>$$
\begin{aligned}
\Delta \theta=\sum_{\text {envs }} \sum_{j=1}^{n} \nabla_{\theta_{\pi}} \log \pi\left(a_{t+n-j} \mid s_{t+n-j}\right) A_{j}\left(s_{t+n-j}, a_{t+n-j}\right) &amp; +\beta \nabla_{\theta_{\pi}} H\left(\pi\left(s_{t+n-j}\right)\right) \
&amp; +\alpha \nabla_{\theta_{V}} A_{j}\left(s_{t+n-j}, a_{t+n-j}\right)^{2}
\end{aligned}
$$</p>
<p>where $A_{j}$ is an advantage estimate given by $\sum_{k=1}^{j} \gamma^{j-k} r_{t+n-k}+\gamma^{j} V\left(\mathbf{s}<em t_n-j="t+n-j">{t+n}\right)-V\left(\mathbf{s}</em>\right), H$ is the policy entropy, $\beta$ is a hyperparameter tuning the degree of entropy regularisation, and $\alpha$ is a hyperparameter controlling the relative learning rates of actor and critic.</p>
<p>These algorithms were chosen for their simplicity and reasonable wallclock speeds, but TreeQN can also be used in other algorithms, as described in Section 3. Our implementations are based on OpenAI Baselines (Hesse et al., 2017).</p>
<p>The canonical neural network architecture in deep RL with visual observations has a series of convolutional layers followed by two fully connected layers, where the final layer produces one output for each action-value. We can think of this network as first calculating an encoding $\mathbf{z}<em t="t">{t}$ of the state $\mathbf{s}</em>, a\right)$ (see Fig. 1).
}$ which is then evaluated by the final layer to estimate $Q^{*}\left(\mathbf{s}_{t<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>In tree-search on-line planning, a look-ahead tree of possible future states is constructed by recursively applying an environment model. These states are typically evaluated by a heuristic, a learned value function, or Monte-Carlo rollouts. Backups through the tree aggregate these values along with the immediate rewards accumulated along each path to estimate the value of taking an action in the current state. This paper focuses on a simple tree-search with a deterministic transition function and no value uncertainty estimates, but our approach can be extended to tree-search variants like UCT (Kocsis \&amp; Szepesvári, 2006; Silver et al., 2016) if the components remain differentiable.</p>
<h1>3 TreEQN</h1>
<p>In this section, we propose TreeQN, a novel end-to-end differentiable tree-structured architecture for deep reinforcement learning. We first give an overview of the architecture, followed by details of each model component and the training procedure.</p>
<p>TreeQN uses a recursive tree-structured neural network between the encoded state $\mathbf{z}<em t="t">{t}$ and the predicted state-action values $Q\left(\mathbf{s}</em>}, a\right)$, instead of directly estimating the state-action value from the current encoded state $\mathbf{z<em t="t">{t}$ using fully connected layers as in DQN (Mnih et al., 2015). Specifically, TreeQN uses a recursive model to refine its estimate of $Q\left(\mathbf{s}</em>, a\right)$ via learned transition, reward, and value functions, and a tree backup (see Fig. 2). Because these learned components are shared throughout the tree, TreeQN implements an inductive bias, missing from DQN, that reflects the prior knowledge that the $Q$-values are properties of a stationary Markov process. We also encode the inductive bias that $Q$-values may be expressed as a sum of scalar rewards and values.</p>
<p>Specifically, TreeQN learns an action-dependent transition function that, given a state representation $\mathbf{z}<em _mid="\mid" l_1="l+1" t="t">{l \mid t}$, predicts the next state representation $\mathbf{z}</em>}^{s_{1}}$ for action $a_{i} \in \mathcal{A}$, and the corresponding reward $\hat{r<em i="i">{l \mid t}^{s</em>}}$. To make the distinction between internal planning steps and steps taken in the environment explicit, we write $\mathbf{z<em 0="0" _mid="\mid" t="t">{l \mid t}$ to denote the encoded state at time $t$ after $l$ internal transitions, starting with $\mathbf{z}</em>$. TreeQN applies this transition function recursively to construct a tree containing the state representations and rewards received for all possible sequences of actions up to some predefined depth $d$ ("Tree Transitioning" in Fig. 2).}$ for the encoding of $\mathbf{s}_{t</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: High-level structure of TreeQN with a tree depth of two and shared transition and evaluation functions (reward prediction and value mixing omitted for simplicity).</p>
<p>The value of each predicted state $V(\mathbf{z})$ is estimated with a value function module. Using these values and the predicted rewards, TreeQN then performs a tree backup, mixing the $k$-step returns along each path in the tree using TD $(\lambda)$ (Sutton, 1988; Sutton \&amp; Barto, 1998). This corresponds to "Value Prediction \&amp; Backup" in Fig. 2 and can be formalized as</p>
<p>$$
\begin{aligned}
Q^{l}\left(\mathbf{z}<em i="i">{l \mid t}, a</em>}\right) &amp; =r\left(\mathbf{z<em i="i">{l \mid t}, a</em>}\right)+\gamma V^{(\lambda)}\left(\mathbf{z<em _mid="\mid" l="l" t="t">{l+1 \mid t}\right) \
V^{(\lambda)}\left(\mathbf{z}</em>}\right) &amp; = \begin{cases}V\left(\mathbf{z<em i="i">{l \mid t}^{a</em>\right) &amp; l=d \
(1-\lambda) V\left(\mathbf{z}}<em i="i">{l \mid t}^{a</em>}}\right)+\lambda \mathrm{b}\left(Q^{l+1}\left(\mathbf{z<em i="i">{l+1 \mid t}^{a</em>
\end{aligned}
$$}}, a_{j}\right)\right) &amp; l&lt;d\end{cases</p>
<p>where $b$ is a function to recursively perform the backup. For $0&lt;\lambda&lt;1$, value estimates of the intermediate states are mixed into the final $Q$-estimate, which encourages the intermediate nodes of the tree to correspond to meaningful states, and reduces the impact of outlier values.</p>
<p>When $\lambda=1$, and $b$ is the standard hard max function, then Eq. 3 simplifies to a backup through the tree using the familiar Bellman equation:</p>
<p>$$
Q\left(\mathbf{z}<em i="i">{l \mid t}, a</em>}\right)=r\left(\mathbf{z<em i="i">{l \mid t}, a</em>}\right)+ \begin{cases}\gamma V\left(\mathbf{z<em i="i">{d \mid t}^{a</em>\right) &amp; l=d-1 \ \gamma \max }<em j="j">{a</em>}} Q\left(\mathbf{z<em i="i">{l+1 \mid t}^{a</em>
$$}}, a_{j}\right) &amp; l&lt;d-1\end{cases</p>
<p>We note that even for a tree depth of only one, TreeQN imposes a significant structure on the value function by decomposing it as a sum of action-conditional reward and next-state value, and using a shared value function to evaluate each next-state representation.</p>
<p>Crucially, during training we backpropagate all the way from the final $Q$-estimate, through the value prediction, tree transitioning, and encoding layers of the tree, i.e., the entire network shown in Fig. 2. Learning these components jointly ensures that they are useful for planning on-line.</p>
<h1>3.1 Model COMPONENTS</h1>
<p>In this section, we describe each of TreeQN's components in more detail.
Encoder function. As in DQN, a series of convolutional layers produces an embedding of the observed state, $\mathbf{z}<em t="t">{0 \mid t}=\operatorname{encode}\left(\mathbf{s}</em>\right)$.</p>
<p>Transition function. We first apply a single fully connected layer to the current state embedding, shared by all actions. This generates an intermediate representation $\left(\mathbf{z}<em i="i">{l+1 \mid t}^{\text {env }}\right)$ that could carry information about action-agnostic changes to the environment. In addition, we use a fully connected layer per action, which is applied to the intermediate representation to calculate a next-state representation that carries information about the effect of taking action $a</em>$. We use residual connections for these layers:</p>
<p>$$
\begin{aligned}
\mathbf{z}<em _mid="\mid" l="l" t="t">{l+1 \mid t}^{\mathrm{env}} &amp; =\mathbf{z}</em>}+\tanh \left(\boldsymbol{W}^{\mathrm{env}} \mathbf{z<em _mid="\mid" l_1="l+1" t="t">{l \mid t}+\mathbf{b}^{\mathrm{env}}\right) \
\mathbf{z}</em>}^{a_{i}} &amp; =\mathbf{z<em i="i">{l+1 \mid t}^{\mathrm{env}}+\tanh \left(\boldsymbol{W}^{a</em>\right)
\end{aligned}
$$}} \mathbf{z}_{l+1 \mid t}^{\mathrm{env}</p>
<p>where $\boldsymbol{W}^{a_{i}}, \boldsymbol{W}^{\text {env }} \in \mathbb{R}^{k \times k}, \mathbf{b}^{\text {env }} \in \mathbb{R}^{k}$ are learnable parameters. Note that the next-state representation is calculated for every action $a_{i}$ independently using the respective transition matrix $\boldsymbol{W}^{a_{i}}$, but this transition function is shared for the same action throughout the tree.</p>
<p>A caveat is that the model can still learn to use different parts of the latent state space in different parts of the tree, which could undermine the intended parameter sharing in the model structure. To help TreeQN learn useful transition functions that maintain quality and diversity in their latent states, we introduce a unit-length projection of the state representations by simply dividing a state's vector representation by its L2 norm before each application of the transition function, $\mathbf{z}<em _mid="\mid" l="l" t="t">{l \mid t}:=\mathbf{z}</em>\right|$. This prevents the magnitude of the representation from growing or shrinking, which encourages the behaviour of the transition function to be more consistent throughout the tree.} /\left|\mathbf{z}_{l \mid t</p>
<p>Reward function. In addition to predicting the next state, we also predict the immediate reward for every action $a_{i} \in \mathcal{A}$ in state $\mathbf{z}_{l \mid t}$ using</p>
<p>$$
\hat{\mathbf{r}}\left(\mathbf{z}<em 2="2">{l \mid t}\right)=\boldsymbol{W}</em>}^{r} \operatorname{ReLU}\left(\boldsymbol{W<em _mid="\mid" l="l" t="t">{1}^{r} \mathbf{z}</em>}+\mathbf{b<em 2="2">{1}^{r}\right)+\mathbf{b}</em>
$$}^{r</p>
<p>where $\boldsymbol{W}<em 2="2">{1}^{r} \in \mathbb{R}^{m \times k}, \boldsymbol{W}</em>}^{r} \in \mathbb{R}^{|\mathcal{A}| \times m}$ and ReLU is the rectified linear unit (Nair \&amp; Hinton, 2010), and the predicted reward for a particular action $\hat{r<em i="i">{l \mid t}^{a</em>\right)$.}}$ is the $i$-th element of the vector $\hat{\mathbf{r}}\left(\mathbf{z}_{l \mid t</p>
<p>Value function. The value of a state representation $\mathbf{z}$ is estimated as</p>
<p>$$
V(\mathbf{z})=\mathbf{w}^{\top} \mathbf{z}+b
$$</p>
<p>where $\mathbf{w} \in \mathbb{R}^{k}$.
Backup function. We use the following function that can be recursively applied to calculate the tree backup:</p>
<p>$$
\mathrm{b}(\mathbf{x})=\sum_{i} x_{i} \operatorname{softmax}(\mathbf{x})_{i}
$$</p>
<p>Using a hard max for calculating the backup would result in gradient information only being used to update parameters along the maximal path in the tree. By contrast, the softmax allows us to use downstream gradient information to update parameters along all paths. Furthermore, it potentially reduces the impact of outlier value predictions. With a learned temperature for the softmax, this function could represent the hard max arbitrarily closely. However, we did not find an empirical difference so we left the temperature at 1 .</p>
<h1>3.2 Grounding THE MODEL COMPONENTS</h1>
<p>The TreeQN architecture is fully differentiable, so we can directly use it in the place of a $Q$-function in any deep RL algorithm with discrete actions. Differentiating through the entire tree ensures that the learned components are useful for planning on-line, as long as that planning is performed in the same way as during training.</p>
<p>However, it seems plausible that auxiliary objectives based on minimising the error in predicting rewards or observations could improve the performance by helping to ground the transition and reward functions to the environment. It could also encourage TreeQN to perform model-based planning in an interpretable manner. In principle, such objectives could give rise to a spectrum of methods from model-free to fully model-based. At one extreme, TreeQN without auxiliary objectives can be seen as a model-free approach that draws inspiration from tree-search planning to encode valuable inductive biases into the neural network architecture. At the other extreme, perfect, grounded reward and transition models could in principle be learned. Using them in our architecture would then correspond</p>
<p>to standard model-based lookahead planning. The sweet spot could be an intermediate level of grounding that maintains the flexibility of end-to-end model-free learning while benefiting from the additional supervision of explicit model learning. To investigate this spectrum, we experiment with two auxiliary objectives.</p>
<p>Reward grounding. We experiment with an L2 loss regressing $\hat{r}<em t_1_1="t+1+1">{l \mid t}^{a</em>\right}$, to the true observed rewards. For each of the $n$ timesteps of $n$-step Q-learning this gives:}}$, the predicted reward at level $l$ of the tree corresponding to the selected action sequence $\left{a_{t} \ldots a_{t+l-1</p>
<p>$$
\mathcal{L}=\mathcal{L}<em r="r">{\text {mtep-Q }}+\eta</em>} \sum_{\text {emv }} \sum_{j=1}^{n} \sum_{l=1}^{\bar{d}}\left(\hat{r<em t_j:="t+j:" t_j_l-1="t+j+l-1">{l \mid t+j}^{a</em>
$$}}-r_{t+j+l-1}\right)^{2</p>
<p>where $\eta_{r}$ is a hyperparameter weighting the loss, and $\bar{d}=\min (d, n-j+1)$ restricts the sum to rewards for which we have already observed the true value.</p>
<p>State grounding. We experiment with a grounding in the latent space, using an L2 loss to regress the predicted latent state $\mathbf{z}<em t_1="t+1">{l \mid t}^{a</em>$, the initial encoding of the true state corresponding to the actions actually taken:}}$ at level $l$ of the tree to $\mathbf{z}_{0 \mid t+l</p>
<p>$$
\mathcal{L}=\mathcal{L}<em s="s">{\text {mtep-Q }}+\eta</em>} \sum_{\text {emv }} \sum_{j=1}^{n} \sum_{l=1}^{\bar{d}}\left(\mathbf{z<em t_j:="t+j:" t_j_l-1="t+j+l-1">{l \mid t+j}^{a</em>
$$}}-\mathbf{z}_{0 \mid t+j+l}\right)^{2</p>
<p>By employing an additional decoder module, we could use a similar loss to regress decoded observations to the true observations. In informal experiments, joint training with such a decoder loss did not yield good performance, as also found by Oh et al. (2017).</p>
<p>In Section 7.1, we present results on the use these objectives, showing that reward grounding gives better performance, but that our method for state grounding does not.</p>
<h1>4 ATREEC</h1>
<p>The intuitions guiding the design of TreeQN are as applicable to policy search as to valuebased RL, in that a policy can use a tree planner to improve its estimates of the optimal action probabilities (Gelly \&amp; Silver, 2007; Silver et al., 2017a). As our proposed architecture is trained end-to-end, it can be easily adapted for use as a policy network.</p>
<p>In particular, we propose ATreeC, an actor-critic extension of TreeQN. In this architecture, the policy network is identical to TreeQN, with an additional softmax layer that converts the $Q$ esti-
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: High-level structure of ATreeC.
mates into the probabilities of a stochastic policy. The critic shares the encoder parameters, and predicts a scalar state value with a single fully connected layer: $V_{\text {cr }}(\mathbf{s})=\mathbf{w}<em _cr="{cr" _text="\text">{\text {cr }}^{\top} \mathbf{z}+b</em>$. We used different parameters for the critic value function and the actor's tree-value-function module, but found that sharing these parameters had little effect on performance. The entire setup, shown in Fig. 3, is trained with A2C as described in Section 2, with the addition of the same auxiliary losses used for TreeQN. Note that TreeQN could also be used in the critic, but we leave this possibility to future work.}</p>
<h2>5 Related Work</h2>
<p>There is a long history of work combining model-based and model-free RL. An early example is Dyna-Q (Sutton, 1990) which trains a model-free algorithm with samples drawn from a learned model. Similarly, van Seijen et al. (2011) train a sparse model with some environment samples that can be used to refine a model-free $Q$-function. Gu et al. (2016) use local linear models to generate</p>
<p>additional samples for their model-free algorithm. However, these approaches do not attempt to use the model on-line to improve value estimates.</p>
<p>In deep RL, value iteration networks (Tamar et al., 2016) use a learned differentiable model to plan on the fly, but require planning over the full state space, which must also possess a spatial structure with local dynamics such that convolution operations can execute the planning algorithm.</p>
<p>The predictron (Silver et al., 2017b) instead learns abstract-state transition functions in order to predict values. However, it is restricted to policy evaluation without control. Value prediction networks (VPNs, Oh et al., 2017) take a similar approach but are more closely related to our work because the learned model components are used in a tree for planning. However, in their work this tree is only used to construct targets and choose actions, and not to compute the value estimates during training. Such estimates are instead produced from non-branching trajectories following on-policy action sequences. By contrast, TreeQN is a unified architecture that constructs the tree dynamically at every timestep and differentiates through it, eliminating any mismatch between the model at training and test time. Furthermore, we do not use convolutional transition functions, and hence do not impose spatial structure on the latent state representations. These differences simplify training, allow our model to be used more flexibly in other training regimes, and explain in part our substantially improved performance on the Atari benchmark.</p>
<p>Donti et al. (2017) propose differentiating through a stochastic programming optimisation using a probabilistic model to learn model parameters with respect to their true objective rather than a maximum likelihood surrogate. However, they do not tackle the full RL setting, and do not use the model to repeatedly or recursively refine predictions.</p>
<p>Imagination-augmented agents (Weber et al., 2017) learn to improve policies by aggregating rollouts predicted by a model. However, they rely on pretraining an observation-space model, which we argue will scale poorly to more complex environments. Further, their aggregation of rollout trajectories takes the form of a generic RNN rather than a value function and tree backup, so the inductive bias based on the structure of the MDP is not explicitly present.</p>
<p>A class of value gradient methods (Deisenroth \&amp; Rasmussen, 2011; Fairbank \&amp; Alonso, 2012; Heess et al., 2015) also differentiates through models to train a policy. However, this approach does not use the model during execution to refine the policy, and requires continuous action spaces.</p>
<p>Oh et al. (2015) and Chiappa et al. (2017) propose methods for learning observation-prediction models in the Atari domain, but use these models only to improve exploration. Variants of scheduled sampling (Bengio et al., 2015) may be used to improve robustness of these models, but scaling to complex domains has proven challenging (Talvitie, 2014).</p>
<h1>6 EXPERIMENTS</h1>
<p>We evaluate TreeQN and ATreeC in a simple box-pushing environment, as well as on the subset of nine Atari environments that Oh et al. (2017) use to evaluate VPN. The experiments are designed to determine whether or not TreeQN and ATreeC outperform DQN, A2C, and VPN, and whether they can scale to complex domains. We also investigate how to best ground the the transition function with auxiliary losses. Furthermore, we compare against alternative ways to increase the number of parameters and computations of a standard DQN architecture, and study the impact of tree depth. Full details of the experimental setup, as well as architecture and training hyperparameters, are given in the appendix.</p>
<p>Grounding. We perform a hyperparameter search over the coefficients $\eta_{r}$ and $\eta_{s}$ of the reward and state grounding auxiliary losses, on the Atari environment Seaquest. These experiments aim to determine the relevant trade-offs between the flexibility of a model-free approach and the potential benefits of a more model-based algorithm.</p>
<p>Box Pushing. We randomly place an agent, 12 boxes, 5 goals and 6 obstacles on the center $6 \times 6$ tiles of an $8 \times 8$ grid. The agent's goal is to push boxes into goals in as few steps as possible while avoiding obstacles. Boxes may not be pushed into each other. The obstacles, however, are 'soft' in that they are do not block movement, but generate a negative reward if the agent or a box moves onto an obstacle. This rewards better planning without causing excessive gridlock. This environment is inspired by Sokoban, as used by Weber et al. (2017), in that poor actions can generate irreversibly</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" />
(a) Reward Grounding.
<img alt="img-4.jpeg" src="img-4.jpeg" />
(b) State Grounding.</p>
<p>Figure 4: Grounding the reward and transition functions using auxiliary losses: final returns on Seaquest plotted against the coefficient of the auxiliary loss.
bad configurations. However, the level generation process for Sokoban is challenging to reproduce exactly and has not been open-sourced. More details of the environment and rewards are given in Appendix A.1.</p>
<p>Atari. To demonstrate the general applicability of TreeQN and ATreeC to complex environments, we evaluate them on the Atari 2600 suite (Bellemare et al., 2013). Following Oh et al. (2017), we use their set of nine environments and a frameskip of 10 to facilitate planning over reasonable timescales.</p>
<p>TreeQN adds additional parameters to a standard DQN architecture. We compare TreeQN to two baseline architectures with increased computation and numbers of parameters to verify the benefit of the additional structure and grounding. DQN-Wide doubles the size of the embedding dimension (1024 instead of 512). DQN-Deep inserts two additional fully connected layers with shared parameters and residual connections between the two fully-connected layers of DQN. This is in effect a non-branching version of the TreeQN architecture that also lacks explicit reward prediction.</p>
<h1>7 ReSults \&amp; DisCussion</h1>
<p>In this section, we present our experimental results for TreeQN and ATreeC.</p>
<h3>7.1 Grounding</h3>
<p>Fig. 4 shows the result of a hyperparameter search on $\eta_{r}$ and $\eta_{s}$, the coefficients of the auxiliary losses on the predicted rewards and latent states. An intermediate value of $\eta_{r}$ helps performance but there is no benefit to using the latent space loss. Subsequent experiments use $\eta_{r}=1$ and $\eta_{s}=0$.</p>
<p>The predicted rewards that the reward-grounding objective encourages the model to learn appear both in its own $Q$-value prediction and in the target for $n$-step $Q$-learning. Consequently, we expect this auxiliary loss to be well aligned with the true objective. By contrast, the state-grounding loss (and other potential auxiliary losses) might help representation learning but would not explicitly learn any part of the desired target. It is possible that this mismatch between the auxiliary and primary objective leads to degraded performance when using this form of state grounding. One potential route to overcoming this obstacle to joint training would be pre-training a model, as done by Weber et al. (2017). Inside TreeQN this model could then be fine-tuned to perform well inside the planner. We leave this possiblity to future work.</p>
<h3>7.2 Box Pushing</h3>
<p>Fig. 5a shows the results of TreeQN with tree depths 1, 2, and 3, compared to a DQN baseline. In this domain, there is a clear advantage for the TreeQN architecture over DQN. TreeQN learns policies that are substantially better at avoiding obstacles and lining boxes up with goals so they can be easily</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 5: Box-pushing results: the $x$-axis shows the number of transitions observed across all of the synchronous environment threads.
pushed in later. TreeQN also substantially speeds up learning. We believe that the greater structure brought by our architecture regularises the model, encouraging appropriate state representations to be learned quickly. Even a depth-1 tree improves performance significantly, as disentangling the estimation of rewards and next-state values makes them easier to learn. This is further facilitated by the sharing of value-function parameters across branches.</p>
<p>When trained with $n$-step Q-learning, the deeper depth-2 and depth-3 trees learn faster and plateau higher than the shallow depth-1 tree. In the this domain, useful transition functions are relatively easy to learn, and the extra computation time with those transition modules can help refine value estimates, yielding advantages for additional depth.</p>
<p>Fig. 5b shows the results of ATreeC with tree depths 1, 2, and 3, compared to an A2C baseline. As with TreeQN, ATreeC substantially outperforms the baseline. Furthermore, thanks to its stochastic policy, it substantially outperforms TreeQN. Whereas TreeQN and DQN sometimes indecisively bounce back and forth between adjacent states, ATreeC captures this uncertainty in its policy probabilities and thus acts more decisively. However, unlike TreeQN, ATreeC shows no pronounced differences for different tree depths. This is in part due to a ceiling effect in this domain. However, ATreeC is also gated by the quality of the critic's value function, which in these experiments was a single linear layer after the state encoding as described in Section 4. Nonetheless, this result demonstrates the ease with which TreeQN can be used as a drop-in replacement for any deep RL algorithm that learns policies or value functions for discrete actions.</p>
<h1>7.3 ATARI</h1>
<p>Table 1 summarises all our Atari results, while Fig. 6 shows learning curves in depth. TreeQN shows substantial benefits in many environments compared to our DQN baseline, which itself often outperforms VPN (Oh et al., 2017). ATreeC always matches or outperforms A2C. We present the mean performance of five random seeds, while the VPN results reported by Oh et al. (2017), shown as dashed lines in Fig. 6, are the mean of the best five seeds of an unspecified number of trials.</p>
<p>TreeQN. In all environments except Frostbite, TreeQN outperforms DQN on average, with the most significant gains in Alien, CrazyClimber, Enduro, Krull, and Seaquest. Many of these environments seem well suited to short horizon look-ahead planning, with simple dynamics that generalise well and tradeoffs between actions that become apparent only after several timesteps. For example, an incorrect action in Alien can trap the agent down a corridor with an alien. In Seaquest, looking ahead could help determine whether it is better to go deeper to collect more points or to surface for oxygen. However, even in a game with mostly reactive decisions like the racing game Enduro, TreeQN shows significant benefits.</p>
<p>TreeQN also outperforms the additional baselines of DQN-Wide and DQN-Deep, indicating that the additional structure and grounding of our architecture brings benefits beyond simply adding model capacity and computation. In particular, it is interesting that DQN-Deep is often outperformed by the vanilla DQN baseline, as optimisation difficulties grow with depth. In contrast, the additional</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" />
(a) DQN and TreeQN trained with $n$-step $Q$-learning.
<img alt="img-7.jpeg" src="img-7.jpeg" />
(b) A2C and ATreeC.</p>
<p>Figure 6: Results for the Atari domain. The y-axis shows the moving average over 100 episodes. Each of five random seeds is plotted faintly, with the mean in bold.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Alien</th>
<th style="text-align: center;">Amidar</th>
<th style="text-align: center;">Crazy Climber</th>
<th style="text-align: center;">Enduro</th>
<th style="text-align: center;">Frostbite</th>
<th style="text-align: center;">Krull</th>
<th style="text-align: center;">Ms. Pacman</th>
<th style="text-align: center;">Q'Bert</th>
<th style="text-align: center;">Seaquest</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">DQN (Oh et al., 2017)</td>
<td style="text-align: center;">1804</td>
<td style="text-align: center;">535</td>
<td style="text-align: center;">41658</td>
<td style="text-align: center;">326</td>
<td style="text-align: center;">3058</td>
<td style="text-align: center;">12438</td>
<td style="text-align: center;">2804</td>
<td style="text-align: center;">12592</td>
<td style="text-align: center;">2951</td>
</tr>
<tr>
<td style="text-align: center;">VPN (Oh et al., 2017)</td>
<td style="text-align: center;">1429</td>
<td style="text-align: center;">641</td>
<td style="text-align: center;">54119</td>
<td style="text-align: center;">382</td>
<td style="text-align: center;">3811</td>
<td style="text-align: center;">15930</td>
<td style="text-align: center;">2689</td>
<td style="text-align: center;">14517</td>
<td style="text-align: center;">5628</td>
</tr>
<tr>
<td style="text-align: center;">$n$-step DQN</td>
<td style="text-align: center;">1969</td>
<td style="text-align: center;">1033</td>
<td style="text-align: center;">71623</td>
<td style="text-align: center;">625</td>
<td style="text-align: center;">3968</td>
<td style="text-align: center;">7860</td>
<td style="text-align: center;">2774</td>
<td style="text-align: center;">14468</td>
<td style="text-align: center;">3465</td>
</tr>
<tr>
<td style="text-align: center;">DQN-Deep</td>
<td style="text-align: center;">1906</td>
<td style="text-align: center;">825</td>
<td style="text-align: center;">53101</td>
<td style="text-align: center;">745</td>
<td style="text-align: center;">493</td>
<td style="text-align: center;">8605</td>
<td style="text-align: center;">2410</td>
<td style="text-align: center;">15094</td>
<td style="text-align: center;">3575</td>
</tr>
<tr>
<td style="text-align: center;">DQN-Wide</td>
<td style="text-align: center;">2187</td>
<td style="text-align: center;">1074</td>
<td style="text-align: center;">91380</td>
<td style="text-align: center;">682</td>
<td style="text-align: center;">3493</td>
<td style="text-align: center;">6603</td>
<td style="text-align: center;">3061</td>
<td style="text-align: center;">15794</td>
<td style="text-align: center;">3909</td>
</tr>
<tr>
<td style="text-align: center;">TreeQN-1</td>
<td style="text-align: center;">2321</td>
<td style="text-align: center;">1030</td>
<td style="text-align: center;">107983</td>
<td style="text-align: center;">800</td>
<td style="text-align: center;">2254</td>
<td style="text-align: center;">10836</td>
<td style="text-align: center;">3030</td>
<td style="text-align: center;">15688</td>
<td style="text-align: center;">9302</td>
</tr>
<tr>
<td style="text-align: center;">TreeQN-2</td>
<td style="text-align: center;">2497</td>
<td style="text-align: center;">1170</td>
<td style="text-align: center;">104932</td>
<td style="text-align: center;">825</td>
<td style="text-align: center;">581</td>
<td style="text-align: center;">11035</td>
<td style="text-align: center;">3277</td>
<td style="text-align: center;">15970</td>
<td style="text-align: center;">8241</td>
</tr>
<tr>
<td style="text-align: center;">A2C</td>
<td style="text-align: center;">2673</td>
<td style="text-align: center;">1525</td>
<td style="text-align: center;">102776</td>
<td style="text-align: center;">642</td>
<td style="text-align: center;">297</td>
<td style="text-align: center;">5784</td>
<td style="text-align: center;">4352</td>
<td style="text-align: center;">24451</td>
<td style="text-align: center;">1734</td>
</tr>
<tr>
<td style="text-align: center;">ATreeC-1</td>
<td style="text-align: center;">3448</td>
<td style="text-align: center;">1578</td>
<td style="text-align: center;">102546</td>
<td style="text-align: center;">678</td>
<td style="text-align: center;">1035</td>
<td style="text-align: center;">8227</td>
<td style="text-align: center;">4866</td>
<td style="text-align: center;">25159</td>
<td style="text-align: center;">1734</td>
</tr>
<tr>
<td style="text-align: center;">ATreeC-2</td>
<td style="text-align: center;">2813</td>
<td style="text-align: center;">1566</td>
<td style="text-align: center;">110712</td>
<td style="text-align: center;">649</td>
<td style="text-align: center;">281</td>
<td style="text-align: center;">8134</td>
<td style="text-align: center;">4450</td>
<td style="text-align: center;">25459</td>
<td style="text-align: center;">2176</td>
</tr>
</tbody>
</table>
<p>Table 1: Summary of Atari results. Each number is the best score throughout training, calculated as the mean of the last 100 episode rewards averaged over exactly five agents trained with different random seeds. Note that Oh et al. (2017) report the same statistic, but average instead over the best five of an unspecified number of agents.
structure and auxiliary loss employed by TreeQN turn its additional depth from a liability into a strength.</p>
<p>ATreeC. ATreeC matches or outperforms its baseline (A2C) in all environments. Compared to TreeQN, ATreeC's performance is better across most environments, particularly on Qbert, reflecting an overall advantage for actor-critic also found by Mnih et al. (2016) and in our box-pushing experiments. However, performance is much worse on Seaquest, revealing a deficiency in exploration as policy entropy collapses too rapidly and consequently the propensity of policy gradient methods to become trapped in a local optimum.</p>
<p>In Krull and Frostbite, most algorithms have poor performance, or high variance in returns from run to run, as agents are gated by their ability to explore. Both of these games require the completion of sub-levels in order to accumulate large scores, and none of our agents reliably explore beyond the initial stages of the game. Mean performance appears to favor TreeQN and ATreeC in Krull, and perhaps DQN in Frostbite, but the returns are too variable to draw conclusions from this number of random seeds. Combining TreeQN and ATreeC with smart exploration mechanisms is an interesting direction for future work to improve robustness of training in these types of environments.</p>
<p>Compared to the box-pushing domain, there is less of a clear performance difference between trees of different depths. In some environments (Amidar, MsPacman), greater depth does appear to be employed usefully by TreeQN to a small extent, resulting in the best-performing individual agents. However, for the Atari domain the embedding size for the transition function we use is much larger ( 512 compared to 128), and the dynamics are much more complex. Consequently, we expect that optimisation difficulties, and the challenge of learning abstract-state transition functions, impede the utility of deeper trees in some cases. We look to future work to further refine methods for learning to plan abstractly in complex domains. However, the decomposition of Q-value into reward and next-state value employed by the first tree expansion is clearly of utility in a broad range of tasks.</p>
<p>When inspecting the learned policies and trees, we find that the values sometimes correspond to intuitive reasoning about sensible policies, scoring superior action sequences above poorer ones. However, we find that the actions corresponding to branches of the tree that are scored most highly are frequently not taken in future timesteps. The flexibility of TreeQN and ATreeC allows our agents to find any useful way to exploit the computation in the tree to refine action-value estimates. As we found no effective way to strongly ground the model components without sacrificing performance, the interpretability of learned trees is limited.</p>
<h1>8 CONCLUSIONS \&amp; FUTURE WORK</h1>
<p>We presented TreeQN and ATreeC, new architectures for deep reinforcement learning in discreteaction domains that integrate differentiable on-line tree planning into the action-value function or policy. Experiments on a box-pushing domain and a set of Atari games show the benefit of these architectures over their counterparts, as well as over VPN. In future work, we intend to investigate enabling more efficient optimisation of deeper trees, encouraging the transition functions to produce interpretable plans, and integrating smart exploration.</p>
<h1>ACKNOWLEDGMENTS</h1>
<p>We thank Sasha Salter, Luisa Zintgraf, and Wendelin Böhmer for their contributions and valuable comments on drafts of this paper. This work was supported by the UK EPSRC CDT in Autonomous Intelligent Machines and Systems. This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement #637713). The NVIDIA DGX-1 used for this research was donated by the NVIDIA Corporation.</p>
<h2>REFERENCES</h2>
<p>Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. J. Artif. Intell. Res.(JAIR), 47:253-279, 2013.</p>
<p>Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. In Advances in Neural Information Processing Systems, pp. 1171-1179, 2015.</p>
<p>Cameron B Browne, Edward Powley, Daniel Whitehouse, Simon M Lucas, Peter I Cowling, Philipp Rohlfshagen, Stephen Tavener, Diego Perez, Spyridon Samothrakis, and Simon Colton. A survey of monte carlo tree search methods. IEEE Transactions on Computational Intelligence and AI in games, 4(1):1-43, 2012.</p>
<p>Silvia Chiappa, Sébastien Racaniere, Daan Wierstra, and Shakir Mohamed. Recurrent environment simulators. arXiv preprint arXiv:1704.02254, 2017.</p>
<p>Rémi Coulom. Efficient selectivity and backup operators in monte-carlo tree search. In Computers and Games, 5th International Conference, CG 2006, Turin, Italy, May 29-31, 2006. Revised Papers, pp. 72-83, 2006. doi: 10.1007/978-3-540-75538-8_7.</p>
<p>Marc Deisenroth and Carl E Rasmussen. Pilco: A model-based and data-efficient approach to policy search. In Proceedings of the 28th International Conference on machine learning (ICML-11), pp. $465-472,2011$.</p>
<p>Priya L Donti, Brandon Amos, and J Zico Kolter. Task-based end-to-end model learning. arXiv preprint arXiv:1703.04529, 2017.</p>
<p>Michael Fairbank and Eduardo Alonso. Value-gradient learning. In Neural Networks (IJCNN), The 2012 International Joint Conference on, pp. 1-8. IEEE, 2012.</p>
<p>Sylvain Gelly and David Silver. Combining online and offline knowledge in uct. In Proceedings of the 24th international conference on Machine learning, pp. 273-280. ACM, 2007.</p>
<p>Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and Sergey Levine. Continuous deep q-learning with model-based acceleration. In International Conference on Machine Learning, pp. 2829-2838, 2016.</p>
<p>Nicolas Heess, Gregory Wayne, David Silver, Tim Lillicrap, Tom Erez, and Yuval Tassa. Learning continuous control policies by stochastic value gradients. In Advances in Neural Information Processing Systems, pp. 2944-2952, 2015.</p>
<p>Christopher Hesse, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, and Yuhuai Wu. Openai baselines. https://github.com/openai/baselines, 2017.</p>
<p>Donald E Knuth and Ronald W Moore. An analysis of alpha-beta pruning. Artificial intelligence, 6 (4):293-326, 1975.</p>
<p>Levente Kocsis and Csaba Szepesvári. Bandit based monte-carlo planning. In Machine Learning: ECML 2006, 17th European Conference on Machine Learning, Berlin, Germany, September 18-22, 2006, Proceedings, pp. 282-293, 2006. doi: 10.1007/11871842_29.</p>
<p>Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533, 2015. doi: 10.1038/nature14236.</p>
<p>Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, pp. 1928-1937, 2016.</p>
<p>Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa, Israel, pp. 807-814, 2010.</p>
<p>Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L Lewis, and Satinder Singh. Action-conditional video prediction using deep networks in atari games. In Advances in Neural Information Processing Systems, pp. 2863-2871, 2015.</p>
<p>Junhyuk Oh, Satinder Singh, and Honglak Lee. Value prediction network. arXiv preprint arXiv:1707.03497, 2017.</p>
<p>Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. arXiv preprint arXiv:1705.05363, 2017.</p>
<p>David Silver, Richard S. Sutton, and Martin Müller. Temporal-difference search in computer go. Machine Learning, 87(2):183-219, 2012. doi: 10.1007/s10994-012-5280-0.</p>
<p>David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484-489, 2016. doi: 10.1038 /nature16961.</p>
<p>David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. Nature, 550(7676):354-359, 2017a.</p>
<p>David Silver, Hado van Hasselt, Matteo Hessel, Tom Schaul, Arthur Guez, Tim Harley, Gabriel Dulac-Arnold, David P. Reichert, Neil Rabinowitz, André Barreto, and Thomas Degris. The predictron: End-to-end learning and planning. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pp. 3191-3199, 2017b.</p>
<p>Nathan R Sturtevant. An analysis of uct in multi-player games. In International Conference on Computers and Games, pp. 37-49. Springer, 2008.</p>
<p>Richard S Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3 (1):9-44, 1988.</p>
<p>Richard S. Sutton. Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In Machine Learning, Proceedings of the Seventh International Conference on Machine Learning, Austin, Texas, USA, June 21-23, 1990, pp. 216-224, 1990.</p>
<p>Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998.</p>
<p>Erik Talvitie. Model regularization for stable sample rollouts. In UAI, pp. 780-789, 2014.
Erik Talvitie. Self-correcting models for model-based reinforcement learning. In AAAI, pp. 25972603, 2017.</p>
<p>Aviv Tamar, Sergey Levine, Pieter Abbeel, Yi Wu, and Garrett Thomas. Value iteration networks. In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pp. 2146-2154, 2016.</p>
<p>Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26-31, 2012.</p>
<p>Harm van Seijen, Shimon Whiteson, Hado van Hasselt, and Marco Wiering. Exploiting best-match equations for efficient reinforcement learning. Journal of Machine Learning Research, 12(Jun): 2045-2094, 2011.</p>
<p>Christopher J. C. H. Watkins and Peter Dayan. Q-learning. Machine Learning, 8:279-292, 1992. doi: 10.1007/BF00992698.</p>
<p>Theophane Weber, Sébastien Racanière, David P. Reichert, Lars Buesing, Arthur Guez, Danilo Jimenez Rezende, Adrià Puigdomènech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li, Razvan Pascanu, Peter Battaglia, David Silver, and Daan Wierstra. Imagination-augmented agents for deep reinforcement learning. CoRR, abs/1707.06203, 2017.</p>
<h1>A APPENDIX</h1>
<h2>A. 1 Box Pushing</h2>
<p>Environment. For each episode, a new level is generated by placing an agent, 12 boxes, 5 goals and 6 obstacles in the center $6 \times 6$ tiles of an $8 \times 8$ grid, sampling locations uniformly. The outer tiles are left empty to prevent initial situations where boxes cannot be recovered.</p>
<p>The agent may move in the four cardinal directions. If the agent steps off the grid, the episode ends and the agent receives a penalty of -1 . If the agent moves into a box, it is pushed in the direction of movement. Moving a box out of the grid generates a penalty of -0.1 . Moving a box into another box is not allowed and trying to do so generates a penalty of -0.1 while leaving all positions unchanged. When a box is pushed into a goal, it is removed and the agent receives a reward of +1 .</p>
<p>Obstacles generate a penalty of -0.2 when the agent or a box is moved onto them. Moving the agent over goals incurs no penalty. Lastly, at each timestep the agent receives a penalty of -0.01 . Episodes terminate when 75 timesteps have elapsed, the agent has left the grid, or no boxes remain.</p>
<p>The observation is given to the model as a tensor of size $5 \times 8 \times 8$. The first four channels are binary encodings of the position of the agent, goals, boxes, and obstacles respectively. The final channel is filled with the number of timesteps remaining (normalised by the total number of timesteps allowed).</p>
<p>Architecture. The encoder consists of (conv-3x3-1-24, conv-3x3-1-24, conv-4x4-1-48, fc-128), where conv-wxh-s-n denotes a convolution with $n$ filters of size $w \times h$ and stride $s$, and fc-h denotes a fully connected layer with $h$ hidden units. All layers are separated with ReLU nonlinearities. The hidden layer of the reward function MLP has 64 hidden units.</p>
<h2>A. 2 ATARI</h2>
<p>Preprocessing of inputs follows the procedure of Mnih et al. (2015), including concatenation of the last four frames as input, although we use a frameskip of 10.</p>
<p>Architecture. The Atari experiments have the same architecture as for box-pushing, except for the encoder architecture which is as follows: (conv-8x8-4-16, conv-4x4-2-32, fc-512).</p>
<h2>A. 3 OTHER HYPERPARAMETERS</h2>
<p>All experiments use RMSProp (Tieleman \&amp; Hinton, 2012) with a learning rate of 1e-4, a decay of $\alpha=0.99$, and $\epsilon=1 \mathrm{e}-5$.</p>
<p>The learning rate was tuned coarsely by running DQN on the Seaquest environment, and kept the same for all subsequent experiments (box-pushing and Atari).</p>
<p>For DQN and TreeQN, $\epsilon$ for $\epsilon$-greedy exploration was decayed linearly from 1 to 0.05 over the first 4 million environment transitions observed (after frameskipping, so over 40 million atomic Atari timesteps).</p>
<p>For A2C and ATreeC, we use a value-function loss coefficient $\alpha=0.5$ and an entropy regularisation $\beta=0.01$.</p>
<p>The reward prediction loss was scaled by $\eta_{r}=1$.
We use $n_{\text {steps }}=5$ and $n_{\text {envs }}=16$, for a total batch size of 80 .
The discount factor is $\gamma=0.99$ and the target networks are updated every 40,000 environment transitions.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{\dagger}$ Our code is available at https://github.com/oxwhirl/treeqn.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
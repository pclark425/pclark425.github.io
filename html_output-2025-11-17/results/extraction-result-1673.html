<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1673 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1673</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1673</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-272987197</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.20291v2.pdf" target="_blank">RL-GSBridge: 3D Gaussian Splatting Based Real2Sim2Real Method for Robotic Manipulation Learning</a></p>
                <p><strong>Paper Abstract:</strong> Sim-to-Real refers to the process of transferring policies learned in simulation to the real world, which is crucial for achieving practical robotics applications. However, recent Sim2real methods either rely on a large amount of augmented data or large learning models, which is inefficient for specific tasks. In recent years, with the emergence of radiance field reconstruction methods, especially 3D Gaussian splatting, it has become possible to construct realistic real-world scenes. To this end, we propose RL-GSBridge, a novel real-to-sim-to-real framework which incorporates 3D Gaussian Splatting into the conventional RL simulation pipeline, enabling zero-shot sim-to-real transfer for vision-based deep reinforcement learning. We introduce a mesh-based 3D GS method with soft binding constraints, enhancing the rendering quality of mesh models. Then utilizing a GS editing approach to synchronize the rendering with the physics simulator, RL-GSBridge could reflect the visual interactions of the physical robot accurately. Through a series of sim-to-real experiments, including grasping and pick-and-place tasks, we demonstrate that RL-GSBridge maintains a satisfactory success rate in real-world task completion during sim-to-real transfer. Furthermore, a series of rendering metrics and visualization results indicate that our proposed mesh-based 3D GS reduces artifacts in unstructured objects, demonstrating more realistic rendering performance.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1673.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1673.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RL-GSBridge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RL-GSBridge: 3D Gaussian Splatting Based Real2Sim2Real Method for Robotic Manipulation Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A real2sim2real framework that uses mesh-based 3D Gaussian Splatting with a soft mesh-binding constraint and physics-driven GS editing to produce photorealistic, editable simulated scenes for training vision-based RL policies that transfer zero-shot to a real robot.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>KUKA iiwa robot arm with Robotiq 2F-140 gripper</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>An industrial 7-DoF KUKA iiwa manipulator equipped with a two-finger Robotiq 2F-140 gripper and an eye-in-hand Intel RealSense D435i RGB camera for first-person visual observations; used for desktop-level grasping and pick-and-place tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>PyBullet with physics-dynamics-driven GS renderer (custom)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>PyBullet physics simulator for rigid-body dynamics and contact plus a synchronized 3D Gaussian Splatting (GS) renderer that uses mesh priors and learned Gaussians to produce photorealistic first-person images; GS models are edited online to reflect simulator poses.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Photorealistic visual rendering (high) combined with approximate physics (PyBullet) — high visual fidelity, approximate dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>High-fidelity visual appearance via 3D Gaussian Splatting (texture, viewpoint-consistent novel view synthesis), mesh-based geometric priors for object geometry, synchronized rigid-body pose updates (object poses, gripper) from PyBullet, first-person camera viewpoint and added image noise/randomized image attributes to observations.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Physics simplified to PyBullet rigid-body approximate contact dynamics (soft/deformable object interactions not accurately simulated), limited realistic lighting/relighting modelling, no detailed soft-tissue/soft-material/continuum deformation physics for non-rigid objects, and approximate friction/contact modeling (some contact discrepancies reported).</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Desktop-level testbed with foam pad or tablecloth backgrounds; target objects (Small cube, Cake, Banana, Bear) placed in a 30×30 cm area; KUKA iiwa with Robotiq gripper and Intel RealSense D435i capturing first-person RGB images; tasks judged by physical outcomes (e.g., lifted 10 cm or cake placed on plate).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Vision-based robotic manipulation: grasping various objects and a pick-and-place (cake-to-plate) task.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Deep reinforcement learning using Soft Actor-Critic (SAC) variant with a baseline controller (SACwB) to speed learning; training uses first-person RGB images from GS renderer plus proprioceptive state.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Task success rate (percentage of trials meeting task success criteria: e.g., lifted 10 cm for grasping, cake placed on plate for pick-and-place).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>Varies by object/task (examples reported): Grasping - Small cube: 96.88% (simulation); Bear: 87.50% (simulation); Pick-and-place (Cake & Plate): 68.75% (simulation).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Varies by object/task (examples reported): Grasping - Small cube: 96.88% (real); Bear: 100.00% (real); overall average sim-to-real drop across scenarios: ~6.6%; Pick-and-place (Cake & Plate): 71.87% (real, +4.54%).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Added random noise to rendered GS images and randomly altered certain image attribute parameters during training to increase robustness; no extensive physical parameter randomization (masses/frictions) detailed.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Visual discrepancies between mesh-shaded simulator and real scenes (texture/appearance mismatch) were primary drivers; inaccuracies in soft-object/ deformable modeling and differences in contact behavior between simulator and reality (e.g., PyBullet contact tolerances) also affected transfer; texture similarity between object and background (e.g., brown tablecloth and brown toy bear) reduced perception performance.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>High-fidelity photorealistic rendering using 3D Gaussian Splatting with soft mesh binding; synchronized GS editing tied to physics simulator poses (visual-physical consistency); use of SACwB with a baseline controller to accelerate learning; image-domain perturbations (noise/attribute randomization); building geometric priors (meshes) combined with GS appearance models.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Paper emphasizes that photorealistic rendering (view-consistent appearance) reduces the visual sim-to-real gap and that accurate geometric priors are important to avoid perception/interaction illusions; also notes that accurate soft-object/contact modeling is still required for fully matching some real results but was not achieved here (no quantitative thresholds provided).</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Compared RL-GSBridge (GS renderer + soft mesh binding) against RL-sim (standard mesh-shaded PyBullet images): RL-sim showed large drops when transferred (examples: Small cube 96.88%→12.50% (↓87%); Bear 93.75%→25.00% (↓73%)), while RL-GSBridge maintained similar performance with only minor changes (overall average sim-to-real drop ~6.6%, and in some cases small increases).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Integrating high-fidelity, editable 3D Gaussian Splatting rendering with physics-synchronized editing substantially reduces the visual sim-to-real gap for vision-based manipulation, enabling zero-shot transfer with small performance degradation (~6.6% average drop across scenarios); limitations remain for soft/deformable object physics and some lighting realism, and domain-randomized image perturbations plus a baseline controller aided training robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RL-GSBridge: 3D Gaussian Splatting Based Real2Sim2Real Method for Robotic Manipulation Learning', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1673.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1673.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RL-sim</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RL-sim (mesh-shaded PyBullet baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline method used in the paper: same RL pipeline but trained on conventional mesh-shaded images rendered in PyBullet rather than GS photorealistic renders; used to quantify the sim-to-real gap introduced by simplistic rendering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>KUKA iiwa robot arm with Robotiq 2F-140 gripper</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Same real robot platform as RL-GSBridge experiments; used as the deployment target for policies trained in the mesh-shaded simulator baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>PyBullet (mesh-shaded rendering)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>PyBullet physics simulation with standard mesh-based shading for rendered images (non-GS); simulates rigid-body dynamics and contacts but with simpler visual appearance.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Approximate physics + low/moderate visual fidelity (mesh shading) — simplified rendering fidelity compared to GS renderer.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Rigid-body dynamics and contacts via PyBullet, basic mesh geometry and shading for visuals.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Visual realism is simplified (mesh shading lacking view-consistent photorealism), no GS-based appearance modeling, limited realism in texture and view synthesis; same constraints on soft-object physics as GS experiments (not modeled well).</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Same real-world testbed as other experiments (KUKA iiwa + Robotiq gripper + RealSense D435i camera), tested on foam pad background in grasping experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Grasping (Small cube, Bear) in desktop-level setup (foam pad background).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Deep reinforcement learning (same algorithm as RL-GSBridge, i.e., SAC/SACwB) but using mesh-shaded rendered images.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Task success rate (percentage).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>Reported examples: Small cube: 96.88% (simulation); Bear: 93.75% (simulation).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Reported examples: Small cube: 12.50% (real); Bear: 25.00% (real) — large drops on transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Insufficient visual realism (appearance mismatch) between mesh-shaded simulator images and real camera images causing the learned policy to fail on real perception; visual artifacts and texture discrepancies produced substantial transfer degradation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Not applicable for baseline — the paper shows this baseline fails to transfer well unless visual realism is improved or other gap-closing techniques are applied.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Implied requirement: photorealistic, view-consistent rendering (as provided by GS) is needed to avoid catastrophic drops in transfer for vision-based policies; no numeric thresholds provided.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Mesh-shaded baseline (RL-sim) experienced dramatic sim-to-real performance drops (examples: Small cube 96.88%→12.50%, Bear 93.75%→25%), demonstrating that lower visual fidelity greatly hinders transfer compared to GS-rendered training.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Training purely on conventional mesh-shaded simulator images produced large sim-to-real failures for vision-based manipulation policies; improving visual realism via GS rendering mitigates these failures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RL-GSBridge: 3D Gaussian Splatting Based Real2Sim2Real Method for Robotic Manipulation Learning', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1673.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1673.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NeRF2Real</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NeRF2Real: Sim2real transfer of vision-guided bipedal motion skills using neural radiance fields</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previously published approach that converts NeRF-style reconstructions into simulator textures to enable sim-to-real transfer for vision-guided locomotion/manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Nerf2real: Sim2real transfer of vision-guided bipedal motion skills using neural radiance fields.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>sim-to-real for control/locomotion (prior work mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RL-GSBridge: 3D Gaussian Splatting Based Real2Sim2Real Method for Robotic Manipulation Learning', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1673.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1673.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ManiGaussian</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ManiGaussian: Dynamic gaussian splatting for multi-task robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited recent work that uses dynamic Gaussian splatting as a scene representation for policy learning and manipulation tasks in robotics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Manigaussian: Dynamic gaussian splatting for multi-task robotic manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotics manipulation (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RL-GSBridge: 3D Gaussian Splatting Based Real2Sim2Real Method for Robotic Manipulation Learning', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1673.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1673.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Quach et al.</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gaussian splatting to real world flight navigation transfer with liquid networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work cited that combines GS representations with specialized networks to train in simulation and deploy policies to real-world drone flight navigation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gaussian splatting to real world flight navigation transfer with liquid networks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotic flight navigation (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RL-GSBridge: 3D Gaussian Splatting Based Real2Sim2Real Method for Robotic Manipulation Learning', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1673.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1673.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GaMeS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GaMeS: Mesh-based adapting and modification of gaussian splatting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mesh-based Gaussian splatting method referenced as a baseline for mesh binding; the current paper extends GaMeS by replacing hard mesh binding with a soft binding constraint to improve flexibility and rendering quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Games: Mesh-based adapting and modification of gaussian splatting.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>radiance-field based scene modeling (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RL-GSBridge: 3D Gaussian Splatting Based Real2Sim2Real Method for Robotic Manipulation Learning', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Nerf2real: Sim2real transfer of vision-guided bipedal motion skills using neural radiance fields. <em>(Rating: 2)</em></li>
                <li>Manigaussian: Dynamic gaussian splatting for multi-task robotic manipulation. <em>(Rating: 2)</em></li>
                <li>Gaussian splatting to real world flight navigation transfer with liquid networks. <em>(Rating: 2)</em></li>
                <li>Games: Mesh-based adapting and modification of gaussian splatting. <em>(Rating: 1)</em></li>
                <li>Reinforcement learning with neural radiance fields. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1673",
    "paper_id": "paper-272987197",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "RL-GSBridge",
            "name_full": "RL-GSBridge: 3D Gaussian Splatting Based Real2Sim2Real Method for Robotic Manipulation Learning",
            "brief_description": "A real2sim2real framework that uses mesh-based 3D Gaussian Splatting with a soft mesh-binding constraint and physics-driven GS editing to produce photorealistic, editable simulated scenes for training vision-based RL policies that transfer zero-shot to a real robot.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "KUKA iiwa robot arm with Robotiq 2F-140 gripper",
            "agent_system_description": "An industrial 7-DoF KUKA iiwa manipulator equipped with a two-finger Robotiq 2F-140 gripper and an eye-in-hand Intel RealSense D435i RGB camera for first-person visual observations; used for desktop-level grasping and pick-and-place tasks.",
            "domain": "general robotics manipulation",
            "virtual_environment_name": "PyBullet with physics-dynamics-driven GS renderer (custom)",
            "virtual_environment_description": "PyBullet physics simulator for rigid-body dynamics and contact plus a synchronized 3D Gaussian Splatting (GS) renderer that uses mesh priors and learned Gaussians to produce photorealistic first-person images; GS models are edited online to reflect simulator poses.",
            "simulation_fidelity_level": "Photorealistic visual rendering (high) combined with approximate physics (PyBullet) — high visual fidelity, approximate dynamics",
            "fidelity_aspects_modeled": "High-fidelity visual appearance via 3D Gaussian Splatting (texture, viewpoint-consistent novel view synthesis), mesh-based geometric priors for object geometry, synchronized rigid-body pose updates (object poses, gripper) from PyBullet, first-person camera viewpoint and added image noise/randomized image attributes to observations.",
            "fidelity_aspects_simplified": "Physics simplified to PyBullet rigid-body approximate contact dynamics (soft/deformable object interactions not accurately simulated), limited realistic lighting/relighting modelling, no detailed soft-tissue/soft-material/continuum deformation physics for non-rigid objects, and approximate friction/contact modeling (some contact discrepancies reported).",
            "real_environment_description": "Desktop-level testbed with foam pad or tablecloth backgrounds; target objects (Small cube, Cake, Banana, Bear) placed in a 30×30 cm area; KUKA iiwa with Robotiq gripper and Intel RealSense D435i capturing first-person RGB images; tasks judged by physical outcomes (e.g., lifted 10 cm or cake placed on plate).",
            "task_or_skill_transferred": "Vision-based robotic manipulation: grasping various objects and a pick-and-place (cake-to-plate) task.",
            "training_method": "Deep reinforcement learning using Soft Actor-Critic (SAC) variant with a baseline controller (SACwB) to speed learning; training uses first-person RGB images from GS renderer plus proprioceptive state.",
            "transfer_success_metric": "Task success rate (percentage of trials meeting task success criteria: e.g., lifted 10 cm for grasping, cake placed on plate for pick-and-place).",
            "transfer_performance_sim": "Varies by object/task (examples reported): Grasping - Small cube: 96.88% (simulation); Bear: 87.50% (simulation); Pick-and-place (Cake & Plate): 68.75% (simulation).",
            "transfer_performance_real": "Varies by object/task (examples reported): Grasping - Small cube: 96.88% (real); Bear: 100.00% (real); overall average sim-to-real drop across scenarios: ~6.6%; Pick-and-place (Cake & Plate): 71.87% (real, +4.54%).",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Added random noise to rendered GS images and randomly altered certain image attribute parameters during training to increase robustness; no extensive physical parameter randomization (masses/frictions) detailed.",
            "sim_to_real_gap_factors": "Visual discrepancies between mesh-shaded simulator and real scenes (texture/appearance mismatch) were primary drivers; inaccuracies in soft-object/ deformable modeling and differences in contact behavior between simulator and reality (e.g., PyBullet contact tolerances) also affected transfer; texture similarity between object and background (e.g., brown tablecloth and brown toy bear) reduced perception performance.",
            "transfer_enabling_conditions": "High-fidelity photorealistic rendering using 3D Gaussian Splatting with soft mesh binding; synchronized GS editing tied to physics simulator poses (visual-physical consistency); use of SACwB with a baseline controller to accelerate learning; image-domain perturbations (noise/attribute randomization); building geometric priors (meshes) combined with GS appearance models.",
            "fidelity_requirements_identified": "Paper emphasizes that photorealistic rendering (view-consistent appearance) reduces the visual sim-to-real gap and that accurate geometric priors are important to avoid perception/interaction illusions; also notes that accurate soft-object/contact modeling is still required for fully matching some real results but was not achieved here (no quantitative thresholds provided).",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Compared RL-GSBridge (GS renderer + soft mesh binding) against RL-sim (standard mesh-shaded PyBullet images): RL-sim showed large drops when transferred (examples: Small cube 96.88%→12.50% (↓87%); Bear 93.75%→25.00% (↓73%)), while RL-GSBridge maintained similar performance with only minor changes (overall average sim-to-real drop ~6.6%, and in some cases small increases).",
            "key_findings": "Integrating high-fidelity, editable 3D Gaussian Splatting rendering with physics-synchronized editing substantially reduces the visual sim-to-real gap for vision-based manipulation, enabling zero-shot transfer with small performance degradation (~6.6% average drop across scenarios); limitations remain for soft/deformable object physics and some lighting realism, and domain-randomized image perturbations plus a baseline controller aided training robustness.",
            "uuid": "e1673.0",
            "source_info": {
                "paper_title": "RL-GSBridge: 3D Gaussian Splatting Based Real2Sim2Real Method for Robotic Manipulation Learning",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "RL-sim",
            "name_full": "RL-sim (mesh-shaded PyBullet baseline)",
            "brief_description": "Baseline method used in the paper: same RL pipeline but trained on conventional mesh-shaded images rendered in PyBullet rather than GS photorealistic renders; used to quantify the sim-to-real gap introduced by simplistic rendering.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "KUKA iiwa robot arm with Robotiq 2F-140 gripper",
            "agent_system_description": "Same real robot platform as RL-GSBridge experiments; used as the deployment target for policies trained in the mesh-shaded simulator baseline.",
            "domain": "general robotics manipulation",
            "virtual_environment_name": "PyBullet (mesh-shaded rendering)",
            "virtual_environment_description": "PyBullet physics simulation with standard mesh-based shading for rendered images (non-GS); simulates rigid-body dynamics and contacts but with simpler visual appearance.",
            "simulation_fidelity_level": "Approximate physics + low/moderate visual fidelity (mesh shading) — simplified rendering fidelity compared to GS renderer.",
            "fidelity_aspects_modeled": "Rigid-body dynamics and contacts via PyBullet, basic mesh geometry and shading for visuals.",
            "fidelity_aspects_simplified": "Visual realism is simplified (mesh shading lacking view-consistent photorealism), no GS-based appearance modeling, limited realism in texture and view synthesis; same constraints on soft-object physics as GS experiments (not modeled well).",
            "real_environment_description": "Same real-world testbed as other experiments (KUKA iiwa + Robotiq gripper + RealSense D435i camera), tested on foam pad background in grasping experiments reported.",
            "task_or_skill_transferred": "Grasping (Small cube, Bear) in desktop-level setup (foam pad background).",
            "training_method": "Deep reinforcement learning (same algorithm as RL-GSBridge, i.e., SAC/SACwB) but using mesh-shaded rendered images.",
            "transfer_success_metric": "Task success rate (percentage).",
            "transfer_performance_sim": "Reported examples: Small cube: 96.88% (simulation); Bear: 93.75% (simulation).",
            "transfer_performance_real": "Reported examples: Small cube: 12.50% (real); Bear: 25.00% (real) — large drops on transfer.",
            "transfer_success": false,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Insufficient visual realism (appearance mismatch) between mesh-shaded simulator images and real camera images causing the learned policy to fail on real perception; visual artifacts and texture discrepancies produced substantial transfer degradation.",
            "transfer_enabling_conditions": "Not applicable for baseline — the paper shows this baseline fails to transfer well unless visual realism is improved or other gap-closing techniques are applied.",
            "fidelity_requirements_identified": "Implied requirement: photorealistic, view-consistent rendering (as provided by GS) is needed to avoid catastrophic drops in transfer for vision-based policies; no numeric thresholds provided.",
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Mesh-shaded baseline (RL-sim) experienced dramatic sim-to-real performance drops (examples: Small cube 96.88%→12.50%, Bear 93.75%→25%), demonstrating that lower visual fidelity greatly hinders transfer compared to GS-rendered training.",
            "key_findings": "Training purely on conventional mesh-shaded simulator images produced large sim-to-real failures for vision-based manipulation policies; improving visual realism via GS rendering mitigates these failures.",
            "uuid": "e1673.1",
            "source_info": {
                "paper_title": "RL-GSBridge: 3D Gaussian Splatting Based Real2Sim2Real Method for Robotic Manipulation Learning",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "NeRF2Real",
            "name_full": "NeRF2Real: Sim2real transfer of vision-guided bipedal motion skills using neural radiance fields",
            "brief_description": "A previously published approach that converts NeRF-style reconstructions into simulator textures to enable sim-to-real transfer for vision-guided locomotion/manipulation tasks.",
            "citation_title": "Nerf2real: Sim2real transfer of vision-guided bipedal motion skills using neural radiance fields.",
            "mention_or_use": "mention",
            "agent_system_name": null,
            "agent_system_description": null,
            "domain": "sim-to-real for control/locomotion (prior work mentioned)",
            "virtual_environment_name": null,
            "virtual_environment_description": null,
            "simulation_fidelity_level": null,
            "fidelity_aspects_modeled": null,
            "fidelity_aspects_simplified": null,
            "real_environment_description": null,
            "task_or_skill_transferred": null,
            "training_method": null,
            "transfer_success_metric": null,
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": null,
            "transfer_enabling_conditions": null,
            "fidelity_requirements_identified": null,
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": null,
            "uuid": "e1673.2",
            "source_info": {
                "paper_title": "RL-GSBridge: 3D Gaussian Splatting Based Real2Sim2Real Method for Robotic Manipulation Learning",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "ManiGaussian",
            "name_full": "ManiGaussian: Dynamic gaussian splatting for multi-task robotic manipulation",
            "brief_description": "A cited recent work that uses dynamic Gaussian splatting as a scene representation for policy learning and manipulation tasks in robotics.",
            "citation_title": "Manigaussian: Dynamic gaussian splatting for multi-task robotic manipulation.",
            "mention_or_use": "mention",
            "agent_system_name": null,
            "agent_system_description": null,
            "domain": "robotics manipulation (related work)",
            "virtual_environment_name": null,
            "virtual_environment_description": null,
            "simulation_fidelity_level": null,
            "fidelity_aspects_modeled": null,
            "fidelity_aspects_simplified": null,
            "real_environment_description": null,
            "task_or_skill_transferred": null,
            "training_method": null,
            "transfer_success_metric": null,
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": null,
            "transfer_enabling_conditions": null,
            "fidelity_requirements_identified": null,
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": null,
            "uuid": "e1673.3",
            "source_info": {
                "paper_title": "RL-GSBridge: 3D Gaussian Splatting Based Real2Sim2Real Method for Robotic Manipulation Learning",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Quach et al.",
            "name_full": "Gaussian splatting to real world flight navigation transfer with liquid networks",
            "brief_description": "Prior work cited that combines GS representations with specialized networks to train in simulation and deploy policies to real-world drone flight navigation tasks.",
            "citation_title": "Gaussian splatting to real world flight navigation transfer with liquid networks.",
            "mention_or_use": "mention",
            "agent_system_name": null,
            "agent_system_description": null,
            "domain": "robotic flight navigation (prior work)",
            "virtual_environment_name": null,
            "virtual_environment_description": null,
            "simulation_fidelity_level": null,
            "fidelity_aspects_modeled": null,
            "fidelity_aspects_simplified": null,
            "real_environment_description": null,
            "task_or_skill_transferred": null,
            "training_method": null,
            "transfer_success_metric": null,
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": null,
            "transfer_enabling_conditions": null,
            "fidelity_requirements_identified": null,
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": null,
            "uuid": "e1673.4",
            "source_info": {
                "paper_title": "RL-GSBridge: 3D Gaussian Splatting Based Real2Sim2Real Method for Robotic Manipulation Learning",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "GaMeS",
            "name_full": "GaMeS: Mesh-based adapting and modification of gaussian splatting",
            "brief_description": "A mesh-based Gaussian splatting method referenced as a baseline for mesh binding; the current paper extends GaMeS by replacing hard mesh binding with a soft binding constraint to improve flexibility and rendering quality.",
            "citation_title": "Games: Mesh-based adapting and modification of gaussian splatting.",
            "mention_or_use": "mention",
            "agent_system_name": null,
            "agent_system_description": null,
            "domain": "radiance-field based scene modeling (related work)",
            "virtual_environment_name": null,
            "virtual_environment_description": null,
            "simulation_fidelity_level": null,
            "fidelity_aspects_modeled": null,
            "fidelity_aspects_simplified": null,
            "real_environment_description": null,
            "task_or_skill_transferred": null,
            "training_method": null,
            "transfer_success_metric": null,
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": null,
            "transfer_enabling_conditions": null,
            "fidelity_requirements_identified": null,
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": null,
            "uuid": "e1673.5",
            "source_info": {
                "paper_title": "RL-GSBridge: 3D Gaussian Splatting Based Real2Sim2Real Method for Robotic Manipulation Learning",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Nerf2real: Sim2real transfer of vision-guided bipedal motion skills using neural radiance fields.",
            "rating": 2,
            "sanitized_title": "nerf2real_sim2real_transfer_of_visionguided_bipedal_motion_skills_using_neural_radiance_fields"
        },
        {
            "paper_title": "Manigaussian: Dynamic gaussian splatting for multi-task robotic manipulation.",
            "rating": 2,
            "sanitized_title": "manigaussian_dynamic_gaussian_splatting_for_multitask_robotic_manipulation"
        },
        {
            "paper_title": "Gaussian splatting to real world flight navigation transfer with liquid networks.",
            "rating": 2,
            "sanitized_title": "gaussian_splatting_to_real_world_flight_navigation_transfer_with_liquid_networks"
        },
        {
            "paper_title": "Games: Mesh-based adapting and modification of gaussian splatting.",
            "rating": 1,
            "sanitized_title": "games_meshbased_adapting_and_modification_of_gaussian_splatting"
        },
        {
            "paper_title": "Reinforcement learning with neural radiance fields.",
            "rating": 1,
            "sanitized_title": "reinforcement_learning_with_neural_radiance_fields"
        }
    ],
    "cost": 0.0149595,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>RL-GSBridge: 3D Gaussian Splatting Based Real2Sim2Real Method for Robotic Manipulation Learning</p>
<p>Yuxuan Wu 
Lei Pan 
Wenhua Wu 
Guangming Wang 
Yanzi Miao 
Fan Xu 
Hesheng Wang wanghesheng@sjtu.edu.cn </p>
<p>Shenzhen Research Institute of Shanghai Jiao Tong University
518000ShenzhenChina</p>
<p>Department of Automa-tion
Shanghai Jiao Tong University
200240ShanghaiChina</p>
<p>MoE Key Lab of Artificial Intelligence
AI Institute</p>
<p>Shanghai Jiao Tong University
200240ShanghaiChina</p>
<p>School of Information and Control Engineering
China University of Mining and Technology
221100XuzhouChina</p>
<p>Department of Engineering
University of Cambridge
CB2 1PZCambridgeU.K</p>
<p>RL-GSBridge: 3D Gaussian Splatting Based Real2Sim2Real Method for Robotic Manipulation Learning
D1554813BDD5DC1B655E3B95E1BFD030
Sim-to-Real refers to the process of transferring policies learned in simulation to the real world, which is crucial for achieving practical robotics applications.However, recent Sim2real methods either rely on a large amount of augmented data or large learning models, which is inefficient for specific tasks.In recent years, with the emergence of radiance field reconstruction methods, especially 3D Gaussian splatting, it has become possible to construct realistic real-world scenes.To this end, we propose RL-GSBridge, a novel real-to-sim-to-real framework which incorporates 3D Gaussian Splatting into the conventional RL simulation pipeline, enabling zero-shot simto-real transfer for vision-based deep reinforcement learning.We introduce a mesh-based 3D GS method with soft binding constraints, enhancing the rendering quality of mesh models.Then utilizing a GS editing approach to synchronize the rendering with the physics simulator, RL-GSBridge could reflect the visual interactions of the physical robot accurately.Through a series of sim-to-real experiments, including grasping and pickand-place tasks, we demonstrate that RL-GSBridge maintains a satisfactory success rate in real-world task completion during sim-to-real transfer.Furthermore, a series of rendering metrics and visualization results indicate that our proposed mesh-based 3D GS reduces artifacts in unstructured objects, demonstrating more realistic rendering performance.</p>
<p>I. INTRODUCTION</p>
<p>Learning robotic action policies in simulation and transferring them to real-world represents an ideal robotic learning strategy that balances both the cost and safety of the learning process.However, a significant bottleneck is the reliability of sim-to-real transfer, which impacts the potential of the entire framework towards substantial challenges.</p>
<p>With the continuous development of the simulation-toreality(Sim2Real) field, extensive work is advancing progress from multiple perspectives [1], [2], [3], [4].However, most Sim2Real methods attempt to expand the distribution of training data to cover various situations that may arise in reality, or to train a highly generalized large model to learn knowledge for different tasks.This significantly increases the difficulty in training stage.</p>
<p>To avoid additional training burden and achieve ideal Sim2Real performance on specific tasks, a novel framework is needed.Recently, advances in radiance field-based reconstruction methods [5], [6], [7], [8] provide new directions for Sim2Real training.Based on a simple idea-using radiance field reconstruction to create a visually realistic robot training environment-can we achieve satisfactory Sim2Real performance?For this purpose, we design a Real2Sim2Real visual reinforcement learning framework, RL-GSBridge, that bridges the real-to-sim gap by 3D Gaussian splatting, as shown in Fig. 1.Utilizing 3D Gaussian splatting and editing techniques, RL-GSBridge provide a 'virtual-reality' simulation platform for policy learning.</p>
<p>For vision-based robot tasks, it is crucial to avoid illusions caused by inconsistencies between visual perception and contact geometry.Thus, it is required to ensure an accurate geometric representation of the model while achieving more realistic rendering results.GaMeS [9] has drawn our attention as it is a mesh-based Gaussian splatting (GS) method, which ensures that the optimization of GS units is performed within the geometric mesh model.However, it enforces Gaussians to be aligned with the mesh grid planes, which could be considered as 'hard mesh binding', thereby limiting the flexibility of GS units.To address this, we propose a soft mesh binding method for Gaussian Splatting, which could further enhance rendering quality while preserving editing capabilities for both objects and the background.</p>
<p>Physics-based dynamics simulation is also a crucial and challenging aspect of sim2real.To tackle this, an off-theshelf physics simulator is used to provide dynamic changing information.The GS editing process simultaneously updates the scene, ensuring that the rendering results align with physical interaction processes.</p>
<p>Based on the designed simulation platform, we train robotic manipulation policies by deep reinforcement learning methods.The model is trained on grasping and pick-andplace tasks across scenarios that include diverse textures, geometric shapes, and patterned desktop backgrounds.</p>
<p>Under the RL-GSBridge framework, the policy shows a minor variation in success rates during Sim2Real transfer.This means that the policy maintains effective performance on real-world tasks, reflecting a strong ability to generalize from simulation to real-world environments.</p>
<p>To summarize our contributions:</p>
<p>• A Novel Sim2Real RL Framework: Leveraging the high-fidelity rendering of 3D GS and the convenience of modeling scenes with only consumer-grade cameras.• A Soft Mesh Binding GS Modeling Method: Proposing a soft mesh binding strategy to replace the hard mesh binding baseline, enhancing the flexibility and render quality.GSBridge framework on physical robots through grasping and pick-and-place tasks in real-world scenarios with complex textures and geometries.</p>
<p>II. RELATED WORK</p>
<p>A. Sim2Real Transfer in RL</p>
<p>RL constructs an interactive learning model in which an agent learns to maximize rewards through trial and error.By incorporating deep learning, deep reinforcement learning(DRL) enhances the framework's ability to tackle more complex tasks [10].DRL has shown impressive performance across various domains, including games [11], finance [12], autonomous driving [13], and robotics [14].However, most applications are restricted to virtual environments due to realworld constraints related to safety, efficiency, and cost.</p>
<p>To improve the feasibility of deploying models in the real world, many researchers strive to bridge the gap between simulation and reality [15].These methods include domain randomization [16] and domain adaptation [17], [18].Domain randomization involves varying task-related parameters in the simulation to cover a broad range of real-world conditions, while domain adaptation focuses on extracting a unified feature space from both sources.Higher-level learning methods include metalearning [19] and distillation learning [20].Meta-learning aims to teach robots the ability to learn new tasks, whereas distillation learning trains a student network using knowledge from expert networks.</p>
<p>Our approach aligns with the first category of gap-bridging methods, but with a unique twist.We use soft mesh binding GS to create realistic simulation environments for robot training.Typically, achieving such fidelity requires expensive 3D scanning equipment or CAD expertise.In contrast, GS models visually realistic simulation environments using only multi-view images captured with consumer-grade devices.</p>
<p>B. Radiance Field in Robotics</p>
<p>Neural Radiance Fields (NeRF) [5] is an implicit representation technique for novel view synthesis.It optimizes the parameters through multi-view images, and allows for the synthesis of any target views using volumetric rendering.NeRF's representation has been applied to various tasks, including Simultaneous Localization and Mapping (SLAM) [7], [21], [22], scene reconstruction [23], scene segmentation [24], navigation [25], and manipulation [26].</p>
<p>NeRF-RL [27] treats the novel view synthesis task of NeRF as a proxy task, where the learned encoding network is directly used as feature input for reinforcement learning.Y. Li et al. [28] uses NeRF as a perceptual decoder for the hidden states of a world model, training an additional dynamics estimation network to predict future state changes.NeRF2Real [29] learns real-world contexts by converting background meshes into a simulator, and trains robots to perform visual navigation, obstacle avoidance, and ballhandling tasks.However, the training and rendering speed of Vanilla NeRF has consistently been a bottleneck limiting its further deployment in practical applications.</p>
<p>3D GS [30] is an explicit radiance field method that directly updates the attributes of each 3D Gaussian component to optimize scene representation.3D GS employs a splatting technique.for rendering, and achieve extremely fast training efficiency through CUDA parallel technology.Moreover, compared to the implicit representation of NeRF, the explicit representation facilitates tracking dynamic scene modeling and editing scene content.</p>
<p>Many robotics works also incorporate 3D Gaussian techniques for perception and motion learning [31].ManiGaussian [32] uses 3D GS as visual and dynamic scene representation for policy learning.Quach et al. [33] combine GS with Liquid networks for real-world drone flight navigation tasks, training in simulation and then deploying the policy in the real world.</p>
<p>In contrast to NeRF2Real [29], which directly textures the foreground objects in simulator, we model each foreground object with editable GS parameters.Also unlike the method designed by Quach et al. [33], which trains drone navigation policies for target-oriented tasks, our approach involves robotic arm manipulation tasks with complex interactions.</p>
<p>III. METHODS</p>
<p>RL-GSBridge aims to harness the potential of high-fidelity 3D GS models in Sim2Real for robot action training tasks.In this paper, we focus on manipulation tasks for robotic arms.As shown in Fig. 1, the overall framework is divided into two parts: Real2Sim and Sim2Real.In Real2Sim stage, we collect real-world image data {I k }, where I k represent the image sequences of the k-th object or background in the scenarios.We will model both the geometry and appearance of the scene to build a simulation environment for the manipulation task.In Sim2Real stage, we use visual perception and deep reinforcement learning to train a policy network, and directly transfer the policy to the real world.1) Real-world Data Preparing: We use a monocular camera or mobile phone to capture a 1-2 minute video of the target object on the experimental platform.We select approximately 200 keyframes from the video and use COLMAP [34] to obtain the camera's internal and external parameters for each frame.Image segmentation algorithm is also needed for extracting the target object, and we use an off-the-shelf segmenter, SAM-track [35], for efficient object segmentation.</p>
<p>To complete the GS model reconstruction for the simulator, it is also necessary to pre-model a geometrically accurate mesh model as a prior model for GS.Here we consider a classic and stable open-source package openMVS to obtain the corresponding mesh model.</p>
<p>2) Real2Sim modeling by soft mesh binding GS: With the object mesh, we can define Gaussian units within triangular faces as in GaMeS [9], a method that binds Gaussians onto the surface of meshes, and optimizes their properties through multi-view consistency.Vanilla GS [30] optimizes the attribute parameters of Gaussian units located at each point cloud position, where θ i = (µ i , r i , s i , σ i , c i ) denotes the position, rotation, scale, opacity, and color of the ith Gaussian unit g i , respectively.To achieve controllable geometric editing effects, GaMeS [9] constrains Gaussian units within the triangular mesh of the object surface, using the mean vector as the convex combination of the mesh vertices to establish the positional relationship between the Gaussian unit and the three vertices of the triangular mesh:
µ i α i 1 , α i 2 , α i 3 = α i 1 v 1 + α i 2 v 2 + α i 3 v 3 ,(1)
here, v 1 , v 2 , v 3 represents the triangular mesh vertex positions, α i 1 , α i 2 , α i 3 are learnable positional weight parameters for g i .However, this approach of enforced mesh-GS binding would diminish the flexibility of 3D Gaussians, limiting the optimization of Gaussian units when the mesh model is not accurate, introducing certain undesirable defects.Releasing the hard constraints of GaMeS [9] in the normal direction for and more flexible object surfaces.</p>
<p>To address this, we propose a soft mesh binding method, which builds upon GaMeS [9], but relaxes the enforced hard mesh binding to a soft binding constraint.We introduce a component along the normal direction into the vector of positions, specifically:
µ i α i 1 , α i 2 , α i 3 , α i n = α i 1 v 1 + α i 2 v 2 + α i 3 v 3 + α i n v n ,(2)
here, α i n is a learnable weight parameter and v n is the normal vector.α i n is constrained within the range of [−1, 1], ensuring the association between each mesh model element and Gaussian pairs.As shown in Fig. 2, our method allows the Gaussian units within the mesh to float within a certain range along the normal vector.This flexibility in Gaussian unit optimization could bring a smoother distribution of Gaussian units on the object's surface.Ultimately, the algorithm not only ensures that the Gaussian model can still represent the accurate geometric structure according to the mesh models, but also offers some tolerance and refinement space.Besides, the binds between the mesh grid and Gaussians could even provide the possibility to handle non-rigid objects, as demonstrated in section IV-B. 4.</p>
<p>3) Physic Dynamics-Based GS Editing: After obtaining the visual GS models {G k } and geometry models {M k } for real-world objects, we combine the dynamic simulation results from the simulator with real-time Gaussian model updates and render views to ensure that the visual representation follows the entire physical change process.We first use RANSAC plane regression and manual alignment methods to align GS models with mesh models in the simulator, and set the initial position of the GS model.</p>
<p>With the aligned initial model, we read the real-time pose changes of each object in the operational scene from the simulator.For the k-th object with its GS model {G k }, We acquire the rotation quaternion q k and the homogeneous transformation matric T k in the world coordinate system.Given Gaussian parameters
θ i k = (µ i k , r i k , s i k , σ i k , c i k ),
The editing process of each Gaussian g i k in the model set G k could be formulated as:
µ i k = T k µ i k , r i k = q k × r i k . (3)
Physical Simulator Simulated Eye-in-hand Image ...  As for local non-rigid transformations on the mesh grid, the Gaussians could be updated according to Equation 2. After applying transformations to all object models, we concat the Gaussian sets to acquire the Gaussian model of the whole scene G scene .Then we use rasterization to render G scene , obtaining the synchronized edited rendering view.</p>
<p>B. Sim2Real: Train in simulation with physic dynamicsbased GS renderer and zero-shot transfer to reality</p>
<p>We use Pybullet [36] as the simulation training platform, and employ SAC (Soft Actor-Critic) [37] algorithm for policy learning, due to its mature development in RL and widespread application in robotics.The SAC is an offline policy algorithm belonging to maximum entropy RL, which consists of two critic networks, Q φ ,1 (s, a) and Q φ ,2 (s, a), and one actor network, π θ (s).For the sampled set B from the replay buffer, the update loss of critic is defined as:
Loss critic = 1
|B| ∑(s t, a t, r t+1 ,s t+1 )∈B (y t − Q φ , j (s t, a t )) 2 , (4) here, (s t, , a t, , r t+1 , s t+1 ) is a tuple belong to B, the y t is calculated by target Q networks Q φ , j :
y t = r t + γ( min j=1,2 Q φ , j (s t+1 , a t+1 ) − α log π θ (a t+1 |s t+1 )). (5)
In continuous action space, policy π θ outputs actions of Gaussian distribution.The loss for actor is defined as:
Loss actor = 1 |B| ∑(s t, a t, r t+1 ,s t+1 )∈B (α log π θ ( a t |s t ) − min j=1,2 Q φ , j (s t , a t )),(6)
where a t is sampled using the reparameterization trick, i.e., a t = f θ (ε t ; s t ), ε is Gaussian random noise.In practical implementation, we set:
f θ = tanh(µ θ (s t ) + σ θ (s t ) ⊙ ε t ).
Since vanilla SAC struggles to converge rapidly under sparse reward, to accelerate the learning process through automated guidance, we propose SACwB by referring to DDPGwB [38], and introduce a lightly designed baseline controller to guide the policy.This approach avoids complex reward designs while eliminating ineffective action spaces.</p>
<p>In SACwB, the agent executes actions from the baseline controller with probability λ and selects the best option between the baseline controller's and the actor's outputs with probability 1 − λ , the objective is typically defined as:
y t = r t + γ max(( min j=1,2 Q φ , j (s t+1 , a t+1 ) − α log π θ (a t+1 |s t+1 )), ( min j=1,2 Q φ , j (s t+1 , µ b (s t+1 )) − α log π θ (µ b (s t+1 )|s t+1 ))). (7)
For the supervision of the actor network, we introduce an additional behavior clone loss L bc in Equation 6to supervise the mean values of the action distribution through base controller actions, with the probability λ :
L bc = ∥µ θ (s) − µ b (s)∥ 2 , (8)
and with the probability 1 − λ :   here, λ is the decay factor, which gradually approaches 0 as training progresses.
L bc = µ θ (s) − µ| arg max( min j=1,2 Q φ , j (s, µ θ (s))) 2 ,(9)
The whole training pipeline in the simulator with physic dynamics-based GS renderer is shown in Fig. 3.The RL algorithm provides executable actions to the physics simulator, which will return state information for actor-critic learning.The GS renderer simultaneously renders the realistic images by physic dynamics-based GS rendering, serving as observation input to the actor network.We rely solely on the first-person perspective image and proprioceptive state of the robotic arm, considering several advantages as described in [39].To enhance sim2real robustness, we add random noise to the rendered images of the GS model and randomly alter certain image attribute parameters during training for environment challenges.</p>
<p>After policy training in the simulator, we directly deploy the actor network onto the real robot arm, with the realworld eye-in-hand camera observations of the environment serving as visual input.The trained policy subsequently outputs the position of the end-effector and the gripper states as executable actions.</p>
<p>IV. EXPERIMENTS</p>
<p>A. Experiment Setup 1) Robot Platform: We use a KUKA iiwa robot arm paired with a Robotiq 2F-140 gripper as the platform.</p>
<p>The Intel RealSense D435i camera fixed on the robot arm captures the RGB images from the first-person perspective.</p>
<p>2) Tasks: As shown in Fig. 1, we design two types of tasks from the robot's first-person perspective: grasping and pick-and-place operations.</p>
<p>For grasping, the robot grasps a target object and lifts it up.The initial positions of the objects are randomized within a 30 × 30 cm section.We use Small cube, Cake, Banana, and Bear as objects.As for the operation platform, we use a foam pad with and without a tablecloth as two different backgrounds.Success is considered when the object is lifted 10 cm above the table.</p>
<p>For pick-and-place tasks, the robotic arm pick up the cake model and place it on a plate.The position of the cake is set similarly as described in the grasping task.The plate is placed in a fixed position on the foam pad.Success is considered when the cake is placed on the plate.</p>
<p>3) Evaluation Setup: For each task, we conduct both the simulation and the real world experiments.During testing, we divide the 30 × 30 cm section into four quadrants.For each task, we test the policy across a fixed number of positions in each quadrant to calculate the success rate.</p>
<p>4) Baseline: To demonstrate that our method effectively reduces Sim2Real gap, we conduct a baseline experiment called RL-sim: using the same learning method but training directly on images rendered from mesh models shaded in the PyBullet simulator.We select two representative and easily shaded objects: the Small cube and Bear.Grasping experiments for RL-sim are conducted on a foam pad.</p>
<p>B. Experiment Results</p>
<p>1) Grasping: In Table I, we compare the grasping performance of RL strategies trained with the baseline (RL-sim) and RL-GSBridge in both simulation and real environments.For both the Small cube with simple geometry and textures and the Bear with complex geometry and textures, RL-sim  shows a significant success rate drop when transferring to real world (an average decrease of 80%) due to visual discrepancies.In contrast, RL-GSBridge demonstrates a minor variation in success rates, maintaining high performance as in simulation.Notably, in the Bear grasping scenario, the success rate in the real world has increased by 14.28%.This may be attributed to the suboptimal simulation of Bear's soft material and non-structured shape in the simulation.</p>
<p>Table II shows that RL-GSBridge experiences an average drop of 6.6% of the success rate in sim-to-real transfer across various complex test scenarios, including diverse objects and desktop backgrounds.This demonstrates that the integration of the physic dynamics-based GS rendering effectively bridges the perception gap between simulation and real environments, maintaining stable strategy transfer across a range of scenarios.Additionally, we observe a noticeable decrease in success rates for the Bear in the Tablecloth background scenario, both in simulation and real environments.The primary reason for this drop is the choice of a brown tablecloth, which has similar texture features to the brown toy bear, causing difficulty in visual perception.</p>
<p>2) Pick-and-Place: As shown in Table III, we test RL-GSBridge's Sim2Real performance in pick-and-place task where a cake is placed onto a plate.The results indicate a 4.54% increase of success rate in real environments, mainly due to the differences in physical contacts between simulation and reality.In simulation, even minor excess contacts during the placement process are considered as task failures.In contrast, some contacts in real environments that do not affect the task can be tolerated, leading to better performance since the task is ultimately completed successfully.</p>
<p>3) Comparison of Sim&amp;Real Behavior Consistency: In Fig. 4, we compare the behavior of the robotic arm in the  IV, we compare the performance of GaMeS [9] and our soft mesh binding GS model under various scenarios and achieve the SOTA performance.Furthermore, Fig. 5 shows that our method obtains fewer artifacts and more detailed texture rendering.As a supplement, in Fig. 6, our soft binding constraint GS modeling method achieves consistent results in editing a non-rigid toy bear.Unfortunately, due to the limitations in simulating soft objects in PyBullet, we do not demonstrate comprehensive experiments on deformable objects manipulation.However, this can be explored as a future direction.</p>
<p>V. CONCLUSIONS</p>
<p>We propose RL-GSBridge, a real-to-sim-to-real framework for robotic reinforcement learning.As an attempt to apply the recently successful radiance field reconstruction methods to construct a realistic robotic simulator, RL-GSBridge has shown promising sim-to-real success rates in desktop-level tasks.This motivates us to explore future directions, such as investigating the simulation of realistic lighting [40], and integrating RL-GSBridge with advanced large-scale policy models [41] and perception learning methods [42], [43], [44].We hope RL-GSBridge will encourage more attempts to apply radiance field reconstruction methods in robotics.</p>
<p>Fig. 1 .
1
Fig. 1.Pipeline of RL-GSBridge.(1) Real2Sim Environment Transfer.Real-world scenarios is reconstructed through a novel soft mesh binding GS model.(2) Learn Policy at Simulator with GS Render.With physical dynamics-based GS editing, RL policies learn through realistic rendered images in simulation.(3) Zero-shot Real-world Robot Manipulation.We directly apply the policy to real-world tasks without fine-tuning.</p>
<p>A. Real2Sim: Building simulator with soft mesh binding GS To obtain a realistic simulation environment, we use consumer-grade cameras to capture 2D image data I k of desktop-level operating platforms.Our goal is to model a geometrically accurate and texture-realistic simulation model for training operational tasks.More specifically, we use mesh models {M k } to represent the accurate geometric information, and Gaussian sets G k = {g i k } for high-quality texture.Below, we sequentially describe the steps to construct the Simulator with GS renderer.</p>
<p>Fig. 2 .
2
Fig. 2. Mesh-based GS Reconstruction with Soft Binding Constraints:Releasing the hard constraints of GaMeS[9] in the normal direction for and more flexible object surfaces.</p>
<p>Fig. 3 .
3
Fig. 3. Policy training pipeline in RL-GSBridge.In the upper half of the figure, physic dynamics-based GS editing receives the transformation signals of objects and synchronizes the states of GS models.In the lower half of the figure, an actor-critic RL network receives first-person perspective images rendered by GS models as input, to learn a vision-based manipulation policy.</p>
<p>Fig. 4 .
4
Fig. 4. Comparison of sim-to-real behavior consistency between RL-GSBridge and RL-sim.</p>
<p>Fig. 5 .
5
Fig. 5. Our soft binding constraint reconstruction method compared with GaMeS [9] on two foreground objects and two backgrounds.</p>
<p>Fig. 6 .
6
Fig.6.The editing capability on non-rigid objects of our soft binding constraint GS modeling method.</p>
<p>TABLE I COMPARISON
I
OF SUCCESS RATES BETWEEN RL-GSBRIDGE AND RL-SIM IN GRASPING EXPERIMENTS, ALL CONDUCTING UNDER FOAM PAD(FP) BACKGROUND.BOLD INDICATES BETTER RESULTS.VALUES
IN PARENTHESES REPRESENT RELATIVE CHANGE IN SUCCESS RATEDURING SIM2REAL TRANSFER. (↓XX%) INDICATES A DECREASE,WHILE (↑XX%) INDICATES AN INCREASE.ObjectSmall cubeBearTest sceneSimRealSimRealRL-sim96.88 12.50 (↓87%) 93.75 25.00 (↓73%)RL-GSBridge 96.8896.8887.50 100.00 (↑14%)</p>
<p>TABLE II SIM2REAL
II
RESULTS FOR GRASPING TASK IN VARIOUS MANIPULATION SCENARIOS.CONTENTS IN PARENTHESES AFTER THE OBJECT NAMES REPRESENT DIFFERENT BACKGROUNDS(BG), WHERE FP DENOTES FOAM PAD, AND TC DENOTES TABLECLOTH.100.00 100.00 100.00 93.75 (↓6%) 96.88 87.50 (↓10%) 100.00 93.75 (↓6%) 100.00 96.88 (↓3%) 87.50 75.00 (↓14%)
Object (Bg)Cake (FP)Banana (FP)Small cube (TC)Cake (TC)Banana (TC)Bear (TC)Test sceneSim Real SimRealSimRealSimRealSimRealSimRealSuccess rate (%) SimulationReal World (Ours)Real World (RL-sim)CameraViewRobot ArmBehaviour</p>
<p>TABLE III THE
III
SIM2REAL RESULT FOR THE PICK AND PLACE TASK.
ObjectCake &amp; PlateTest sceneSimRealSuccess rate (%)68.7571.87 (↑4%)</p>
<p>TABLE IV OUR
IV
SOFT MESH BINDING METHOD COMPARED WITH GAMES [9] ON MULTIPLE FOREGROUND OBJECTS AND BACKGROUND RENDERING METRICS.SSIM IS SCALE STRUCTURAL SIMILARITY INDEX.PSNR IS PEAK SIGNAL-TO-NOISE RATIO.LPIPS IS LEARNED PERCEPTUAL IMAGE PATCH SIMILARITY.BG REFERS TO THE BACKGROUND, WHILE CONTENTS IN PARENTHESES EXPLAIN THE DETAILED DIFFERENCES.Ours GaMeS Ours GaMeS Ours GaMeS Ours GaMeS Ours GaMeS Ours GaMeS Ours GaMeS SSIM↑ 0.989 0.894 0.964 0.956 0.975 0.964 0.989 0.989 0.944 0.939 0.781 0.776 0.776 0.756 PSNR↑ 35.46 26.53 29.82 28.18 33.38 30.73 37.18 36.64 28.40 27.32 22.88 21.86 21.86 20.80 LPIPS↓ 0.026 0.049 0.034 0.040 0.066 0.079 0.012 0.012 0.144 0.149 0.248 0.308 0.308 0.311
BananaBearCakeSmall cubeBg (Foam Pad) Bg (Tablecloth)Bg (Plate)BananaCakeBg (tablecloth)Bg (plate)GTGaMeSOurs
This work was supported in part by the Shenzhen Science and Technology Program under Grant KJZD20230923114812027.
Towards closing the sim-to-real gap in collaborative multi-robot deep reinforcement learning. W Zhao, J P Queralta, L Qingqing, T Westerlund, 2020 5th International conference on robotics and automation engineering (ICRAE). IEEE2020</p>
<p>Bayesian domain randomization for sim-to-real transfer. F Muratore, C Eilers, M Gienger, J Peters, arXiv:2003.024712020arXiv preprint</p>
<p>Meta reinforcement learning for sim-to-real domain adaptation. K Arndt, M Hazara, A Ghadirzadeh, V Kyrki, 2020 IEEE international conference on robotics and automation (ICRA). IEEE2020</p>
<p>Continual reinforcement learning deployed in reallife using policy distillation and sim2real transfer. R Traoré, H Caselles-Dupré, T Lesort, T Sun, N Díaz-Rodríguez, D Filliat, arXiv:1906.044522019arXiv preprint</p>
<p>Nerf: Representing scenes as neural radiance fields for view synthesis. B Mildenhall, P P Srinivasan, M Tancik, J T Barron, R Ramamoorthi, R Ng, ECCV. 2020</p>
<p>K Zhang, G Riegler, N Snavely, V Koltun, arXiv:2010.07492Nerf++: Analyzing and improving neural radiance fields. 2020arXiv preprint</p>
<p>Nice-slam: Neural implicit scalable encoding for slam. Z Zhu, S Peng, V Larsson, W Xu, H Bao, Z Cui, M R Oswald, M Pollefeys, CVPR. 2022</p>
<p>Sni-slam: Semantic neural implicit slam. S Zhu, G Wang, H Blum, J Liu, L Song, M Pollefeys, H Wang, 2024CVPR</p>
<p>Games: Mesh-based adapting and modification of gaussian splatting. J Waczyńska, P Borycki, S Tadeja, J Tabor, P Spurek, arXiv:2402.014592024arXiv preprint</p>
<p>An introduction to deep reinforcement learning. V Franc ¸ois-Lavet, P Henderson, R Islam, M G Bellemare, J Pineau, Foundations and Trends® in Machine Learning. 201811</p>
<p>Mastering the game of go with deep neural networks and tree search. D Silver, A Huang, C J Maddison, A Guez, L Sifre, G Van Den Driessche, J Schrittwieser, I Antonoglou, V Panneershelvam, M Lanctot, nature. 52975872016</p>
<p>Deep direct reinforcement learning for financial signal representation and trading. Y Deng, F Bao, Y Kong, Z Ren, Q Dai, IEEE transactions on neural networks and learning systems. 201628</p>
<p>Virtual to real reinforcement learning for autonomous driving. X Pan, Y You, Z Wang, C Lu, arXiv:1704.039522017arXiv preprint</p>
<p>Asymmetric actor critic for image-based robot learning. L Pinto, M Andrychowicz, P Welinder, W Zaremba, P Abbeel, arXiv:1710.065422017arXiv preprint</p>
<p>Aligning cyber space with physical world: A comprehensive survey on embodied ai. Y Liu, W Chen, Y Bai, J Luo, X Song, K Jiang, Z Li, G Zhao, J Lin, G Li, arXiv:2407.068862024arXiv preprint</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE2017</p>
<p>Using simulation and domain adaptation to improve efficiency of deep robotic grasping. K Bousmalis, A Irpan, P Wohlhart, Y Bai, M Kelcey, M Kalakrishnan, L Downs, J Ibarz, P Pastor, K Konolige, 2018 IEEE international conference on robotics and automation (ICRA). IEEE2018</p>
<p>Your data is not perfect: Towards cross-domain out-of-distribution detection in class-imbalanced data. X Fang, A Easwaran, B Genest, P N Suganthan, 2024ESWA</p>
<p>Learning to reinforcement learn. J X Wang, Z Kurth-Nelson, D Tirumala, H Soyer, J Z Leibo, R Munos, C Blundell, D Kumaran, M Botvinick, arXiv:1611.057632016arXiv preprint</p>
<p>Policy distillation. A A Rusu, S G Colmenarejo, C Gulcehre, G Desjardins, J Kirkpatrick, R Pascanu, V Mnih, K Kavukcuoglu, R Hadsell, arXiv:1511.062952015arXiv preprint</p>
<p>Sni-slam: Semantic neural implicit slam. S Zhu, G Wang, H Blum, J Liu, L Song, M Pollefeys, H Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202421177</p>
<p>Semgauss-slam: Dense semantic gaussian splatting slam. S Zhu, R Qin, G Wang, J Liu, H Wang, arXiv:2403.074942024arXiv preprint</p>
<p>Monosdf: Exploring monocular geometric cues for neural implicit surface reconstruction. Z Yu, S Peng, M Niemeyer, T Sattler, A Geiger, NeurIPS. 322022</p>
<p>In-place scene labelling and understanding with implicit scene representation. S Zhi, T Laidlow, S Leutenegger, A J Davison, ICCV. 2021847</p>
<p>Vision-only robot navigation in a neural radiance world. M Adamkiewicz, T Chen, A Caccavale, R Gardner, P Culbertson, J Bohg, M Schwager, RA-L. 2022</p>
<p>Graspnerf: multiview-based 6-dof grasp detection for transparent and specular objects using generalizable nerf. Q Dai, Y Zhu, Y Geng, C Ruan, J Zhang, H Wang, ICRA. 2023</p>
<p>Reinforcement learning with neural radiance fields. D Driess, I Schubert, P Florence, Y Li, M Toussaint, NeurIPS. 2022</p>
<p>3d neural scene representations for visuomotor control. Y Li, S Li, V Sitzmann, P Agrawal, A Torralba, CoRL. 2022</p>
<p>Nerf2real: Sim2real transfer of vision-guided bipedal motion skills using neural radiance fields. A Byravan, J Humplik, L Hasenclever, A Brussee, F Nori, T Haarnoja, B Moran, S Bohez, F Sadeghi, B Vujatovic, ICRA. 2023</p>
<p>3d gaussian splatting for real-time radiance field rendering. B Kerbl, G Kopanas, T Leimkuehler, G Drettakis, ACM Trans. Graph. 4242023</p>
<p>. S Zhu, G Wang, D Kong, H Wang, arXiv:2410.122622024arXiv preprint3d gaussian splatting in robotics: A survey</p>
<p>Manigaussian: Dynamic gaussian splatting for multi-task robotic manipulation. G Lu, S Zhang, Z Wang, C Liu, J Lu, Y Tang, arXiv:2403.083212024arXiv preprint</p>
<p>Gaussian splatting to real world flight navigation transfer with liquid networks. A Quach, M Chahine, A Amini, R Hasani, D Rus, arXiv:2406.151492024arXiv preprint</p>
<p>Structure-from-motion revisited. J L Schönberger, J.-M Frahm, Conference on Computer Vision and Pattern Recognition (CVPR). 2016</p>
<p>Segment and track anything. Y Cheng, L Li, Y Xu, X Li, Z Yang, W Wang, Y Yang, arXiv:2305.065582023arXiv preprint</p>
<p>Pybullet, a python module for physics simulation for games, robotics and machine learning. E Coumans, Y Bai, 2016</p>
<p>Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. T Haarnoja, A Zhou, P Abbeel, S Levine, International conference on machine learning. PMLR2018</p>
<p>Learning of longhorizon sparse-reward robotic manipulator tasks with base controllers. G Wang, M Xin, W Wu, Z Liu, H Wang, IEEE Transactions on Neural Networks and Learning Systems. 3532022</p>
<p>Vision-based manipulators need to also see from their hands. K Hsu, M J Kim, R Rafailov, J Wu, C Finn, International Conference on Learning Representations. 2021</p>
<p>Relightable 3d gaussians: Realistic point cloud relighting with brdf decomposition and ray tracing. J Gao, C Gu, Y Lin, Z Li, H Zhu, X Cao, L Zhang, Y Yao, European Conference on Computer Vision. Springer2024</p>
<p>A survey on vision-language-action models for embodied ai. Y Ma, Z Song, Y Zhuang, J Hao, I King, arXiv:2405.140932024arXiv preprint</p>
<p>Structured world models from human videos. R Mendonca, S Bahl, D Pathak, arXiv:2308.109012023arXiv preprint</p>
<p>You can ground earlier than see: An effective and efficient pipeline for temporal sentence grounding in compressed videos. X Fang, D Liu, P Zhou, G Nan, CVPR. 2023</p>
<p>Rethinking weakly-supervised video temporal grounding from a game perspective. X Fang, Z Xiong, W Fang, X Qu, C Chen, J Dong, K Tang, P Zhou, Y Cheng, D Liu, ECCV. 2025</p>            </div>
        </div>

    </div>
</body>
</html>
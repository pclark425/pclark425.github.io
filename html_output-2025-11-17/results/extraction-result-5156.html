<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5156 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5156</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5156</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-109.html">extraction-schema-109</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <p><strong>Paper ID:</strong> paper-247187974</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2203.00600v1.pdf" target="_blank">Dual Embodied-Symbolic Concept Representations for Deep Learning</a></p>
                <p><strong>Paper Abstract:</strong> Motivated by recent findings from cognitive neural science, we advocate the use of a dual-level model for concept representations: the embodied level consists of concept-oriented feature representations, and the symbolic level consists of concept graphs. Embodied concept representations are modality specific and exist in the form of feature vectors in a feature space. Symbolic concept representations, on the other hand, are amodal and language specific, and exist in the form of word / knowledge-graph embeddings in a concept / knowledge space. The human conceptual system comprises both embodied representations and symbolic representations, which typically interact to drive conceptual processing. As such, we further advocate the use of dual embodied-symbolic concept representations for deep learning. To demonstrate their usage and value, we discuss two important use cases: embodied-symbolic knowledge distillation for few-shot class incremental learning, and embodied-symbolic fused representation for image-text matching. Dual embodied-symbolic concept representations are the foundation for deep learning and symbolic AI integration. We discuss two important examples of such integration: scene graph generation with knowledge graph bridging, and multimodal knowledge graphs.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5156.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5156.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dual embodied-symbolic model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dual Embodied-Symbolic Concept Representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-level functional model in which concepts are represented both as modality-specific embodied feature representations (e.g., sensory/motor feature vectors) and as amodal, language-linked symbolic structures (e.g., concept/knowledge graphs); the two levels interact during conceptual processing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Dual embodied-symbolic model</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Concepts are represented at two interacting functional levels: (1) an embodied level of modality-specific feature/vector representations that capture sensorimotor-derived knowledge, and (2) a symbolic level of amodal, language-linked concept graphs or word embeddings that capture distributional/relational knowledge; both levels are employed adaptively depending on task demands.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>Hybrid: distributed/feature-based embodied representations + amodal symbolic/graph representations</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>Two distinct representational formats (modality-specific vectors and amodal graphs); interaction and alignment between levels; context-sensitive recruitment (linguistic shortcut vs sensorimotor simulation); supports compositional relational structure at the symbolic level and perceptual/detail-rich representations at the embodied level.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Cited behavioral and neuroscientific findings summarized in the paper: category production studies showing both sensorimotor and linguistic predictors; neurocognitive evidence favoring modality-specific and amodal hub interactions for concrete concepts; improved downstream ML tasks (few-shot CIL, image-text matching) when dual representations are used in algorithms described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Noted challenges include grounding abstract concepts that lack direct sensorimotor referents (necessitating additional emotional/social/linguistic modalities) and the difficulty of learning/integrating amodal symbolic graphs from raw data; paper argues these are addressed by hybridizing representations but acknowledges open implementation questions.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Categorization and category production (human data cited); image-text matching and retrieval; few-shot class-incremental learning; scene-graph generation and bridging to commonsense KGs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Presented as superior to purely embodied or purely symbolic models by combining strengths: embodied models capture perceptual detail while symbolic/graphic representations capture amodal relational/compositional structure; contrasts with distributional-only approaches which lack grounded sensorimotor detail.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Adaptive recruitment: linguistic/symbolic shortcuts are used when distributional information suffices; sensorimotor simulation (embodied vectors) is engaged for detailed representations or perceptually grounded tasks; alignment mechanisms map embodied vectors into symbolic/semantic spaces (e.g., via projection or distillation) to enable interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>How to learn and align the two levels from data in a biologically plausible way; how abstract concepts are grounded across emotional/introspective/social modalities; specifics of the mapping/translation mechanisms between embodied vectors and symbolic graphs remain under-specified and are an active area for ML and cognitive work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual Embodied-Symbolic Concept Representations for Deep Learning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5156.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5156.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dual coding framework</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dual Coding of Knowledge in the Human Brain</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Framework proposing that conceptual knowledge is represented both via sensorimotor-derived (imagistic/embodied) codes and via language-derived (verbal/symbolic) codes that can be stored and used separately or together.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dual Coding of Knowledge in the Human Brain</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Dual coding framework</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Meaning is supported by two representational formats: imagistic/embodied codes derived from sensorimotor experience and verbal/amodal codes derived from linguistic systems; both contribute to conceptual representation and can be differentially engaged.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>Dual-format: imagistic/embodied representations + amodal/verbal symbolic representations</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>Separate but interacting codes; modality-specific richness (imagistic) versus language-mediated abstraction (verbal); potential redundancy and facilitation between codes.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Referenced neurocognitive and behavioral literature summarized in the paper arguing for modality-specific (sensorimotor) and linguistic contributions to concept use and memory; supports findings in category production and multimodal model comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Purely imagistic accounts cannot readily explain abstract concept representation; pure verbal accounts miss perceptual detailâ€”thus dual coding proposed to reconcile evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Explains results in category production, conceptual retrieval, and supports architectures that combine image/text embeddings with knowledge graphs in ML.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Contrast to purely embodied or purely distributional/symbolic theories; positioned as integrative, explaining why both kinds of information predict behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Parallel codes can be consulted independently or co-activated; linguistic codes allow fast, economical responses (linguistic shortcut) while imagistic codes allow detailed simulation when required.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>Mechanistic details of how codes are aligned and how task demands control selection between codes; how many modalities beyond vision/language are needed for full coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual Embodied-Symbolic Concept Representations for Deep Learning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5156.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5156.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linguistic shortcut hypothesis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linguistic Shortcut Hypothesis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Hypothesis that people favor computationally cheaper linguistic/distributional information to support conceptual tasks when it suffices, resorting to sensorimotor simulation only when more detailed representation is required.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Linguistic shortcut hypothesis</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Functional-level proposal that the cognitive system uses distributional linguistic knowledge as a fast, low-cost proxy for conceptual information in many tasks, invoking embodied/simulative resources only when task demands require richer, perceptually grounded representations.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>Strategy over representational formats: distributional/symbolic shortcuts vs embodied/simulative representations</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>Cost-sensitive selection between representations; context- and task-dependent recruitment; explains fast linguistic-driven responses vs slower simulation-driven responses.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Supported by category production data and analyses in which linguistic proximity predicts naming order and frequency above sensorimotor similarity, indicating use of distributional cues in normal conceptual tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Does not deny embodied contributions but needs specification of boundary conditions (when simulation is invoked); also must account for tasks where distributional cues are insufficient or misleading.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Category production, lexical access, tasks where distributional corpora-derived signals predict human responses; motivates ML designs that use semantic word vectors for quick generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Explains divergence between purely simulation-based embodied theories and distributional-only accounts by positing an economic selection mechanism; complements dual coding/hybrid models.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>A decision-like mechanism that assesses sufficiency of distributional cues and either returns a linguistic-based response or triggers sensorimotor simulation to enrich the representation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>What internal signals determine sufficiency; how the brain implements rapid assessment and switching; empirical boundaries across tasks and concept types (concrete vs abstract).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual Embodied-Symbolic Concept Representations for Deep Learning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5156.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5156.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hybrid hub-and-spoke</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hybrid models: modality-specific spokes + amodal semantic hubs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Hybrid functional models posit that modality-specific systems (spokes) represent perceptual/action/emotional features while one or more amodal semantic hubs integrate these features into overarching, supramodal conceptual representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Varieties of Abstract Concepts and Their Grounding in Perception or Action</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Hybrid hub-and-spoke model</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Conceptual knowledge is functionally distributed: modality-specific regions encode feature-level content (spokes) and amodal hub regions encode integrated, abstracted conceptual representations that support generalization and cross-modal access.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>Hybrid: modality-specific feature representations (spokes) + amodal integrative hub representations</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>Integration of multiple modalities; supports both modality-tied detail and supramodal abstraction; enables generalization across contexts and modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Neurocognitive evidence for interactions between modality-specific, multimodal and amodal hub areas for concrete concepts; consistent with lesion and imaging studies cited in the paper's review.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Precise localization and computational role of hubs vs spokes debated; accounting for abstract concepts requires extending spokes to include emotional/introspective/social modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Concrete concept representation, semantic memory, concept generalization, and supports ML ideas that combine modality-specific embeddings with centralized semantic embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Improves on strictly modality-specific embodied theories by providing a mechanism for amodal abstraction and on strictly symbolic theories by preserving perceptual detail in spokes.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Message-passing style integration: modality-specific feature encodings are combined in hub(s) to produce supramodal representations; hubs support cross-modal retrieval and inference.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>How hubs compute abstract representations from heterogeneous spokes; how many hubs or levels of hubs exist; how abstract, social, and affective content are integrated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual Embodied-Symbolic Concept Representations for Deep Learning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5156.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5156.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Category production findings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Category Production: sensorimotor vs linguistic predictors</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical finding that both sensorimotor similarity (measured across multiple perceptual/action dimensions) and linguistic distributional proximity predict which category members are produced and their order/frequency, with linguistic proximity contributing above sensorimotor similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Linguistic Distributional Knowledge and Sensorimotor Grounding both Contribute to Semantic Category Production</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Category production empirical finding (sensorimotor + distributional predictors)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Behavioral data from free category naming tasks show that conceptual access and retrieval are jointly influenced by perceptual/sensorimotor feature overlap and by linguistic co-occurrence structures; linguistic cues often bias early responses.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>Empirical evidence for hybrid representations: feature-based embodied information + distributional/symbolic information</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>Both distributions over linguistic contexts and feature-similarity structure guide retrieval; order/frequency of production reflect combined weights.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Category production experiments using an 11-dimensional sensorimotor strength representation and corpus-derived word co-occurrence measures found both predictors significant; linguistic proximity exerted additional predictive power.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Findings imply that purely embodied accounts are insufficient; need to explain why linguistic proximity sometimes overrides sensorimotor similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Category production, naming tasks, semantic memory retrieval benchmarks; informs cognitive models and ML systems for class prediction and semantic grouping.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Supports hybrid/dual models over single-format models and motivates mechanism like linguistic shortcut to explain rapid responses.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Interaction during retrieval where distributional cues can provide a fast route to candidate items and sensorimotor similarity refines or verifies selections.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>Relative weighting of linguistic vs sensorimotor information across concept types and tasks; how these weights are learned or adaptively set.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual Embodied-Symbolic Concept Representations for Deep Learning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5156.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5156.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Grounding of abstract concepts findings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Grounding of Concrete and Abstract Concepts (multimodal grounding evidence)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evidence that concrete concepts rely on modality-specific and amodal hub interactions, whereas abstract concepts recruit broader modal systems (emotional, introspective, social, linguistic) and are best explained by hybrid models that include these modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Varieties of Abstract Concepts and Their Grounding in Perception or Action</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Multimodal grounding for abstract concepts</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Abstract concepts lack direct perceptual referents and are functionally represented by a wider set of modal contributions (e.g., emotional, introspective, social) plus linguistic/metaphoric mappings, integrated by multimodal and amodal systems.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>Multimodal hybrid: emotional/introspective/social modality representations + linguistic/amodal integration</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>Greater inter-subject/context variability; reliance on non-sensorimotor modalities; integration via amodal hubs; more dependence on language and social/emotional systems than concrete concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Review of studies indicates that abstract concept representation engages emotional/introspective/social processing systems and linguistic information; multimodal models that include affective inputs better capture human judgments for abstract items.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Challenges purely embodied accounts which struggle to ground abstract concepts; modeling the heterogeneity of abstract concepts requires many modalities and context-dependence.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Modeling abstract-concept semantics, semantic similarity judgments, psycholinguistic tasks involving abstract word meaning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Extends hybrid hub-and-spoke approaches by emphasizing additional non-perceptual modalities for abstract concepts; outperforms unimodal embodied or purely distributional models on abstract items when multimodal inputs are included.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Combination of metaphorical mapping, situational/relational encoding, emotional/introspective features and linguistic distributional cues to form functional representations that support use and inference.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>Which modalities are essential for particular abstract concepts and how stable these representations are across individuals and cultures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual Embodied-Symbolic Concept Representations for Deep Learning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5156.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5156.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multimodal model comparison study</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Visual and Affective Multimodal Models of Word Meaning in Language and Mind</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical comparison showing that multimodal models combining linguistic with perceptual or affective information better capture human representational behavior for both concrete and abstract concepts than unimodal linguistic models alone.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Visual and Affective Multimodal Models of Word Meaning in Language and Mind</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Multimodal semantic models (distributional + perceptual/affective)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Models that combine distributional linguistic vectors with perceptual (e.g., visual) and affective features provide richer functional representations of concept meaning that align better with human judgments and behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>Multimodal distributed representations: concatenated or integrated vectors combining linguistic, perceptual, and affective components</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>Integrative: captures complementary sources of information; better discrimination at basic levels; supports both perceptual detail and affective/social dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Study cited demonstrates that multimodal models outperform unimodal (linguistic-only) models in explaining human behavior, particularly for basic-level concepts and when perceptual/affective information is relevant.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Requires gathering/encoding perceptual and affective features; weighting and integration strategies influence performance; may still struggle with highly abstract or culturally variable concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Semantic similarity judgments, concept clustering, language-and-vision tasks, informs ML architectures that fuse modalities for improved semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Shown to beat pure distributional and pure word-association models in behavioral prediction; supports the shift away from unimodal distributional explanations of semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Integration of vector sources (e.g., concatenation, projection, multimodal fusion) to form composite representations used in similarity, retrieval, and categorization processes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>Best practices for fusion, which perceptual/affective features are necessary, and how to scale multimodal information collection for broad vocabularies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual Embodied-Symbolic Concept Representations for Deep Learning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Dual Coding of Knowledge in the Human Brain <em>(Rating: 2)</em></li>
                <li>Linguistic Distributional Knowledge and Sensorimotor Grounding both Contribute to Semantic Category Production <em>(Rating: 2)</em></li>
                <li>Varieties of Abstract Concepts and Their Grounding in Perception or Action <em>(Rating: 2)</em></li>
                <li>Visual and Affective Multimodal Models of Word Meaning in Language and Mind <em>(Rating: 2)</em></li>
                <li>Reconciling Deep Learning with Symbolic Artificial Intelligence: Representing Objects and Relations <em>(Rating: 1)</em></li>
                <li>Bridging Knowledge Graphs to Generate Scene Graphs <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5156",
    "paper_id": "paper-247187974",
    "extraction_schema_id": "extraction-schema-109",
    "extracted_data": [
        {
            "name_short": "Dual embodied-symbolic model",
            "name_full": "Dual Embodied-Symbolic Concept Representation",
            "brief_description": "A two-level functional model in which concepts are represented both as modality-specific embodied feature representations (e.g., sensory/motor feature vectors) and as amodal, language-linked symbolic structures (e.g., concept/knowledge graphs); the two levels interact during conceptual processing.",
            "citation_title": "here",
            "mention_or_use": "use",
            "theory_or_model_name": "Dual embodied-symbolic model",
            "theory_or_model_description": "Concepts are represented at two interacting functional levels: (1) an embodied level of modality-specific feature/vector representations that capture sensorimotor-derived knowledge, and (2) a symbolic level of amodal, language-linked concept graphs or word embeddings that capture distributional/relational knowledge; both levels are employed adaptively depending on task demands.",
            "representation_format_type": "Hybrid: distributed/feature-based embodied representations + amodal symbolic/graph representations",
            "key_properties": "Two distinct representational formats (modality-specific vectors and amodal graphs); interaction and alignment between levels; context-sensitive recruitment (linguistic shortcut vs sensorimotor simulation); supports compositional relational structure at the symbolic level and perceptual/detail-rich representations at the embodied level.",
            "empirical_support": "Cited behavioral and neuroscientific findings summarized in the paper: category production studies showing both sensorimotor and linguistic predictors; neurocognitive evidence favoring modality-specific and amodal hub interactions for concrete concepts; improved downstream ML tasks (few-shot CIL, image-text matching) when dual representations are used in algorithms described in this paper.",
            "empirical_challenges": "Noted challenges include grounding abstract concepts that lack direct sensorimotor referents (necessitating additional emotional/social/linguistic modalities) and the difficulty of learning/integrating amodal symbolic graphs from raw data; paper argues these are addressed by hybridizing representations but acknowledges open implementation questions.",
            "applied_domains_or_tasks": "Categorization and category production (human data cited); image-text matching and retrieval; few-shot class-incremental learning; scene-graph generation and bridging to commonsense KGs.",
            "comparison_to_other_models": "Presented as superior to purely embodied or purely symbolic models by combining strengths: embodied models capture perceptual detail while symbolic/graphic representations capture amodal relational/compositional structure; contrasts with distributional-only approaches which lack grounded sensorimotor detail.",
            "functional_mechanisms": "Adaptive recruitment: linguistic/symbolic shortcuts are used when distributional information suffices; sensorimotor simulation (embodied vectors) is engaged for detailed representations or perceptually grounded tasks; alignment mechanisms map embodied vectors into symbolic/semantic spaces (e.g., via projection or distillation) to enable interaction.",
            "limitations_or_open_questions": "How to learn and align the two levels from data in a biologically plausible way; how abstract concepts are grounded across emotional/introspective/social modalities; specifics of the mapping/translation mechanisms between embodied vectors and symbolic graphs remain under-specified and are an active area for ML and cognitive work.",
            "uuid": "e5156.0",
            "source_info": {
                "paper_title": "Dual Embodied-Symbolic Concept Representations for Deep Learning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Dual coding framework",
            "name_full": "Dual Coding of Knowledge in the Human Brain",
            "brief_description": "Framework proposing that conceptual knowledge is represented both via sensorimotor-derived (imagistic/embodied) codes and via language-derived (verbal/symbolic) codes that can be stored and used separately or together.",
            "citation_title": "Dual Coding of Knowledge in the Human Brain",
            "mention_or_use": "mention",
            "theory_or_model_name": "Dual coding framework",
            "theory_or_model_description": "Meaning is supported by two representational formats: imagistic/embodied codes derived from sensorimotor experience and verbal/amodal codes derived from linguistic systems; both contribute to conceptual representation and can be differentially engaged.",
            "representation_format_type": "Dual-format: imagistic/embodied representations + amodal/verbal symbolic representations",
            "key_properties": "Separate but interacting codes; modality-specific richness (imagistic) versus language-mediated abstraction (verbal); potential redundancy and facilitation between codes.",
            "empirical_support": "Referenced neurocognitive and behavioral literature summarized in the paper arguing for modality-specific (sensorimotor) and linguistic contributions to concept use and memory; supports findings in category production and multimodal model comparisons.",
            "empirical_challenges": "Purely imagistic accounts cannot readily explain abstract concept representation; pure verbal accounts miss perceptual detailâ€”thus dual coding proposed to reconcile evidence.",
            "applied_domains_or_tasks": "Explains results in category production, conceptual retrieval, and supports architectures that combine image/text embeddings with knowledge graphs in ML.",
            "comparison_to_other_models": "Contrast to purely embodied or purely distributional/symbolic theories; positioned as integrative, explaining why both kinds of information predict behavior.",
            "functional_mechanisms": "Parallel codes can be consulted independently or co-activated; linguistic codes allow fast, economical responses (linguistic shortcut) while imagistic codes allow detailed simulation when required.",
            "limitations_or_open_questions": "Mechanistic details of how codes are aligned and how task demands control selection between codes; how many modalities beyond vision/language are needed for full coverage.",
            "uuid": "e5156.1",
            "source_info": {
                "paper_title": "Dual Embodied-Symbolic Concept Representations for Deep Learning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Linguistic shortcut hypothesis",
            "name_full": "Linguistic Shortcut Hypothesis",
            "brief_description": "Hypothesis that people favor computationally cheaper linguistic/distributional information to support conceptual tasks when it suffices, resorting to sensorimotor simulation only when more detailed representation is required.",
            "citation_title": "",
            "mention_or_use": "mention",
            "theory_or_model_name": "Linguistic shortcut hypothesis",
            "theory_or_model_description": "Functional-level proposal that the cognitive system uses distributional linguistic knowledge as a fast, low-cost proxy for conceptual information in many tasks, invoking embodied/simulative resources only when task demands require richer, perceptually grounded representations.",
            "representation_format_type": "Strategy over representational formats: distributional/symbolic shortcuts vs embodied/simulative representations",
            "key_properties": "Cost-sensitive selection between representations; context- and task-dependent recruitment; explains fast linguistic-driven responses vs slower simulation-driven responses.",
            "empirical_support": "Supported by category production data and analyses in which linguistic proximity predicts naming order and frequency above sensorimotor similarity, indicating use of distributional cues in normal conceptual tasks.",
            "empirical_challenges": "Does not deny embodied contributions but needs specification of boundary conditions (when simulation is invoked); also must account for tasks where distributional cues are insufficient or misleading.",
            "applied_domains_or_tasks": "Category production, lexical access, tasks where distributional corpora-derived signals predict human responses; motivates ML designs that use semantic word vectors for quick generalization.",
            "comparison_to_other_models": "Explains divergence between purely simulation-based embodied theories and distributional-only accounts by positing an economic selection mechanism; complements dual coding/hybrid models.",
            "functional_mechanisms": "A decision-like mechanism that assesses sufficiency of distributional cues and either returns a linguistic-based response or triggers sensorimotor simulation to enrich the representation.",
            "limitations_or_open_questions": "What internal signals determine sufficiency; how the brain implements rapid assessment and switching; empirical boundaries across tasks and concept types (concrete vs abstract).",
            "uuid": "e5156.2",
            "source_info": {
                "paper_title": "Dual Embodied-Symbolic Concept Representations for Deep Learning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Hybrid hub-and-spoke",
            "name_full": "Hybrid models: modality-specific spokes + amodal semantic hubs",
            "brief_description": "Hybrid functional models posit that modality-specific systems (spokes) represent perceptual/action/emotional features while one or more amodal semantic hubs integrate these features into overarching, supramodal conceptual representations.",
            "citation_title": "Varieties of Abstract Concepts and Their Grounding in Perception or Action",
            "mention_or_use": "mention",
            "theory_or_model_name": "Hybrid hub-and-spoke model",
            "theory_or_model_description": "Conceptual knowledge is functionally distributed: modality-specific regions encode feature-level content (spokes) and amodal hub regions encode integrated, abstracted conceptual representations that support generalization and cross-modal access.",
            "representation_format_type": "Hybrid: modality-specific feature representations (spokes) + amodal integrative hub representations",
            "key_properties": "Integration of multiple modalities; supports both modality-tied detail and supramodal abstraction; enables generalization across contexts and modalities.",
            "empirical_support": "Neurocognitive evidence for interactions between modality-specific, multimodal and amodal hub areas for concrete concepts; consistent with lesion and imaging studies cited in the paper's review.",
            "empirical_challenges": "Precise localization and computational role of hubs vs spokes debated; accounting for abstract concepts requires extending spokes to include emotional/introspective/social modalities.",
            "applied_domains_or_tasks": "Concrete concept representation, semantic memory, concept generalization, and supports ML ideas that combine modality-specific embeddings with centralized semantic embeddings.",
            "comparison_to_other_models": "Improves on strictly modality-specific embodied theories by providing a mechanism for amodal abstraction and on strictly symbolic theories by preserving perceptual detail in spokes.",
            "functional_mechanisms": "Message-passing style integration: modality-specific feature encodings are combined in hub(s) to produce supramodal representations; hubs support cross-modal retrieval and inference.",
            "limitations_or_open_questions": "How hubs compute abstract representations from heterogeneous spokes; how many hubs or levels of hubs exist; how abstract, social, and affective content are integrated.",
            "uuid": "e5156.3",
            "source_info": {
                "paper_title": "Dual Embodied-Symbolic Concept Representations for Deep Learning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Category production findings",
            "name_full": "Category Production: sensorimotor vs linguistic predictors",
            "brief_description": "Empirical finding that both sensorimotor similarity (measured across multiple perceptual/action dimensions) and linguistic distributional proximity predict which category members are produced and their order/frequency, with linguistic proximity contributing above sensorimotor similarity.",
            "citation_title": "Linguistic Distributional Knowledge and Sensorimotor Grounding both Contribute to Semantic Category Production",
            "mention_or_use": "mention",
            "theory_or_model_name": "Category production empirical finding (sensorimotor + distributional predictors)",
            "theory_or_model_description": "Behavioral data from free category naming tasks show that conceptual access and retrieval are jointly influenced by perceptual/sensorimotor feature overlap and by linguistic co-occurrence structures; linguistic cues often bias early responses.",
            "representation_format_type": "Empirical evidence for hybrid representations: feature-based embodied information + distributional/symbolic information",
            "key_properties": "Both distributions over linguistic contexts and feature-similarity structure guide retrieval; order/frequency of production reflect combined weights.",
            "empirical_support": "Category production experiments using an 11-dimensional sensorimotor strength representation and corpus-derived word co-occurrence measures found both predictors significant; linguistic proximity exerted additional predictive power.",
            "empirical_challenges": "Findings imply that purely embodied accounts are insufficient; need to explain why linguistic proximity sometimes overrides sensorimotor similarity.",
            "applied_domains_or_tasks": "Category production, naming tasks, semantic memory retrieval benchmarks; informs cognitive models and ML systems for class prediction and semantic grouping.",
            "comparison_to_other_models": "Supports hybrid/dual models over single-format models and motivates mechanism like linguistic shortcut to explain rapid responses.",
            "functional_mechanisms": "Interaction during retrieval where distributional cues can provide a fast route to candidate items and sensorimotor similarity refines or verifies selections.",
            "limitations_or_open_questions": "Relative weighting of linguistic vs sensorimotor information across concept types and tasks; how these weights are learned or adaptively set.",
            "uuid": "e5156.4",
            "source_info": {
                "paper_title": "Dual Embodied-Symbolic Concept Representations for Deep Learning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Grounding of abstract concepts findings",
            "name_full": "Grounding of Concrete and Abstract Concepts (multimodal grounding evidence)",
            "brief_description": "Evidence that concrete concepts rely on modality-specific and amodal hub interactions, whereas abstract concepts recruit broader modal systems (emotional, introspective, social, linguistic) and are best explained by hybrid models that include these modalities.",
            "citation_title": "Varieties of Abstract Concepts and Their Grounding in Perception or Action",
            "mention_or_use": "mention",
            "theory_or_model_name": "Multimodal grounding for abstract concepts",
            "theory_or_model_description": "Abstract concepts lack direct perceptual referents and are functionally represented by a wider set of modal contributions (e.g., emotional, introspective, social) plus linguistic/metaphoric mappings, integrated by multimodal and amodal systems.",
            "representation_format_type": "Multimodal hybrid: emotional/introspective/social modality representations + linguistic/amodal integration",
            "key_properties": "Greater inter-subject/context variability; reliance on non-sensorimotor modalities; integration via amodal hubs; more dependence on language and social/emotional systems than concrete concepts.",
            "empirical_support": "Review of studies indicates that abstract concept representation engages emotional/introspective/social processing systems and linguistic information; multimodal models that include affective inputs better capture human judgments for abstract items.",
            "empirical_challenges": "Challenges purely embodied accounts which struggle to ground abstract concepts; modeling the heterogeneity of abstract concepts requires many modalities and context-dependence.",
            "applied_domains_or_tasks": "Modeling abstract-concept semantics, semantic similarity judgments, psycholinguistic tasks involving abstract word meaning.",
            "comparison_to_other_models": "Extends hybrid hub-and-spoke approaches by emphasizing additional non-perceptual modalities for abstract concepts; outperforms unimodal embodied or purely distributional models on abstract items when multimodal inputs are included.",
            "functional_mechanisms": "Combination of metaphorical mapping, situational/relational encoding, emotional/introspective features and linguistic distributional cues to form functional representations that support use and inference.",
            "limitations_or_open_questions": "Which modalities are essential for particular abstract concepts and how stable these representations are across individuals and cultures.",
            "uuid": "e5156.5",
            "source_info": {
                "paper_title": "Dual Embodied-Symbolic Concept Representations for Deep Learning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Multimodal model comparison study",
            "name_full": "Visual and Affective Multimodal Models of Word Meaning in Language and Mind",
            "brief_description": "Empirical comparison showing that multimodal models combining linguistic with perceptual or affective information better capture human representational behavior for both concrete and abstract concepts than unimodal linguistic models alone.",
            "citation_title": "Visual and Affective Multimodal Models of Word Meaning in Language and Mind",
            "mention_or_use": "mention",
            "theory_or_model_name": "Multimodal semantic models (distributional + perceptual/affective)",
            "theory_or_model_description": "Models that combine distributional linguistic vectors with perceptual (e.g., visual) and affective features provide richer functional representations of concept meaning that align better with human judgments and behavior.",
            "representation_format_type": "Multimodal distributed representations: concatenated or integrated vectors combining linguistic, perceptual, and affective components",
            "key_properties": "Integrative: captures complementary sources of information; better discrimination at basic levels; supports both perceptual detail and affective/social dimensions.",
            "empirical_support": "Study cited demonstrates that multimodal models outperform unimodal (linguistic-only) models in explaining human behavior, particularly for basic-level concepts and when perceptual/affective information is relevant.",
            "empirical_challenges": "Requires gathering/encoding perceptual and affective features; weighting and integration strategies influence performance; may still struggle with highly abstract or culturally variable concepts.",
            "applied_domains_or_tasks": "Semantic similarity judgments, concept clustering, language-and-vision tasks, informs ML architectures that fuse modalities for improved semantics.",
            "comparison_to_other_models": "Shown to beat pure distributional and pure word-association models in behavioral prediction; supports the shift away from unimodal distributional explanations of semantics.",
            "functional_mechanisms": "Integration of vector sources (e.g., concatenation, projection, multimodal fusion) to form composite representations used in similarity, retrieval, and categorization processes.",
            "limitations_or_open_questions": "Best practices for fusion, which perceptual/affective features are necessary, and how to scale multimodal information collection for broad vocabularies.",
            "uuid": "e5156.6",
            "source_info": {
                "paper_title": "Dual Embodied-Symbolic Concept Representations for Deep Learning",
                "publication_date_yy_mm": "2022-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Dual Coding of Knowledge in the Human Brain",
            "rating": 2,
            "sanitized_title": "dual_coding_of_knowledge_in_the_human_brain"
        },
        {
            "paper_title": "Linguistic Distributional Knowledge and Sensorimotor Grounding both Contribute to Semantic Category Production",
            "rating": 2,
            "sanitized_title": "linguistic_distributional_knowledge_and_sensorimotor_grounding_both_contribute_to_semantic_category_production"
        },
        {
            "paper_title": "Varieties of Abstract Concepts and Their Grounding in Perception or Action",
            "rating": 2,
            "sanitized_title": "varieties_of_abstract_concepts_and_their_grounding_in_perception_or_action"
        },
        {
            "paper_title": "Visual and Affective Multimodal Models of Word Meaning in Language and Mind",
            "rating": 2,
            "sanitized_title": "visual_and_affective_multimodal_models_of_word_meaning_in_language_and_mind"
        },
        {
            "paper_title": "Reconciling Deep Learning with Symbolic Artificial Intelligence: Representing Objects and Relations",
            "rating": 1,
            "sanitized_title": "reconciling_deep_learning_with_symbolic_artificial_intelligence_representing_objects_and_relations"
        },
        {
            "paper_title": "Bridging Knowledge Graphs to Generate Scene Graphs",
            "rating": 1,
            "sanitized_title": "bridging_knowledge_graphs_to_generate_scene_graphs"
        }
    ],
    "cost": 0.013027749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Dual Embodied-Symbolic Concept Representations for Deep Learning</p>
<p>Daniel T Chang dtchang43@gmail.com 
Dual Embodied-Symbolic Concept Representations for Deep Learning
C9D883FB721C375593C778749DBE936F
Motivated by recent findings from cognitive neural science, we advocate the use of a dual-level model for concept representations: the embodied level consists of concept-oriented feature representations, and the symbolic level consists of concept graphs.Embodied concept representations are modality specific and exist in the form of feature vectors in a feature space.Symbolic concept representations, on the other hand, are amodal and language specific, and exist in the form of word / knowledge-graph embeddings in a concept / knowledge space.The human conceptual system comprises both embodied representations and symbolic representations, which typically interact to drive conceptual processing.As such, we further advocate the use of dual embodied-symbolic concept representations for deep learning.To demonstrate their usage and value, we discuss two important use cases: embodied-symbolic knowledge distillation for few-shot class incremental learning, and embodied-symbolic fused representation for image-text matching.Dual embodied-symbolic concept representations are the foundation for deep learning and symbolic AI integration.We discuss two important examples of such integration: scene graph generation with knowledge graph bridging, and multimodal knowledge graphs.</p>
<p>Introduction</p>
<p>Motivated by recent findings from cognitive neural science, we advocate the use of a dual-level model for concept representations [1][2]: the embodied level consists of concept-oriented feature representations, and the symbolic level consists of concepts in the form of concept graphs.For concrete concepts, the two levels are associated / connected.The embodied level corresponds to sensorimotor-derived knowledge representations of the dual-coding framework [4]; the symbolic level corresponds to language-derived knowledge representations of that framework.</p>
<p>Embodied concept representations are modality specific and exist in the form of feature vectors in a feature space.</p>
<p>Symbolic concept representations, on the other hand, are amodal and language specific, and exist in the form of word / knowledge-graph embeddings in a concept / knowledge space.Traditionally, they are learned separately and used independently.For example, deep learning for computer vision learns embodied concept representations in the form of visual feature vectors for image classification, whereas deep learning for natural language processing learns symbolic concept representations in the form of semantic word vectors for sentiment analysis.The human conceptual system comprises both simulated information of sensorimotor experience (i.e., embodied representations) and linguistic distributional information of how words are used in language (i.e., symbolic representations) [5][6][7].Further, the linguistic shortcut hypothesis predicts that people will use computationally cheaper linguistic distributional information where it is sufficient for the task in question.However, people will resort to sensorimotor simulation to provide a more detailed, precise conceptual representation when required.Therefore, both symbolic and embodied representations are inherent to the functioning of the human conceptual system.Further, they typically interact to drive conceptual processing.</p>
<p>As such, we further advocate the use of dual embodied-symbolic concept representations for deep learning.To demonstrate their usage and value, we discuss two important use cases: embodied-symbolic knowledge distillation for fewshot class incremental learning (CIL) and embodied-symbolic fused representation for image-text matching.The first use case [16] demonstrates that embodied-symbolic knowledge distillation mitigates both the catastrophic forgetting problem for CIL and the overfitting problem for few-shot learning.The second use case [17] demonstrates that embodied-symbolic fused representation closes the semantic gap between images and text leading to improved performance when matching an image with text.</p>
<p>Dual embodied-symbolic concept representations are the foundation for deep learning and symbolic AI integration.We discuss two important examples of such integration: scene graph generation with knowledge graph bridging, and multimodal knowledge graphs.The first example [19] presents a unified formulation of scene graphs (SGs) and commonsense knowledge graphs (CSKGs), where a SG is seen as an image-conditioned embodiment of a CSKG.The second example [21] grounds symbols in knowledge graphs (KGs) to corresponding image, text, sound and video data and maps symbols to their corresponding referents with meanings in the physical world, which is a key step towards the realization of human-level AI.</p>
<p>Human Conceptual System: Symbolic and Embodied</p>
<p>The human conceptual system comprises both simulated information of sensorimotor experience (i.e., embodied representations) and linguistic distributional information of how words are used in language (i.e., symbolic representations) [5].Further, the linguistic shortcut hypothesis predicts that people will use computationally cheaper linguistic distributional information where it is sufficient for the task in question.However, people will resort to sensorimotor simulation to provide a more detailed, precise conceptual representation when required.Therefore, both symbolic and embodied representations are inherent to the functioning of the human conceptual system, and they typically interact to drive conceptual processing.</p>
<p>In the following we discuss three recent findings in cognitive science that demonstrate the crucial role of dual embodiedsymbolic concept representations in human cognition.</p>
<p>Category Production</p>
<p>A common way of testing how concepts are structured and accessed from long-term memory is with a category production task, whereby a participant is presented with a category name such as 'animal', and asked to name concepts belonging to that category.In a category production study [5], measures of sensorimotor similarity between a category and member concept (based on an 11-dimensional representation of sensorimotor strength) and linguistic proximity between the category name and member-concept name (based on word co-occurrence derived from a large corpus) are used to test category production performance.Both measures predict the order and frequency of category production, but linguistic proximity has an effect above and beyond sensorimotor similarity.</p>
<p>Sensorimotor and linguistic distributional information, therefore, are found to offer an explanation to the mechanisms driving responses in category production tasks [5].In terms of linguistic distributional information (i.e., symbolic information), it is evident that the shared linguistic contexts between member-concept names and category name (e.g., between cat / dog and 'animal') in corpus-derived linguistic space is an effective predictor of category membership.In terms of sensorimotor information (i.e., embodied information), as suggested by many theories of conceptual structure, categorical distinctions emerge from common features (e.g., fur, four-legged) of member concepts that we perceive when interacting with the world.Furthermore, sensorimotor and linguistic distributional information typically interact to drive conceptual processing, and the linguistic shortcut hypothesis is the process by which people arrive at the most-frequently-named and first-named member concepts of a category.</p>
<p>Grounding of Concrete and Abstract Concepts</p>
<p>Concrete concepts (e.g., animal) have perceivable referents.In general, their semantic content can be clearly characterized, and their conceptual taxonomies can be unequivocally defined.Evidence obtained with concrete concepts [6] suggests interplay between modality-specific, multimodal and amodal semantic hub regions.Modality-specific and multimodal regions represent conceptual features, whereas amodal semantic hubs code conceptual information in an overarching supramodal fashion.The available evidence is most consistent with hybrid models of conceptual representations combining modality-specific and multimodal circuits with amodal conceptual hubs, i.e., combining embodied and symbolic representations.</p>
<p>Abstract concepts (e.g., justice) have referents which cannot be directly perceived, like mental or emotional states, abstract ideas, social constellations and scientific theories.Their semantic content is highly variable across individuals and contexts.As such, abstract concepts are a particular challenge for embodied / grounded cognition theories [6] because at the first glance, it is hard to imagine how abstract concepts, without a referent which can be perceived or acted upon, could be grounded in the sensorimotor brain systems.Embodied / grounded cognition theories, however, have been refined [6] in order to account for the representation of abstract concepts.Besides sensorimotor information due to metaphoric mapping or due to relations to classes of situations, the relevance of emotional, introspective, social and linguistic information has been stressed.</p>
<p>As is the case with concrete concepts, the findings with regard to abstract concepts [6] can be accommodated best by hybrid models of conceptual representations assuming an interaction between modality-specific, multimodal and amodal hub areas.Modality-specific systems include the sensorimotor systems, but also the modal systems involved in the processing of emotions, introspections, mentalizing, social constellations, and language.The latter modal systems are probably more important for abstract than for concrete concepts.</p>
<p>Meaning in Language and Mind</p>
<p>There are different theories on how much linguistic and sensorimotor representations contribute to meaning [7].In embodied theories, meaning is based on the relation between words and internal bodily states corresponding to multiple modalities, such as vision, olfaction, and perhaps even internal affective states.By contrast, symbolic theories suggest that the meaning of word (e.g., rose) can be derived in a recursive fashion by considering its relation to the meaning of words in its linguistic context (e.g., red, flower).Current theories of semantics tend to posit that both symbolic and embodied information contribute to meaning.</p>
<p>A study [7] is made to evaluate how well different kinds of models account for people's representations of both concrete and abstract concepts.The models include unimodal linguistic models as well as multimodal models which combine linguistic with perceptual or affective information.There are two types of linguistic models considered: the distributional linguistic model which derives word meaning from word co-occurrences derived from large language corpora, and the word association model which measures the meaning of a word as a distribution of respectively weighted associative links encoded in a large semantic network.The study demonstrates that both visual and affective multimodal models better capture behavior that reflects human representations, especially for basic-level concepts that belong to the same superordinate category.The conclusion is that multimodal information (i.e., symbolic and embodied information) is important for capturing both abstract and concrete concepts.</p>
<p>Symbolic Concept Representations</p>
<p>Embodied concept representations (i.e., modality-specific concept-oriented feature representations) have been discussed previously [1][2][3].In particular, we recommend their learning using exemplar-based contrastive self-supervised learning (CSSL) [3] since it is concept (class) centric and it supports class incremental learning.</p>
<p>In the following we discuss symbolic concept representations (i.e., amodal, language-specific concept graphs) since they are an integral part of dual embodied-symbolic concept representations, but we have not discussed them previously.They come in two forms: word embedding for representing concepts (namely, word semantics in natural language corpora or texts), and knowledge graph embedding for representing concept graphs (a.k.a.knowledge graphs), i.e., conceptual knowledge (e.g.commonsense knowledge).</p>
<p>Note that we will not discuss either word embedding learning or knowledge-graph embedding learning since they are of secondly importance to the focus of this paper.They are discussed in [9][10] and [12][13][14] respectively.</p>
<p>Word Embedding</p>
<p>Word embeddings [8][9] are dense representations of words in semantic vector spaces generated from language corpora or texts, in which semantically similar words have similar embedding vectors.Word embeddings have played an important role for tasks of natural language processing including complex and pertinent ones such as information retrieval and sentiment analysis.They are efficient to learn, highly scalable for large corpora (thousands of millions of words).</p>
<p>Two most widely used word embedding models [8][9][10] are Word2Vec and GloVe.Word2Vec employs two model architectures: Continuous Bag-of-Words (CBOW) model which aims to predict the occurrence of a word given other words that constitute its context, and Skip Gram (SG) model which deals with predicting a context given the word.Word2Vec considers only local word co-occurrences.GloVe is based on a model that reduces the dimensionality of a global cooccurrence matrix of the word-word type in a corpus, with the statistics of the entire corpus captured directly by the model.</p>
<p>GloVe focuses on global word co-occurrences.</p>
<p>For deep-learning based natural language processing [10] word embedding is the basic building block that maps words in the input sentences into continuous space vectors, and usually used (pretrained) in the first layer of a neural network.</p>
<p>Based on the word embedding, complex networks such as recurrent neural networks (RNNs) can be used for feature extraction and build, for example, context-aware word embedding or phrase / sentence embedding.</p>
<p>Note that the methods that generate word embedding based purely on information in a language corpus or text fail to take advantage of the semantic relational structure that exits between words in concurrent contexts.To overcome this limitation, the corpus or text is enhanced with extra morphological, syntactic, semantic and domain knowledge from knowledge sources (e.g., Wikipedia, Wordnet) to generate knowledge-aware word embedding [11].</p>
<p>Knowledge Graph Embedding</p>
<p>The knowledge graph (KG) [12][13][14][15] (a.k.a.concept graph) is a representation of conceptual knowledge (specifically, structured relational information) in the form of concepts and relations between them.We can view a KG as a set of statements (facts) having the form of subject-predicate-object triples, using the notation (h, r, t) (head, relation, tail) to identify a statement.We can also view a KG as a directed labeled graph, where nodes represent concepts and edges represent relations between concepts.</p>
<p>Knowledge graph embedding (KGE) [12][13][14][15] is a widely adopted approach to KG representation in which concepts and relations are embedded in low-dimensional continuous vector spaces.Most methods create a vector for each concept and each relation.These embeddings are generated in a way to capture latent properties of the semantics in the KG: similar concepts and similar relations will be represented with similar vectors.KGE offers precise, effective and structural representation of symbolic conceptual information.</p>
<p>Most of the KGE approaches rely mainly on the use of the subject-predicate-object triples present in the KG to generate the vector representations, (h, r, t), for (head, relation, tail) [12][13][14][15].These approaches can be broadly classified into two groups: translation-based models and semantic matching models.Translation-based models (e.g., TransE) are based on learning the translations from the head concept to the tail concept.They use distance-based measures to generate the similarity score for a pair of concepts and their relations.The training objective is to achieve, mathematically, h + r ~ t.</p>
<p>Semantic matching models (e.g., RESCAL) use a multiplicative approach and represent the relations as matrices / tensors in the vector space.For example, RESCAL relies on a tensor factorization approach upon the 3-dimensional tensor generated by considering subject-predicate-object as the 3 dimensions of the tensor.Note that these models only consider each individual fact, while their intrinsic associations are neglected, which is not sufficient for capturing deeper semantics for better embedding.They, therefore, cannot meet the requirements of KGE.</p>
<p>New approaches leverage the graph nature of KG, and use neural network (NN) based models for various tasks [14][15].</p>
<p>When treated as a graph, KG can be seen as a heterogeneous graph, with the logical relations of more importance than the graph structure.NN-based models can consider the type of concept or relation, neighborhood / substructure information, path information, and temporal information.The use of convolutional neural networks (CNNs) or attention mechanisms also helps to generate better embeddings.Examples of CNN-based models include ConvE and ConvKB [14].</p>
<p>Graph neural networks (GNNs) are neural networks that can be directly applied to graphs, and provide an easy way to generate node-level, edge-level, and graph-level embeddings.They have a somewhat universal architecture in common, referred to as Graph Convolutional Networks (GCNs) which use deep, multi-layer processing known as message passing.</p>
<p>Examples of GCN-based models include RGCN and SACN [14].</p>
<p>GCNs have a strong ability to mine the underlying semantics of KGs [14].In general, GCN-based models can incorporate additional information, such as node types, relation types, node attributes and substructures, to generate better embeddings.For example, if the node information of a multi-hop domain can be aggregated, the accuracy of the model in specific tasks can be greatly improved.A knowledge distillation approach suitable for few-shot CIL is proposed in [16], which is based on the use of dual To mitigate the overfitting problem, multiple visual embeddings for classes are generated, where each is designed specifically for a group of classes.The semantic word vectors are used to separate classes into several groups.The number of groups / visual embeddings is defined by the superclass (cluster) knowledge obtained from the semantic word vector space.</p>
<p>In the semantic space, there is a semantic word vector for each class.The set of superclasses is attained from the semantic word vector space representations of the base classes, and are then held fixed.</p>
<p>For the first (base) task, involving base classes, the steps for obtaining groups / visual embeddings are:</p>
<p>ï‚· Train the network backbone on the base classes, which is then kept frozen.</p>
<p>ï‚· Apply k-means clustering, where k = N, on base semantic word vectors and assign a superclass (cluster) label to each base class.</p>
<p>ï‚· Train N embeddings on the base task using superclass labels as group identity.</p>
<p>For other subsequent (novel) task, involving novel classes, the cluster centers (obtained in the base task) are used to assign superclass labels to novel classes.To assign a superclass label to novel classes, the minimum Euclidean distance between the semantic word vector of a novel class and cluster centers is used.Hence, given a novel class, there is a selection of groups / visual embeddings that each may be more or less suited.</p>
<p>An attention module is used to merge multiple visual embeddings of a class to generate its final visual embedding, i.e., visual vector.A mapping module is then used to project the visual vector from the visual space into the semantic space to align the visual vector with its associated semantic word vector.For the first (base) task, training involves attention loss and classification loss; for other subsequent (novel) task, training involves attention loss, distillation loss (i.e., alignment loss) and classification loss.</p>
<p>Embodied-Symbolic Fused Representation for Image-Text Matching</p>
<p>Image and text matching [17] is an important vision-language cross-modality task for many applications including image retrieval and caption.Before calculating the similarity between an image and text, a matching model needs to obtain a rich representation of the image (and text) first.Most of the current image-text matching models utilize pre-trained neural networks to extract feature embeddings as the representation of images.The image embeddings, however, fail to extract highlevel semantic information.So the semantic gap between images and text leads to limited performance when matching an image with text.</p>
<p>Learning semantic concepts is useful to enhance image representation and can significantly improve the performance of both image-to-text and text-to-image retrieval.Frequently co-occurred concepts in the same image (scene), e.g.bedroom and bed, can provide commonsense knowledge to discover other semantic-related concepts.[17] uses a Scene Concept Graph (SCG) to support this by aggregating image scene graphs and extracting frequently co-occurred concepts as commonsense knowledge.Moreover, it proposes a novel model to incorporate this knowledge to improve image-text matching.Specifically, semantic concepts are detected from images and then expanded by the SCG to include commonly-related concepts (which may be occluded or long-tailed).Afterwards, it fuses their representations with the image embeddings, as semantic-enhanced image embeddings, to use (with text embeddings) for image-text matching.</p>
<p>The model uses dual embodied-symbolic concept representations.The visual embodied representation exists in the form of image embeddings.There are two symbolic representations.The text symbolic representation exists in the form of (context-aware) word embeddings.The concept symbolic (semantic) representation for images exists in the form of concept embeddings.Finally, there is an embodied-symbolic (image-concept) fused representation existing in the form of conceptenhanced image embeddings.</p>
<p>The scene graph [17] of an image is a graph consisting of concepts and relations between them.It can be represented as a set of triples of <subject, relation, object>.The SCG is constructed from scene graphs by aggregating co-occurred <subject, object> pairs from scene graphs of all images.To generate the concept embeddings for an image, first, a concept detection module (a multi-label image classification model) is used to extract semantic concepts from the image on a small concept vocabulary.With the SCG in hand, a concept expansion module is then used to expand the semantic concepts to include commonly-related concepts.Finally, a concept prediction module is used to predict relevant concepts from these and generate the concept embeddings.The concept embeddings of an image are fused with its visual embedding, by the imageconcept fusion module, to generate a concept-enhanced image representation.</p>
<p>The image-text matching problem is formulated as a ranking model.Given the input image and the text, the output is the similarity score of matching their respective visual and language representations, with the visual representation being an image-concept (embodied-symbolic) fused representation and the language representation being word embeddings.To learn image and text matching as well as image-relevant semantic concepts jointly in an end-to-end fashion, the loss function consists of two parts: image-text matching loss and concept prediction loss.</p>
<p>Deep Learning and Symbolic AI Integration</p>
<p>Symbolic AI and deep learning both have strength and weakness, which tend to be each other's opposites.A significant challenge today [18] is to effect a reconciliation.Symbolic AI is based on manipulation of abstract compositional representations whose elements stand for concepts and relations.Therefore, to facilitate reconciliation, a key objective for deep learning is to develop architectures capable of discovering concepts and relations in raw data, and learning how to represent them.</p>
<p>An excellent example for doing this for image data has been discussed in 5 Embodied-Symbolic Fused Representation for Image-Text Matching.(Note that only co-occurred concepts are considered in [17].However, their relations will also be considered in future work.)As discussed there, dual embodied-symbolic concept representations are the foundation for deep learning and symbolic AI integration.</p>
<p>In the following, we discuss two additional important examples of deep learning and symbolic AI integration.</p>
<p>Scene Graph Generation with Knowledge Graph Bridging</p>
<p>Scene graphs (SGs) are powerful representations that extract semantic concepts and their relations from images, which facilitate visual comprehension and reasoning.(For an example usage, see 5 Embodied-Symbolic Fused Representation for Image-Text Matching.)A SG can be represented as a set of triples of <subject, relation, object>.On the other hand, commonsense knowledge graphs (CSKGs) [20] are rich repositories that encode how the world is structured (i.e., commonsense knowledge), and how common concepts are related and interact.</p>
<p>GB-Net (Graph Bridging Network) [19] presents a unified formulation of these two constructs, where a SG is seen as an image-conditioned embodiment of a CSKG.Based on this perspective, the SG generation is formulated as the inference of a bridge between the SG and CSKG, where each concept or relation in the SG must be linked to its corresponding concept or relation in the CSKG.Specifically, both SG and CSKG are defined as special types of knowledge graph (KG): GB-Net fuses the SG embedding and CSKG embedding through a dynamic message passing and bridging algorithm using a graph neural networks (GNN).The method iteratively propagates messages to update nodes, then compares nodes to update bridge edges, and repeats until the two graphs are carefully connected.This results in the SG embedding with bridges to the CSKG embedding.</p>
<p>Multimodal Knowledge Graphs</p>
<p>Knowledge graphs (KGs) have found great use in a wide range of applications including text understanding, recommendation system, natural language question answering, and image understanding (see 5 Embodied-Symbolic Fused Representation for Image-Text Matching and 6.1 Scene Graph Generation with Knowledge Graph Bridging).More and more KGs have been created, covering common sense knowledge [20] (see 6.1 Scene Graph Generation with Knowledge Graph Bridging), lexical knowledge, encyclopedia knowledge, taxonomic knowledge, and geographic knowledge.</p>
<p>Most of the existing KGs are represented with pure symbols, denoted in the form of text, without grounding to the physical world experience.However, both symbolic and embodied representations are inherent to the functioning of the human conceptual system, and they typically interact to drive conceptual processing (see 2 Human Conceptual System: Symbolic and Embodied).Therefore, it is necessary to ground symbols in KGs to corresponding image, text, sound and video data and map symbols to their corresponding referents with meanings in the physical world.That is, the multi-modalization</p>
<p>Conclusion</p>
<p>Motivated by recent findings from cognitive neural science, we advocate the use of a dual-level model for concept representations: the embodied level consists of concept-oriented feature representations, and the symbolic level consists of concept graphs (a.k.a.knowledge graphs).The human conceptual system comprises both embodied representations and symbolic representations, which typically interact to drive conceptual processing.As such, we further advocate the use of dual embodied-symbolic concept representations for deep learning.That is, deep learning should learn from data not only modality-specific embodied representations such as image embeddings, text embeddings, etc., but also the corresponding amodal symbolic (semantic) representation as knowledge graph embeddings, with links to commonsense knowledge graphs.</p>
<p>Dual embodied-symbolic concept representations are the foundation for deep learning and symbolic AI integration, which is an important direction for deep learning and AI since their integration reinforces each other's strength, compensates each other's weakness, and takes a major step toward human-level AI (e.g., grounding in experience / data, understanding, reasoning, explanation, etc.).</p>
<p>4</p>
<p>Embodied-Symbolic Knowledge Distillation for Few-Shot CIL In CIL [2], a model learns tasks continually, with each task containing a batch of new classes.In few-shot CIL [3], only the training set of the first (base) task may have large-scale training data for base classes, while other subsequent (novel) task just contains few-shot instances for novel classes.Few-shot CIL requires transferring knowledge (i.e., knowledge distillation) from old classes to new classes in solving the catastrophic forgetting problem, which is generic to CIL.The additional challenge of few-shot learning is the overfitting problem.</p>
<p>embodied-symbolic concept (class) representations.The embodied (visual) representation exists in the form of visual vectors.Semantic word vectors (Word2Vec and GloVe) are used as the symbolic (semantic) representation to facilitate knowledge distillation, which are generated from unsupervised learning on an unannotated text corpus.The semantically guided network does not add new parameters while adding new classes incrementally.The knowledge distillation process only includes semantic word vectors of novel classes in addition to the base classes, which are used to help the network remembering base class training, generalizing to novel classes, and generating well-separated embodied representation (visual vectors) of classes.Note that though the network may not have had the opportunity to see instances of novel classes, nevertheless, novel classes may very well share semantic properties with base classes it has seen.For example, if hyena is a novel class, many typical hyena attributes like 'face', 'body', etc., may have been seen by the network from base classes.</p>
<p>ï‚·</p>
<p>A KG is a set of nodes of type concept (C) or relation (R), and a set of directed, weighted edges (Î•) between the nodes.ï‚· A CSKG is a type of KG with commonsense concept (CC) nodes and commonsense relation (CR) nodes.Commonsense edges are of four types: CC-&gt;CC, CC-&gt;CR, CR-&gt;CC, and CR-&gt;CR.ï‚· A SG is a type of KG with scene concept (SC) nodes and scene relation (SR) nodes.Scene edges are of four types: SC-&gt;SR (subjectOf), SC-&gt;SR (objectOf), SR-&gt;SC (hasSubject), and SR-&gt;SC (hasObject).ï‚· The SG and CSKG are connected through four types of bridge edges: SC-&gt;CC, SR-&gt;CR, CC-&gt;SC and CR-&gt;SR.GB-Net uses dual embodied-symbolic concept representations.The visual embodied representation for images exists in the form of image embeddings.There are two symbolic concept representations.The CSKG symbolic representation exists in the form of CSKG embedding (of commonsense nodes and edges).The SG symbolic (semantic) representation for images exists in the form of SG embedding (of scene nodes and edges).The CSKG embedding and the SG embedding are interconnected through the bridge edges.</p>
<p>Acknowledgement: Thanks to my wife Hedy (éƒ‘æœŸèŠ³) for her support.of KGs[21]is an inevitable key step towards the realization of human-level AI, which results in Multimodal Knowledge Graphs (MMKGs)[12,15,21].To support symbol grounding in MMKG[21], one can take multimodal data as particular attribute values of concepts or relations.This can be denoted in a triple (s, r, d), where s denotes a concept or relation, d denotes one of its corresponding multimodal data, and the relation r is, e.g., "hasImage" when d is an image.Symbol grounding, therefore, can be divided into concept grounding and relation grounding.As an example, concept grounding aims to find representative, discriminative and diverse images for visual concepts.A major challenge is to find representative images for a visual concept from a group of relevant images.The representativeness and discriminativeness of images can be scored in terms of results of cluster-based methods, such as K-means, based on visual embeddings.The captions of images can also be utilized to evaluate the representativeness and discriminativeness of images, at the semantic level, based on text embeddings.IKRL and DKRL are two well-known examples of MMKG. IKRL (Image-embodied Knowledge RepresentationLearning)[12,15]provides a method to integrate images inside the scoring function of the KG embedding model (TransE).Essentially, IKRL uses multiple images for each concept and use the AlexNet CNN to generate embeddings for the images.These embeddings are then selected and combined with the use of attention to be finally projected in the KG embedding space.DKRL (Description-Embodied Knowledge Representation Learning)[12,15], on the other hand, includes the description of concepts in the representation.It uses a CNN to encode the concept description into a vector representation and uses this representation in the loss function.DKRL learns two embeddings for each concept, one that is structure-based (i.e., KG, like TransE) and one that is based on the concept descriptions.The two embeddings are interconnected / integrated.The above discussion, though brief, shows that MMKG uses dual embodied-symbolic concept representations.The KG symbolic representation exists in the form of KG embedding.There are various embodied representations, depending on the multimodal data involved.For image data, the visual embodied representation exists in the form of image embedding.For text (descriptions), the textual embodied representation exists in the form of text embedding.The KG embedding and the modality-specific embedding(s) are interconnected / integrated.
Concept-Oriented Deep Learning. T Daniel, Chang, arXiv:1806.017562018arXiv preprint</p>
<p>Concept Representation Learning with Contrastive Self-Supervised Learning. T Daniel, Chang, arXiv:2112.056772021arXiv preprint</p>
<p>Exemplar-Based Contrastive Self-Supervised Learning with Few-Shot Class Incremental Learning. T Daniel, Chang, arXiv:2202.026012022arXiv preprint</p>
<p>Dual Coding of Knowledge in the Human Brain. Y Bi, Trends in Cognitive Sciences. 2510October 2021</p>
<p>Linguistic Distributional Knowledge and Sensorimotor Grounding both Contribute to Semantic Category Production. B Banks, C Wingfield, L Connell, Cogn. Sci. 45e130552021</p>
<p>Varieties of Abstract Concepts and Their Grounding in Perception or Action. M Kiefer, M Harpaintner, Open Psychol. 20202</p>
<p>Visual and Affective Multimodal Models of Word Meaning in Language and Mind. S D Deyne, D Navarro, G Collell, A Perfors, Cogn, Sci. 45e129222021</p>
<p>A Systematic Literature Review on Word Embeddings. L GutiÃ©rrez, B Keith, Proc. Int. Conf. Softw. Process Improvement. Int. Conf. Softw. ess Improvement2018</p>
<p>U Naseem, I Razzak, S K Khan, M Prasad, arXiv:2010.15036A Comprehensive Survey on Word Representation Models: From Classical to State-of-the-art Word Representation Language Models. 2020</p>
<p>M Zhou, N Duan, S Liu, H.-Y Shum, Progress in Neural NLP: Modeling, Learning, and Reasoning," in Engineering. 20206</p>
<p>Incorporating Extra Knowledge to Enhance Word Embedding. A Roy, S Pan, Proceedings of the 29th International Joint Conference on Artificial Intelligence. the 29th International Joint Conference on Artificial Intelligence202020</p>
<p>Knowledge Graph Embeddings and Explainable AI. Federico Bianchi, Gaetano Rossiello, Luca Costabello, Matteo Palmonari, Pasquale Minervini, arXiv:2004.148432020arXiv preprint</p>
<p>A Survey of Knowledge Graph Embedding and Their Applications. Shivani Choudhary, Tarun Luthra, Ashima Mittal, Rajat Singh, arXiv:2107.078422021arXiv preprint</p>
<p>A Survey on Knowledge Graph Embeddings for Link Prediction. Meihong Wang, Linling Qiu, Xiaoli Wang, Symmetry. 202113485</p>
<p>Application and Evaluation of Knowledge Graph Embeddings in Biomedical Data. Mona Alshahrani, Maha A Thafar, Magbubah Essack, PeerJ Computer Science. 72021</p>
<p>Semantic-aware Knowledge Distillation for Few-Shot Class-Incremental Learning. Ali Cheraghian, Shafin Rahman, Pengfei Fang, Soumava Kumar Roy, Lars Petersson, Mehrtash Harandi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)June 2021</p>
<p>Knowledge Aware Semantic Concept Expansion for Image-Text Matching. B Shi, L Ji, P Lu, Z Niu, N Duan, IJCAI. 2019</p>
<p>Reconciling Deep Learning with Symbolic Artificial Intelligence: Representing Objects and Relations. Marta Garnelo, Murray Shanahan, Current Opinion in Behavioral Sciences. 292019</p>
<p>Bridging Knowledge Graphs to Generate Scene Graphs. A Zareian, S Karaman, S F Chang, arXiv:2001.023142020arXiv preprint</p>
<p>CSKG: The Commonsense Knowledge Graph. F Ilievski, P Szekely, B Zhang, Extended Semantic Web Conference (ESWC). 2020</p>
<p>Multi-Modal Knowledge Graph Construction and Application: A Survey. Xiangru Zhu, Zhixu Li, Xiaodan Wang, Xueyao Jiang, Penglei Sun, Xuwu Wang, Yanghua Xiao, Nicholas Jing Yuan, arXiv:2202.057862022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1661 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1661</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1661</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-259694462</p>
                <p><strong>Paper Title:</strong> Sim-to-Real Transfer of Soft Robotic Navigation Strategies That Learns From the Virtual Eye-in-Hand Vision</p>
                <p><strong>Paper Abstract:</strong> To steer a soft robot precisely in an unconstructed environment with minimal collision remains an open challenge for soft robots. When the environments are unknown, prior motion planning for navigation may not always be available. This article presents a novel Sim-to-Real method to guide a cable-driven soft robot in a static environment under the Simulation Open Framework Architecture (SOFA). The scenario aims to resemble one of the steps during a simplified transoral tracheal intubation process where a robotic endotracheal tube is guided to the upper trachea–larynx location by a flexible video-assisted endoscope/stylet. In SOFA, we employ the quadratic programming inverse solver to obtain collision-free motion strategies for the endoscope/stylet manipulation based on the robot model and encode the virtual eye-in-hand vision. Then, we associate the anatomical features recognized by the virtual vision and the joint space motion using a closed-loop nonlinear autoregressive exogenous model (NARX) network. Afterward, we transfer the learned knowledge to the robot prototype, expecting it to navigate to the desired spot in a new phantom environment automatically based on its eye-in-hand vision only. Experiment results indicate that our soft robot can efficaciously navigate through the unstructured phantom to the desired spot with minimal collision motion according to what it has learned from the virtual environment. The results show that the average R-squared coefficient between the closed-loop NARX-forecasted and SOFA-referenced robot's cable and prismatic joint space motion are 0.963 and 0.997, respectively. The eye-in-hand visions also demonstrate a good alignment between the robot tip and the glottis.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1661.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1661.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SOFA-Sim2Real Soft Navigation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SOFA-based Sim-to-Real Transfer for Soft Robotic Navigation Using Virtual Eye-in-Hand Vision</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-loop sim-to-real method that trains a NARX recurrent network on SOFA-generated virtual eye-in-hand data (YOLO-labeled anatomical features + QP-solved joint-space motions) to drive a 3D-printed cable-driven two-segment soft endoscope/stylet in real phantom experiments with minimal collision.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Cable-driven two-segment soft robotic endoscope/stylet</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>3D-printed, two coupled flexible segments (proximal 60 mm, distal 70 mm, outer diameter 6.2 mm) actuated by six cables (three per segment) plus a prismatic feed; designed as a steerable stylet/endoscope with an eye-in-hand CMOS camera (OV6946) and powered remotely by motors via flexible shafts for autonomous navigation in confined oropharyngeal environments.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>medical soft-robotic navigation (endoscopic/stylet manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>SOFA (Simulation Open Framework Architecture) v22.06.99</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Physics-based soft-body FEM simulation including tetrahedral meshing of the soft robot, meshed anatomical phantom geometry, collision detection primitives, QP inverse solver for contact-aware inverse actuation, and a virtual eye-in-hand OpenGL viewport to render endoscopic views for feature labeling.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>High-fidelity physics for soft bodies (FEM) and contact/collision handling; rendering is synthetic/overanimated (not fully photorealistic).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Soft-body deformation via FEM, contact dynamics and collision detection (point/line/triangle narrow-phase), cable geometric constraints and actuation-to-deformation mapping, prismatic feed, gravity, virtual camera geometry (eye-in-hand viewport).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Photorealistic tissue appearance and lighting (overanimated visuals), sensor noise and camera exposure variability (not modeled), haptic/force feedback omitted, some phantom geometry trimmed (teeth, small muscles), actuator transmission losses approximated (real-world amplification required), no explicit domain randomization of physics parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Laboratory phantom experiments using a commercially available oropharyngeal phantom different from the SOFA scene; the physical two-segment soft robot prototype with OV6946 LED-equipped 1.8 mm camera at tip; magnet-based tracker for tip position; robot base on linear slide; real-time YOLO feature detection streamed to NARX-based controller.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Autonomous navigation of a soft endoscopic/stylet tip to the upper glottis in confined anatomy while minimizing body collisions, using visual feature sequences (uvula, epiglottis, glottis) to predict joint-space control commands.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised/time-series learning: YOLOv5s trained for anatomical feature detection on a blended dataset (SOFA-rendered images + phantom images) and a closed-loop NARX recurrent network trained on SOFA-generated sequences that map labeled visual features to QP-computed joint-space actuation trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Phantom success rate (clear endoscopic view of vocal cords) and fit metrics between predicted and reference joint motions (R-squared).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>NARX forecast fit to SOFA-generated joint motions: mean R^2 ≈ 0.963 for cable joints, R^2 ≈ 0.997 for prismatic joint (reported averages against SOFA references).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Phantom experiments: success in reaching upper glottis in 17/20 consecutive trials (85%); open-loop path reproduction (separate test) average spatial positioning error below 2 mm.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Not used in the sense of physics/randomization; training data diversity achieved by (1) blending SOFA images with a small set of phantom and medical images for YOLO, and (2) adding modest target-position offsets ([±3, ±3, ±5] mm) across simulations to produce 20 varied paths for NARX training.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Visual rendering gap (overanimated synthetic images vs real camera images), camera exposure/lighting differences (overexposure caused some failures), absence of modeled sensor noise and haptic feedback, trimmed anatomical details, actuator transmission and scaling differences requiring hand-tuned amplification, limited anatomical variability in simulation datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Use of a physics-based FEM simulator (SOFA) with collision-aware QP inverse solver to generate plausible whole-body soft-deformation motion; embedding a virtual eye-in-hand camera to generate visual sequences; blending simulated images with phantom/medical images for YOLO training; using a lightweight closed-loop NARX model to map visual labels to joint commands; introducing target-position offsets during simulation to increase generalization; experimental tolerance to weak robot-patient registration (±5 mm).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Paper emphasizes accurate soft-body FEM-based contact/collision modeling and an embedded virtual eye-in-hand camera as critical for transfer fidelity; photorealism is helpful but not strictly required (they blended in real images to reduce visual gap); no quantitative numeric thresholds were provided.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A SOFA-based sim-to-real pipeline that learns virtual eye-in-hand visual sequences and QP-computed collision-avoiding joint trajectories can be transferred in a one-off manner to a physical cable-driven soft endoscope/stylet; the closed-loop NARX mapping generalizes sufficiently to reach the upper glottis in a phantom with 85% success and high correspondence (R^2 ~0.96–0.997) between predicted and simulated joint motions; main transfer limitations are visual domain gap (lighting/overexposure), limited anatomical diversity, and lack of haptic sensing, while techniques that enabled transfer include high-fidelity FEM/contact modeling, virtual eye-in-hand data, blended visual datasets, and modest variation in simulated target positions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim-to-Real Transfer of Soft Robotic Navigation Strategies That Learns From the Virtual Eye-in-Hand Vision', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1661.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1661.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zhang et al. MuJoCo Sim2Real</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sim2real learning of obstacle avoidance for robotic manipulators in uncertain environments (Zhang et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited sim-to-real approach that trains a rigid manipulator in MuJoCo for obstacle avoidance and transfers the policy to the real robot using 3D bounding boxes estimated from RGB-D vision.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sim2real learning of obstacle avoidance for robotic manipulators in uncertain environments.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Rigid robotic manipulator (unspecified model in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Rigid-arm manipulator trained in MuJoCo for obstacle avoidance; transfer employed vision-based 3D bounding boxes.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation / obstacle avoidance</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>A physics simulator for rigid-body dynamics used to learn collision-avoidance policies.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Mature rigid-body dynamics simulator (MuJoCo) — high fidelity for rigid robots</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Rigid-body dynamics, contact/obstacle geometry, simulated sensors (RGB-D-derived 3D bounding boxes referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Per-paper summary only; specific photorealism or sensor noise treatments not detailed in this article.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Physical robot using RGB-D vision to estimate 3D bounding boxes for transfer; details not given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Obstacle avoidance policy for a rigid manipulator</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Simulated learning in MuJoCo (implicit supervised or RL in cited work), then transferred using vision-derived inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Not detailed here beyond implicit perception-to-vision mapping via 3D bounding boxes to bridge sim-to-real gap.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Use of RGB-D-derived 3D bounding boxes as an intermediate perceptual representation to enable transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper cited as an example where sim-to-real has been applied to rigid manipulators by leveraging MuJoCo-trained policies and transferring using perception via 3D bounding boxes from RGB-D.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim-to-Real Transfer of Soft Robotic Navigation Strategies That Learns From the Virtual Eye-in-Hand Vision', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1661.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1661.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PyBullet KUKA Peg-in-Hole</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sim-to-real transfer for reinforcement learning without dynamics randomization (Kaspar et al.) — KUKA LBR iiwa peg-in-hole (cited)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited work using PyBullet to train reinforcement learning policies for a KUKA LBR iiwa peg-in-hole task and then transferring them to the physical robot, presented as an example of sim-to-real for rigid-arm manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sim2real transfer for reinforcement learning without dynamics randomization.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>KUKA LBR iiwa robotic arm (peg-in-hole task)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A rigid 7-DoF manipulator used for peg-in-hole insertion tasks; trained in PyBullet in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotic manipulation / assembly</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>PyBullet</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Physics simulator for rigid-body and contact dynamics commonly used for RL experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Approximate rigid-body dynamics useful for RL; contact modeling used for insertion tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Rigid-body dynamics, contact interactions relevant to peg-in-hole tasks (in cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Exact details not provided in this paper's discussion; per-citation the cited work emphasizes transfer without dynamics randomization.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Physical KUKA LBR iiwa arm executing peg-in-hole after transfer (from cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Peg-in-hole insertion policy</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning in PyBullet (in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Cited as an example 'without dynamics randomization' (i.e., transfer claimed without physics randomization).</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Cited work's claim: transfer without dynamics randomization (details in original paper).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as a related example where sim-to-real transfer for RL was achieved on a rigid manipulator using PyBullet without dynamics randomization (details in the cited publication).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim-to-Real Transfer of Soft Robotic Navigation Strategies That Learns From the Virtual Eye-in-Hand Vision', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1661.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1661.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Soft-Robot Sim2Real Examples (mentions)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Various cited sim-to-real works for soft robots (differentiable simulation, proprioceptive sensing, scalable design transfer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper references multiple recent sim-to-real examples for soft robots, including differentiable-simulation-based transfer for soft robotic fish, zero-shot sim-to-real for pneumatic proprioceptive sensing, and scalable design transfer approaches, as contextual related work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Various soft robotic systems (soft robotic fish, pneumatic soft robots, modular soft robots)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Cited works cover a range of soft robot platforms where sim-to-real techniques have been explored (e.g., differentiable simulation for locomotion, proprioceptive 3D sensing transfer, scalable design transfer).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>soft robotics (locomotion, proprioception, design transfer)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Varied (differentiable simulators, specialized FEM/soft-body simulators, custom pipelines)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Simulators that support differentiable dynamics, FEM soft-body modeling, or other soft-matter approximations depending on the cited study.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>High-fidelity soft-body/differentiable simulation in some cited works; specifics depend on each citation.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Soft-body dynamics, deformation, contact for locomotion and sensing tasks (as reported in cited works).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Per-citation simplifications vary; this paper does not detail specifics beyond citing these works.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Physical soft robots in diverse tasks (e.g., fish-like locomotion, pneumatic actuators) in the cited literature.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Examples include locomotion policies, proprioceptive sensing models, and design-to-reality transfer for soft robots.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Varies across cited works (differentiable simulation-based learning, supervised transfer, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited to show that sim-to-real for soft robots is emerging (examples include differentiable simulators and proprioceptive sensing transfer), motivating the current work which focuses on navigation and virtual eye-in-hand vision in SOFA.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim-to-Real Transfer of Soft Robotic Navigation Strategies That Learns From the Virtual Eye-in-Hand Vision', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sim2real learning of obstacle avoidance for robotic manipulators in uncertain environments. <em>(Rating: 2)</em></li>
                <li>Sim2real transfer for reinforcement learning without dynamics randomization. <em>(Rating: 2)</em></li>
                <li>Sim2real for soft robotic fish via differentiable simulation. <em>(Rating: 2)</em></li>
                <li>Toward zero-shot sim-to-real transfer learning for pneumatic soft robot 3D proprioceptive sensing. <em>(Rating: 2)</em></li>
                <li>Scalable sim-to-real transfer of soft robot designs. <em>(Rating: 2)</em></li>
                <li>Nerf2real: Sim2real transfer of vision-guided bipedal motion skills using neural radiance fields. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1661",
    "paper_id": "paper-259694462",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "SOFA-Sim2Real Soft Navigation",
            "name_full": "SOFA-based Sim-to-Real Transfer for Soft Robotic Navigation Using Virtual Eye-in-Hand Vision",
            "brief_description": "A closed-loop sim-to-real method that trains a NARX recurrent network on SOFA-generated virtual eye-in-hand data (YOLO-labeled anatomical features + QP-solved joint-space motions) to drive a 3D-printed cable-driven two-segment soft endoscope/stylet in real phantom experiments with minimal collision.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Cable-driven two-segment soft robotic endoscope/stylet",
            "agent_system_description": "3D-printed, two coupled flexible segments (proximal 60 mm, distal 70 mm, outer diameter 6.2 mm) actuated by six cables (three per segment) plus a prismatic feed; designed as a steerable stylet/endoscope with an eye-in-hand CMOS camera (OV6946) and powered remotely by motors via flexible shafts for autonomous navigation in confined oropharyngeal environments.",
            "domain": "medical soft-robotic navigation (endoscopic/stylet manipulation)",
            "virtual_environment_name": "SOFA (Simulation Open Framework Architecture) v22.06.99",
            "virtual_environment_description": "Physics-based soft-body FEM simulation including tetrahedral meshing of the soft robot, meshed anatomical phantom geometry, collision detection primitives, QP inverse solver for contact-aware inverse actuation, and a virtual eye-in-hand OpenGL viewport to render endoscopic views for feature labeling.",
            "simulation_fidelity_level": "High-fidelity physics for soft bodies (FEM) and contact/collision handling; rendering is synthetic/overanimated (not fully photorealistic).",
            "fidelity_aspects_modeled": "Soft-body deformation via FEM, contact dynamics and collision detection (point/line/triangle narrow-phase), cable geometric constraints and actuation-to-deformation mapping, prismatic feed, gravity, virtual camera geometry (eye-in-hand viewport).",
            "fidelity_aspects_simplified": "Photorealistic tissue appearance and lighting (overanimated visuals), sensor noise and camera exposure variability (not modeled), haptic/force feedback omitted, some phantom geometry trimmed (teeth, small muscles), actuator transmission losses approximated (real-world amplification required), no explicit domain randomization of physics parameters.",
            "real_environment_description": "Laboratory phantom experiments using a commercially available oropharyngeal phantom different from the SOFA scene; the physical two-segment soft robot prototype with OV6946 LED-equipped 1.8 mm camera at tip; magnet-based tracker for tip position; robot base on linear slide; real-time YOLO feature detection streamed to NARX-based controller.",
            "task_or_skill_transferred": "Autonomous navigation of a soft endoscopic/stylet tip to the upper glottis in confined anatomy while minimizing body collisions, using visual feature sequences (uvula, epiglottis, glottis) to predict joint-space control commands.",
            "training_method": "Supervised/time-series learning: YOLOv5s trained for anatomical feature detection on a blended dataset (SOFA-rendered images + phantom images) and a closed-loop NARX recurrent network trained on SOFA-generated sequences that map labeled visual features to QP-computed joint-space actuation trajectories.",
            "transfer_success_metric": "Phantom success rate (clear endoscopic view of vocal cords) and fit metrics between predicted and reference joint motions (R-squared).",
            "transfer_performance_sim": "NARX forecast fit to SOFA-generated joint motions: mean R^2 ≈ 0.963 for cable joints, R^2 ≈ 0.997 for prismatic joint (reported averages against SOFA references).",
            "transfer_performance_real": "Phantom experiments: success in reaching upper glottis in 17/20 consecutive trials (85%); open-loop path reproduction (separate test) average spatial positioning error below 2 mm.",
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": "Not used in the sense of physics/randomization; training data diversity achieved by (1) blending SOFA images with a small set of phantom and medical images for YOLO, and (2) adding modest target-position offsets ([±3, ±3, ±5] mm) across simulations to produce 20 varied paths for NARX training.",
            "sim_to_real_gap_factors": "Visual rendering gap (overanimated synthetic images vs real camera images), camera exposure/lighting differences (overexposure caused some failures), absence of modeled sensor noise and haptic feedback, trimmed anatomical details, actuator transmission and scaling differences requiring hand-tuned amplification, limited anatomical variability in simulation datasets.",
            "transfer_enabling_conditions": "Use of a physics-based FEM simulator (SOFA) with collision-aware QP inverse solver to generate plausible whole-body soft-deformation motion; embedding a virtual eye-in-hand camera to generate visual sequences; blending simulated images with phantom/medical images for YOLO training; using a lightweight closed-loop NARX model to map visual labels to joint commands; introducing target-position offsets during simulation to increase generalization; experimental tolerance to weak robot-patient registration (±5 mm).",
            "fidelity_requirements_identified": "Paper emphasizes accurate soft-body FEM-based contact/collision modeling and an embedded virtual eye-in-hand camera as critical for transfer fidelity; photorealism is helpful but not strictly required (they blended in real images to reduce visual gap); no quantitative numeric thresholds were provided.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "A SOFA-based sim-to-real pipeline that learns virtual eye-in-hand visual sequences and QP-computed collision-avoiding joint trajectories can be transferred in a one-off manner to a physical cable-driven soft endoscope/stylet; the closed-loop NARX mapping generalizes sufficiently to reach the upper glottis in a phantom with 85% success and high correspondence (R^2 ~0.96–0.997) between predicted and simulated joint motions; main transfer limitations are visual domain gap (lighting/overexposure), limited anatomical diversity, and lack of haptic sensing, while techniques that enabled transfer include high-fidelity FEM/contact modeling, virtual eye-in-hand data, blended visual datasets, and modest variation in simulated target positions.",
            "uuid": "e1661.0",
            "source_info": {
                "paper_title": "Sim-to-Real Transfer of Soft Robotic Navigation Strategies That Learns From the Virtual Eye-in-Hand Vision",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Zhang et al. MuJoCo Sim2Real",
            "name_full": "Sim2real learning of obstacle avoidance for robotic manipulators in uncertain environments (Zhang et al.)",
            "brief_description": "A cited sim-to-real approach that trains a rigid manipulator in MuJoCo for obstacle avoidance and transfers the policy to the real robot using 3D bounding boxes estimated from RGB-D vision.",
            "citation_title": "Sim2real learning of obstacle avoidance for robotic manipulators in uncertain environments.",
            "mention_or_use": "mention",
            "agent_system_name": "Rigid robotic manipulator (unspecified model in this paper)",
            "agent_system_description": "Rigid-arm manipulator trained in MuJoCo for obstacle avoidance; transfer employed vision-based 3D bounding boxes.",
            "domain": "general robotics manipulation / obstacle avoidance",
            "virtual_environment_name": "MuJoCo",
            "virtual_environment_description": "A physics simulator for rigid-body dynamics used to learn collision-avoidance policies.",
            "simulation_fidelity_level": "Mature rigid-body dynamics simulator (MuJoCo) — high fidelity for rigid robots",
            "fidelity_aspects_modeled": "Rigid-body dynamics, contact/obstacle geometry, simulated sensors (RGB-D-derived 3D bounding boxes referenced).",
            "fidelity_aspects_simplified": "Per-paper summary only; specific photorealism or sensor noise treatments not detailed in this article.",
            "real_environment_description": "Physical robot using RGB-D vision to estimate 3D bounding boxes for transfer; details not given in this paper.",
            "task_or_skill_transferred": "Obstacle avoidance policy for a rigid manipulator",
            "training_method": "Simulated learning in MuJoCo (implicit supervised or RL in cited work), then transferred using vision-derived inputs.",
            "transfer_success_metric": null,
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Not detailed here beyond implicit perception-to-vision mapping via 3D bounding boxes to bridge sim-to-real gap.",
            "transfer_enabling_conditions": "Use of RGB-D-derived 3D bounding boxes as an intermediate perceptual representation to enable transfer.",
            "fidelity_requirements_identified": null,
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": "Paper cited as an example where sim-to-real has been applied to rigid manipulators by leveraging MuJoCo-trained policies and transferring using perception via 3D bounding boxes from RGB-D.",
            "uuid": "e1661.1",
            "source_info": {
                "paper_title": "Sim-to-Real Transfer of Soft Robotic Navigation Strategies That Learns From the Virtual Eye-in-Hand Vision",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "PyBullet KUKA Peg-in-Hole",
            "name_full": "Sim-to-real transfer for reinforcement learning without dynamics randomization (Kaspar et al.) — KUKA LBR iiwa peg-in-hole (cited)",
            "brief_description": "Cited work using PyBullet to train reinforcement learning policies for a KUKA LBR iiwa peg-in-hole task and then transferring them to the physical robot, presented as an example of sim-to-real for rigid-arm manipulation.",
            "citation_title": "Sim2real transfer for reinforcement learning without dynamics randomization.",
            "mention_or_use": "mention",
            "agent_system_name": "KUKA LBR iiwa robotic arm (peg-in-hole task)",
            "agent_system_description": "A rigid 7-DoF manipulator used for peg-in-hole insertion tasks; trained in PyBullet in the cited work.",
            "domain": "robotic manipulation / assembly",
            "virtual_environment_name": "PyBullet",
            "virtual_environment_description": "Physics simulator for rigid-body and contact dynamics commonly used for RL experiments.",
            "simulation_fidelity_level": "Approximate rigid-body dynamics useful for RL; contact modeling used for insertion tasks.",
            "fidelity_aspects_modeled": "Rigid-body dynamics, contact interactions relevant to peg-in-hole tasks (in cited work).",
            "fidelity_aspects_simplified": "Exact details not provided in this paper's discussion; per-citation the cited work emphasizes transfer without dynamics randomization.",
            "real_environment_description": "Physical KUKA LBR iiwa arm executing peg-in-hole after transfer (from cited work).",
            "task_or_skill_transferred": "Peg-in-hole insertion policy",
            "training_method": "Reinforcement learning in PyBullet (in cited work)",
            "transfer_success_metric": null,
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": false,
            "domain_randomization_details": "Cited as an example 'without dynamics randomization' (i.e., transfer claimed without physics randomization).",
            "sim_to_real_gap_factors": null,
            "transfer_enabling_conditions": "Cited work's claim: transfer without dynamics randomization (details in original paper).",
            "fidelity_requirements_identified": null,
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": "Mentioned as a related example where sim-to-real transfer for RL was achieved on a rigid manipulator using PyBullet without dynamics randomization (details in the cited publication).",
            "uuid": "e1661.2",
            "source_info": {
                "paper_title": "Sim-to-Real Transfer of Soft Robotic Navigation Strategies That Learns From the Virtual Eye-in-Hand Vision",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Soft-Robot Sim2Real Examples (mentions)",
            "name_full": "Various cited sim-to-real works for soft robots (differentiable simulation, proprioceptive sensing, scalable design transfer)",
            "brief_description": "The paper references multiple recent sim-to-real examples for soft robots, including differentiable-simulation-based transfer for soft robotic fish, zero-shot sim-to-real for pneumatic proprioceptive sensing, and scalable design transfer approaches, as contextual related work.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_system_name": "Various soft robotic systems (soft robotic fish, pneumatic soft robots, modular soft robots)",
            "agent_system_description": "Cited works cover a range of soft robot platforms where sim-to-real techniques have been explored (e.g., differentiable simulation for locomotion, proprioceptive 3D sensing transfer, scalable design transfer).",
            "domain": "soft robotics (locomotion, proprioception, design transfer)",
            "virtual_environment_name": "Varied (differentiable simulators, specialized FEM/soft-body simulators, custom pipelines)",
            "virtual_environment_description": "Simulators that support differentiable dynamics, FEM soft-body modeling, or other soft-matter approximations depending on the cited study.",
            "simulation_fidelity_level": "High-fidelity soft-body/differentiable simulation in some cited works; specifics depend on each citation.",
            "fidelity_aspects_modeled": "Soft-body dynamics, deformation, contact for locomotion and sensing tasks (as reported in cited works).",
            "fidelity_aspects_simplified": "Per-citation simplifications vary; this paper does not detail specifics beyond citing these works.",
            "real_environment_description": "Physical soft robots in diverse tasks (e.g., fish-like locomotion, pneumatic actuators) in the cited literature.",
            "task_or_skill_transferred": "Examples include locomotion policies, proprioceptive sensing models, and design-to-reality transfer for soft robots.",
            "training_method": "Varies across cited works (differentiable simulation-based learning, supervised transfer, etc.).",
            "transfer_success_metric": null,
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": null,
            "transfer_enabling_conditions": null,
            "fidelity_requirements_identified": null,
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": "Cited to show that sim-to-real for soft robots is emerging (examples include differentiable simulators and proprioceptive sensing transfer), motivating the current work which focuses on navigation and virtual eye-in-hand vision in SOFA.",
            "uuid": "e1661.3",
            "source_info": {
                "paper_title": "Sim-to-Real Transfer of Soft Robotic Navigation Strategies That Learns From the Virtual Eye-in-Hand Vision",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sim2real learning of obstacle avoidance for robotic manipulators in uncertain environments.",
            "rating": 2,
            "sanitized_title": "sim2real_learning_of_obstacle_avoidance_for_robotic_manipulators_in_uncertain_environments"
        },
        {
            "paper_title": "Sim2real transfer for reinforcement learning without dynamics randomization.",
            "rating": 2,
            "sanitized_title": "sim2real_transfer_for_reinforcement_learning_without_dynamics_randomization"
        },
        {
            "paper_title": "Sim2real for soft robotic fish via differentiable simulation.",
            "rating": 2,
            "sanitized_title": "sim2real_for_soft_robotic_fish_via_differentiable_simulation"
        },
        {
            "paper_title": "Toward zero-shot sim-to-real transfer learning for pneumatic soft robot 3D proprioceptive sensing.",
            "rating": 2,
            "sanitized_title": "toward_zeroshot_simtoreal_transfer_learning_for_pneumatic_soft_robot_3d_proprioceptive_sensing"
        },
        {
            "paper_title": "Scalable sim-to-real transfer of soft robot designs.",
            "rating": 2,
            "sanitized_title": "scalable_simtoreal_transfer_of_soft_robot_designs"
        },
        {
            "paper_title": "Nerf2real: Sim2real transfer of vision-guided bipedal motion skills using neural radiance fields.",
            "rating": 1,
            "sanitized_title": "nerf2real_sim2real_transfer_of_visionguided_bipedal_motion_skills_using_neural_radiance_fields"
        }
    ],
    "cost": 0.015405249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Member, IEEEJiewen Lai jiewen.lai@cuhk.edu.hk 0000-0002-9480-4637
Ti-Ao Ren taren@buct.edu.cn 0000-0002-6488-1551
Wenchao Yue 0000-0002-9788-3705
Member, IEEEShijian Su shijiansu@cuhk.edu.hk 0000-0001-7155-2660
Jason Y K Chan jasonchan@ent.cuhk.edu.hk 0000-0002-9480-4637
Senior Member, IEEEHongliang Ren hlren@ieee.org 0000-0002-6488-1551
Tian-Ao Ren </p>
<p>Department of Electronic Engineering, The Chinese University of Hong Kong,
Hong Kong, SAR, China Beijing 100013, China, Shenzhen 518057, China SAR, China</p>
<p>College of Mechanical and Electrical Engineering, Beijing University of Chemical Technology,</p>
<p>CUHK Shenzhen Research Institute (SZRI),</p>
<p>Department of Otorhinolaryngology, Head and Neck Surgery, The Chinese University of Hong Kong, Hong Kong,</p>
<p>AC46CEDFE6F9D24C3BA5B30254F7DCF210.1109/TII.2023.3291699received 30 March 2023; revised 19 May 2023; accepted 28 June 2023. Date of publication 3 July 2023; date of current version 19 January 2024.Motion planningrobot learningsimulationsoft robotics
To steer a soft robot precisely in an unconstructed environment with minimal collision remains an open challenge for soft robots.When the environments are unknown, prior motion planning for navigation may not always be available.This article presents a novel Sim-to-Real method to guide a cable-driven soft robot in a static environment under the Simulation Open Framework Architecture (SOFA).The scenario aims to resemble one of the steps during a simplified transoral tracheal intubation process where a robotic endotracheal tube is guided to the upper trachea-larynx location by a flexible video-assisted endoscope/stylet.In SOFA, we employ the quadratic programming inverse solver to obtain collision-free motion strategies for the endoscope/stylet manipulation based on the robot model and encode the virtual eye-in-hand vision.Then, we associate the anatomical features recognized by the virtual vision and the joint space motion using a closedloop nonlinear autoregressive exogenous model (NARX) network.Afterward, we transfer the learned knowledge to the robot prototype, expecting it to navigate to the desired spot in a new phantom environment automatically based on its eye-in-hand vision only.Experiment results indicate that our soft robot can efficaciously navigate through the unstructured phantom to the desired spot with minimal collision motion according to what it has learned from Manuscript</p>
<p>Sim-to-Real Transfer of Soft Robotic Navigation</p>
<p>Strategies That Learns From the Virtual Eye-in-Hand Vision</p>
<p>I. INTRODUCTION</p>
<p>M ANY soft robot manipulators and systems have been designed and intended for the applications of medical intervention in the past few decades [1].They are ideal candidates for robotic surgical tools when force transmission is a noncritical factor [2]. Inspired by biological compliant structures, these soft continuum robots can navigate or work in complex environments with the employment of well-established kinematics, dynamics, and mechanics [3], [4], [5].Besides, due to the challenges in describing those highly nonlinear compliant manipulators made from soft materials with low Young's Modulus and interaction [6], model-free approaches like visual servoing (VS) [7], sensorimotor learning [8], and finite-element methods (FEM) [9] were widely utilized in the soft/continuum robotic control.In general, a flexible robotic medical intervention would request a 2-D/3-D reconstruction of the device in an occlusive environment.The used-to-be difficult reconstruction is now becoming convenient because of the technological advances in sensors, such as electromagnetic sensing [10], Fiber Bragg grating [11], and learning-based strain gauges-liked networks [8], but at a relatively high cost.In addition, prototypedependent sensory systems often require recalibration on every individual device, as the multisensor assembly may differ from one to another.For example, a learning-based embodied strain gauges array system may become invalid when deployed to another identical flexible robotic system due to minor assembly errors in the real world, necessitating local black-box relearning.We expect that an approach with a minimal amount of sensors and on-site calibration could significantly generalize these novel soft devices to practical use with lower cost and higher reproductivity.</p>
<p>Simulation-to-Reality ("Sim-to-Real" or "Sim2Real") transfer learning will meet our claimed expectations.Bonding the linkage between simulation and reality is one of the essential steps toward the metaverse.With extensive prior study in simulation, one can reproduce the virtual result on a physical soft robot.For example, with extensive studies in kinematics, dynamics, mechanics, and morphology, model-based simulators can be developed to contribute to different control problems such as contact detection [12], soft materials shrinkage upon actuation [13], hybrid rigid-soft robots [14], soft parallel robots [15], etc.In addition to robot control, simulators can sometimes help us to design better soft robots to fit different applications [16], [17], [18].</p>
<p>Another stream of soft robotic simulations may include the virtual world's sensing, and the physical interaction [19].A cohort of researchers from French institutes developed a soft robots plugin for the Simulation Open Framework Architecture (SOFA) [20], [21], with physics-based soft body dynamics.The soft robots plugin is capable of deriving the quantitative relationship between the robot's deformation and the changes in the inputs of the actuators (i.e., joint space) based on a real-time direct/inverse FEM solver that considers mechanical parameters like material, geometry, and morphology.Such an open-source toolkit has been useful for the community to probe into the robots' modeling, characterization, and interaction problems with plausible visualization before or during the transfer to prototypes.</p>
<p>However, most of the reported applications are solely for visualization without online deployment.In fact, a reliable simulation can be utilized to advise a closed-loop control strategy based on the robots' perception in the virtual environment.By using different simulation techniques, sim-to-real transfer learning was applicable to industrial robots.For instance, Zhang et al. [22] present a sim-to-real learning method that trains a rigid manipulator in MuJoCo to avoid colliding with obstacles and then transfers to the physical world using 3-D bounding boxes estimated from RGB-D vision.In [23], a sim-to-real transfer method is introduced for reinforcement learning deployed on a KUKA LBR iiwa arm for a peg-in-hole task with PyBullet.Due to the availability of the well-developed simulation platform and mature robot models in a unified robotics description format (URDF), perception beyond joint space is no longer a must in the closed-loop feedback.As a result, the learned policy of rigid robots is oftentimes and readily applicable in the real world.While sim-to-real-based control policies are common in rigid robots (e.g., CoppeliaSim, MuJoCo, and RoboDK, to name a few), they have rarely been reported on soft robots until recent years.</p>
<p>Soft robot-wise, sim-to-real transfer methods can assist the robots design and fabrication [24].The calibration of visionbased 3-D shape sensing of pneumatic soft robots can also be trained in simulation and transferred to real-world deployment [25].In [26], an open-source sim-to-real transfer method is put forward to predict the morphology of cube-based soft robotic dice.The work is further extended to transfer the simulated locomotion to reality [17].By exploring planar kinematics, which can be geometrically simulated, Greer et al. [27] present the autonomous navigation task for soft growing robots in a tortuous maze with an overhead view.However, 3-D navigation, a capability of soft manipulators that are often sought after and competent in and which could have benefited from the sim-to-real transfer, has yet to be reported.</p>
<p>This article proposes an SOFA-based sim-to-real method for soft robotic navigation that learns from virtual eye-in-hand vision.Based on the underlying principle of the simulator, we assume that the SOFA's results may be very likely to resemble the real case scenarios.Aiming to navigate a cable-driven soft robot in a confined environment, we first reconstruct a simulation scene in the SOFA framework that resembles the situation to perform collision-free path and motion planning that could be useful for endoscopic manipulation.Then, we employ the prior knowledge from the simulation to train a closed-loop control policy for a soft robot's navigation.By learning what the robot "sees" and how it simultaneously "moves" in the joint space according to a series of optimized motions in the virtual world, we can transfer the learned policy to the physical robot and teach it how to "move" depending on what it "sees" in the real world-and the environment is unknown to the robot except for regular anatomical features we are intrigued in.A dynamic neural network called nonlinear autoregressive exogenous model (NARX) [28] is adopted for virtual learning that features time-series modeling.Instead of presenting a simple open-loop sim-to-real method, our closed-loop policy can be directly transferred to the tangible robot in a one-off manner, which can considerably reduce the on-site calibration and multisensory employment for the navigation tasks.Experiments with further evaluation validate the method's feasibility and performance.This article contributes to the soft and medical robot communities in following ways.</p>
<p>1) A newly presented 3-D-printed cable-driven soft robotic system featuring a miniature manipulator, soft material, and mechatronic-decoupled design for soft robot-based endoscopic manipulation.2) A novel SOFA-based sim-to-real method that learns from the virtual eye-in-hand vision for simulated and realworld soft robotic navigation relying on a light-weighted NARX network.3) An interdisciplinary pilot study of autonomous soft robotbased endoscopic manipulation powered by our sim-toreal method.4) A comprehensive experimental validation and evaluation of our sim-to-real method for soft robots.To the best of our knowledge, this is the first physical simulation-based sim-to-real method that enables soft robotic navigation that learns from virtual eye-in-hand vision.The method enables the transfer of complicated soft robot motion computed by a numerical solver in SOFA to a real-world robot with additional visual perception to improve transfer fidelity.</p>
<p>The rest of the article are organized as follows.Section II describes the gist of (soft) robotic transoral tracheal intubation with its background introduction.Section III sketches out the design and assembly of the robotic system that will be used in simulation and experiment.Section IV demonstrates the kernels of how we construct the soft robotic-based endoscope (stylet) manipulation scene, motion planning, and generation of the dataset in SOFA.Section V presents the implementation of learning.Transferring the learned policies that actuate the soft robot with an optimal motion from the SOFA environment to a real-world system based on virtual and real eye-in-hand vision.Fig. 2. Task description: The robot tip is automatically steered to reach the upper glottis with the help of the selected anatomical features obtained by its eye-in-hand vision.The soft body's motion with a minimal collision with the surrounding is realized by the control strategy that learns from the simulation.</p>
<p>Section VI shows the experimental validation of the proposed method on a phantom.Finally, Section VII concludes this article.</p>
<p>II. TASK DESCRIPTION</p>
<p>We assume the primary task for the soft robot is that it can automatically guide the soft robot's tip to a reachable 3-D spot in a confined environment with optimal body motion throughout the navigation process.Here, we choose the robotic endoscope/stylet manipulation in transoral tracheal intubation (TI) as an example to investigate the feasibility and performance of our proposed sim-to-real method.A stylet or flexible bronchoscope is typically used to guide the endotracheal tube (ETT) to reach the desired spot.Despite the conventional "blind" stylets, video-assisted [29] and semi-robotic stylets [30] were proposed to help with the transoral TI.However, it comes to our attention that, except for pink tissues, the endoscope fails to provide identifiable views for a good while during the navigation.The situation poses a major challenge in deploying VS for the task.Nonetheless, we see the potential to overcome the challenge by using a soft robot with a sim-to-real capability.</p>
<p>To automate this procedure, as illustrated by Fig. 2, we assumed that a vision-embedded soft robotic manipulator would work as a steerable stylet to autonomously navigate to the locations near the upper glottis with minimal robot-environment collisions during the feeding.Along with feeding, there are two major turnings for the soft body.The first major turning, obviously, occurs near the palatine uvula that separates the oral cavity and oropharynx.After a blackout period when no key features can be perceived, the second turning occurs near the arytenoid (corniculate) cartilage at the hypo-pharyngeal area separating the trachea and esophagus [31]: the former belongs to the respiratory system, whereas the latter belongs to the digestive system.We can employ the null space motion of a multisegment soft robot with proper motion planning to produce a dexterous motion that avoids collisions as much as possible.</p>
<p>III. SOFT ROBOTIC SYSTEM: DESIGN AND ASSEMBLY</p>
<p>A soft robotic system and its manipulator parameters were especially designed to validate our method.As shown in Fig. 3, the cable-driven soft robot has two coupled flexible segments.Three independent cables actuate each segment.The cables (∅0.38 mm nylon wire) are threaded through their respective ∅0.8-mm channels that are isometrically distributed in a radius of 2.1 mm.A 2-mm-diameter main channel is reserved at the axial center.The robot base is mounted on a linear slide for a feed motion along the axial direction.Our design features a proximal segment of 60 mm and a distal segment of 70 mm in length, with a unified diameter of 6.2 mm to imitate a stylet or an endoscope and ease of fabrication and assembly.The soft bodies were made from Agilus30 photopolymer using a PolyJet 3-D printer (J826 Prime, Stratasys).</p>
<p>Each cable is winded on a spool mounted on a bearing on the fixture base.The spools can be rotated by the couplers that are connected to the flexible shafts actuated by the respective dc motor (1000:1 gear ratio, 6 V) with an encoder for angular feedback.The dc motors are proportional-integral differential (PID)-controlled by a low-cost self-assembled motion controller equipped with three L293D units and a general microcontroller.The linear slide is driven by a stepper motor drive.With some step-down transformers, all electronic components are wellfitted in a portable acrylic box with a standard power cable (220-V ac) and a USB port for communication with the PC.Such spatial-mechatronic-decoupled design reduces the footprint size, increases the system's portability, and facilitates the free space posing of the robot base.</p>
<p>IV. SIMULATION: SOFA-BASED ROBOT MODELING</p>
<p>A. Virtual Environment</p>
<p>We configured the simulation in SOFA v22.06.99.To align with the physical scene where the robot tip would be facing toward the ground, we set the virtual gravity to be [0, 0, 9.81] m/s 2 .As the baseline, this work is predicated on ideal anatomic scenes with clear visibility of the airway anatomic structures, with the uvula of class-I visibility in the Mallampati score [32] and vocal cords of grade-I classification in the Cormack-Lehane system [33].The assumption is further depicted in Fig. 4. It should be noted that unless extensive data with various anatomic conditions are used, the reconstructed and phantom environments we present may not fully reflect the complexity and variety of real cases.The sim-to-real discrepancy at the current visibility class/grade can be further reduced by using Fourier domain adaptation [34] and style transfer [35] to even achieve pixel-grade cross-domain (SOFA/phantom) feature segmentation [36].A modified oropharyngeal-tracheal 3-D phantom [37] was directly imported into the scene in obj format.To reduce the expensive finite-element computation, we trimmed some insignificant entities from the phantom, such as teeth and miscellaneous muscles, leaving the phantom with 26 706 triangular surfaces, as shown in Fig. 5(a) and 5(b).</p>
<p>B. Robot Modeling</p>
<p>In SOFA, robot modeling depends on the meshed solid model of the soft bodies and their geometric constraints, including cable distribution, actuation regulation, and partial solidification.To do that, the soft robot was first sketched in FreeCAD (a free and open-source software under the LGPL-2.0-or-laterlicense) and exported to brep or step format.Then, we imported the model into Gmsh (a free software under the general public license) for the meshing and exported it to both vtk and stl format.The vtk was used to add the finite-element model in SOFA, and the stl was used to define the visual model and collision model.After some trial-and-errors with the consideration of computational cost and rationality, we tetrahedrally meshed the soft segments into voxels with 4985 vertices, excluding any cable channels in the mesh.</p>
<p>Based on the prototype fact, we resembled the physical properties in SOFA with the Young's modulus E = 0.8 MPa, Poisson's ratio ν = 0.45, and the total mass m soft = 7 g.The cable actuation mechanism was geometrically constrained in the Python script, which can be expressed as
L i,j,k = ⎡ ⎢ ⎣ (−1) j • r c • sin (j − 1) β −r c • cos (j − 1) β (k − 1) d ⎤ ⎥ ⎦(1)
where i = 1 and i = 2 represents the proximal and distal segment, respectively; j = {1, 2, 3} denotes the indexed cable; and k = {1, 2, . .., N i } indicates the kth node along the soft body.Note that N i varies from different segments, and N 2 &gt; N 1 .For the constants, r c = 2.2 mm is the radius for cable distribution, β = 2π/3 is the angular offset between the neighboring cables of the same segment, and d = 5 mm denotes the sampling distance along the soft body.As for the rigid shaft, it has to be solidified as a rigid part.The working range of the prismatic joint along the feeding axis was set to be [0, 80) mm.Fig. 5 gives an intuitive illustration of our robot modeling in SOFA.</p>
<p>C. Collision Avoidance Motion Planning</p>
<p>With all the preparations ready, we then defined a relative position between the soft robot's base and the environment.For the sake of convenience and valid computation, we located the robot base frame {B} at P target = [0, −55, 160] mm with respect to the target site as a basic status (also refer to Figs. 1  and 9).The relative frames relationship can also be found in the weak registration in the real-world deployment, which will be discussed in Section VI.</p>
<p>The built-in QPInverseProblemSolver was used for the collision avoidance motion planning.The cost function is a quadratic function that minimizes the actuation and the distance between robot meshes (a function of the actuation) and the environment.The solver implements the QP problem with linear complementarity constraints (QPCC) [20] based on the qpOASES library to inversely compute the corrected FEM-based robot model in response to the actuators, actuator constraints, and surroundings.Different primitives, including point, line, and triangle, were utilized in the narrow phase intersect detection.A local minimum distance proximity method was used to evaluate the anticipation of contact with an alarming distance of 2 mm and a contact distance of 0.5 mm.The pseudo-code in Algorithm 1 depicts the collision-free navigation deployment workflow in SOFA.Fig. 6 demonstrates two resultant examples in collision avoidance and evaluation with the said proximity method.As the robot moves, the objective values from the QP formulation converge.</p>
<p>Since we would be interested in the eye-in-hand vision, a camera frame {C} was additionally attached at the robot's tip to provide an endoscopic view.For simplicity, we ignored that offset between the camera and the tip frame and coincided with them, i.e., {C} ≈ {T }.Given that there are no available tools to acquire the endoscopic view, we defined a fixed frame on the plane perpendicular to all cable ends-based on trigonometryas the camera frame.The coordinates of three cable ends can be indicated w.r.t. the robot base as p j = L 2,j,N 2 ∈ R 3×1 where j = {1, 2, 3}.Then, the plane formulated by those coordinates can be calculated by
α β γ = (p 1 − p 2 ) × (p 3 − p 2 ) . (2)
Thereby, the orientation of the camera frame can be computed by
C = ⎛ ⎜ ⎜ ⎝ ⎡ ⎢ ⎢ ⎣ arccos α 2 +γ 2 α 2 +β 2 +γ 2 arctan α γ 0 ⎤ ⎥ ⎥ ⎦ ⎞ ⎟ ⎟ ⎠ ⊗ • R x (π) • R z (π) (3)
where (•) ⊗ denotes the operations that convert an Euler angle to a rotation matrix, and R x (θ) and R z (θ) represent the rotation matrices of θ on the subscripted axis.The rotation matrix C was then converted to a quaternion for use.The frame origin was located at 1  3 (p 1 + p 2 + p 3 ) .With the specific definition of viewport coordinate and focal length, the eye-in-hand view can be acquired from the QtViewer using OpenGL.This method would grant us theoretically unlimited virtual endoscopic image data of anatomical features/organs, as long as we can build in SOFA, without concerning privacy issues.</p>
<p>V. LEARNING FROM THE VIRTUAL VISION</p>
<p>In this work, the learnings can be divided into two parts and will be introduced in this section.Section V-A describes the use of primarily SOFA-generated images (i.e., eye-in-hand viewport) with a small number of phantom pictures blended for anatomical feature recognition.While Section V-B depicts the recurrent learning between the SOFA-generated joint space motion-subjected to the QP-constraints for collision avoidance-and the resulting labels of recognized features in the virtual environment.</p>
<p>A. Recognizing Anatomical Features Using YOLO</p>
<p>We employed You Only Look Once (YOLO) [39], a real-time object detection algorithm, for the online anatomical feature recognition task.Due to the limitation in available virtual 3-D models, the SOFA environment is overanimated, which fails to satisfy the feature recognition task in the real world.To bridge the gap between simulation and reality in this regard, we blended the simulated endoscopic dataset with some pictures of the phantoms.The dataset size is given in Table I.The images in the dataset were labeled using bounding boxes with corresponding feature tags.The dataset was arbitrarily divided into training (80%), validation (10%), and test set (10%).Among the four</p>
<p>TABLE II MODEL SUMMARY OF YOLOV5S-BASED FEATURE RECOGNITION</p>
<p>released models (https://github.com/ultralytics/yolov5),namely the 5s, 5m, 5l, and 5x, we opted for the most lightweight YOLOv5s model.The model was set to be trained for 800 epochs with a batch size of 4, and the early stopping (patience at 100) was triggered at the 481st epoch, meaning that the best results were observed at epoch 380.The network performance is given by Fig. 7 and Table II, showing that it can classify the three classes with a high precision of 0.989, 1, and 0.989, for uvula, epiglottis, and glottis, respectively.As an indicator metric for object detection, Table II also explicitly provides the mean average precision (mAP) for intersection over union (IoU) greater than 0.5 and from 0.5 to 0.95.Figs. 8 and 9 demonstrate the effectiveness of the trained feature recognition model in both simulation and reality scenes, where the recognized features could be parameterized into the respective category (see the Z-axis of Fig. 11) with coordinated bounding boxes in real time.The generalization of feature recognition can be improved further by taking into account anatomic appearances with varying visibility grades introduced in Section IV-A.</p>
<p>B. Eye-Hand Learning Using NARX</p>
<p>Since the virtual endoscopic images and the robot actuation ("eye-hand") are temporal dependents, the NARX model [40] was used for the learning.NARX is a class of discrete-time nonlinear models that are often utilized as an open-loop or closed-loop form multistep predictor in times-series modeling.The advantage of using the NARX is that the whole operation can be involved by a single model.Such a model can be algebraically represented by [28] y (t + 1) = f <a href="4"> y(t), y (t − 1) , . . ., y (t − n y + 1)
u(t), u (t − 1) , . . . , u (t − n u + 1) </a>
where y(t) and u(t) are, respectively, the output and the input sequence of the network at the discrete time step of t.Meanwhile, n y and n u are the delays in output and input, respectively, subject to n u ≥ n y ≥ 1.The dependent output value at the next time step y(t + 1) is regressed on its previous output and previous independent exogenous input.Since we cannot provide a perfect driver sequence y(t) in prior to the NARX network for prediction, we need to train the network in a closed-loop way, i.e., using the newly predicted driver sequence as part of the input, then combining it with the visually recognized labels for the next prediction.Besides, we improved the training process regarding the model generalization and overfitting avoidance by using the early stopping method with automated regularization under the Bayesian framework [41].The model training was implemented using the Neural Network Toolbox of MATLAB.In the NARX network, d 1 and d 2 denote non-negative input delays and output (feedback) delays, respectively.These hyperparameters must be tuned based on the specific problem and data characteristics, and no reference values exist.Here, we empirically initialized the input delays as d 1 = [1 : n u ] and the output delays as d 2 = [1 : n y ], where n u = 7 and n y = 5.For the initialization of network training, we found that the NARX network would produce more stable initial predictions if the input delay were replenished with some small nonzero values at the first seven timesteps, which is in response to the input delay.In our work, we supplemented the SOFA-generated joint motion with arbitrary small values as
y init = 10 −4 ⎡ ⎢ ⎣ | | . . . | 1 2 . . . 7 | | . . . | ⎤ ⎥ ⎦(5)
for each training.It has been tested that using other small values for initialization would not cause a significant difference.Whereas, the initial amended labels can be u init ∈ R 1×7 as long as it does not interpret any executable features.</p>
<p>After training a total of 300 NARX networks, we kept the network with the most satisfactory performance.The selection was made by feeding each network with ten sets of new label data from YOLO and driven data from SOFA exclusive from the original dataset for training, validation, and testing, and obtaining the mean squared error performance for comparison.To diversify the simulation data, we added a cohort of offsets of [±3, ±3, ±5] ∈ Z in millimeter on the target position P target in three axes, resulting in 20 groups of simulation paths in SOFA.Using offset targets for the training could contribute to weak robot-patient registration in a real-world deployment.The offsets were also selected to fulfill the consequence that each path would correspond to a unique endoscopic view and QP-solved actuation.Fig. 11 illustrates the sequences of feature(s) captured by the eye-in-hand vision in SOFA.The variable target positions affect the virtual visual perception in terms of timing, duration, and recognized features, enriching the simulation data.Such variation mimics the slight individual difference in physiological appearance among people, which would benefit the model's adaption to new oropharyngeal environments.</p>
<p>The architecture diagram in Fig. 10 summarizes our proposed method.The robot modeling and the feature recognition were implemented in Python and YOLOv5 (PyTorch framework), respectively, while the eye-hand learning using the NARX network was performed in MATLAB R2020a.The networks were Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.Fig. 9. Snapshots of one of the SOFA simulation groups: Based on the built-in QPInverseProblemSolver, a set of feasible actuators' solutions that avoid physical body collision can be derived and fed back into the simulator for visualization.By associating a camera frame {C} to the robot's tip, a simulated endoscopic vision (i.e., eye-in-hand) can be obtained for further feature learning.The view encounters a "blackout" period without recognizing any key features.Fig. 10.System architecture of the proposed sim-to-real transfer learning method: The system uses the simulation data from SOFA for the NARX network training.While in a real environment, the system codes the recognized features from its real eye-in-hand vision (labeled as u(t + 1)) and current joint space motion y(t) to forecast the next move y(t + 1).Here, the S refers to the length of simulation timestep.</p>
<p>trained on an NVIDIA GeForce RTX 3060 GPU.Since multiple platforms were involved, we employed a User Datagram Protocol (UDP) socket that allows the Python program to stream the real-time recognition to where the MATLAB host on the actuator side could receive it.</p>
<p>VI. EXPERIMENT</p>
<p>A. Experiment Setup</p>
<p>A simplified robotic TI scene was set for the experiments.A CMOS image sensor (OV6946, OmniVision, CA, USA) was used to provide eye-in-hand vision.The 1.8-mm-diameter LEDequipped image sensor can capture 400 × 400 resolution video stream at a 30 fps frame rate.To show the generalization of the proposed method, in the experiment, we used a commercially available clinical oropharyngeal phantom, which was different from the simulated scene that produced the eye-hand dataset for NARX network training.The experiment setup is shown in Fig. 12.The linear slide that holds the manipulator was vertically installed with the robot tip pointing toward the ground and fixed on an adjustable holder.A permanent-magnetic-based tracking system was used to obtain the 3-D position of a magnet attached to the robot tip.A tiny NdFeB magnet was used to avoid excessive payload.Based on the magnet's size, the valid measurement range of the tracking system is about 100 mm above the array.Before configuring the sim-to-real method, we tested the robotic system with open-loop control.As shown in Fig. 12, two tip paths were imported to the SOFA to derive the inverse solution in joint space.The paths (in millimeter) for the circle and ∞-shape are, in t = 0 : π/500 : 2π time steps,
x ref,o = 16 • sin(t), y ref,o = 16 • cos(t), and x ref,∞ = 22 • sgn(cos(t)) • (cos(t) • cos(t)), y ref,∞ = 22 • sgn(cos(t)) • sin(t) • (cos(t) • cos(t))
, respectively, with a height of 52 mm above the magnetometer array.The • operator denotes the element-wise (Hadamard) product.Due to the unit problem, the SOFAgenerated inputs necessitate an overall amplification to fit the prototype setup, such as spool sizes and the minor transmission losses of using flexible shafts.The measured results show that our setup can reproduce the desired path with an average spatial positioning error below 2 mm in open-loop mode, which is adequate for using the latter sim-to-real validation with closedloop control.</p>
<p>B. NARX Performance</p>
<p>We performed a prior experiment in the simulation to evaluate the closed-loop NARX network performance in our joint space motion forecasting task.Initially, three new target positions proximate to the upper glottis were randomly selected in SOFA.13 Notably, the selected target position was intentionally excluded from the training, validation, and testing datasets to prevent biases.After running the simulations, we obtained the SOFAgenerated joint space motion with a time-series feature sequence observed by the endoscopic vision in the virtual scene.After that, we tried to feed the recorded feature to the trained NARX in sequence-imitating a real-time feature sequence that the actual camera vision would attain-and examined the alignment between the forecasted and SOFA-generated joint space motion.The result is given in Fig. 13.It can be seen that the endoscopic vision would observe the features differently in terms of time and the ROI of features.Such variant observed features and the closed-loop mechanism would result in NARX-forecasted joint motions that are deviated from the reference joint motions computed by the QP solver of SOFA.However, the deviations are insignificant for the overall robot motion.As shown in Fig. 13, the cable joints of the proximal segment, joints 1-3, are nearly merged with only minor differentiation.The resultant task motion showed that such joint motion would stiffen the proximal motion to antagonistically resist the passive bending motion caused by the distal segment, which conforms to the literature [42], [43] and simulation.Table III demonstrates the R-squared coefficient of determination of the motion of each Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.joint for the aforementioned experiments.The R-squared coefficient is given by
R 2 = 1 − SS res SS tot
where SS res and SS tot denote the residual sum of squares and the total sum of squares, respectively.It measures how well the NARX network forecast can fit the SOFA-generated outcomes.Based on the aforementioned three experiments, the table shows that the average R 2 coefficient for the cable joints is 0.963, with the lowest performance shown in joint 3 (one of the cable joints for the proximal segment).In contrast, joint 7 (the prismatic joint) has an average R 2 coefficient of 0.997.The prior results validate the fidelity between what the trained NARX network produces and the computation from the SOFA.It supports the closed-loop sim-to-real implementation in Section VI-C.Moreover, a comparison was made between the performance of the NARX network and the Long Short-Term Memory (LSTM) network, which is frequently employed for learning time-series sequential data.The trained networks were assigned a new target position of [0, −55, 155] mm for joint motion predictions, and the outcomes were subsequently contrasted with the joint motion computed by the QP solver in SOFA.Fig. 14 presents a side-by-side comparison of the network predictions.The R 2 coefficients for the LSTM and open-loop NARX networks are 0.983 and 0.997, respectively, and the training time for these open-loop models take, respectively, about 120 and 10 s with an Intel i9 12th-gen CPU using MATLAB.Nevertheless, when fitting the predicted and referenced outputs based on such a single trial, the NARX network exhibited a mean square error (MSE) that was 15.7% lower than LSTM.These results suggest that the NARX network outperforms LSTM in terms of performance with less training time.Furthermore, the NARX network has a simpler architecture, requires less computation, and exhibits better generalization and robustness to input changes than LSTM especially when only small amounts of training data are available [44].Here, training a usable closedloop NARX network for 3000 epochs takes about 12 min on the same PC configuration.And thanks to the relatively short training time, we could explicitly test the training effect, adjust the hyperparameters promptly, and select the optimal model in due course.</p>
<p>C. Validating the Sim-to-Real Transfer on Phantom</p>
<p>An overview of our sim-to-real-driven deployment is shown in Fig. 15.A video is also available in the supplementary material.Using a phantom different from the simulation, the soft robot's tip was placed about 15 mm above the uvula to resemble the initial scene of the SOFA's as much as possible.When operated, the robot would navigate to the upper glottis location based on only its real-time eye-in-hand vision.The vision underwent the YOLO algorithm for feature detection, and the recognized feature sequences were decoded into executable joint space motion by the trained-NARX network.Fig. 16 demonstrates the selected video sequence from the eye-in-hand vision of two experiments with slightly different initial settings of the relative positions between the robot and phantom.Their corresponding recognized features are also given in Fig. 16.Even with different initial placements, it can be observed that the robot can efficaciously navigate to the desired location without significant collision with the environment, which is unstructured in any of our simulations.The measured tip paths during the navigation are available in Fig. 17, which also indicate the variability of the network's decision depending on what the eye-in-hand vision receives in real time.As we have planned in the SOFA scene, the soft robotic endoscope/stylet manipulation should:</p>
<p>1) avoid colliding with the uvula at the beginning; 2) perform the first major bending at the oropharynx; 3) distinguish esophagus and glottis, and then, align to the latter.To evaluate the aforementioned criteria: 1) can be visually examined; 2) can be verified by post-evaluation of tip position measurement; and 3) can be visually evaluated by the real-world endoscopic view.</p>
<p>The experiment results showed that the robot could conduct the given navigation task automatically based on what it had learned from the simulations.Due to the different initial settings, the robot "saw" different feature sequences, resulting in diverse NARX-forecasted joint space motions as shown in Fig. 18-both of them are capable of driving the robot to fulfill the task.The stiffening effect on the proximal soft segment due to the antagonistic actuation was also realized on the prototype as planned in the physical simulation.As designed in the simulation, the proposed method allows a relative malpositioning between the robot and the phantom.The experimental observation suggested an error tolerance of [0 ± 5, −55 ± 5, 160 ± 5] mm of P target in {B}, which surpasses to the simulation.The success rate of sim-to-real transfer has usually been one of the indicators to evaluate deploying performance [45].Table IV shows the success rate of the sim-to-real transfer in reaching the upper glottis in our phantom experiments.Here, a success reach was   judged by whether it could provide a clear endoscopic view showing the vocal cords or not.While the simulation can always obtain a viable view at the end, a high success rate of 17 out of 20 consecutive trials were found capable of delivering the camera into the spots in real-world phantom experiments.In our observation, failed trials were primarily accused of overexposure due to the intense LED light (can tell from Exp. 2 in Fig. 16), which interferes with YOLO-detection when acquiring valid features for closed-loop feedback.</p>
<p>D. Discussion</p>
<p>The major novelty of our visual-dependent sim-to-real method for soft robots can be highlighted as the following.</p>
<p>1) General Eye-in-Hand VSs:</p>
<p>In general, VS requires detected features in the loop at all times.But oftentimes, the endoscope sees nothing or invalid frames (this has been verified in simulation and real phantom), which is inadequate for valid closed-loop feedback.In contrast, our sim-to-real method can refer to the "memories" that the robot learns from the virtual world, allowing its navigation without relying on a continuous eye of sight of features.If we use an eye-in-hand VS method in our task, it may require many feature labels along the navigation to determine the next step, whereas ours only requires a few, significantly reducing the time-consuming training in feature recognition.Furthermore, compared to the traditional VS method, a sim-to-real method reduces the on-site calibration of the visual system's extrinsic and intrinsic matrices [46], initial Jacobian estimation between the joint and task space [47], and position-configuration measurement [48].In addition, general Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.</p>
<p>TABLE V COMPARISON OF DATASET THAT BLENDS WITH PHANTOM IMAGES ONLY AND WITH ADDITIONAL MEDICAL IMAGES FROM</p>
<p>eye-in-hand VS may not be able to control the whole-body motion of a multisegment soft robot, which is essentially required for the navigation task in a tortuous environment, while our method is capable of whole-body motion control.Due to privacy and ethical concerns, the medical dataset used to deploy VS is typically inaccessible.However, simulation scenes are more accessible and customizable.Computer graphics experts might contribute to more realistic virtual scenes for future sim-to-real deployment.Therefore, as supplementary to the VS methods, a vision-based sim-to-real method like ours will be meaningful for the developers and roboticists.</p>
<p>2) Model-Based Simulation Frameworks: Our soft robot simulation is based on the open-source SOFA framework, instead of model-based simulators.Due to its physics engine, the framework is friendly to deformable entities of soft materials with multiple collision models and collision detection methods, providing a rich source of simulation data for the sim-to-real transfer.The use of embedded eye-in-hand vision in the virtual environment, which is not available in general model-based simulators, improves transfer fidelity as well.</p>
<p>3) Deep Learning Frameworks: A light-weighted network like NARX is more suitable for our desired application.There are only a few anatomical features in the human oropharynx structure, which can be handily covered by the permutation of explainable feature labels.However, a deep learning network requires more expensive computational overhead as it would also account for the voided vision that further challenges the sim-to-real discrepancy.Also, the explainable feature labels with pathologies/defectives can be added to further enrich the simulation dataset in a separate recognition training process.</p>
<p>4) Possible Extension to Real-World Anatomy:</p>
<p>We have also explored the possibility of further extending part of this work to real anatomical application.One of the critical parts will be reducing the discrepancy between simulation and reality regarding the YOLO-based feature recognition.Following a similar strategy, we blended the datasets with some real medical images from open access sources [49], [50] to train a model applicable in simulation, phantom, and anatomical environment.Detailed configuration of the datasets is given in Table V, indicating a comparative mAP to the model that was trained using only SOFA's and phantom images.To further verify the newly introduced model, we input new video clips (i.e., excluded from the learning process) into the model after some necessary trimming (4:21-4:32 and 4:57-5:51) for the recognition test.As shown in Fig. 19, the model was able to recognize the intended features, even though the real anatomical images only account for 7.61% of the dataset.The preliminary results reveal the possibility of applying the proposed sim-to-real transfer strategy in future real-world trials.</p>
<p>5) Limitation: We acknowledge that there are also some limits to this work.For instance, the method lacks perceptual/control mechanisms to deal with possible collisions in different realworld environments.Such drawbacks can be further improved by employing additional haptic sensors and including them in the simulations to diversify the virtual sensing (other than only vision) in future works.Moreover, the proposed method can be further completed by introducing variations on virtual scenes to reduce the discrepancy among patients.</p>
<p>VII. CONCLUSION</p>
<p>This article proposed an SOFA-based sim-to-real method for soft robotic navigation that learns from the virtual eye-in-hand vision using the NARX network.Motivated by the soft robotic endoscope/stylet manipulation procedure before the transoral TI, this work first presented a two-segment 3-D-printed cabledriven soft robotic system featuring a miniature manipulator, soft material, and mechatronic-decoupled design.Based on the prototype and open-source 3-D phantom models, a virtual environment that resembled the soft robot navigation during stylet manipulation was reconstructed in SOFA.The SOFA's built-in QP solver was used to compute the minimal collision motions in both task space and joint space.Meanwhile, eye-in-hand visions in the virtual world were obtained.The YOLOv5 algorithm was configured to recognize the observed anatomical features, namely, the uvula, glottis, and epiglottis, with a high precision of over 98.9% in virtual and phantom environments.</p>
<p>Then, using only the simulation data, a closed-loop NARX network was trained to associate the time-series anatomical features sequence with the SOFA-generated joint space motion.After that, the trained network was employed in the real-world soft robotic system.Equipped with eye-in-hand vision, the soft robot with the NARX network could compute the joint space motion in real time based on what it observes, despite the diverse environment and robot-phantom setting, and autonomously navigate to the desired spot with minimal collision to the environment.The experiment results showed that the soft robot can efficaciously navigate through the unstructured phantom to the desired spot near the upper glottis with minimal collision motion according to what it has learned from SOFA.The average Rsquared coefficient between the closed-loop NARX-forecasted and SOFA-referenced robot's cable and prismatic joint space motion were 0.963 and 0.997, respectively.The eye-in-hand visions demonstrated a good alignment between the robot tip and the glottis.</p>
<p>Fig. 1 .
1
Fig.1.Schematic diagram: Transferring the learned policies that actuate the soft robot with an optimal motion from the SOFA environment to a real-world system based on virtual and real eye-in-hand vision.</p>
<p>Fig. 3 .
3
Fig. 3. (a) CAD schematic of the soft robot system in this work.A total of six cable joints are enabled by the motors' rotation that is transmitted by the flexible shafts.(b) Outer diameter of the soft robot is 6.2 mm.Notation of frames: {B}: base; {M }: middle; {T }: tip; and {C}: camera.</p>
<p>Fig. 4 .
4
Fig. 4. Work predicated on encountering a uvula with a Class-I visibility in the Mallampati system and the vocal cords with a Grade-I visibility in the Cormack-Lehane system.</p>
<p>Fig. 5 .
5
Fig. 5. Entities modeling in SOFA.(a) Meshed modified oral cavity.(b) Meshed pharynx and trachea.Adapted from [37] and [38] under CC BY-NC-SA license.(c) Cables' geometric constraints (exploded view) of the meshed two-segment cable-driven robot.</p>
<p>Fig. 6 .Algorithm 1 :
61
Fig. 6.Simulation snapshots of the collision avoidance motion planning of our soft robot in different SOFA scenes using the built-in QP solver and the local minimum distance-based proximity method, interacting with (a) a 90°pipe, and (b) phantom in Section IV-A.The respective objective values are given.The default initial objective values are 250.</p>
<p>Fig. 7 .
7
Fig. 7. Performance metrics of the YOLOv5s for anatomical recognition.Early stopping was triggered at epoch 481 as no improvement was observed in the last 100 epochs.Best results observed at epoch 380.</p>
<p>Fig. 8 .
8
Fig. 8. Trained PyTorch model (YOLOv5s) can effectively recognize and track single or multiple anatomical features (uvula, epiglottis, and glottis) in simulation and real environments.The aforementioned examples were fed to the model with confidences = 0.7.</p>
<p>Fig. 11 .
11
Fig. 11.With variable targets, 20 groups of feature sequences obtained by the SOFA's eye-in-hand vision were utilized for the NARX training.</p>
<p>Fig. 12 .
12
Fig. 12. Experiment setup and the open-loop control tests.The robot joint space motions are computed by the QP solver of the SOFA framework.The tip positions are captured by the magnetometer array.</p>
<p>Fig. 13 .
13
Fig. 13.Closed-loop NARX-forecasted joint space motions based on only the virtual eye-in-hand vision, compared with the ideal SOFAgenerated joint space motion.Three examples are given in (a)-(c), with variant target positions of [0, −55, 155] , [3, −55, 155] , and [0, −58, 155] mm, respectively.The resultant observed features are shown in the right column.</p>
<p>Fig. 14 .
14
Fig. 14.Joint space motions predicted by (a) LSTM and (b) open-loop NARX, given a target position of [0, −55, 155] mm.The former demonstrates an R 2 coefficient of 0.983 and an MSE of 0.4419, while the latter shows an R 2 coefficient of 0.997 and an MSE of 0.3820, respectively.</p>
<p>Fig. 15 .
15
Fig. 15.(a)-(f) Snapshots of the robot motion generated by the proposed sim-to-real method.The joint space motions were automatically computed based on the endoscopic vision and the closed-loop NARX algorithm.(g) Weak robot-patient registration.</p>
<p>Fig. 16 .
16
Fig. 16.Snapshots from the eye-in-hand vision of two experiments with different initial settings of the relative positions between the robot and phantom and their corresponding recognized features (0: None; 1: Glottis; 2: Uvula; 3: Epiglottis; 4: Glottis and Uvula; and 5: Glottis and Epiglottis).</p>
<p>Fig. 17 .
17
Fig. 17.Measured NARX-forecasted tip motion for (a) Experiment 1 and (b) Experiment 2 of Fig. 16.Numbered tags: 1) Avoid colliding with the uvula; 2) first major bending at the oropharynx; and 3) second major bending, aligning to the glottis instead of the esophagus.</p>
<p>Fig. 18 .
18
Fig. 18.Recorded NARX-forecasted joint space motion for (a) Experiment 1 and (b) Experiment 2 of Fig. 16.</p>
<p>Fig. 19 .
19
Fig. 19.Recognizing real-world anatomical features from open accessible clips [51] using a model trained with mostly (i.e., 92.39%) simulation and phantom data.</p>
<p>TABLE I SIZE
I
OF THE BLENDED DATASET FOR FEATURE RECOGNITION (UNIT: FRAME)</p>
<p>TABLE III R
III
-SQUARED FITNESS FOR FIG.</p>
<p>TABLE IV SIM
IV
AND REAL PERFORMANCE-SUCCESS RATE OF REACHING THE UPPER GLOTTIS</p>
<p>Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.
ACKNOWLEDGMENTThe authors would like to thank Dr. H. Talbot and Dr. E. Coevoet, who promptly replied to their inquiries on GitHub when they had trouble using SOFA and SOFTROBOTS plugin.This work was supported in part by the Hong Kong Research Grants Council (RGC) Collaborative Research Fund under Grant CRF-C4026-21GF, in part by the Hong Kong Research Grants Council (RGC) Research Impact Fund under Grant RIF-R4020-22, and in part by the Dr. Barbara Kwok Young Postdoctoral Researcher Travel Grants Award.Paper no. TII-23-1094.(Jiewen Lai and Tian-Ao Ren contributed equally to this work.Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.
Biomedical applications of soft robotics. M Cianchetti, C Laschi, A Menciassi, P Dario, Nat. Rev. Mat. 362018</p>
<p>A robotic system with multichannel flexible parallel manipulators for single port access surgery. C Li, X Gu, X Xiao, C M Lim, H Ren, IEEE Trans. Ind. Inform. 153Mar. 2019</p>
<p>Continuum robots for medical applications: A survey. J Burgner-Kahrs, D C Rucker, H Choset, IEEE Trans. Robot. 316Dec. 2015</p>
<p>Design and kinematic modeling of constant curvature continuum robots: A review. R J Webster, B A Jones, Int. J. Robot. Res. 29132010</p>
<p>Statics and dynamics of continuum robots with general tendon routing and external loading. D C Rucker, R J Webster, Iii , IEEE Trans. Robot. 276Dec. 2011</p>
<p>Flexible robot with variable stiffness in transoral surgery. C Li, X Gu, X Xiao, C M Lim, H Ren, IEEE/ASME Trans. Mechatron. 251Feb. 2020</p>
<p>Visual servoing of soft robot manipulator in constrained environments with an adaptive controller. H Wang, B Yang, Y Liu, W Chen, X Liang, R Pfeifer, IEEE/ASME Trans. Mechatron. 221Feb. 2017</p>
<p>Soft robot perception using embedded soft sensors and recurrent neural networks. T G Thuruthel, B Shih, C Laschi, M T Tolley, Sci. Robot. 4262019</p>
<p>Control of elastic soft robots based on real-time finite element method. C Duriez, Proc IEEE Int. Conf. Robot. Autom. 2013</p>
<p>Real-time shape estimation for wire-driven flexible robots with multiple bending sections based on quadratic Bézier curves. S Song, Z Li, M Q .-H. Meng, H Yu, H Ren, IEEE Sensors J. 1511Jul. 2015</p>
<p>Shape detection algorithm for soft manipulator based on fiber bragg gratings. H Wang, R Zhang, W Chen, X Liang, R Pfeifer, IEEE/ASME Trans. Mechatron. 216Dec. 2016</p>
<p>Modal-based kinematics and contact detection of soft robots. Y Chen, 20218Soft Robot</p>
<p>Constrained motion planning of a cable-driven soft robot with compressible curvature modeling. J Lai, B Lu, Q Zhao, H K Chu, IEEE Robot. Autom. Lett. 72Apr. 2022</p>
<p>TMTDyn: A MATLAB package for modeling and control of hybrid rigid-continuum robots based on discretized lumped systems and reduced-order models. S H Sadati, Int. J. Robot. Res. 4012021</p>
<p>Kinematic modeling and characterization of soft parallel robots. X Huang, X Zhu, G Gu, IEEE Trans. Robot. 386Dec. 2022</p>
<p>Dynamic simulation of articulated soft robots. W Huang, X Huang, C Majidi, M K Jawed, Nat. Commun. 1112020</p>
<p>A soft robot that adapts to environments through shape change. D S Shah, Nat. Mach. Intell. 312021</p>
<p>Dynamic simulation of soft multimaterial 3D-printed objects. J Hiller, H Lipson, 20141Soft Robots</p>
<p>On the use of simulation in robotics: Opportunities, challenges, and suggestions for moving forward. H Choi, Proc. Nat. Acad. Sci. Nat. Acad. SciArt2021. e1907856118118</p>
<p>Optimization-based inverse model of soft robots with contact handling. E Coevoet, A Escande, C Duriez, IEEE Robot. Autom. Lett. 23Jul. 2017</p>
<p>Software toolkit for modeling, simulation, and control of soft robots. E Coevoet, Adv. Robot. 31222017</p>
<p>Sim2real learning of obstacle avoidance for robotic manipulators in uncertain environments. T Zhang, K Zhang, J Lin, W.-Y G Louie, H Huang, IEEE Robot. Autom. Lett. 71Jan. 2022</p>
<p>Sim2real transfer for reinforcement learning without dynamics randomization. M Kaspar, J D M Osorio, J Bock, Proc. IEEE/RSJ Int. IEEE/RSJ Int2020</p>
<p>Sim2real for soft robotic fish via differentiable simulation. J Z Zhang, Proc. IEEE/RSJ Int. IEEE/RSJ Int2022</p>
<p>Toward zero-shot sim-to-real transfer learning for pneumatic soft robot 3D proprioceptive sensing. U Yoo, H Zhao, A Altamirano, W Yuan, C Feng, Proc. null2023</p>
<p>Scalable sim-to-real transfer of soft robot designs. S Kriegman, Proc. IEEE 3rd Int. Conf. Soft Robot. IEEE 3rd Int. Conf. Soft Robot2020</p>
<p>Robust navigation of a soft growing robot by exploiting contact with the environment. J D Greer, L H Blumenschein, R Alterovitz, E W Hawkes, A M Okamura, Int. J. Robot. Res. 39142020</p>
<p>Long-term time series prediction with the NARX network: An empirical evaluation. J M P MenezesJr, G A Barreto, Neurocomputing. 71162008</p>
<p>A new video intubating device: Trachway intubating stylet. J Ong, Anaesthesia. 64102009</p>
<p>REALITI: A robotic endoscope automated via laryngeal imaging for tracheal intubation. Q Boehler, IEEE Trans. Med. Robot. Bionics. 22May 2020</p>
<p>Surgical anatomy of the trachea. P W Furlow, D J Mathisen, Ann. Cardiothorac. Surg. 722552018</p>
<p>A clinical sign to predict difficult tracheal intubation; a prospective study. S R Mallampati, Can. Anaesth. Soc. J. 321985</p>
<p>Difficult tracheal intubation in obstetrics. R Cormack, J Lehane, Anaesthesia. 39111984</p>
<p>FDA: Fourier domain adaptation for semantic segmentation. Y Yang, S Soatto, Proc. IEEE/CVF Conf. Compt. Vis. Pattern Recognit. IEEE/CVF Conf. Compt. Vis. Pattern Recognit2020</p>
<p>Arbitrary style transfer in real-time with adaptive instance normalization. X Huang, S Belongie, Proc. IEEE Int. Conf. Compt. Vis. IEEE Int. Conf. Compt. Vis2017</p>
<p>Domain adaptive simto-real segmentation of oropharyngeal organs. G Wang, T.-A Ren, J Lai, L Bai, H Ren, Med. Biol. Eng. Comput. 2023</p>
<p>. Photorealistic Human, Cubebrush Mouth, Aug. 1, 2022</p>
<p>You only look once: Unified, real-time object detection. J Redmon, S Divvala, R Girshick, A Farhadi, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern Recognit2016</p>
<p>Learning long-term dependencies in NARX recurrent neural networks. T Lin, B G Horne, P Tino, C L Giles, IEEE Trans. Neural Netw. 76Nov. 1996</p>
<p>Bayesian interpolation. D J Mackay, Neural Comput. 431992</p>
<p>Variable-stiffness control of a dual-segment soft robot using depth vision. J Lai, B Lu, H K Chu, IEEE/ASME Trans. Mechatron. 272Apr. 2022</p>
<p>Stiffening in soft robotics: A review of the state of the art. M Manti, V Cacucciolo, M Cianchetti, IEEE Robot. Autom. Mag. 233Sep. 2016</p>
<p>Groundwater level forecasting with artificial neural networks: A comparison of long short-term memory (LSTM), convolutional neural networks (CNNs), and non-linear autoregressive networks with exogenous input (NARX). A Wunsch, T Liesch, S Broda, Hydrol. Earth Syst. Sci. 2532021</p>
<p>Nerf2real: Sim2real transfer of vision-guided bipedal motion skills using neural radiance fields. A Byravan, Proc. null2023</p>
<p>Visual servoing of a cable-driven soft robot manipulator with shape feature. F Xu, H Wang, W Chen, Y Miao, IEEE Robot. Autom. Lett. 63Jul. 2021</p>
<p>Toward vision-based adaptive configuring of a bidirectional two-segment soft continuum manipulator. J Lai, K Huang, B Lu, H K Chu, Proc. IEEE/ASME Int. Conf. IEEE/ASME Int. Conf2020</p>
<p>Safety-enhanced model-free visual servoing for continuum tubular robots through singularity avoidance in confined environments. K Wu, IEEE Access. 72019</p>
<p>Cook Children's Health Care System. Apr. 18, 2023Laryngoscopy Education Video</p>
<p>Adult AirTraq intubation with reinforced endotracheal tube. Openairway, Apr. 18, 2023</p>
<p>Endotracheal intubation. Interanest, Apr. 18, 2023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4885 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4885</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4885</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-104.html">extraction-schema-104</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <p><strong>Paper ID:</strong> paper-f3caa43a7016fbbf309d45112b31b20230eaf8da</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f3caa43a7016fbbf309d45112b31b20230eaf8da" target="_blank">Playing Text-Adventure Games with Graph-Based Deep Reinforcement Learning</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> A deep reinforcement learning architecture that represents the game state as a knowledge graph which is learned during exploration that is used to prune the action space, enabling more efficient exploration.</p>
                <p><strong>Paper Abstract:</strong> Text-based adventure games provide a platform on which to explore reinforcement learning in the context of a combinatorial action space, such as natural language. We present a deep reinforcement learning architecture that represents the game state as a knowledge graph which is learned during exploration. This graph is used to prune the action space, enabling more efficient exploration. The question of which action to take can be reduced to a question-answering task, a form of transfer learning that pre-trains certain parts of our architecture. In experiments using the TextWorld framework, we show that our proposed technique can learn a control policy faster than baseline alternatives. We have also open-sourced our code at https://github.com/rajammanabrolu/KG-DQN.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4885.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4885.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Graph Deep Q-Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deep reinforcement learning agent that constructs and maintains a knowledge graph as a persistent state representation to prune action spaces and estimate Q-values using graph attention and an LSTM observation encoder; portions of the network are pre-trained via question-answering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Deep Q-learning agent that (1) extracts RDF triples from textual observations (OpenIE + domain rules) to build a knowledge graph, (2) embeds the graph via multi-head graph attention to produce a persistent state vector, (3) encodes observations with a Sliding Bidirectional LSTM, (4) encodes candidate action texts with an LSTM, and (5) computes Q(s,a) as the dot product of state and action embeddings. Parts of the encoder are pre-trained with a QA model (DrQA).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>TextWorld (home theme)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Play generated text-adventure quests (small: 10 rooms, quest length 5; large: 20 rooms, quest length 10) to maximize reward by taking natural-language commands; evaluate training convergence and steps-to-complete.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>External structured persistent memory (knowledge graph / RDF triple store) maintained across timesteps; short-term relations around the 'you' node plus longer-term room/object relations.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Structured RDF triples (subject, relation, object) representing rooms, exits, object locations, agent inventory and relations inferred from observations; node features are averaged word embeddings of node tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Graph is updated after every action: triples extracted from observation via OpenIE + authored rules; relations out of 'you' node are refreshed each action (except inventory), room-linking and exit relations persist; moving actions add directional room links.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Graph embedding via multi-head Graph Attention Network (self-attention over node neighborhoods up to third-order neighbors); attention weights determine which graph portions influence the state vector used for Q-value estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>KG-DQN variants (all include the knowledge graph) converge faster: on small games KG-DQN converges ~40% faster than the best baseline; step-to-complete for Full KG-DQN (pruned + pre-trained) on small game = 73.7 ± 8.5 steps (Table 3) vs LSTM-DQN = 72.4 ± 4.6. On large games Full KG-DQN = 265.9 ± 9.4 steps vs LSTM-DQN = 260.3 ± 4.5 (Table 4). KG-DQN reaches maximum small-game reward faster and converges to higher reward more quickly on large games than baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Ablations examine: (1) Un-pruned actions + pre-training (still uses graph but doesn't prune) — results worse (small: 131.7 ± 7.7 steps); (2) Pruned actions without pre-training — intermediate (small: 97.3 ± 9.0 steps; large: 340 ± 6.4); (3) Pruned actions with pre-training (Full KG-DQN) — best (small: 73.7 ± 8.5, large: 265.9 ± 9.4). Authors note that all KG-DQN variants (which share the knowledge graph) converge faster than baselines, attributing faster convergence primarily to the persistent graph memory. They also compare to LSTM-DQN and BOW-DQN baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>OpenIE extraction is noisy/not optimized for game text, requiring hand-authored rules to fill gaps; action pruning based on graph may eliminate actions necessary to complete quests (mitigated via epsilon2 exploration); persistent graph speeds convergence but does not guarantee higher-quality or shortest quest solutions by itself; QA pre-training transfers but cannot replace exploration/trial-and-error.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Maintain an explicit structured persistent memory (knowledge graph) to prune action spaces and speed RL convergence; combine graph attention for focused retrieval with pre-training (QA) to improve action selection and shorten solution sequences; include exploration safeguards (e.g., epsilon2) because pruning can remove necessary novel actions; hand-crafted extraction rules or improved IE tuned to game text are helpful to improve graph quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Playing Text-Adventure Games with Graph-Based Deep Reinforcement Learning', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4885.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4885.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSTM-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LSTM Deep Q-Network (Narasimhan et al. 2015 baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline deep Q-network that encodes observations with an LSTM and scores verb and object arguments to choose actions; uses an LSTM internal state to capture history but is not explicitly conditioned on a persistent external memory structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language Understanding for Textbased Games Using Deep Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LSTM-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Deep reinforcement learning baseline which encodes the textual observation using an LSTM to produce a state representation; separate scoring networks output verb and object argument scores which are combined to yield Q-values for actions. Memory of prior context is implicitly stored in the LSTM hidden/cell state.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>TextWorld (as baseline comparison in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Play the same TextWorld quests as other agents to learn control policies mapping textual observations to natural-language actions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Implicit RNN internal memory (LSTM hidden and cell states); no explicit external persistent memory structure.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Dense hidden/cell state vectors of the LSTM representing summarized recent observation history.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>LSTM state updated after each observation/action timestep via recurrent updates.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Implicit: downstream layers use the current LSTM hidden state; there is no explicit retrieval or attention over a stored memory graph.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported in paper as baseline performance: small game steps-to-complete = 72.4 ± 4.6 (Table 3); large game = 260.3 ± 4.5 (Table 4). Converges slower than KG-DQN on large games (LSTM-DQN required >300 episodes to converge to KG-DQN performance on large set).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Authors contrast LSTM-DQN (no external KG) with KG-DQN and conclude that LSTM internal memory captures some action-selection knowledge but is less effective for faster convergence in partially observable, combinatorial-action settings; no direct ablation of LSTM internal state vs. no internal state is performed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Implicit LSTM memory makes long-term, explicit, retrievable facts harder to access or reason over; slower convergence in larger environments lacking an explicit persistent structured memory.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>For partially observable text games, combine sequence encoders (LSTM) with an explicit structured persistent memory (knowledge graph) for faster convergence and better action pruning; use LSTM internal state for local/short-term context while representing longer-term facts in an external graph.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Playing Text-Adventure Games with Graph-Based Deep Reinforcement Learning', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4885.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4885.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DRRN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Reinforcement Relevance Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior model that encodes states and natural language actions separately and computes pairwise interactions to score state-action pairs, but is not conditioned on previous observations (no persistent memory).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep Reinforcement Learning with a Natural Language Action Space</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DRRN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Model that uses two separate neural encoders for state observation and each candidate action and computes interaction scores to estimate Q-values; used as related work baseline illustrating approaches that do not use persistent memory.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Referenced in related work for text-based games (not directly evaluated in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Handle combinatorial natural-language action spaces in text-adventure games by encoding state and actions independently and computing relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper notes DRRN is not conditioned on previous observations and thus lacks persistent memory, which authors argue is a disadvantage in partially observable games.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Lacks conditioning on previous observations (no persistent memory), leading to difficulties in complex partially observable environments and slower or inefficient exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Authors suggest that conditioning on previous observations (i.e., persistent memory) and pruning the action space can improve exploration and convergence compared to architectures like DRRN.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Playing Text-Adventure Games with Graph-Based Deep Reinforcement Learning', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4885.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4885.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AEN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Action Eliminating Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that learns to eliminate unlikely actions per state (top-k selection) using emulator feedback to restrict the action set, improving exploration efficiency though not conditioned on previous observations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning How Not to Act in Text-Based Games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Action Eliminating Network (AEN)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Network that ranks/eliminates actions in a given state using feedback to restrict the action set to top-k candidates, reducing exploration over the combinatorial action space; does not use persistent memory across timesteps.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Referenced as related work for text-based games</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Reduce the action space each timestep to the most likely actions to allow more effective exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Referenced as showing action-space reduction helps exploration; authors note it also is not conditioned on previous observations (no persistent memory).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Not conditioned on previous observations; thus may miss leveraging long-term context; relies on emulator feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Action-space reduction helps exploration, but combining it with persistent memory may yield additional benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Playing Text-Adventure Games with Graph-Based Deep Reinforcement Learning', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4885.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4885.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DrQA (pre-training)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DrQA (Reading Wikipedia to answer open-domain questions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A question-answering architecture used by the authors to pre-train the SB-LSTM observation encoder and embedding layers to transfer QA competence from other generated games to new test games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reading Wikipedia to answer open-domain questions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DrQA (as pre-training model)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A QA system (document encoder + answer encoder) trained on oracle walkthrough traces from similar but different TextWorld games; weights from the DrQA document encoder initialize the SB-LSTM and embedding layers of KG-DQN to provide transferred representations.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Not an evaluated gameplay agent here; used for pre-training on TextWorld walkthrough traces</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Predict the oracle action given a text observation (treated as question-answer pairs) to pre-train parts of KG-DQN.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Pre-training with DrQA improved KG-DQN's ability to find shorter action sequences and yielded better steps-to-complete (full KG-DQN outperformed non-pre-trained pruned variant). Pre-training QA exact-match scores reported (Table 2): Small EM=46.20, Large EM=34.13.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>QA pre-training helps but cannot replace exploration; transfer requires games in same domain but varied configurations so the model must generalize rather than memorize.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Use QA-formulated pre-training (oracle walkthrough traces) to transfer general action-selection knowledge into the RL agent; initialize embedding and encoder weights from QA models (e.g., DrQA with GloVe initialization).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Playing Text-Adventure Games with Graph-Based Deep Reinforcement Learning', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language Understanding for Textbased Games Using Deep Reinforcement Learning <em>(Rating: 2)</em></li>
                <li>Deep Reinforcement Learning with a Natural Language Action Space <em>(Rating: 2)</em></li>
                <li>Learning How Not to Act in Text-Based Games <em>(Rating: 2)</em></li>
                <li>Reading Wikipedia to answer open-domain questions <em>(Rating: 1)</em></li>
                <li>TextWorld : A Learning Environment for Text-based Games <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4885",
    "paper_id": "paper-f3caa43a7016fbbf309d45112b31b20230eaf8da",
    "extraction_schema_id": "extraction-schema-104",
    "extracted_data": [
        {
            "name_short": "KG-DQN",
            "name_full": "Knowledge Graph Deep Q-Network",
            "brief_description": "A deep reinforcement learning agent that constructs and maintains a knowledge graph as a persistent state representation to prune action spaces and estimate Q-values using graph attention and an LSTM observation encoder; portions of the network are pre-trained via question-answering.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "KG-DQN",
            "agent_description": "Deep Q-learning agent that (1) extracts RDF triples from textual observations (OpenIE + domain rules) to build a knowledge graph, (2) embeds the graph via multi-head graph attention to produce a persistent state vector, (3) encodes observations with a Sliding Bidirectional LSTM, (4) encodes candidate action texts with an LSTM, and (5) computes Q(s,a) as the dot product of state and action embeddings. Parts of the encoder are pre-trained with a QA model (DrQA).",
            "llm_model_name": null,
            "game_or_benchmark_name": "TextWorld (home theme)",
            "task_description": "Play generated text-adventure quests (small: 10 rooms, quest length 5; large: 20 rooms, quest length 10) to maximize reward by taking natural-language commands; evaluate training convergence and steps-to-complete.",
            "memory_used": true,
            "memory_type": "External structured persistent memory (knowledge graph / RDF triple store) maintained across timesteps; short-term relations around the 'you' node plus longer-term room/object relations.",
            "memory_representation": "Structured RDF triples (subject, relation, object) representing rooms, exits, object locations, agent inventory and relations inferred from observations; node features are averaged word embeddings of node tokens.",
            "memory_update_mechanism": "Graph is updated after every action: triples extracted from observation via OpenIE + authored rules; relations out of 'you' node are refreshed each action (except inventory), room-linking and exit relations persist; moving actions add directional room links.",
            "memory_retrieval_mechanism": "Graph embedding via multi-head Graph Attention Network (self-attention over node neighborhoods up to third-order neighbors); attention weights determine which graph portions influence the state vector used for Q-value estimation.",
            "performance_with_memory": "KG-DQN variants (all include the knowledge graph) converge faster: on small games KG-DQN converges ~40% faster than the best baseline; step-to-complete for Full KG-DQN (pruned + pre-trained) on small game = 73.7 ± 8.5 steps (Table 3) vs LSTM-DQN = 72.4 ± 4.6. On large games Full KG-DQN = 265.9 ± 9.4 steps vs LSTM-DQN = 260.3 ± 4.5 (Table 4). KG-DQN reaches maximum small-game reward faster and converges to higher reward more quickly on large games than baselines.",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "ablation_or_analysis": "Ablations examine: (1) Un-pruned actions + pre-training (still uses graph but doesn't prune) — results worse (small: 131.7 ± 7.7 steps); (2) Pruned actions without pre-training — intermediate (small: 97.3 ± 9.0 steps; large: 340 ± 6.4); (3) Pruned actions with pre-training (Full KG-DQN) — best (small: 73.7 ± 8.5, large: 265.9 ± 9.4). Authors note that all KG-DQN variants (which share the knowledge graph) converge faster than baselines, attributing faster convergence primarily to the persistent graph memory. They also compare to LSTM-DQN and BOW-DQN baselines.",
            "challenges_or_limitations": "OpenIE extraction is noisy/not optimized for game text, requiring hand-authored rules to fill gaps; action pruning based on graph may eliminate actions necessary to complete quests (mitigated via epsilon2 exploration); persistent graph speeds convergence but does not guarantee higher-quality or shortest quest solutions by itself; QA pre-training transfers but cannot replace exploration/trial-and-error.",
            "best_practices_or_recommendations": "Maintain an explicit structured persistent memory (knowledge graph) to prune action spaces and speed RL convergence; combine graph attention for focused retrieval with pre-training (QA) to improve action selection and shorten solution sequences; include exploration safeguards (e.g., epsilon2) because pruning can remove necessary novel actions; hand-crafted extraction rules or improved IE tuned to game text are helpful to improve graph quality.",
            "uuid": "e4885.0",
            "source_info": {
                "paper_title": "Playing Text-Adventure Games with Graph-Based Deep Reinforcement Learning",
                "publication_date_yy_mm": "2018-12"
            }
        },
        {
            "name_short": "LSTM-DQN",
            "name_full": "LSTM Deep Q-Network (Narasimhan et al. 2015 baseline)",
            "brief_description": "Baseline deep Q-network that encodes observations with an LSTM and scores verb and object arguments to choose actions; uses an LSTM internal state to capture history but is not explicitly conditioned on a persistent external memory structure.",
            "citation_title": "Language Understanding for Textbased Games Using Deep Reinforcement Learning",
            "mention_or_use": "use",
            "agent_name": "LSTM-DQN",
            "agent_description": "Deep reinforcement learning baseline which encodes the textual observation using an LSTM to produce a state representation; separate scoring networks output verb and object argument scores which are combined to yield Q-values for actions. Memory of prior context is implicitly stored in the LSTM hidden/cell state.",
            "llm_model_name": null,
            "game_or_benchmark_name": "TextWorld (as baseline comparison in this paper)",
            "task_description": "Play the same TextWorld quests as other agents to learn control policies mapping textual observations to natural-language actions.",
            "memory_used": true,
            "memory_type": "Implicit RNN internal memory (LSTM hidden and cell states); no explicit external persistent memory structure.",
            "memory_representation": "Dense hidden/cell state vectors of the LSTM representing summarized recent observation history.",
            "memory_update_mechanism": "LSTM state updated after each observation/action timestep via recurrent updates.",
            "memory_retrieval_mechanism": "Implicit: downstream layers use the current LSTM hidden state; there is no explicit retrieval or attention over a stored memory graph.",
            "performance_with_memory": "Reported in paper as baseline performance: small game steps-to-complete = 72.4 ± 4.6 (Table 3); large game = 260.3 ± 4.5 (Table 4). Converges slower than KG-DQN on large games (LSTM-DQN required &gt;300 episodes to converge to KG-DQN performance on large set).",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "ablation_or_analysis": "Authors contrast LSTM-DQN (no external KG) with KG-DQN and conclude that LSTM internal memory captures some action-selection knowledge but is less effective for faster convergence in partially observable, combinatorial-action settings; no direct ablation of LSTM internal state vs. no internal state is performed in this paper.",
            "challenges_or_limitations": "Implicit LSTM memory makes long-term, explicit, retrievable facts harder to access or reason over; slower convergence in larger environments lacking an explicit persistent structured memory.",
            "best_practices_or_recommendations": "For partially observable text games, combine sequence encoders (LSTM) with an explicit structured persistent memory (knowledge graph) for faster convergence and better action pruning; use LSTM internal state for local/short-term context while representing longer-term facts in an external graph.",
            "uuid": "e4885.1",
            "source_info": {
                "paper_title": "Playing Text-Adventure Games with Graph-Based Deep Reinforcement Learning",
                "publication_date_yy_mm": "2018-12"
            }
        },
        {
            "name_short": "DRRN",
            "name_full": "Deep Reinforcement Relevance Network",
            "brief_description": "A prior model that encodes states and natural language actions separately and computes pairwise interactions to score state-action pairs, but is not conditioned on previous observations (no persistent memory).",
            "citation_title": "Deep Reinforcement Learning with a Natural Language Action Space",
            "mention_or_use": "mention",
            "agent_name": "DRRN",
            "agent_description": "Model that uses two separate neural encoders for state observation and each candidate action and computes interaction scores to estimate Q-values; used as related work baseline illustrating approaches that do not use persistent memory.",
            "llm_model_name": null,
            "game_or_benchmark_name": "Referenced in related work for text-based games (not directly evaluated in this paper)",
            "task_description": "Handle combinatorial natural-language action spaces in text-adventure games by encoding state and actions independently and computing relevance.",
            "memory_used": false,
            "memory_type": null,
            "memory_representation": null,
            "memory_update_mechanism": null,
            "memory_retrieval_mechanism": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "ablation_or_analysis": "Paper notes DRRN is not conditioned on previous observations and thus lacks persistent memory, which authors argue is a disadvantage in partially observable games.",
            "challenges_or_limitations": "Lacks conditioning on previous observations (no persistent memory), leading to difficulties in complex partially observable environments and slower or inefficient exploration.",
            "best_practices_or_recommendations": "Authors suggest that conditioning on previous observations (i.e., persistent memory) and pruning the action space can improve exploration and convergence compared to architectures like DRRN.",
            "uuid": "e4885.2",
            "source_info": {
                "paper_title": "Playing Text-Adventure Games with Graph-Based Deep Reinforcement Learning",
                "publication_date_yy_mm": "2018-12"
            }
        },
        {
            "name_short": "AEN",
            "name_full": "Action Eliminating Network",
            "brief_description": "A method that learns to eliminate unlikely actions per state (top-k selection) using emulator feedback to restrict the action set, improving exploration efficiency though not conditioned on previous observations.",
            "citation_title": "Learning How Not to Act in Text-Based Games",
            "mention_or_use": "mention",
            "agent_name": "Action Eliminating Network (AEN)",
            "agent_description": "Network that ranks/eliminates actions in a given state using feedback to restrict the action set to top-k candidates, reducing exploration over the combinatorial action space; does not use persistent memory across timesteps.",
            "llm_model_name": null,
            "game_or_benchmark_name": "Referenced as related work for text-based games",
            "task_description": "Reduce the action space each timestep to the most likely actions to allow more effective exploration.",
            "memory_used": false,
            "memory_type": null,
            "memory_representation": null,
            "memory_update_mechanism": null,
            "memory_retrieval_mechanism": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "ablation_or_analysis": "Referenced as showing action-space reduction helps exploration; authors note it also is not conditioned on previous observations (no persistent memory).",
            "challenges_or_limitations": "Not conditioned on previous observations; thus may miss leveraging long-term context; relies on emulator feedback.",
            "best_practices_or_recommendations": "Action-space reduction helps exploration, but combining it with persistent memory may yield additional benefits.",
            "uuid": "e4885.3",
            "source_info": {
                "paper_title": "Playing Text-Adventure Games with Graph-Based Deep Reinforcement Learning",
                "publication_date_yy_mm": "2018-12"
            }
        },
        {
            "name_short": "DrQA (pre-training)",
            "name_full": "DrQA (Reading Wikipedia to answer open-domain questions)",
            "brief_description": "A question-answering architecture used by the authors to pre-train the SB-LSTM observation encoder and embedding layers to transfer QA competence from other generated games to new test games.",
            "citation_title": "Reading Wikipedia to answer open-domain questions",
            "mention_or_use": "use",
            "agent_name": "DrQA (as pre-training model)",
            "agent_description": "A QA system (document encoder + answer encoder) trained on oracle walkthrough traces from similar but different TextWorld games; weights from the DrQA document encoder initialize the SB-LSTM and embedding layers of KG-DQN to provide transferred representations.",
            "llm_model_name": null,
            "game_or_benchmark_name": "Not an evaluated gameplay agent here; used for pre-training on TextWorld walkthrough traces",
            "task_description": "Predict the oracle action given a text observation (treated as question-answer pairs) to pre-train parts of KG-DQN.",
            "memory_used": false,
            "memory_type": null,
            "memory_representation": null,
            "memory_update_mechanism": null,
            "memory_retrieval_mechanism": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "ablation_or_analysis": "Pre-training with DrQA improved KG-DQN's ability to find shorter action sequences and yielded better steps-to-complete (full KG-DQN outperformed non-pre-trained pruned variant). Pre-training QA exact-match scores reported (Table 2): Small EM=46.20, Large EM=34.13.",
            "challenges_or_limitations": "QA pre-training helps but cannot replace exploration; transfer requires games in same domain but varied configurations so the model must generalize rather than memorize.",
            "best_practices_or_recommendations": "Use QA-formulated pre-training (oracle walkthrough traces) to transfer general action-selection knowledge into the RL agent; initialize embedding and encoder weights from QA models (e.g., DrQA with GloVe initialization).",
            "uuid": "e4885.4",
            "source_info": {
                "paper_title": "Playing Text-Adventure Games with Graph-Based Deep Reinforcement Learning",
                "publication_date_yy_mm": "2018-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language Understanding for Textbased Games Using Deep Reinforcement Learning",
            "rating": 2
        },
        {
            "paper_title": "Deep Reinforcement Learning with a Natural Language Action Space",
            "rating": 2
        },
        {
            "paper_title": "Learning How Not to Act in Text-Based Games",
            "rating": 2
        },
        {
            "paper_title": "Reading Wikipedia to answer open-domain questions",
            "rating": 1
        },
        {
            "paper_title": "TextWorld : A Learning Environment for Text-based Games",
            "rating": 2
        }
    ],
    "cost": 0.0122435,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Playing Text-Adventure Games with Graph-Based Deep Reinforcement Learning</h1>
<p>Prithviraj Ammanabrolu<br>School of Interactive Computing<br>Georgia Institute of Technology<br>Atlanta, GA<br>raj.ammanabrolu@gatech.edu</p>
<p>Mark O. Riedl<br>School of Interactive Computing<br>Georgia Institute of Technology<br>Atlanta, GA<br>riedl@cc.gatech.edu</p>
<h4>Abstract</h4>
<p>Text-based adventure games provide a platform on which to explore reinforcement learning in the context of a combinatorial action space, such as natural language. We present a deep reinforcement learning architecture that represents the game state as a knowledge graph which is learned during exploration. This graph is used to prune the action space, enabling more efficient exploration. The question of which action to take can be reduced to a question-answering task, a form of transfer learning that pre-trains certain parts of our architecture. In experiments using the TextWorld framework, we show that our proposed technique can learn a control policy faster than baseline alternatives. We have also open-sourced our code at https://github.com/rajammanabrolu/KGDQN.</p>
<h2>1 Introduction</h2>
<p>Natural language communication can be used to affect change in the real world. Text adventure games, in which players must make sense of the world through text descriptions and declare actions through natural language, can provide a stepping stone toward more real-world environments where agents must communicate to understand the state of the world and indirectly affect change in the world. Text adventure games are also useful for developing and testing reinforcement learning algorithms that must deal with the partial observability of the world (Narasimhan et al., 2015; He et al., 2016).</p>
<p>In text adventure games, the agent receives an incomplete textual description of the current state of the world. From this information, and previous interactions with the world, a player must determine the next best action to take to achieve some quest or goal. The player must then com-
pose a textual description of the action they intend to make and receive textual feedback of the effects of the action. Formally, a text-based game is a partially observable Markov decision process (POMDP), represented as a 7-tuple of $\langle S, T, A, \Omega, O, R, \gamma\rangle$ representing the set of environment states, conditional transition probabilities between states, words used to compose text commands, observations, observation conditional probabilities, reward function, and the discount factor respectively (Côté et al., 2018).</p>
<p>In text-based games, the agent never has access to the true underlying world state and has to reason about how to act in the world based only on the textual observations. Additionally, the agent's actions must be expressed through natural language commands, ensuring that the action space is combinatorially large. Thus, text-based games pose a different set of challenges than traditional video games. Text-based games require a greater understanding of previous context to be able to explore the state-action space more effectively. Such games have historically proven to be difficult to play for AI agents, and the more complex variants such as Zork still remain firmly out of the reach of existing approaches.</p>
<p>We introduce three contributions to text-based game playing to deal with the combinatorially large state and action spaces. First, we show that a state representation in the form of a knowledge graph gives us the ability to effectively prune an action space. A knowledge graph captures the relationships between entities as a directed graph. The knowledge graph provides a persistent memory of the world over time and enables the agent to have a prior notion of what actions it should not take at a particular stage of the game.</p>
<p>Our second contribution is a deep reinforcement learning architecture, Knowledge Graph DQN (KG-DQN), that effectively uses this state rep-</p>
<p>resentation to estimate the $Q$-value for a stateaction pair. This architecture leverages recent advances in graph embedding and attention techniques (Guan et al., 2018; Veličković et al., 2018) to learn which portions of the graph to pay attention to given an input state description in addition to having a mechanism that allows for natural language action inputs. Finally, we take initial steps toward framing the POMDP as a questionanswering (QA) problem wherein a knowledgegraph can be used to not only prune actions but to answer the question of what action is most appropriate. Previous work has shown that many NLP tasks can be framed as instances of questionanswering and that we can transfer knowledge between these tasks (McCann et al., 2017). We show how pre-training certain parts of our KG-DQN network using existing QA methods improves performance and allows knowledge to be transferred from different games.</p>
<p>We provide results on ablative experiments comparing our knowledge-graph based approach approaches to strong baselines. Results show that incorporating a knowledge-graph into a reinforcement learning agent results in converges to the highest reward more than $40 \%$ faster than the best baseline. With pre-training using a questionanswering paradigm, we achieve this fast convergence rate while also achieving high quality quest solutions as measured by the number of steps required to complete the quests.</p>
<h2>2 Related Work</h2>
<p>A growing body of research has explored the challenges associated with text-based games (Bordes et al., 2010; Narasimhan et al., 2015; He et al., 2016; Fulda et al., 2017; Haroush et al., 2018; Côté et al., 2018; Tao et al., 2018). Narasimhan et al. (2015) attempts to solve parser-based text games by encoding the observations using an LSTM. This encoding vector is then used by an action scoring network that determines the scores for the action verb and each of the corresponding argument objects. The two scores are then averaged to determine $Q$-value for the state-action pair. He et al. (2016) present the Deep Reinforcement Relevance Network (DRRN) which uses two separate deep neural networks to encode the state and actions. The $Q$-value for a state-action pair is then computed by a pairwise interaction function between the two encoded representations. Both of
these methods are not conditioned on previous observations and so are at a disadvantage when dealing with complex partially observable games. Additionally, neither of these approaches prune the action space and so end up wasting trials exploring state-action pairs that are likely to have low $Q$ values, likely leading to slower convergence times for combinatorially large action spaces.</p>
<p>Haroush et al. (2018) introduce the Action Eliminating Network (AEN) that attempts to restrict the actions in each state to the top- $k$ most likely ones, using the emulator's feedback. The network learns which actions should not be taken given a particular state. Their work shows that reducing the size of the action space allows for more effective exploration, leading to better performance. Their network is also not conditioned on previous observations.</p>
<p>Knowledge graphs have been demonstrated to improve natural language understanding in other domains outside of text adventure games. For example, Guan et al. (2018) use commonsense knowledge graphs such as ConceptNet (Speer and Havasi, 2012) to significantly improve the ability of neural networks to predict the end of a story. They represent the graph in terms of a knowledge context vector using features from ConceptNet and graph attention (Veličković et al., 2018). The state representation that we have chosen as well as our method of action pruning builds on the strengths of existing approaches while simultaneously avoiding the shortcomings of ineffective exploration and lack of long-term context.</p>
<h2>3 Knowledge Graph DQN</h2>
<p>In this section we introduce our knowledge graph representation, action pruning and deep $Q$ network architecture.</p>
<h3>3.1 Knowledge Graph Representation</h3>
<p>In our approach, our agent learns a knowledge graph, stored as a set of RDF triples, i.e. 3-tuples of $\langle$ subject, relation, object $\rangle$. These triples are extracted from the observations using Stanford's Open Information Extraction (OpenIE) (Angeli et al., 2015). OpenIE is not optimized to the regularities of text adventure games and there are a lot of relations that can be inferred from the typical structure of descriptive texts. For example, from a phrase such as "There is an exit to the north" one can infer a has relation between the current</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Graph state update example given two observations
location and the direction of the exit. These additional rules fill in the information not provided by OpenIE. The resultant knowledge graph gives the agent what essentially amounts to a mental map of the game world.</p>
<p>The knowledge graph is updated after every agent action (see Figure 1). The update rules are defined such that there are portions of the graph offering short and long-term context. A special node-designated "you"-represents the agent and relations out of this node are updated after every action with the exception of relations denoting the agent's inventory. Other relations persist after each action. We intend for the update rules to be applied to text-based games in different domains and so only hand-craft a minimal set of rules that we believe apply generally. They are:</p>
<ul>
<li>Linking the current room type (e.g. "basement", "chamber') to the items found in the room with the relation "has", e.g. $\langle$ chamber, has, bed stand $\rangle$</li>
<li>Extracting information regarding entrances and exits and linking them to the current room, e.g. $\langle$ basement, has, exit to north $\rangle$</li>
<li>Removing all relations relating to the "you" node with the exception of inventory every action, e.g. $\langle$ you, have, cubical key $\rangle$</li>
<li>Linking rooms with directions based on the action taken to move between the rooms, e.g. $\langle$ chamber, east of, basement $\rangle$ after the action "go east" is taken to go from the basement to the chamber</li>
</ul>
<p>All other RDF triples generated are taken from OpenIE.</p>
<h3>3.2 Action Pruning</h3>
<p>The number of actions available to an agent in a text adventure game can be quite large: $A=$ $\mathcal{O}(|V| \times|O|^{2})$ where $V$ is the number of action verbs, and $O$ is the number of distinct objects in the world that the agent can interact with, assuming that verbs can take two arguments. Some actions, such as movement, inspecting inventory, or observing the room, do not have arguments.</p>
<p>The knowledge graph is used to prune the combinatorially large space of possible actions available to the agent as follows. Given the current state graph representation $G_{t}$, the action space is pruned by ranking the full set of actions and selecting the top- $k$. Our action scoring function is:</p>
<ul>
<li>+1 for each object in the action that is present in the graph; and</li>
<li>+1 if there exists a valid directed path between the two objects in the graph.</li>
</ul>
<p>We assume that each action has at most two objects (for example inserting a key in a lock).</p>
<h3>3.3 Model Architecture and Training</h3>
<p>Following Narasimhan et al. (2015), all actions $A$ that will be accepted by the game's parser are available to the agent at all times. When playing the game, the agent chooses an action and receives an observation $o_{t}$ from the simulator, which is a textual description of current game state. The state graph $G_{t}$ is updated according to the given observation, as described in Section 3.1.</p>
<p>We use the $Q$-Learning technique (Watkins and Dayan, 1992) to learn a control policy $\pi\left(a_{t} \mid s_{t}\right)$, $a_{t} \in A$, which gives us the probability of taking action $a_{t}$ given the current state $s_{t}$. The policy is</p>
<p>determined by the $Q$-value of a particular stateaction pair, which is updated using the Bellman equation (Sutton and Barto, 2018):</p>
<p>$$
\begin{aligned}
&amp; Q_{t+1}\left(s_{t+1}, a_{t+1}\right)= \
&amp; \quad E\left[r_{t+1}+\gamma \max <em t="t">{a \in A</em>\right]
\end{aligned}
$$}} Q_{t}(s, a) \mid s_{t}, a_{t</p>
<p>where $\gamma$ refers to the discount factor and $r_{t+1}$ is the observed reward. The policy is thus to take the action that maximizes the $Q$-value in a particular state, which will correspond to the action that maximizes the reward expectation given that the agent has taken action $a_{t}$ at the current state $s_{t}$ and followed the policy $\pi(a \mid s)$ after.</p>
<p>The architecture in Figure 2 is responsible for computing the representations for both the state $s_{t}$ and the actions $a^{(i)} \in A$ and coming to an estimation of the $Q$-value for a particular state and action. During the forward activation, the agent uses the observation to update the graph $G_{t}$ using the rules outlined in Section 3.2.</p>
<p>The graph is then embedded into a single vector $\mathbf{g}<em _mathbf_1="\mathbf{1">{\mathbf{t}}$. We use Graph Attention (Veličković et al., 2018) with an attention mechanism similar to that described in Bahdanau et al. (2014). Formally, the Multi-headed Graph Attention component receives a set of node features $H=$ $\left{\mathbf{h}</em>}}, \mathbf{h<em _mathbf_N="\mathbf{N">{\mathbf{2}}, \ldots, \mathbf{h}</em>}}\right}, \mathbf{h<em t="t">{\mathbf{i}} \in \mathbb{R}^{\mathrm{F}}$, where $N$ is the number of nodes and $F$ the number of features in each node, and the adjacency matrix of $G</em>$ applied to all the node features:}$. Each of the node features consist of the averaged word embeddings for the tokens in that node, as determined by the preceding graph embedding layer. The attention mechanism is set up using self-attention on the nodes after a learnable linear transformation $W \in \mathbb{R}^{2 \mathrm{~F} \times \mathrm{F}</p>
<p>$$
e_{i j}=\operatorname{LeakyReLU}\left(\mathbf{p} \cdot W\left(\mathbf{h}<em _mathbf_j="\mathbf{j">{\mathbf{i}} \oplus \mathbf{h}</em>\right)\right)
$$}</p>
<p>where $\mathbf{p} \in \mathbb{R}^{2 \mathrm{~F}}$ is a learnable parameter. The attention coefficients $\alpha_{i j}$ are then computed by normalizing over the choices of $k \in \mathcal{N}$ using the softmax function. Here $\mathcal{N}$ refers to the neighborhood in which we compute the attention coefficients. This is determined by the adjacency matrix for $G_{t}$ and consists of all third-order neighbors of a particular node.</p>
<p>$$
\alpha_{i j}=\frac{\exp \left(e_{i j}\right)}{\sum_{k \in \mathcal{N}} \exp \left(e_{i k}\right)}
$$</p>
<p>Multi-head attention is then used, calculating multiple independent attention coefficients. The resulting features are then concatenated and passed
into a linear layer to determine $\mathbf{g}_{\mathbf{t}}$ :</p>
<p>$$
\mathbf{g}<em g="g">{\mathbf{t}}=f\left(W</em>\left(\left|<em _in="\in" _mathcal_N="\mathcal{N" j="j">{k=1}^{K} \sigma\left(\sum</em>}} \alpha_{i j}^{(k)} \mathbf{W}^{(k)} \mathbf{h<em g="g">{j}\right)\right)+b</em>\right)\right.
$$</p>
<p>where $k$ refers to the parameters of the $k^{t h}$ independent attention mechanism, $W_{g}$ and $b_{g}$ the weights and biases of this component's output linear layer, and $|$ represents concatenation.</p>
<p>Simultaneously, an encoded representation of the observation $\mathbf{o}<em _mathbf_t="\mathbf{t">{\mathbf{t}}$ is computed using a Sliding Bidirectional LSTM (SB-LSTM). The final state representation $\mathbf{s}</em>$ is computed as:}</p>
<p>$$
\mathbf{s}<em l="l">{\mathbf{t}}=f\left(W</em>}\left(\mathbf{g<em _mathbf_t="\mathbf{t">{\mathbf{t}} \oplus \mathbf{o}</em>\right)
$$}}\right)+b_{l</p>
<p>where $W_{l}, b_{l}$ represent the final linear layer's weights and biases and $\mathbf{o}_{\mathbf{t}}$ is the result of encoding the observation with the SB-LSTM.</p>
<p>The entire set of possible actions $A$ is pruned by scoring each $a \in A$ according to the mechanism previously described using the newly updated $G_{t+1}$. We then embed and encode all of these action strings using an LSTM encoder (Sutskever et al., 2014). The dashed lines in Figure 2 denotes non-differentiable processes.</p>
<p>The final $Q$-value for a state-action pair is:</p>
<p>$$
Q\left(\mathbf{s}<em _mathbf_t="\mathbf{t">{\mathbf{t}}, \mathbf{a}</em>}}\right)=\mathbf{s<em _mathbf_t="\mathbf{t">{\mathbf{t}} \cdot \mathbf{a}</em>
$$}</p>
<p>This method of separately computing the representations for the state and action is similar to the approach taken in the DRRN (He et al., 2016).</p>
<p>We train the network using experience replay (Lin, 1993) with prioritized sampling (cf., (Moore and Atkeson, 1993)) and a modified version of the $\epsilon$-greedy algorithm (Sutton and Barto, 2018) that we call the $\epsilon_{1}, \epsilon_{2}$-greedy learning algorithm. The experience replay strategy finds paths in the game, which are then stored as transition tuples in a experience replay buffer $D$. The $\epsilon_{1}, \epsilon_{2}$-greedy algorithm explores by choosing actions randomly from $A$ with probability $\epsilon_{1}$ and from $A_{t}$ with a probability $\epsilon_{2}$. The second threshold is needed to account for situations where an action must be chosen to advance the quest for which the agent has no prior in $G_{t}$. That is, action pruning may remove actions essential to quest completion because those actions involve combinations of entities that have not been encountered before.</p>
<p>We then sample a mini-batch of transition tuples consisting of $\left\langle\mathbf{s}<em _mathbf_k="\mathbf{k">{\mathbf{k}}, \mathbf{a}</em>}}, r_{k+1}, \mathbf{s<em _mathbf_k="\mathbf{k">{\mathbf{k}+\mathbf{1}}, \mathbf{A}</em>\right\rangle$ from}+\mathbf{1}}, p_{k</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: KG-DQN architecture, blue shading (or the symbol 'B') indicates components that can be pre-trained and red (or the symbol 'R') indicates no pre-training. The solid lines indicate gradient flow for learnable components.
$D$ and compute the temporal difference loss as:</p>
<p>$$
\begin{aligned}
L(\theta)= &amp; r_{k+1}+ \
&amp; \gamma \max <em _mathbf_k="\mathbf{k">{\mathbf{a} \in \mathbf{A}</em>}+1}} Q\left(\mathbf{s<em _mathbf_t="\mathbf{t">{\mathbf{t}}, \mathbf{a} ; \theta\right)-Q\left(\mathbf{s}</em> ; \theta\right)
\end{aligned}
$$}}, \mathbf{a}_{\mathbf{t}</p>
<p>Replay sampling from $D$ is done by sampling a fraction $\rho$ from transition tuples with a positive reward and $1-\rho$ from the rest. As shown in (Narasimhan et al., 2015), prioritized sampling from experiences with a positive reward helps the deep $Q$-network more easily find the sparse set of transitions that advance the game. The exact training mechanism is described in Algorithm 1.</p>
<h2>4 Game Play as Question Answering</h2>
<p>Previous work has shown that many NLP tasks can be framed as instances of question-answering and that in doing so, one can transfer knowledge between these tasks (McCann et al., 2017). In the abstract, an agent playing a text adventure game can be thought of as continuously asking the question "What is the right action to perform in this situation?" When appropriately trained, the agent may be able to answer the question for itself and select a good next move to execute. Treating the problem as question-answering will not replace the need for exploration in text-adventure games. However, we hypothesize that it will cut down on the amount of exploration needed during testing time, theoretically allowing it to complete quests faster; one of the challenges of text adventure games is that the quests are puzzles and even after training, execution of the policy requires a significant amount of exploration.</p>
<p>To teach the agent to answer the question of what action is best to take given an observation,
we use an offline, pre-training approach. The data for the pre-training approach is generated using an oracle, an agent capable of finishing a game perfectly in the least number of steps possible. Specifically, the agent knows exactly what action to take given the state observation in order to advance the game in the most optimal manner possible. Through this process, we generate a set of traces consisting of state observations and actions such that the state observation provides the context for the implicit question of "What action should be taken?" and the oracle's correct action is the answer. We then use the DrQA (Chen et al., 2017) question-answering technique to train a paired question encoder and an answer encoder that together predict the answer (action) from the question (text observation). The weights from the SB-LSTM in the document encoder in the DrQA system are then used to initialize the weights of the SB-LSTM. Similarly, embedding layers of both the graph and the LSTM action encoder are initialized with the weights from the embedding layer of same document encoder. Since the DrQA embedding layers are initialized with GloVe, we are transferring word embeddings that are tuned during the training of the QA architecture.</p>
<p>The game traces used to train the questionanswering come from a set of games of the same domain but have different specific configurations of the environment and different quests. We use the TextWorld framework (Côté et al., 2018), which uses a grammar to generate random worlds and quests. The types of rooms are the same, but their relative spatial configuration, the types of objects, and the specific sequence of actions needed to complete the quest are different each time. This</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Small</th>
<th style="text-align: left;">Large</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Rooms</td>
<td style="text-align: left;">10</td>
<td style="text-align: left;">20</td>
</tr>
<tr>
<td style="text-align: left;">Total objects</td>
<td style="text-align: left;">20</td>
<td style="text-align: left;">40</td>
</tr>
<tr>
<td style="text-align: left;">Quest length</td>
<td style="text-align: left;">5</td>
<td style="text-align: left;">10</td>
</tr>
<tr>
<td style="text-align: left;">Branching factor</td>
<td style="text-align: left;">143</td>
<td style="text-align: left;">562</td>
</tr>
<tr>
<td style="text-align: left;">Vocab size</td>
<td style="text-align: left;">746</td>
<td style="text-align: left;">819</td>
</tr>
<tr>
<td style="text-align: left;">Average words per obs.</td>
<td style="text-align: left;">67.5</td>
<td style="text-align: left;">94.0</td>
</tr>
<tr>
<td style="text-align: left;">Average new RDF triples per obs.</td>
<td style="text-align: left;">7.2</td>
<td style="text-align: left;">10.5</td>
</tr>
</tbody>
</table>
<p>Table 1: Generated game details.
means that the agent cannot simply memorize quests. For pre-training to work, the agent must develop a general question-answering competence that can transfer to new quests. Our approach to question-answering in the context of text adventure game playing thus represents a form of transfer learning.</p>
<h2>5 Experiments</h2>
<p>We conducted experiments in the TextWorld framework (Côté et al., 2018) using their "home" theme. TextWorld uses a grammar to randomly generate game worlds and quests with given parameters. Games generated with TextWorld start with a zero-th observation that gives instructions for the quest; we do not allow our agent to access this information. The TextWorld API also provides a list of admissible actions at each state-the actions that can be performed based on the objects that are present. We do not allow our agent to access the admissible actions.</p>
<p>We generated two sets of games with different random seeds, representing different game difficulties, which we denote as small and large. Small games have ten rooms and quests of length five and large games have twenty rooms and quests of length ten. Statistics on the games are given in Table 1. Quest length refers to the number of actions that the agent is required to perform in order to finish the quest; more actions are typically necessary to move around the environment and find the objects that need to be interacted with. The branching factor is the size of the action set $A$ for that particular game.</p>
<p>The reward function provided by TextWorld is as follows: +1 for each action taken that moves the agent closer to finishing the quest; -1 for each action taken that extends the minimum number of steps needed to finish the quest from the current stage; 0 for all other situations. The maximum achievable reward for the small and large sets of games are 5 and 10 respectively. This allows for</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">EM</th>
<th style="text-align: left;">Precision</th>
<th style="text-align: left;">Recall</th>
<th style="text-align: left;">F1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Small</td>
<td style="text-align: left;">46.20</td>
<td style="text-align: left;">56.57</td>
<td style="text-align: left;">63.38</td>
<td style="text-align: left;">57.94</td>
</tr>
<tr>
<td style="text-align: left;">Large</td>
<td style="text-align: left;">34.13</td>
<td style="text-align: left;">52.53</td>
<td style="text-align: left;">64.72</td>
<td style="text-align: left;">55.06</td>
</tr>
</tbody>
</table>
<p>Table 2: Pre-training accuracy.
a large amount of variance in quest quality-as measured by steps to complete the quest-that receives maximum reward.</p>
<p>The following procedure for pre-training was done separately for each set of games. Pre-training of the SB-LSTM within the question-answering architecture is conducted by generating 200 games from the same TextWorld theme. The QA system was then trained on data from walkthroughs of a randomly-chosen subset of 160 of these generated games, tuned on a dev set of 20 games, and evaluated on the held-out set of 20 games. Table 2 provides details on the Exact Match (EM), precision, recall, and F1 scores of the QA system after training for the small and large sets of games. Precision, recall, and F1 scores are calculated by counting the number of tokens between the predicted answer and ground truth. An Exact Match is when the entire predicted answer matches with the ground truth. This score is used to tune the model based on the dev set of games.</p>
<p>A random game was chosen from the test-set of games and used as the environment for the agent to train its deep $Q$-network on. Thus, at no time did the QA system see the final testing game prior to the training of the KG-DQN network.</p>
<p>We compare our technique to three baselines:</p>
<ul>
<li>Random command, which samples from the list of admissible actions returned by the TextWorld simulator at each step.</li>
<li>LSTM-DQN, developed by Narasimhan et al. (2015).</li>
<li>Bag-of-Words DQN, which uses a bag-ofwords encoding with a multi-layer feed forward network instead of an LSTM.</li>
</ul>
<p>To achieve the most competitive baselines, we used a randomized grid search to choose the best hyperparameters (e.g., hidden state size, $\gamma, \rho$, final $\epsilon$, update frequency, learning rate, replay buffer size) for the BOW-DQN and LSTM-DQN baselines.</p>
<p>We tested three versions of our KG-DQN:</p>
<ol>
<li>Un-pruned actions with pre-training</li>
</ol>
<p>Algorithm $1 \epsilon_{1}, \epsilon_{2}$-greedy learning algorithm for KG-DQN</p>
<table>
<thead>
<tr>
<th style="text-align: center;">1: for episode $=1$ to $M$ do</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">2:</td>
<td style="text-align: center;">Initialize action dictionary $A$ and graph $G_{0}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">3:</td>
<td style="text-align: center;">Reset the game simulator</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">4:</td>
<td style="text-align: center;">Read initial observation $o_{1}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">5:</td>
<td style="text-align: center;">$G_{1} \leftarrow$ updateGraph $\left(G_{0}, o_{1}\right) ; A_{1} \leftarrow$ pruneActions $\left(A, G_{0}\right)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\triangleright$ Section 3.2</td>
</tr>
<tr>
<td style="text-align: center;">6:</td>
<td style="text-align: center;">for step $t=1$ to $T$ do</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">7:</td>
<td style="text-align: center;">if random ()$&lt;\epsilon_{1}$ then</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">8:</td>
<td style="text-align: center;">if random() $&lt;\epsilon_{2}$ then</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">9:</td>
<td style="text-align: center;">Select random action $a_{t} \in A$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">10:</td>
<td style="text-align: center;">else</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">11:</td>
<td style="text-align: center;">Select random action $a_{t} \in A_{t}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">12:</td>
<td style="text-align: center;">else</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">13:</td>
<td style="text-align: center;">Compute $Q\left(\mathbf{a}_{\mathbf{t}}, \mathbf{a}^{(1)} ; \theta\right)$ for $a^{(1)} \in A$ for network parameters $\theta$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\triangleright$ Section 3.3, Eq. 6</td>
</tr>
<tr>
<td style="text-align: center;">14:</td>
<td style="text-align: center;">Select $a_{t}$ based on $\pi\left(a \mid s_{t}\right)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">15:</td>
<td style="text-align: center;">Execute action $a_{t}$ in the simulator and observe reward $r_{t}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">16:</td>
<td style="text-align: center;">Receive next observation of $\pm 1$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">17:</td>
<td style="text-align: center;">$G_{t+1} \leftarrow$ updateGraph $\left(G_{t}, o_{t+1}\right) ; A_{t+1} \leftarrow$ pruneActions $\left(A, G_{t+1}\right)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\triangleright$ Section 3.1</td>
</tr>
<tr>
<td style="text-align: center;">18:</td>
<td style="text-align: center;">Compute $\mathbf{a}<em _mathbf_t="\mathbf{t">{\mathbf{t}+1}$ and $\mathbf{A}</em>\right \in A}$}+1}=\left(\mathbf{a}^{\wedge 1)}\right.$ for all $\left.a^{\wedge 1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\triangleright$ Section 3.3</td>
</tr>
<tr>
<td style="text-align: center;">19:</td>
<td style="text-align: center;">Set priority $p_{t}=1$ if $r_{t}&gt;0$, else $p_{t}=0$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">20:</td>
<td style="text-align: center;">Store transition $\left(\mathbf{a}<em _mathbf_t="\mathbf{t">{\mathbf{t}}, \mathbf{a}</em>}}, r_{t}, \mathbf{a<em _mathbf_t="\mathbf{t">{\mathbf{t}+1}, \mathbf{A}</em>\right)$ in replay buffer $D$}+1}, p_{t</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">21:</td>
<td style="text-align: center;">Sample mini-batch of transitions $\left(\mathbf{a}<em _mathbf_k="\mathbf{k">{\mathbf{k}}, \mathbf{a}</em>}}, r_{k}, \mathbf{a<em _mathbf_k="\mathbf{k">{\mathbf{k}+1}, \mathbf{A}</em>=1$}+1}, p_{k}\right)$ from $D$, with fraction $\rho$ having $p_{k</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">22:</td>
<td style="text-align: center;">Set $y_{k}=r_{k}+\gamma \max <em _mathbf_k="\mathbf{k">{\mathbf{a} \in \mathbf{A}</em>}+1}} Q\left(\mathbf{a<em k="k">{\mathbf{t}}, \mathbf{a} ; \theta\right)$, or $y</em>$ is terminal}=r_{k}$ if $s_{k+1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">23:</td>
<td style="text-align: center;">Perform gradient descent step on loss function $L(\theta)=\left(y_{k}-Q\left(\mathbf{a}<em _mathbf_t="\mathbf{t">{\mathbf{t}}, \mathbf{a}</em>$}} ; \theta\right)\right)^{2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<ol>
<li>Pruned actions without pre-training</li>
<li>Pruned actions with pre-training (full)</li>
</ol>
<p>Our models use 50 -dimensional word embeddings, 2 heads on the graph attention layers, minibatch size of 16 , and perform a gradient descent update every 5 steps taken by the agent.</p>
<p>All models are evaluated by observing the (a) time to reward convergence, and (b) the average number of steps required for the agent to finish the game with $\epsilon=0.1$ over 5 episodes after training has completed. Following Narasimhan et al. (2015) we set $\epsilon$ to a non-zero value because text adventure games, by nature, require exploration to complete the quests. All results are reported based on multiple independent trials. For the large set of games, we only perform experiments on the best performing models found in the small set of games. Also note that for experiments on large games, we do not display the entire learning curve for the LSTM-DQN baseline, as it converges significantly more slowly than KG-DQN. We run each experiment 5 times and average the results.</p>
<p>Additionally, human performance on the both the games was measured by counting the number of steps taken to finish the game, with and without instructions on the exact quest. We modified Textworld to give the human players reward feedback in the form of a score, the reward function itself is identical to that received by the deep reinforcement learning agents. In one variation of this experiment, the human was given instructions on the potential sequence of steps that are required
to finish the game in addition to the reward in the form of a score and in the other variation, the human received no instructions.</p>
<h2>6 Results and Discussion</h2>
<p>Recall that the number of steps required to finish the game for the oracle agent is 5 and 10 for the small and large maps respectively. It is impossible to achieve this ideal performance due to the structure of the quest. The player needs to interact with objects and explore the environment in order to figure out the exact sequence of actions required to finish the quest. To help benchmark our agent's performance, we observed people unaffiliated with the research playing through the same TextWorld "home" quests as the other models. Those who did not receive instructions on how to finish the quest never finished a single quest and gave up after an average of 184 steps on the small map and an average of 190 steps on the large map. When given instructions, human players completed the quest on the large map in an average of 23 steps, finishing the game with the maximum reward possible. Also note that none of the deep reinforcement learning agents received instructions.</p>
<p>On both small and large maps, all versions of KG-DQN tested converge faster than baselines (see Figure 3 for the small game and Figure 4 for the large game). We don't show BOW-DQN because it is strictly inferior to LSTM-DQN in all situations). KG-DQN converges $40 \%$ faster than baseline on the small game; both KG-DQN and the LSTM-DQN baseline reaches the maximum reward of five. On the large game, no</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Reward learning curve for select experiments with the small games. Best viewed in color.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Steps</th>
</tr>
</thead>
<tbody>
<tr>
<td>Random Command</td>
<td>319.8</td>
</tr>
<tr>
<td>BOW-DQN</td>
<td>83.1 ± 8.0</td>
</tr>
<tr>
<td>LSTM-DQN</td>
<td>72.4 ± 4.6</td>
</tr>
<tr>
<td>Unpruned, pre-trained KG-DQN</td>
<td>131.7 ± 7.7</td>
</tr>
<tr>
<td>Pruned, non-pre-trained KG-DQN</td>
<td>97.3 ± 9.0</td>
</tr>
<tr>
<td>Full KG-DQN</td>
<td>73.7 ± 8.5</td>
</tr>
</tbody>
</table>
<p>Table 3: Average number of steps (and standard deviation) taken to complete the small game.</p>
<p>agents achieve the maximum reward of 10, and the LSTM-DQN requires more than 300 episodes to converge at the same level as KG-DQN. Since all versions of KG-DQN converge at approximately the same rate, we conclude that the knowledge graph—i.e., persistent memory—is the main factor helping convergence time since it is the common element across all experiments.</p>
<p>After training is complete, we measure the number of steps each agent needs to complete each quest. Full KG-DQN requires an equivalent number of steps in the small game (Table 3) and in the large game (Table 4). Differences between LSTM-DQN and full KG-DQN are not statistically significant, <em>p</em> = 0.199 on an independent T-test. The ablated versions of KG-DQN—unpruned KG-DQN and non-pre-trained KG-DQN—require many more steps to complete quests. TextWorld's reward function allows for a lot of exploration of the environment without penalty so it is possible for a model that has converged on reward to complete quests in as few as five steps or in many hundreds of steps. From these results, we conclude that the pre-training using our question-answering paradigm is allowing the agent to find a general understanding of how to pick good actions even when the agent has never seen the final</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Reward learning curve for select experiments with the large games. Best viewed in color.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Steps</th>
</tr>
</thead>
<tbody>
<tr>
<td>Random Command</td>
<td>2054.8</td>
</tr>
<tr>
<td>LSTM-DQN</td>
<td>260.3 ± 4.5</td>
</tr>
<tr>
<td>Pruned, non-pre-trained KG-DQN</td>
<td>340 ± 6.4</td>
</tr>
<tr>
<td>Full KG-DQN</td>
<td>265.9 ± 9.4</td>
</tr>
</tbody>
</table>
<p>Table 4: Average number of steps (and standard deviation) taken to complete the large game.</p>
<p>test game. LSTM-DQN also learns how to choose actions efficiently, but this knowledge is captured in the LSTM's cell state, whereas in KG-DQN this knowledge is made explicit in the knowledge graph and retrieved effectively by graph attention. Taken together, KG-DQN converges faster without loss of quest solution quality.</p>
<h2>7 Conclusions</h2>
<p>We have shown that incorporating knowledge graphs into an deep <em>Q</em>-network can reduce training time for agents playing text-adventure games of various lengths. We speculate that this is because the knowledge graph provides a persistent memory of the world as it is being explored. While the knowledge graph allows the agent to reach optimal reward more quickly, it doesn't ensure a high quality solution to quests. Action pruning using the knowledge graph and pre-training of the embeddings used in the deep <em>Q</em>-network result in shorter action sequences needed to complete quests.</p>
<p>The insight into pre-training portions of the agent's architecture is based on converting text-adventure game playing into a question-answering activity. That is, at every step, the agent is asking—and trying to answer—what is the most important thing to try. The pre-training acts as a form of transfer learning from different, but re-</p>
<p>lated games. However, question-answering alone cannot solve the text-adventure playing problem because there will always be some trial and error required.</p>
<p>By addressing the challenges of partial observability and combinatorially large action, spaces through persistent memory, our work on playing text-adventure games addresses a critical need for reinforcement learning for language. Textadventure games can be seen as a stepping stone toward more complex, real-world tasks; the human world is one of partial understanding through communication and acting on the world using language.</p>
<h2>References</h2>
<p>Gabor Angeli, Johnson Premkumar, Melvin Jose, and Christopher D. Manning. 2015. Leveraging Linguistic Structure For Open Domain Information Extraction. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers).</p>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. arXiv:1409.0473.</p>
<p>Antoine Bordes, Nicolas Usunier, Ronan Collobert, and Jason Weston. 2010. Towards understanding situated natural language. In Proceedings of the 2010 International Conference on Artificial Intelligence and Statistics.</p>
<p>Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading Wikipedia to answer opendomain questions. In Association for Computational Linguistics (ACL).</p>
<p>Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben Kybartas, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, and Adam Trischler. 2018. TextWorld : A Learning Environment for Text-based Games. In Proceedings of the ICML/IJCAI 2018 Workshop on Computer Games, page 29.</p>
<p>Nancy Fulda, Daniel Ricks, Ben Murdoch, and David Wingate. 2017. What can you do with a rock? affordance extraction via word embeddings. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17, pages 1039-1045.</p>
<p>Jian Guan, Yansen Wang, and Minlie Huang. 2018. Story Ending Generation with Incremental Encoding and Commonsense Knowledge. arXiv:1808.10113v1.</p>
<p>Matan Haroush, Tom Zahavy, Daniel J Mankowitz, and Shie Mannor. 2018. Learning How Not to Act in Text-Based Games. In Workshop Track at ICLR 2018, pages 1-4.</p>
<p>Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari Ostendorf. 2016. Deep Reinforcement Learning with a Natural Language Action Space. In Association for Computational Linguistics (ACL).</p>
<p>Long-Ji Lin. 1993. Reinforcement learning for robots using neural networks. Ph.D. thesis, Carnegie Mellon University.</p>
<p>Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. 2017. The Natural Language Decathlon : Multitask Learning as Question Answering. arXiv:1806.08730.</p>
<p>Andrew W. Moore and Christopher G. Atkeson. 1993. Prioritized sweeping: Reinforcement learning with less data and less time. Machine Learning, 13(1):103-130.</p>
<p>Karthik Narasimhan, Tejas Kulkarni, and Regina Barzilay. 2015. Language Understanding for Textbased Games Using Deep Reinforcement Learning. In Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Robert Speer and Catherine Havasi. 2012. Representing General Relational Knowledge in ConceptNet 5. In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC).</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104-3112.</p>
<p>Richard S Sutton and Andrew G Barto. 2018. Reinforcement Learning: An Introduction. MIT Press.</p>
<p>Ruo Yu Tao, Marc-Alexandre Côté, Xingdi Yuan, and Layla El Asri. 2018. Towards solving text-based games by producing adaptive action spaces. In Proceedings of the 2018 NeurIPS Workshop on Wordplay: Reinforcement and Language Learning in Text-based Games.</p>
<p>Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. 2018. Graph Attention Networks. International Conference on Learning Representations (ICLR).</p>
<p>Christopher J. C. H. Watkins and Peter Dayan. 1992. Q-learning. Machine Learning, 8(3):279-292.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4256 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4256</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4256</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-98.html">extraction-schema-98</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <p><strong>Paper ID:</strong> paper-278338773</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.03418v1.pdf" target="_blank">Knowledge Augmented Complex Problem Solving with Large Language Models: A Survey</a></p>
                <p><strong>Paper Abstract:</strong> Problem-solving has been a fundamental driver of human progress in numerous domains. With advancements in artificial intelligence, Large Language Models (LLMs) have emerged as powerful tools capable of tackling complex problems across diverse domains. Unlike traditional computational systems, LLMs combine raw computational power with an approximation of human reasoning, allowing them to generate solutions, make inferences, and even leverage external computational tools. However, applying LLMs to real-world problem-solving presents significant challenges, including multi-step reasoning, domain knowledge integration, and result verification. This survey explores the capabilities and limitations of LLMs in complex problem-solving, examining techniques including Chain-of-Thought (CoT) reasoning, knowledge augmentation, and various LLM-based and tool-based verification techniques. Additionally, we highlight domain-specific challenges in various domains, such as software engineering, mathematical reasoning and proving, data analysis and modeling, and scientific research. The paper further discusses the fundamental limitations of the current LLM solutions and the future directions of LLM-based complex problems solving from the perspective of multi-step reasoning, domain knowledge integration and result verification.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4256.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4256.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM literature review & synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Model-assisted Literature Review and Synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of LLMs to process corpora of scholarly papers to summarize findings, uncover trends, identify knowledge gaps, and synthesize higher-level qualitative insights across a body of literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM-assisted literature synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>The survey describes LLMs being applied to summarize large textual corpora (scholarly papers) to extract trends and knowledge gaps. Typical pipelines referenced involve retrieval of relevant papers, prompting LLMs to produce summaries or syntheses, optionally iterating (e.g., multi-turn or agentic workflows) and combining outputs into higher-level thematic syntheses. The paper emphasises retrieval/augmentation (RAG-like) and multi-agent or human-in-the-loop refinement for more reliable outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>cross-domain scientific literature (biomedicine, environmental science, general scientific research)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>empirical generalizations, thematic trends, knowledge gaps, high-level hypotheses/insights</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>survey notes common practice: human expert assessment and downstream empirical validation; also highlights that LLMs are poor self-evaluators and human/experimental verification is required</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not specified in survey; typically compared informally to manual literature review by humans in cited works</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Survey reports LLMs can accelerate literature summarization and idea synthesis but struggle to reliably evaluate quality of synthesized ideas; effective pipelines combine retrieval (RAG), human feedback, and experimental/empirical validation.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>LLM hallucination, unreliable self-evaluation, difficulty in assessing novelty/quality of generated hypotheses, need for domain expertise and external validation; evaluation metrics for synthesized qualitative laws are lacking.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge Augmented Complex Problem Solving with Large Language Models: A Survey', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4256.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4256.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG/GraphRAG document grounding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation and GraphRAG for Document-grounded Knowledge Extraction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Retrieval-based grounding methods (RAG, GraphRAG) that fetch relevant document passages or graph-structured context from collections of scholarly papers to ground LLM outputs and support extracting structured knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>RAG / GraphRAG retrieval + LLM synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>The survey describes using retrieval-augmented generation (RAG) and graph-based retrieval (GraphRAG) to supply LLMs with relevant passages or graph-context from document collections; these retrieved contexts are used as in-context grounding so LLMs can produce more faithful summaries, extract relations, and help construct knowledge graphs from papers. The paper also mentions building knowledge graphs from document collections and using them during inference (retrieval-augmentation) or for data synthesis for training.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>document corpora / scientific literature across domains</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>structured factual relations, entity–relation triples, cross-paper thematic patterns, and grounded knowledge statements (that can support qualitative generalizations)</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Survey indicates such systems require careful validation/verification (human validation, comparison to curated KBs); specific evaluation methods depend on the cited work but often include human judgement and precision/recall on extracted triples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared in cited literature against vanilla LLM prompting without retrieval and against simpler retrieval baselines; specifics not given in survey</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Grounding LLMs with retrieved document context or graphs improves faithfulness and domain-specific accuracy; knowledge graphs constructed from papers can be used both to synthesize training data and to augment inference.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Long-tail and domain-specific facts remain hard to capture; building and validating large theme-specific knowledge graphs from papers is nontrivial; retrieval quality and up-to-dateness limit extraction fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge Augmented Complex Problem Solving with Large Language Models: A Survey', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4256.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4256.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OneKE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced system that deploys schema-guided LLM agents to extract structured knowledge from document collections (papers) in an automated, dockerized pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>schema-guided LLM agent extraction (OneKE)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>As cited in the survey, OneKE uses an LLM-agent architecture guided by extraction schemas: agents are provided with extraction templates/schemas and instructed to parse documents (papers) to populate structured knowledge records. It is deployed in a dockerized pipeline to scale extraction across corpora. The survey mentions it in the context of systems that extract knowledge from papers for downstream knowledge-graph construction and tooling.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>general scientific/technical document collections</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>structured knowledge records/triples and procedural or conceptual relationships that can support later synthesis of principles</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Listed as an example of a practical system for extracting structured knowledge from documents; exemplifies schema-guided agentic extraction pipelines referenced by the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Survey does not provide evaluation details; general limitations include dependency on schema design, quality of prompts, and LLM hallucinations during extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge Augmented Complex Problem Solving with Large Language Models: A Survey', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4256.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4256.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ResearchAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced agentic approach that iteratively reads scientific literature and generates research ideas or hypotheses by synthesizing across papers using LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>iterative literature-driven idea generation (ResearchAgent)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Cited in the survey as an example where LLMs iteratively process scientific literature to propose research ideas: the system retrieves relevant papers, uses LLM prompting to extract salient findings, and iteratively refines and combines these to generate candidate hypotheses or research directions, possibly with multi-turn feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>scientific research literature (cross-domain)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>hypotheses, conceptual patterns, high-level theoretical ideas or research directions (not formal laws)</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Survey notes difficulty: selection and evaluation of high-quality ideas is challenging; cited works often rely on human expert assessment or downstream experimental follow-up</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLM-based agents can generate plausible research ideas from literature, but automated evaluation and selection of high-quality, novel, and valid ideas remains a key bottleneck.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Same limitations as literature synthesis: unreliable self-evaluation, potential hallucination, difficulty assessing novelty and correctness without human or experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge Augmented Complex Problem Solving with Large Language Models: A Survey', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4256.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4256.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM automated systematic review</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-automated Systematic Review (emergent use)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Works that apply LLMs to automate components or whole pipelines of systematic literature reviews, including screening, summarization, and synthesis of bodies of scholarly papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The emergence of Large Language Models (LLM) as a tool in literature reviews: an LLM automated systematic review</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM-automated systematic review</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Referenced survey and empirical works apply LLMs to perform tasks in systematic reviews such as paper screening, extracting study findings, summarizing results, and synthesizing cross-study patterns. Approaches combine retrieval, prompting for extraction/summarization, and human-in-the-loop verification to build literature syntheses.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>systematic reviews across biomedical and other scientific domains</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>cross-study empirical generalizations, recurring patterns across studies, meta-level findings and identified gaps</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Typically evaluated by comparison to human systematic-review outputs and expert judgement; survey stresses need for human validation and replication.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared in cited literature to manual (human) systematic review pipelines or to narrower automated baselines; survey does not list numeric outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs can speed up review tasks and surface cross-study patterns, but quality control (human oversight, verification) is necessary to avoid propagation of errors or hallucinated syntheses.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Errors in extraction, hallucinated claims, sensitivity to prompt and retrieval quality, and lack of standardized evaluation metrics for synthesized qualitative laws.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge Augmented Complex Problem Solving with Large Language Models: A Survey', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4256.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4256.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Theme-specific KG construction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated Construction of Theme-specific Knowledge Graphs from Scholarly Documents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automated pipelines that extract structured concept–relation representations from document collections to form theme-specific knowledge graphs that can support discovery of domain principles and workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automated construction of theme-specific knowledge graphs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>theme-specific KG construction from documents</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Cited works propose extracting entities, relations, and procedural/technical descriptions from papers and aggregating them into knowledge graphs capturing domain techniques and relationships; survey suggests such KGs can be used both for training data synthesis and for retrieval-augmentation at inference to help LLMs surface domain-relevant principles.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>academic/scientific literature (theme-specific domains)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>structured relationships, method–outcome associations, procedural templates, and conceptual hierarchies that support later derivation of qualitative principles</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Survey references careful validation and verification of extracted KGs; typical evaluations include manual curation checks and precision/recall of extracted relations in cited literature.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Theme-specific KGs are promising for giving LLMs structured domain grounding and for synthesizing domain knowledge into reusable templates and principles.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Constructing accurate KGs at scale requires robust extraction pipelines, schema design, and verification; long-tail information and nuanced domain knowledge present extraction difficulties.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge Augmented Complex Problem Solving with Large Language Models: A Survey', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models <em>(Rating: 2)</em></li>
                <li>OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System <em>(Rating: 2)</em></li>
                <li>The emergence of Large Language Models (LLM) as a tool in literature reviews: an LLM automated systematic review <em>(Rating: 2)</em></li>
                <li>Automated construction of theme-specific knowledge graphs <em>(Rating: 2)</em></li>
                <li>Automating research synthesis with domain-specific large language model fine-tuning <em>(Rating: 2)</em></li>
                <li>Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources <em>(Rating: 1)</em></li>
                <li>Retrieval-augmented generation for large language models: A survey <em>(Rating: 1)</em></li>
                <li>KnowAgent: Knowledge-augmented planning for llm-based agents <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4256",
    "paper_id": "paper-278338773",
    "extraction_schema_id": "extraction-schema-98",
    "extracted_data": [
        {
            "name_short": "LLM literature review & synthesis",
            "name_full": "Large Language Model-assisted Literature Review and Synthesis",
            "brief_description": "Use of LLMs to process corpora of scholarly papers to summarize findings, uncover trends, identify knowledge gaps, and synthesize higher-level qualitative insights across a body of literature.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "method_name": "LLM-assisted literature synthesis",
            "method_description": "The survey describes LLMs being applied to summarize large textual corpora (scholarly papers) to extract trends and knowledge gaps. Typical pipelines referenced involve retrieval of relevant papers, prompting LLMs to produce summaries or syntheses, optionally iterating (e.g., multi-turn or agentic workflows) and combining outputs into higher-level thematic syntheses. The paper emphasises retrieval/augmentation (RAG-like) and multi-agent or human-in-the-loop refinement for more reliable outputs.",
            "number_of_papers": null,
            "domain_or_field": "cross-domain scientific literature (biomedicine, environmental science, general scientific research)",
            "type_of_laws_extracted": "empirical generalizations, thematic trends, knowledge gaps, high-level hypotheses/insights",
            "example_laws_extracted": null,
            "evaluation_method": "survey notes common practice: human expert assessment and downstream empirical validation; also highlights that LLMs are poor self-evaluators and human/experimental verification is required",
            "performance_metrics": null,
            "comparison_baseline": "Not specified in survey; typically compared informally to manual literature review by humans in cited works",
            "key_findings": "Survey reports LLMs can accelerate literature summarization and idea synthesis but struggle to reliably evaluate quality of synthesized ideas; effective pipelines combine retrieval (RAG), human feedback, and experimental/empirical validation.",
            "challenges_limitations": "LLM hallucination, unreliable self-evaluation, difficulty in assessing novelty/quality of generated hypotheses, need for domain expertise and external validation; evaluation metrics for synthesized qualitative laws are lacking.",
            "uuid": "e4256.0",
            "source_info": {
                "paper_title": "Knowledge Augmented Complex Problem Solving with Large Language Models: A Survey",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "RAG/GraphRAG document grounding",
            "name_full": "Retrieval-Augmented Generation and GraphRAG for Document-grounded Knowledge Extraction",
            "brief_description": "Retrieval-based grounding methods (RAG, GraphRAG) that fetch relevant document passages or graph-structured context from collections of scholarly papers to ground LLM outputs and support extracting structured knowledge.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "method_name": "RAG / GraphRAG retrieval + LLM synthesis",
            "method_description": "The survey describes using retrieval-augmented generation (RAG) and graph-based retrieval (GraphRAG) to supply LLMs with relevant passages or graph-context from document collections; these retrieved contexts are used as in-context grounding so LLMs can produce more faithful summaries, extract relations, and help construct knowledge graphs from papers. The paper also mentions building knowledge graphs from document collections and using them during inference (retrieval-augmentation) or for data synthesis for training.",
            "number_of_papers": null,
            "domain_or_field": "document corpora / scientific literature across domains",
            "type_of_laws_extracted": "structured factual relations, entity–relation triples, cross-paper thematic patterns, and grounded knowledge statements (that can support qualitative generalizations)",
            "example_laws_extracted": null,
            "evaluation_method": "Survey indicates such systems require careful validation/verification (human validation, comparison to curated KBs); specific evaluation methods depend on the cited work but often include human judgement and precision/recall on extracted triples.",
            "performance_metrics": null,
            "comparison_baseline": "Compared in cited literature against vanilla LLM prompting without retrieval and against simpler retrieval baselines; specifics not given in survey",
            "key_findings": "Grounding LLMs with retrieved document context or graphs improves faithfulness and domain-specific accuracy; knowledge graphs constructed from papers can be used both to synthesize training data and to augment inference.",
            "challenges_limitations": "Long-tail and domain-specific facts remain hard to capture; building and validating large theme-specific knowledge graphs from papers is nontrivial; retrieval quality and up-to-dateness limit extraction fidelity.",
            "uuid": "e4256.1",
            "source_info": {
                "paper_title": "Knowledge Augmented Complex Problem Solving with Large Language Models: A Survey",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "OneKE",
            "name_full": "OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System",
            "brief_description": "A referenced system that deploys schema-guided LLM agents to extract structured knowledge from document collections (papers) in an automated, dockerized pipeline.",
            "citation_title": "OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "method_name": "schema-guided LLM agent extraction (OneKE)",
            "method_description": "As cited in the survey, OneKE uses an LLM-agent architecture guided by extraction schemas: agents are provided with extraction templates/schemas and instructed to parse documents (papers) to populate structured knowledge records. It is deployed in a dockerized pipeline to scale extraction across corpora. The survey mentions it in the context of systems that extract knowledge from papers for downstream knowledge-graph construction and tooling.",
            "number_of_papers": null,
            "domain_or_field": "general scientific/technical document collections",
            "type_of_laws_extracted": "structured knowledge records/triples and procedural or conceptual relationships that can support later synthesis of principles",
            "example_laws_extracted": null,
            "evaluation_method": null,
            "performance_metrics": null,
            "comparison_baseline": null,
            "key_findings": "Listed as an example of a practical system for extracting structured knowledge from documents; exemplifies schema-guided agentic extraction pipelines referenced by the survey.",
            "challenges_limitations": "Survey does not provide evaluation details; general limitations include dependency on schema design, quality of prompts, and LLM hallucinations during extraction.",
            "uuid": "e4256.2",
            "source_info": {
                "paper_title": "Knowledge Augmented Complex Problem Solving with Large Language Models: A Survey",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "ResearchAgent",
            "name_full": "ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models",
            "brief_description": "A referenced agentic approach that iteratively reads scientific literature and generates research ideas or hypotheses by synthesizing across papers using LLMs.",
            "citation_title": "ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "method_name": "iterative literature-driven idea generation (ResearchAgent)",
            "method_description": "Cited in the survey as an example where LLMs iteratively process scientific literature to propose research ideas: the system retrieves relevant papers, uses LLM prompting to extract salient findings, and iteratively refines and combines these to generate candidate hypotheses or research directions, possibly with multi-turn feedback.",
            "number_of_papers": null,
            "domain_or_field": "scientific research literature (cross-domain)",
            "type_of_laws_extracted": "hypotheses, conceptual patterns, high-level theoretical ideas or research directions (not formal laws)",
            "example_laws_extracted": null,
            "evaluation_method": "Survey notes difficulty: selection and evaluation of high-quality ideas is challenging; cited works often rely on human expert assessment or downstream experimental follow-up",
            "performance_metrics": null,
            "comparison_baseline": null,
            "key_findings": "LLM-based agents can generate plausible research ideas from literature, but automated evaluation and selection of high-quality, novel, and valid ideas remains a key bottleneck.",
            "challenges_limitations": "Same limitations as literature synthesis: unreliable self-evaluation, potential hallucination, difficulty assessing novelty and correctness without human or experimental validation.",
            "uuid": "e4256.3",
            "source_info": {
                "paper_title": "Knowledge Augmented Complex Problem Solving with Large Language Models: A Survey",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "LLM automated systematic review",
            "name_full": "LLM-automated Systematic Review (emergent use)",
            "brief_description": "Works that apply LLMs to automate components or whole pipelines of systematic literature reviews, including screening, summarization, and synthesis of bodies of scholarly papers.",
            "citation_title": "The emergence of Large Language Models (LLM) as a tool in literature reviews: an LLM automated systematic review",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "method_name": "LLM-automated systematic review",
            "method_description": "Referenced survey and empirical works apply LLMs to perform tasks in systematic reviews such as paper screening, extracting study findings, summarizing results, and synthesizing cross-study patterns. Approaches combine retrieval, prompting for extraction/summarization, and human-in-the-loop verification to build literature syntheses.",
            "number_of_papers": null,
            "domain_or_field": "systematic reviews across biomedical and other scientific domains",
            "type_of_laws_extracted": "cross-study empirical generalizations, recurring patterns across studies, meta-level findings and identified gaps",
            "example_laws_extracted": null,
            "evaluation_method": "Typically evaluated by comparison to human systematic-review outputs and expert judgement; survey stresses need for human validation and replication.",
            "performance_metrics": null,
            "comparison_baseline": "Compared in cited literature to manual (human) systematic review pipelines or to narrower automated baselines; survey does not list numeric outcomes.",
            "key_findings": "LLMs can speed up review tasks and surface cross-study patterns, but quality control (human oversight, verification) is necessary to avoid propagation of errors or hallucinated syntheses.",
            "challenges_limitations": "Errors in extraction, hallucinated claims, sensitivity to prompt and retrieval quality, and lack of standardized evaluation metrics for synthesized qualitative laws.",
            "uuid": "e4256.4",
            "source_info": {
                "paper_title": "Knowledge Augmented Complex Problem Solving with Large Language Models: A Survey",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Theme-specific KG construction",
            "name_full": "Automated Construction of Theme-specific Knowledge Graphs from Scholarly Documents",
            "brief_description": "Automated pipelines that extract structured concept–relation representations from document collections to form theme-specific knowledge graphs that can support discovery of domain principles and workflows.",
            "citation_title": "Automated construction of theme-specific knowledge graphs",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "method_name": "theme-specific KG construction from documents",
            "method_description": "Cited works propose extracting entities, relations, and procedural/technical descriptions from papers and aggregating them into knowledge graphs capturing domain techniques and relationships; survey suggests such KGs can be used both for training data synthesis and for retrieval-augmentation at inference to help LLMs surface domain-relevant principles.",
            "number_of_papers": null,
            "domain_or_field": "academic/scientific literature (theme-specific domains)",
            "type_of_laws_extracted": "structured relationships, method–outcome associations, procedural templates, and conceptual hierarchies that support later derivation of qualitative principles",
            "example_laws_extracted": null,
            "evaluation_method": "Survey references careful validation and verification of extracted KGs; typical evaluations include manual curation checks and precision/recall of extracted relations in cited literature.",
            "performance_metrics": null,
            "comparison_baseline": null,
            "key_findings": "Theme-specific KGs are promising for giving LLMs structured domain grounding and for synthesizing domain knowledge into reusable templates and principles.",
            "challenges_limitations": "Constructing accurate KGs at scale requires robust extraction pipelines, schema design, and verification; long-tail information and nuanced domain knowledge present extraction difficulties.",
            "uuid": "e4256.5",
            "source_info": {
                "paper_title": "Knowledge Augmented Complex Problem Solving with Large Language Models: A Survey",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models",
            "rating": 2,
            "sanitized_title": "researchagent_iterative_research_idea_generation_over_scientific_literature_with_large_language_models"
        },
        {
            "paper_title": "OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System",
            "rating": 2,
            "sanitized_title": "oneke_a_dockerized_schemaguided_llm_agentbased_knowledge_extraction_system"
        },
        {
            "paper_title": "The emergence of Large Language Models (LLM) as a tool in literature reviews: an LLM automated systematic review",
            "rating": 2,
            "sanitized_title": "the_emergence_of_large_language_models_llm_as_a_tool_in_literature_reviews_an_llm_automated_systematic_review"
        },
        {
            "paper_title": "Automated construction of theme-specific knowledge graphs",
            "rating": 2,
            "sanitized_title": "automated_construction_of_themespecific_knowledge_graphs"
        },
        {
            "paper_title": "Automating research synthesis with domain-specific large language model fine-tuning",
            "rating": 2,
            "sanitized_title": "automating_research_synthesis_with_domainspecific_large_language_model_finetuning"
        },
        {
            "paper_title": "Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources",
            "rating": 1,
            "sanitized_title": "chainofknowledge_grounding_large_language_models_via_dynamic_knowledge_adapting_over_heterogeneous_sources"
        },
        {
            "paper_title": "Retrieval-augmented generation for large language models: A survey",
            "rating": 1,
            "sanitized_title": "retrievalaugmented_generation_for_large_language_models_a_survey"
        },
        {
            "paper_title": "KnowAgent: Knowledge-augmented planning for llm-based agents",
            "rating": 1,
            "sanitized_title": "knowagent_knowledgeaugmented_planning_for_llmbased_agents"
        }
    ],
    "cost": 0.016949,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Knowledge Augmented Complex Problem Solving with Large Language Models: A Survey
6 May 2025</p>
<p>Da A Zheng 
Ant Group, ChinaJunwei Su junweisu@connect.hku.hk 
Lun Du 
Lanning Wei weilanning.wln@antgroup.com 
Software Dev 
Scientific Lifecycle 
Principle </p>
<p>The University of Hong Kong
China</p>
<p>Ant Group
BeijingChina</p>
<p>Ant Group
BeijingChina</p>
<p>The University of Hong Kong
Hong KongChina</p>
<p>Yuchen Tian
Ant Group
BeijingChina</p>
<p>Yuqi Zhu
Zhejiang University
HangzhouChina</p>
<p>Jintian Zhang
Zhejiang University
HangzhouChina</p>
<p>Ant Group
BeijingChina</p>
<p>Ningyu Zhang
Zhejiang University
HangzhouChina</p>
<p>Zhejiang University
Huajun Chen, HangzhouChina</p>
<p>Knowledge Augmented Complex Problem Solving with Large Language Models: A Survey
6 May 20252709B65B4A30EA877D6626196F450236arXiv:2505.03418v1[cs.LG]Large language modelsreasoningcomplex problem solving Medical Diagnosis Data Analysis Software Engineering Legal Judgments Required Knowledge Mathematics Solving
Problem-solving has been a fundamental driver of human progress in numerous domains.With advancements in artificial intelligence, Large Language Models (LLMs) have emerged as powerful tools capable of tackling complex problems across diverse domains.Unlike traditional computational systems, LLMs combine raw computational power with an approximation of human reasoning, allowing them to generate solutions, make inferences, and even leverage external computational tools.However, applying LLMs to real-world problemsolving presents significant challenges, including multi-step reasoning, domain knowledge integration, and result verification.This survey explores the capabilities and limitations of LLMs in complex problem-solving, examining techniques including Chain-of-Thought (CoT) reasoning, knowledge augmentation, and various LLM-based and tool-based verification techniques.Additionally, we highlight domain-specific challenges in various domains, such as software engineering, mathematical reasoning and proving, data analysis and modeling, and scientific research.The paper further discusses the fundamental limitations of the current LLM solutions and the future directions of LLM-based complex problems solving from the perspective of multi-step reasoning, domain knowledge integration and result verification.</p>
<p>Introduction</p>
<p>The history of human civilization has been shaped by the ability of solving problems, ranging from constructing shelters in ancient times to unlocking the mysteries of the universe.For example, ancient astronomers calculated the Earth's size, while modern scientists predict weather using computational models.With technological advancements, humanity has gradually shifted from relying solely on individual or collective intellect to leveraging powerful tools like computers to address increasingly complex challenges.This transition marks a paradigm shift in problem solving, evolving from purely human-centered approaches to a synergy between human ingenuity and computational capability.</p>
<p>Today, LLM-based AI systems represent a groundbreaking advancement [77,96,171,178].Unlike traditional computers, which excel at precise calculations, LLMs simulate aspects of human reasoning, such as generating creative solutions and making contextual inferences.This positions LLMs as tools that combine computational power with an approximation of human thought to solve complex problems that are challenging to humans.Similar to human problem-solving, LLMs can directly solve problems and generate final results; LLMs can leverage computers to solve problems by writing and executing code to get results.</p>
<p>The scope of complex problem solving spans a wide range of domains, encompassing challenges that touch virtually every aspect of human society (Figure 1).For instance, designing robust software system architectures requires balancing scalability, reliability, and user needs, while proving mathematical theorems demands rigorous logical reasoning and abstraction.In the realm of data science, building accurate models to interpret vast datasets is essential for informed decisionmaking.Similarly, drug discovery involves navigating intricate molecular interactions to identify effective therapies, and constructing physical models enables us to simulate and understand natural phenomena.These examples highlight the diversity of complex problems humanity strives to solve, each requiring a blend of domain expertise, reasoning, and creativity.Solving these real-world complex problems involves leveraging domain knowledge or experience and progressing through multiple reasoning steps to arrive at a final solution.In the community, mathematical reasoning is frequently studied as a representative form of complex problem-solving and current research predominantly focuses on the mathematical reasoning problems with definitive final answers.In contrast, mathematical theorem proving tasks -which are more representative of challenges encountered in higher education and research -are often overlooked because they typically lack a single final answer to verify.In practice, many real-world complex problems are even more challenging than mathematical reasoning tasks.First, these problems are inherently difficult to verify.For instance, in data science, numerous modeling techniques can be applied to the same dataset, yet their performance may vary greatly.Moreover, the effectiveness of a model is highly context-dependent, differing across problems and datasets.This variability makes it difficult to determine the optimal solution for any given modeling task.Second, solving such real-world problems requires substantial domain expertise.Using data modeling again as an example, one must not only understand the nuances of the data but also be proficient in a wide range of modeling techniques to achieve strong performance.</p>
<p>Solving real-world complex problems requires three key components: multi-step reasoning, domain knowledge, and result verification.This problem-solving process presents multiple challenges when LLMs are applied to real-world problems.</p>
<p>• Multi-step reasoning: Solving a complex problem requires taking multiple steps to reach the final outcome.The size of the search space is largely determined by the number of steps needed to solve a complex problem, and can grow exponentially as the number of reasoning steps increases.This makes it challenging to identify the correct path to the final result.In addition, any errors that occur in the search process can propagate and lead to an incorrect result.• Domain knowledge: Knowledge plays a crucial role in guiding the problem solver through the search space, helping to identify the next step or recognize when the solution has been reached.Domain-specific applications, such as machine learning tasks and mathematical theorem proving tasks, typically require to utilize long-tail domain knowledge while it is well-known that LLMs cannot master low-tail knowledge well [121].This requires an LLMbased system to take extra care to master domain knowledge and reliably retrieve and apply the required knowledge to solve problems.• Result verification: Each step must be carefully evaluated to assess whether it contributes to a correct solution or whether the entire solutions can solve the given problem.This evaluation can be particularly challenging in many applications where standard outcomes or predefined solution procedures are lacking.The difficulty is even greater for open-ended problems with ill-defined goals, such as those found in scientific research and data mining.</p>
<p>Recent development of LLMs have demonstrated their strong reasoning capabilities on some complex problems that have well-defined goals and whose results can be easily verified, making it ideal for tasks like mathematical reasoning and competitive coding challenges.Chain-of-Thought (CoT) reasoning is the major technique to solve multi-step reasoning [13,136,149,175].There is an inference scaling law in CoT reasoning that the likelihood of finding a correct solution improves significantly as the number of CoT paths increases [20] and it is often possible to generate correct solutions for many challenging problems with a sufficient number of CoT paths [10].Because the target applications, such as mathematical reasoning and competitive coding, are easily verifiable, many works [23,65] are using reinforcement learning to train LLMs to improve their reasoning capabilities for these applications [74].The release of GPT-o1 by OpenAI [99] and DeepSeek-R1 [23] showcases the potential of this CoT reasoning approach [161].</p>
<p>While CoT reasoning is an important technique to solve complex problems, it is necessary to adopt an agentic approach that enables access to external knowledge bases and the use of verification tools to further improve LLMs' capability in solving real-world complex problems.Previous studies have shown that LLMs have difficulty retaining long-tail knowledge [121], and domain-specific knowledge often falls into this category.It is essential to make external knowledge integration for knowledge-intensive tasks such as scientific discovery [1], mathematical theorem proving [138], and data science [44], where domain expertise is critical for accurate and informed decision-making.Knowledge can be retrieved from documents through techniques like RAG [36] and GraphRAG [30,48], or by leveraging knowledge graphs constructed from document collections [78].Additionally, agents may interact with humans to acquire domain knowledge directly [26,174].Result verification is also essential to ensure valid solutions from large language models (LLMs), both during training and inference.Reasoning-focused LLMs are often trained with synthetic data, which requires a verifier to select high-quality data for model training [20].During inference, the inference scaling law highlights the need for a verifier to identify the correct solution among multiple candidates [10].Various types of verifiers can be employed for this purpose, including LLM-as-a-judge approaches [41], symbolic reasoning tools [19], and even experimental validation systems [76].</p>
<p>Despite the significant advancements in LLMs for complex problem solving, each domain presents its own unique challenges when applying LLMs to practical applications.Take some domains in Figure 1 as examples.In software engineering, LLMs are tasked with generating or modifying code within large code repositories for bug fixings and new feature implementations.This requires them not only to reason about code generation but also to have a comprehensive understanding of the entire codebase and project requirements [146].Furthermore, software development demands not just code correctness but also optimization in terms of computational efficiency and memory usage [111], adding an additional layer of complexity to the evaluation process.Mathematics encompasses two primary types of tasks: calculations and provings.While extensive data are available for basic arithmetic and computational tasks, data scarcity remains a significant challenge in advanced mathematics, particularly in higher education and research [39].To address this limitation, it is essential to leverage domain knowledge more effectively for data synthesis to mitigate the impact of data scarcity and utilize existing mathematical knowledge, such as theorems, to improve mathematical proving.In addition, mathematical theorem proving usually lack an effective way to verify the proving solution, making it difficult to train LLM models to generate rigorously correct mathematical reasoning solutions.Data science involves working with large datasets, yet task descriptions often lack sufficient details about the distribution of input data, making it challenging for LLMs to generate the most suitable solutions to model the large datasets well [12].This also complicates the evaluation of LLM-generated outputs, necessitating multilevel assessments.Furthermore, leveraging a comprehensive knowledge base of data modeling techniques is crucial for developing more effective methods to tackle complex data science problems.Scientific research often involves open-ended problems, which prevents us from training LLMs to solve scientific problems directly.One potential solution is to involve humans in the process (human-LLM collaboration), allowing for iterative collaboration between humans and LLMs to explore existing scientific literature and human knowledge [8,60,85,88,109,123], generate novel ideas [5,112,128,131] and automate entire research pipelines [87].These challenges highlight the need for further research into complex problem-solving that go beyond the current reasoning LLMs.</p>
<p>This paper provides a high-level overview of the current advancements in LLMs for solving complex problems and goes beyond the literature of reasoning LLMs.Our goal is to review the key techniques developed for LLMs and how these methods are being applied to address the challenges in different domains.The paper is structured into four sections to discuss the current LLM research:</p>
<p>• Definition of complex problem solving: We begin by formally defining complex problem solving from the perspectives of cognitive science and computational theory (Section 2).• Methodologies: We examine the key methodologies in LLM research for solving complex problems, including multi-step reasoning, knowledge augmentations and result verifications (Section 3).• Domains: We explore complex problem solving across four domains -software engineering, data science, mathematics, and scientific research -highlighting the unique challenges in each and the solutions developed to address them (Section 4).• Current limitations and future directions: We discuss the limitations of current research and propose potential directions for future studies (Section 5).</p>
<p>Definition of Complex Problem Solving</p>
<p>We can define complex problem solving from two perspectives: cognitive science and computational theory.Cognitive science investigates how humans use their inherent abilities to solve problems.Computational theory, by contrast, explores how to leverage machines for problem solving, emphasizing the design of algorithms to automate intricate computations.When considering the role of LLMs in addressing complex problems, two potential paradigms emerge: (1) Direct problem solving: LLMs autonomously generate solutions akin to human experts.1 ; (2) Leveraging computational systems for complex problems: LLMs extract and define the computational components of a problem, utilizing traditional computers to execute intensive calculations while focusing on designing solutions and orchestrating processes.With these paradigms in mind, this section will delve deeper into how CPS is defined in the frameworks of cognitive science and computational theory.</p>
<p>Definitions</p>
<p>Definition 1 (Problem).A problem Π(X, Y, P) is described by (1) a description of its parameters X, and (2) a statement (i.e., a predicate logic) P ( ;  ) that characterizes the properties the solution must satisfy.Formally, the goal set is defined as G = { ∈ Y | P ( ;  )}, where Y is the space of final results, and P is a predicate logic that means P ( ;  ) represents the property that a final result  should satisfy when X =  .An instance  of the problem is obtained by specifying particular values for all the problem parameters, i.e.,  := Π(X =  ).</p>
<p>A problem can be seen as a task that involves finding a solution from a set of possible candidates.The predicate P ( ;  ) specifies the condition that an answer must satisfy to be considered valid.In different problems, the predicate P ( ;  ) may either be well-defined or not.For instance, in the shortest path problem, the answer space Y consists of all possible paths, and the predicate P ( ;  ) is well-defined, specifying that a final result  (a path) must satisfy the property of having the minimum total weight.In contrast, in data mining tasks, the goal is to discover insightful patterns within the data.However, what constitutes an "insightful" pattern is not clearly defined, making the predicate P ( ;  ) more subjective and context-dependent.</p>
<p>Based on the definition of a problem, we can now formally define problem solving as the process of identifying a sequence of transformations that leads from an initial state to a goal state.Definition 2 (Problem Solving).Problem solving is the process of finding a solution trace  () ∈ T   ⊆ T for a problem instance , where T   is the set of all possible solution traces, formally defined as:
T 𝑓 𝑒𝑎𝑠𝑖𝑏𝑙𝑒 := {𝑋 → 𝑂 1 → . . . → 𝑂 𝜅 → 𝑌 | 𝑋 ∈ X, 𝑌 ∈ G, 𝜅 ∈ N + , ∀ 1≤𝑖 ≤𝜅 𝑂 𝑖 ∈ O},
, T is the set of all possible traces:
T := {𝑋 → 𝑂 1 → . . . → 𝑂 𝜅 → 𝑌 |𝑋 ∈ X, 𝑌 ∈ Y, 𝜅 ∈ N + , 𝑂 𝑖 ∈ O},
, and O is the set of all possible intermediate states during the problem-solving process.This definition emphasizes the iterative and state-dependent nature of problem solving, where intermediate states   capture the evolving understanding or partial solutions leading to the final result  .However, the mechanisms driving state transitions and the constraints on feasible solution traces vary depending on the nature of the problem solver.</p>
<p>In a human-centered perspective, problem solving is inherently constrained by individual cognitive capabilities.The transition from one state to another is influenced not only by logical reasoning but also by domain knowledge, prior experience, and real-time feedback.As a result, different individuals may follow different paths within T   based on their available cognitive resources.The formal definition is as follows: Definition 3 (Human-Centered Problem Solving).Human-centered Problem solving is the process of finding a solution trace  () for a problem instance  by a person with cognitive capabilities C, which include domain knowledge, logical reasoning, leveraging real-time feedback and other cognitive resources [67].The transition from an intermediate state   to the next state  +1 is governed by a cognition-guided transition function:
Γ : O × C → P (O),
where P (O) is the power set of O, representing all possible next states, and the transition function Γ(  , ) determines the set of feasible next states given the solver's cognitive capacity.</p>
<p>Conversely, in a computer-assisted perspective, problem solving is approached from the lens of computational theory.Here, state transitions are governed by formal algorithms rather than cognitive capabilities.Definition 4 (Computer-assisted Problem Solving).Problem solving is the process of designing algorithms A to solve a problem Π(X, Y, ).An algorithm is a finite sequence of instructions executable by a computer2 to produce solutions.Formally, an algorithm is defined as a quintuple:
A := (X, Y, O, 𝛿, 𝜎 0 ),
where X is the input space, describing all possible parameters of the problem; Y is the output space, representing all potential solutions; O is the state space, containing all possible states during the execution of the algorithm;  : X × O → O is the state transition function, specifying how the algorithm transitions from one state to the next based on the input;  0 ∈ O is the initial state, representing the starting condition of the algorithm.An algorithm is said to solve a problem Π if, for any instance  of Π, it guarantees to produce a solution  ∈ G that satisfies the predicate  ( ;  ).</p>
<p>By comparing the two definitions, we observe that they share a fundamental similarity: both focus on finding the steps to solve a problem.However, they differ in their emphasis.Humancentered Problem Solving mainly focuses on the process of solving a specific problem instance , while Computer-assisted Problem Solving emphasizes designing relatively general algorithms to address a class of problems Π.</p>
<p>Example</p>
<p>Let's use machine learning tasks as an example (Figure 3).Developing a high-quality machine learning model can be framed as a problem-solving process, where we seek a feasible solution trace  () ∈ T   .Each step in this process corresponds to a state transition   →  +1 , driven by reasoning, domain knowledge, and iterative evaluation.Initially, we define the problem by identifying the task and structuring it into a machine learning formulation.Then, we transition through intermediate states by analyzing data, applying preprocessing techniques, and performing feature engineering.Once the data is processed, we select suitable modeling techniques and develop the model for training.To refine these transitions, domain knowledge plays a crucial role, guiding the selection of appropriate models and training strategies.Knowledge may originate from historical approaches, theoretical research, or expert intuition, shaping the feasible state space T   .Developing effective machine learning models requires multiple rounds of evaluation for each method, including both human assessments and experimental evaluations.Since machine learning models rely on learning data distributions from training data to make predictions, assessing the quality of a solution solely by examining it is challenging.Instead, empirical validation through human assessments and experimental testing determines the effectiveness of a model before convergence to an optimal solution  .</p>
<p>Methodology</p>
<p>Figure 4 illustrates LLM-based techniques for complex problem solving.Current Chain-of-Thought (CoT) LLMs are trained through data synthesis.The process begins with generating CoT data, followed by selecting the correct CoT samples using a verifier for model training.During inference, the LLM generates multiple CoT solutions, and a verifier is used to identify the correct one for the given task.There are multiple approaches to synthesizing data.One method involves having the LLM generate CoT data autonomously, which requires the base model to be well-trained.For applications with limited training data, knowledge mining can be conducted on existing datasets to synthesize data, while human expertise can also be incorporated.Additionally, the mined knowledge can be injected into the LLM during inference rather than being solely used for training.Certain applications produce results that are difficult to verify, such as machine learning tasks.In such cases, multiple verification methods can be employed.Besides using an LLM-based verifier, symbolic verification and experimental evaluations can be conducted.Additionally, human experts may also be involved in the verification process.</p>
<p>Multi-step reasoning</p>
<p>Chain-of-thought reasoning with LLM has been proven to be effective for solving complex problems.This line of research started from [136] that shows chain-of-thought prompting with a few examples of reasoning paths can enhance the reasoning capabilities of LLM.[64] later demonstrates that chain-of-thought reasoning can improve performance in a zero-shot setting by simply encouraging LLM to generate intermediate reasoning steps with the "Let's think step by step" prompt.[132] shows that sampling multiple reasoning paths and using majority vote can further improve the performance of LLM in reasoning tasks.[155] introduces Tree of Thoughts (ToT) that allows LLMs to explore multiple reasoning paths over thoughts to improve the reasoning capabilities of LLMs.</p>
<p>We can utilize the architecture depicted in Figure 4 to improve chain-of-thought reasoning for tackling complex problems.When a question is posed, a generator powered by a large language model (LLM) produces multiple reasoning paths.These paths are then evaluated by a verifier to determine their accuracy.If some of the reasoning paths are validated as correct, they are used to formulate an answer to the question.However, if none of the paths are deemed correct, a corrector is employed to create new reasoning paths by modifying the incorrect ones and incorporating additional feedback from the verifier.In this approach, enhancing the likelihood of getting a correct solution for any given question requires improving two key metrics:</p>
<p>• coverage: the percentage of problems that can be solved using at least one of the generated reasoning paths, • precision: the probability of selecting the correct reasoning path from all generated paths.To boost coverage, we need to fine-tune both the generator and the corrector to increase the chances of producing a valid reasoning path.To enhance precision, the verifier must be fine-tuned to more accurately identify the correct path.</p>
<p>Generator.To refine the generator, we must move beyond human-produced data and instead synthesize data with reasoning paths.[159] present an iterative procedure that generates multiple reasoning paths, selects the correct ones, and uses them to further fine-tune the LLM, progressively improving its ability to produce accurate reasoning.Additionally, they introduce a "rationalization" technique that leverages the problem's answer as a hint to enhance the generation of reasoning paths.[113] takes a similar iterative approach of generating reasoning paths to fine-tune LLM and using fine-tuned LLM to generate more reasoning paths.The main difference is that this work generates reasoning paths by using temporature sampling to generate multiple paths and use a binary reward function to score them, while [159] uses greedy decoding to generate reasoning paths.Both works show that the LLM model overfits quickly with the generated data.[7] shows that it is not necessary to use a strong LLM to generate high-quality synthetic data and the models fine-tuned on data generated by weaker LLMs can consistently outperform those trained on data generated by stronger models.</p>
<p>Self-correction.We can use the incorrect reasoning paths from the previous attempt and the feedbacks from the verifier to increase the probability of generating a correct one in the next iteration.This process is considered as self-correction.[52] shows that the existing LLM, such as GPT-4 and Llama-2, cannot increase the probablity of generating right paths when being used for self-correction.They tend to decrease the probability of getting right solutions compared with standard prompting methods.This indicates that we need a specifically fine-tuned LLM for self-correction.Pair-SFT [139] trains an independent corrector model to refine the outputs of a generator model.They create a dataset consisting of response pairs (,  ′ ), where  is the initial response to a question and  ′ is the corrected version, to train the corrector.SCoRe [65] employs a reinforcement learning approach to train a single LLM that both generates the initial response and self-corrects it.They find that previous methods are ineffective due to distribution shifts or the amplification of biases from the base model.By using a single LLM for both response generation and correction, SCoRe avoids the distribution mismatch that arises between separate generator and corrector models.</p>
<p>Inference scaling law.It is costly to generate many reasoning paths and select a correct one.The more difficult a problem is, the more reasoning paths we may need to sample.A key research question here is how to use the compute resource wisely to find a right path for any given problem.[10] illustrates the scaling law for inference-time compute.They observe that the coverage grows nearly log-linearly with the number of samples generated from LLM and may reach to 100% coverage if many reasoning paths are generated.They further discover that it may be more costeffective to generate more samples with a weaker model than using larger LLMs when solving some simpler problems; however, stronger LLMs are preferred when solving more difficult problems.[114] investigates the "compute-optimal" strategy for scaling inference-time compute in multiple aspects when generating a right reasoning path.When using a reward model to search for a good reasoning path, they evaluated different search strategies, including best-of-N search, beam search and lookahead search, and conclude that beam search is preferable for harder problems and lower computational budgets, while best-of-N is more effective for easier problems and higher budgets.Another aspect is to update the proposal distribution of the generator model to increase the probability of generating a good reasoning path.An option is to generate multiple reasoning path in parallel, while the other is to use a fine-tuned LLM to iteratively revise their own answers, which results in a sequential test-time inference.They show that easier questions benefit from sequential inference while harder problems require some ratio of sequential to parallel inference.Compute resources can also be allocated to pre-training.To solve hard problems, some compute resources should be used for pre-training while for easier problems, we only need to use compute resources for inference.</p>
<p>Knowledge</p>
<p>Solving complex problems requires leveraging knowledge effectively.On one hand, many complex problem solving tasks are inherently domain-specific, and without specialized knowledge, solving them effectively becomes a challenge.On the other hand, the process of tackling these tasks involves multifaceted procedures, and existing large language models often lack the reliability and robustness required.Consequently, acquiring and enhancing this type of specialized knowledge represents a critical issue for effective complex problem solving.</p>
<p>To acquire this type of knowledge, the simplest and most direct approach is domain-specific pre-training [115].While LLMs acquire world knowledge during training, research has shown that they are unreliable in memorizing and applying such knowledge-particularly long-tail information [121]-to practical tasks, and multiple studies [38,106] indicate that they are unable to acquire new factual knowledge through supervised fine-tuning (SFT) after pre-training.Unlike these approaches, prompt-based enhancement techniques-such as RAG [36], GraphRAG [30,48], and KAG [78]-can directly embed domain knowledge into the context of a specific task.Building on this, many studies have explored acquiring such knowledge through methods like information extraction [141,167,168], constructing domain-specific knowledge graphs [24,169,181], or procedure extraction [158], as well as directly generating task-specific workflows with large language models [104,165] and refining them through human interactive feedback [9].The following sections introduce various works categorized by the type of knowledge they address.Domain Knowledge.Domain knowledge is designed to provide prior information for complex tasks, offering comprehensive introductions, detailed descriptions, and relevant background [18,42,102].[80] proposes a computational framework that enhances agents' problem-solving abilities by integrating a goal-directed dynamic knowledge generation mechanism.[126] introduces Knowledge-Driven Chain-of-Thought (KD-CoT), a framework that leverages external knowledge to verify and refine reasoning traces, thereby mitigating hallucinations and reducing error propagation.[73] introduces Chain-of-Knowledge (CoK), an innovative framework that enhances large language models by dynamically integrating grounding information from diverse sources.[101] proposes Physics Reasoner, a knowledge-augmented framework that leverages large language models to solve physics problems.</p>
<p>Procedure Knowledge.Procedure Knowledge refers to workflows or cognitive patterns designed to address complex problems, typically used to standardize and guide the reasoning processes of large models.Techniques like MoT [72] leverage synthetic and extracted high-quality thought processes as external memory, providing the model with superior problem-solving examples.Further, the BoT [150] paradigm introduces meta-buffers that store cross-task cognitive templates encompassing general reasoning patterns and knowledge structures, which can be reused and instantiated across various specific problems, thereby enabling efficient reasoning.Furthermore, methods like Expel [177] also involve collecting an experience pool through environmental interactions, wherein the model learns from similar experiences and contrasts successful and unsuccessful trajectories to gain new insights and improve task inference capabilities.[182] introduces KnowAgent, an approach that boosts the planning capabilities of LLMs by integrating explicit action knowledge.Additional research by [135,165] utilize workflows that selectively guide agents for complex problem solving.</p>
<p>Human-computer Interaction.Even with an external knowledge base, LLMs can still struggle with nuanced or domain-specific information, often lacking the deep contextual understanding that human experts possess.To address this, LLMs can collaborate with humans to bridge this gap by enabling humans to provide critical insights, ensuring that the LLM focuses on relevant information and refines its interpretations based on specialized knowledge.For instance, in tasks like legal or medical research, humans can guide LLMs to prioritize certain references or nuances that the model might overlook [4,55,117,140].To enable such human-LLM collaboration, we need to design intuitive, user-friendly interfaces that facilitate effective communication and interaction between humans and LLMs [26,174].These interfaces should enable effective bidirectional communication, where users can provide feedback, clarify ambiguous inputs, and track the LLM's reasoning in real-time.A well-designed interface fosters trust, enhances collaboration, and ensures that the LLM can be effectively used by both experts and non-experts alike.</p>
<p>Evaluation</p>
<p>When tackling complex problems, it is essential to evaluate the effectiveness of solutions to enhance the reliability of LLM-based systems and identify better approaches.Prior research [97,110] has demonstrated that LLMs are easily distracted by irrelevant information in mathematical reasoning.This suggests that LLMs may not truly grasp mathematical concepts but rather rely on pattern matching to generate responses.Additionally, [93] highlights that LLMs perform worse on rare tasks than on more frequent ones, even when the tasks share the same level of complexity.Moreover, LLMs are sensitive to the probability distribution of inputs and outputs in their training data (Internet text), even for deterministic tasks.These challenges become even more pronounced when LLMs are applied to domain-specific problems that are less commonly found online.To comprehensively assess solutions, multiple evaluation criteria-such as correctness and efficiency-may be considered.Ensuring that a solution meets practical requirements necessitates the development and integration of diverse evaluation techniques to effectively analyze LLM-generated solutions.</p>
<p>To improve chain-of-thought reasoning, we need a verifier model for selecting a right reasoning path.This was first demonstrated by [20] in solving math problems in GSM8K.This work shows that training a verifier model to select a right solution among multiple solutions can significantly improve the test solve rate compared with just fine-tuning LLM.Therefore, a key problem here is how to train a reliable verifier model to determine the right reasoning path.[81] shows the effectivenss of using process supervision to train a process reward model (PRM).This method first generates multiple reasoning paths for a problem and has human labelers to annotate labels for each individual step of the reasoning paths.This approach requires many human resources to prepare for the training data.[81] adopts active learning to reduce human labeling efforts.[127] proposes a method that eliminates the need for human labeling when training a PRM.To assess the correctness of intermediate steps in a reasoning path, they use a fine-tuned LLM to generate multiple subsequent reasoning paths from a given step.The correctness score for that step is then determined by the number of paths that successfully lead to the correct answer.[166] trains a generative model as a verifier and shows that a generative model outperforms a discriminative verifier.In addition, they show that training a single LLM for both generation and verification can outperform separate LLMs for generation and verification.</p>
<p>In addition to LLM-based verification, tools are employed to validate model outputs, mitigating hallucinations and enhancing accuracy.These verification methods can be broadly categorized into symbolic verification and experimental verification.</p>
<p>Symbolic verification uses formal methods to ensure the correctness of the outputs of LLMs.This includes generating executable code and verifying its syntax and semantics through compilation [14,34].Additionally, outputs are compared against knowledge bases or knowledge graphs to validate factual accuracy.These methods are especially effective for tasks that require logical consistency, such as mathematical proofs or domain-specific fact-checking [19].PAL [34] uses symbolic reasoning to interpret natural language problems and generate programs as intermediate steps.These programs are verified in runtime environments, like Python interpreters, ensuring that the logic and structure of the generated code are valid and executable.In mathematical reasoning, tools like those in [162] provide specialized interfaces for numerical computation, equation solving, and expression transformation.These interfaces allow the model to verify and correct each step, ensuring the correctness of the reasoning process, much like symbolic theorem proving.Factool [19] provides a flexible, domain-independent framework designed to identify factual errors.It enhances fact verification across domains by utilizing multiple verification tools, including search engines, academic databases, and code interpreters.</p>
<p>In contrast, experimental verification involves validating models through real-world testing and empirical experiments [12,40,76].This approach is useful when formal verification is impractical or when the goal is performance optimization.Models are tested in practical environments or simulations, with performance measured against benchmarks or competing solutions.In automated data science, frameworks like AutoKaggle [76] exemplify experimental verification.These models autonomously participate in Kaggle competitions, optimizing data analysis pipelines and achieving top-tier performance by iterating through real-world testing, model tuning, and comparative analysis.Grosnit et al. [40] orchestrates structured reasoning to automatically analyze and optimize solutions, while Li et al. [76] uses a multi-agent framework to generate, test, and refine models.</p>
<p>For critical applications, ensuring safety and robustness is critical when applying LLMs in highstakes or unpredictable environments, where incorrect outputs can lead to severe consequences.LLMs, while powerful, can generate unreliable or unsafe responses due to hallucinations, misinterpretations, or unexpected inputs.In this case, we should introduce human oversight to validate and correct outputs, ensuring safer and more reliable decision-making.For instance, in medical diagnosis, human experts can verify AI-generated treatment recommendations to avoid misdiagnoses or unsafe prescriptions [47? ].</p>
<p>Domains</p>
<p>This paper examines four domains of real-world applications where LLMs can be applied to solve complex problems in these domains: software engineering, mathematics, data science, and scientific research.We will discuss the challenges in these applications from the perspective of multi-step reasoning, knowledge integration and result verifications.</p>
<p>Software Engineering</p>
<p>This involves enabling LLMs to perform complex software engineering tasks with minimal human intervention.The core tasks in this domain are generally categorized into two major areas: code generation and code understanding.Code generation encompasses program synthesis [58,69,71,124,173], code translation [16,100,145], automatic program repair [53,59,107], and code optimization [28,144], where LLMs must produce functionally correct and efficient code that satisfies diverse specifications.Code understanding, on the other hand, focuses on analyzing and interpreting existing code, involving tasks such as source code summarization [70,133,163], code review [154], and code search [27,122].Although these tasks differ in goals, they both demand that LLMs deeply understand the syntax, semantics, and structure of codebases and reason across multiple levels of abstraction.</p>
<p>Solving complex software engineering tasks using LLMs presents several unique challenges.First, these tasks require multi-step reasoning, as software development often involves decomposing problems, maintaining contextual consistency across files or functions, and iteratively refining code.Second, knowledge integration is essential-LLMs must possess foundational programming knowledge (e.g., syntax, algorithms), domain-specific practices (e.g., tool usage, design patterns) as well as the large code repository.Third, result verification is nontrivial: generating syntactically correct code is insufficient; it must also compile, execute correctly, and meet performance goals.Unlike natural language tasks, software correctness can be formally tested, creating both an opportunity and a challenge for using execution feedback effectively.</p>
<p>To address these challenges, a variety of models and frameworks have been proposed.For program synthesis, methods such as Code Evol-Instruct [90] and OSS-INSTRUCT [137] enhance LLM capabilities through synthetic data generation and fine-tuning, while approaches like GraphCoder [84] and GALLa [176] inject structural representations (e.g., code graphs) to improve syntactic and semantic understanding.Feedback-based mechanisms-such as Self-Debugging [15], LDB [180], and RLTF [82]-use runtime outputs, compiler errors, or test cases to iteratively guide model refinement.In repository comprehension, tools like StarCoder2 [86], DeepSeek-Coder [43], SWE-GPT [91], and RepoCoder [164], CoCoMIC [25] and RepoFuse [79] utilize repository-level information, dependency graphs, and retrieval-augmented generation (RAG) to help models navigate large and interdependent codebases.For code optimization, frameworks like PIE-Problem [157] and SBLLM [35] introduce multi-programmer solution sets and evolutionary search strategies to help LLMs learn from diverse optimization techniques and refine code based on execution metrics.</p>
<p>Future work in automating software engineering will likely focus on three directions.First, building stronger reasoning-aware models that can generate and revise code through intermediate abstractions, such as pseudocode or symbolic plans.Second, enhancing long-context and memory mechanisms to handle complex repositories and cross-file reasoning.Third, incorporating closedloop feedback systems that integrate automated test generation, runtime profiling, and formal verification into the code generation process.By combining these approaches, we can expect LLMbased agents to evolve from basic code assistants into capable autonomous software engineers.</p>
<p>Mathematics</p>
<p>Mathematical reasoning has emerged as a crucial benchmark for evaluating the capabilities of LLMs, as it requires not only natural language understanding but also precise logical reasoning, symbolic manipulation, and deep domain knowledge [108,120].The main tasks in this field include arithmetic computation problems [45,56,83,151], math word problems (MWPs) [37,49,63,116], and automated theorem proving (ATP) [2,152].These tasks test core competencies such as computational accuracy, deductive reasoning, the ability to model real-world scenarios mathematically, and the application of formal mathematical knowledge.Together, they serve as a rigorous framework for assessing whether LLMs can go beyond surface-level language generation to engage in structured, rule-based problem solving.</p>
<p>However, solving mathematical problems presents unique challenges that distinguish it from other complex domains.One major challenge is multi-step reasoning, as many mathematical tasks require sequential and logically dependent operations where one misstep can derail the entire solution.Another critical challenge is knowledge integration-LLMs must not only understand abstract principles (e.g., induction), but also domain-specific concepts and theorems, and recognize when and how to apply them, especially in the graduate level and research.This requires retrieving and manipulating domain-specific knowledge that is usually long-tail knowledge for LLMs.A third challenge is result verification, especially in settings like theorem proving, where the correctness of a result can only be confirmed through human evaluations or rigorous formal checking.Recent studies [92] have shown that the current state-of-the-art LLMs generate correct final results but incorrect solutions in mathematical competitions.These challenges demand more than just fluent text generation-they require models to reason with precision, incorporate external tools or knowledge bases, and verify the correctness of multi-step solutions.</p>
<p>To address these challenges, recent research has introduced a range of specialized strategies and systems.For computational ability, models such as MathGLM [153] are pre-trained on progressively complex mathematical problems using curriculum learning, achieving superior accuracy even compared to larger general-purpose models.Prompting-based methods like MathPrompter [56] improve accuracy in arithmetic by generating and cross-verifying multiple solution paths.In reasoning tasks, symbolic integrations [151] with Prolog or proof assistants like Lean (e.g., LeanDojo [148], AlphaProof [22]) help bridge the gap between informal reasoning and formal logic to verify the mathematical reasoning generated by LLMs.In modeling and abstraction, efforts such as symbolic solvers for MWPs and autoformalization benchmarks (e.g., LeanEuclid) [98] illustrate how LLMs can map real-world problems or geometric reasoning into formal mathematical representations.Moreover, retrieval-augmented systems and knowledge-grounded toolkits like DOCMATH-EVAL [179] and LeanDojo [148] show that integrating structured mathematical knowledge significantly boosts performance in tasks that require prior theorems or domain-specific reasoning strategies.</p>
<p>Looking forward, future work in LLM-based mathematical reasoning may focus on deepening the model's ability to conduct formal reasoning with external feedback and process supervision.Developing hybrid frameworks that combine LLMs with theorem provers, symbolic execution engines, or even formal verification compilers could further enhance result correctness and logical soundness.Additionally, enriching LLMs with structured mathematical knowledge bases, improving their ability to retrieve relevant prior knowledge, and training them on fine-grained proof steps could enhance their capacity for advanced mathematical reasoning.Ultimately, achieving generalizable, verifiable, and domain-aware mathematical reasoning will be key to pushing LLMs closer to humanlevel mathematical understanding.</p>
<p>Data Science</p>
<p>This is a field where we perform data analysis and data modeling on a large amount of data [172].The main tasks in data science revolve around a complex, multi-stage pipeline that includes task understanding, data exploration and analysis, feature engineering, model selection, model training and evaluation.Each of these stages is interrelated, requiring not only technical execution but also careful reasoning and adaptation based on the input data.Unlike domains where problems are well-defined and static, data science demands continuous adjustments to explore the input data.</p>
<p>The unique challenges in this domain stem from its dynamic and data-dependent nature.First, multi-step reasoning is essential, as decisions made in early stages (e.g., feature extraction) significantly affect later ones (e.g., model performance).Second, effective solutions often require domain-specific knowledge that is not easily captured by general-purpose LLMs; integrating such knowledge is vital to handle real-world complexity.Third, verifying the quality of a solution is particularly difficult because the performance depends heavily on the input data rather than just problem descriptions.This makes it challenging to assess modeling strategies.</p>
<p>Current research efforts have made significant progress in addressing these challenges through the development of agent-based systems.Data Interpreter [50] introduces a graph-based agent that models dependencies between pipeline stages and automates code generation and refinement accordingly.AutoKaggle [76] employs a multi-agent framework-featuring specialized agents such as the Planner, Developer, and Reviewer-to provide end-to-end solutions for tabular data tasks, complete with iterative debugging and testing.Agent K [40] optimizes performance through learned memory mechanisms, using reinforcement signals to retain useful strategies for future tasks.Meanwhile, DS-Agent [44] takes a knowledge-based approach by building a repository of expert insights derived from Kaggle competitions and applying case-based reasoning to generate better solutions.These systems are benchmarked using platforms like DS-Bench [61], MLE-Bench [12], and MLAgentBench [54], which provide structured tasks rooted in real-world ML challenges to evaluate performance across the entire modeling pipeline.</p>
<p>Looking ahead, future research in this domain should focus on enhancing LLMs' ability to reason, adapt, and learn from data-driven experimentation.One key direction is the development of knowledge-enriched modeling agents that can incorporate advanced, domain-specific techniques beyond commonly used libraries.Another promising area is the integration of experiment-driven reasoning, enabling agents to iteratively test, evaluate, and refine their modeling strategies based on actual performance metrics.Finally, training LLMs with chain-of-thought (CoT) mechanisms that incorporate feedback loops from experiment results and domain-specific cues may offer a path toward more intelligent and adaptive data science agents.</p>
<p>Scientific Research</p>
<p>Artificial intelligence (AI) are increasingly playing a transformative role in scientific research, supporting tasks such as data analysis, simulation, literature review, and idea generation.Their application spans numerous domains, including biology, where tools like AlphaFold [62] and RoseTTAFold [6] revolutionized protein structure prediction; physics, where AI assist in accelerating particle simulations [66]; and astronomy, where they aid in exoplanet detection [94].In these contexts, LLMs are primarily used in scientific research in two ways: as tools that enhance human research capabilities and as co-creators that propose novel scientific hypotheses or ideas.</p>
<p>Despite these advances, the use of LLMs in scientific discovery presents several notable challenges.First, scientific research typically involves open-ended problems with unclear goals, making it difficult to apply LLMs in ways that guarantee accurate or verifiable solutions.Moreover, scientific research requires deep domain-specific knowledge, and LLMs must effectively leverage this expertise to make reliable predictions.These challenges make it difficult for LLMs to autonomously navigate the full research cycle, especially when the tasks involve open-ended reasoning, abstract synthesis, or interdisciplinary knowledge.</p>
<p>Due to the challenges of scientific research, LLMs are mainly used as tools to assist scientific tasks.For example, LLMs have been employed to accelerate data interpretation in fields like biomedicine and environmental science, where pre-trained models such as BioBERT and SciBERT help contextualize domain-specific data [8,57,68,85].In simulation and predictive modeling, LLMs have been applied to climate forecasting and molecular modeling, leveraging their world knowledge to support scenarios where traditional simulations may be limited [11].For literature review and synthesis, LLMs help researchers uncover trends and identify knowledge gaps by summarizing extensive textual corpora [8,60,85,88,109,123].More experimental efforts use LLMs for research idea generation-some studies show that LLMs can generate novel scientific ideas, but also highlight the difficulties of evaluating and selecting high-quality ideas, especially as LLMs themselves are not reliable evaluators [5,112,128,131].Additionally, agent-based systems like AI Scientist [87] and HEADS [118] demonstrate the feasibility of automating entire research pipelines, from idea generation to simulated peer review, though they fall short of validating these pipelines in solving truly difficult, real-world scientific problems.Future research will likely focus on improving the reliability and impact of LLMs in scientific discovery by integrating more rigorous evaluation mechanisms and enabling deeper domainspecific reasoning.One key direction is building multi-agent collaboration frameworks that mimic scientific team dynamics to diversify and refine generated ideas.Another is combining LLMs with external tools-such as experiment databases, simulation engines, or formal verification systems-to support result verification and reduce hallucinations.Finally, improving the feedback loop between LLM-generated outputs and human or experimental validation will be critical for realizing LLMs as trusted collaborators in the scientific process.These developments will help move from speculative generation toward verifiable, impactful contributions to scientific research.</p>
<p>Discussions and Future Directions</p>
<p>Although there has been notable progress in LLM research for solving complex problems, significant challenges persist.To further enhance LLMs' ability to tackle complex problems, we should focus on improving them from the three key perspectives: multi-step reasoning, knowledge and verification.</p>
<p>Multi-step reasoning.There are two major problems in training LLMs for multi-step reasoning: data scarcity and high computation cost.</p>
<p>CoT LLMs are typically pre-trained on vast amounts of Internet data, with further enhancement achieved through synthetic data generated by LLMs.However, data scarcity is still a challenge in many specialized domains.For instance, while widely-used programming languages like Python have large code corpora available online, lesser-known languages such as Lean [21] suffer from limited data.Although synthetic data generation via LLMs can improve LLMs, it relies on the base LLMs being well pre-trained for the specific domain.As a result, using data synthesis to improve an LLM's ability to generate code in languages like Lean remains a significant challenge.Similar issues arise in other fields, including mathematics and science.One approach to tackle the data scarcity problem is to develop agents that combine LLMs with custom models specifically trained for the target applications.For example, in formal theorem proving, where data is limited, custom models can help determine the applicability of mathematical strategies (tactics) and assess whether the proof has progressed towards the goal after each step [152].These models guide LLMs in making informed decisions with reinforcement learning [103,125,134], enhancing their reasoning capabilities even in domains with sparse data.</p>
<p>Another problem is high computation cost.The inference scaling law has been identified as a way to enhance LLMs' ability to tackle complex problems [17,32,119,129].By generating numerous reasoning paths, LLMs are more likely to find a path that leads to a solution for highly complex problems at the cost of increased computation.For example, GPT-o1 and its successor GPT-o3 exhibit significantly higher inference costs compared to GPT-4.Therefore, it is crucial to reduce inference costs.We can address the computational challenges from several angles.First, we can train better LLMs for generation and self-correction to reduce the number of trials to generate reasoning paths/tokens [? ].Second, we should explore various search algorithms to generate reasoning paths more efficiently.Besides best-of-N, we should also explore beam search and Monte Carlo Tree Search.Third, we can reduce the model size of LLMs to speed up inference, which includes techniques like distilling LLMs into smaller models and decoupling knowledge from LLMs to create more compact versions, thus reducing the computational requirements.</p>
<p>Knowledge.Knowledge is fundamental to solving complex problems.Currently, LLMs acquire world knowledge through next-token prediction on massive data during pre-training, which leads to several challenges.LLMs may not reliably memorize world knowledge, particularly long-tail knowledge [121].As a result, the current LLM cannot work well on domains where training data are scarce.Even if LLMs retain knowledge, they may struggle to recall relevant information when solving complex problems or LLMs may lack the ability to correctly apply knowledge to solve complex problems.</p>
<p>To effectively leverage knowledge in solving complex problems, one approach is to construct comprehensive knowledge graphs that go beyond traditional triplet-based structures, which contain only entities and relations.In the context of machine learning, a specialized knowledge graph should include not only verbal descriptions of techniques but also their mathematical formulations and corresponding implementation code.Additionally, it should capture relationships between different techniques to facilitate the exploration of diverse approaches and foster innovation in problem-solving.Such a knowledge graph can be systematically built by extracting information from academic papers, technical reports, and textbooks, with careful validation and verification [89].Once established, this knowledge graph can be utilized in two key ways.First, it can be used to synthesize data for model training, addressing data scarcity challenges.Second, it can support problem-solving during inference through a retrieval-augmented generation (RAG) approach, enabling the model to access and apply relevant knowledge in real time [46].</p>
<p>However, large language models still face challenges in representing and discovering knowledge.Their reliance on chain-of-thought reasoning for complex tasks is hindered by current serialized techniques, which struggle to structurally capture domain-specific knowledge and logic (workflow) while providing limited support for human intervention [95].Additionally, large language models encounter difficulties in balancing innovative knowledge discovery with logical credibility, often leading to hallucinatory outputs.Compounding these issues, the large language model' dynamic adaptation capabilities are insufficient to keep pace with rapidly changing environments, as delayed knowledge updates can render decision-making strategies ineffective.These interconnected challenges underscore the need for further research into improving thought process modeling, enhancing domain knowledge discovery, updating (editing) [29,130,156,170], and developing more robust adaptation mechanisms for complex problem solving [33,143].</p>
<p>Evaluation.The current LLM research, such as OpenAI o1, focuses on complex problems whose final results can be easily verified, such as competitive coding and mathematical reasoning.However, practical applications have much more complex requirements that complicate the verification of final results.First, some applications not only require correctness of solutions but also to achieve efficiency or yield better accuracy.In machine learning tasks, for example, while baseline methods like random forecasts or multilayer perceptrons can be considered as "correct" solutions, they may not meet the desired performance, and more effective solutions are preferred.Furthermore, problems in many applications are difficult to define comprehensively.Again using machine learning tasks as an example.Both the task description and the distribution of input data are essential for designing an effective solution.However, conveying the data distribution of the input data to an LLM is challenging.In addition, in certain scientific fields, such as drug discovery, climate modeling, or social sciences, validation of results often requires extensive experimental testing, replication, or further theoretical analysis to confirm their accuracy and reliability.</p>
<p>These challenges emphasize the need for robust evaluation frameworks and the integration of domain-specific expertise to ensure the reliability of LLM-generated outputs.To enhance the credibility of LLM outputs, it is crucial to employ multiple evaluation approaches.Consider machine learning tasks as an example-there are several ways to assess the effectiveness of a machine learning algorithm: First, the algorithm's performance can be evaluated by comparing it with previously published results, such as academic papers and technical reports.Secondly, an LLM-based evaluator can be utilized to assess the quality of a solution.To improve its accuracy, data analysis should be conducted to extract comprehensive insights from the input data and feed them to LLM.Thirdly, implementing the machine learning algorithm and conducting experiments provides an empirical assessment of its effectiveness.Fourth, for certain machine learning algorithms, we can perform some theoretical analysis on the algorithm, which is further verified by symbolic verification tools like Lean, ensuring rigorous validation of the algorithm's correctness and effectiveness.By combining all these different evaluation approaches, we can potentially perform thorough evaluation of machine learning algorithms.We believe similar evaluation principles (the combinations of LLM-based evaluations, empirical experiments, theoretical evaluations) can also be applied to other domains.</p>
<p>Related Works</p>
<p>Several survey papers have addressed LLM-based reasoning.Early works, such as Qiao et al. [105] and Huang and Chang [51], provide an overview of LLM-based reasoning, which is crucial for complex problem-solving.However, these surveys primarily focus on the initial developments in this area.With the release of GPT-o1 [99], the effectiveness of LLM-based reasoning was significantly demonstrated.Following this, numerous studies have explored the potential mechanisms behind GPT-o1.For example, Zeng et al. [160] and Xu et al. [142] delve into techniques that could enable o1-like reasoning, particularly through reinforcement learning.In contrast, this work takes a broader perspective, addressing all the different capabilities needed for complex problem-solving, rather than focusing solely on reasoning.</p>
<p>Numerous survey papers focus on specific domains of LLM-based reasoning.For instance, Yang et al. [147] examines the progress, challenges, and future directions in formal mathematical reasoning.Eger et al. [31] explores recent advances in using LLMs to support scientific research, covering applications such as literature search, idea generation, text and multimodal content (e.g., scientific figures and diagrams) generation, and AI-based peer review.Ahn et al. [3] provides an overview of various types of mathematical reasoning using LLMs.However, this work does not address o1-like techniques.Meanwhile, Li et al. [75] focuses on theorem proving within mathematical reasoning.Instead of solely relying on LLMs, this survey breaks down theorem proving into multiple components and discusses the application of various deep learning techniques for each aspect.</p>
<p>Conclusions</p>
<p>In this survey paper, we define complex problem-solving from the perspectives of both cognitive science and computational theory, and analyze the characteristics of different complex problems.We then investigate significant advancements in large language models (LLMs), with a focus on chain-of-thought reasoning and agent-based approaches in the context of complex problem-solving.We discuss how data synthesis and reinforcement learning have enhanced LLMs' abilities for multi-step reasoning.Additionally, we explore how the agent-based approach enables AI systems to harness external knowledge, tools for execution and result verification.However, we also examine the limitations of these methods when applied to different types of complex problems.</p>
<p>Fig. 4 .
4
Fig. 4. The loops of complex problem solving.</p>
<p>I can solve it by myself ! First, i need ... Then, i will ... Finally, .... First, i need ... Then, i will ... Finally, .... This can be done by code! Problem Solution Problem Code I can solve it by myself. Problem This can be done by code! C od in g I am de bu gg in g ... Code utils.py main.py utils.py main.py Fig</p>
<p>. 2. Two paradigms of problem solving by humans and AI.
Knowledge Augmented Complex Problem Solving with Large Language Models: A Survey111:5ProblemSolution(a) Cognitive science.(b) Computation theory.(c) Cognitive science within AI.(d) Computation theory within AI.
J. ACM, Vol.37, No. 4, Article 111.Publication date: August 2018.</p>
<p>J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.
+wln+: generate plans and manage the tools to solve the problems like human experts. (These components will be mentioned in the following methodologies and domains.)
Refers to a Turing machine or an equivalent computational architecture.</p>
<p>Large Language Models for Mathematical Reasoning: Progresses and Challenges. Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, Wenpeng Yin, Proceedings of the 18th Conference of the European Chapter. the 18th Conference of the European ChapterSt. Julian's, MaltaStudent Research Workshop2024. 2024</p>
<p>Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, Wenpeng Yin, arXiv:2402.00157Large language models for mathematical reasoning: Progresses and challenges. 2024. 2024arXiv preprint</p>
<p>Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, Dan Mané, arXiv:1606.06565Concrete problems in AI safety. 2016. 2016arXiv preprint</p>
<p>Knowledge Augmented Complex Problem Solving with Large Language Models: A Survey. J Acm, August 20183719Publication date</p>
<p>ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang, arXiv:2404.07738[cs.CL2024</p>
<p>Accurate prediction of protein structures and interactions using a three-track neural network. Minkyung Baek, Frank Dimaio, Ivan Anishchenko, Justas Dauparas, Sergey Ovchinnikov, Rie Gyu, Jue Lee, Qian Wang, Lisa N Cong, R Dustin Kinch, Claudia Schaeffer, Hahnbeom Millán, Carson Park, Caleb R Adams, Andy Glassman, Jose H Degiovanni, Andria V Pereira, Alberdina A Rodrigues, Ana C Van Dijk, Ebrecht, J Diederik, Theo Opperman, Christoph Sagmeister, Tea Buhlheller, Manoj K Pavkov-Keller, Udit Rathinaswamy, Calvin K Dalwadi, John E Yip, K Christopher Burke, Garcia, 10.1126/science.abj8754Science. Nick V. Grishin, Paul D. Adams, Randy J. Read, and David Baker3732021. 2021</p>
<p>Hritik Bansal, Arian Hosseini, Rishabh Agarwal, Q Vinh, Mehran Tran, Kazemi, arXiv:2408.16737[cs.CL]Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling. 2024</p>
<p>SciBERT: A pretrained language model for scientific text. Iz Beltagy, Kyle Lo, Arman Cohan, arXiv:1903.106762019. 2019arXiv preprint</p>
<p>The crowdless future? Generative AI and creative problem-solving. Léonard Boussioux, Jacqueline N Lane, Miaomiao Zhang, Vladimir Jacimovic, Karim R Lakhani, Organization Science. 352024. 2024</p>
<p>Large Language Monkeys: Scaling Inference Compute with Repeated Sampling. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V Le, Christopher Ré, Azalia Mirhoseini, arXiv:2407.21787[cs.LG]2024</p>
<p>Language models are few-shot learners. Tom B Brown, arXiv:2005.141652020. 2020arXiv preprint</p>
<p>Neil Jun Shern Chan, Oliver Chowdhury, James Jaffe, Dane Aung, Evan Sherburn, Giulio Mays, Kevin Starace, Leon Liu, Tejal Maksin, Lilian Patwardhan, Aleksander Weng, Madry, 10.48550/ARXIV.2410.07095arXiv:2410.07095MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering. 2024. 2024</p>
<p>Towards reasoning era: A survey of long chain-of-thought for reasoning large language models. Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, Wangxiang Che, arXiv:2503.095672025. 2025arXiv preprint</p>
<p>Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, Trans. Mach. Learn. Res. 20232023. 2023</p>
<p>Teaching large language models to self-debug. Xinyun Chen, Maxwell Lin, Nathanael Schärli, Denny Zhou, arXiv:2304.051282023. 2023arXiv preprint</p>
<p>Tree-to-tree neural networks for program translation. Xinyun Chen, Chang Liu, Dawn Song, Advances in neural information processing systems. 312018. 2018</p>
<p>Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, arXiv:2412.21187Do not think that much for 2+ 3=? on the overthinking of o1-like llms. 2024. 2024arXiv preprint</p>
<p>Knowprompt: Knowledge-aware prompt-tuning with synergistic optimization for relation extraction. Xiang Chen, Ningyu Zhang, Xin Xie, Shumin Deng, Yunzhi Yao, Chuanqi Tan, Fei Huang, Luo Si, Huajun Chen, Proceedings of the ACM Web conference 2022. the ACM Web conference 20222022</p>
<p>FacTool: Factuality Detection in Generative AI -A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios. I-Chun Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, Pengfei Liu, 10.48550/ARXIV.2307.13528arXiv:2307.135282023. 2023</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, arXiv:2110.14168[cs.LG]Training Verifiers to Solve Math Word Problems. 2021</p>
<p>The Lean Theorem Prover (System Description). Leonardo De Moura, Soonho Kong, Jeremy Avigad, Floris Van Doorn, Jakob Von Raumer, Automated Deduction -CADE-25, Amy P. Felty and Aart Middeldorp2015Springer International PublishingCham</p>
<p>AI solves IMO problems at silver medal level. Deepmind, 2024</p>
<p>Publication date. Deepseek-Ai , Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z F Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J L Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R J Chen, R L Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, S S Shuting Pan, Shuang Li, Shaoqing Zhou, Shengfeng Wu, Tao Ye, Tian Yun, Tianyu Pei, T Sun, Wangding Wang, Wanjia Zeng, Wen Zhao, Wenfeng Liu, Wenjun Liang, Wenqin Gao, Wentao Yu, W L Zhang, Wei Xiao, Xiaodong An, Xiaohan Liu, Xiaokang Wang, Xiaotao Chen, Xin Nie, Xin Cheng, Xin Liu, Xingchao Xie, Xinyu Liu, Xinyuan Yang, Xuecheng Li, Xuheng Su, X Q Lin, Xiangyue Li, Xiaojin Jin, Xiaosha Shen, Xiaowen Chen, Xiaoxiang Sun, Xinnan Wang, Xinyi Song, Xianzu Zhou, Xinxia Wang, Y K Shan, Y Q Li, Y X Wang, Yang Wei, Yanhong Zhang, Yao Xu, Yao Li, Yaofeng Zhao, Yaohui Sun, Yi Wang, Yichao Yu, Yifan Zhang, Yiliang Shi, Ying Xiong, Yishi He, Yisong Piao, Yixuan Wang, Yiyang Tan, Yiyuan Ma, Yongqiang Liu, Yuan Guo, Yuduan Ou, Yue Wang, Yuheng Gong, Yujia Zou, Yunfan He, Yuxiang Xiong, Yuxiang Luo, Yuxuan You, Yuyang Liu, Y X Zhou, Yanhong Zhu, Yanping Xu, Yaohui Huang, Yi Li, Yuchen Zheng, Yunxian Zhu, Ying Ma, Yukun Tang, Yuting Zha, Z Z Yan, Zehui Ren, Zhangli Ren, Zhe Sha, Zhean Fu, Zhenda Xu, Zhengyan Xie, Zhewen Zhang, Zhicheng Hao, Zhigang Ma, Zhiyu Yan, Zihui Wu, Zijia Gu, Zijun Zhu, Zilin Liu, Ziwei Li, Xie, arXiv:2501.12948[cs.CLJ. ACM. 374August 2018. 20Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</p>
<p>Automated construction of theme-specific knowledge graphs. Linyi Ding, Sizhe Zhou, Jinfeng Xiao, Jiawei Han, arXiv:2404.191462024. 2024arXiv preprint</p>
<p>Yangruibo Ding, Zijian Wang, Uddin Wasi, Murali Ahmad, Ramesh Krishna Ramanathan, Parminder Nallapati, Dan Bhatia, Bing Roth, Xiang, arXiv:2212.10007Cocomic: Code completion by jointly modeling in-file and cross-file context. 2022. 2022arXiv preprint</p>
<p>Jin Hyo, Rachel Do, Justin D Ostrand, Casey Weisz, Prasanna Dugan, Dennis Sattigeri, Keerthiram Wei, Werner Murugesan, Geyer, arXiv:2405.20434Facilitating Human-LLM Collaboration through Factuality Scores and Source Attributions. 2024. 2024arXiv preprint</p>
<p>Is a single model enough? mucos: A multi-model ensemble learning approach for semantic code search. Lun Du, Xiaozhou Shi, Yanlin Wang, Ensheng Shi, Shi Han, Dongmei Zhang, Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management. the 30th ACM International Conference on Information &amp; Knowledge Management2021</p>
<p>Mercury: An efficiency benchmark for llm code synthesis. Mingzhe Du, Anh Tuan Luu, Bin Ji, See-Kiong Ng, arXiv:2402.078442024. 2024arXiv preprint</p>
<p>Yiming Du, Wenyu Huang, Danna Zheng, Zhaowei Wang, Sebastien Montella, Mirella Lapata, Kam-Fai Wong, Jeff Z Pan, arXiv:2505.00675Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions. 2025. 2025arXiv preprint</p>
<p>From local to global: A graph rag approach to query-focused summarization. Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropolitansky, Robert Osazuwa Ness, Jonathan Larson, arXiv:2404.161302024. 2024arXiv preprint</p>
<p>Steffen Eger, Yong Cao, D' Jennifer, Andreas Souza, Christian Geiger, Stephanie Greisinger, Yufang Gross, Brigitte Hou, Anne Krenn, Yizhi Lauscher, Chenghua Li, Nafise Sadat Lin, Wei Moosavi, Tristan Zhao, Miller, arXiv:2502.05151[cs.CLTransforming Science with Large Language Models: A Survey on AI-assisted Scientific Discovery, Experimentation, Content Generation, and Evaluation. 2025</p>
<p>Sicheng Feng, Gongfan Fang, Xinyin Ma, Xinchao Wang, arXiv:2504.10903Efficient Reasoning Models: A Survey. 2025. 2025arXiv preprint</p>
<p>Dayuan Fu, Keqing He, Yejie Wang, Wentao Hong, Zhuoma Gongque, Weihao Zeng, Wei Wang, Jingang Wang, Xunliang Cai, Weiran Xu, arXiv:2501.01702AgentRefine: Enhancing Agent Generalization through Refinement Tuning. 2025. 2025arXiv preprint</p>
<p>PAL: Program-aided Language Models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, International Conference on Machine Learning, ICML 2023. Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, Jonathan Scarlett, Honolulu, Hawaii, USAPMLR2023. July 2023202Proceedings of Machine Learning Research</p>
<p>Search-based llms for code optimization. Shuzheng Gao, Cuiyun Gao, Wenchao Gu, Michael Lyu, 2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE). IEEE Computer Society2024</p>
<p>Retrieval-augmented generation for large language models: A survey. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Haofen Wang, Haofen Wang, arXiv:2312.109972023. 20232arXiv preprint</p>
<p>Reasoning in Large Language Models Through Symbolic Math Word Problems. Vedant Gaur, Nikunj Saunshi, Findings of the Association for Computational Linguistics: ACL 2023. Association for Computational Linguistics2023. August 20183721J. ACM</p>
<p>. Canada Toronto, 10.18653/v1/2023.findings-acl.364</p>
<p>Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?. Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, Jonathan Herzig, 10.18653/v1/2024.emnlp-main.444Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. Mohit Al-Onaizan, Yun-Nung Bansal, Chen, the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>Elliot Glazer, Ege Erdil, Tamay Besiroglu, Diego Chicharro, Evan Chen, Alex Gunning, Caroline Falkman Olsson, Jean-Stanislas Denain, arXiv:2411.04872Frontiermath: A benchmark for evaluating advanced mathematical reasoning in ai. Anson Ho, Emily de Oliveira Santos2024arXiv preprintet al. 2024</p>
<p>Antoine Grosnit, Alexandre Maraval, James Doran, Giuseppe Paolo, Albert Thomas, Refinath , Shahul Hameed, Nabeezath Beevi, Jonas Gonzalez, Khyati Khandelwal, Ignacio Iacobacci, Abdelhakim Benechehab, Hamza Cherkaoui, Youssef Attia El-Hili, Kun Shao, Jianye Hao, arXiv:2411.03562[cs.LGLarge Language Models Orchestrating Structured Reasoning Achieve Kaggle Grandmaster Level. Jun Yao, Balazs Kegl, Haitham Bou-Ammar, and Jun Wang. 2024</p>
<p>Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, Saizhuo Wang, Kun Zhang, Yuanzhuo Wang, Wen Gao, arXiv:2411.15594[cs.CLLionel Ni, and Jian Guo. 2025. A Survey on LLM-as-a-Judge. </p>
<p>Xiezhi: An ever-updating benchmark for holistic domain knowledge evaluation. Zhouhong Gu, Xiaoxuan Zhu, Haoning Ye, Lin Zhang, Jianchen Wang, Yixin Zhu, Sihang Jiang, Zhuozhi Xiong, Zihan Li, Weijie Wu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, Li, arXiv:2401.14196DeepSeek-Coder: When the Large Language Model Meets Programming-The Rise of Code Intelligence. 2024. 2024arXiv preprint</p>
<p>DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning. Siyuan Guo, Cheng Deng, Ying Wen, Hechang Chen, Yi Chang, Jun Wang, arXiv:2402.17453[cs.LG2024</p>
<p>ArthModel: Enhance Arithmetic Skills to Large Language Model. Yingdi Guo, arXiv:2311.186092023. 2023arXiv preprint</p>
<p>Lightrag: Simple and fast retrieval-augmented generation. Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, Chao Huang, 2024. 2024</p>
<p>The Need for Guardrails with Large Language Models in Medical Safety-Critical Settings: An Artificial Intelligence Application in the Pharmacovigilance Ecosystem. Jeffery L Joe B Hakim, Darmendra Painter, Vijay Ramcharran, Greg Kara, Paulina Powell, Chiho Sobczak, Andrew Sato, Andrew Bate, Beam, arXiv:2407.18322[cs.CL2024</p>
<p>Retrieval-augmented generation with graphs (graphrag). Haoyu Han, Yu Wang, Harry Shomer, Kai Guo, Jiayuan Ding, Yongjia Lei, Mahantesh Halappanavar, Ryan A Rossi, Subhabrata Mukherjee, Xianfeng Tang, arXiv:2501.003092024. 2024arXiv preprint</p>
<p>Yue He, arXiv:2304.09102Solving Math Word Problems by Combining Language Models With Symbolic Solvers. 2023. 2023arXiv preprint</p>
<p>Sirui Hong, Yizhang Lin, Bang Liu, Bangbang Liu, Binhao Wu, Danyang Li, Jiaqi Chen, Jiayi Zhang, Jinlin Wang, Li Zhang, Lingyao Zhang, Min Yang, Mingchen Zhuge, Taicheng Guo, Tuo Zhou, Wei Tao, Wenyi Wang, Xiangru Tang, Xiangtao Lu, Xiawu Zheng, Xinbing Liang, Yaying Fei, Yuheng Cheng, Zongze Xu, Chenglin Wu, arXiv:2402.18679[cs.AI]Data Interpreter: An LLM Agent For Data Science. 2024</p>
<p>Towards Reasoning in Large Language Models: A Survey. Jie Huang, Kevin Chen, -Chuan Chang, 10.18653/v1/2023.findings-acl.67Findings of the Association for Computational Linguistics: ACL 2023. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Large Language Models Cannot Self-Correct Reasoning Yet. Jie Huang, Xinyun Chen, Swaroop Mishra, Steven Huaixiu, Adams Wei Zheng, Xinying Yu, Denny Song, Zhou, The Twelfth International Conference on Learning Representations. 2024</p>
<p>An Empirical Study on Fine-Tuning Large Language Models of Code for Automated Program Repair. Kai Huang, Xiangxin Meng, Jian Zhang, Yang Liu, Wenjie Wang, Shuhao Li, Yuqing Zhang, 38th IEEE/ACM International Conference on Automated Software Engineering (ASE). 2023. 2023. 2023265054960</p>
<p>MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation. Qian Huang, Jian Vora, Percy Liang, Jure Leskovec, Forty-first International Conference on Machine Learning, ICML 2024. Vienna, Austria2024. July 21-27, 2024. August 201837Publication date</p>
<p>Human as AI mentor: Enhanced human-in-the-loop reinforcement learning for safe and efficient autonomous driving. Zilin Huang, Zihao Sheng, Chengyuan Ma, Sikai Chen, 10.1016/j.commtr.2024.100127Communications in Transportation Research. 41001272024. Dec. 2024</p>
<p>Shima Imani, Liang Du, Harsh Shrivastava, arXiv:2303.05398Mathprompter: Mathematical reasoning using large language models. 2023. 2023arXiv preprint</p>
<p>Leveraging large language models for data analysis automation. Jacqueline A Jansen, Artür Manukyan, Nour Al Khoury, Altuna Akalin, bioRxiv. 2023. 2023</p>
<p>A Survey on Large Language Models for Code Generation. Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, Sunghun Kim, arXiv:2406.005152024. 2024arXiv preprint</p>
<p>Impact of Code Language Models on Automated Program Repair. Nan Jiang, Kevin Liu, Thibaud Lutellier, Lin Tan, IEEE/ACM 45th International Conference on Software Engineering (ICSE). 2023. 2023. 2023256808267</p>
<p>Qiao Jin, Robert Leaman, Zhiyong Lu, arXiv:2307.09683PubMed and Beyond: Biomedical Literature Search in the Age of Artificial Intelligence. 2023. 2023arXiv preprint</p>
<p>DSBench: How Far Are Data Science Agents to Becoming Data Science Experts?. Liqiang Jing, Zhehui Huang, Xiaoyang Wang, Wenlin Yao, Wenhao Yu, Kaixin Ma, Hongming Zhang, Xinya Du, Dong Yu, 10.48550/ARXIV.2409.07703arXiv:2409.077032024. 2024</p>
<p>Pushmeet Kohli, and Demis Hassabis. 2021. Highly accurate protein structure prediction with AlphaFold. John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, Alex Bridgland, Clemens Meyer, A A Simon, Andrew J Kohl, Andrew Ballard, Bernardino Cowie, Stanislav Romera-Paredes, Rishub Nikolov, Jonas Jain, Trevor Adler, Stig Back, David Petersen, Ellen Reiman, Michal Clancy, Martin Zielinski, Michalina Steinegger, Tamas Pacholska, Sebastian Berghammer, David Bodenstein, Oriol Silver, Andrew W Vinyals, Koray Senior, Kavukcuoglu, 10.1038/s41586-021-03819-2Nature. 5962021</p>
<p>Can LLMs Solve Longer Math Word Problems Better? AI Models. Lee Kim, 2023. 2023</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Proceedings of the 36th International Conference on Neural Information Processing Systems. the 36th International Conference on Neural Information Processing SystemsNew Orleans, LA, USA2024. 161315</p>
<p>Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, M Lei, Kay Zhang, Disha Mckinney, Cosmin Shrivastava, George Paduraru, Tucker, arXiv:2409.12917[cs.LG]Doina Precup, Feryal Behbahani, and Aleksandra Faust. 2024. Training Language Models to Self-Correct via Reinforcement Learning. </p>
<p>Krishna Kumar, Yongjin Choi, arXiv:2309.13348Accelerating Particle and Fluid Simulations with Differentiable Graph Networks for Solving Forward and Inverse Problems. 2023physics.geo-ph</p>
<p>A gentle introduction to Soar, an architecture for human cognition. Je Laird, Lehman, Rosenbloom, 1996. 1996QYLWDWLRQ WR &amp;RJQLWLYH 6FLHQFH</p>
<p>BioBERT: a pre-trained biomedical language representation model for biomedical text mining. Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, Jaewoo Kang, Bioinformatics. 362020. 2020</p>
<p>Structured chain-of-thought prompting for code generation. Jia Li, Ge Li, Yongmin Li, Zhi Jin, ACM Transactions on Software Engineering and Methodology. 342025. 2025</p>
<p>Kaixin Li, Ziyang Meng, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang, Tat-Seng Chua, ScreenSpot-Pro: GUI Grounding for Professional High-Resolution Computer Use. 2025</p>
<p>Mmcode: Evaluating multi-modal code large language models with visually rich programming problems. Kaixin Li, Yuchen Tian, Qisheng Hu, Ziyang Luo, Jing Ma, arXiv:2404.094862024. 2024arXiv preprint</p>
<p>MoT: Memory-of-Thought Enables ChatGPT to Self-Improve. Xiaonan Li, Xipeng Qiu, 10.18653/V1/2023.EMNLP-MAIN.392Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023. 2023. December 6-10, 2023</p>
<p>Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources. Xingxuan Li, Ruochen Zhao, Ken Yew, Bosheng Chia, Shafiq Ding, Soujanya Joty, Lidong Poria, Bing, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Limr: Less is more for rl scaling. Xuefeng Li, Haoyang Zou, Pengfei Liu, arXiv:2502.118862025. 2025arXiv preprint</p>
<p>Knowledge Augmented Complex Problem Solving with Large Language Models: A Survey. J Acm, August 20183723Publication date</p>
<p>Zhaoyu Li, Jialiang Sun, Logan Murphy, Qidong Su, Zenan Li, Xian Zhang, Kaiyu Yang, Xujie Si, arXiv:2404.09939[cs.AIA Survey on Deep Learning for Theorem Proving. 2024</p>
<p>AutoKaggle: A Multi-Agent Framework for Autonomous Data Science Competitions. Ziming Li, Qianbo Zang, David Ma, Jiawei Guo, Tuney Zheng, Minghao Liu, Xinyao Niu, Yue Wang, Jian Yang, Jiaheng Liu, Wanjun Zhong, Wangchunshu Zhou, Wenhao Huang, Ge Zhang, 10.48550/ARXIV.2410.20424arXiv:2410.204242024. 2024</p>
<p>Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, arXiv:2502.17419From system 1 to system 2: A survey of reasoning large language models. 2025. 2025arXiv preprint</p>
<p>Kag: Boosting llms in professional domains via knowledge augmented generation. Lei Liang, Mengshu Sun, Zhengke Gui, Zhongshu Zhu, Zhouyu Jiang, Ling Zhong, Yuan Qu, Peilong Zhao, Zhongpu Bo, Jin Yang, arXiv:2409.137312024. 2024arXiv preprint</p>
<p>Ming Liang, Xiaoheng Xie, Gehao Zhang, Xunjin Zheng, Peng Di, Hongwei Chen, Chengpeng Wang, Gang Fan, arXiv:2402.14323REPOFUSE: Repository-Level Code Completion with Fused Dual Context. 2024. 2024arXiv preprint</p>
<p>Beyond subgoaling: A dynamic knowledge generation framework for creative problem solving in cognitive architectures. Antonio Lieto, Federico Perrone, Gian Luca Pozzato, Eleonora Chiodino, Cognitive Systems Research. 582019. 2019</p>
<p>Let's Verify Step by Step. Vineet Hunter Lightman, Yuri Kosaraju, Harrison Burda, John Edwards ; Leike, Ilya Schulman, Karl Sutskever, Cobbe, The Twelfth International Conference on Learning Representations. Bowen Baker, Teddy LeeJan. 2024</p>
<p>Jiate Liu, Yiqin Zhu, Kaiwen Xiao, Qiang Fu, Xiao Han, Wei Yang, Deheng Ye, arXiv:2307.04349Rltf: Reinforcement learning from unit test feedback. 2023. 2023arXiv preprint</p>
<p>Tiedong Liu, Bryan Kian, Hsiang Low, arXiv:2305.14201Goat: Fine-tuned llama outperforms gpt-4 on arithmetic tasks. 2023. 2023arXiv preprint</p>
<p>GraphCoder: Enhancing Repository-Level Code Completion via Code Context Graph-based Retrieval and Language Model. Wei Liu, Ailun Yu, Daoguang Zan, Bo Shen, Wei Zhang, Haiyan Zhao, Zhi Jin, Qianxiang Wang, arXiv:2406.070032024. 2024arXiv preprint</p>
<p>Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, Dan S Weld, arXiv:1911.02782S2ORC: The semantic scholar open research corpus. 2019. 2019arXiv preprint</p>
<p>Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, arXiv:2402.19173Starcoder 2 and the stack v2: The next generation. 2024. 2024arXiv preprint</p>
<p>Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.06292[cs.AIThe AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery. 2024</p>
<p>Large language models surpass human experts in predicting neuroscience results. Xiaoliang Luo, Akilles Rechardt, Guangzhi Sun, Kevin K Nejad, Felipe Yáñez, Bati Yilmaz, Kangjoo Lee, Alexandra O Cohen, Valentina Borghesani, Anton Pashkov, Nature Human Behaviour. 2024. 2024</p>
<p>Yujie Luo, Xiangyuan Ru, Kangwei Liu, Lin Yuan, Mengshu Sun, Ningyu Zhang, Lei Liang, Zhiqiang Zhang, Jun Zhou, Lanning Wei, arXiv:2412.20005OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System. 2024. 2024arXiv preprint</p>
<p>Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, Daxin Jiang, arXiv:2306.08568Wizardcoder: Empowering code large language models with evol-instruct. 2023. 2023arXiv preprint</p>
<p>Yingwei Ma, Rongyu Cao, Yongchang Cao, Yue Zhang, Jue Chen, Yibo Liu, Yuchen Liu, Binhua Li, Fei Huang, Yongbin Li, arXiv:2411.00622Lingma SWE-GPT: An Open Development-Process-Centric Language Model for Automated Software Improvement. 2024. 2024arXiv preprint</p>
<p>Hamed Mahdavi, Alireza Hashemi, Majid Daliri, Pegah Mohammadipour, Alireza Farhadi, Samira Malek, Yekta Yazdanifard, Amir Khasahmadi, Vasant Honavar, arXiv:2504.01995Brains vs. Bytes: Evaluating LLM Proficiency in Olympiad Mathematics. 2025. 2025arXiv preprint</p>
<p>Embers of Autoregression: Understanding Large Language Models Through the Problem They are Trained to Solve. R Thomas Mccoy, Shunyu Yao, Dan Friedman, Matthew Hardy, Thomas L Griffiths, arXiv:2309.13638[cs.CL2023</p>
<p>AI for Astronomy. Qinghai Miao, Fei-Yue Wang, Artificial Intelligence for Science (AI4S) Frontiers and Perspectives Based on Parallel Intelligence. Springer2024</p>
<p>So Yeon, Min , Yue Wu, Jimin Sun, Max Kaufmann, Fahim Tajwar, Yonatan Bisk, Ruslan Salakhutdinov, arXiv:2502.04576Self-Regulation and Requesting Interventions. 2025. 2025arXiv preprint</p>
<p>Publication date. J Acm, August 201837</p>
<p>Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue Cheng, Huatong Song, arXiv:2412.09413Imitate, explore, and self-improve: A reproduction report on slow-thinking reasoning systems. 2024. 2024arXiv preprint</p>
<p>Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, arXiv:2410.05229[cs.LGOncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. 2024. GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models. </p>
<p>Logan Murphy, Kaiyu Yang, Jialiang Sun, Zhaoyu Li, Anima Anandkumar, Xujie Si, arXiv:2405.17216Autoformalizing Euclidean Geometry. 2024. 2024arXiv preprint</p>
<p>OpenAI o1 Guide: How It Works, Use Cases, API &amp; More. 2024. 2024OpenAI</p>
<p>Understanding the effectiveness of large language models in code translation. Rangeet Pan, Reza Ali, Rahul Ibrahimzada, Divya Krishna, Lambert Sankar, Michele Pouguem Wassi, Boris Merler, Raju Sobolev, Saurabh Pavuluri, Reyhaneh Sinha, Jabbarvand, arXiv:2308.031092023. 2023arXiv preprint</p>
<p>Xinyu Pang, Ruixin Hong, Zhanke Zhou, Fangrui Lv, Xinwei Yang, Zhilong Liang, Bo Han, Changshui Zhang, arXiv:2412.13791Physics Reasoner: Knowledge-Augmented Reasoning for Solving Physics Problems with Large Language Models. 2024. 2024arXiv preprint</p>
<p>Domain knowledge matters: Improving prompts with fix templates for repairing python type errors. Yun Peng, Shuzheng Gao, Cuiyun Gao, Yintong Huo, Michael Lyu, Proceedings of the 46th ieee/acm international conference on software engineering. the 46th ieee/acm international conference on software engineering2024</p>
<p>Emre Cheng Qian, Qi Can Acikgoz, Hongru He, Xiusi Wang, Dilek Chen, Gokhan Hakkani-Tür, Heng Tur, Ji, arXiv:2504.13958ToolRL: Reward is All Tool Learning Needs. 2025. 2025arXiv preprint</p>
<p>. Shuofei Qiao, Runnan Fang, Zhisong Qiu, Xiaobin Wang, Ningyu Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen, arXiv:2410.078692024. 2024Benchmarking Agentic Workflow Generation. arXiv preprint</p>
<p>Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Huajun Chen, arXiv:2212.09597[cs.CLReasoning with Language Model Prompting: A Survey. 2023</p>
<p>Learning or Self-aligning? Rethinking Instruction Fine-tuning. Mengjie Ren, Boxi Cao, Hongyu Lin, Cao Liu, Xianpei Han, Ke Zeng, Guanglu Wan, Xunliang Cai, Le Sun, arXiv:2402.18243[cs.CL2024</p>
<p>GPT-3-Powered Type Error Debugging: Investigating the Use of Large Language Models for Code Repair. Francisco Ribeiro, José Nuno Macedo, Kanae Tsushima, Rui Abreu, João Saraiva, Proceedings of the 16th ACM SIGPLAN International Conference on Software Language Engineering. the 16th ACM SIGPLAN International Conference on Software Language Engineering2023. 2023264306954</p>
<p>Mathematical discoveries from program search with large language models. Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, Pawan Kumar, Emilien Dupont, Francisco Jr Ruiz, Jordan S Ellenberg, Pengming Wang, Omar Fawzi, Nature. 6252024. 2024</p>
<p>The emergence of Large Language Models (LLM) as a tool in literature reviews: an LLM automated systematic review. Dmitry Scherbakov, Nina Hubig, Vinita Jansari, Alexander Bakumenko, Leslie A Lenert, arXiv:2409.046002024. 2024arXiv preprint</p>
<p>Large Language Models Can Be Easily Distracted by Irrelevant Context. Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Chi, Nathanael Schärli, Denny Zhou, arXiv:2302.00093[cs.CL2023</p>
<p>Learning performance-improving code edits. Alexander Shypula, Aman Madaan, Yimeng Zeng, Uri Alon, Jacob Gardner, Milad Hashemi, Graham Neubig, Osbert Parthasarathy Ranganathan, Amir Bastani, Yazdanbakhsh, arXiv:2302.078672023. 2023arXiv preprint</p>
<p>Chenglei Si, Diyi Yang, Tatsunori Hashimoto, arXiv:2409.04109[cs.CL]Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers. 2024</p>
<p>Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models. Avi Singh, John D Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter J Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron T Parisi, Abhishek Kumar, Alexander A Alemi, Alex Rizkowsky, Azade Nova, Ben Adlam, Bernd Bohnet, Gamaleldin Fathy Elsayed, Hanie Sedghi, Igor Mordatch, Isabelle Simpson, Izzeddin Gur, Jasper Snoek, Jeffrey Pennington, Jiri Hron, Kathleen Kenealy, Kevin Swersky, Kshiteej Mahajan, Laura A Culp, Lechao Xiao, Maxwell Bileschi, Transactions on Machine Learning Research. Noah Constant, Roman Novak, Rosanne Liu, Tris Warkentin, Yamini Bansal, Ethan Dyer, Behnam Neyshabur, Jascha Sohl-Dickstein, and Noah Fiedel2024. 2024</p>
<p>Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar, arXiv:2408.03314[cs.LG]Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters. 2024</p>
<p>Knowledge Augmented Complex Problem Solving with Large Language Models: A Survey. J Acm, August 20183725Publication date</p>
<p>Zirui Song, Bin Yan, Yuhan Liu, Miao Fang, Mingzhe Li, Rui Yan, Xiuying Chen, arXiv:2502.10708Injecting Domain-Specific Knowledge into Large Language Models: A Comprehensive Survey. 2025. 2025arXiv preprint</p>
<p>What Makes Math Word Problems Challenging for LLMs. Aditya Kv, Ekaterina Srivatsa, Kochmar, 10.18653/v1/2024.findings-naacl.72Findings of the Association for Computational Linguistics: NAACL 2024. Mexico City, MexicoAssociation for Computational Linguistics2024</p>
<p>Learning to summarize with human feedback. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul F Christiano, Advances in Neural Information Processing Systems. 332020. 2020</p>
<p>Haoyang Su, Renqi Chen, Shixiang Tang, Xinzhe Zheng, Jingzhe Li, Zhenfei Yin, Wanli Ouyang, Nanqing Dong, arXiv:2410.09403[cs.AITwo Heads Are Better Than One: A Multi-Agent System Has the Potential to Improve Scientific Idea Generation. 2024</p>
<p>Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Hanjie Chen, Xia Hu, arXiv:2503.16419Stop overthinking: A survey on efficient reasoning for large language models. 2025. 2025arXiv preprint</p>
<p>Challenging the Boundaries of Reasoning: An Olympiad-Level Math Benchmark for Large Language Models. Haoxiang Sun, Yingqian Min, Zhipeng Chen, Wayne Xin Zhao, Zheng Liu, Zhongyuan Wang, Lei Fang, Ji-Rong Wen, arXiv:2503.213802025. 2025arXiv preprint</p>
<p>Head-to-Tail: How Knowledgeable are Large Language Models (LLMs)? A.K.A. Will LLMs Replace Knowledge Graphs?. Kai Sun, Ethan Yifan, Hanwen Xu, Yue Zha, Xin Liu, Dong Luna, arXiv:2308.10168[cs.CL2024</p>
<p>A Survey of Source Code Search: A 3-Dimensional Perspective. Weisong Sun, Chunrong Fang, Yifei Ge, Yuling Hu, Yuchen Chen, Quanjun Zhang, Xiuting Ge, Yang Liu, Zhenyu Chen, 10.1145/3656341ACM Trans. Softw. Eng. Methodol. 33166512024. June 2024</p>
<p>Automating research synthesis with domain-specific large language model fine-tuning. Teo Susnjak, Peter Hwang, Napoleon H Reyes, Andre Lc Barczak, Timothy R Mcintosh, Surangika Ranathunga, arXiv:2404.086802024. 2024arXiv preprint</p>
<p>Yuchen Tian, Weixiang Yan, Qian Yang, Qian Chen, Wen Wang, Ziyang Luo, Lei Ma, arXiv:2405.00253CodeHalu: Code Hallucinations in LLMs Driven by Execution-based Verification. 2024. 2024arXiv preprint</p>
<p>Hongru Wang, Cheng Qian, Wanjun Zhong, Xiusi Chen, Jiahao Qiu, Shijue Huang, Bowen Jin, Mengdi Wang, Kam-Fai Wong, Heng Ji, arXiv:2504.14870OTC: Optimal Tool Calls via Reinforcement Learning. 2025. 2025arXiv preprint</p>
<p>Knowledge-driven cot: Exploring faithful reasoning in llms for knowledge-intensive question answering. Keheng Wang, Feiyu Duan, Sirui Wang, Peiguang Li, Yunsen Xian, Chuantao Yin, Wenge Rong, Zhang Xiong, arXiv:2308.132592023. 2023arXiv preprint</p>
<p>Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations. Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, Zhifang Sui, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. Lun-Wei Ku, Andre Martins, Vivek Srikumar, the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, arXiv:2305.14259[cs.CLSciMON: Scientific Inspiration Machines Optimized for Novelty. 2024</p>
<p>Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for Large Language Models. Rui Wang, Hongru Wang, Boyang Xue, Jianhui Pang, Shudong Liu, Yi Chen, Jiahao Qiu, Derek Fai Wong, Heng Ji, Kam-Fai Wong, arXiv:2503.243772025. 2025arXiv preprint</p>
<p>Knowledge editing for large language models: A survey. Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, Jundong Li, Comput. Surveys. 572024. 2024</p>
<p>Wenxiao Wang, Lihui Gu, Liye Zhang, Yunxiang Luo, Yi Dai, Chen Shen, Liang Xie, Binbin Lin, Xiaofei He, Jieping Ye, arXiv:2410.23166[cs.CLSciPIP: An LLM-based Scientific Paper Idea Proposer. 2024</p>
<p>Self-Consistency Improves Chain of Thought Reasoning in Language Models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Cocogum: Contextual code summarization with multi-relational gnn on umls. Yanlin Wang, Lun Du, Ensheng Shi, Yuxuan Hu, Shi Han, Dongmei Zhang, MSR-TR-2020-162020. 2020MicrosoftTech. Rep.</p>
<p>Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, Eli Gottlieb, arXiv:2504.20073RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning. 2025. 2025arXiv preprint</p>
<p>Publication date. J Acm, August 20183726</p>
<p>Agent Workflow Memory. Zora Zhiruo, Wang , Jiayuan Mao, Daniel Fried, Graham Neubig, 10.48550/ARXIV.2409.07429arXiv:2409.074292024. 2024</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, Proceedings of the 36th International Conference on Neural Information Processing Systems. the 36th International Conference on Neural Information Processing SystemsNew Orleans, LA, USA2024. 180014</p>
<p>Magicoder: Source code is all you need. Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, Lingming Zhang, arXiv:2312.021202023. 2023arXiv preprint</p>
<p>NaturalProver: Grounded Mathematical Proof Generation with Language Models. Sean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi, Yejin Choi, arXiv:2205.12910[cs.CL2022</p>
<p>Generating Sequences by Learning to Self-Correct. Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, Yejin Choi, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Human-inthe-Loop Deep Reinforcement Learning with Application to Autonomous Driving. Jingda Wu, Zhiyu Huang, Chao Huang, Zhongxu Hu, Peng Hang, Yang Xing, Chen Lv, arXiv:2104.072462021</p>
<p>Large language models for generative information extraction: A survey. Derong Xu, Wei Chen, Wenjun Peng, Chao Zhang, Tong Xu, Xiangyu Zhao, Xian Wu, Yefeng Zheng, Yang Wang, Enhong Chen, Frontiers of Computer Science. 181863572024. 2024</p>
<p>Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, Chenyang Shao, Yuwei Yan, Qinglong Yang, Yiwen Song, Sijian Ren, Xinyuan Hu, Yu Li, Jie Feng, Chen Gao, Yong Li, arXiv:2501.09686[cs.AITowards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models. 2025</p>
<p>An illusion of progress? assessing the current state of web agents. Tianci Xue, Weijian Qi, Tianneng Shi, Chan Hee Song, Boyu Gou, Dawn Song, Huan Sun, Yu Su, arXiv:2504.013822025. 2025arXiv preprint</p>
<p>Codescope: An execution-based multilingual multitask multidimensional benchmark for evaluating llms on code understanding and generation. Weixiang Yan, Haitian Liu, Yunkun Wang, Yunzhe Li, Qian Chen, Wen Wang, Tingyu Lin, Weishan Zhao, Li Zhu, Hari Sundaram, arXiv:2311.085882023. 2023arXiv preprint</p>
<p>Codetransocean: A comprehensive multilingual benchmark for code translation. Weixiang Yan, Yuchen Tian, Yunzhe Li, Qian Chen, Wen Wang, arXiv:2310.049512023. 2023arXiv preprint</p>
<p>John Yang, Carlos E Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, arXiv:2405.15793Swe-agent: Agent-computer interfaces enable automated software engineering. Karthik Narasimhan, and Ofir Press2024. 2024arXiv preprint</p>
<p>Kaiyu Yang, Gabriel Poesia, Jingxuan He, Wenda Li, Kristin Lauter, Swarat Chaudhuri, Dawn Song, arXiv:2412.16075[cs.AIFormal Mathematical Reasoning: A New Frontier in AI. 2024</p>
<p>Leandojo: Theorem proving with retrieval-augmented language models. Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan J Prenger, Animashree Anandkumar, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>Buffer of thoughts: Thought-augmented reasoning with large language models. Ling Yang, Zhaochen Yu, Tianjun Zhang, Shiyi Cao, Minkai Xu, Wentao Zhang, Joseph E Gonzalez, Bin Cui, Advances in Neural Information Processing Systems. 372024. 2024</p>
<p>Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models. Ling Yang, Zhaochen Yu, Tianjun Zhang, Shiyi Cao, Minkai Xu, Wentao Zhang, Joseph E Gonzalez, Bin Cui, 10.48550/ARXIV.2406.04271arXiv:2406.042712024. 2024</p>
<p>Arithmetic Reasoning with LLM: Prolog Generation &amp; Permutation. Xiaocheng Yang, Bingsen Chen, Yik-Cheung Tam, arXiv:2405.178932024. 2024arXiv preprint</p>
<p>CARTS: Advancing Neural Theorem Proving with Diversified Tactic Calibration and Bias-Resistant Tree Search. Xiao-Wen Yang, Zhi Zhou, Haiming Wang, Aoxue Li, Wen-Da Wei, Hui Jin, Zhenguo Li, Yu-Feng Li, 2025In ICLR. OpenReview.net</p>
<p>Zhen Yang, Ming Ding, Qingsong Lv, Zhihuan Jiang, Zehai He, Yuyi Guo, Jinfeng Bai, Jie Tang, arXiv:2309.03241Gpt can solve mathematical problems without a calculator. 2023. 2023arXiv preprint</p>
<p>Zezhou Yang, Cuiyun Gao, Zhaoqiang Guo, Zhenhao Li, Kui Liu, Xin Xia, Yuming Zhou, arXiv:2405.18216A Survey on Modern Code Review: Progresses, Challenges and Opportunities. 2024. 2024arXiv preprint</p>
<p>Tree of Thoughts: Deliberate Problem Solving with Large Language Models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik R Narasimhan, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, Ningyu Zhang, arXiv:2305.13172Editing large language models: Problems, methods, and opportunities. 2023. 2023arXiv preprint</p>
<p>Knowledge Augmented Complex Problem Solving with Large Language Models: A Survey. J Acm, August 20183727Publication date</p>
<p>Tengfei Tong Ye, Lingfei Ma, Xuhong Wu, Shouling Zhang, Wenhai Ji, Wang, arXiv:2406.11935Iterative or Innovative? A Problem-Oriented Perspective for Code Optimization. 2024. 2024arXiv preprint</p>
<p>LongProc: Benchmarking Long-Context Language Models on Long Procedural Generation. Xi Ye, Fangcong Yin, Yinghui He, Joie Zhang, Howard Yen, Tianyu Gao, Greg Durrett, Danqi Chen, arXiv:2501.054142025. 2025arXiv preprint</p>
<p>Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah D Goodman, arXiv:2203.14465[cs.LG]STaR: Bootstrapping Reasoning With Reasoning. 2022</p>
<p>Zhiyuan Zeng, Qinyuan Cheng, Zhangyue Yin, Bo Wang, Shimin Li, Yunhua Zhou, Qipeng Guo, Xuanjing Huang, Xipeng Qiu, arXiv:2412.14135[cs.AIScaling of Search and Learning: A Roadmap to Reproduce o1 from Reinforcement Learning Perspective. 2024</p>
<p>Zhiyuan Zeng, Qinyuan Cheng, Zhangyue Yin, Yunhua Zhou, Xipeng Qiu, arXiv:2502.12215Revisiting the Test-Time Scaling of o1-like Models: Do they Truly Possess Test-Time Scaling Capabilities?. 2025. 2025arXiv preprint</p>
<p>Evaluating and Improving Tool-Augmented Computation-Intensive Math Reasoning. Beichen Zhang, Kun Zhou, Xilin Wei, Xin Zhao, Jing Sha, Shijin Wang, Ji-Rong Wen, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023. Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, Sergey Levine, NeurIPS; New Orleans, LA, USA2023. 2023. December 10 -16, 2023</p>
<p>A Survey of Automatic Source Code Summarization. Chunyan Zhang, Junchao Wang, Qinglei Zhou, Ting Xu, Ke Tang, Hairen Gui, Fudong Liu, 10.3390/sym14030471Symmetry. 1432022. 2022</p>
<p>Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, Weizhu Chen, arXiv:2303.12570Repocoder: Repository-level code completion through iterative retrieval and generation. 2023. 2023arXiv preprint</p>
<p>Jiayi Zhang, Jinyu Xiang, Zhaoyang Yu, Fengwei Teng, Xionghui Chen, Jiaqi Chen, Mingchen Zhuge, Xin Cheng, Sirui Hong, Jinlin Wang, arXiv:2410.10762Aflow: Automating agentic workflow generation. 2024. 2024arXiv preprint</p>
<p>Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, Rishabh Agarwal, arXiv:2408.15240[cs.LG]Generative Verifiers: Reward Modeling as Next-Token Prediction. 2024</p>
<p>Document-level Relation Extraction as Semantic Segmentation. Ningyu Zhang, Xiang Chen, Xin Xie, Shumin Deng, Chuanqi Tan, Mosha Chen, Fei Huang, Luo Si, Huajun Chen, Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence Organization. the Thirtieth International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence Organization2021</p>
<p>Long-tail Relation Extraction via Knowledge Graph Embeddings and Graph Convolution Networks. Ningyu Zhang, Shumin Deng, Zhanlin Sun, Guanying Wang, Xi Chen, Wei Zhang, Huajun Chen, Proceedings of the 2019 Conference of the North American Chapter. Long and Short Papers. the 2019 Conference of the North American ChapterHuman Language Technologies20191</p>
<p>Alicg: Fine-grained and evolvable conceptual graph construction for semantic search at alibaba. Ningyu Zhang, Qianghuai Jia, Shumin Deng, Xiang Chen, Hongbin Ye, Hui Chen, Huaixiao Tou, Gang Huang, Zhao Wang, Nengwei Hua, Proceedings of the 27th ACM SIGKDD conference on knowledge discovery &amp; data mining. the 27th ACM SIGKDD conference on knowledge discovery &amp; data mining2021</p>
<p>Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, arXiv:2401.01286A comprehensive study of knowledge editing for large language models. 2024. 2024arXiv preprint</p>
<p>Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Zhihan Guo, Yufei Wang, Irwin King, Xue Liu, Chen Ma, arXiv:2503.24235What, how, where, and how well? a survey on test-time scaling in large language models. 2025. 2025arXiv preprint</p>
<p>. Yuge Zhang, Qiyang Jiang, Xingyu Han, Nan Chen, Yuqing Yang, Kan Ren, arXiv:2402.171682024. 2024Benchmarking data science agents. arXiv preprint</p>
<p>Yuxiang Zhang, Shangxi Wu, Yuqi Yang, Jiangming Shu, Jinlin Xiao, Chao Kong, Jitao Sang, arXiv:2412.00154o1-coder: an o1 replication for coding. 2024. 2024arXiv preprint</p>
<p>Zihan Zhang, Black Sun, Pengcheng An, arXiv:2501.09165Breaking Barriers or Building Dependency? Exploring Team-LLM Collaboration in AI-infused Classroom Debate. 2025. 2025arXiv preprint</p>
<p>Igniting language intelligence: The hitchhiker's guide from chain-of-thought reasoning to language agents. Zhuosheng Zhang, Yao Yao, Aston Zhang, Xiangru Tang, Xinbei Ma, Zhiwei He, Yiming Wang, Mark Gerstein, Rui Wang, Gongshen Liu, Comput. Surveys. 572025. 2025</p>
<p>Ziyin Zhang, Hang Yu, Shijie Li, Peng Di, Jianguo Li, Rui Wang, arXiv:2409.04183GALLa: Graph Aligned Large Language Models for Improved Source Code Understanding. 2024. 2024arXiv preprint</p>
<p>ExpeL: LLM Agents Are Experiential Learners. Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, Gao Huang, 10.1609/AAAI.V38I17.2993628 on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence. J Michael, Jennifer G Wooldridge, Sriraam Dy, Natarajan, Vancouver, CanadaAAAI Press2024. August 2018. February 20-27, 202437Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Dong, arXiv:2303.18223A survey of large language models. 2023. 20231arXiv preprint</p>
<p>DocMath-eval: Evaluating math reasoning capabilities of LLMs in understanding long and specialized documents. Yilun Zhao, Yitao Long, Hongjun Liu, Ryo Kamoi, Linyong Nan, Lyuhao Chen, Yixin Liu, Xiangru Tang, Rui Zhang, Arman Cohan, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>Li Zhong, Zilong Wang, Jingbo Shang, arXiv:2402.16906Ldb: A large language model debugger via verifying runtime execution step-by-step. 2024. 2024arXiv preprint</p>
<p>A comprehensive survey on automatic knowledge graph construction. Lingfeng Zhong, Jia Wu, Qian Li, Hao Peng, Xindong Wu, Comput. Surveys. 562023. 2023</p>
<p>Yuqi Zhu, Shuofei Qiao, Yixin Ou, Shumin Deng, Ningyu Zhang, Shiwei Lyu, Yue Shen, Lei Liang, Jinjie Gu, Huajun Chen, arXiv:2403.03101Knowagent: Knowledge-augmented planning for llm-based agents. 2024. 2024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
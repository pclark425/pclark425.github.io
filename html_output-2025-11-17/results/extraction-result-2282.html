<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2282 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2282</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2282</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-64.html">extraction-schema-64</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <p><strong>Paper ID:</strong> paper-258841365</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.14259v7.pdf" target="_blank">SciMON: Scientific Inspiration Machines Optimized for Novelty</a></p>
                <p><strong>Paper Abstract:</strong> We explore and enhance the ability of neural language models to generate novel scientific directions grounded in literature. Work on literature-based hypothesis generation has traditionally focused on binary link prediction--severely limiting the expressivity of hypotheses. This line of work also does not focus on optimizing novelty. We take a dramatic departure with a novel setting in which models use as input background contexts (e.g., problems, experimental settings, goals), and output natural language ideas grounded in literature. We present SciMON, a modeling framework that uses retrieval of"inspirations"from past scientific papers, and explicitly optimizes for novelty by iteratively comparing to prior papers and updating idea suggestions until sufficient novelty is achieved. Comprehensive evaluations reveal that GPT-4 tends to generate ideas with overall low technical depth and novelty, while our methods partially mitigate this issue. Our work represents a first step toward evaluating and developing language models that generate new ideas derived from the scientific literature</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2282.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2282.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCIMON</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scientific Inspiration Machines Optimized for Novelty (SCIMON)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that generates natural-language scientific idea suggestions grounded in literature by retrieving inspirations from prior papers and explicitly optimizing for novelty via iterative compare-and-update loops and contrastive training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SCIMON</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>End-to-end framework that (1) extracts background problem contexts and seed terms from papers, (2) augments the context with retrieved 'inspirations' from semantic-neighbor examples, a background knowledge graph, and citation neighbors, (3) generates candidate idea sentences using LLMs (few-shot GPT-3.5/GPT-4 and fine-tuned T5/Meditron variants) possibly trained with an in-context contrastive objective, and (4) iteratively compares generated ideas to a literature reference set and instructs the model to update ideas until they are sufficiently novel. Retrieval uses SentenceBERT embeddings (all-mpnet-base-v2) for semantic similarity; novelty boosting uses a retrieval-and-penalize loop with a cosine-similarity threshold and LLM re-generation conditioned on retrieved similar literature as negatives.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>general scientific research (demonstrated in NLP and biomedicine)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>open-ended exploration (generate novel research directions grounded in literature)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>Semantic similarity between generated idea I and literature reference sentences R computed with SentenceBERT (all-mpnet-base-v2) cosine similarity; a novelty-inducing penalty γ_nov(I,R) is applied. Iterative procedure retrieves top-k nearest literature sentences (k=20) and compares their cosine similarity S_i against threshold µ (µ = 0.6) to determine if idea is 'too close' to prior work. Automated similarity metrics (BERTScore, ROUGE, BARTScore) are used for other evaluations but not as primary novelty signals.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td>Reported empirical effects from human evaluations and ablations: iterative novelty boosting produced first-iteration novelty improvements (examples: for semantic-neighbor (SN) variant, 88.9% of updated ideas were substantially different from initial ideas and 55.6% showed increased novelty; second-iteration further increased novelty for 57.8% of ideas that continued). Aggregate reported deltas in Table 3: first-iteration novelty deltas around +46–+55% for top variants (paper reports +54.4%, +55.6% and similar numbers for different configurations).</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td>No explicit quantitative trade-off analysis between novelty and feasibility is provided. Qualitative/human-eval evidence indicates more-novel model outputs still lack technical depth and utility compared to ground-truth paper ideas: in Study II, the original paper idea (ground truth) was judged to have significantly higher technical level and novelty in 85% of comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Iterative retrieve-compare-update (novelty boosting) where generated idea I_t is used to retrieve top-k similar literature R_i (k=20) using SentenceBERT; for any R_i with cosine similarity S_i >= µ (µ=0.6) the system provides those R_i as negative examples to the LLM with an instruction to make the idea significantly different, repeating until all S_i < µ. Additional strategies: retrieval-augmented input (semantic neighbors, KG neighbors, citation neighbors) and an in-context contrastive objective (InfoNCE) during fine-tuning to discourage copying.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td>Multiple human studies with domain-expert annotators. Key findings: (Study I) GPT4 few-shot variants and GPT4+KG scored highest by annotators on combined criteria (relevance, novelty, clarity, reasonableness). (Study II) GPT4FS+KG was judged to have higher technical detail in 48% of compared pairs and to be less incremental (more novel) in 45% of pairs vs GPT4FS; however, ground-truth paper ideas were judged significantly higher in technical level and novelty in 85% of comparisons. (Study III) For SN novelty iterations: 88.9% of updated ideas were substantially different from initial ideas and 55.6% increased measured novelty; second iteration increased novelty in 57.8% of continued examples. A biochemical case study: human biochemical experts rated ~80% of generated directions positively.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>GPT-3.5 zero/few-shot, GPT-4 zero/few-shot, T5 fine-tuned baseline, variants of T5 with semantic-neighbor (SN), KG, and in-context contrastive (CL) augmentations; also Meditron-7b in biochemical case.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Human judgments favored GPT-4 few-shot variants (GPT4FS, GPT4FS+KG) over GPT-3.5 and baseline T5 in helpfulness. Fine-tuned T5 variants (T5+SN+CL) outperformed other non-GPT baselines in some automatic metrics. Iterative novelty boosting substantially increased novelty measures for the subsets it was applied to (first-iteration novelty increases reported in the +46% to +55% range across top configurations; SN-specific numbers: 88.9% difference and 55.6% increase in novelty). Ground-truth ideas still judged superior in novelty/technical depth in 85% of cases.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>Framework generalized to a biochemical domain in a preliminary experiment: a fine-tuned biomedical LLM (Meditron-7b variant) produced outputs that biochemical experts rated positively ~80% of the time, and evaluators in that domain were sometimes more satisfied with generated outputs than with the extracted ground-truth idea regarding technical detail; however, the authors note this was an initial test and not a comprehensive domain evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciMON: Scientific Inspiration Machines Optimized for Novelty', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2282.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2282.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Iterative Novelty Boosting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative Novelty Boosting with Retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An algorithmic module that increases idea novelty by repeatedly retrieving similar literature, measuring similarity, and instructing the generation model to update the idea until it is sufficiently dissimilar from prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Iterative Novelty Boosting (retrieve-compare-update)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Given an initially generated idea I_0, the system queries a reference corpus R (all training-set ideas) using SentenceBERT to retrieve the k nearest literature ideas (k = 20). It computes cosine similarity S_i between I_t and each retrieved R_i; if any S_i >= µ (µ = 0.6), those R_i are presented to the LLM as negative examples along with an instruction to make the idea significantly different. The LLM generates a revised idea I_{t+1}; the loop repeats until all S_i < µ. The pipeline effectively incorporates a novelty-inducing penalty γ_nov(I,R) into training/inference by using explicit similarity checks and LLM conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>general scientific research (applied in NLP and biomedicine in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>open-ended exploration (increase novelty of generated research ideas)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>Cosine similarity between generated idea and retrieved literature sentences using SentenceBERT (all-mpnet-base-v2); top-k retrieval (k=20); threshold µ = 0.6 determines 'too similar'. The novelty-inducing penalty γ_nov(I,R) is conceptually used to penalize closeness to existing work.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td>Empirical improvements reported when applied: e.g., for semantic neighbor (SN) variant, 88.9% of updated ideas were substantially different after first iteration and 55.6% showed increased novelty; second-iteration increased novelty for 57.8% of continued items. Table 3 reports first-iteration novelty delta values for various system combinations in the +46–+55% range.</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Iterative constrained re-generation using retrieved similar literature as negative prompts; stops when all retrieved similarities fall below a fixed threshold (µ). This is a heuristic, iterative refinement approach rather than an explicit multi-objective optimizer.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td>Study III annotated regenerated vs initial ideas: large fraction of regenerated ideas were substantially different (e.g., 88.9% for SN) and a majority showed increased novelty (55.6% first-iteration); further iterations provided additional increases (57.8% for second iteration on continued items). Annotators also observed that many novelty updates were superficial recombinations of common concepts rather than deep technical novelties.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Non-iterative generation (single-pass LLM outputs) and retrieval-augmented generation without iterative novelty updates.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Iterative novelty boosting produced higher judged novelty than the initial single-pass outputs; specific percentages: +54.4% to +55.6% reported first-iteration novelty deltas for top-configured systems in Table 3; SN-specific clear numbers: 88.9% different and 55.6% more novel.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>When applied in the biochemical case study, the iterative/augmentation techniques contributed to a high positive rate (~80%) in domain expert ratings, though detailed per-component ablations in biomedicine were not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciMON: Scientific Inspiration Machines Optimized for Novelty', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2282.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2282.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Inspiration Retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inspiration Retrieval Module (Semantic, KG, and Citation Neighbors)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval module that supplies literature-derived 'inspirations' to the generator via semantic neighbors (training-set examples), one-hop knowledge-graph neighbors, and citation neighbors, using SentenceBERT for semantic similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Inspiration Retrieval Module</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Three retrieval sources supply inspirations: (1) Semantic Neighbors: find semantically-similar input-target pairs from the training set by embedding the base input (prompt+context) with SentenceBERT (all-mpnet-base-v2) and retrieving top-k neighbors; use their target salient terms as inspirations. (2) KG Neighbors: construct a background scientific knowledge graph (nodes: tasks/methods/materials/metrics from IE) and supply adjacent one-hop nodes for a seed term. (3) Citation Neighbors: retrieve cited paper titles from the source document's citation set, ranked by SentenceBERT similarity. Retrieved inspirations are concatenated into the LM input as additional context.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>scientific literature-informed idea generation (NLP primary, extended to biomedicine)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>context-grounded idea generation (provide literature grounding/inspiration)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>Semantic retrieval score (SentenceBERT cosine similarity) is used to select inspirations; for novelty boosting, similarity between generated idea and literature is measured similarly. Inspiration retrieval itself is not a novelty metric but provides grounding and candidate contrasts.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Use retrieved inspirations as positive grounding and (in iterative novelty boosting) as negative examples when retrieved items are too similar to the generated idea. Also used for few-shot retrieval prompts (provide input+output examples).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td>Retrieval-augmented variants (e.g., GPT4FS+SN, GPT4FS+KG, T5+SN+CL) improved human-judged helpfulness and technical detail relative to non-augmented baselines. Specific results: GPT4FS and GPT4FS+KG were top-rated in Study I; T5+SN+CL performed best among non-GPT baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Generation without retrieval augmentation (plain prompt+context); few-shot with random examples vs retrieval-based few-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Retrieval-augmented few-shot (semantic neighbor retrieval for in-context examples) and KG augmentation led to higher human preference and often more technical detail; GPT4FS+KG showed advantages in technical depth in Study II (48% of pairs favored).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>KG-based inspirations sometimes lead to more technical depth (noted in human evaluations), and the approach generalizes to the biomedical domain using PubTator-based IE to build the KG.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciMON: Scientific Inspiration Machines Optimized for Novelty', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2282.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2282.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>In-context Contrastive</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-context Contrastive Objective (InfoNCE over decoder states)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fine-tuning objective that discourages copying from the input context by applying an InfoNCE contrastive loss over decoder hidden states comparing ground-truth outputs to in-context negative examples drawn from the input.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>In-context Contrastive Augmentation (InfoNCE)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>During fine-tuning of T5, randomly select in-context negatives from sentences appearing in the input (i.e., background/context sentences). Compute an InfoNCE-style contrastive loss over averaged decoder hidden states: y+ = σ(Avg(W_y h+ + b_y)), y−_k = σ(Avg(W_y h−_k + b_y)), L_cl = exp(y+/τ) / (exp(y+/τ) + Σ_k exp(y−_k/τ)). The loss is optimized jointly with cross-entropy to increase the probability of the ground-truth target against in-context negatives, thereby reducing propensity to copy the background.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>machine-learning / NLP generation fine-tuning for idea generation</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>avoidance of copying and encouragement of novelty in generated text</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>Encourages novelty indirectly by increasing discrimination between ground-truth idea and context-derived negatives; no explicit numerical novelty metric embedded in loss beyond InfoNCE scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td>Models trained with contrastive objective (T5+CL variants) showed improved automatic similarity metrics (BERTScore/ROUGE) and were judged better by humans than baseline fine-tuning in some setups (exact numeric deltas not always tabulated separately).</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Joint optimization of cross-entropy loss and InfoNCE contrastive loss during fine-tuning; negative examples sampled from the input context.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td>In-context contrastive augmentation helped reduce copying from the context and improved human judgments compared to baseline fine-tuning in some cases; T5+SN+CL was a top-performing non-GPT variant in human evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>T5 fine-tuning without contrastive loss; few-shot LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Contrastive augmented T5 variants outperformed plain T5 in automatic metrics (BERTScore/ROUGE) and in human preference relative to some baselines (detailed per-model numbers in paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciMON: Scientific Inspiration Machines Optimized for Novelty', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2282.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2282.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM Generation Module</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Idea Generation (GPT-3.5, GPT-4, T5, Meditron fine-tuned variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The generation component uses few-shot in-context prompting of GPT-3.5/GPT-4 and fine-tuned sequence-to-sequence models (T5-large, Meditron-7b for biomedical) to produce natural-language idea sentences conditioned on background context and retrieved inspirations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM Generation Module (few-shot GPT / fine-tuned T5 / Meditron)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Several generation strategies were used: GPT-3.5 and GPT-4 with zero-shot, random few-shot, and retrieval-based few-shot prompts (selecting semantically-similar in-context examples); T5-large was fine-tuned on the automatically collected dataset, with optional retrieval inputs and the in-context contrastive objective; Meditron-7b was fine-tuned for the biochemical case. Decoding used beam search (beam size 5) and other generation hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>NLP and biomedical idea generation</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>open-ended idea synthesis and method suggestion</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>Not intrinsic to generators: novelty was measured post-hoc using SentenceBERT cosine similarity against literature and human novelty judgments; automatic similarity metrics (BERTScore/ROUGE/BARTScore) were used to compare to ground-truth but do not equate to novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td>Human evaluations indicate GPT-4 few-shot outputs were longer and more preferred by annotators, but tended to have lower measured novelty and depth compared to ground-truth (ground-truth judged superior in 85% of comparisons). Fine-tuned T5 variants scored higher in automatic similarity metrics but were sometimes less preferred by humans.</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td>Qualitative: longer/more-polished outputs (e.g., GPT-4) were preferred by human raters yet often lacked technical depth/novelty relative to real papers; no explicit quantitative feasibility vs novelty tradeoff analysis was provided.</td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Improve generation via retrieval-augmented prompts, in-context retrieval of semantically similar examples, in-context contrastive losses during fine-tuning, and iterative novelty boosting that re-prompts LLMs when generated ideas are too similar to existing literature.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td>Human studies (Study I, II) show GPT4 few-shot variants (with retrieval and KG augmentations) are rated highest for helpfulness; GPT4FS+KG often leads to more technical depth (48% of pairwise comparisons favored it). However, compared to actual paper ground-truth ideas, models performed substantially worse in novelty and technical depth in the majority of comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Zero-shot and random few-shot GPT prompts, T5 baseline (no retrieval/contrastive), retrieval-based few-shot, and fine-tuned LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>GPT-4 few-shot variants outperform GPT-3.5 variants and most T5 baselines in human preference, but T5 variants with contrastive and semantic neighbor augmentations outperform GPT-based models on automatic similarity metrics; iterative novelty boosting improves novelty for all applicable generators.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>In the biochemical case study, fine-tuned Meditron-7b variants produced ideas that biochemical experts rated positively (~80% positive), and in that domain evaluators sometimes preferred generated outputs over extracted ground-truth in technical detail.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciMON: Scientific Inspiration Machines Optimized for Novelty', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Undiscovered public knowledge <em>(Rating: 2)</em></li>
                <li>Agatha: automatic graph mining and transformer based hypothesis generation approach <em>(Rating: 2)</em></li>
                <li>PaperRobot: Incremental draft generation of scientific ideas <em>(Rating: 2)</em></li>
                <li>A computational inflection for scientific discovery <em>(Rating: 2)</em></li>
                <li>Unsupervised word embeddings capture latent knowledge from materials science literature <em>(Rating: 1)</em></li>
                <li>Literature based discovery: models, methods, and trends <em>(Rating: 2)</em></li>
                <li>Scientific language models for biomedical knowledge base completion: an empirical study <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2282",
    "paper_id": "paper-258841365",
    "extraction_schema_id": "extraction-schema-64",
    "extracted_data": [
        {
            "name_short": "SCIMON",
            "name_full": "Scientific Inspiration Machines Optimized for Novelty (SCIMON)",
            "brief_description": "A framework that generates natural-language scientific idea suggestions grounded in literature by retrieving inspirations from prior papers and explicitly optimizing for novelty via iterative compare-and-update loops and contrastive training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "SCIMON",
            "system_description": "End-to-end framework that (1) extracts background problem contexts and seed terms from papers, (2) augments the context with retrieved 'inspirations' from semantic-neighbor examples, a background knowledge graph, and citation neighbors, (3) generates candidate idea sentences using LLMs (few-shot GPT-3.5/GPT-4 and fine-tuned T5/Meditron variants) possibly trained with an in-context contrastive objective, and (4) iteratively compares generated ideas to a literature reference set and instructs the model to update ideas until they are sufficiently novel. Retrieval uses SentenceBERT embeddings (all-mpnet-base-v2) for semantic similarity; novelty boosting uses a retrieval-and-penalize loop with a cosine-similarity threshold and LLM re-generation conditioned on retrieved similar literature as negatives.",
            "research_domain": "general scientific research (demonstrated in NLP and biomedicine)",
            "problem_type": "open-ended exploration (generate novel research directions grounded in literature)",
            "novelty_metric": "Semantic similarity between generated idea I and literature reference sentences R computed with SentenceBERT (all-mpnet-base-v2) cosine similarity; a novelty-inducing penalty γ_nov(I,R) is applied. Iterative procedure retrieves top-k nearest literature sentences (k=20) and compares their cosine similarity S_i against threshold µ (µ = 0.6) to determine if idea is 'too close' to prior work. Automated similarity metrics (BERTScore, ROUGE, BARTScore) are used for other evaluations but not as primary novelty signals.",
            "novelty_score": "Reported empirical effects from human evaluations and ablations: iterative novelty boosting produced first-iteration novelty improvements (examples: for semantic-neighbor (SN) variant, 88.9% of updated ideas were substantially different from initial ideas and 55.6% showed increased novelty; second-iteration further increased novelty for 57.8% of ideas that continued). Aggregate reported deltas in Table 3: first-iteration novelty deltas around +46–+55% for top variants (paper reports +54.4%, +55.6% and similar numbers for different configurations).",
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": "No explicit quantitative trade-off analysis between novelty and feasibility is provided. Qualitative/human-eval evidence indicates more-novel model outputs still lack technical depth and utility compared to ground-truth paper ideas: in Study II, the original paper idea (ground truth) was judged to have significantly higher technical level and novelty in 85% of comparisons.",
            "optimization_strategy": "Iterative retrieve-compare-update (novelty boosting) where generated idea I_t is used to retrieve top-k similar literature R_i (k=20) using SentenceBERT; for any R_i with cosine similarity S_i &gt;= µ (µ=0.6) the system provides those R_i as negative examples to the LLM with an instruction to make the idea significantly different, repeating until all S_i &lt; µ. Additional strategies: retrieval-augmented input (semantic neighbors, KG neighbors, citation neighbors) and an in-context contrastive objective (InfoNCE) during fine-tuning to discourage copying.",
            "human_evaluation": true,
            "human_evaluation_results": "Multiple human studies with domain-expert annotators. Key findings: (Study I) GPT4 few-shot variants and GPT4+KG scored highest by annotators on combined criteria (relevance, novelty, clarity, reasonableness). (Study II) GPT4FS+KG was judged to have higher technical detail in 48% of compared pairs and to be less incremental (more novel) in 45% of pairs vs GPT4FS; however, ground-truth paper ideas were judged significantly higher in technical level and novelty in 85% of comparisons. (Study III) For SN novelty iterations: 88.9% of updated ideas were substantially different from initial ideas and 55.6% increased measured novelty; second iteration increased novelty in 57.8% of continued examples. A biochemical case study: human biochemical experts rated ~80% of generated directions positively.",
            "comparative_baseline": "GPT-3.5 zero/few-shot, GPT-4 zero/few-shot, T5 fine-tuned baseline, variants of T5 with semantic-neighbor (SN), KG, and in-context contrastive (CL) augmentations; also Meditron-7b in biochemical case.",
            "comparative_results": "Human judgments favored GPT-4 few-shot variants (GPT4FS, GPT4FS+KG) over GPT-3.5 and baseline T5 in helpfulness. Fine-tuned T5 variants (T5+SN+CL) outperformed other non-GPT baselines in some automatic metrics. Iterative novelty boosting substantially increased novelty measures for the subsets it was applied to (first-iteration novelty increases reported in the +46% to +55% range across top configurations; SN-specific numbers: 88.9% difference and 55.6% increase in novelty). Ground-truth ideas still judged superior in novelty/technical depth in 85% of cases.",
            "domain_specific_findings": "Framework generalized to a biochemical domain in a preliminary experiment: a fine-tuned biomedical LLM (Meditron-7b variant) produced outputs that biochemical experts rated positively ~80% of the time, and evaluators in that domain were sometimes more satisfied with generated outputs than with the extracted ground-truth idea regarding technical detail; however, the authors note this was an initial test and not a comprehensive domain evaluation.",
            "uuid": "e2282.0",
            "source_info": {
                "paper_title": "SciMON: Scientific Inspiration Machines Optimized for Novelty",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Iterative Novelty Boosting",
            "name_full": "Iterative Novelty Boosting with Retrieval",
            "brief_description": "An algorithmic module that increases idea novelty by repeatedly retrieving similar literature, measuring similarity, and instructing the generation model to update the idea until it is sufficiently dissimilar from prior work.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Iterative Novelty Boosting (retrieve-compare-update)",
            "system_description": "Given an initially generated idea I_0, the system queries a reference corpus R (all training-set ideas) using SentenceBERT to retrieve the k nearest literature ideas (k = 20). It computes cosine similarity S_i between I_t and each retrieved R_i; if any S_i &gt;= µ (µ = 0.6), those R_i are presented to the LLM as negative examples along with an instruction to make the idea significantly different. The LLM generates a revised idea I_{t+1}; the loop repeats until all S_i &lt; µ. The pipeline effectively incorporates a novelty-inducing penalty γ_nov(I,R) into training/inference by using explicit similarity checks and LLM conditioning.",
            "research_domain": "general scientific research (applied in NLP and biomedicine in paper)",
            "problem_type": "open-ended exploration (increase novelty of generated research ideas)",
            "novelty_metric": "Cosine similarity between generated idea and retrieved literature sentences using SentenceBERT (all-mpnet-base-v2); top-k retrieval (k=20); threshold µ = 0.6 determines 'too similar'. The novelty-inducing penalty γ_nov(I,R) is conceptually used to penalize closeness to existing work.",
            "novelty_score": "Empirical improvements reported when applied: e.g., for semantic neighbor (SN) variant, 88.9% of updated ideas were substantially different after first iteration and 55.6% showed increased novelty; second-iteration increased novelty for 57.8% of continued items. Table 3 reports first-iteration novelty delta values for various system combinations in the +46–+55% range.",
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": "Iterative constrained re-generation using retrieved similar literature as negative prompts; stops when all retrieved similarities fall below a fixed threshold (µ). This is a heuristic, iterative refinement approach rather than an explicit multi-objective optimizer.",
            "human_evaluation": true,
            "human_evaluation_results": "Study III annotated regenerated vs initial ideas: large fraction of regenerated ideas were substantially different (e.g., 88.9% for SN) and a majority showed increased novelty (55.6% first-iteration); further iterations provided additional increases (57.8% for second iteration on continued items). Annotators also observed that many novelty updates were superficial recombinations of common concepts rather than deep technical novelties.",
            "comparative_baseline": "Non-iterative generation (single-pass LLM outputs) and retrieval-augmented generation without iterative novelty updates.",
            "comparative_results": "Iterative novelty boosting produced higher judged novelty than the initial single-pass outputs; specific percentages: +54.4% to +55.6% reported first-iteration novelty deltas for top-configured systems in Table 3; SN-specific clear numbers: 88.9% different and 55.6% more novel.",
            "domain_specific_findings": "When applied in the biochemical case study, the iterative/augmentation techniques contributed to a high positive rate (~80%) in domain expert ratings, though detailed per-component ablations in biomedicine were not reported.",
            "uuid": "e2282.1",
            "source_info": {
                "paper_title": "SciMON: Scientific Inspiration Machines Optimized for Novelty",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Inspiration Retrieval",
            "name_full": "Inspiration Retrieval Module (Semantic, KG, and Citation Neighbors)",
            "brief_description": "A retrieval module that supplies literature-derived 'inspirations' to the generator via semantic neighbors (training-set examples), one-hop knowledge-graph neighbors, and citation neighbors, using SentenceBERT for semantic similarity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Inspiration Retrieval Module",
            "system_description": "Three retrieval sources supply inspirations: (1) Semantic Neighbors: find semantically-similar input-target pairs from the training set by embedding the base input (prompt+context) with SentenceBERT (all-mpnet-base-v2) and retrieving top-k neighbors; use their target salient terms as inspirations. (2) KG Neighbors: construct a background scientific knowledge graph (nodes: tasks/methods/materials/metrics from IE) and supply adjacent one-hop nodes for a seed term. (3) Citation Neighbors: retrieve cited paper titles from the source document's citation set, ranked by SentenceBERT similarity. Retrieved inspirations are concatenated into the LM input as additional context.",
            "research_domain": "scientific literature-informed idea generation (NLP primary, extended to biomedicine)",
            "problem_type": "context-grounded idea generation (provide literature grounding/inspiration)",
            "novelty_metric": "Semantic retrieval score (SentenceBERT cosine similarity) is used to select inspirations; for novelty boosting, similarity between generated idea and literature is measured similarly. Inspiration retrieval itself is not a novelty metric but provides grounding and candidate contrasts.",
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": "Use retrieved inspirations as positive grounding and (in iterative novelty boosting) as negative examples when retrieved items are too similar to the generated idea. Also used for few-shot retrieval prompts (provide input+output examples).",
            "human_evaluation": true,
            "human_evaluation_results": "Retrieval-augmented variants (e.g., GPT4FS+SN, GPT4FS+KG, T5+SN+CL) improved human-judged helpfulness and technical detail relative to non-augmented baselines. Specific results: GPT4FS and GPT4FS+KG were top-rated in Study I; T5+SN+CL performed best among non-GPT baselines.",
            "comparative_baseline": "Generation without retrieval augmentation (plain prompt+context); few-shot with random examples vs retrieval-based few-shot.",
            "comparative_results": "Retrieval-augmented few-shot (semantic neighbor retrieval for in-context examples) and KG augmentation led to higher human preference and often more technical detail; GPT4FS+KG showed advantages in technical depth in Study II (48% of pairs favored).",
            "domain_specific_findings": "KG-based inspirations sometimes lead to more technical depth (noted in human evaluations), and the approach generalizes to the biomedical domain using PubTator-based IE to build the KG.",
            "uuid": "e2282.2",
            "source_info": {
                "paper_title": "SciMON: Scientific Inspiration Machines Optimized for Novelty",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "In-context Contrastive",
            "name_full": "In-context Contrastive Objective (InfoNCE over decoder states)",
            "brief_description": "A fine-tuning objective that discourages copying from the input context by applying an InfoNCE contrastive loss over decoder hidden states comparing ground-truth outputs to in-context negative examples drawn from the input.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "In-context Contrastive Augmentation (InfoNCE)",
            "system_description": "During fine-tuning of T5, randomly select in-context negatives from sentences appearing in the input (i.e., background/context sentences). Compute an InfoNCE-style contrastive loss over averaged decoder hidden states: y+ = σ(Avg(W_y h+ + b_y)), y−_k = σ(Avg(W_y h−_k + b_y)), L_cl = exp(y+/τ) / (exp(y+/τ) + Σ_k exp(y−_k/τ)). The loss is optimized jointly with cross-entropy to increase the probability of the ground-truth target against in-context negatives, thereby reducing propensity to copy the background.",
            "research_domain": "machine-learning / NLP generation fine-tuning for idea generation",
            "problem_type": "avoidance of copying and encouragement of novelty in generated text",
            "novelty_metric": "Encourages novelty indirectly by increasing discrimination between ground-truth idea and context-derived negatives; no explicit numerical novelty metric embedded in loss beyond InfoNCE scoring.",
            "novelty_score": "Models trained with contrastive objective (T5+CL variants) showed improved automatic similarity metrics (BERTScore/ROUGE) and were judged better by humans than baseline fine-tuning in some setups (exact numeric deltas not always tabulated separately).",
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": "Joint optimization of cross-entropy loss and InfoNCE contrastive loss during fine-tuning; negative examples sampled from the input context.",
            "human_evaluation": true,
            "human_evaluation_results": "In-context contrastive augmentation helped reduce copying from the context and improved human judgments compared to baseline fine-tuning in some cases; T5+SN+CL was a top-performing non-GPT variant in human evaluations.",
            "comparative_baseline": "T5 fine-tuning without contrastive loss; few-shot LLMs.",
            "comparative_results": "Contrastive augmented T5 variants outperformed plain T5 in automatic metrics (BERTScore/ROUGE) and in human preference relative to some baselines (detailed per-model numbers in paper tables).",
            "domain_specific_findings": null,
            "uuid": "e2282.3",
            "source_info": {
                "paper_title": "SciMON: Scientific Inspiration Machines Optimized for Novelty",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "LLM Generation Module",
            "name_full": "LLM-based Idea Generation (GPT-3.5, GPT-4, T5, Meditron fine-tuned variants)",
            "brief_description": "The generation component uses few-shot in-context prompting of GPT-3.5/GPT-4 and fine-tuned sequence-to-sequence models (T5-large, Meditron-7b for biomedical) to produce natural-language idea sentences conditioned on background context and retrieved inspirations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "LLM Generation Module (few-shot GPT / fine-tuned T5 / Meditron)",
            "system_description": "Several generation strategies were used: GPT-3.5 and GPT-4 with zero-shot, random few-shot, and retrieval-based few-shot prompts (selecting semantically-similar in-context examples); T5-large was fine-tuned on the automatically collected dataset, with optional retrieval inputs and the in-context contrastive objective; Meditron-7b was fine-tuned for the biochemical case. Decoding used beam search (beam size 5) and other generation hyperparameters.",
            "research_domain": "NLP and biomedical idea generation",
            "problem_type": "open-ended idea synthesis and method suggestion",
            "novelty_metric": "Not intrinsic to generators: novelty was measured post-hoc using SentenceBERT cosine similarity against literature and human novelty judgments; automatic similarity metrics (BERTScore/ROUGE/BARTScore) were used to compare to ground-truth but do not equate to novelty.",
            "novelty_score": "Human evaluations indicate GPT-4 few-shot outputs were longer and more preferred by annotators, but tended to have lower measured novelty and depth compared to ground-truth (ground-truth judged superior in 85% of comparisons). Fine-tuned T5 variants scored higher in automatic similarity metrics but were sometimes less preferred by humans.",
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": "Qualitative: longer/more-polished outputs (e.g., GPT-4) were preferred by human raters yet often lacked technical depth/novelty relative to real papers; no explicit quantitative feasibility vs novelty tradeoff analysis was provided.",
            "optimization_strategy": "Improve generation via retrieval-augmented prompts, in-context retrieval of semantically similar examples, in-context contrastive losses during fine-tuning, and iterative novelty boosting that re-prompts LLMs when generated ideas are too similar to existing literature.",
            "human_evaluation": true,
            "human_evaluation_results": "Human studies (Study I, II) show GPT4 few-shot variants (with retrieval and KG augmentations) are rated highest for helpfulness; GPT4FS+KG often leads to more technical depth (48% of pairwise comparisons favored it). However, compared to actual paper ground-truth ideas, models performed substantially worse in novelty and technical depth in the majority of comparisons.",
            "comparative_baseline": "Zero-shot and random few-shot GPT prompts, T5 baseline (no retrieval/contrastive), retrieval-based few-shot, and fine-tuned LLMs.",
            "comparative_results": "GPT-4 few-shot variants outperform GPT-3.5 variants and most T5 baselines in human preference, but T5 variants with contrastive and semantic neighbor augmentations outperform GPT-based models on automatic similarity metrics; iterative novelty boosting improves novelty for all applicable generators.",
            "domain_specific_findings": "In the biochemical case study, fine-tuned Meditron-7b variants produced ideas that biochemical experts rated positively (~80% positive), and in that domain evaluators sometimes preferred generated outputs over extracted ground-truth in technical detail.",
            "uuid": "e2282.4",
            "source_info": {
                "paper_title": "SciMON: Scientific Inspiration Machines Optimized for Novelty",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Undiscovered public knowledge",
            "rating": 2,
            "sanitized_title": "undiscovered_public_knowledge"
        },
        {
            "paper_title": "Agatha: automatic graph mining and transformer based hypothesis generation approach",
            "rating": 2,
            "sanitized_title": "agatha_automatic_graph_mining_and_transformer_based_hypothesis_generation_approach"
        },
        {
            "paper_title": "PaperRobot: Incremental draft generation of scientific ideas",
            "rating": 2,
            "sanitized_title": "paperrobot_incremental_draft_generation_of_scientific_ideas"
        },
        {
            "paper_title": "A computational inflection for scientific discovery",
            "rating": 2,
            "sanitized_title": "a_computational_inflection_for_scientific_discovery"
        },
        {
            "paper_title": "Unsupervised word embeddings capture latent knowledge from materials science literature",
            "rating": 1,
            "sanitized_title": "unsupervised_word_embeddings_capture_latent_knowledge_from_materials_science_literature"
        },
        {
            "paper_title": "Literature based discovery: models, methods, and trends",
            "rating": 2,
            "sanitized_title": "literature_based_discovery_models_methods_and_trends"
        },
        {
            "paper_title": "Scientific language models for biomedical knowledge base completion: an empirical study",
            "rating": 1,
            "sanitized_title": "scientific_language_models_for_biomedical_knowledge_base_completion_an_empirical_study"
        }
    ],
    "cost": 0.016982249999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Scientific Inspiration Machines Optimized for Novelty
3 Jun 2024</p>
<p>Qingyun Wang qingyun4@illinois.edu 
University of Illinois at Urbana-Champaign</p>
<p>Doug Downey 
Allen Institute for Artificial Intelligence (AI2)</p>
<p>Heng Ji hengji@illinois.edu 
University of Illinois at Urbana-Champaign</p>
<p>Tom Hope tomh@allenai.org 
Allen Institute for Artificial Intelligence (AI2)</p>
<p>The Hebrew University of Jerusalem</p>
<p>Scientific Inspiration Machines Optimized for Novelty
3 Jun 20248AFCF73D6229DB6E6028F735BDF4E3F3arXiv:2305.14259v7[cs.CL]
We explore and enhance the ability of neural language models to generate novel scientific directions grounded in literature.Work on literature-based hypothesis generation has traditionally focused on binary link predictionseverely limiting the expressivity of hypotheses.This line of work also does not focus on optimizing novelty.We take a dramatic departure with a novel setting in which models use as input background contexts (e.g., problems, experimental settings, goals), and output natural language ideas grounded in literature.We present SCIMON, a modeling framework that uses retrieval of "inspirations" from past scientific papers, and explicitly optimizes for novelty by iteratively comparing to prior papers and updating idea suggestions until sufficient novelty is achieved.Comprehensive evaluations reveal that GPT-4 tends to generate ideas with overall low technical depth and novelty, while our methods partially mitigate this issue.Our work represents a first step toward evaluating and developing language models that generate new ideas derived from the scientific literature 1 .</p>
<p>Introduction</p>
<p>Can machines mine scientific papers and learn to suggest new directions?The idea that information from the literature can be used for automatically generating hypotheses has been around for decades (Swanson, 1986).To date, the focus has been on a specific setting: hypothesizing links between pairs of concepts (often in drug discovery applications (Henry and McInnes, 2017), e.g., new drug-disease links), where concepts are obtained from papers or knowledge bases previously derived from papers (Sybrandt et al., 2020;Nadkarni et al., 2021).</p>
<p>This common setting has fundamental drawbacks.Reducing the "language of scientific ideas" (Hope et al., 2023) to this simplistic form limits the expressivity of the hypotheses we can hope to generate, and does not capture nuanced contexts that scientists consider: target application settings, requirements and constraints, motivations and challenges.In light of the strong progress recently made with large language models (LLMs), in this paper we explore a dramatically different setting: models that take descriptions of problem contextsand return natural language suggestions of novel scientific directions that are grounded in literature.</p>
<p>We develop a framework named SCIMON (Scientific Inspiration Machines with Optimization for Novelty), named after Nobel laureate and AI pioneer Herbert Simon who authored early foundational work on automated scientific discovery (Newell and Simon, 1956;Simon, 1973).We first present an automated data collection methodology that collects examples of past problems and proposed ideas from scientific papers.We then use this data for both fine-tuning and in-context training of LLMs-training them to take problem descriptions and output proposed ideas to address them.We observe that state-of-art LLMs (e.g., GPT-4 (OpenAI, 2023)) struggle with generating novel scientific ideas, and contribute a new modeling framework for generating hypotheses that makes progress in improving the hypothesis generation ability of LLMs (Figure 1).Given a background problem description, models first dynamically retrieve inspirations from past literature in the form of related problems and their solutions along with contexts from a scientific knowledge graph.These retrieved inspirations serve to ground the generated ideas in existing literature.We then endow models with the ability to iteratively boost the novelty of generated ideas.Given an idea I generated by the LLM at step t, the model compares I with existing research in the literature; if it finds strongly overlapping research, the model is tasked with updating its idea to be more novel relative to prior work (much like a good researcher would do).We also introduce an in-context contrastive model which encourages novelty with respect to background context.</p>
<p>We perform the first comprehensive evaluation of language models for generating scientific ideas in our new generative, contextual setting.We focus on AI/NLP ideas to facilitate analysis by AI researchers themselves, and also demonstrate generalization to the biomedical domain.We design extensive evaluation experiments using human annotators with domain expertise to assess relevance, utility, novelty, and technical depth.Our methods substantially improve the ability of LLMs in our task; however, analyses show that ideas still fall far behind scientific papers in terms of novelty, depth and utility-raising fundamental challenges toward building models that generate scientific ideas.</p>
<p>Background and New Setting</p>
<p>We begin with a brief description of related work and background.We then present our novel setting.</p>
<p>Literature-based discovery Nearly four decades have passed since Don Swanson pioneered Literature-Based Discovery (LBD), based on the premise that the literature can be used for generating hypotheses (Swanson, 1986).LBD has been focused on a very specific, narrow type of hypothesis: links between pairs of concepts (often drugs/diseases).The classic formalization of LBD goes back to Swanson (1986) who proposed the "ABC" model where two concepts (terms) A and C are hypothesized as linked if they both co-occur with some intermediate concept B in papers.More recent work has used word vectors (Tshitoyan et al., 2019) or link prediction models (Wang et al., 2019;Sybrandt et al., 2020;Xu et al., 2023) to discover scientific hypotheses as pairwise links between concepts.A tightly related body of research focuses on scientific knowledge graph link prediction (Nadkarni et al., 2021), where predicted links may correspond to new hypotheses, and knowledge bases are reflections of existing scientific knowledge in specific domains, derived from literature.A fundamental gap in this line of work is in the lack of approaches for modeling nuanced contexts (Sosa and Altman, 2022) (e.g., the specific settings in which a drug may be relevant for a disease) for generating ideas in open-ended problem settings with unbounded hypothesis spaces, and for optimizing novelty.Our setting can be viewed as a radical departure addressing the limitations in existing settings.</p>
<p>LLMs for Scientific Innovation Large language models (LLMs) have made remarkable progress in interpreting and producing natural language content and handling knowledge-intensive tasks such as in the medical domain (Nori et al., 2023).Very recent work (Boiko et al., 2023) has explored the use of LLMs in a robotic chemistry lab setting, planning chemical syntheses of known compounds and executing experiments.Robotic lab settings are inherently limited to narrow sub-areas where such experiments are possible and relevant.Other very recent work (Huang et al., 2023) used LLMs to produce code for machine learning tasks such as Kaggle competitions, finding that a GPT-4 agent achieved 0% accuracy on research challenges such as BabyLM (Warstadt et al., 2023).GPT-4 has been anecdotally reported as having "strengths less like those of having a human co-author, and more like a mathematician working with a calculator" (Carlini, 2023).Our goal is to conduct a non-anecdotal evaluation and enhancement of strong LLMs' ability to generate novel open-ended scientific ideas.</p>
<p>SCIMON Problem Setting</p>
<p>We are motivated by imagining an AI-based assistant that suggests ideas in natural language.The assistant takes as input background context B consisting of (1) current problems, motivations, experimental settings and constraints, denoted as M; and optionally (2) a seed term v that should be a focus point of the generated idea I.The seed term is motivated by considering a user-provided cue for the model to limit its hypothesis space.Importantly, generated ideas should not merely paraphrase the background-the output should be novel with respect to B and the broader literature corpus.text that describes problems with "pretrained language models" in the lifelong integration of information sources, including computational costs.The assistant aims to generate an idea for performing "knowledge acquisition" within this context.Given this input, we aim to generate a full sentence describing a novel idea.</p>
<p>Automated Training Data Collection</p>
<p>We obtain training data derived from papers with scientific information extraction (IE) modelsextracting past examples of background sentences and corresponding ideas (e.g., descriptions of methods used for specific problems in the background sentences), along with salient entities as seed terms.This data is used for training in both in-context learning and fine-tuning setups.</p>
<p>We construct a corpus D from 67,408 ACL Anthology papers from S2ORC (Lo et al., 2020) (we later also conduct an experiment with a biomedical corpus §4.1).Given a title and the corresponding abstract from a document d, to select problem/motivation sentences M we first perform sci-entific sentence classification (Cohan et al., 2019) to classify sentences from the abstract into one of {Background, Method, Objective}, selecting sentences with labels of Background and treating the remaining sentences as target sentences T which will serve as desired output examples (Figure 3).</p>
<p>For seed term selection, we apply a state-ofthe-art scientific IE system (Ye et al., 2022) to T to extract entities corresponding to Task, Method, Evaluation Metric, Material, and relations of the form [method,used-for,task]-mentions of methods and the tasks they are used for, materials used for tasks, etc.We treat the head (e.g., method) or tail (e.g., task) entity as the seed term, and name the other entity (tail/head, respectively) as a target term t ∈ T .Continuing our example from Figure 2, Figure 3 shows how the seed and target terms ("knowledge acquisition" and "function preserved model expansion") are extracted from T .During training, each instance contains (B,T ) pairs; during evaluation, target information is removed.</p>
<p>We use SciCo (Cattan et al., 2021) to obtain coreference links for entity normalization, and use ScispaCy (Neumann et al., 2019) to replace abbreviations with a more informative long form.We also collect paper metadata, including the citation network G c .We split our dataset temporally (train/dev/test correspond to papers from years &lt;2021 / 2021 / 2022 respectively).For our experiments, we used model checkpoints trained on data preceding 2022, avoiding the risk of data contamination ( §6).Table 1 shows data statistics. 2uality of IE Preprocessing During preprocessing, we only keep high-confidence outputs from IE models to reduce errors.We observe this removes many of the noisy cases.To validate this, we manually evaluate the precision of each preprocessing step on a random sample of papers and observe that all steps yield high precision (91%-100%) except relation extraction (65%); in total, the rate of instances passing all steps was 79.7%. 3   Gold Test Set We create a high-quality, clean test set.We remove test instances where models can trivially use surface-level background information to infer the ground truth to create a more challenging set, selecting instances with low similarity between background and ground truth sentences.We compute the cosine similarity between each instance's background and corresponding ground truth sentence in the test set and take pairs with similarity ≤ 0.074, which amounts to the tenth percentile of pairs.We further annotate this subset to create a gold subset.We manually exclude instances with trivial overlap between ground truth and background, remove cases with irrelevant background, and retain only instances where the target relation (from which the seed term is taken) is salient to the target sentence.We also remove test pairs that have unexplained terms in the background.We obtain a total of 194 instances.</p>
<p>SCIMON Models</p>
<p>We present a new module to retrieve inspirations as contextual input ( §3.1).Then, we describe another module to generate ideas given the con-text+inspiration ( §3.2).Finally, we introduce a new iterative novelty optimization method to further improve idea quality ( §3.3). 5</p>
<p>3 See Table 6 in Appendix. 4Full annotation details are in Appendix C. 5 Training and hyperparameter details in Appendix B.</p>
<p>Inspiration Retrieval Module</p>
<p>We take broad inspiration from cognitive aspects of innovation (Hope et al., 2023): when researchers generate a new idea, they are grounded in a web of existing concepts and papers bearing on the new idea.We aim to enrich the context of each background by retrieving "inspirations"-pieces of information that can guide hypothesis generation.As illustrated in Figure 2, for a given instance of the SCIMON task, our retrieval augmentation can retrieve from three types of sources.Each source uses a different form of query and output.</p>
<p>Semantic Neighbors For a given problem/motivation as input, ideas proposed for related problems in the training set can serve as a guiding reference for generating a new idea.Given the background context B with a seed term v and problem/motivation M, we construct a base input b: a concatenation of M with a prompt P belonging to one of two templates: "v is used for p" or "v is done by using p", where p is one of Task/Method/Material/Metric.In short, b := P ⊕ context:M.For example, in Figure 2, the concatenation is "Knowledge acquisition is done by using Method; Context:...requires plms to integrate information...lifelong manner...".</p>
<p>We then retrieve inputs from the training set that are semantically related to a new base input b, and obtain target sentences T corresponding to each retrieved training input.We extract the target term t ∈ T matching the seed term in b ( §2.2) as inspiration for input b.Simply put, this means we use as inspiration the salient aspect of the solution proposed in T , which we found empirically to help remove noisy/irrelevant information in T .For example, in Figure 2, we find "informative entities are done by using Method context: in this work, we aim at equipping pre-trained language models with structured knowledge."as similar to the input and use t ="linked knowledge graph" as inspiration.</p>
<p>Technically, we first construct a fully connected graph G S based on the training set where each node is a pair of input text b i and target term t i .We define the weight between two nodes i and j as the cosine similarity between b i and b j based on representations from SentenceBERT (Reimers and Gurevych, 2019) (all-mpnet-base-v2).Given b, we first insert it into G S and compute the weights of its connected edges.We then retrieve neighbors input text {b 1 , . . ., b k } from the training set with the largest edge weight, where k is the number of retrieved instances.We consider the corresponding target terms {t 1 , . . ., t k } as semantic inspirations.</p>
<p>KG Neighbors</p>
<p>We also explore enriching the context by linking it to a background KG with information on related methods and tasks.Using the same IE process used to extract our training examples ( §2.2), we create a global background KG G B which covers all papers in the corpus D Y prior to a given year Y (i.e., the nodes in G B correspond to tasks/methods/materials/metrics, and the edges are used-for relations, extracted and normalized from across the entire corpus as described earlier).Then, given a seed term v at query time, we select adjacent nodes {n 1 , n 2 , ...} from G B as inspirations.</p>
<p>As an example, in Figure 2, the neighbor nodes of "knowledge acquisition" include "collaborative web text annotation editor", "image matching", etc., which we select as inspirations.</p>
<p>Citation Neighbors Another notion of contextual relatedness we explore is via citation graph links.Here, given as input background context B, we assume access to the original source document d from which B was extracted, and consider its cited paper title set C d as potential candidates.This can be seen as a stronger assumption on information available to the model-assuming a researcher using the model provides relevant candidate documents from which ideas could be pooled.Because the training set only contains papers before year Y, we only select papers C dY ⊆ C d prior to year Y.</p>
<p>We then retrieve the top-k titles with the highest cosine similarity to d from C dY based on their Sen-tenceBERT embeddings as earlier.For instance, in Figure 2, the paper ELLE (Qin et al., 2022) cites the paper (de Masson d' Autume et al., 2019).Therefore, we choose the title "episodic memory in lifelong language learning" as inspiration information.</p>
<p>Generation Module</p>
<p>The idea generation module is given retrieved inspirations i 1 , . . ., i k along with context M as input.</p>
<p>In-Context</p>
<p>Learning We experiment with recent state-of-the-art LLMs, GPT3.5 davinci-003 (Ouyang et al., 2022) andGPT4 gpt-4-0314 checkpoint (OpenAI, 2023).We first ask the model to generate sentences based on the seed term and the context in the zero-shot setting without any in-context examples (GPT3.5ZS,GPT4ZS).We then ask the model to generate sentences in a few-shot setting by prompting randomly chosen pairs of input and output from the training set (GPT3.5FS, GPT4FS).Inspired by Liu et al. (2022), we further employ a few-shot setting using semantically similar examples.Instead of random in-context examples, we use the top-k examples from the training set with the highest cosine similarity to the query (GPT3.5Retr).This few-shot retrieval setting differs from the semantic neighbor discussed above, in that we provide both the input and output of each instance rather than solely supplying target entities as additional input.</p>
<p>Fine Tuning We fine-tune T5 (Raffel et al., 2020) (more recent models may be used too; see our biomedical experiment §4.1 fine-tuning an LLM).We observe that the generation models tend to copy phrases from the background context.For example, given the context "...hierarchical tables challenge numerical reasoning ...", the model will generate "hierarchical table reasoning for question answering" as the top prediction.For generating suggestions of novel ideas, we wish to discourage overly copying from the background context.We introduce a new in-context contrastive objective, where negative examples are taken from the text in the input (e.g., in Figure 2, the in-context negatives are plms, pretraining, etc).We compute an InfoNCE loss (Oord et al., 2018) over the hidden states of the decoder, aiming to maximize the probability of the ground truth against those of in-context negatives:
y + = σ(Avg(W y h + + b y )) y − k = σ(Avg(W y h − k + b y )) L cl = exp (y + /τ ) k exp y − k /τ + exp (y + /τ )(1)
where h + and h − k are decoder hidden states from the positive and k-th negative samples, W y and b y are learnable parameters, σ is a sigmoid function, τ is a temperature hyperparameter, and Avg( * ) denotes the average pooling function based on the target sequence length.We optimize with both contrastive loss L cl and the cross-entropy loss.</p>
<p>Iterative Novelty Boosting with Retrieval</p>
<p>We further improve the novelty of generated ideas with a new iterative retrieve-compare-update scheme.Conceptually, we consider a noveltyinducing penalty γ nov (I, R) that penalizes ideas I that are too "close" to existing ideas in literature reference examples R. γ nov (I, R) is included during in-context learning and inference, providing numerical feedback in the form of a score reflecting similarity to existing work.We wish to minimize this score while ensuring I remains relevant to the background context B; we do so iteratively by (1) retrieving related work from R, (2) measuring degree of novelty, (3) instructing the model to update I to be more novel w.r.t R, conditioning on B.</p>
<p>Specifically, in our implementation, we construct a reference corpus R based on all papers in the training set.We then propose an iterative algorithm that compares generated ideas against R. We start with the initial idea I 0 generated by the generation module.At each time step t, we use the generated idea I t as a query to retrieve k nearest ideas from the literature reference corpus R = {R 1 , ..., R k } based on SentenceBERT, with the top-k highest cosine similarity scores to I t (we use k = 20).For each retrieved ground truth literature idea R i , we compare its cosine similarity score S i against a threshold µ (we use 0.6).We provide all the retrieved ground truth ideas R that pass the threshold as additional negative examples for the large language models with the following instruction prompt: "Your idea has similarities with existing research as demonstrated by these j sentences: R Make sure the idea you suggest is significantly different from the existing research mentioned in the above sentences.Let's give it another try."We stop the iteration once all S i are lower than µ. Figure 2 and Table 5 demonstrate novelty iterations.</p>
<p>Experiments</p>
<p>Human Evaluation</p>
<p>We present four human evaluation studies, exploring different facets of our problem and approach.</p>
<p>Study I: Comparing Outputs across Model Variants</p>
<p>We recruit six volunteer NLP experts with graduatelevel education to rate the system.Raters are told to envision an AI assistant that suggests new paper ideas.We randomly select 50 instances (back-ground+seed) from the gold subset.Each annotator receives ten instances, each paired with system outputs from different model variants (Table 2).We ask raters to assess idea quality by considering each output's relevance to the context, novelty, clarity, and whether the idea is reasonable (positive ratings are dubbed "helpful" as shorthand, indicating they pass the multiple considerations).We observe moderately high rater agreement. 6Raters are blind to the condition, and system outputs are randomly shuffled across instances.We instruct annotators to only provide positive ratings to ideas sufficiently different from the input context.In Study I, we ask raters not to anticipate groundbreaking novelty from the system but rather a narrower expectation of quality and utility; in Study II below, we enrich the analysis to examine ranking between top models and also "raise the bar" and compare to actual ideas from papers. 7n a preliminary experiment, we also collected human ratings for GPT4-ZS (zero-shot) vs. GPT4-FS (few-shot) using the same criteria, finding GPT4-FS ranked higher in 65% of cases, with the rest mostly tied; thus, zero-shot GPT-4 was left out of the remainder of study I and subsequent studies to reduce annotation effort and cost.</p>
<p>Results Overall, GPT4FS and GPT4FS+KG outperform other models by a wide margin (Table 2).Apart from GPT4, T5+SN+CL performs best compared to other baselines, given its stronger prior knowledge of useful similar background hypotheses.In general, GPT3.5 models performed worse than fine-tuned T5 and its variants, which echoes results in other work in the scientific NLP domain (Jimenez Gutierrez et al., 2022).GPT4 outputs tended to be longer, which may partially explain higher human preference.Table 2: Percent (%) of total votes each system output receives from human raters.H denotes a helpful output, while U denotes an unhelpful output."3FS" refers to the GPT3.5FS."3Rt" refers to the GPT3.5Retr."4" refers to GPT4FS, and "4+KG" refers to the GPT4FS+KG."T5+SN" refers to the T5+SN+CL.GPT4FS and GPT4FS+KG are rated much higher.While GPT4FS has a slightly higher rating than the KG variant, a further human study reveals that GPT4FS+KG often leads to more technical depth ( §4.1).</p>
<p>Study II: Comparing GPT4 Variants against Real Papers</p>
<p>We conduct a follow-up human study of close competitors GPT4FS and GPT4FS+KG with a subset of the annotators to evaluate the incrementality and novelty of the generated ideas.In this study, model outputs are now ranked, unlike the binary classification of helpful/not in Study I. Suggestions are ranked according to the level of technical detail and innovation in comparison to each other-i.e., ranking which of GPT4FS and GPT4FS+KG had a higher degree of technical detail and novelty, or whether they are roughly the same (tied).Finally, outputs are rated versus the ground truth idea, according to whether or not the suggestions were roughly at the same level of technical detail and innovation as the original paper's idea, or significantly lower.</p>
<p>Results Overall, GPT4FS+KG is found to have higher technical detail in 48% of the compared pairs, and found to be less incremental (more novel) in 45% of the pairs.Among the remaining 52%/55% (respectively), the vast majority are ties, indicating that whenever GPT4FS+KG is not favored, it is of roughly the same quality as GPT4FS, but not vice versa.However, the most crucial aspect is comparing the results against the original ground truth idea on the quality of innovation.Here, we find that in 85% of comparisons, the ground truth is considered to have significantly higher technical level and novelty; and in the remaining 15%, the ground truth was ambiguous or lacking additional context from the paper abstract.This points to a major challenge in obtaining high-quality idea generations using existing state-of-the-art models.</p>
<p>Study III: Evaluation on Iterative Novelty Boosting</p>
<p>We conduct a fine-grained evaluation of our novelty mechanism with qualitative and quantitative evaluation of novelty.Specifically, we ask five annotators to further compare the novelty-enhanced results against the initially generated ideas.We randomly select 70 instances (background+seed) from the sentence generation gold subset.We ask annotators to check whether the new ideas are different than the initial ideas (e.g., adding new information or approaches), and whether they are more novel (i.e., a new idea can be different, but not necessarily more novel).Since GPT4FS+SN outperforms other models, for this model, we further instruct annotators to compare the novelty of the second iteration results against the first iteration results.</p>
<p>Results</p>
<p>For SN, in the first iteration 88.9% of updated ideas are substantially different from initial ideas, and for 55.6% we are able to increase nov- Ideas after novelty iterations are longer than initial ideas.We examine the new terms added after filtering 359 words, including stopwords, as many generic words and terms are often added (e.g., "novel model/method/approach").While our method helps boost novelty, overall the model often tends to suggest combinations between popular concepts ( §4.2).Novelty boosting seemed to often focus on adding dynamic/adaptive modeling, graph models and representations, the fusion of multiple modalities and sources-and sometimes all at once (e.g., "Dynamic Syntax-Aware Graph Fusion Networks (DSAGFN)"), and to explicitly compare against existing ideas from literature (Table 5).Table 4: Human evaluations results of each system output for the idea sentence prediction task on Biomedical Domain."vs.GT" refers to percents which system outputs are better than ground truth ideas.</p>
<p>Domain Generalization Case Study</p>
<p>Our domain-agnostic framework can be applied to other domains by changing the IE system used in the preprocessing procedure.To demonstrate A novel method called Adaptive Speech Unit Boundary Detection (ASUBD) ... a combination of attention mechanisms to focus on relevant acoustic and linguistic features and reinforcement learning to guide the system to make optimal predictions of unit boundaries based on previous decisions... Ground Truth ... an efficient monotonic segmentation module ... accumulate acoustic information incrementally and detect proper speech unit boundaries.</p>
<p>Table 5: Example of iterative novelty iterations.Our novelty iteration method enhances ideas overall; however ideas are often based on superficial recombinations of common concepts, far from the technical depth of scientific papers.</p>
<p>this, we conduct an additional initial experiment in the biochemical domain.We follow a similar data creation procedure as for NLP papers.We collect a dataset from PubMed papers and use PubTator 3 (Islamaj et al., 2021;Wei et al., 2022;Luo et al., 2023;Wei et al., 2023;Lai et al., 2023) as an IE system to extract a KG from paper abstracts.We use a sentence classifier trained on annotated abstracts (Huang et al., 2020) to select background context.We fine-tune a state-of-the-art biomedical large language model (Chen et al., 2023) on our data and evaluate on a test split past its pre-training cutoff date. 8We ask two biochemical domain experts with graduate-level education to evaluate the quality of the results as before, finding them to overall rate 80% of the generated directions positively.Finally, in contrast to NLP-domain experiments, evaluators were more satisfied with the generated outputs than the ground truth regarding technical detail.Detailed results are in Table 4.However, this preliminary experiment was meant mainly to demonstrate the generality of our approach, and a more in-depth exploration of utility and quality is left for future work.</p>
<p>Error Analysis</p>
<p>Models often made generic suggestions, woven together with specific details copied directly from the context (e.g., "NLP with ML algorithms and sentiment analysis" for some problem X, or "data augmentation and transfer learning" for Y, or "BERT or RoBERTa" for Z).Our techniques reduced this behavior but did not fully solve it.GPT4 models, especially, seemed to generate generic descriptions of common steps in NLP workflows (e.g., "Data preprocessing: Clean the text data, remove unnec-8 More data and training details in Appendix A.2, B.2.3.essary characters, perform tokenization...").All models often copied and rephrased directly from the context.In certain cases, models applied simple logical modifications to the context; e.g., when contexts described problems such as "high latency" or "efficiency limitations", the suggestions would include phrases such as "low latency" or "highly efficient".</p>
<p>Automated Evaluation Analysis</p>
<p>In open-ended tasks such as ours, automatic evaluations comparing system output to ground truth texts may be limited.Nonetheless, automated metrics such as ROUGE (Lin, 2004), BERTScore (Zhang* et al., 2020) and BARTScore (Yuan et al., 2021), that check the similarity between ground truth and generated output, may surface interesting findings.We find GPT-based models to be outperformed by T5-based models; GPT4 outputs are much longer than T5, explaining why they underperform in automatic metrics but outperform in human evaluations ( §4.1).Generated sentences often follow certain templates (e.g., "In this paper, we propose a new ... for ..."), which also helps explain why T5 fine-tuned on many examples scores higher superficially.At the same time, our in-context contrastive examples which encourage novelty with respect to background context, helped models perform better than baseline fine-tuning by reducing reliance on copying.See results in Table 9 (Appendix B.4).</p>
<p>Conclusions and Future Directions</p>
<p>We propose a new setting, model and comprehensive evaluation for scientific hypothesis generation with language models that are grounded in literature and optimized for novelty.We present a new framework named SCIMON in which mod-els take background problem contexts and provide suggestions that are novel while based on literature.Models retrieve inspirations from semantic similarity graphs, knowledge graphs, and citation networks.We introduce a new iterative novelty boosting mechanism that helps large language models (LLMs) such as GPT-4 generate more novel ideas by explicitly comparing ideas to prior work and refining them.Our experiments demonstrate that the task of generating natural language scientific hypotheses is highly challenging.While our methods improve upon baseline LLMs, generated ideas tend to be incremental and with insufficient detail.Generating novel and meaningful scientific concepts and their compositions remains a fundamental problem (Hope et al., 2023).Evaluation in this setting is also highly challenging, with a huge space of potentially plausible hypotheses formulated in natural language.One interesting direction is to expand SCIMON with a multimodal analysis of formulas, tables, and figures to provide a more comprehensive background context.</p>
<p>Limitations</p>
<p>We discuss limitations extensively throughout the paper, such as in terms of evaluation challenges and data quality.Here we include additional details on limitations.</p>
<p>Limitations of Data Collection</p>
<p>We crawled papers with Semantic Scholar Academic Graph API from 1952 to June 2022.The number of available papers is limited by the data we crawled from the Semantic Scholar Academic Graph.We also crawled papers from PubMed 1988 to 2024/01.We remove papers that are not English.We also remove papers where abstracts are not correctly parsed from paper PDFs.We will expand our models to papers written in other languages and other domains in the future.</p>
<p>Limitations of System Performance</p>
<p>Our dataset is based on state-of-the-art IE systems, which may be noisy.For instance, the coreference and SciSpacy abbreviation resolution models fail to link A2LCTC to Action-to-Language Connectionist Temporal Classification.The background context detection may also have errors: e.g., the sentence classification component fails to treat "For example, the language models are overall more positive towards the stock market, but there are significant differences in preferences between a pair of industry sectors, or even within a sector." as background context.In our human-vetted gold data subset, we make sure to filter such cases, but they remain in the training data.SentenceBert (Reimers and Gurevych, 2019), and GPT3.5/4 are not finetuned and might be biased towards pretraining datasets.The idea novelty boosting method is limited by the quality of retrieval models.Better retrieval models may be explored in the future.Due to hardware constraints, we mainly investigated models with up to 7 billion parameters.Due to API change and model randomness, our GPT3.5/4results might not be easily reproducible.</p>
<p>Limitations of Evaluation</p>
<p>We recruit annotators from Ph.D. students; their opinions may differ from annotators who have different levels of domain knowledge.Our setting uses a seed term taken from the ground truth as input, to emulate a scenario where a human provides guidance to an assistant model.Future work could explore methods in the setting without a seed term, an even harder task, or evaluate in an interactive setting with user-provided seed terms.In addition, while the seed is sampled from the ground truth, in our human-annotated gold subset, we make sure that in no case does the input context trivially leak the output.et al. (2023) reports that LLMs tend to memorize part of their training data, a well-known concern in evaluating current LLMs.Therefore, we examine the pretraining data of each model:</p>
<p>Memorization Check</p>
<p>Carlini</p>
<p>• T5: Raffel et al. (2020) shows that T5 is pretrained on C4 which was crawled from web prior to April 2019.</p>
<p>• GPT3.5:Based on the documentation,9 GPT-3.5 series is pretrained on a combination of test and code from before Q4 2021.</p>
<p>• GPT4: OpenAI (2023) shows that the GPT-4 checkpoint we used utilizes most pertaining data before September 2021.Despite this, the pretraining and post-training data contain "a small amount" of more recent data.10</p>
<p>Because we evaluate our models on papers published in 2022, the likelihood of test papers appearing in the pretraining corpora for the models is substantially reduced.We additionally performed a manual examination of GPT-4 memorization in our gold set based on 2022 ACL Anthology papers, by seeing if GPT-4 could complete information such as method names or generate text that strongly mimics the ground truth papers, and found no evidence of this occurring.The Meditron-7b (Chen et al., 2023) uses PubMed with a cut-off in August 2023, and our biochemical test set only includes PubMed papers after 2023/08.</p>
<p>A Dataset Collection</p>
<p>A.1 NLP Dataset Collection</p>
<p>We download ACL Anthology papers from 1952 to 2022 using Semantic Scholar Academic Graph API. 11We filter out papers without abstracts and not written in English to obtain 67,408 papers.Our dataset has 58,874 papers before 2021, 5,946 papers from 2021, and 2,588 from 2022.We first use PL-Marker (Ye et al., 2022) pretrained on Sci-ERC (Luan et al., 2018) to extract nodes belonging to six types: Task, Method, Evaluation Metric, Material, Other Scientific Terms, and Generic Terms.The model then predicts relations between nodes belonging to seven relation types: Usedfor, Feature-of, Evaluate-for, Hyponym-of, Partof, Compare, and Conjunction.Because we want to generate new ideas, we focus on used-for relations in papers.Next, we use SciCo (Cattan et al., 2021) with checkpoint from Hugging Face12 to obtain entity coreference to merge identical nodes.Then, we use ScispaCy (Neumann et al., 2019) to perform unsupervised abbreviation detection to replace the abbreviation with a more informative long form.Finally, we perform scientific sentence classification (Cohan et al., 2019) 13 to classify sentences from the abstract into five categories including Background, Method, Objective, Other, and Result.We select sentences with labels of Background and Other as background context.During preprocessing, we only keep high-confidence outputs from IE models.Figure 4 shows an example of the IE systems pipeline.</p>
<p>A.2 Biochemical Dataset Collection</p>
<p>We collect PubMed papers from 1988 to 2024 using Entrez Programming Utilities API14 for the following topics, including Yarrowia, Saccharomyces cerevisiae, Issatchenkia orientalis, and Rhodosporidium toruloides.We use PubTator 3 (Islamaj et al., 2021;Wei et al., 2022;Luo et al., 2023;Wei et al., 2023;Lai et al., 2023).The PubTator 3 performs named entity recognition, relation extraction, entity coreference and linking, and entity normalization for the abstracts in the dataset.Pub-Tator 3 identifies bio entities belonging to seven types: gene, chemical, chromosome, cell line, variant, disease, and speciesl and relations belonging to 13 types: associate, cause, compare, convert, contract, drug interact, inhibit, interact, negative correlate, positive correlate, prevent, stimulate, and treat.Finally, we use a sentence classifier trained on CODA-19 (Huang et al., 2020) to classify sentences in abstracts into background, purpose, method, finding, and other.We select sentences with labels of background as background context and remove sentences with labels of other.We treat the rest sentences that have at least one entity as the target sentence.We only keep samples with low similarity between background context and corresponding ground truth sentences. 15Our final dataset has 4,767 papers before 2023/02, 642 papers from 2023/02 to 2023/08, and 299 papers after 2023/08.</p>
<p>B Finetuning and Automated Evaluation details B.1 Inspiration Retrieval Module</p>
<p>The statistics of each inspiration type are in Table 7.</p>
<p>Table 8 shows sample retrieved inspirations.</p>
<p>B.1.1 Semantic Neighbors</p>
<p>We use all-mpnet-base-v2 from SentenceBert (Reimers and Gurevych, 2019), which performs best in semantic search to retrieve similar nodes from the training set based on query q in §3.1.We retrieve up to 20 relevant semantic neighbors R from the training set for each instance.We treat the target nodes from R as semantic neighbors.</p>
<p>B.1.2 KG Neighbors</p>
<p>We use one-hop connected neighbors from the background KG G B constructed on papers before 2021(i.e., the papers in the training set).Because of the scarcity of KG neighbors, we do not limit the number of KG neighbors.</p>
<p>B.1.3 Citation Neighbors</p>
<p>Similar to semantic neighbors, we use all − mpnet − base − v2 from Sentence-Bert (Reimers and Gurevych, 2019) to retrieve cited paper titles similar to query q.We restrict cited papers only before 2021.We retrieve up to 5 relevant citation neighbors from the papers' citation network.</p>
<p>B.2 Generation Module</p>
<p>Our T5 model and their variants are built based on the Huggingface framework (Wolf et al., 2020). 16e optimize those models by AdamW (Loshchilov and Hutter, 2019) with the linear warmup scheduler.17Those models are finetuned on 4 NVIDIA A6000 48GB GPUs with distributed data parallel. 18he training time for each model is about 10 hours.</p>
<p>Used-for</p>
<p>Used-for</p>
<p>Feature-of</p>
<p>Used-for</p>
<p>Used-for</p>
<p>Coref</p>
<p>Method</p>
<p>Other Scientific Terms Task Impressive milestones have been achieved in text matching by adopting a cross-attention mechanism to capture pertinent semantic connections between two sentence representations.However, regular cross-attention focuses on word-level links between the two input sequences, neglecting the importance of contextual information.</p>
<p>We propose a context-aware interaction network (COIN) to properly align two sequences and infer their semantic relationship.</p>
<p>B.2.1 In-Context Learning</p>
<p>We choose GPT3.5 davinci-003 19 (Brown et al., 2020) as our out-of-the-box causal language modeling baseline.We select 5 instances from the training set as examples for the few-shot setting.We randomly select those examples for GPT3.5FS.For GPT3.5Retr, similar to semantic neighbors, we use all-mpnet-base-v2 from SentenceBert (Reimers and Gurevych, 2019), which performs best in semantic search to retrieve similar instances from the training set based on query q in §3.1.The input length is limited to 2048 tokens due to OpenAI API limits.We choose gpt-4-0314 as our GPT4 model.Our input for GPT4 is similar to GPT3.5.</p>
<p>For each selected example from the training set with forward relation, the template is "Consider the following context: M In that context, which p can be used for v, and why?T ", where M is the background context, p is the target node type, v is the seed term, and T is the target idea sentence; for backward relation, the template is "Consider the following context: M In that context, which p do we use v, and why? s".For selected examples with 19 openai.com/api/additional retrieval inspirations, we concatenate the following additional template to the M: "The retrieval results are: i 1 , . . ., i k ", where i 1 , . . ., i k are retrieved inspirations.For the final prompt, the template is similar to the above example template.However, the target sentence T will not be included.We ask the model to generate 10 outputs.We will select the best output and skip the empty output.</p>
<p>B.2.2 Fine Tuning</p>
<p>Given input without any inspirations, the input combines the prompt P and context M as shown in §3.1 (i.e., P | context: M).Given input with inspirations, the input is P | retrieve: i 1 , . . ., i k | context: M, with i 1 , . . ., i k as retrieved inspirations.The input length is limited to 512 tokens.</p>
<p>For both tasks, we finetune our model based on T5-large with a learning rate of 6 × 10 −6 and ϵ = 1 × 10 −6 .The batch size is 8 for each GPU.</p>
<p>The maximum training epoch for all models is 10 with 4 patience.During decoding, we use beamsearch to generate results with a beam size of 5 and a repetition penalty of 1.5.</p>
<p>In-context Contrastive Augmentation</p>
<p>We randomly select 2 sentences that appeared in the input as in-context negatives.For example, in Figure 1, the in-context negatives could be "knowledge acquisition is done by using Method", "this requires plms to integrate the information from all the sources in a lifelong manner .".</p>
<p>B.2.3 Biochemical Case Study</p>
<p>Our Meditron-7b (Chen et al., 2023) and its variants are built based on the Huggingface framework (Wolf et al., 2020). 20We use its epfl-llm/meditron-7b as the base model.We finetune those models with a learning rate of 2 × 10 −6 and ϵ = 5 × 10 −8 .The maximum training epoch for all models is 5.All models are finetuned on 4 NVIDIA A100 80 GB GPUs with Fully Sharded Data Parallel. 21The training time for each model is about 20 hours.</p>
<p>B.3 The Scale of Retrieval Set</p>
<p>We retrieve from a set of 59k papers with over 374k sentences in the NLP domain, the focus of our experiments.Our background KG built on the training set has more than 197k nodes and 261k relations.Moreover, we collect 87k paper titles from citation networks.This represents a large-scale and diverse domain; retrieving inspirations from this set is expected, in principle, to be more than enough for generating novel ideas.Indeed, NLP papers typically cite each other and build on each other as inspirations to create new ideas -which motivates our inspiration retrieval.</p>
<p>B.4 Automated Evaluation</p>
<p>We use BERTScore (Zhang* et al., 2020) with SciBERT checkpoint for both tasks.</p>
<p>The hash of the checkpoint is allenai/scibert_scivocab_uncased_L8 _no-idf_version=0.3.12(hug_trans=4.19.2).The automated evaluation results are in Table 9.</p>
<p>C Human Annotation and Evaluation Details</p>
<p>Gold Dataset Annotation Details The gold dataset annotation interface is in Figure 5.The quality of the instances in the test set is judged given three criteria: (1) whether the ground truth sentence trivially overlaps with background context;</p>
<p>(2) whether background context contains relevant information for the target relation; (3) whether the target relation (from which the seed term is taken) is a salient aspect of the idea proposed in the target paper.</p>
<p>Study I The instructions for human evaluation can be found in Figure 6, while an example of the 20 github.com/huggingface/transformers 21https://huggingface.co/docs/accelerate/usage _guides/fsdp human evaluation interface is provided in Figure 7 and 8. Human annotators are required to evaluate each system output based on the following criteria: (1) Is the candidate relevant to the context + seed term?(2) Does the candidate copy too much from the context, or is it sufficiently novel/different from the context?(3) Does the candidate's suggestion generally make sense to you scientifically?(4) Is the language sufficiently clear and coherent to understand the suggestion?The input for sample human annotation is in Table 10 and the human labels are in Table 11.The human annotation agreement is in Table 13.</p>
<p>Study III We ask the following questions to human annotators to evaluate the quality of regeneration results: (1) Is the regenerated idea substantially different from the original?(2) Is the regenerated idea more novel and creative than the original idea?(3) Does the second iteration increase novelty?The human annotation agreement is in Table 14.</p>
<p>D Scientific Artifacts</p>
<p>We list the licenses of the scientific artifacts used in this paper: Semantic Scholar Academic Graph API (API license agreement22 ), Huggingface Transformers (Apache License 2.0), SBERT (Apache-2.0license), BERTScore (MIT license), Meditron-7b (Llama2), Entrez Programming Utilities API (Copyright23 ), PubTator 3 (Data use policy24 ), and OpenAI (Terms of use 25 ).</p>
<p>E Ethical Consideration</p>
<p>The SCIMON task and corresponding models we have designed in this paper are limited to the natural language processing (NLP) and biochemical domain, and might not apply to other scenarios.</p>
<p>E.1 Usage Requirement</p>
<p>This paper aims to provide investigative leads for a scientific domain, specifically natural language processing.The final results are not intended to be used without human review.Accordingly, domain experts might use this tool as a research writing assistant to develop ideas.However, our system does not do any fact-checking with external knowledge.In addition, we train our models on the ACL</p>
<p>E.2 Data Collection</p>
<p>We collect 67,408 ACL Anthology papers from 1952 to 2022 using Semantic Scholar Academic Graph API, under API license agreement26 .We ensure our data collection procedure follows the Terms of Use at https://allenai.org/terms.According to the agreement, our dataset can only be used for non-commercial purposes.As mentioned in §4, we perform the human evaluation.All CT is a model with citation inspirations.R-L denotes ROUGE-L.BERT denotes BERTscore with SciBERT as its encoder.† indicates that differences between models are not statistically significant (p ≤ 0.05) when compared to each other but are still significant when compared to the other models on t-test.</p>
<p>Type Content</p>
<p>Seed Term Prompt Irish language learning is done by using Method Context Irish is a minority language which means that l2 learners have limited opportunities for exposure to the language, and additionally, there are also limited (digital) learning resources available.One method that could be used for Irish language learning is computer assisted language learning (CALL) unhelpful</p>
<p>GPT3.5FS+CT</p>
<p>In that context, the most appropriate task for precision grammar would be the development of a part-of-speech tagger for Irish using finite-state morphology and constraint grammar disambiguation.</p>
<p>unhelpful</p>
<p>GPT3.5FS+KG</p>
<p>Cló: An Irish Language Learning Platform -Leveraging Digital Resources and Human Expertise unhelpful</p>
<p>GPT4FS</p>
<p>We propose using a technology-enhanced approach, such as computer-assisted language learning (CALL) or natural language processing (NLP) techniques, to create interactive and engaging Irish language learning resources, as these methods can provide personalized learning experiences and enhance learners' exposure to the language despite limited resources available.</p>
<p>helpful</p>
<p>GPT4FS+KG</p>
<p>Exploiting Social Media for Irish Language Learning: An Analysis of Twitter Data.In this context, we use social media data, particularly from Twitter, as a method for Irish language learning, because it provides a rich source of authentic and diverse language examples that can be used to enhance learning opportunities for L2 learners in a minority language setting.helpful</p>
<p>T5</p>
<p>In this paper, we present a novel machine translation approach to Irish language learning.unhelpful T5+SN+CL</p>
<p>A conversational virtual environment for Irish language learning helpful Table 11: Sample human labels for idea-sentence generation results for seed term prompt "Irish language learning is done by using Method", with context "irish is a minority language ... limited digital learning resources available".</p>
<p>Type Content</p>
<p>Seed Term ARO10 Context</p>
<p>Huangjiu is known for its unique aroma, primarily attributed to its high concentration of bphenylethanol (ranging from 40 to 130 mg/L).</p>
<p>Meditron</p>
<p>We then selected six key genes involved in the phenylalanine metabolism pathway and predicted that their product flux may affect the synthesis of b-phenylethanol.</p>
<p>Meditron + CT</p>
<p>We found that the key amino acid residue that controls the activity of Aro10p was not conserved in wine yeast strains, which may explain the lower b-phenylethanol production in wine fermentation compared with that in Chinese huangjiu.Meditron + KG Both target genes, SSA1 and ARO10, were deleted using the CRISPR-Cas9 genome editing system.Meditron + SN Herein, we report that the key barrier for b-phenylethanol production in Huangjiu is ARO10, the only bi-functional amino acid decarboxylase in Saccharomyces cerevisiae.</p>
<p>Table 12: Input and idea-sentence generation results for seed gene "ARO10" in the biochemical domain.</p>
<p>Annotator Pair 1-2 1-3 1-4 1-5 1-6</p>
<p>Agreement % 68.8 75.0 56.2 43.8 75.0</p>
<p>Table 13: Percent (%) of same labels from overlapped 10 human evaluation instances on each pair of annotators for Study I.</p>
<p>Figure 1: SCIMON takes background context and generates ideas grounded in literature inspirations, optimizing novelty by iteratively comparing to related work.</p>
<p>Figure 2 illustrates the setting, showing a background ...a method that combines continual learning with a dynamic knowledge distillation approach for efficient knowledge acquisition ... 1. Different from previous knowledge distillation methods ... student model learns from teacher model for incremental knowledge extraction ... ...continual learning for knowledge acquisition...This approach is more efficient than exhaustive pre-training on all existing data...</p>
<p>a method that leverages memory-augmented neural networks for knowledge acquisition in a lifelong learning scenario...</p>
<p>Figure 2 :
2
Figure 2: Architecture overview.Our models retrieve inspirations and then pass the background input and retrieved inspirations to an LM-based generation module, which iteratively optimizes novelty.Input from Qin et al. (2022).</p>
<p>Figure 3 :
3
Figure3: We use IE to obtain literature data for our approach: problems/motivations (background) and proposed ideas (target), as well as salient seed terms.</p>
<p>Figure 5 :
5
Figure 5: Gold subset annotation interface</p>
<p>Percent (%) of same labels from overlapped 20 human evaluation instances on each pair of annotators for Study III.(1-3) has 60 shared questions.The rest of the pairs each share 40 questions.</p>
<p>Figure 6 :
6
Figure 6: Human evaluation instructions</p>
<p>4
SplitForwardBackwardTotalTrain55,88458,426114,310Valid7,9388,25716,195Test2,6232,6865,309Table 1: Dataset statistics. Considering a relation of theform [v used-for u], we define [v used-for ?] asforward, and [? used-for u] as backward.</p>
<p>Table 3 :
3
Relative improvements of iterative novelty boosting.Iterations are applied to the ideas for which sufficiently similar related work is detected ( §3.3)."1st Novelty" is % of the 1st iteration ideas that gained novelty over the initial idea, and "2nd Novelty" is the % of gain over the 1st iteration.Our method substantially increases novelty for ideas to which it is applied.To save annotation resources, we only annotate second iteration results for the best-performing method (SN).We report the average number of new terms added, after filtering.elty/creativity (meaning that, e.g., if 100 examples were updated, we would gain 56 examples that are more novel).The 2nd iteration, further increases novelty for 57.8% of the ideas that continued to another iteration.For ideas not considered more novel after applying our method, we do not observe a drop in novelty-the method either increases or maintains novelty.
TypeGPT4FS+SN+CT+KG1st Novelty ∆ (%)+54.4+55.6 +47.8 +46.72nd Novelty ∆(%)-+57.8--1st new terms ∆+23.1+22.8 +22.1 +21.92nd new terms ∆-+21.5--</p>
<p>seed term: speech unit boundaries ; context (abridged): ... generate partial sentence translation given a streaming speech input.existingapproaches... break the acoustic units in speech, as boundaries between acoustic units in speech are not even....Initial ideaA pause prediction model to identify speech unit boundaries ... Iteration 1A method that leverages acoustic and linguistic features to predict speech unit boundaries dynamically, ensuring smooth transitions ... differs from the existing research as it combines both acoustic properties and linguistic context ... adapting to variations in speaker characteristics, speaking styles, and languages.Iteration 2
TypeContentInput (Donget al., 2022)</p>
<p>Table 6 :
6
Human quality evaluation of preprocessing stages(%).Overall pass rate after all steps are applied is 79.7%.
Background Sentence</p>
<p>Table 7 :
7
Average of # of neighbors for each instance, excluding those which do not have any neighbor</p>
<p>an effective solution to data scarcity in low -resource scenarios.however, when applied to token-level tasks such as ner , data augmentation methods often suffer from token-label misalignment, which leads to unsatsifactory performance.
TypeContentSeed Term Promptdata augmentation is used for TaskContext data augmentation is Semantic Neighbors st and automatic speech recognition (asr), low-resource tagging tasks, end-to-endspeech translation, neural online chats response selection, neural machine translation,semi-supervised ner, entity and context learning, semi-supervised setting, dependency pars-ing, low-resource machine translation, slot filling, dialog state tracking, visual questionanswering, visual question answering (vqa), low-resource neural machine translationKG Neighborsnmt-based text normalization, task-oriented dialog systems, task-oriented dialogue system,low-resource languages (lrl), end-to-end speech translation, visual question answering (vqa),multiclass utterance classification, clinical semantic textual similarity, neural online chatsresponse selection, context-aware neural machine translationCitation NeighborsContextual Augmentation: Data Augmentation by Words with Paradigmatic Re-lations,An Analysis of Simple Data Augmentation for Named Entity Recognition,DataAugmentationforLow-ResourceNeuralMachineTranslation,DAGA: Data Augmentation with a Generation Approach for Low-resource Tagging Tasks,EDA: Easy Data Augmentation Techniques for Boosting Performance on Text ClassificationTasksGround TruthELM: Data Augmentation with Masked Entity Language Modeling for Low-Resource NER</p>
<p>Table 8 :
8
Example (from (Zhou et al., 2022)) of retrieved inspirations.Inspirations similar to ground truth are underlined.
context containsRelationrelevantis a partIs theIE is ofinformation forof theoutputsufficienttarget relationmaintrivallyquality(Conservative filterideaoverlap(not-only flag casesproposewith thegeneric,where context isd by theinputcontextentityoutputrelationrel_sentcontextcorrect)highly irrelevant)papertransformer -basedlanguage models usuallytreat texts as linearsequences . however ,most texts also have anWe propose a novel approach toinherent hierarchicalformulate , extract , encode and injectstructure , i.e. , parts of ahierarchical structure informationtext can be identifiedexplicitly into an extractiveusing their position insummarization model based on a pre -this hierarchy . intrained , encoder -only Transformerextractive textaddition , section titleslanguage model ( HiStruct+ model ) ,summarizatiousually indicate thewhich improves SOTA ROUGEs forn is done bycommon topic of theirextractive textextractive summarization on PubMedusing Metricrespective sentences .summarization sota rouges used forand arXiv substantially .</p>
<p>Table 9 :
9
Automatic evaluation results for the challenging and gold subsets.CL is a model with in-context contrastive augmentation.SN is a model with semantic inspirations.KG is a model with KG inspirations.
SubsetChallengingGoldModelR-L↑BERT↑R-L↑BERT↑GPT4ZS0.1200.5810.1300.583GPT4FS0.1430.6180.1510.624T50.2230.672  †0.2460.685GPT4FS+SN0.1440.6200.1490.627GPT4FS+KG0.1430.6190.1520.626GPT4FS+CT0.1440.6170.1490.622T5+CL0.225  †0.671  †0.251  †0.686  †T5+SN+CL0.228  †0.671  †0.258  †0.686  †T5+KG+CL0.223  †0.6690.2480.681  †T5+CT+CL0.225  †0.671  †0.250  †0.686  †</p>
<p>Table 10 :
10
Input for sample human annotation results
ModelOutput
Code, data, and resources are publicly available for research purposes: https://github.com/eaglew/clbd.
More details are in Appendix C.
The agreement scores are in Table13Appendix C.
Full evaluator guidelines are in Appendix C. The sample annotations are in Table 11.
platform.openai.com/docs/model-index-for-res earchers
See footnote 10, page 10 of OpenAI (2023).
huggingface.co/allenai/longformer-scico
github.com/allenai/sequential_sentence_class ification
www.ncbi.nlm.nih.gov/books/NBK25501/
The similarity is calculated with all-mpnet-base-v2.
github.com/huggingface/transformers
huggingface.co/docs/transformers/main_classe s/optimizer_schedules#transformers.get_linear_sc hedule_with_warmup
pytorch.org/tutorials/intermediate/ddp_tutor ial.html
api.semanticscholar.org/license/
www.ncbi.nlm.nih.gov/books/about/copyright/
www.ncbi.nlm.nih.gov/home/about/policies/
openai.com/policies/terms-of-use
https://api.semanticscholar.org/license/ annotators involved in human evaluation are voluntary participants with a fair wage. We further collect 5,708 PubMed papers from 1988 to 2024 using Entrez Programming Utilities
API 27 . We follow their data usage
guidelines 28 . 27 www.ncbi.nlm.nih.gov/books/NBK25501/ 28 www.ncbi.nlm.nih.gov/books/about/copyright/
AcknowledgementsThis work is supported by the Molecule Maker Lab Institute: an AI research institute program supported by NSF under award No. 2019897, by DOE Center for Advanced Bioenergy and Bioproducts Innovation U.S. Department of Energy, Office of Science, Office of Biological and Environmental Research under Award Number DESC0018420, by U.S. the AI Research Institutes program by National Science Foundation and the Institute of Education Sciences, Department of Education through Award No. 2229873 -AI Institute for Transforming Education for Children with Speech and Language Processing Challenges, and by AI Agriculture: the Agriculture and Food Research Initiative (AFRI)grant no.2020-67021-32799/project accession no.1024178 from the USDA National Institute of Food and Agriculture.The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied of, the National Science Foundation, the U.S. Department of Energy, and the U.S. Government.The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.
Autonomous chemical research with large language models. Robert Daniil A Boiko, Ben Macknight, Gabe Kline, Gomes, Nature. 62479922023</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlishCurran Associates, Inc202033Language models are few-shot learners</p>
<p>A llm assisted exploitation of ai-guardian. Nicholas Carlini, arXiv:2307.15008Cryptography and Security Repository. 2023</p>
<p>Quantifying memorization across neural language models. Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, Chiyuan Zhang, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Scico: Hierarchical cross-document coreference for scientific concepts. Arie Cattan, Sophie Johnson, Ido Daniel S Weld, Iz Dagan, Doug Beltagy, Tom Downey, Hope, 20213rd Conference on Automated Knowledge Base Construction</p>
<p>Meditron-70b: Scaling medical pretraining for large language models. Zeming Chen, Alejandro Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami, arXiv:2311.16079Computation and Language Repository. 2023</p>
<p>Pretrained language models for sequential sentence classification. Arman Cohan, Iz Beltagy, Daniel King, Bhavana Dalvi, Dan Weld, 10.18653/v1/D19-1383Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Episodic memory in lifelong language learning. Cyprien De Masson D'autume, Sebastian Ruder, Lingpeng Kong, Dani Yogatama, Advances in Neural Information Processing Systems. Curran Associates, Inc201932</p>
<p>Learning when to translate for streaming speech. Qian Dong, Yaoming Zhu, Mingxuan Wang, Lei Li, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Literature based discovery: models, methods, and trends. Sam Henry, Bridget T Mcinnes, 201774Journal of biomedical informatics</p>
<p>A computational inflection for scientific discovery. Tom Hope, Doug Downey, Oren Etzioni, Eric Daniel S Weld, Horvitz, Communications of the ACM. 2023</p>
<p>Context-aware interaction network for question matching. Zhe Hu, Zuohui Fu, Yu Yin, Gerard De, Melo , 10.18653/v1/2021.emnlp-main.312Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Benchmarking large language models as ai research agents. Qian Huang, Jian Vora, Percy Liang, Jure Leskovec, arXiv:2310.03302Machine Learning Repository. 2023</p>
<p>CODA-19: Using a non-expert crowd to annotate research aspects on 10,000+ abstracts in the COVID-19 open research dataset. Ting-Hao Kenneth Huang, Chieh-Yang Huang, Chien-Kuang Cornelia Ding, Yen-Chia Hsu, C Lee Giles, Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020. the 1st Workshop on NLP for COVID-19 at ACL 2020Association for Computational Linguistics2020</p>
<p>Nlm-chem, a new resource for chemical entity recognition in pubmed full text literature. Rezarta Islamaj, Robert Leaman, Sun Kim, Dongseop Kwon, Chih-Hsuan Wei, Donald C Comeau, Yifan Peng, David Cissel, Cathleen Coss, Carol Fisher, Rob Guzman, Preeti Gokal Kochar, Stella Koppel, Dorothy Trinh, Keiko Sekiya, Janice Ward, Deborah Whitman, Susan Schmidt, Zhiyong Lu, 10.1038/s41597-021-00875-1Scientific Data. 81912021</p>
<p>Thinking about GPT-3 in-context learning for biomedical IE? think again. Jimenez Bernal, Nikolas Gutierrez, Clayton Mcneal, You Washington, Lang Chen, Huan Li, Yu Sun, Su, Findings of the Association for Computational Linguistics: EMNLP 2022. Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Biorex: Improving biomedical relation extraction by leveraging heterogeneous datasets. Po-Ting Lai, Chih-Hsuan Wei, Ling Luo, Qingyu Chen, Zhiyong Lu, 10.1016/j.jbi.2023.104487Journal of Biomedical Informatics. 1461044872023</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>What makes good in-context examples for GPT-3?. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu Chen, 10.18653/v1/2022.deelio-1.10The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures. Dublin, Ireland and OnlineAssociation for Computational Linguistics2022. DeeLIO 2022Proceedings of Deep Learning Inside Out</p>
<p>S2ORC: The semantic scholar open research corpus. Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, Daniel Weld, 10.18653/v1/2020.acl-main.447Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Decoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, Proceedings of the 7th International Conference on Learning Representations. the 7th International Conference on Learning Representations2019</p>
<p>Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction. Yi Luan, Luheng He, Mari Ostendorf, Hannaneh Hajishirzi, 10.18653/v1/D18-1360Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>AIONER: all-in-one scheme-based biomedical named entity recognition using deep learning. Ling Luo, Chih-Hsuan Wei, Po-Ting Lai, Robert Leaman, Qingyu Chen, Zhiyong Lu, 10.1093/bioinformatics/btad310Bioinformatics. 3953102023</p>
<p>Scientific language models for biomedical knowledge base completion: an empirical study. Rahul Nadkarni, David Wadden, Iz Beltagy, Noah A Smith, Hannaneh Hajishirzi, Tom Hope, 2021AKBC</p>
<p>ScispaCy: Fast and robust models for biomedical natural language processing. Mark Neumann, Daniel King, Iz Beltagy, Waleed Ammar, 10.18653/v1/W19-5034Proceedings of the 18th BioNLP Workshop and Shared Task. the 18th BioNLP Workshop and Shared TaskFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>The logic theory machine-a complex information processing system. Allen Newell, Herbert Simon, IRE Transactions on information theory. 231956</p>
<p>Representation learning with contrastive predictive coding. Harsha Nori, Nicholas King, Scott Mayer Mckinney, Dean Carignan, Eric Horvitz, arXiv:2303.13375arXiv:1807.03748Computation and Language Repository. 2023. 2018Capabilities of GPT-4 on medical challenge problems</p>
<p>arXiv:2303.08774Gpt-4 technical report. Computation and Language Repository. 2023OpenAI</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>ELLE: Efficient lifelong pre-training for emerging data. Yujia Qin, Jiajie Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, Jie Zhou, 10.18653/v1/2022.findings-acl.220Findings of the Association for Computational Linguistics: ACL 2022. Dublin, IrelandAssociation for Computational Linguistics2022</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 211402020</p>
<p>Sentence-BERT: Sentence embeddings using Siamese BERTnetworks. Nils Reimers, Iryna Gurevych, 10.18653/v1/D19-1410Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Does scientific discovery have a logic? Philosophy of science. Simon Herbert, 197340</p>
<p>Contexts and contradictions: a roadmap for computational drug repurposing with knowledge inference. N Daniel, Russ B Sosa, Altman, Briefings in Bioinformatics. 2342682022</p>
<p>Undiscovered public knowledge. Don R Swanson, The Library Quarterly. 5621986</p>
<p>Agatha: automatic graph mining and transformer based hypothesis generation approach. Justin Sybrandt, Ilya Tyagin, Michael Shtutman, Ilya Safro, 10.1145/3340531.3412684Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management. the 29th ACM International Conference on Information &amp; Knowledge Management2020</p>
<p>Unsupervised word embeddings capture latent knowledge from materials science literature. John Vahe Tshitoyan, Leigh Dagdelen, Alexander Weston, Ziqin Dunn, Olga Rong, Kristin A Kononova, Gerbrand Persson, Anubhav Ceder, Jain, Nature. 57177632019</p>
<p>PaperRobot: Incremental draft generation of scientific ideas. Qingyun Wang, Lifu Huang, Zhiying Jiang, Kevin Knight, Heng Ji, Mohit Bansal, Yi Luan, 10.18653/v1/P19-1191Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>Findings of the babylm challenge: Sample-efficient pretraining on developmentally plausible corpora. Alex Warstadt, Aaron Mueller, Leshem Choshen, Ethan Wilcox, Chengxu Zhuang, Juan Ciro, Rafael Mosquera, Bhargavi Paranjabe, Adina Williams, Tal Linzen, Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning2023</p>
<p>Aleksandar Milosavljevic, and Zhiyong Lu. 2022. tmvar 3.0: an improved variant concept recognition and normalization tool. Chih-Hsuan Wei, Alexis Allot, Kevin Riehle, Bioinformatics. 3818</p>
<p>GNorm2: an improved gene name recognition and normalization system. Chih-Hsuan Wei, Ling Luo, Rezarta Islamaj, Po-Ting Lai, Zhiyong Lu, 10.1093/bioinformatics/btad599Bioinformatics. 39105992023</p>
<p>Transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Le Scao, Mariama Gugger, Quentin Drame, Alexander Lhoest, Rush, 10.18653/v1/2020.emnlp-demos.6Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsOnline. Association for Computational Linguistics2020</p>
<p>Exploring and verbalizing academic ideas by concept co-occurrence. Yi Xu, Shuqian Sheng, Bo Xue, Luoyi Fu, Xinbing Wang, Chenghu Zhou, 10.18653/v1/2023.acl-long.727Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Packed levitated marker for entity and relation extraction. Deming Ye, Yankai Lin, Peng Li, Maosong Sun, 10.18653/v1/2022.acl-long.337Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>BARTScore: Evaluating generated text as text generation. Weizhe Yuan, Graham Neubig, Pengfei Liu, Advances in Neural Information Processing Systems. 2021</p>
<p>Bertscore: Evaluating text generation with bert. Tianyi Zhang, * , Varsha Kishore, * , Felix Wu, * , Kilian Q Weinberger, Yoav Artzi, Proceedings of the 8th International Conference on Learning Representations. the 8th International Conference on Learning Representations2020</p>
<p>MELM: Data augmentation with masked entity language modeling for low-resource NER. Ran Zhou, Xin Li, Ruidan He, Lidong Bing, Erik Cambria, Luo Si, Chunyan Miao, 10.18653/v1/2022.acl-long.160Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, Ireland20221Association for Computational Linguistics</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-913 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-913</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-913</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-22.html">extraction-schema-22</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <p><strong>Paper ID:</strong> paper-50f44ef10335d59cec145b15effae20ff22c1fdb</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/50f44ef10335d59cec145b15effae20ff22c1fdb" target="_blank">ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This paper seeks inspiration from modern computer architectures to augment LLMs with symbolic memory for complex multi-hop reasoning, and proposes a symbolic memory framework instantiated as an LLM and a set of SQL databases.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) with memory are computationally universal. However, mainstream LLMs are not taking full advantage of memory, and the designs are heavily influenced by biological brains. Due to their approximate nature and proneness to the accumulation of errors, conventional neural memory mechanisms cannot support LLMs to simulate complex reasoning. In this paper, we seek inspiration from modern computer architectures to augment LLMs with symbolic memory for complex multi-hop reasoning. Such a symbolic memory framework is instantiated as an LLM and a set of SQL databases, where the LLM generates SQL instructions to manipulate the SQL databases. We validate the effectiveness of the proposed memory framework on a synthetic dataset requiring complex reasoning. The project website is available at https://chatdatabase.github.io/ .</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e913.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e913.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatDB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatDB (LLM + Databases as Symbolic Memory)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that augments an off-the-shelf LLM controller (ChatGPT GPT-3.5 Turbo) with an external SQL database as symbolic memory and a 'chain-of-memory' multi-step procedure, where the LLM generates SQL to read/write the database and decomposes tasks into intermediate SQL steps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>ChatDB (LLM controller + MySQL symbolic memory)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An LLM controller (ChatGPT GPT-3.5 Turbo, temperature=0) that generates and updates SQL statements to manipulate a MySQL database used as structured symbolic memory; uses a chain-of-memory decomposition and in-context chain-of-thought exemplars to produce intermediate memory-operation steps and a final summary.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>Fruit Shop Dataset question-answering (50 questions; 15 easy, 35 hard)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>82% accuracy (13/15 easy; 28/35 hard; 41/50 overall)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Database manipulation via chain-of-memory (processing chronological shop records and multi-step SQL queries/updates to store/query historical state)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use; multi-step reasoning; sequential symbolic memory manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Not reported as a separate metric; paper reports deterministic/successful SQL execution and infers correct interactive behavior from improved QA accuracy (see QA performance).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>External symbolic memory (MySQL database); chain-of-memory decomposition; LLM controller that emits SQL; symbolic execution of calculations by the DB; use of in-context exemplars and chain-of-thought prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Prompting / in-context learning with chain-of-thought exemplars (no model fine-tuning or RL reported).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>architectural change; prompting strategy</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Introduce a structured external database as symbolic memory and a 'chain-of-memory' procedure: the LLM generates a sequence of intermediate SQL operations (insert, update, select, delete) that are executed symbolically by the DB; combine with in-context chain-of-thought exemplars so the LLM emits multi-step memory operations and summary outputs. Delegates arithmetic and aggregations to the DB to avoid numeric/sequence errors.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Substantial improvement on the Fruit Shop QA benchmark: compared to the ChatGPT baseline (22% overall accuracy), ChatDB achieved 82% overall accuracy; hard question accuracy improved from 1/35 to 28/35.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>The paper attributes the gap to non-symbolic/approximate neural memory and manipulation (vector embeddings, memory matrices) that are unstructured and prone to error accumulation, limited context window sizes causing truncation, and LLMs' fragile numerical/multi-step reasoning when not delegating precise operations to symbolic executors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e913.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e913.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (GPT-3.5 Turbo) baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Off-the-shelf conversational LLM used as the baseline in experiments; answers Fruit Shop questions by taking the chronological records as part of the prompt (no external symbolic memory).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>ChatGPT (GPT-3.5 Turbo) baseline</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An autoregressive transformer conversational LLM used directly to answer questions by including historical records in the prompt (max token length 4096, temperature=0); no external symbolic database or deterministic tool calls in the evaluated setup.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>Fruit Shop Dataset question-answering (50 questions; 15 easy, 35 hard)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>22% accuracy (10/15 easy; 1/35 hard; 11/50 overall)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Transformer-based conversational LLM without external symbolic memory in the experiment; performs calculations and multi-hop reasoning purely in-token.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Prompting / in-context learning (records included in prompt); no fine-tuning or external tool use reported for baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Paper attributes baseline failures to token-limited context concatenation, brittle in-prompt numerical computation and multi-hop reasoning, and sequential error accumulation when performing several reasoning/calculation steps without symbolic execution.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Memory augmented large language models are computationally universal <em>(Rating: 2)</em></li>
                <li>Toolformer: Language models can teach themselves to use tools <em>(Rating: 2)</em></li>
                <li>Generative agents: Interactive simulacra of human behavior <em>(Rating: 2)</em></li>
                <li>Recurrent Memory Transformer <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks <em>(Rating: 2)</em></li>
                <li>SQL-PaLM: Improved large language model adaptation for text-to-SQL <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-913",
    "paper_id": "paper-50f44ef10335d59cec145b15effae20ff22c1fdb",
    "extraction_schema_id": "extraction-schema-22",
    "extracted_data": [
        {
            "name_short": "ChatDB",
            "name_full": "ChatDB (LLM + Databases as Symbolic Memory)",
            "brief_description": "A framework that augments an off-the-shelf LLM controller (ChatGPT GPT-3.5 Turbo) with an external SQL database as symbolic memory and a 'chain-of-memory' multi-step procedure, where the LLM generates SQL to read/write the database and decomposes tasks into intermediate SQL steps.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "ChatDB (LLM controller + MySQL symbolic memory)",
            "model_description": "An LLM controller (ChatGPT GPT-3.5 Turbo, temperature=0) that generates and updates SQL statements to manipulate a MySQL database used as structured symbolic memory; uses a chain-of-memory decomposition and in-context chain-of-thought exemplars to produce intermediate memory-operation steps and a final summary.",
            "model_size": null,
            "qa_task_name": "Fruit Shop Dataset question-answering (50 questions; 15 easy, 35 hard)",
            "qa_performance": "82% accuracy (13/15 easy; 28/35 hard; 41/50 overall)",
            "interactive_task_name": "Database manipulation via chain-of-memory (processing chronological shop records and multi-step SQL queries/updates to store/query historical state)",
            "interactive_task_type": "tool use; multi-step reasoning; sequential symbolic memory manipulation",
            "interactive_performance": "Not reported as a separate metric; paper reports deterministic/successful SQL execution and infers correct interactive behavior from improved QA accuracy (see QA performance).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "External symbolic memory (MySQL database); chain-of-memory decomposition; LLM controller that emits SQL; symbolic execution of calculations by the DB; use of in-context exemplars and chain-of-thought prompting.",
            "training_method": "Prompting / in-context learning with chain-of-thought exemplars (no model fine-tuning or RL reported).",
            "intervention_type": "architectural change; prompting strategy",
            "intervention_description": "Introduce a structured external database as symbolic memory and a 'chain-of-memory' procedure: the LLM generates a sequence of intermediate SQL operations (insert, update, select, delete) that are executed symbolically by the DB; combine with in-context chain-of-thought exemplars so the LLM emits multi-step memory operations and summary outputs. Delegates arithmetic and aggregations to the DB to avoid numeric/sequence errors.",
            "intervention_effect": "Substantial improvement on the Fruit Shop QA benchmark: compared to the ChatGPT baseline (22% overall accuracy), ChatDB achieved 82% overall accuracy; hard question accuracy improved from 1/35 to 28/35.",
            "hypothesized_cause_of_gap": "The paper attributes the gap to non-symbolic/approximate neural memory and manipulation (vector embeddings, memory matrices) that are unstructured and prone to error accumulation, limited context window sizes causing truncation, and LLMs' fragile numerical/multi-step reasoning when not delegating precise operations to symbolic executors.",
            "uuid": "e913.0",
            "source_info": {
                "paper_title": "ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "ChatGPT (baseline)",
            "name_full": "ChatGPT (GPT-3.5 Turbo) baseline",
            "brief_description": "Off-the-shelf conversational LLM used as the baseline in experiments; answers Fruit Shop questions by taking the chronological records as part of the prompt (no external symbolic memory).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "ChatGPT (GPT-3.5 Turbo) baseline",
            "model_description": "An autoregressive transformer conversational LLM used directly to answer questions by including historical records in the prompt (max token length 4096, temperature=0); no external symbolic database or deterministic tool calls in the evaluated setup.",
            "model_size": null,
            "qa_task_name": "Fruit Shop Dataset question-answering (50 questions; 15 easy, 35 hard)",
            "qa_performance": "22% accuracy (10/15 easy; 1/35 hard; 11/50 overall)",
            "interactive_task_name": null,
            "interactive_task_type": null,
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "Transformer-based conversational LLM without external symbolic memory in the experiment; performs calculations and multi-hop reasoning purely in-token.",
            "training_method": "Prompting / in-context learning (records included in prompt); no fine-tuning or external tool use reported for baseline.",
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "Paper attributes baseline failures to token-limited context concatenation, brittle in-prompt numerical computation and multi-hop reasoning, and sequential error accumulation when performing several reasoning/calculation steps without symbolic execution.",
            "uuid": "e913.1",
            "source_info": {
                "paper_title": "ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Memory augmented large language models are computationally universal",
            "rating": 2
        },
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools",
            "rating": 2
        },
        {
            "paper_title": "Generative agents: Interactive simulacra of human behavior",
            "rating": 2
        },
        {
            "paper_title": "Recurrent Memory Transformer",
            "rating": 2
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "rating": 2
        },
        {
            "paper_title": "SQL-PaLM: Improved large language model adaptation for text-to-SQL",
            "rating": 1
        }
    ],
    "cost": 0.011706499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ChatDB: Augmenting LLMs with Databases as Their SYMBOLIC MEMORY</h1>
<p>Chenxu Hu ${ }^{1 *}$ Jie Fu ${ }^{2 * \dagger}$ Chenzhuang Du ${ }^{1}$ Simian Luo ${ }^{1}$ Junbo Zhao ${ }^{3}$ Hang Zhao ${ }^{1 \dagger}$<br>${ }^{1}$ Tsinghua University ${ }^{2}$ Beijing Academy of Artificial Intelligence ${ }^{3}$ Zhejiang University<br>fujie@baai.ac.cn hangzhao@mail.tsinghua.edu.cn</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) with memory are computationally universal (Schuurmans, 2023). However, mainstream LLMs are not taking full advantage of memory, and the designs are heavily influenced by biological brains. Due to their approximate nature and proneness to the accumulation of errors, conventional neural memory mechanisms cannot support LLMs to simulate complex reasoning. In this paper, we seek inspiration from modern computer architectures to augment LLMs with symbolic memory for complex multi-hop reasoning. Such a symbolic memory framework is instantiated as an LLM and a set of SQL databases, where the LLM generates SQL instructions to manipulate the SQL databases. We validate the effectiveness of the proposed memory framework on a synthetic dataset requiring complex reasoning. The project website is available at https: //chatdatabase.github.io/.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overall workflow of ChatDB. The LLM controller controls the read and write operations to the memory. The memory stores historical information and provides relevant historical information to assist in responding to user input. In ChatDB, we focus on augmenting LLMs with databases as their symbolic memory.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>1 Introduction</p>
<p>Large language models (LLMs), such as GPT-4 <em>(OpenAI, 2023)</em> and PaLM 2 <em>(Anil et al., 2023)</em>, have increasingly become an essential component of modern artificial intelligence (AI) systems, revolutionizing our understanding of natural language processing (NLP) and transforming various industries <em>(Hao et al., 2023; Wang et al., 2023)</em>. While LLMs have made significant strides in understanding and generating contextually relevant responses, they have limitations <em>(Chen et al., 2023)</em>. One of the main challenges is that multi-turn interactions with language models generate a large number of tokens, which can easily exceed the input token limit of LLMs. For example, GPT-4 (32K) can only handle 32,000 tokens. As the interaction progresses, the LLMs must maintain contextual information (e.g., user inputs, and previous responses) and generate responses based on the accumulated data. Simply concatenating all contextual information and cramming it into LLMs, however, can easily exceed the processing capabilities of LLMs and accumulate errors, causing the model to lose track of the conversation and produce less accurate responses.</p>
<p>Some neural memory mechanisms have been explored <em>(Wu et al., 2022a; Khattab et al., 2022; Zhong et al., 2022)</em> to overcome the limited token input issue of LLMs. The memory components serve as a storage and retrieval system for relevant information from previous interactions. However, augmenting LLMs with conventional neural memory usually leads to difficulties in storing, retrieving, and manipulating historical information in memory, especially for tasks requiring complex multi-hop reasoning. Two main causes are (a) They do not store historical information in a structured form; (b) Their manipulation of the information stored in memory is not symbolic, as they all rely on some vector similarity calculations, which can be inaccurate, thus leading to the accumulation of errors.</p>
<p>To address the aforementioned issues, we propose using databases as novel symbolic memory for LLMs. The whole framework is named ChatDB. As shown in Figure 1, ChatDB consists of two components: an LLM controller and its memory. The LLM controller can be any commonly used LLM <em>(OpenAI, 2023; Touvron et al., 2023; Du et al., 2022; Zeng et al., 2022)</em> and is responsible for controlling the read and write operations to the memory. The memory of LLMs, which can be symbolic or non-symbolic, or a combination of both, is responsible for storing historical information and providing information when needed to assist the LLM in responding to user input. In ChatDB, we focus on using databases as symbolic memory, which allows for the structured storage of historical information through the execution of a symbolic language, namely SQL statements. These SQL statements are generated by an LLM. Incorporating a database as symbolic memory is particularly useful in scenarios requiring precise recording, modification, querying, deletion, and analysis of historical data. For example, a store manager needs to maintain daily sales records, where using plain text or matrices as memory is unsuitable <em>(Chen et al., 2023)</em>. However, using a database as external symbolic memory is highly suitable. The database enables accurate operations, including data insertion, deletion, update, and selection, using SQL statements. Thus, employing databases as external symbolic memory ensures precision and efficiency in managing and manipulating historical data, significantly enhancing the performance of LLMs in scenarios that require high accuracy and long-term data recording and processing.</p>
<p>In the ChatDB framework, we propose the chain-of-memory (CoM) approach to manipulate the external symbolic memory more effectively, thereby further enhancing the reasoning capabilities of LLMs. The chain-of-memory approach transforms user input into a series of intermediate memory operation steps that lead to final results. Through the chain-of-memory approach, a complex problem is decomposed into multiple steps of memory operations, significantly reducing the complexity of problem-solving. In ChatDB, each intermediate step involves one or more SQL statements.</p>
<p>Our ChatDB makes several contributions to the field of LLMs. Firstly, we propose augmenting LLMs with databases as their external symbolic memory, allowing for structured storage of historical data and enabling symbolic and complex data operations using SQL statements. Secondly, our chain-of-memory approach enables effective memory manipulation by converting user input into multi-step intermediate memory operations, which enhance the performance of ChatDB, enabling it to handle complex, multi-table database interactions with improved accuracy and stability. Finally, our experiments demonstrate that augmenting LLMs with symbolic memory improves multi-hop reasoning capabilities and prevents error accumulation, thereby enabling ChatDB to significantly outperform ChatGPT on a synthetic dataset.</p>
<h2>2 Related Work</h2>
<p>Memory-Augmented Large Language Models (LLMs). LLMs, such as GPT-4 <em>(OpenAI, 2023)</em> and PaLM 2 <em>(Anil et al., 2023)</em>, have demonstrated powerful reasoning and decision-making abilities. However, LLMs are often hindered by their limited context window sizes (e.g., GPT-4 can only handle 32K tokens). Memory-augmented LLMs <em>(Wu et al., 2022a, b; Zhong et al., 2022; Lewis et al., 2020; Guu et al., 2020; Park et al., 2023; Khattab et al., 2022; Izacard et al., 2022)</em> incorporate a memory module that prevents the model from forgetting crucial information and allows it to handle long text inputs that exceed the context window size. Retrieval-augmented in-context learning <em>(Khattab et al., 2022)</em></p>
<p>uses retrieval models (RM) to retrieve relevant information that can be inserted into the LLM as a prompt. For example, Auto-GPT ${ }^{3}$ and Generative Agents (Park et al., 2023) utilize a memory module to store the text prompt directly, allowing the agent to keep track of its history. The past and current prompts are then input into the LLM for processing. Neural Turing Machines (NMT) (Graves et al., 2014), which incorporate the recurrent neural network (RNN) with external trainable memory resources and learn to interact with the memory module with gradient descent. Gated Graph Sequence Neural Network (GGS-NN) (Johnson, 2017) constructs and modifies graphs and utilizes the graphs to produce reasonable outputs. Recurrent Memory Transformer (RMT) (Bulatov et al., 2022) introduces additional memory tokens to the input and output sequences to store, process and exchange local and global information between segments of long sequences, and then train the model to control both memory operation and sequence representations processing.
Reasoning with LLMs. LLMs are known to struggle in complex reasoning tasks. Previous methods focus on incorporating specially designed supervisory signals or fine-tuning to enhance the reasoning ability of language models (Piękos et al., 2021; Ran et al., 2019; Andor et al., 2019; Cobbe et al., 2021; Chen et al., 2022). Recent methods mainly improve the reasoning ability of language models through In-Context Learning (Brown et al., 2020; Lester et al., 2021; Wei et al., 2021, 2022; Wang et al., 2022). The most representative of these is Chain-of-Thought (CoT) (Wei et al., 2022), which presents the intermediate reasoning process of solving sample problems to the language model, greatly enhancing its reasoning capabilities.
LLMs with DBs. LLMs have demonstrated an impressive capability in generating code, including Python code, execution commands for Excel, and Structured Query Language (SQL) for databases (OpenAI, 2023). ChatExcel ${ }^{4}$ uses LLMs to generate the Excel execution command, simplifying the user interaction process. BINDER (Cheng et al., 2022) proposes a framework that maps task inputs to executable programs in a programming language (e.g., Python code) bound with an API to call LLMs to perform a wide range of functionalities. SQL-PALM (Sun et al., 2023) proposes an LLM-based Text-to-SQL model, using the execution-based self-consistent prompting approach, and outperforms previous Text-2-SQL methods by a large margin. While previous works involve databases to some extent, our proposed ChatDB system differs significantly from these methods. In specific, ChatDB views the databases as the external symbolic memory module for the LLM, and then leverages the database for reading and writing essential data information to enhance the reasoning process via chain-of-memory, leading to more accurate reasoning results.
Tool-using LLMs. From the tool-using perspective, ChatDB can also be seen as an LLM utilizing DBs as a tool (Schick et al., 2023; Shen et al., 2023; Surís et al., 2023; Paranjape et al., 2023). Toolformer (Schick et al., 2023), through a series of demonstrations, instructs the language model that it can invoke some APIs to utilize external tools to solve the current problem. Another representative work is Auto-GPT ${ }^{5}$, which enables the language models to complete a series of impressive tasks using a search engine. The advantage of ChatDB, which uses a database as an external tool, is that it allows the language model to maintain more accurate records and use historical data, thereby addressing more complex problems, especially those that require accurate historical data for reasoning.</p>
<h1>3 ChatDB</h1>
<p>In this section, we first briefly introduce the task definition and setting. We then describe the overall framework of our proposed ChatDB. Finally, we delve into the details of the chain-of-memory approach, which is the main component of ChatDB.</p>
<h3>3.1 Task Definition</h3>
<p>Given a user input in natural language and details of existing tables in the database (not required if there are no existing tables), the goal is to manipulate the symbolic memory, i.e., the external database, to fulfill the user's request. For instance, if the user (e.g., a store manager) command is to record, modify, query, and delete specific data, the corresponding SQL operation should be to insert, update, select, and delete the relevant data in the appropriate tables, respectively. These operations often involve multiple tables in the database.</p>
<h3>3.2 Framework Overview</h3>
<p>ChatDB framework consists of three main stages: input processing, chain-of-memory, and response summary, as illustrated in Figure 2. Algorithm 1 provides a detailed illustration of the entire algorithmic procedure of ChatDB to respond to the user input.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: ChatDB framework. The red arrow lines represent the process flow of chain-of-memory, indicating the connection between multiple memory operations. The red arrow lines between database tables represent the reference relationships between primary keys and foreign keys, which start from primary keys to foreign keys. Only the first four columns of each table are shown for brevity. This example showcases the process of returning goods purchased on 2023-01-02 by a customer with the phone number 823451.</p>
<p>Input Processing. ChatDB generates a series of intermediate steps to manipulate the symbolic memory by utilizing LLMs if responding to the user input requires the use of memory. Otherwise, we use LLMs directly to generate a reply.
Chain-of-Memory. ChatDB executes a series of intermediate memory operation steps to interact with symbolic memory. ChatDB manipulates the symbolic memory in sequence according to a series of previously generated SQL statements, including operations such as insert, update, select, delete, etc. The external database executes the corresponding SQL statements, updates the database, and returns the results. It is worth noting that ChatDB decides whether to update the memory operation step based on the results of previous SQL statements before executing this operation. ChatDB executes the next step following the same procedure until all operations on the memory are completed.
Response Summary. ChatDB summarizes the final response to the user based on the results of a series of chain-ofmemory steps.</p>
<h1>3.3 Chain-of-Memory</h1>
<p>Chain-of-thought (Wei et al., 2022) emphasizes decomposing complex reasoning into a series of intermediate steps. Chain-of-memory (CoM) can be seen as a way to augment chain-of-thought by providing a symbolic memory mechanism to support the storage associated with those intermediate steps.
The purpose of chain-of-memory is to enhance the reasoning capabilities and robustness of LLMs when manipulating symbolic memory. The approach involves converting the user input into a sequence of intermediate memory operations, enabling LLMs to more accurately and effectively manipulate the memory in a symbolic way. The ability to manipulate symbolic memory is particularly valuable for real-world applications that involve complex and accurate interactions with historical data, such as record-keeping and data analysis in a management setting.
To enhance the performance and stability of our approach, we employ in-context learning (Brown et al., 2020), providing prompt exemplars of several sequences of chain-of-memory steps, and chain-of-thought prompting. A robust and accurate chain-of-memory process enables LLMs to better reason over symbolic memory and handle more complex scenarios.</p>
<p>The advantages of chain-of-memory are twofold. Firstly, it enables LLMs to perform complex database manipulations with greater accuracy, enhancing their multi-hop reasoning ability over symbolic memory. Secondly, by decomposing complex operations into a sequence of intermediate memory operations, the chain-of-memory approach enhances the capability of LLMs when handling complex, multi-table interactions. This approach enables LLMs to handle edge cases and unexpected scenarios better, making it a promising method for real-world applications.</p>
<div class="codehilite"><pre><span></span><code><span class="nv">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="nv">The</span><span class="w"> </span><span class="nv">algorithm</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">ChatDB</span>
<span class="nv">Input</span>:<span class="w"> </span><span class="nv">userInput</span>,<span class="w"> </span><span class="nv">dataBase</span>
<span class="nv">Output</span>:<span class="w"> </span><span class="nv">reply</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nv">need</span><span class="w"> </span><span class="nv">manipulate</span><span class="w"> </span><span class="nv">memory</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">respond</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">user</span><span class="w"> </span><span class="nv">Input</span><span class="w"> </span><span class="k">then</span>
<span class="w">    </span><span class="nv">memOps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">LLM</span><span class="w"> </span><span class="nv">getSteps</span><span class="ss">(</span><span class="nv">userInput</span><span class="ss">)</span>
<span class="w">    </span><span class="k">else</span>
<span class="w">        </span><span class="nv">reply</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">LLM</span><span class="ss">(</span><span class="nv">userInput</span><span class="ss">)</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="nv">reply</span>
<span class="w">    </span><span class="k">end</span><span class="w"> </span><span class="k">if</span>
<span class="w">    </span><span class="nv">sqlResults</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>[]
<span class="w">    </span><span class="nv">newMemOps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>[]
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="nv">each</span><span class="w"> </span><span class="nv">memOp</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="nv">memOps</span><span class="w"> </span><span class="k">do</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="nv">need</span><span class="w"> </span><span class="nv">update</span><span class="w"> </span><span class="nv">memOp</span><span class="w"> </span><span class="nv">based</span><span class="w"> </span><span class="nv">on</span><span class="w"> </span><span class="nv">sqlResults</span><span class="w"> </span><span class="k">then</span>
<span class="w">            </span><span class="nv">newMemOp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">LLM</span><span class="w"> </span><span class="nv">updateOperation</span><span class="ss">(</span><span class="nv">memOp</span>,<span class="w"> </span><span class="nv">sqlResults</span><span class="ss">)</span>
<span class="w">        </span><span class="k">else</span>
<span class="w">            </span><span class="nv">newMemOp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">memOp</span>
<span class="w">        </span><span class="k">end</span><span class="w"> </span><span class="k">if</span>
<span class="w">        </span><span class="nv">sqlResult</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">executeOperation</span><span class="ss">(</span><span class="nv">newMemOp</span>,<span class="w"> </span><span class="nv">dataBase</span><span class="ss">)</span>
<span class="w">        </span><span class="nv">sqlResults</span>.<span class="nv">append</span><span class="ss">(</span><span class="nv">sqlResult</span><span class="ss">)</span>
<span class="w">        </span><span class="nv">newMemOps</span>.<span class="nv">append</span><span class="ss">(</span><span class="nv">newMemOp</span><span class="ss">)</span>
<span class="w">    </span><span class="k">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">        </span><span class="nv">reply</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">LLM</span><span class="w"> </span><span class="nv">summary</span><span class="ss">(</span><span class="nv">userInput</span>,<span class="w"> </span><span class="nv">newMemOps</span>,<span class="w"> </span><span class="nv">sqlResults</span><span class="ss">)</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="nv">reply</span>
</code></pre></div>

<p>$\triangleright$ Begin of Input Processing
$\triangleright$ Generate intermediate steps using LLMs
$\triangleright$ Generate reply using LLMs directly
$\triangleright$ Begin of Chain-of-Memory</p>
<h1>3.4 Comparison with previous memory-augmented LLMs</h1>
<p>Table 1: Comparison with prompt-based memory and matrix-based memory.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Memory format</th>
<th style="text-align: center;">Supported operations</th>
<th style="text-align: center;">Memory storage</th>
<th style="text-align: center;">Memory execution</th>
<th style="text-align: center;">Interpretability</th>
<th style="text-align: center;">State tracking</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Symbolic</td>
<td style="text-align: center;">ChatDB</td>
<td style="text-align: center;">Symbolic storage <br> (e.g., database)</td>
<td style="text-align: center;">Insert, Delete Update, Select</td>
<td style="text-align: center;">Structured</td>
<td style="text-align: center;">Symbolic</td>
<td style="text-align: center;">High</td>
<td style="text-align: center;">Yes</td>
</tr>
<tr>
<td style="text-align: center;">Prompt-based</td>
<td style="text-align: center;">Auto-GPT</td>
<td style="text-align: center;">Content and its vector embeddings</td>
<td style="text-align: center;">Insert, Select</td>
<td style="text-align: center;">Semi-Structured</td>
<td style="text-align: center;">Non-Symbolic</td>
<td style="text-align: center;">Normal</td>
<td style="text-align: center;">No</td>
</tr>
<tr>
<td style="text-align: center;">Matrix-based</td>
<td style="text-align: center;">RMT</td>
<td style="text-align: center;">Memory tokens/ memory matrices</td>
<td style="text-align: center;">Read, Write</td>
<td style="text-align: center;">Semi-Structured</td>
<td style="text-align: center;">Non-Symbolic</td>
<td style="text-align: center;">Low</td>
<td style="text-align: center;">Yes</td>
</tr>
</tbody>
</table>
<p>In this subsection, we provide a comprehensive comparison between ChatDB and recent approaches that augment Transformer-based language models with memory module. The memory modules of language models proposed in previous work can be broadly categorized into two types. The first type of memory stores context and uses a retrieval model to find content from past interactions that's most relevant to the current conversation, and then uses it as a prompt for the language model (Khattab et al., 2022). We refer to this type of memory as prompt-based memory. The second type of approach utilizes additional memory tokens or memory matrices as memory (Bulatov et al., 2022), which we refer to as matrix-based memory. We compare ChatDB with these approaches based on the following aspects:</p>
<ol>
<li>Memory Format. This aspect relates to the format used for storing memory. ChatDB utilizes databases as its memory. Prompt-based memory (Park et al., 2023) stores relevant interaction content and/or their corresponding vector embeddings. Matrix-based memory employs additional trainable memory tokens (Bulatov et al., 2022, 2023) or trainable memory matrices (Graves et al., 2014).</li>
<li>Supported Operations. This aspect refers to the operations supported for manipulating memory. ChatDB supports operations such as insertion, deletion, update, and selection of data in the database memory. The prompt-based memory primarily supports insertion and selection operations but lacks complete support for updates and deletions. Matrix-based memory supports reading (selection) and writing (insertion, updating, deletion) operations. However, the exact operations performed by the neural network are not explicitly known.</li>
<li>Memory Storage. This aspect refers to the format in which data is stored in memory, specifically whether it is structured or not. ChatDB stores memory in a structured format using databases, while both the prompt-based memory</li>
</ol>
<p>and matrix-based memory are viewed as semi-structured. The reason is that vector embeddings and memory matrices have specific dimensions and sizes, but each individual dimension does not carry a specific and explicit meaning.
4. Memory Execution. This aspect focuses on how memory operations are executed, specifically whether they are symbolic or not. ChatDB executes operations on its database memory using SQL, which is a symbolic language, thus making it inherently symbolic. Prompt-based memory performs selections based on a similarity metric using vector embeddings and employs a language encoder to obtain vector embeddings for insertions. Both of these are considered non-symbolic executions. In matrix-based memory augmentated LLMs, memory operations are entirely controlled by the neural network, resulting in non-symbolic executions as well.
5. Interpretability. This aspect refers to the degree of interpretability of memory. In ChatDB, memory is stored in a structured and explicit format, and its operations are symbolic, leading to a high level of interpretability. In the prompt-based memory, interpretability is generally limited due to inherent challenges in interpreting vector embeddings. For matrix-based memory approach, the degree of interpretability is low since memory is completely controlled by the neural network implicitly.
6. State Tracking. This aspect refers to whether the memory effectively tracks the current state of the LLMs. In the case of ChatDB, its memory accurately tracks the current state of the LLMs. The fruit shop experiment serves as a demonstration, where after processing each record, ChatDB's database memory is updated to reflect the latest state of the fruit shop. This showcases how ChatDB's memory effectively tracks its current state. Thanks to the symbolic memory execution, ChatDB's memory allows for easy rollback to any desired timestamp, providing greater flexibility and controllability. In matrix-based memory approach, memory is continuously updated and altered by the model itself, enabling it to track the current state of the LLMs. However, the prompt-based memory approach simply stores the historical context and only knows what has happened in the past without a clear understanding of the current state.
By examining these aspects, we observe the distinctive features and capabilities of ChatDB in comparison to existing approaches. The superiority of ChatDB highlights the advantages of using symbolic memory to augment LLMs.</p>
<h1>4 Evaluation</h1>
<p>In this section, we conduct experiments to evaluate the effectiveness of augmenting LLMs with databases as their symbolic memory. Our experiment results demonstrate that ChatDB significantly outperforms the baseline model ChatGPT, highlighting the advantages of symbolic memory integration.</p>
<h3>4.1 Experimental Setup</h3>
<p>As previously mentioned, using a database as symbolic memory is particularly suitable for scenarios that require precise recording and processing of historical information, such as various data management scenarios. To adapt to the use case of ChatDB and enable quantitative comparisons with other models, we constructed a synthetic dataset simulating the management of a fruit shop.
Furthermore, in order to evaluate the performance of the models, we collected a set of 50 questions with annotated standard answers. These questions varied in difficulty, ranging from hard ones that required multi-hop reasoning to easy ones that only required retrieving information from historical data. There are 15 easy questions and 35 hard questions. Each question is independently answered by the models.</p>
<h3>4.1.1 Model Configuration</h3>
<p>ChatDB. The LLM used in ChatDB is ChatGPT (GPT-3.5 Turbo), and the hyperparameter temperature is set to 0 . We used the MySQL database as the external symbolic memory.
Baseline. We use ChatGPT (GPT-3.5 Turbo) as the baseline model with a maximum token length of 4096. Similar to ChatDB, we set the temperature to 0 .</p>
<h3>4.1.2 Dataset</h3>
<p>We synthesize a dataset of fruit shop management records, referred to as the "Fruit Shop Dataset". This dataset simulates four common operations in a shop: purchasing, selling, changing prices, and goods returns. We ensure that all historical records are valid and do not encounter issues such as negative inventory. We generate 70 records arranged in chronological order, totaling approximately 3.3 k tokens, which is within the maximum token length limit of ChatGPT (4096 tokens).</p>
<p>(a) Purchasing goods: the shop purchases goods from suppliers.</p>
<p>Record: We restocked our store on 2023-01-01 with a new supply of fruits from "ABC" (abc_sup@example.com, 10080). The purchased quantities include 24 kg cherry, at unit prices of 0.8 . Our intended selling price of cherry is 1.3 dollars per unit.
ChatDB:
Begin to interact with the external database (symbolic memory).
Step1: Insert supplier 'ABC' if not exists
INSERT INTO suppliers (supplier_name, contact_number, email)
SELECT 'ABC', "10080", 'abc_sup@example.com'
WHERE NOT EXISTS (SELECT 1 FROM suppliers WHERE supplier_name $\sim$ - "ABC");
Step2: Insert fruit (set the selling price to NULL and stock quantity to 0 ) if not exists</p>
<p>INSERT INTO fruits (fruit_name, selling_price, stock_quantity, $\sim$ fruit_type, shelf_life)
SELECT 'cherry', NULL, 0, NULL, NULL
WHERE NOT EXISTS (SELECT 1 FROM fruits WHERE fruit_name $\sim$ 'cherry');
Step3: Insert purchase
INSERT INTO purchases (supplier_id, purchase_date, $\sim$ total_cost)
VALUES (SELECT supplier_id FROM suppliers WHERE
$\sim$ supplier_name - "ABC"), "2023-01-01", 24 - 0.8);
Step4: Insert purchase item
INSERT INTO purchase_items (purchase_id, fruit_id,
$\sim$ quantity_purchased, cost_per_item, item_total_cost)
VALUES (SELECT MAX(purchase_id) FROM purchases), (SELECT
$\sim$ fruit_id FROM fruits WHERE fruit_name - "cherry"), 24,
$\sim 0.8,24$ - 0.8);
Step5: Update the stock quantity of cherry
UPDATE fruits
SET stock_quantity = stock_quantity + 24
WHERE fruit_name = "cherry";
Step6: Update the selling price of cherry if given new selling price
UPDATE fruits
SET selling_price $=1.3$
WHERE fruit_name = "cherry";
(c) Return of goods: customers bring back purchased items to the shop.</p>
<p>Record: Because the customer returned their purchase, we are required to undo the sales transaction made by customer 'John Doe' (phone: 123-4567890, email: john.doe@example.com) on 2023-01-05.
ChatDB:
Begin to interact with the external database (symbolic memory).
Step1: Find the sale_id for this customer on this date
SELECT sale_id FROM sales
WHERE customer_id = (SELECT customer_id FROM customers WHERE
$\sim$ phone_number = "123-456-7890" AND email =
"john.doe@example.com") AND sale_date = "2023-01-05";
Database response:
$\qquad$
| sale_id |
$+$............
1 9
$+$............
Step2: Get all the fruit_id and quantity_sold for this sale, replace <sale_id> with the results from the previous queries
SELECT fruit_id, quantity_sold FROM sale_items
WHERE sale_id = 9 [<sale_id>];
Database response:
$+$.............
| fruit_id | quantity_sold |
$+$.............
| 7 | 10
1 0
$+$............
$\square$</p>
<h2>(b) Selling goods: customers buy goods from the shop</h2>
<p>Record: A sale was made on 2023-01-02 to 'Bob Smith' (contact details: 123-456-7893, bob.smith@example.com). The items purchased were 9 kg apple, 4 kg cherry.
ChatDB:
Begin to interact with the external database (symbolic memory).
Step1: Insert customer 'Bob Smith' if not exists
INSERT INTO customers (first_name, last_name, phone_number, $\sim$ email)
SELECT 'Bob', 'Smith', "123-456-7893",
$\sim$ 'bob.smith@example.com'
WHERE NOT EXISTS (SELECT 1 FROM customers WHERE phone_number $\sim$ = "123-456-7893");
Step2: Insert sale
INSERT INTO sales (customer_id, sale_date, total_price)
VALUES (SELECT customer_id FROM customers WHERE phone_number
$\sim$ = "123-456-7893"), "2023-01-02", (SELECT selling_price
$\sim$ FROM fruits WHERE fruit_name = "apple") + 9 + (SELECT
$\sim$ selling_price FROM fruits WHERE fruit_name = "cherry") + $\sim$ 4);</p>
<p>Step3: Insert sale item
INSERT INTO sale_items (sale_id, fruit_id, quantity_sold,
$\sim$ price_per_item, item_total_price)
VALUES (SELECT MAX(sale_id) FROM sales), (SELECT fruit_id
$\sim$ FROM fruits WHERE fruit_name = "apple"), 9, (SELECT
$\sim$ selling_price FROM fruits WHERE fruit_name = "apple"),
$\sim$ (SELECT selling_price FROM fruits WHERE fruit_name =
$\sim$ "apple") + 9),
((SELECT MAX(sale_id) FROM sales), (SELECT fruit_id FROM
$\sim$ fruits WHERE fruit_name = "cherry"), 4, (SELECT
$\sim$ selling_price FROM fruits WHERE fruit_name = "cherry"),
$\sim$ (SELECT selling_price FROM fruits WHERE fruit_name =
$\sim$ "cherry") + 4);
Step4: Update the stock quantity of apple and cherry
UPDATE fruits
SET stock_quantity = CASE
WHERE fruit_name = "apple" THEN stock_quantity = 9
WHERE fruit_name = "cherry" THEN stock_quantity = 4
ELSE stock_quantity
END
WHERE fruit_name IN ("apple", "cherry");</p>
<p>Step3: Increase the stock_quantity for each fruit sold in this sale, replace <quantity_sold> <fruit_id> with the results from the previous queries
UPDATE fruits
SET stock_quantity = stock_quantity + 10 [<quantity_sold>]
WHERE fruit_id = 7 [<fruit_id>];
UPDATE fruits
SET stock_quantity = stock_quantity + 9 [<quantity_sold>]
WHERE fruit_id = 6 [<fruit_id>];
Step4: Delete the sale items for this sale, replace <sale_id> with the results from the previous queries
DELETE FROM sale_items WHERE sale_id = 9 [<sale_id>];
Step5: Delete the sale record, replace <sale_id> with the results from the previous queries
DELETE FROM sales WHERE sale_id = 9 [<sale_id>];
(d) Changing prices: modify the price of the goods.</p>
<p>Record: On 2023-01-05, the sale price of pear in the store was changed to 1.6 dollar per unit.
ChatDB:
Begin to interact with the external database (symbolic memory).
Step1: Update the selling price of pear
UPDATE fruits
SET selling_price $=1.6$
WHERE fruit_name = "pear";</p>
<p>Figure 3: Examples showcasing ChatDB's response to the four common operations in the records of the Fruit Shop Dataset. " 9 [<sale_id>]" represents that <sale_id> is replaced with 9 in the updated SQL statement. Due to page size limitations, some unnecessary details and ChatDB's summary are omitted.</p>
<p>Suppose you are a fruit shop manager and good at analyzing history records.
The fruit shop newly opened on January 1, 2023. Given the history records for the fruit shop in January 2023, which include customer names, transaction dates, fruit prices, quantities purchased, and whether the items were returned, you need to answer some questions.
By default, exclude the sales transactions that have been returned when performing calculations.
Here are the historical records of the fruit shop, which are arranged in chronological order based on the occurrence time, surrounded by triple backticks:
$\cdots$
${$ records $}$
$\cdots$
Based on the history records, answer the question about the fruit shop:
${$ question $}$
Figure 4: Prompt for ChatGPT to answer questions in the Fruit Shop Dataset. The placeholders "records" and "question" will be replaced by specific details when it is actually used.</p>
<p>Why do we limit the token length of the dataset? If the token length of the dataset exceeds the maximum token length of ChatGPT, memory becomes necessary. However, the mainstream memory retrieval methods based on vector embedding are prone to errors. This inevitably leads to a decline in the performance of ChatGPT, which is not desired. Therefore, we deliberately design the token length of the dataset to be within the maximum token length of ChatGPT to avoid using memory and maximize the model's performance. Note that ChatDB's performance is generally unaffected by the token length of the dataset. Thus, if ChatDB outperforms ChatGPT when the dataset is small, it indicates that ChatDB is also superior to memory-augmented ChatGPT when the dataset is large.</p>
<h1>4.1.3 Processing records</h1>
<p>For ChatDB, the first step is to initialize the database. We need to generate a reasonable database schema for the specific task scenario and create tables in the database. The generation of the database schema can be done manually or using LLMs. Next, for each record in the dataset, ChatDB processes them one by one. Using the LLM controller, ChatDB manipulates the external database (i.e., symbolic memory) following Algorithm 1. We provide examples of ChatDB's response to the four common operations in the Fruit Shop Dataset, namely purchasing, selling, changing prices, and goods returns, as shown in Figure 3. It is worth emphasizing that ChatDB processes record one by one, so it is not sensitive to the total number of records. Furthermore, each step of the database operation in ChatDB is symbolic and free from errors. Therefore, in theory, ChatDB can handle an infinite number of historical records without sacrificing performance. However, for ChatGPT or existing memory-augmented LLMs, excessively long historical records can significantly degrade performance. In this experiment, as for the ChatGPT baseline, since the records are not long, we simply treat them as part of the prompt.</p>
<h3>4.1.4 Answering questions</h3>
<p>When answering questions, ChatDB no longer requires records to be part of the prompt. After processing the records, the information is stored in symbolic memory. Following Algorithm 1, ChatDB utilizes SQL statements to perform a series of database queries (including calculations) in order to answer the question. On the other hand, ChatGPT includes records as part of the prompt and directly asks the question. The prompt template is shown in Figure 4.</p>
<h3>4.2 Results</h3>
<p>The experimental results are presented in Table 2, which clearly demonstrate that ChatDB outperforms ChatGPT with significantly higher accuracy. While ChatGPT is capable of answering easy questions, it falls short in handling hard questions that necessitate multi-hop reasoning and precise calculations. Consequently, ChatGPT exhibits a low accuracy rate for these hard questions. In contrast, ChatDB exhibits a notably high accuracy rate, underscoring the advantages of utilizing a database as symbolic memory. This approach not only prevents error accumulation but also enhances the multi-hop reasoning and precise calculation capabilities of LLMs.
We present several examples of the two models answering questions in Figure 5 for comparison. In all these examples, ChatDB correctly answers the questions while ChatGPT fails. ChatGPT often exhibits errors in calculating the total price of each sale transaction, as observed in Figure 5(a). Sometimes, the formulas are correct but the calculations are wrong, while other times, even the formulas are incorrect. In addition, ChatGPT struggles to find all valid sale</p>
<p>(a) Question: What was the total revenue for January 2023?</p>
<h2>ChatGPT:</h2>
<p>To calculate the total revenue for January 2023, we need to sum up the revenue generated from each sales transaction. We can use the following formula to calculate the revenue for each transaction: revenue $=$ quantity $*$ selling price We also need to exclude the sales transactions that have been returned. Based on the history records, the following sales transactions have been returned:</p>
<ul>
<li>Alice Smith on 2023-01-17 - Bob Smith on 2023-01-05 [...]</li>
</ul>
<p>Therefore, we need to exclude these transactions when calculating the total revenue. Here is the calculation for each sales transaction:</p>
<ul>
<li>Bob Smith on 2023-01-02: $(9 \mathrm{~kg} * 3.8 \mathrm{USD} / \mathrm{kg})+(4 \mathrm{~kg} * 1.3 \mathrm{USD} / \mathrm{kg})=39.1$ USD [WRONG]: calculation error, should be 39.4</li>
<li>Sue Davis on 2023-01-02: $1 \mathrm{~kg} * 3.8$ USD/kg $=3.8$ USD</li>
<li>Bob Smith on 2023-01-05: $(5 \mathrm{~kg} * 1.8 \mathrm{USD} / \mathrm{kg})+(2 \mathrm{~kg} * 3.0 \mathrm{USD} / \mathrm{kg})+(6 \mathrm{~kg}$ * 2.8 USD/kg) $=47.6$ USD [WRONG]: calculation error and this transaction has been returned.
[...] [WRONG]: most of the sale transactions are calculated incorrectly.
[STOP]: Because it exceeds the maximum token length.
(b) Question: Which day in January 2023 had the highest revenue?</li>
</ul>
<h2>ChatGPT:</h2>
<p>To calculate the revenue for each day, we need to sum up the total sales for that day (excluding any returned items) and multiply by the selling price. We can then compare the revenue for each day to determine which day had the highest revenue.
Here is the breakdown of sales for each day:</p>
<ul>
<li>January 2: 9 kg apple at $3.8=34.2,4 \mathrm{~kg}$ cherry at $1.3=5.2,3 \mathrm{~kg}$ cherry at $1.3=$ $3.9,5 \mathrm{~kg}$ orange at $2.1=6.3$, total sales $=49.6$ [WRONG]: should be 43.2</li>
<li>January 4: 2 kg apple at $1.5=3,4 \mathrm{~kg}$ pear at $1.6=6.4,7 \mathrm{~kg}$ cherry at $1.3=9.1$, total sales $=18.5$ [WRONG]: should be 27.9</li>
<li>January 5: 5 kg cherry at $1.9=9.5,10 \mathrm{~kg}$ orange at $1.8=18,1 \mathrm{~kg}$ cherry at 1.9 $=1.9,1 \mathrm{~kg}$ grape at $3.2=3.2$, total sales $=32.6$ [WRONG]: should be 83</li>
<li>January 6: 3 kg apple at $1.5=4.5,10 \mathrm{~kg}$ grape at $3.2=32$, total sales $=36.5$ [WRONG]: should be 29.4</li>
<li>January 7: 4 kg pear at $1.6=6.4$, total sales $=6.4$ [WRONG]: should be 37.2</li>
<li>January 8: 6 kg peach at $3=18$, total sales $=18$ [WRONG]: should be 21.6
[...] [WRONG]: most of the daily revenues are calculated incorrectly.
[STOP]: Because it exceeds the maximum token length.</li>
</ul>
<h2>ChatDB:</h2>
<p>Begin to interact with the external database (symbolic memory).
Step1: Calculate the total revenue for January 2023
SELECT SUM (total_price) AS total_revenue
FROM sales
WHERE sale_date &gt;= <em>2023-01-01</em> AND sale_date &lt; <em>2023-02-01</em>;
Database response:
{
\total_revenue
$\mid$
707.8
}</p>
<h2>ChatDB:</h2>
<p>Begin to interact with the external database (symbolic memory).
Step1: Calculate the total revenue for each day in January 2023
SELECT sale_date, SUM (total_price) AS revenue
FROM sales
WHERE sale_date BETWEEN <em>2023-01-01</em> AND <em>2023-01-31</em> GROUP BY sale_data;</p>
<p>Database response:
{
\sale_date | revenue
}
2023-01-02 43.2
2023-01-04 27.9
Step2: Find the day with the highest revenue
SELECT sale_date, SUM (total_price) AS revenue
FROM sales
WHERE sale_date BETWEEN <em>2023-01-01</em> AND <em>2023-01-31</em> GROUP BY sale_date ORDER BY revenue DESC LIMIT 1;</p>
<p>Database response:
}
\sale_date | revenue
}
2023-01-30 119.7
}
(c) Question: What was the average weight of fruit per sales transaction in January 2023?</p>
<p>ChatGPT: To calculate the average weight of fruit per sales transaction, we need to first identify all the sales transactions in January 2023 and calculate the total weight of fruit sold in those transactions. Then, we can divide the total weight by the number of sales transactions to get the average weight per transaction. Here are the sales transactions in January 2023:</p>
<ul>
<li>Bob Smith on 2023-01-02: 9 kg apple, 4 kg cherry</li>
<li>Sue Davis on 2023-01-02: 1 kg apple</li>
<li>Bob Smith on 2023-01-05: 5 kg orange, 2 kg peach, 6 kg pear [WRONG]: this transaction has been returned.</li>
<li>Sue Davis on 2023-01-05: 5 kg pear, 10 kg orange, 1 kg cherry</li>
<li>Alice Smith on 2023-01-05: 5 kg cherry
[...] [WRONG]: not find out all valid sale transactions.
To calculate the total weight of fruit sold, we can add up the quantities of each fruit sold in the above transactions:</li>
<li>Apples: 16 kg - Cherries: 12 kg - Oranges: 10 kg - Pears: 18 kg - Kiwis: 12 kg Grapes: 18 kg - Bananas: 27 kg - Strawberries: 7 kg - Peaches: 9 kg - Plums: 8 kg
The total weight of fruit sold in January 2023 is 127 kg . [WRONG]: It should be 278. Even according to the calculation above, the total should be 137, not 127. To calculate the average weight per sales transaction, we need to divide the total weight by the number of sales transactions. There are 18 sales transactions in January 2023, [WRONG]: It should be 28 valid sale transactions in total. so the average weight [STOP]: Because it exceeds the maximum token length.</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: left;">ChatDB:</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Begin to interact with the external database (symbolic memory).</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Step1: Calculate the total weight of fruit sold in January 2023</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">SELECT SUM(si.quantity_sold) as</td>
<td style="text-align: left;">Database response:</td>
</tr>
<tr>
<td style="text-align: left;">$\rightarrow$ total_weight</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">FROM sale_items si</td>
<td style="text-align: left;">total_weight</td>
</tr>
<tr>
<td style="text-align: left;">JOIN sales $x$ ON si.sale_id = s.sale_id</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">WHERE s.sale_date BETWEEN <em>2023-01-01</em></td>
<td style="text-align: left;">278</td>
</tr>
<tr>
<td style="text-align: left;">$\rightarrow$ AND <em>2023-01-31</em></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Step2: Count the number of sales transactions in January 2023
SELECT COUNT (DISTINCT s.sale_id) as
$\rightarrow$ num_sales
FROM sales $x$
WHERE s.sale_date BETWEEN <em>2023-01-01</em> $\rightarrow$ AND <em>2023-01-31</em></p>
<div class="codehilite"><pre><span></span><code>\s avg_weight_per_sale
</code></pre></div>

<p>Step3: Calculate the average weight of fruit per sales transaction in January 2023, replace <total_weight> and <num_sales> with the results from the previous queries</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SELECT 278</td>
<td style="text-align: left;"><total_weight></td>
</tr>
<tr>
<td style="text-align: left;">$\rightarrow$</td>
<td style="text-align: left;">$&lt;$ <num_sales></td>
</tr>
<tr>
<td style="text-align: left;">as avg_weight_per_sale</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">9.93</td>
</tr>
</tbody>
</table>
<p>Figure 5: Examples of ChatDB and ChatGPT answering questions. While ChatGPT fails to answer any of the questions correctly, ChatDB successfully answers all of them. The issues with ChatGPT are highlighted in red. Some irrelevant details and ChatDB's summary are omitted for brevity. The gray ellipsis [...] indicates that the response is trimmed.</p>
<p>Table 2: The experimental results of answering questions in the Fruit Shop Dataset. There are 50 questions in total, with 15 being easy questions and 35 being hard questions.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Easy</th>
<th>Hard</th>
<th>All</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>ChatGPT</td>
<td>$10 / 15$</td>
<td>$1 / 35$</td>
<td>$11 / 50$</td>
<td>$22 \%$</td>
</tr>
<tr>
<td>ChatDB (ours)</td>
<td>$13 / 15$</td>
<td>$28 / 35$</td>
<td>$41 / 50$</td>
<td>$82 \%$</td>
</tr>
</tbody>
</table>
<p>transactions, leading to errors in its answering process. This issue is commonly encountered and evident in all these examples. Furthermore, ChatGPT tends to make sequential errors, resulting in significant error accumulation.</p>
<p>In contrast, ChatDB performs quite well in these examples. During the initial processing of records, symbolic operations (i.e., SQL operations) are applied to manipulate the database (i.e., symbolic memory), ensuring that all information is stored in a structured form within the database. When answering questions, ChatDB generates SQL statements to query the database. The three examples demonstrate the effectiveness of ChatDB in solving problems requiring one, two, and three chain-of-memory steps, respectively. We can observe that ChatDB accurately answers the questions, and the execution logic of the chain-of-memory is clear, with each step tightly connected and approaching the ultimate answer. From these examples, the advantages of ChatDB are evident in two aspects:</p>
<ol>
<li>Through the chain-of-memory approach, complex problems are decomposed into multiple steps of memory operations, simplifying the problem's complexity. Each step's result is accurately stored as an intermediate outcome and used in subsequent steps, which greatly assists in complex reasoning.</li>
<li>Symbolic memory enables precise operations and calculations. ChatDB delegates many calculational tasks to the external database by executing SQL statements, ensuring the accuracy of each step and preventing error accumulation.</li>
</ol>
<p>In summary, by leveraging external databases as symbolic memory, ChatDB significantly outperforms ChatGPT in this experiment.</p>
<h1>5 Conclusion</h1>
<p>In this paper, we introduce ChatDB, a framework that augments LLMs with symbolic memory in the form of databases. We demonstrate the advantages and capabilities of symbolic memory and chain-of-memory approach in enhancing complex reasoning and preventing error accumulation. By providing a precise storage mechanism for intermediate results, symbolic memory enables accurate and reliable operations. Moreover, the use of symbolic languages, such as SQL, allows symbolic computation and manipulation of stored information. Through experimental evaluation, we observe a significant improvement in performance with ChatDB compared to ChatGPT. The integration of symbolic memory in ChatDB substantially enhances the model's ability to handle various queries and reasoning tasks in management settings. This improvement highlights the benefits and effectiveness of leveraging symbolic memory in LLMs.</p>
<h2>References</h2>
<p>Andor, D., He, L., Lee, K., and Pitler, E. (2019). Giving bert a calculator: Finding operations and arguments with reading comprehension. arXiv preprint arXiv:1909.00109.</p>
<p>Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen, Z., et al. (2023). Palm 2 technical report. arXiv preprint arXiv:2305.10403.</p>
<p>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Bulatov, A., Kuratov, Y., and Burtsev, M. (2022). Recurrent memory transformer. Advances in Neural Information Processing Systems, 35:11079-11091.</p>
<p>Bulatov, A., Kuratov, Y., and Burtsev, M. S. (2023). Scaling transformer to 1m tokens and beyond with rmt. arXiv preprint arXiv:2304.11062.</p>
<p>Chen, A., Phang, J., Parrish, A., Padmakumar, V., Zhao, C., Bowman, S. R., and Cho, K. (2023). Two failures of self-consistency in the multi-step reasoning of llms. arXiv preprint arXiv:2305.14279.</p>
<p>Chen, W., Ma, X., Wang, X., and Cohen, W. W. (2022). Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588.</p>
<p>Cheng, Z., Xie, T., Shi, P., Li, C., Nadkarni, R., Hu, Y., Xiong, C., Radev, D., Ostendorf, M., Zettlemoyer, L., et al. (2022). Binding language models in symbolic languages. arXiv preprint arXiv:2210.02875.</p>
<p>Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. (2021). Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Du, Z., Qian, Y., Liu, X., Ding, M., Qiu, J., Yang, Z., and Tang, J. (2022). Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 320-335.</p>
<p>Graves, A., Wayne, G., and Danihelka, I. (2014). Neural turing machines. arXiv preprint arXiv:1410.5401.
Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M. (2020). Retrieval augmented language model pre-training. In International conference on machine learning, pages 3929-3938. PMLR.</p>
<p>Hao, S., Gu, Y., Ma, H., Hong, J. J., Wang, Z., Wang, D. Z., and Hu, Z. (2023). Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992.</p>
<p>Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel, S., and Grave, E. (2022). Few-shot learning with retrieval augmented language models. arXiv preprint arXiv:2208.03299.</p>
<p>Johnson, D. D. (2017). Learning graphical state transitions. In International conference on learning representations.
Khattab, O., Santhanam, K., Li, X. L., Hall, D., Liang, P., Potts, C., and Zaharia, M. (2022). Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp. arXiv preprint arXiv:2212.14024.</p>
<p>Lester, B., Al-Rfou, R., and Constant, N. (2021). The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691.</p>
<p>Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.-t., Rocktäschel, T., et al. (2020). Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459-9474.</p>
<p>OpenAI (2023). Gpt-4 technical report.
Paranjape, B., Lundberg, S., Singh, S., Hajishirzi, H., Zettlemoyer, L., and Ribeiro, M. T. (2023). Art: Automatic multi-step reasoning and tool-use for large language models. arXiv preprint arXiv:2303.09014.</p>
<p>Park, J. S., O’Brien, J. C., Cai, C. J., Morris, M. R., Liang, P., and Bernstein, M. S. (2023). Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442.</p>
<p>Piękos, P., Michalewski, H., and Malinowski, M. (2021). Measuring and improving bert's mathematical abilities by predicting the order of reasoning. arXiv preprint arXiv:2106.03921.</p>
<p>Ran, Q., Lin, Y., Li, P., Zhou, J., and Liu, Z. (2019). Numnet: Machine reading comprehension with numerical reasoning. arXiv preprint arXiv:1910.06701.</p>
<p>Schick, T., Dwivedi-Yu, J., Dessì, R., Raileanu, R., Lomeli, M., Zettlemoyer, L., Cancedda, N., and Scialom, T. (2023). Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761.</p>
<p>Schuurmans, D. (2023). Memory augmented large language models are computationally universal. arXiv preprint arXiv:2301.04589.</p>
<p>Shen, Y., Song, K., Tan, X., Li, D., Lu, W., and Zhuang, Y. (2023). Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580.</p>
<p>Sun, R., Arik, S. O., Nakhost, H., Dai, H., Sinha, R., Yin, P., and Pfister, T. (2023). Sql-palm: Improved large language modeladaptation for text-to-sql.</p>
<p>Surís, D., Menon, S., and Vondrick, C. (2023). Vipergpt: Visual inference via python execution for reasoning. arXiv preprint arXiv:2303.08128.</p>
<p>Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al. (2023). Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.</p>
<p>Wang, W., Chen, Z., Chen, X., Wu, J., Zhu, X., Zeng, G., Luo, P., Lu, T., Zhou, J., Qiao, Y., et al. (2023). Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. arXiv preprint arXiv:2305.11175.</p>
<p>Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and Zhou, D. (2022). Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.</p>
<p>Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. (2021). Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652.</p>
<p>Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., and Zhou, D. (2022). Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.</p>
<p>Wu, Y., Rabe, M. N., Hutchins, D., and Szegedy, C. (2022a). Memorizing transformers. arXiv preprint arXiv:2203.08913.</p>
<p>Wu, Y., Zhao, Y., Hu, B., Minervini, P., Stenetorp, P., and Riedel, S. (2022b). An efficient memory-augmented transformer for knowledge-intensive nlp tasks. arXiv preprint arXiv:2210.16773.</p>
<p>Zeng, A., Liu, X., Du, Z., Wang, Z., Lai, H., Ding, M., Yang, Z., Xu, Y., Zheng, W., Xia, X., et al. (2022). Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414.</p>
<p>Zhong, Z., Lei, T., and Chen, D. (2022). Training language models with memory augmentation. arXiv preprint arXiv:2205.12674.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://github.com/Significant-Gravitas/Auto-GPT
${ }^{4}$ https://chatexcel.com/
${ }^{5}$ https://github.com/Significant-Gravitas/Auto-GPT&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4720 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4720</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4720</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-102.html">extraction-schema-102</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <p><strong>Paper ID:</strong> paper-8c7846c9805834dbe2fb0c8f48253b8d65b79d6a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/8c7846c9805834dbe2fb0c8f48253b8d65b79d6a" target="_blank">Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> Goat, a fine-tuned LLaMA model that significantly outperforms GPT-4 on a range of arithmetic tasks, is introduced and an approach that classifies tasks based on their learnability, and subsequently decomposes unlearnable tasks into a series of learnable tasks by leveraging basic arithmetic principles is proposed.</p>
                <p><strong>Paper Abstract:</strong> We introduce Goat, a fine-tuned LLaMA model that significantly outperforms GPT-4 on a range of arithmetic tasks. Fine-tuned on a synthetically generated dataset, Goat achieves state-of-the-art performance on BIG-bench arithmetic sub-task. In particular, the zero-shot Goat-7B matches or even surpasses the accuracy achieved by the few-shot PaLM-540B. Surprisingly, Goat can achieve near-perfect accuracy on large-number addition and subtraction through supervised fine-tuning only, which is almost impossible with previous pretrained language models, such as Bloom, OPT, GPT-NeoX, etc. We attribute Goat's exceptional performance to LLaMA's consistent tokenization of numbers. To tackle more challenging tasks like large-number multiplication and division, we propose an approach that classifies tasks based on their learnability, and subsequently decomposes unlearnable tasks, such as multi-digit multiplication and division, into a series of learnable tasks by leveraging basic arithmetic principles. We thoroughly examine the performance of our model, offering a comprehensive evaluation of the effectiveness of our proposed decomposition steps. Additionally, Goat-7B can be easily trained using LoRA on a 24GB VRAM GPU, facilitating reproducibility for other researchers. We release our model, dataset, and the Python script for dataset generation.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4720.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4720.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Goat-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Goat (fine-tuned LLaMA-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A LLaMA-7B model fine-tuned via supervised instruction tuning on ~1M synthetic arithmetic examples; uses explicit Chain-of-Thought (CoT) decomposition for multi-digit multiplication/division and generates direct answers for addition/subtraction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Goat-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA-7B base model fine-tuned with LoRA on ~1M synthetically generated instruction-answer pairs; batch size 128, LR 3e-4, 1 epoch typical; trained to output CoT for unlearnable composite tasks and direct answers for learnable tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Addition, Subtraction (up to 16D), Multiplication (nD×1D learnable; multi-digit decomposed with CoT), Division (nD÷1D learnable; multi-digit decomposed with CoT); evaluated on BIG-bench arithmetic and extra large-number tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Primary hypothesis: consistent digit-level tokenization in LLaMA enables the model to learn digitwise patterns for direct arithmetic (especially addition/subtraction); for inherently unlearnable composite tasks, supervised CoT that decomposes into learnable subtasks enables reliable computation by reducing required working memory and exposing intermediate supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>High accuracy for addition/subtraction after supervised finetuning without CoT (near-perfect on test sets); ablation studies where removing CoT for multi-digit multiplication/division yields exact-match accuracy ~0; tokenization table (Table 5) showing LLaMA splits digits into individual tokens; fine-tuning other models on identical data fails to match LLaMA's performance.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Limited extrapolation beyond training-digit distributions (performance degrades gradually for digits > trained max); some special multi-digit cases can be overfit without CoT (e.g., exhaustive 2-digit×2-digit), indicating memorization/overfitting can imitate learning under constrained settings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 3 summary (Exact String Match / Digit Match): ADD (Goat-7B) 1D..5D: 100/100,100/100,99.4/99.8,98.3/99.5,98.1/99.4; 8D+8D 97.8/99.4; 16D+8D 97.1/99.6; 16D+16D 97.6/99.7. SUB similar high numbers (≥95% exact for large sizes). MUL: 1D..5D 100/100,100/100,97.8/99.4,96.9/99.2,96.7/99.3; 1D×16D 99.7/99.9; 4D×8D 88.1/97.8; 6D×6D 96.8/99.5. DIV: similarly high: e.g., 6D÷3D 94.1/96.1; 12D÷6D 89.3/93.5. Many tasks reach near-perfect digit-match even when exact-match slightly lower.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Ablation studies: removing CoT steps (split, expand, add-term-by-term) for 4D×4D reduces exact-match to near 0; 'adding term by term' step is most critical. Simplified synthetic environment used to control NL variance. Cross-model fine-tuning: identical dataset applied to Bloom/OPT/GPT-NeoX/Pythia yields much higher training loss and poor arithmetic performance, implicating tokenization as an intervention variable.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Limited extrapolation to numbers outside training distribution; multi-digit multiplication/division require CoT decomposition — direct-answer training fails; training cost grows with digit length; special failure modes observed in other models (misalignment, copying errors, incorrect intermediate nD×1D products) also noted for GPT-4 when it fails; Goat's CoT design is human-interpretable but may not be the most computationally efficient algorithm for the model to learn.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Goat-7B (fine-tuned LLaMA-7B) outperforms GPT-4 on many BIG-bench arithmetic sub-tasks (Table 3) in zero-shot settings and matches or exceeds PaLM-540B few-shot; other open models (Bloom, OPT, GPT-NeoX, Pythia) fine-tuned on same data fail to reach LLaMA/Goat performance, attributed primarily to tokenization differences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4720.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4720.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA tokenization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA consistent digit-level tokenization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLaMA's tokenizer splits individual decimal digits into separate tokens, creating consistent digit-level representations which the paper argues are key to learning arithmetic patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llama: Open and efficient foundation language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA tokenizer (representation property)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Subword tokenizer used by LLaMA that, according to the paper's analysis, often maps each digit character to an individual token, producing stable per-digit tokenization across numbers of different lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Relevant across all integer arithmetic tasks (addition, subtraction, multiplication, division) because digitwise representation affects sequence modeling of numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Consistent digit-level tokenization permits the model to learn per-digit transformations and positional patterns (carry/borrow behaviors) as localized sequence patterns rather than having to learn variable-width token meanings, enabling supervised learning of arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Tokenization comparisons in Table 5 show LLaMA token sequences for multi-digit numbers correspond to digits; models with inconsistent tokenization (GPT-4, Bloom, OPT, Pythia, etc.) show tokens that aggregate digits irregularly. Fine-tuning identical datasets yields markedly better learning in LLaMA than in other models.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Not a formal causal proof; other factors (architecture, pretraining data) could contribute; the paper notes ChatGLM also tokenizes digits individually but was not evaluated, leaving open whether digit tokenization alone suffices.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Indirect: models with digit-level tokenization (LLaMA→Goat) achieve near-perfect exact/digit matches on many tasks; models without consistent digit-level tokenization show much higher loss and poor arithmetic learning when fine-tuned on same data (no numeric table provided per model beyond qualitative statements and Table 3 for GPT-4).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Cross-model fine-tuning experiment: identical arithmetic dataset and hyperparameters applied to Bloom/OPT/GPT-J/GPT-NeoX/Pythia; these models struggle to learn tasks LLaMA can, consistent with tokenization hypothesis. No direct ablation of tokenizer within LLaMA reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Correlation, not definitive causation; other emergent factors may contribute. Tokenization does not solve working-memory-style composite tasks (multi-digit multiplication/division) without CoT decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Compared tokenizations (Table 5): LLaMA splits digits; GPT-4 and Bloom often tokenize multi-digit numbers into irregular multi-digit tokens, which the paper links to worse arithmetic learning and digit-alignment problems in CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4720.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4720.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Supervised Chain-of-Thought decomposition for arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised CoT protocol that decomposes multi-digit multiplication and division into a sequence of learnable subtasks (extraction, split, expansion, product, add term by term) to enable LLMs to compute reliably.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chain-of-Thought decomposition (arithmetic sketchpad)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A hand-designed sequence of intermediate steps (1 extraction, 2 split, 3 expansion via distributive law, 4 compute nD×1D products, 5 add terms iteratively) used as intermediate supervision during fine-tuning so the model outputs CoT then final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-digit multiplication (nD×mD) and multi-digit division (nD÷mD) where both operands have >1 digit.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Decomposing an unlearnable composite task into polynomially many learnable subtasks reduces the complexity and working-memory demand, enabling sequence models to learn the computation via supervised intermediate steps.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Ablation studies (Fig. 2 and 3) show that with CoT the model attains high exact-match accuracy on 4D×4D and 6D÷3D tasks, while without CoT accuracy remains ~0; removing specific CoT steps (especially 'adding term by term') substantially harms learning. Few-shot prompting of GPT-4 with the decomposition also improves its correctness vs. default long multiplication/division.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>CoT increases sequence length and training cost; not all CoT designs optimal—authors acknowledge other algorithms might be more efficient; GPT-4 sometimes produces incorrect intermediate steps yet still correct final answers, suggesting CoT is not always necessary or that models may use alternative internal heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>With CoT, Goat-7B achieves high exact-match on multi-digit multiplication/division tasks (e.g., 4D×4D and 6D÷3D reach high exact percentages reported in Table 3: 6D×6D 96.8/99.5; 12D÷6D 89.3/93.5). Without CoT, exact-match near 0 on those tasks (ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Ablation: removing split/expand/add steps; 'adding term by term' critical. Simplified synthetic environment used to isolate CoT effect from NL variability. Authors trained models to produce CoT then answer, showing faster convergence (e.g., 2-digit×2-digit direct answer took ~10 epochs to overfit, whereas CoT achieved comparable accuracy in 1 epoch).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>CoT relies on human-designed intermediate steps; may not generalize to arbitrarily large inputs due to extrapolation limits; increases supervision and output length; design choices (which subtasks) affect learning and efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>CoT approach aligns with prior scratchpad/CoT studies (Nye et al., 2021; Kojima et al., 2022) but in this paper CoT is necessary for multi-digit composite tasks in LLaMA fine-tuning context, and applying the same CoT few-shot to GPT-4 improves but does not fully close performance gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4720.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4720.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (arithmetic behavior)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large, state-of-the-art pretrained autoregressive transformer which performs well on many reasoning tasks but shows notable failure modes on large-number arithmetic, especially multi-digit multiplication/division without specialized prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-4 technical report.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary large-scale transformer model (details in OpenAI technical report); evaluated via API (May 10th) on BIG-bench arithmetic tasks and extra tasks with and without Chain-of-Thought prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Addition, Subtraction (large numbers sometimes OK), Multiplication and Division (multi-digit operations problematic), evaluated on BIG-bench arithmetic sub-task and extra tasks with large-digit numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>GPT-4 tends to use long multiplication/long division heuristics when prompted for step-by-step solutions, but has limited working memory and possibly inconsistent digit tokenization that impedes accurate digit alignment and copying required for exact arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical failure modes: when generating CoT, GPT-4's intermediate steps often exhibit digit misalignment, copying errors, or incorrect nD×1D intermediate products; despite sometimes incorrect steps, final answer occasionally correct, implying nontransparent internal heuristics. Performance table (Table 3) shows GPT-4 accuracy falls sharply on multi-digit multiplication/division (e.g., MUL: 3D 30.3/83.0, 4D 5.3/61.8, 5D 0.0/47.9; DIV similarly drops for larger divisors).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>GPT-4 performs well on many addition/subtraction tasks and on some multi-digit cases (e.g., 8D+8D and 16D+16D), indicating that failure is not universal; also few-shot decomposition prompting helps improve correctness, showing GPT-4 can follow structured CoT when given guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 3 (Exact/Digit): ADD: generally high (e.g., 16D+16D 94.1/99.5 though some anomalies); SUB: drops on large cross-size tasks (e.g., 16D-8D 10.6/68.8); MUL: severe decline past small digits (e.g., 6D×6D 0.0/49.8); DIV: 12D÷6D 0.0/29.5, etc. Digit match often much higher than exact string match, indicating many correct digits but some errors.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Appending 'Solve it step by step' yields marginal improvement; few-shot prompting with the paper's decomposition examples improves GPT-4's correctness compared to its default long-multiplication/long-division CoT but still exhibits digit-alignment and copying mistakes. Authors identified three common GPT-4 errors: digit alignment, copying of numbers, and incorrect nD×1D intermediate results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Short working memory and inconsistent tokenization lead to digit misalignment, copying errors, and bad intermediate multiplication results; poor extrapolation when numbers exceed seen distributions; CoT sometimes does not help because GPT-4 may not leverage intermediate supervision effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Compared directly to Goat-7B in zero-shot on BIG-bench and extra tasks: Goat often outperforms GPT-4 on multi-digit arithmetic (Table 3). GPT-4 still strong on many smaller or similar-sized-addition/subtraction tasks but weaker on composite multiplication/division without tailored CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4720.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4720.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Other LLMs (tokenization failures)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bloom, OPT, GPT-NeoX, GPT-J, Pythia, etc. (open LLMs with irregular tokenization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of open-source LLMs that, when fine-tuned on the same arithmetic dataset, underperform relative to LLaMA/Goat, attributed primarily to inconsistent tokenization of numbers and higher fine-tuning loss.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Bloom / OPT / GPT-NeoX / GPT-J / Pythia (grouped)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source autoregressive transformer models with various tokenizer designs; Table 5 shows they commonly tokenize multi-digit numbers into variable-length subword tokens that do not align to single digits consistently.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Same integer arithmetic tasks as Goat (addition, subtraction, multiplication, division) when fine-tuned on the synthetic dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Irregular tokenization (tokens representing varying numbers of digits) disrupts the model's ability to learn digitwise arithmetic patterns, making tasks effectively harder and increasing reliance on memorization or overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Experiments fine-tuning these models on identical arithmetic datasets yielded significantly higher training loss and poor test performance compared to LLaMA; Table 5 documents irregular tokenization patterns for these models compared to LLaMA's digit-level tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>No in-paper evidence entirely ruling out other contributors (model size, pretraining corpus); ChatGLM also tokenizes digits individually yet was not evaluated, so tokenization is likely necessary but may not be sufficient alone.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative: models 'struggle with arithmetic tasks' and 'cannot match LLaMA's arithmetic ability' per Section 5.3; exact numeric performance per model not listed beyond GPT-4 comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Fine-tuning intervention: identical dataset/hyperparameters used; these models showed slower/poorer convergence. No tokenizer-retraining or controlled ablation isolating tokenizer effect was reported beyond cross-model comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>High training loss and inability to learn learnable tasks that LLaMA can; potential overfitting needed to memorize small-range tasks; tokenization irregularity introduces representational inconsistency.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Direct comparison in paper: LLaMA/Goat significantly outperforms these models when all are fine-tuned on the same synthetic arithmetic data; tokenization differences identified as main explanatory factor.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4720.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4720.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Learnability classification</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learnable vs Unlearnable arithmetic tasks (empirical taxonomy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An empirical classification separating arithmetic tasks the model can learn to output direct answers for (learnable) from composite tasks that require decomposition/CoT (unlearnable) under supervised finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Learnability taxonomy (arithmetic)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Task-level classification based on observed ability of finetuned LLaMA to generate direct answers: learnable tasks include copying, split, comparison, ordering, addition, subtraction, nD×1D multiplication, nD÷1D division; unlearnable tasks include multi-digit nD×mD multiplication and nD÷mD division with n,m>1.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Used to determine when CoT decomposition is necessary (multi-digit composite tasks) vs when direct supervised outputs suffice (addition/subtraction, nD×1D, nD÷1D).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Complexity and working-memory requirement correlate with unlearnability; tasks decomposable into a polynomial number of simple subtasks become learnable when intermediate supervision is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical fine-tuning results showing direct-answer training succeeds for learnable tasks but fails (even with extensive training) for unlearnable tasks; referencing theoretical work (Wies et al., 2022) that intermediate supervision makes certain composite tasks learnable.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Some unlearnable tasks can be brute-force overfit on small finite domains (e.g., exhaustive 2D×2D enumeration), indicating boundary cases where classification depends on dataset and training regime rather than inherent impossibility.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 1 enumerates learnable vs unlearnable categories; experimental outcomes: learnable tasks reach high exact/digit-match with supervised finetuning (see Goat-7B performance metrics), unlearnable tasks require CoT to reach similar accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Ablation and synthetic-environment experiments show breakdown of learning behavior with/without intermediate supervision; authors cite Wies et al. (2022) for theoretical grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Taxonomy empirical and partially heuristic; boundaries depend on model/tokenization/training data; not a proof of impossibility—some unlearnable tasks become learnable with overfitting or different algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Classification is model-dependent: a task learnable for LLaMA might be unlearnable for other LLMs (observed experimentally).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Llama: Open and efficient foundation language models <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Show your work: Scratchpads for intermediate computation with language models <em>(Rating: 2)</em></li>
                <li>Investigating the limitations of transformers with simple arithmetic tasks <em>(Rating: 2)</em></li>
                <li>Sub-task decomposition enables learning in sequence to sequence tasks <em>(Rating: 2)</em></li>
                <li>Recursion of thought: Divide and conquer reasoning with language models <em>(Rating: 1)</em></li>
                <li>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks <em>(Rating: 1)</em></li>
                <li>Have you seen that number? investigating extrapolation in question answering models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4720",
    "paper_id": "paper-8c7846c9805834dbe2fb0c8f48253b8d65b79d6a",
    "extraction_schema_id": "extraction-schema-102",
    "extracted_data": [
        {
            "name_short": "Goat-7B",
            "name_full": "Goat (fine-tuned LLaMA-7B)",
            "brief_description": "A LLaMA-7B model fine-tuned via supervised instruction tuning on ~1M synthetic arithmetic examples; uses explicit Chain-of-Thought (CoT) decomposition for multi-digit multiplication/division and generates direct answers for addition/subtraction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Goat-7B",
            "model_description": "LLaMA-7B base model fine-tuned with LoRA on ~1M synthetically generated instruction-answer pairs; batch size 128, LR 3e-4, 1 epoch typical; trained to output CoT for unlearnable composite tasks and direct answers for learnable tasks.",
            "arithmetic_task_type": "Addition, Subtraction (up to 16D), Multiplication (nD×1D learnable; multi-digit decomposed with CoT), Division (nD÷1D learnable; multi-digit decomposed with CoT); evaluated on BIG-bench arithmetic and extra large-number tasks.",
            "mechanism_hypothesis": "Primary hypothesis: consistent digit-level tokenization in LLaMA enables the model to learn digitwise patterns for direct arithmetic (especially addition/subtraction); for inherently unlearnable composite tasks, supervised CoT that decomposes into learnable subtasks enables reliable computation by reducing required working memory and exposing intermediate supervision.",
            "evidence_for_mechanism": "High accuracy for addition/subtraction after supervised finetuning without CoT (near-perfect on test sets); ablation studies where removing CoT for multi-digit multiplication/division yields exact-match accuracy ~0; tokenization table (Table 5) showing LLaMA splits digits into individual tokens; fine-tuning other models on identical data fails to match LLaMA's performance.",
            "evidence_against_mechanism": "Limited extrapolation beyond training-digit distributions (performance degrades gradually for digits &gt; trained max); some special multi-digit cases can be overfit without CoT (e.g., exhaustive 2-digit×2-digit), indicating memorization/overfitting can imitate learning under constrained settings.",
            "performance_metrics": "Table 3 summary (Exact String Match / Digit Match): ADD (Goat-7B) 1D..5D: 100/100,100/100,99.4/99.8,98.3/99.5,98.1/99.4; 8D+8D 97.8/99.4; 16D+8D 97.1/99.6; 16D+16D 97.6/99.7. SUB similar high numbers (≥95% exact for large sizes). MUL: 1D..5D 100/100,100/100,97.8/99.4,96.9/99.2,96.7/99.3; 1D×16D 99.7/99.9; 4D×8D 88.1/97.8; 6D×6D 96.8/99.5. DIV: similarly high: e.g., 6D÷3D 94.1/96.1; 12D÷6D 89.3/93.5. Many tasks reach near-perfect digit-match even when exact-match slightly lower.",
            "probing_or_intervention_results": "Ablation studies: removing CoT steps (split, expand, add-term-by-term) for 4D×4D reduces exact-match to near 0; 'adding term by term' step is most critical. Simplified synthetic environment used to control NL variance. Cross-model fine-tuning: identical dataset applied to Bloom/OPT/GPT-NeoX/Pythia yields much higher training loss and poor arithmetic performance, implicating tokenization as an intervention variable.",
            "limitations_and_failure_modes": "Limited extrapolation to numbers outside training distribution; multi-digit multiplication/division require CoT decomposition — direct-answer training fails; training cost grows with digit length; special failure modes observed in other models (misalignment, copying errors, incorrect intermediate nD×1D products) also noted for GPT-4 when it fails; Goat's CoT design is human-interpretable but may not be the most computationally efficient algorithm for the model to learn.",
            "comparison_to_other_models": "Goat-7B (fine-tuned LLaMA-7B) outperforms GPT-4 on many BIG-bench arithmetic sub-tasks (Table 3) in zero-shot settings and matches or exceeds PaLM-540B few-shot; other open models (Bloom, OPT, GPT-NeoX, Pythia) fine-tuned on same data fail to reach LLaMA/Goat performance, attributed primarily to tokenization differences.",
            "uuid": "e4720.0",
            "source_info": {
                "paper_title": "Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "LLaMA tokenization",
            "name_full": "LLaMA consistent digit-level tokenization",
            "brief_description": "LLaMA's tokenizer splits individual decimal digits into separate tokens, creating consistent digit-level representations which the paper argues are key to learning arithmetic patterns.",
            "citation_title": "Llama: Open and efficient foundation language models",
            "mention_or_use": "use",
            "model_name": "LLaMA tokenizer (representation property)",
            "model_description": "Subword tokenizer used by LLaMA that, according to the paper's analysis, often maps each digit character to an individual token, producing stable per-digit tokenization across numbers of different lengths.",
            "arithmetic_task_type": "Relevant across all integer arithmetic tasks (addition, subtraction, multiplication, division) because digitwise representation affects sequence modeling of numbers.",
            "mechanism_hypothesis": "Consistent digit-level tokenization permits the model to learn per-digit transformations and positional patterns (carry/borrow behaviors) as localized sequence patterns rather than having to learn variable-width token meanings, enabling supervised learning of arithmetic.",
            "evidence_for_mechanism": "Tokenization comparisons in Table 5 show LLaMA token sequences for multi-digit numbers correspond to digits; models with inconsistent tokenization (GPT-4, Bloom, OPT, Pythia, etc.) show tokens that aggregate digits irregularly. Fine-tuning identical datasets yields markedly better learning in LLaMA than in other models.",
            "evidence_against_mechanism": "Not a formal causal proof; other factors (architecture, pretraining data) could contribute; the paper notes ChatGLM also tokenizes digits individually but was not evaluated, leaving open whether digit tokenization alone suffices.",
            "performance_metrics": "Indirect: models with digit-level tokenization (LLaMA→Goat) achieve near-perfect exact/digit matches on many tasks; models without consistent digit-level tokenization show much higher loss and poor arithmetic learning when fine-tuned on same data (no numeric table provided per model beyond qualitative statements and Table 3 for GPT-4).",
            "probing_or_intervention_results": "Cross-model fine-tuning experiment: identical arithmetic dataset and hyperparameters applied to Bloom/OPT/GPT-J/GPT-NeoX/Pythia; these models struggle to learn tasks LLaMA can, consistent with tokenization hypothesis. No direct ablation of tokenizer within LLaMA reported.",
            "limitations_and_failure_modes": "Correlation, not definitive causation; other emergent factors may contribute. Tokenization does not solve working-memory-style composite tasks (multi-digit multiplication/division) without CoT decomposition.",
            "comparison_to_other_models": "Compared tokenizations (Table 5): LLaMA splits digits; GPT-4 and Bloom often tokenize multi-digit numbers into irregular multi-digit tokens, which the paper links to worse arithmetic learning and digit-alignment problems in CoT.",
            "uuid": "e4720.1",
            "source_info": {
                "paper_title": "Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "CoT decomposition",
            "name_full": "Supervised Chain-of-Thought decomposition for arithmetic",
            "brief_description": "A supervised CoT protocol that decomposes multi-digit multiplication and division into a sequence of learnable subtasks (extraction, split, expansion, product, add term by term) to enable LLMs to compute reliably.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Chain-of-Thought decomposition (arithmetic sketchpad)",
            "model_description": "A hand-designed sequence of intermediate steps (1 extraction, 2 split, 3 expansion via distributive law, 4 compute nD×1D products, 5 add terms iteratively) used as intermediate supervision during fine-tuning so the model outputs CoT then final answer.",
            "arithmetic_task_type": "Multi-digit multiplication (nD×mD) and multi-digit division (nD÷mD) where both operands have &gt;1 digit.",
            "mechanism_hypothesis": "Decomposing an unlearnable composite task into polynomially many learnable subtasks reduces the complexity and working-memory demand, enabling sequence models to learn the computation via supervised intermediate steps.",
            "evidence_for_mechanism": "Ablation studies (Fig. 2 and 3) show that with CoT the model attains high exact-match accuracy on 4D×4D and 6D÷3D tasks, while without CoT accuracy remains ~0; removing specific CoT steps (especially 'adding term by term') substantially harms learning. Few-shot prompting of GPT-4 with the decomposition also improves its correctness vs. default long multiplication/division.",
            "evidence_against_mechanism": "CoT increases sequence length and training cost; not all CoT designs optimal—authors acknowledge other algorithms might be more efficient; GPT-4 sometimes produces incorrect intermediate steps yet still correct final answers, suggesting CoT is not always necessary or that models may use alternative internal heuristics.",
            "performance_metrics": "With CoT, Goat-7B achieves high exact-match on multi-digit multiplication/division tasks (e.g., 4D×4D and 6D÷3D reach high exact percentages reported in Table 3: 6D×6D 96.8/99.5; 12D÷6D 89.3/93.5). Without CoT, exact-match near 0 on those tasks (ablation).",
            "probing_or_intervention_results": "Ablation: removing split/expand/add steps; 'adding term by term' critical. Simplified synthetic environment used to isolate CoT effect from NL variability. Authors trained models to produce CoT then answer, showing faster convergence (e.g., 2-digit×2-digit direct answer took ~10 epochs to overfit, whereas CoT achieved comparable accuracy in 1 epoch).",
            "limitations_and_failure_modes": "CoT relies on human-designed intermediate steps; may not generalize to arbitrarily large inputs due to extrapolation limits; increases supervision and output length; design choices (which subtasks) affect learning and efficiency.",
            "comparison_to_other_models": "CoT approach aligns with prior scratchpad/CoT studies (Nye et al., 2021; Kojima et al., 2022) but in this paper CoT is necessary for multi-digit composite tasks in LLaMA fine-tuning context, and applying the same CoT few-shot to GPT-4 improves but does not fully close performance gaps.",
            "uuid": "e4720.2",
            "source_info": {
                "paper_title": "Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-4 (arithmetic behavior)",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "A large, state-of-the-art pretrained autoregressive transformer which performs well on many reasoning tasks but shows notable failure modes on large-number arithmetic, especially multi-digit multiplication/division without specialized prompting.",
            "citation_title": "Gpt-4 technical report.",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Proprietary large-scale transformer model (details in OpenAI technical report); evaluated via API (May 10th) on BIG-bench arithmetic tasks and extra tasks with and without Chain-of-Thought prompting.",
            "arithmetic_task_type": "Addition, Subtraction (large numbers sometimes OK), Multiplication and Division (multi-digit operations problematic), evaluated on BIG-bench arithmetic sub-task and extra tasks with large-digit numbers.",
            "mechanism_hypothesis": "GPT-4 tends to use long multiplication/long division heuristics when prompted for step-by-step solutions, but has limited working memory and possibly inconsistent digit tokenization that impedes accurate digit alignment and copying required for exact arithmetic.",
            "evidence_for_mechanism": "Empirical failure modes: when generating CoT, GPT-4's intermediate steps often exhibit digit misalignment, copying errors, or incorrect nD×1D intermediate products; despite sometimes incorrect steps, final answer occasionally correct, implying nontransparent internal heuristics. Performance table (Table 3) shows GPT-4 accuracy falls sharply on multi-digit multiplication/division (e.g., MUL: 3D 30.3/83.0, 4D 5.3/61.8, 5D 0.0/47.9; DIV similarly drops for larger divisors).",
            "evidence_against_mechanism": "GPT-4 performs well on many addition/subtraction tasks and on some multi-digit cases (e.g., 8D+8D and 16D+16D), indicating that failure is not universal; also few-shot decomposition prompting helps improve correctness, showing GPT-4 can follow structured CoT when given guidance.",
            "performance_metrics": "Table 3 (Exact/Digit): ADD: generally high (e.g., 16D+16D 94.1/99.5 though some anomalies); SUB: drops on large cross-size tasks (e.g., 16D-8D 10.6/68.8); MUL: severe decline past small digits (e.g., 6D×6D 0.0/49.8); DIV: 12D÷6D 0.0/29.5, etc. Digit match often much higher than exact string match, indicating many correct digits but some errors.",
            "probing_or_intervention_results": "Appending 'Solve it step by step' yields marginal improvement; few-shot prompting with the paper's decomposition examples improves GPT-4's correctness compared to its default long-multiplication/long-division CoT but still exhibits digit-alignment and copying mistakes. Authors identified three common GPT-4 errors: digit alignment, copying of numbers, and incorrect nD×1D intermediate results.",
            "limitations_and_failure_modes": "Short working memory and inconsistent tokenization lead to digit misalignment, copying errors, and bad intermediate multiplication results; poor extrapolation when numbers exceed seen distributions; CoT sometimes does not help because GPT-4 may not leverage intermediate supervision effectively.",
            "comparison_to_other_models": "Compared directly to Goat-7B in zero-shot on BIG-bench and extra tasks: Goat often outperforms GPT-4 on multi-digit arithmetic (Table 3). GPT-4 still strong on many smaller or similar-sized-addition/subtraction tasks but weaker on composite multiplication/division without tailored CoT.",
            "uuid": "e4720.3",
            "source_info": {
                "paper_title": "Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Other LLMs (tokenization failures)",
            "name_full": "Bloom, OPT, GPT-NeoX, GPT-J, Pythia, etc. (open LLMs with irregular tokenization)",
            "brief_description": "A set of open-source LLMs that, when fine-tuned on the same arithmetic dataset, underperform relative to LLaMA/Goat, attributed primarily to inconsistent tokenization of numbers and higher fine-tuning loss.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Bloom / OPT / GPT-NeoX / GPT-J / Pythia (grouped)",
            "model_description": "Open-source autoregressive transformer models with various tokenizer designs; Table 5 shows they commonly tokenize multi-digit numbers into variable-length subword tokens that do not align to single digits consistently.",
            "arithmetic_task_type": "Same integer arithmetic tasks as Goat (addition, subtraction, multiplication, division) when fine-tuned on the synthetic dataset.",
            "mechanism_hypothesis": "Irregular tokenization (tokens representing varying numbers of digits) disrupts the model's ability to learn digitwise arithmetic patterns, making tasks effectively harder and increasing reliance on memorization or overfitting.",
            "evidence_for_mechanism": "Experiments fine-tuning these models on identical arithmetic datasets yielded significantly higher training loss and poor test performance compared to LLaMA; Table 5 documents irregular tokenization patterns for these models compared to LLaMA's digit-level tokens.",
            "evidence_against_mechanism": "No in-paper evidence entirely ruling out other contributors (model size, pretraining corpus); ChatGLM also tokenizes digits individually yet was not evaluated, so tokenization is likely necessary but may not be sufficient alone.",
            "performance_metrics": "Qualitative: models 'struggle with arithmetic tasks' and 'cannot match LLaMA's arithmetic ability' per Section 5.3; exact numeric performance per model not listed beyond GPT-4 comparisons.",
            "probing_or_intervention_results": "Fine-tuning intervention: identical dataset/hyperparameters used; these models showed slower/poorer convergence. No tokenizer-retraining or controlled ablation isolating tokenizer effect was reported beyond cross-model comparisons.",
            "limitations_and_failure_modes": "High training loss and inability to learn learnable tasks that LLaMA can; potential overfitting needed to memorize small-range tasks; tokenization irregularity introduces representational inconsistency.",
            "comparison_to_other_models": "Direct comparison in paper: LLaMA/Goat significantly outperforms these models when all are fine-tuned on the same synthetic arithmetic data; tokenization differences identified as main explanatory factor.",
            "uuid": "e4720.4",
            "source_info": {
                "paper_title": "Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Learnability classification",
            "name_full": "Learnable vs Unlearnable arithmetic tasks (empirical taxonomy)",
            "brief_description": "An empirical classification separating arithmetic tasks the model can learn to output direct answers for (learnable) from composite tasks that require decomposition/CoT (unlearnable) under supervised finetuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Learnability taxonomy (arithmetic)",
            "model_description": "Task-level classification based on observed ability of finetuned LLaMA to generate direct answers: learnable tasks include copying, split, comparison, ordering, addition, subtraction, nD×1D multiplication, nD÷1D division; unlearnable tasks include multi-digit nD×mD multiplication and nD÷mD division with n,m&gt;1.",
            "arithmetic_task_type": "Used to determine when CoT decomposition is necessary (multi-digit composite tasks) vs when direct supervised outputs suffice (addition/subtraction, nD×1D, nD÷1D).",
            "mechanism_hypothesis": "Complexity and working-memory requirement correlate with unlearnability; tasks decomposable into a polynomial number of simple subtasks become learnable when intermediate supervision is provided.",
            "evidence_for_mechanism": "Empirical fine-tuning results showing direct-answer training succeeds for learnable tasks but fails (even with extensive training) for unlearnable tasks; referencing theoretical work (Wies et al., 2022) that intermediate supervision makes certain composite tasks learnable.",
            "evidence_against_mechanism": "Some unlearnable tasks can be brute-force overfit on small finite domains (e.g., exhaustive 2D×2D enumeration), indicating boundary cases where classification depends on dataset and training regime rather than inherent impossibility.",
            "performance_metrics": "Table 1 enumerates learnable vs unlearnable categories; experimental outcomes: learnable tasks reach high exact/digit-match with supervised finetuning (see Goat-7B performance metrics), unlearnable tasks require CoT to reach similar accuracy.",
            "probing_or_intervention_results": "Ablation and synthetic-environment experiments show breakdown of learning behavior with/without intermediate supervision; authors cite Wies et al. (2022) for theoretical grounding.",
            "limitations_and_failure_modes": "Taxonomy empirical and partially heuristic; boundaries depend on model/tokenization/training data; not a proof of impossibility—some unlearnable tasks become learnable with overfitting or different algorithms.",
            "comparison_to_other_models": "Classification is model-dependent: a task learnable for LLaMA might be unlearnable for other LLMs (observed experimentally).",
            "uuid": "e4720.5",
            "source_info": {
                "paper_title": "Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Llama: Open and efficient foundation language models",
            "rating": 2
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Show your work: Scratchpads for intermediate computation with language models",
            "rating": 2
        },
        {
            "paper_title": "Investigating the limitations of transformers with simple arithmetic tasks",
            "rating": 2
        },
        {
            "paper_title": "Sub-task decomposition enables learning in sequence to sequence tasks",
            "rating": 2
        },
        {
            "paper_title": "Recursion of thought: Divide and conquer reasoning with language models",
            "rating": 1
        },
        {
            "paper_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "rating": 1
        },
        {
            "paper_title": "Have you seen that number? investigating extrapolation in question answering models",
            "rating": 1
        }
    ],
    "cost": 0.01589775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks</h1>
<p>Tiedong Liu<br>National University of Singapore<br>tiedong.liu@u.nus.edu</p>
<h4>Abstract</h4>
<p>We introduce Goat, a fine-tuned LLaMA model that significantly outperforms GPT-4 on a range of arithmetic tasks. Fine-tuned on a synthetically generated dataset, Goat achieves state-of-the-art performance on BIG-bench arithmetic sub-task. In particular, the zero-shot Goat7B matches or even surpasses the accuracy achieved by the few-shot PaLM-540B. Surprisingly, Goat can achieve near-perfect accuracy on large-number addition and subtraction through supervised fine-tuning only, which is almost impossible with previous pretrained language models, such as Bloom, OPT, GPTNeoX, etc. We attribute Goat's exceptional performance to LLaMA's consistent tokenization of numbers. To tackle more challenging tasks like large-number multiplication and division, we propose an approach that classifies tasks based on their learnability, and subsequently decomposes unlearnable tasks, such as multi-digit multiplication and division, into a series of learnable tasks by leveraging basic arithmetic principles. We thoroughly examine the performance of our model, offering a comprehensive evaluation of the effectiveness of our proposed decomposition steps. Additionally, Goat-7B can be easily trained using LoRA on a 24GB VRAM GPU, facilitating reproducibility for other researchers. We release our model, dataset, and the Python script for dataset generation. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) have shown remarkable proficiency across a wide range of natural language processing (NLP) tasks (Brown et al., 2020; Chowdhery et al., 2022; Thoppilan et al., 2022). Notably, GPT-4 (OpenAI, 2023) has achieved state-of-the-art performances in such tasks. However, it is surprising that such powerful language models still struggle with elementary arithmetic tasks. The performance of GPT-4 in</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>Bryan Kian Hsiang Low</h2>
<p>National University of Singapore
lowkh@comp.nus.edu.sg</p>
<h2>Goat-7B</h2>
<p>$$
\begin{aligned}
&amp; 3978640188+42886272= \
&amp; 3978640188+42886272=4021526460 \
&amp; 4523646 \text { minus } 67453156 \
&amp; 4523646-67453156=-62929510 \
&amp; \text { Calculate } 397 \times 4429 \
&amp; 397 \times 4429 \
&amp; =4429 \times(300+90+7) \
&amp; =4429 \times 300+4429 \times 90+4429 \times 7 \
&amp; =1328700+398610+31003 \
&amp; =1727310+31003 \
&amp; =1758313
\end{aligned}
$$</p>
<h2>What is 8914/64?</h2>
<p>$$
\begin{aligned}
&amp; 8914-64 \times 100=8914-6400=2514 \
&amp; 2514-64 \times 30=2514-1920=594 \
&amp; 594-64 \times 9=594-576=18 \
&amp; \text { Therefore, } 8914 \div 64=139 \mathrm{R} 18
\end{aligned}
$$</p>
<p>Figure 1: Example of Goat's response on addition, subtraction, multiplication, and division tasks. Prompts are marked in bold, followed by Goat-7B's response.
arithmetic tasks, particularly multiplication and division of large numbers, currently remains far from optimal, with accuracy levels trending toward zero.</p>
<p>In this paper, we present Goat, a fine-tuned language model that is GOod at Arithmetic Tasks. Goat achieves state-of-the-art performance in elementary arithmetic, including addition, subtraction, multiplication, and division of integers. We adopt an end-to-end supervised instruction-finetuning paradigm on LLaMA (Touvron et al., 2023), leveraging a synthetically generated dataset containing around 1 million samples. Unlike previous research on arithmetic computation (Lee and Kim, 2023;</p>
<p>Nogueira et al., 2021; Nye et al., 2021; Qian et al., 2022; Zhou et al., 2022b), our study demonstrates that through supervised fine-tuning alone and without applying any special techniques, our model is capable of generating direct answers for largenumber addition and subtraction with near-perfect accuracy in a zero-shot setting. We attribute this exceptional arithmetic ability to LLaMA's consistent tokenization of numbers and show that this is almost impossible to achieve for previous LLMs such as Bloom (Scao et al., 2022), OPT (Zhang et al., 2022), GPT-NeoX (Black et al., 2022), Pythia (Biderman et al., 2023), etc.</p>
<p>However, the model encounters significant difficulties when generating direct answers for arithmetic tasks like large-number multiplication and division. To overcome this challenge, we propose an approach that categorizes various arithmetic tasks into learnable and unlearnable tasks, subsequently decomposing the unlearnable tasks, such as multidigit multiplication and division, into a series of learnable tasks by leveraging basic arithmetic principles. Our approach ensures that the intermediate supervision which facilitates the model's learning is also easily understandable and interpretable by humans. We fine-tune our model to generate the proposed CoT before generating the final answer, similar to sketchpad (Nye et al., 2021). Our method outperforms GPT-4's long multiplication and long division methods by a large margin. We assess the performance of our model using BIG-bench (Srivastava et al., 2022) arithmetic sub-task, and provide a comprehensive evaluation of the effectiveness of our proposed method. Our findings suggest that the model can learn the pattern and generalize to unseen data instead of purely memorizing the computation. Additionally, Goat-7B can be conveniently trained using Low-Rank Adaptation (LoRA) (Hu et al., 2021) technique on a 24GB VRAM GPU, making it easily reproducible for other researchers.</p>
<p>To summarize, our contributions include:</p>
<ul>
<li>Our model achieves state-of-the-art performance on various elementary arithmetic tasks, including addition, subtraction, multiplication, and division of positive integers (Section 4). We show that an open-sourced model finetuned on a synthetically generated dataset has the potential to achieve even higher accuracy on arithmetic tasks compared to GPT-4.</li>
<li>To the best of our knowledge, we are the first to demonstrate the feasibility that supervised fine-tuning alone can enable LLMs to generate direct answers for certain elementary arithmetic tasks, such as large-number addition and subtraction, without applying any special techniques (Section 3.3). Previously effective chain-of-thought (CoT) methods, such as those used for addition in sketchpad (Nye et al., 2021) and LM Tutor (Qian et al., 2022), are no longer necessary. The impressive performance is mainly attributed to LLaMA's consistent tokenization of numbers.</li>
<li>To solve large-number multiplication and division, we propose a novel decomposition method based on the learnability of the task, leveraging basic arithmetic principles to ensure human interpretability (Section 3.4).</li>
<li>We systematically investigate the proposed decomposition method and demonstrate its effectiveness (Section 5). We conduct thorough experiments on the decomposition steps in a fully synthetic environment by mitigating many hard-to-control aspects of natural language. Our experimental setup offers an ideal platform to study the impact of CoT and intermediate supervision.</li>
<li>Our end-to-end instruction tuning pipeline can be easily integrated into existing instructiontuned language models (Chiang et al., 2023; Taori et al., 2023) and potentially enhance their mathematical reasoning for math word problems. We release the model, dataset, and script for generating the dataset.</li>
</ul>
<h2>2 Related Work</h2>
<h3>2.1 Instruction Tuning</h3>
<p>Instruction tuning (Chung et al., 2022; Ouyang et al., 2022; Sanh et al., 2021) is a technique used to align pretrained language models with human instructions. It enables targeted customization of LLMs to specific tasks, enhancing their ability to generate more accurate and contextually relevant responses and improving the zero-shot performance. The dataset used for instruction tuning can be human-written (Ouyang et al., 2022), machinegenerated (Peng et al., 2023; Taori et al., 2023; Wang et al., 2022), or collected from web (Geng et al., 2023). Recently, there has been extensive research on fine-tuning LLaMA (Touvron et al.,</p>
<p>2023) for various downstream tasks using instruction tuning (Chiang et al., 2023; Geng et al., 2023; Taori et al., 2023; Xu et al., 2023; Yunxiang et al., 2023). Creating high-quality instruction tuning datasets can be expensive and time-consuming. In this study, we utilize a simple Python program to generate input-output pairs for arithmetic tasks.</p>
<h3>2.2 Arithmetic Reasoning</h3>
<p>Arithmetic reasoning has been a topic of interest in NLP research for many years (Lu et al., 2022). Recently, the use of pretrained models (Brown et al., 2020; OpenAI, 2023) has shown great capabilities in solving math word problems. Particularly, chain of thought (CoT) (Kojima et al., 2022; Wei et al., 2022; Zhou et al., 2022a) provides the model with the intermediate steps to derive the final answer. However, studies have shown that LLMs struggle with basic arithmetic computation and often make arithmetic mistakes, even though the reasoning process is correct (Cobbe et al., 2021; Gao et al., 2022; Schick et al., 2023). Consequently, one key challenge of arithmetic reasoning, aside from mapping natural language to arithmetic expressions, is how to compute the generated arithmetic expressions with high accuracy.</p>
<h3>2.3 Arithmetic Computation</h3>
<p>Recent studies have explored using external tools to evaluate arithmetic expressions. Toolformer (Schick et al., 2023) and GSM8K (Cobbe et al., 2021) invoke an external calculator to compute the generated arithmetic expression. PoT (Chen et al., 2022) and PAL (Gao et al., 2022) generate programs that can be executed to produce the final answer. While arithmetic can be solved using calculators or programs easily, the ability to perform arithmetic computation is a remarkable trait of human intelligence, and we anticipate LLMs should possess this ability as well.</p>
<p>Previous studies have evaluated the arithmetic abilities of LLMs. Nogueira et al. (2021) have evaluated addition and subtraction tasks. Muffo et al. (2022) have further examined 2-digit multiplication. Yuan et al. (2023) have tested different types of arithmetic operations. CoT seems to be a promising solution for arithmetic computation as well. Similar to humans, autoregressive language model may rely on intermediate supervision to generate the final answer. Scratchpad (Nye et al., 2021) finetunes the language models to produce CoT before generating an answer, and has demon- strated effectiveness on 8-digit addition. However, we show that previously effective CoT methods, such as those used for addition in sketchpad (Nye et al., 2021) and LM Tutor (Qian et al., 2022), are no longer necessary for certain arithmetic tasks like addition. By leveraging simple supervised finetuning alone, our model can perform addition and subtraction with sufficiently high accuracy. For challenging tasks like large-number multiplication and division, previous studies (Muffo et al., 2022; Lee and Kim, 2023) either fail to compute or are inefficient. Furthermore, our model is trained end-to-end such that it can follow human instructions.</p>
<h2>3 Method</h2>
<h3>3.1 Language Model</h3>
<p>LLaMA (Touvron et al., 2023) is a collection of open-source pretrained language models trained on trillions of tokens using publicly available datasets, and achieves state-of-the-art performance on many benchmarks.</p>
<p>Previous studies (Kim et al., 2021; Nogueira et al., 2021) have shown that tokenization is important for LLM's arithmetic ability. Many commonlyused subword tokenization techniques today are not ideal to represent numbers. However, LLaMA splits each digit into an individual token (Yuan et al., 2023), thereby ensuring consistent tokenization of numbers, as shown in Appendix B.</p>
<p>The selection of language models is crucial to our work. We believe the remarkable arithmetic ability demonstrated in this work is mainly attributed to LLaMA's consistent tokenization of numbers. We experimentally verify that other LLMs, such as Bloom, OPT, GPT-NeoX, and Pythia, finetuned on the same arithmetic dataset, cannot match LLaMA's arithmetic ability.</p>
<h3>3.2 Learnability of Arithmetic Tasks</h3>
<p>Wies et al. (2022) have provided a theoretical analysis on the use of intermediate supervision for solving composite tasks. Specifically, they have shown that for any family of tasks which on the one hand, are unlearnable, and on the other hand, can be decomposed into a polynomial number of simple subtasks, unlearnable composite problems can become learnable by using intermediate supervision or step-by-step CoT.</p>
<p>Building upon their analysis, we first experimentally categorize learnable and unlearnable tasks. In the context of arithmetic computation, learnable</p>
<table>
<thead>
<tr>
<th></th>
<th>Task</th>
<th>Input</th>
<th>Output</th>
</tr>
</thead>
<tbody>
<tr>
<td>1-5.5[2pt/2pt]</td>
<td>{Learnable}</td>
<td>Copying</td>
<td>59265395</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Split</td>
<td>4536</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Comparison</td>
<td>8116449, 97863</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Ordering</td>
<td>3568, 9591, 8061</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Addition</td>
<td>1270769 + 264985867430</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Subtraction</td>
<td>40920 - 6173772696</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Multiplication $n D \times 1 D$</td>
<td>591714761929184 $\times 4$</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Division $n D \div 1 D$</td>
<td>339229815457 $\div 4$</td>
</tr>
<tr>
<td>1-5.5[2pt/2pt]</td>
<td>{Unlearnable}</td>
<td>Multiplication $n D \times m D$</td>
<td>6983387 $\times$ 16919</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Division $n D \div m D$</td>
<td>64729486 $\div$ 472</td>
</tr>
</tbody>
</table>
<p>Table 1: Summary and examples of learnable and unlearnable arithmetic tasks. For example, $n D \div 1 D$ means $n$-digit by 1-digit division, where $n \geq 1$. Unlearnable tasks are mainly multi-digit multiplication and division where $n, m&gt;1$. There are some special cases mentioned in Appendix E.
tasks generally refer to those for which the model can be successfully trained to generate direct answers, achieving sufficiently high accuracy within a predefined number of training epochs. Conversely, unlearnable tasks are those that the model struggles to learn and generate direct answers correctly even with extensive training. While the exact reason behind the varying learnability of tasks is not yet fully understood and requires further investigation, we hypothesize that it is associated with the complexity of the underlying pattern and the size of working memory required for completing the task (Bubeck et al., 2023).</p>
<p>We experimentally examine the learnability of these tasks by fine-tuning the model specifically for each task in a simplified synthetic environment (Table 7). Our recognized learnable and unlearnable tasks are listed in Table 1.</p>
<p>The categorization of tasks also aligns with human perception. With practice, humans can mentally calculate the addition and subtraction of two large numbers, writing down the final numerical answer directly from the left (most significant figure) to the right (least significant figure) without the need for sketchpad. However, mentally solving large-number multiplication and division is undeniably a challenging task.</p>
<p>We also observe that our classification of tasks is consistent with the performance of GPT-4. In particular, GPT-4 excels in generating direct answers for large-number addition and subtraction. However, its accuracy significantly drops when it comes to multi-digit multiplication and division tasks. Our observation aligns with the claim made by Bubeck et al. (2023) that GPT-4 has a short working memory and performs poorly on composite arithmetic tasks. This is particularly evident in the case of multiplication, which involves multiple steps of addition. The inability of powerful models like GPT-4 to directly solve unlearnable tasks may suggest that generating direct answers for such tasks is extremely challenging, even with extensive training.</p>
<p>It is noteworthy that a task that is learnable for LLaMA may not necessarily be learnable for other LLMs, which is validated in our experiments in Section 5.3. Furthermore, not all tasks classified as unlearnable are entirely impossible for the model to learn. For instance, 2-digit by 2-digit multiplication is considered an unlearnable task in our case. However, the model can still learn to generate the direct answer by overfitting to the training set, which contains an exhaustive enumeration of all possible 2-digit multiplication. Nevertheless, the process takes nearly 10 epochs to achieve around $90 \%$ accuracy. In contrast, by inserting our proposed CoT before the final answer, the model can achieve comparable accuracy in 2-digit multiplication with only 1 epoch of training. These findings align with the claim (Wies et al., 2022) that the presence of intermediate supervision facilitates the learning process.</p>
<h3>3.3 Addition and Subtraction</h3>
<p>Addition and subtraction tasks are learnable, as with supervised fine-tuning alone, the model exhibits a remarkable ability to accurately generate direct numerical answers. The model successfully captures the underlying patterns of the arithmetic operations. This is evident from the model's near-</p>
<p>perfect accuracy on the unseen test set, despite being trained on a very limited subset of the data. It is worth mentioning that addition and subtraction operations do not require the use of CoT. This contrasts with previous studies that have employed CoT for addition and subtraction tasks (Lee and Kim, 2023; Nye et al., 2021; Qian et al., 2022).</p>
<h3>3.4 Multiplication</h3>
<p>We experimentally verify that $n$-digit by 1-digit multiplication is learnable. In contrast, multi-digit multiplication poses significant challenges for the model, suggesting it to be an unlearnable task. To overcome this issue, we adopt a similar strategy used in sketchpad (Nye et al., 2021), which finetunes the LLMs to generate CoT before generating the answer. Specifically, we propose a CoT that decomposes the multi-digit multiplication into a series of 5 learnable sub-tasks: (1) extraction: extract the arithmetic expression from the natural language instruction, (2) split: split the smaller number of the two into place values, (3) expansion: expand the sum based on the distributive property, (4) product: compute each product simultaneously, and (5) adding term by term: add the first two terms and copy the rest, and the final sum is obtained.</p>
<p>Consider the example in Fig. 1. Firstly, the arithmetic expression $397 \times 4429$ is extracted from the instruction, which can be considered as a "copying" task. Secondly, $397 \times 4429=4429 \times(300+90+7)$ involves two learnable tasks. The larger number of the two is placed in front and then the smaller one is split, which is similar to "ordering" and "split" learnable tasks. The ordering ensures that there are fewer summation terms in the next step, thereby reducing the CoT length. Thirdly, the sum is expanded using distributive law: $4429 \times(300+90+7)=4429 \times 300+4429 \times$ $90+4429 \times 7$, which is similar to "copying" task. Next, $4429 \times 300+4429 \times 90+4429 \times 7=$ $1328700+398610+31003$ where the products are computed at once by applying "multiplication $n$-digit by 1-digit" with zeros copied at the end of each product. Finally, we take the sum of the first two terms at each step, and copy the rest terms, leveraging "addition" and "copying". Hence, a composite unlearnable task is broken down into simpler tasks that are all learnable.</p>
<h3>3.5 Division</h3>
<p>Similarly, we observe that $n$-digit by 1-digit division is learnable. However, multi-digit division is unlearnable. We design a novel CoT leveraging a modified slow division method based on the following recurrence equation</p>
<p>$$
R_{j}-D \times\left(q_{n-(j+1)} \times 10^{j}\right)=R_{j+1}
$$</p>
<p>where $R_{j}$ is the $j$-th partial remainder of the division, $q_{n-(j+1)}$ is the digit of the quotient in position $n-(j+1)$ numbered from least significant 0 to most significant $n-1, n$ is the number of digits in the quotient, and $D$ is the divisor. Specifically, the main idea is to subtract multiples of the divisor from the dividend until the remainder is less than the divisor.</p>
<p>Here is a detailed breakdown of the CoT used in Fig. 1. Consider the first iteration (first equation). The first step $8914-64 \times 100$ requires the model to copy the dividend and the divisor, and subsequently generate a number $q_{n-(j+1)} \times 10^{j}$ such that the product of $q_{n-(j+1)} \times 10^{j}$ and the divisor $D$ is less than or equal to the partial remainder $R_{j}$. This inherently involves two learnable tasks: " $n$-digit by 1digit multiplication" and "comparison". We experimentally show that this composite task is learnable. The second step $8914-64 \times 100=8914-6400$ mainly involves a "copying" task and an " $n$-digit by 1-digit multiplication" task. The third step $8914-6400=2514$ leverages "subtraction". The process iterates until the leftover is less than the divisor, which implies the model has to implicitly learn comparison. Finally, the model generates the quotient by combining all $q_{n-(j+1)}$ 's in previous iterations, which can be considered as the inverse of the "split" task, and finally copies the remainder if it is not zero.</p>
<p>A summary of prompts and expected output for various tasks are shown in Table 2.</p>
<h3>3.6 Settings</h3>
<p>In this paper, we consider the addition and subtraction of two positive integers with each containing up to 16 digits. It is worth noting that the result of subtraction can be negative. To limit the maximum generated sequence length, we consider the multiplication of two positive integers whose product falls within 12 digits, and the division of two positive integers resulting in a quotient within 6 digits where the dividend is less than 12 digits. Since we focus on arithmetic tasks of integers, we aim</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Learnable</th>
<th>Prompt</th>
<th>CoT</th>
<th>Target</th>
</tr>
</thead>
<tbody>
<tr>
<td>ADD</td>
<td>$\checkmark$</td>
<td>$1463456+2107$</td>
<td>$\boldsymbol{X}$</td>
<td>$1463456+2107=1465563$</td>
</tr>
<tr>
<td>SUB</td>
<td>$\checkmark$</td>
<td>2348233 minus 483579 ?</td>
<td>$\boldsymbol{X}$</td>
<td>$2348233-483579=1864654$</td>
</tr>
<tr>
<td>MUL</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>$n D \times 1 D$</td>
<td>$\checkmark$</td>
<td>593295 times 7</td>
<td>$\boldsymbol{X}$</td>
<td>$593295 * 7=4153065$</td>
</tr>
<tr>
<td>$n D \times m D$</td>
<td>$\boldsymbol{X}$</td>
<td>Calculate $24 \times 79$</td>
<td>$\checkmark$</td>
<td>$24 * 79=24 *(70+9)=24 * 70+\backslash$</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td>$24 * 9=1680+216=1896$</td>
</tr>
<tr>
<td>DIV</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>$n D \div 1 D$</td>
<td>$\checkmark$</td>
<td>Please tell 3651803/7</td>
<td>$\boldsymbol{X}$</td>
<td>$3651803 / 7=521686$ R 1</td>
</tr>
<tr>
<td>$n D \div m D$</td>
<td>$\boldsymbol{X}$</td>
<td>What is 2546/38?</td>
<td>$\checkmark$</td>
<td>$2546-38 * 60=2546-2280=266$</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td>$266-38 * 7=266-266=0$</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Therefore, $2546 / 38=67$</td>
</tr>
</tbody>
</table>
<p>Table 2: Examples of prompts and targets for fine-tuning LLaMA. "InAnswer: " is appended at the end of each prompt. It should be noted that there are a few special cases when CoT is not required (see Appendix E).
to obtain the least positive remainder in the case when it is not divisible.</p>
<p>In Section 5.2, we present an analysis showcasing the limited extrapolation capabilities of finetuned LLMs. Consequently, input data that falls outside the distribution of the training data is unlikely to yield reasonable answers. Our method potentially applies to numbers with more digits, though the training cost will increase correspondingly.</p>
<h3>3.7 Dataset</h3>
<p>We generate the dataset synthetically using a Python script. The dataset consists of around 1 million question-answer pairs. The answer contains the proposed CoT as well as the final numerical output. The numbers are randomly generated, hence ensuring a very low probability of instances being duplicated, although small numbers may be sampled multiple times. We sample from log space to ensure the numbers are equally likely to be sampled from different orders of magnitude, which is similar to the sampling method used by Lee and Kim (2023). The details of the dataset are presented in Appendix F.</p>
<h3>3.8 Fine-tuning</h3>
<p>To enable the model to solve arithmetic problems based on instructions and facilitate natural language question answering, we generate hundreds of instruction templates using ChatGPT (Table 6). During the instruction tuning process, we randomly select a template for each arithmetic input from the training set, and fine-tune LLaMA-7B similar to the method used in Alpaca (Taori et al., 2023). We apply various techniques to enhance the model's adaptability to diverse question formats, such as randomly removing spaces between numbers and symbols in the arithmetic expression, replacing "*" with " $x$ " or "times", etc.</p>
<p>Goat-7B can be easily fine-tuned using LoRA on a 24GB VRAM GPU. In particular, the fine-tuning process for a specific arithmetic sub-task, such as 8-digit addition using 100K instances, takes only approximately 1.5 hours on an A10 GPU to achieve near-perfect accuracy. The training hyperparameters are listed in Appendix A.</p>
<h2>4 Experiments</h2>
<p>We evaluate our model using BIG-bench arithmetic dataset (Srivastava et al., 2022), as well as our extra selected tasks. The results are shown in Table 3. Notably, in a zero-shot setting, Goat-7B achieves comparable or even higher accuracy on BIG-bench compared to the few-shot PaLM-540B.</p>
<h3>4.1 Metric</h3>
<p>We first compute the accuracy based on the standard exact string match (Appendix C). We observe that GPT-4's accuracy under exact string match is almost identically zero on tasks involving large numbers. However, in many cases where the final answer is incorrect, the majority of digits in the generated answer align with the target number, with only a few digits being incorrect. Inspired by recent study on the emergent abilities of LLMs (Schaeffer et al., 2023), we include a digit match metric that can reflect the per-token error rate of the output, as each digit is uniquely represented by a token in LLaMA.</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>BIG-bench</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>Extra Tasks</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>ADD</td>
<td>1D</td>
<td>2D</td>
<td>3D</td>
<td>4D</td>
<td>5D</td>
<td>8D+8D</td>
<td>16D+8D</td>
<td>16D+16D</td>
</tr>
<tr>
<td>GPT-4</td>
<td>100/100</td>
<td>100/100</td>
<td>99.6/99.9</td>
<td>98.8/99.6</td>
<td>94.1/98.5</td>
<td>92.1/98.3</td>
<td>9.4/70.4</td>
<td>94.1/99.5</td>
</tr>
<tr>
<td>Goat-7B</td>
<td>100/100</td>
<td>100/100</td>
<td>99.4/99.8</td>
<td>98.3/99.5</td>
<td>98.1/99.4</td>
<td>97.8/99.4</td>
<td>97.1/99.6</td>
<td>97.6/99.7</td>
</tr>
<tr>
<td>SUB</td>
<td>1D</td>
<td>2D</td>
<td>3D</td>
<td>4D</td>
<td>5D</td>
<td>8D-8D</td>
<td>16D-8D</td>
<td>16D-16D</td>
</tr>
<tr>
<td>GPT-4</td>
<td>100/100</td>
<td>100/100</td>
<td>99.2/99.6</td>
<td>98.9/99.6</td>
<td>92.4/98.1</td>
<td>70.5/91.5</td>
<td>10.6/68.8</td>
<td>59.6/88.2</td>
</tr>
<tr>
<td>Goat-7B</td>
<td>100/100</td>
<td>100/100</td>
<td>99.7/99.9</td>
<td>98.6/99.6</td>
<td>98.4/99.5</td>
<td>96.8/99.3</td>
<td>95.8/99.2</td>
<td>96.3/99.3</td>
</tr>
<tr>
<td>MUL</td>
<td>1D</td>
<td>2D</td>
<td>3D</td>
<td>4D</td>
<td>5D</td>
<td>1D $\times$ 16D</td>
<td>4D $\times$ 8D</td>
<td>6D $\times$ 6D</td>
</tr>
<tr>
<td>GPT-4</td>
<td>100/100</td>
<td>99.4/99.8</td>
<td>30.3/83.0</td>
<td>5.3/61.8</td>
<td>0.0/47.9</td>
<td>61.5/92.3</td>
<td>0.0/45.9</td>
<td>0.0/49.8</td>
</tr>
<tr>
<td>Goat-7B</td>
<td>100/100</td>
<td>100/100</td>
<td>97.8/99.4</td>
<td>96.9/99.2</td>
<td>96.7/99.3</td>
<td>99.7/99.9</td>
<td>88.1/97.8</td>
<td>96.8/99.5</td>
</tr>
<tr>
<td>DIV</td>
<td>1D</td>
<td>2D</td>
<td>3D</td>
<td>4D</td>
<td>5D</td>
<td>16D $\div$ 1D</td>
<td>6D $\div$ 3D</td>
<td>12D $\div$ 6D</td>
</tr>
<tr>
<td>GPT-4</td>
<td>100/100</td>
<td>100/100</td>
<td>94.5/96.3</td>
<td>90.9/92.1</td>
<td>53.4/73.2</td>
<td>54.0/84.3</td>
<td>6.4/48.6</td>
<td>0.0/29.5</td>
</tr>
<tr>
<td>Goat-7B</td>
<td>100/100</td>
<td>100/100</td>
<td>99.5/99.7</td>
<td>99.0/99.5</td>
<td>96.5/98.1</td>
<td>99.0/99.7</td>
<td>94.1/96.1</td>
<td>89.3/93.5</td>
</tr>
</tbody>
</table>
<p>Table 3: The result of GPT-4 and Goat-7B on BIG-bench Arithmetic sub-task and extra selected arithmetic tasks, using metrics Exact String Match/Digit Match (Appendix C), shown in percentage. We test GPT-4 and Goat with exactly the same questions and prompts. We evaluate GPT-4 using the API version on May 10th. For Big-bench tasks, $n D$ refers the $n$-digit by $n$-digit operation, except for division where $n D$ means $n$-digit by $m$-digit where $m \leq n$. BIG-bench only includes division operation without remainder, whereas in extra tasks we include the cases where the remainder is not zero and ask GPT-4 to output the answer in "quotient R remainder" format. It should be noted that we exclude the BIG-bench test data from our training dataset as much as possible, although the overlap is unavoidable for operations involving small numbers.</p>
<h3>4.2 Comparison</h3>
<p>Comparing the performance of Goat and GPT-4 for large-number multiplication and division may seem unfair, as GPT-4 generates direct answers while Goat relies on CoT. Hence, we also evaluate GPT-4's performance with CoT by appending "Solve it step by step" at the end of each prompt. By default, GPT-4 uses long multiplication and long division methods. However, we observe that generating CoT only leads to marginal improvement in accuracy. In some cases, the intermediate steps from long multiplication and division are incorrect, but surprisingly the final answer is correct. This implies that GPT-4 does not effectively take advantage of intermediate supervision from CoT to improve the final output. We identify the following 3 common errors from GPT-4's solution, which results in incorrect final answers: (1) the alignment of corresponding digits, (2) copying of numbers, and (3) the intermediate result from $n$-digit by 1-digit multiplication.</p>
<p>Additionally, we observe that GPT-4 performs reasonably well on $8 D+8 D$ and $16 D+16 D$ tasks, but fails on most $16 D+8 D$ tasks, though intuitively $16 D+8 D$ should be relatively easier than $16 D+16 D$. While the exact reason for this remains unclear, one possible factor could be GPT-4's inconsistent number tokenization (Table 5), which makes it difficult to align the corresponding digits of two numbers.</p>
<h2>5 Analysis</h2>
<h3>5.1 Ablation study</h3>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: Accuracy (exact string match) against the number of samples seen during the training of $4 D \times 4 D$ task. Evaluated on the same randomly generated unseen test set using training checkpoints.</p>
<p>Here we want to study the usefulness and effectiveness of each intermediate decomposition step. Specifically, for multiplication (Fig. 2), we com-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Accuracy (exact string match) against the number of samples seen during the training of 6D ÷ 3D task. Evaluated on the same randomly generated unseen test set using training checkpoints.</p>
<p>pare the accuracy of 4-digit by 4-digit multiplication by removing one particular step in the CoT, including split, expansion, adding term by term (referring to G), as well as no CoT. For division (Fig. 3), we compare the accuracy of 6-digit by 3-digit division after removing the middle step that computes the product (referring to G), as well as no CoT. To minimize the impact caused by natural language, we conduct an ablation study in a simplified synthetic environment (Table 7).</p>
<p>The multiplication results suggest that the "adding term by term" step plays a crucial role in obtaining the final answer. In contrast, the "split" and "expand" steps have minimal impact, and can potentially be omitted for generating more concise CoT. This can be attributed to the nature of these two intermediate steps, which primarily involve simple and learnable tasks like copying and comparison. Nevertheless, we still retain these steps to ensure human interpretability.</p>
<p>The accuracy of exact string match without CoT remains consistently at zero for both 4D × 4D multiplication and 6D ÷ 3D division. This further showcases the validity of our approach, as breaking down complex arithmetic tasks into a series of learnable tasks can indeed facilitate the training process for LLMs.</p>
<h3>5.2 Extrapolation</h3>
<p>Extrapolation refers to the ability of the model to predict data that lies out-of-distribution (OOD) of training data. We test addition for numbers larger than those in the training data distribution. The results reveal that the model has limited extrapolation capabilities. There is a gradual drop in accuracy, as the test set deviates further from the training set. This observation is consistent with the result reported in (Kim et al., 2021), highlighting a limitation of our fine-tuned model and underscoring the significance of training data distribution.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Accuracy against the number of digits for the addition task. The model is trained up to 16D+16D, and tested on 17D+17D onward.</p>
<h3>5.3 Comparison with Other LLMs</h3>
<p>We conduct comprehensive experiments on a variety of LLMs, including Bloom, OPT, GPT-J, GPT-NeoX, and Pythia. These models are fine-tuned using the identical dataset as that for Goat, maintaining consistency in the training hyperparameters. Our experiment shows that they all struggle with arithmetic tasks. Even for tasks that are considered learnable for LLaMA, such as multi-digit addition, the loss during fine-tuning is significantly higher than that of LLaMA. The observation underscores the claim made in (Nogueira et al., 2021) that tokenization is a crucial factor in the performance of arithmetic tasks.</p>
<h3>5.4 Few-shot Prompting with GPT-4</h3>
<p>GPT-4 demonstrates powerful in-context learning abilities. We further examine the effectiveness of our proposed decomposition method for solving large-number multiplication and division by using few-shot prompting with GPT-4 (see Appendix H). We observe that our decomposition method allows GPT-4 to generate correct answers more frequently than using its default long multiplication and division methods. This further supports the effectiveness and validity of our approach. Examples of the prompt and output are shown in Appendix H.</p>
<h2>6 Limitations</h2>
<p>Humans are capable of performing multiplication and division on arbitrarily large numbers, providing sufficient time and space for calculations. In contrast, LLMs often suffer from extrapolation problem.</p>
<p>lems. The models are unlikely to generate reasonable answers if the input deviates significantly from the distribution of training data. To enhance the human interpretability of intermediate supervision, we use the straightforward CoT that follows simple basic arithmetic rules. However, this design may not be the most efficient way to facilitate the final answer generation. There are potentially more suitable multiplication and division algorithms for the model to learn. Besides, our research only focuses on elementary arithmetic operations involving integers. Nevertheless, we anticipate that our method could be applicable to decimal computation as well.</p>
<h2>7 Conclusion</h2>
<p>In summary, we demonstrate the feasibility that supervised fine-tuning alone can enable LLMs to perform certain basic arithmetic operations with high accuracy. With our proposed CoT, our model achieves state-of-the-art performance on various elementary arithmetic tasks. Our research offers an excellent platform for investigating the mechanism of working memory and the influence of intermediate supervision on text generation. Our method can be easily integrated with other instruction-tuned LLMs and has the potential to further enhance arithmetic reasoning abilities in solving math word problems.</p>
<h2>References</h2>
<p>Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373.</p>
<p>Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. 2022. GPT-NeoX-20B: An opensource autoregressive language model. In Proceedings of BigScience Episode #5 - Workshop on Challenges \&amp; Perspectives in Creating Large Language Models, pages 95-136, virtual+Dublin. Association for Computational Linguistics.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4.</p>
<p>Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. 2022. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. 2023. Vicuna: An open-source chatbot impressing gpt-4 with $90 \%$ * chatgpt quality.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2022. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435.</p>
<p>Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song. 2023. Koala: A dialogue model for academic research. Blog post, April, 1.</p>
<p>Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685.</p>
<p>Jeonghwan Kim, Giwon Hong, Kyung-min Kim, Junmo Kang, and Sung-Hyon Myaeng. 2021. Have you seen that number? investigating extrapolation in question answering models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7031-7037, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916.</p>
<p>Soochan Lee and Gunhee Kim. 2023. Recursion of thought: Divide and conquer reasoning with language models.</p>
<p>Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and Kai-Wei Chang. 2022. A survey of deep learning for mathematical reasoning. arXiv preprint arXiv:2212.10535.</p>
<p>Matteo Muffo, Aldo Cocco, and Enrico Bertino. 2022. Evaluating transformer language models on arithmetic operations using number decomposition. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 291-297, Marseille, France. European Language Resources Association.</p>
<p>Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. 2021. Investigating the limitations of transformers with simple arithmetic tasks. arXiv preprint arXiv:2102.13019.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114.</p>
<p>OpenAI. 2023. Gpt-4 technical report.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744.</p>
<p>Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277.</p>
<p>Jing Qian, Hong Wang, Zekun Li, Shiyang Li, and Xifeng Yan. 2022. Limitations of language models in arithmetic and symbolic induction. arXiv preprint arXiv:2208.05051.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2021. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207.</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. 2022. Bloom: A 176bparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100.</p>
<p>Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. 2023. Are emergent abilities of large language models a mirage? arXiv preprint arXiv:2304.15004.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615.</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. GitHub repository.</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.</p>
<p>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.</p>
<p>Noam Wies, Yoav Levine, and Amnon Shashua. 2022. Sub-task decomposition enables learning in sequence to sequence tasks. arXiv preprint arXiv:2204.02892.</p>
<p>Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. 2023. Baize: An open-source chat model with parameter-efficient tuning on self-chat data. arXiv preprint arXiv:2304.01196.</p>
<p>Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, and Songfang Huang. 2023. How well do large language models perform in arithmetic tasks? arXiv preprint arXiv:2304.02015.</p>
<p>Li Yunxiang, Li Zihan, Zhang Kai, Dan Ruilong, and Zhang You. 2023. Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge. arXiv preprint arXiv:2303.14070.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.</p>
<p>Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068.</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. 2022a. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625.</p>
<p>Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, and Hanie Sedghi. 2022b. Teaching algorithmic reasoning via incontext learning. arXiv preprint arXiv:2211.09066.</p>
<h2>A Hyperparameters</h2>
<table>
<thead>
<tr>
<th style="text-align: center;">Hyperparameter</th>
<th style="text-align: center;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">batch size</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: center;">learning rate</td>
<td style="text-align: center;">0.0003</td>
</tr>
<tr>
<td style="text-align: center;">lora r</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: center;">lora alpha</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: center;">lora target module</td>
<td style="text-align: center;">$\mathrm{q}, \mathrm{v}, \mathrm{k}, \mathrm{o}$</td>
</tr>
<tr>
<td style="text-align: center;">lora dropout</td>
<td style="text-align: center;">0.05</td>
</tr>
<tr>
<td style="text-align: center;">epoch</td>
<td style="text-align: center;">1</td>
</tr>
</tbody>
</table>
<p>Table 4: Hyperparameters for fine-tuning LLaMA-7B.</p>
<h2>B Tokenization</h2>
<p>Nogueira et al. (2021) demonstrate that models with inconsistent tokenization of numbers barely learn the addition of 2-digit numbers, and it completely fails to learn the addition of larger numbers. Specifically, it has an accuracy of zero for 5 digits or more. They attribute this failure to the lack of systematic tokenization of individual digits. For instance, " 123 " might be tokenized as " 12 " and " 3 ", while " 234 " might be tokenized as " 2 " and " 34 ". Consequently, the model is required to learn that the embedding of a token may represent either a single digit or two digits and so on. Hence, it might be challenging for the model to learn to map an embedding to a number when the number of digits it represents changes irregularly. In Table 5, we compare number tokenization across different LLMs.</p>
<h2>C Metric</h2>
<p>Exact string match is defined as 1 if the output string exactly matches the target string, and 0 otherwise. Then we take the average of exact string match for each task. Char error rate (CER) is defined as the percentage of characters that were incorrectly predicted. We compute CER using Python torchmetrics package. Then we define digit match accuracy as $1-$ cer. We include this metric because, for difficult tasks, the exact string match could be identically zero, making it hard to evaluate the performance. In many cases, both GPT-4 and Goat may have very few incorrect digits in the middle of the generated answer, and the number of digits in the generated answer generally matches the target number.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Number</th>
<th style="text-align: center;">Tokenization</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LLaMA</td>
<td style="text-align: center;">74815</td>
<td style="text-align: center;">[1, 29871, 29955, 29946, 29947, 29896, 29945]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7481</td>
<td style="text-align: center;">[1, 29871, 29955, 29946, 29947, 29896]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">748</td>
<td style="text-align: center;">[1, 29871, 29955, 29946, 29947]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">[1, 29871, 29955, 29946]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">[1, 29871, 29955]</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">74815</td>
<td style="text-align: center;">[20338, 868]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7481</td>
<td style="text-align: center;">[20338, 16]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">748</td>
<td style="text-align: center;">[20338]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">[5728]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">[22]</td>
</tr>
<tr>
<td style="text-align: center;">Bloom</td>
<td style="text-align: center;">74815</td>
<td style="text-align: center;">[88241, 2057]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7481</td>
<td style="text-align: center;">[88241, 20]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">748</td>
<td style="text-align: center;">[88241]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">[8771]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">[26]</td>
</tr>
<tr>
<td style="text-align: center;">OPT</td>
<td style="text-align: center;">74815</td>
<td style="text-align: center;">[2, 39373, 996]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7481</td>
<td style="text-align: center;">[2, 406, 34490]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">748</td>
<td style="text-align: center;">[2, 39373]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">[2, 5243]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">[2, 406]</td>
</tr>
<tr>
<td style="text-align: center;">Pythia</td>
<td style="text-align: center;">74815</td>
<td style="text-align: center;">[24, 2385, 1010]</td>
</tr>
<tr>
<td style="text-align: center;">GPT-NeoX-20B</td>
<td style="text-align: center;">7481</td>
<td style="text-align: center;">[24, 34474]</td>
</tr>
<tr>
<td style="text-align: center;">MPT-7B</td>
<td style="text-align: center;">748</td>
<td style="text-align: center;">[24, 2385]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">[3566]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">[24]</td>
</tr>
<tr>
<td style="text-align: center;">GPT-J</td>
<td style="text-align: center;">74815</td>
<td style="text-align: center;">[48246, 1314]</td>
</tr>
<tr>
<td style="text-align: center;">GPT-Neo</td>
<td style="text-align: center;">7481</td>
<td style="text-align: center;">[22, 40271]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">748</td>
<td style="text-align: center;">[48246]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">[4524]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">[22]</td>
</tr>
<tr>
<td style="text-align: center;">ChatGLM</td>
<td style="text-align: center;">74815</td>
<td style="text-align: center;">$[5,25,16,23,9,15,130001,130004]$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7481</td>
<td style="text-align: center;">$[5,25,16,23,9,130001,130004]$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">748</td>
<td style="text-align: center;">$[5,25,16,23,130001,130004]$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">$[5,25,16,130001,130004]$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">$[5,25,130001,130004]$</td>
</tr>
</tbody>
</table>
<p>Table 5: Comparison of number tokenization of various LLMs. It should be noted that ChatGLM also splits each digit into an individual token. Evaluating ChatGLM's arithmetic abilities will be left as future work.</p>
<table>
<thead>
<tr>
<th>Index</th>
<th>Template</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>{arithmetic} =</td>
</tr>
<tr>
<td>2</td>
<td>What is {arithmetic}?</td>
</tr>
<tr>
<td>3</td>
<td arithmetic="arithmetic">Compute</td>
</tr>
<tr>
<td>4</td>
<td arithmetic="arithmetic">Solve</td>
</tr>
<tr>
<td>5</td>
<td arithmetic="arithmetic">Determine</td>
</tr>
<tr>
<td>6</td>
<td arithmetic="arithmetic">Find</td>
</tr>
<tr>
<td>7</td>
<td>What is the result of {arithmetic}?</td>
</tr>
<tr>
<td>8</td>
<td>Please help me calculate {arithmetic}.</td>
</tr>
<tr>
<td>9</td>
<td arithmetic="arithmetic">Solve the following problem:</td>
</tr>
<tr>
<td>10</td>
<td>I am looking for the value of {arithmetic}. Can you help?</td>
</tr>
<tr>
<td>11</td>
<td>What is the numerical value of {arithmetic}?</td>
</tr>
<tr>
<td>12</td>
<td arithmetic="arithmetic">Help me obtain</td>
</tr>
<tr>
<td>13</td>
<td>Show me the result of {arithmetic}?</td>
</tr>
<tr>
<td>14</td>
<td>Kindly calculate {arithmetic} for me.</td>
</tr>
<tr>
<td>15</td>
<td>Determine the value for {arithmetic}.</td>
</tr>
<tr>
<td>16</td>
<td>Can you please compute {arithmetic}?</td>
</tr>
<tr>
<td>17</td>
<td>Find the numerical value of {arithmetic}?</td>
</tr>
<tr>
<td>18</td>
<td>I would appreciate it if you could assist me in calculating {arithmetic}.</td>
</tr>
<tr>
<td>19</td>
<td>Please work out {arithmetic}.</td>
</tr>
<tr>
<td>20</td>
<td>What is the answer to {arithmetic}?</td>
</tr>
<tr>
<td>$\ldots$</td>
<td>$\ldots$</td>
</tr>
</tbody>
</table>
<p>Table 6: Example templates to fine-tune arithmetic tasks with natural language instructions, generated by ChatGPT. During training, ${$ arithmetic $}$ is replaced by the randomly generated arithmetic expression, like $3425 * 5823$.</p>
<h2>D Simplified Synthetic Environment</h2>
<p>We use the simplified synthetic environment to study the effectiveness of various CoT, by avoiding many hard-to-control aspects of natural languages. The difference between this and Goat is that we use a more structured prompt without any instruction template and a straightforward completion of the task. This enables easy comparison between the model's performance on different tasks, allowing us to examine the learnability of various sub-tasks and explore the effectiveness of the proposed CoT. The input and output examples for the simplified synthetic environment are shown in Table 7.</p>
<h2>E Special Cases</h2>
<p>In general, multi-digit multiplication and division are considered unlearnable, and we use the decomposition method to solve them. However, some special cases within multi-digit multiplication and division are learnable, and in these cases, we omit CoT and generate the direct answer:</p>
<ul>
<li>For multiplication, one of the two numbers contains only one non-zero digit, such as $857483 \times 400=342993200$. This type of
task is similar to learnable $n$-digit by 1-digit multiplication, with the zeros being copied at the end of the product.</li>
<li>The dividend is equal to the divisor. In that case, the quotient is identically one. For example, $358 \div 358=1$.</li>
<li>The dividend is less than the divisor. In that case, the quotient is zero and the remainder equals the dividend. For example, $423 \div 968=0$ R 423 .</li>
</ul>
<h2>F Dataset</h2>
<p>In general, it is difficult to determine the optimal proportion for each task. The number and composition of data samples also depend on the problem settings (see Section 3.6). We empirically find that $n$-digit by 1-digit multiplication and division may be easier than other tasks, as it requires fewer samples to reach the same level of accuracy as other tasks during task-specific fine-tuning in the simplified synthetic environment. It is noteworthy that the data samples are all randomly generated, so the probability of the occurrence of duplicated samples is very low for large numbers. Therefore, the train-</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>CoT</th>
<th>Prompt</th>
<th>Target</th>
</tr>
</thead>
<tbody>
<tr>
<td>Addition</td>
<td>$\boldsymbol{x}$</td>
<td>$1463456+2107$</td>
<td>1465563</td>
</tr>
<tr>
<td>Subtraction</td>
<td>$\boldsymbol{x}$</td>
<td>$2348233-483579$</td>
<td>1864654</td>
</tr>
<tr>
<td>Multiplication</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>$n d \times 1 d$</td>
<td>$\boldsymbol{x}$</td>
<td>$593295 * 7$</td>
<td>4153065</td>
</tr>
<tr>
<td>$n d \times m d$</td>
<td>$\boldsymbol{\checkmark}$</td>
<td>$24 * 79$</td>
<td>$24 *(70+9)$</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>$=24 * 70+24 * 9=1680+216=1896$</td>
</tr>
<tr>
<td>Division</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>$n d \div 1 d$</td>
<td>$\boldsymbol{x}$</td>
<td>$3651803 / 7$</td>
<td>521686 R 1</td>
</tr>
<tr>
<td>$n d \div m d$</td>
<td>$\boldsymbol{\checkmark}$</td>
<td>$2551 / 38$</td>
<td>$2546-38 * 60=2546-2280=266$</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>$266-38 * 7=266-266=0$</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>Therefore, $2551 / 38=67$</td>
</tr>
</tbody>
</table>
<p>Table 7: Examples of input and output for training and testing in the simplified synthetic environment, which is used for testing the learnability of sub-tasks and ablation studies. Specifically, " + ", "-", "*", and "\" are used for addition, subtraction, multiplication, and division, respectively. Space is inserted between numbers and symbols. The input and output are formatted to mitigate the influence of natural language.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Composition of tasks in the dataset.
ing loss can reflect the test accuracy on unseen the test set, if the dataset is only trained for one epoch. Since the synthetic dataset can be generated very easily, we first create a dataset that contains a sufficient number of data samples for training and then observe the training loss and apply early stopping. We observe that the training loss does not show any significant decrease after training on about one million samples. It should be noted that convergence also depends on other hyper-parameters such as batch size and learning rate. Hence, it is recommended to use a dataset larger than what is necessary and terminate the training process when the training loss no longer decreases.</p>
<h2>G Ablation Study</h2>
<p>We name the steps (shown in the box below) as (1) extraction, (2) split, (3) expansion, (4) product, and $(5,6, \ldots)$ adding term by term. The ablation study is performed by removing one particular step while keeping other steps unchanged. We exclude the (1) "extraction" and (4) "product" steps from
the ablation study as it is crucial for multi-digit multiplication.</p>
<h2>Multiplication</h2>
<p>Calculate 397 x 4429 \nAnswer:</p>
<p>$$
\begin{aligned}
&amp; 397 \times 4429 \
&amp; =4429 \times(300+90+7) \
&amp; =4429 \times 300+4429 \times 90+4429 \times 7 \
&amp; =1328700+398610+31003 \
&amp; =1727310+31003 \
&amp; =1758313
\end{aligned}
$$</p>
<p>For division, the ablation study is performed by removing the middle step (bold) that computes the product for all iterations, while keeping other steps unchanged.</p>
<h2>Division</h2>
<p>What is 8914/64? \nAnswer:</p>
<p>$$
\begin{aligned}
&amp; 8914-64 \times 100=\mathbf{8 9 1 4}-\mathbf{6 4 0 0}=2514 \
&amp; 2514-64 \times 30=\mathbf{2 5 1 4}-\mathbf{1 9 2 0}=594 \
&amp; 594-64 \times 9=\mathbf{5 9 4}-\mathbf{5 7 6}=18 \
&amp; \text { Therefore, } 8914 \div 64=139 \mathrm{R} 18
\end{aligned}
$$</p>
<h2>H Few-shot Prompting with GPT-4</h2>
<p>Here are two examples showing 3-shot prompting using GPT-4.</p>
<h2>3-Shot prompting for multiplication</h2>
<h2>Prompt:</h2>
<p>Here are 3 examples of question-solution pairs for solving arithmetic multiplication problems. Solve the last question using the same format given in the solutions.
"question": "235658 * $91204=$ ",
"solution": "235658 * $91204=235658$ * $(90000+1000+200+4)=235658$ * $90000+235658 * 1000+235658 * 200+$ $235658 * 4=21209220000+235658000$ $+47131600+942632=21444878000$ $+47131600+942632=21492009600+$ $942632=21492952232^{\prime \prime}$
"question": "59909636 * $6070=$ ",
"solution": "59909636 * $6070=59909636$ * $(6000+70)=59909636 * 6000+59909636$ * $70=359457816000+4193674520=$ $363651490520^{\prime \prime}$
"question": "7661 * $9461=$ ",
"solution": "7661 * $9461=9461$ * (7000 $+600+60+1)=9461 * 7000+9461 *$ $600+9461 * 60+9461 * 1=66227000$ $+5676600+567660+9461=71903600$ $+567660+9461=72471260+9461=$ 72480721"
"question": "58061 * $3569=$ ",</p>
<h2>GPT-4:</h2>
<p>To solve the multiplication problem of 58061 multiplied by 3569 in the same format, we'll distribute 58061 across the sum of the digits of 3569 multiplied by their respective places.
So,
"solution": "58061 * $3569=58061$ * $(3000+500+60+9)=58061 * 3000$ $+58061 * 500+58061 * 60+58061 *$ $9=174183000+29030500+3483660+$ $522549=203213500+3483660+522549$ $=206697160+522549=207219709^{\prime \prime}$</p>
<h2>3-Shot prompting for division</h2>
<h2>Prompt:</h2>
<p>Here are 3 examples of question-solution pairs for solving arithmetic division problem. Solve the last question using the same format given in the solutions.
"question": "440331 / $22=$ ",
"solution": "440331 - $22 * 20000=440331$ $-440000=331 \ln 331-22 * 10=331-$ $220=111 \ln 111-22 * 5=111-110=$ 1\nTherefore, $440331 / 22=20015$ R 1"
"question": "115247727 / 970051 = ", "solution": "115247727 - 970051 * $100=115247727-97005100=$ 18242627\n18242627 - 970051 * $10=$ 18242627 - 9700510 = 8542117\n8542117 - 970051 * $8=8542117-7760408=$ 781709\nTherefore, 115247727 / 970051 = 118 R 781709"
"question": "4144442 / $234443=$ ",
"solution": "4144442 - $234443 * 10=$ $4144442-2344430=1800012 \ln 1800012$ - $234443 * 7=1800012-1641101=$ 158911\nTherefore, $4144442 / 234443=17$ R 158911"
"question": "262122 / 591 = "</p>
<h2>GPT-4:</h2>
<p>To solve the division problem of 262122 divided by 591 in the same format, we'll subtract multiples of 591 from 262122 until we reach a number less than 591, then that's our remainder.
So,
"solution": "262122 - 591 * $400=262122$ -$236400=25722 \ln 25722-591 * 40=25722$ $-23640=2082 \ln 2082-591 * 3=2082-$ $1773=309 \ln$ Therefore, $262122 / 591=443$ R 309"</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://github.com/liutiedong/goat.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
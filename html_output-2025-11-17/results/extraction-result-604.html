<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-604 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-604</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-604</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-19.html">extraction-schema-19</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <p><strong>Paper ID:</strong> paper-b134d0911e2e13ac169ffa5f478a39e6ef77869a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b134d0911e2e13ac169ffa5f478a39e6ef77869a" target="_blank">Snapshot Ensembles: Train 1, get M for free</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This paper proposes a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost by training a single neural network, converging to several local minima along its optimization path and saving the model parameters.</p>
                <p><strong>Paper Abstract:</strong> Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters. To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective. We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e604.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e604.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SGD stochasticity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stochasticity of Stochastic Gradient Descent (mini-batch noise and optimizer randomness)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper attributes variability in optimization trajectories and final model weights to the intrinsic randomness of SGD (mini-batch sampling, ordering) and related optimizer behaviors, which causes different runs to converge to different local minima with different error patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>deep learning / computer vision</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Training convolutional neural networks for image classification (CIFAR-10, CIFAR-100, SVHN, Tiny ImageNet, ImageNet)</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Mini-batch sampling noise and ordering, random weight initialization, stochasticity in SGD updates (intrinsic random motion across gradient steps), optimizer hyperparameters (learning rate schedule), data augmentation randomness, and stochastic regularizers referenced (Dropout, DropConnect, Stochastic Depth, Swapout).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Indirectly measured via downstream metrics: test error (percentage) across snapshots/ensembles, pairwise correlation of softmax outputs between models, and parameter-space interpolation behavior (loss along linear interpolation between parameter vectors).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Different initializations / minibatch orderings lead to different local minima and different mistake patterns; cyclic SGD produced multiple convergences whose snapshots each had competitive test error. Quantitatively, snapshot ensembles reduced CIFAR-100 error from single-model 19.25% to 17.41% (DenseNet-100, α0=0.2), and produced lower pairwise softmax correlations (qualitative / plotted) compared to NoCycle snapshots. Table 3 shows error vs M: M={2,4,6,8,10} produced test errors {22.92,22.07,21.93,21.89,22.16}% (DenseNet-40 on CIFAR-100).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Non-convex loss with exponentially many local minima; stochasticity in SGD and random initialization causing run-to-run variability; sensitivity to learning rate schedule and training budget per cycle (insufficient epochs per cycle prevents convergence on large datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Ensembling (Snapshot Ensembles) to average out run-to-run variability; using cyclic cosine annealing (SGDR) to deliberately visit multiple minima; controlling restart learning rate (α0) to adjust perturbation strength; data augmentation and implicit ensembling methods (Dropout, Stochastic Depth) when applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Snapshot ensembling reduced test error in many settings (e.g., CIFAR-100 DenseNet-100: 19.25% -> 17.41% with α0=0.2), ensembles of just 2 snapshots often outperform single-model baselines, and larger restart learning rates (α0=0.2 vs 0.1) tend to increase diversity and improve ensemble performance. Exact variance reductions are not reported numerically beyond error-rate improvements and correlation plots.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Experiments typically perform one training run per experimental condition that produces M snapshots (M varies: commonly M=5 or M=6; they also vary M in {2,4,6,8,10}); comparisons include separate baselines (Single model, NoCycle, SingleCycle, and true ensembles trained from independent initializations).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>SGD's intrinsic stochasticity (mini-batch noise and initialization differences) produces multiple distinct local minima whose predictions differ; exploiting this via cyclic restarts and ensembling snapshots reduces test error substantially compared to single models and often matches ensembling of multiple independently trained models at lower training cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Snapshot Ensembles: Train 1, get M for free', 'publication_date_yy_mm': '2017-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e604.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e604.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Snapshot Ensembling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Snapshot Ensembling (cyclic cosine annealing with weight snapshots)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method introduced in this paper that obtains an explicit ensemble 'for free' by taking weight snapshots at multiple local minima visited during a single training run using cyclic cosine-annealed learning-rate restarts, then averaging softmax outputs at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>deep learning / computer vision</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Creating ensembles of CNN classifiers by saving model weights at the end of each learning-rate cycle and averaging predictions for image classification benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>The method intentionally leverages variability induced by cyclic learning-rate restarts (restart learning rate α0), SGD stochasticity, and random initialization to generate diverse model snapshots; variability also arises from number/length of cycles M and total training budget B.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Test error (%) of individual snapshots and ensembles, ensemble size m vs error curves, pairwise softmax output correlation, parameter-space linear interpolation loss curves, and ablation across M and α0.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Snapshot ensembles consistently lowered error vs single-model baselines: examples include DenseNet-100 CIFAR-100: single 19.25% -> snapshot 17.41% (α0=0.2); DenseNet-40 CIFAR-100 varied M produced test errors 22.92% (M=2) down to 21.89% (M=8). Cyclic snapshots showed lower pairwise softmax correlations (i.e., higher diversity) than NoCycle snapshots and distinct minima in interpolation plots.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Requires careful allocation of training epochs per cycle (too short cycles hurt convergence); sensitivity to α0 and M; potential that very late snapshots may lie in same basin and add little diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Use cyclic cosine annealing (SGDR) with appropriately chosen α0 and M, ensemble only the last m snapshots (which have lowest test error), tune cycle length given total training budget, combine with data augmentation and implicit ensembling (Dropout, Stochastic Depth) if desired.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Quantitative improvements: e.g., CIFAR-100 DenseNet-100 error reduced from 19.25% to 17.41% (absolute 1.84% points) with α0=0.2; Snapshot Ensembles with M=2 achieved ImageNet validation 23.33% vs single 24.01% (ResNet-50 with B=90). Ensembles of 2–3 snapshots often yield most of the benefit; increasing α0 generally increases diversity and improves ensemble performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Typical experimental runs produce M snapshots per run (commonly M=5 or M=6); comparisons include NoCycle Snapshot (same snapshot count but non-cyclic schedule), SingleCycle Ensembles (reinitialize each cycle), and true ensembles (multiple independent full-training runs).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Snapshot Ensembling uses controlled training stochasticity (cyclic LR restarts) to produce multiple accurate and diverse models in a single training budget, and ensembling those snapshots reduces test error substantially compared to single models and approaches the benefit of small true ensembles.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Snapshot Ensembles: Train 1, get M for free', 'publication_date_yy_mm': '2017-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e604.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e604.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Diversity & reproducibility diagnostics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Diagnostics for model diversity and (implicit) reproducibility: softmax correlation and parameter interpolation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper uses pairwise correlations of softmax outputs and linear parameter-space interpolation (loss vs mixing coefficient) between snapshot weight vectors to diagnose whether snapshots lie in the same minimum (low diversity) or different minima (high diversity), which relates to ensemble effectiveness and the stability of results across snapshots.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>deep learning / computer vision</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Evaluating diversity among models obtained during training and assessing whether mixing parameters degrades or preserves test error.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Different optimization cycles (cyclic vs non-cyclic), restart learning rate, and SGD-induced randomness produce differences observed in these diagnostics.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Pairwise Pearson correlation (visualized as matrices) of softmax output vectors between snapshot models; test error J(λ) along linear interpolation between parameter vectors θ1 and θ2 (spike or smooth curve indicates distinct vs same minima); ensemble test error vs ensemble size m.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Cyclic snapshots show lower pairwise softmax correlations across snapshots than NoCycle snapshots (qualitative from Figure 6). Interpolation plots (Figure 5) show sharp spikes in interpolation loss for NoCycle early snapshots (indicating different quality and/or same minima for some snapshots) while cyclic snapshots generally do not lie in the exact same minimum as the final model. No numeric correlation values are given in text; results are presented in plotted form and correlated with ensemble error improvements (e.g., ensembles of 2 snapshots already outperform baseline in many cases).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Diagnostics show that certain snapshots (especially after non-cyclic LR drops) can converge to the same minima (high correlation), limiting reproducibility of diverse behaviours; evaluation depends on choice of metric (softmax correlation vs interpolation).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Prefer cyclic LR restarts to obtain snapshots that lie in different minima (higher diversity); select snapshots based on diagnostic signals (low correlation, distinct interpolation losses); ensemble only later snapshots which balance accuracy and diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Qualitative: cyclic scheduling produced snapshots with visibly lower softmax correlations and non-convex interpolation losses compared to NoCycle, which correlates with better ensemble accuracy; no explicit numerical reductions in variance are reported beyond test-error improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Diagnostics performed on snapshots generated within single training runs (one run produces multiple snapshots); comparative diagnostics shown for cyclic vs NoCycle schedules and for varying α0 and M.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pairwise softmax correlation and parameter-space interpolation are effective diagnostics: cyclic cosine annealing yields snapshots that are both accurate and less correlated (higher diversity) than those from non-cyclic schedules, explaining why snapshot ensembling improves test accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Snapshot Ensembles: Train 1, get M for free', 'publication_date_yy_mm': '2017-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>SGDR: Stochastic Gradient Descent with Warm Restarts <em>(Rating: 2)</em></li>
                <li>On large-batch training for deep learning: Generalization gap and sharp minima <em>(Rating: 2)</em></li>
                <li>Qualitatively characterizing neural network optimization problems <em>(Rating: 2)</em></li>
                <li>Dropout: a simple way to prevent neural networks from overfitting <em>(Rating: 1)</em></li>
                <li>Stochastic Depth for deep networks (Deep networks with stochastic depth) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-604",
    "paper_id": "paper-b134d0911e2e13ac169ffa5f478a39e6ef77869a",
    "extraction_schema_id": "extraction-schema-19",
    "extracted_data": [
        {
            "name_short": "SGD stochasticity",
            "name_full": "Stochasticity of Stochastic Gradient Descent (mini-batch noise and optimizer randomness)",
            "brief_description": "The paper attributes variability in optimization trajectories and final model weights to the intrinsic randomness of SGD (mini-batch sampling, ordering) and related optimizer behaviors, which causes different runs to converge to different local minima with different error patterns.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "deep learning / computer vision",
            "experimental_task": "Training convolutional neural networks for image classification (CIFAR-10, CIFAR-100, SVHN, Tiny ImageNet, ImageNet)",
            "variability_sources": "Mini-batch sampling noise and ordering, random weight initialization, stochasticity in SGD updates (intrinsic random motion across gradient steps), optimizer hyperparameters (learning rate schedule), data augmentation randomness, and stochastic regularizers referenced (Dropout, DropConnect, Stochastic Depth, Swapout).",
            "variability_measured": true,
            "variability_metrics": "Indirectly measured via downstream metrics: test error (percentage) across snapshots/ensembles, pairwise correlation of softmax outputs between models, and parameter-space interpolation behavior (loss along linear interpolation between parameter vectors).",
            "variability_results": "Different initializations / minibatch orderings lead to different local minima and different mistake patterns; cyclic SGD produced multiple convergences whose snapshots each had competitive test error. Quantitatively, snapshot ensembles reduced CIFAR-100 error from single-model 19.25% to 17.41% (DenseNet-100, α0=0.2), and produced lower pairwise softmax correlations (qualitative / plotted) compared to NoCycle snapshots. Table 3 shows error vs M: M={2,4,6,8,10} produced test errors {22.92,22.07,21.93,21.89,22.16}% (DenseNet-40 on CIFAR-100).",
            "reproducibility_assessed": false,
            "reproducibility_metrics": null,
            "reproducibility_results": null,
            "reproducibility_challenges": "Non-convex loss with exponentially many local minima; stochasticity in SGD and random initialization causing run-to-run variability; sensitivity to learning rate schedule and training budget per cycle (insufficient epochs per cycle prevents convergence on large datasets).",
            "mitigation_methods": "Ensembling (Snapshot Ensembles) to average out run-to-run variability; using cyclic cosine annealing (SGDR) to deliberately visit multiple minima; controlling restart learning rate (α0) to adjust perturbation strength; data augmentation and implicit ensembling methods (Dropout, Stochastic Depth) when applicable.",
            "mitigation_effectiveness": "Snapshot ensembling reduced test error in many settings (e.g., CIFAR-100 DenseNet-100: 19.25% -&gt; 17.41% with α0=0.2), ensembles of just 2 snapshots often outperform single-model baselines, and larger restart learning rates (α0=0.2 vs 0.1) tend to increase diversity and improve ensemble performance. Exact variance reductions are not reported numerically beyond error-rate improvements and correlation plots.",
            "comparison_with_without_controls": true,
            "number_of_runs": "Experiments typically perform one training run per experimental condition that produces M snapshots (M varies: commonly M=5 or M=6; they also vary M in {2,4,6,8,10}); comparisons include separate baselines (Single model, NoCycle, SingleCycle, and true ensembles trained from independent initializations).",
            "key_findings": "SGD's intrinsic stochasticity (mini-batch noise and initialization differences) produces multiple distinct local minima whose predictions differ; exploiting this via cyclic restarts and ensembling snapshots reduces test error substantially compared to single models and often matches ensembling of multiple independently trained models at lower training cost.",
            "uuid": "e604.0",
            "source_info": {
                "paper_title": "Snapshot Ensembles: Train 1, get M for free",
                "publication_date_yy_mm": "2017-04"
            }
        },
        {
            "name_short": "Snapshot Ensembling",
            "name_full": "Snapshot Ensembling (cyclic cosine annealing with weight snapshots)",
            "brief_description": "A method introduced in this paper that obtains an explicit ensemble 'for free' by taking weight snapshots at multiple local minima visited during a single training run using cyclic cosine-annealed learning-rate restarts, then averaging softmax outputs at test time.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "deep learning / computer vision",
            "experimental_task": "Creating ensembles of CNN classifiers by saving model weights at the end of each learning-rate cycle and averaging predictions for image classification benchmarks.",
            "variability_sources": "The method intentionally leverages variability induced by cyclic learning-rate restarts (restart learning rate α0), SGD stochasticity, and random initialization to generate diverse model snapshots; variability also arises from number/length of cycles M and total training budget B.",
            "variability_measured": true,
            "variability_metrics": "Test error (%) of individual snapshots and ensembles, ensemble size m vs error curves, pairwise softmax output correlation, parameter-space linear interpolation loss curves, and ablation across M and α0.",
            "variability_results": "Snapshot ensembles consistently lowered error vs single-model baselines: examples include DenseNet-100 CIFAR-100: single 19.25% -&gt; snapshot 17.41% (α0=0.2); DenseNet-40 CIFAR-100 varied M produced test errors 22.92% (M=2) down to 21.89% (M=8). Cyclic snapshots showed lower pairwise softmax correlations (i.e., higher diversity) than NoCycle snapshots and distinct minima in interpolation plots.",
            "reproducibility_assessed": false,
            "reproducibility_metrics": null,
            "reproducibility_results": null,
            "reproducibility_challenges": "Requires careful allocation of training epochs per cycle (too short cycles hurt convergence); sensitivity to α0 and M; potential that very late snapshots may lie in same basin and add little diversity.",
            "mitigation_methods": "Use cyclic cosine annealing (SGDR) with appropriately chosen α0 and M, ensemble only the last m snapshots (which have lowest test error), tune cycle length given total training budget, combine with data augmentation and implicit ensembling (Dropout, Stochastic Depth) if desired.",
            "mitigation_effectiveness": "Quantitative improvements: e.g., CIFAR-100 DenseNet-100 error reduced from 19.25% to 17.41% (absolute 1.84% points) with α0=0.2; Snapshot Ensembles with M=2 achieved ImageNet validation 23.33% vs single 24.01% (ResNet-50 with B=90). Ensembles of 2–3 snapshots often yield most of the benefit; increasing α0 generally increases diversity and improves ensemble performance.",
            "comparison_with_without_controls": true,
            "number_of_runs": "Typical experimental runs produce M snapshots per run (commonly M=5 or M=6); comparisons include NoCycle Snapshot (same snapshot count but non-cyclic schedule), SingleCycle Ensembles (reinitialize each cycle), and true ensembles (multiple independent full-training runs).",
            "key_findings": "Snapshot Ensembling uses controlled training stochasticity (cyclic LR restarts) to produce multiple accurate and diverse models in a single training budget, and ensembling those snapshots reduces test error substantially compared to single models and approaches the benefit of small true ensembles.",
            "uuid": "e604.1",
            "source_info": {
                "paper_title": "Snapshot Ensembles: Train 1, get M for free",
                "publication_date_yy_mm": "2017-04"
            }
        },
        {
            "name_short": "Diversity & reproducibility diagnostics",
            "name_full": "Diagnostics for model diversity and (implicit) reproducibility: softmax correlation and parameter interpolation",
            "brief_description": "The paper uses pairwise correlations of softmax outputs and linear parameter-space interpolation (loss vs mixing coefficient) between snapshot weight vectors to diagnose whether snapshots lie in the same minimum (low diversity) or different minima (high diversity), which relates to ensemble effectiveness and the stability of results across snapshots.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "deep learning / computer vision",
            "experimental_task": "Evaluating diversity among models obtained during training and assessing whether mixing parameters degrades or preserves test error.",
            "variability_sources": "Different optimization cycles (cyclic vs non-cyclic), restart learning rate, and SGD-induced randomness produce differences observed in these diagnostics.",
            "variability_measured": true,
            "variability_metrics": "Pairwise Pearson correlation (visualized as matrices) of softmax output vectors between snapshot models; test error J(λ) along linear interpolation between parameter vectors θ1 and θ2 (spike or smooth curve indicates distinct vs same minima); ensemble test error vs ensemble size m.",
            "variability_results": "Cyclic snapshots show lower pairwise softmax correlations across snapshots than NoCycle snapshots (qualitative from Figure 6). Interpolation plots (Figure 5) show sharp spikes in interpolation loss for NoCycle early snapshots (indicating different quality and/or same minima for some snapshots) while cyclic snapshots generally do not lie in the exact same minimum as the final model. No numeric correlation values are given in text; results are presented in plotted form and correlated with ensemble error improvements (e.g., ensembles of 2 snapshots already outperform baseline in many cases).",
            "reproducibility_assessed": false,
            "reproducibility_metrics": null,
            "reproducibility_results": null,
            "reproducibility_challenges": "Diagnostics show that certain snapshots (especially after non-cyclic LR drops) can converge to the same minima (high correlation), limiting reproducibility of diverse behaviours; evaluation depends on choice of metric (softmax correlation vs interpolation).",
            "mitigation_methods": "Prefer cyclic LR restarts to obtain snapshots that lie in different minima (higher diversity); select snapshots based on diagnostic signals (low correlation, distinct interpolation losses); ensemble only later snapshots which balance accuracy and diversity.",
            "mitigation_effectiveness": "Qualitative: cyclic scheduling produced snapshots with visibly lower softmax correlations and non-convex interpolation losses compared to NoCycle, which correlates with better ensemble accuracy; no explicit numerical reductions in variance are reported beyond test-error improvements.",
            "comparison_with_without_controls": true,
            "number_of_runs": "Diagnostics performed on snapshots generated within single training runs (one run produces multiple snapshots); comparative diagnostics shown for cyclic vs NoCycle schedules and for varying α0 and M.",
            "key_findings": "Pairwise softmax correlation and parameter-space interpolation are effective diagnostics: cyclic cosine annealing yields snapshots that are both accurate and less correlated (higher diversity) than those from non-cyclic schedules, explaining why snapshot ensembling improves test accuracy.",
            "uuid": "e604.2",
            "source_info": {
                "paper_title": "Snapshot Ensembles: Train 1, get M for free",
                "publication_date_yy_mm": "2017-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "SGDR: Stochastic Gradient Descent with Warm Restarts",
            "rating": 2
        },
        {
            "paper_title": "On large-batch training for deep learning: Generalization gap and sharp minima",
            "rating": 2
        },
        {
            "paper_title": "Qualitatively characterizing neural network optimization problems",
            "rating": 2
        },
        {
            "paper_title": "Dropout: a simple way to prevent neural networks from overfitting",
            "rating": 1
        },
        {
            "paper_title": "Stochastic Depth for deep networks (Deep networks with stochastic depth)",
            "rating": 1
        }
    ],
    "cost": 0.01279775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Snapshot Ensembles: Train 1, Get $M$ for free</h1>
<p>Gao Huang<em>, Yixuan Li, ${ }^{</em>}$ Geoff Pleiss<br>Cornell University<br>{gh349, y12363}@cornell.edu, geoff@cs.cornell.edu</p>
<p>Zhuang Liu
Tsinghua University
liuzhuangthu@gmail.com</p>
<p>John E. Hopcroft, Kilian Q. Weinberger<br>Cornell University<br>jeh@cs.cornell.edu, kqw4@cornell.edu</p>
<h4>Abstract</h4>
<p>Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters. To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective. We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of $3.4 \%$ and $17.4 \%$ respectively.</p>
<h2>1 INTRODUCTION</h2>
<p>Stochastic Gradient Descent (SGD) (Bottou, 2010) and its accelerated variants (Kingma \&amp; Ba, 2014; Duchi et al., 2011) have become the de-facto approaches for optimizing deep neural networks. The popularity of SGD can be attributed to its ability to avoid and even escape spurious saddle-points and local minima (Dauphin et al., 2014). Although avoiding these spurious solutions is generally considered positive, in this paper we argue that these local minima contain useful information that may in fact improve model performance.</p>
<p>Although deep networks typically never converge to a global minimum, there is a notion of "good" and "bad" local minima with respect to generalization. Keskar et al. (2016) argue that local minima with flat basins tend to generalize better. SGD tends to avoid sharper local minima because gradients are computed from small mini-batches and are therefore inexact (Keskar et al., 2016). If the learningrate is sufficiently large, the intrinsic random motion across gradient steps prevents the optimizer from reaching any of the sharp basins along its optimization path. However, if the learning rate is small, the model tends to converge into the closest local minimum. These two very different behaviors of SGD are typically exploited in different phases of optimization (He et al., 2016a). Initially the learning rate is kept high to move into the general vicinity of a flat local minimum. Once this search has reached a stage in which no further progress is made, the learning rate is dropped (once or twice), triggering a descent, and ultimately convergence, to the final local minimum.</p>
<p>It is well established (Kawaguchi, 2016) that the number of possible local minima grows exponentially with the number of parameters-of which modern neural networks can have millions. It is therefore not surprising that two identical architectures optimized with different initializations or minibatch orderings will converge to different solutions. Although different local minima often have very similar error rates, the corresponding neural networks tend to make different mistakes. This</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Left: Illustration of SGD optimization with a typical learning rate schedule. The model converges to a minimum at the end of training. Right: Illustration of Snapshot Ensembling. The model undergoes several learning rate annealing cycles, converging to and escaping from multiple local minima. We take a snapshot at each minimum for test-time ensembling.</p>
<p>diversity can be exploited through ensembling, in which multiple neural networks are trained from different initializations and then combined with majority voting or averaging (Caruana et al., 2004). Ensembling often leads to drastic reductions in error rates. In fact, most high profile competitions, e.g. Imagenet (Deng et al., 2009) or Kaggle<sup>1</sup>, are won by ensembles of deep learning architectures.</p>
<p>Despite its obvious advantages, the use of ensembling for deep networks is not nearly as widespread as it is for other algorithms. One likely reason for this lack of adaptation may be the cost of learning multiple neural networks. Training deep networks can last for weeks, even on high performance hardware with GPU acceleration. As the training cost for ensembles increases linearly, ensembles can quickly become uneconomical for most researchers without access to industrial scale computational resources.</p>
<p>In this paper we focus on the seemingly-contradictory goal of learning an ensemble of multiple neural networks <em>without incurring any additional training costs</em>. We achieve this goal with a training method that is simple and straightforward to implement. Our approach leverages the non-convex nature of neural networks and the ability of SGD to converge to and escape from local minima on demand. Instead of training <em>M</em> neural networks independently from scratch, we let SGD converge <em>M</em> times to local minima along its optimization path. Each time the model converges, we save the weights and add the corresponding network to our ensemble. We then restart the optimization with a large learning rate to escape the current local minimum. More specifically, we adopt the cycling procedure suggested by Loshchilov &amp; Hutter (2016), in which the learning rate is abruptly raised and then quickly lowered to follow a cosine function. Because our final ensemble consists of snapshots of the optimization path, we refer to our approach as <em>Snapshot Ensembling</em>. Figure 1 presents a high-level overview of this method.</p>
<p>In contrast to traditional ensembles, the training time for the entire ensemble is identical to the time required to train a <em>single</em> traditional model. During testing time, one can evaluate and average the last (and therefore most accurate) <em>m</em> out of <em>M</em> models. Our approach is naturally compatible with other methods to improve the accuracy, such as data augmentation, stochastic depth (Huang et al., 2016b), or batch normalization (Ioffe &amp; Szegedy, 2015). In fact, Snapshot Ensembles can even be ensembled, if for example parallel resources are available during training. In this case, an ensemble of <em>K</em> Snapshot Ensembles yields <em>K</em> × <em>M</em> models at <em>K</em> times the training cost.</p>
<p>We evaluate the efficacy of Snapshot Ensembles on three state-of-the-art deep learning architectures for object recognition: ResNet (He et al., 2016b), Wide-ResNet (Zagoruyko &amp; Komodakis, 2016), and DenseNet (Huang et al., 2016a). We show across four different data sets that Snapshot Ensembles almost always reduce error without increasing training costs. For example, on CIFAR-10 and CIFAR-100, Snapshot Ensembles obtain error rates of 3.44% and 17.41% respectively.</p>
<p><sup>1</sup>www.kaggle.com</p>
<h1>2 Related Work</h1>
<p>Neural network ensembles have been widely studied and applied in machine learning (Hansen \&amp; Salamon, 1990; Krogh et al., 1995). However, most of these prior studies focus on improving the generalization performance, while few of them address the cost of training ensembles.</p>
<p>As an alternative to traditional ensembles, so-called "implicit" ensembles have high efficiency during both training and testing (Srivastava et al., 2014; Wan et al., 2013; Huang et al., 2016b; Singh et al., 2016; Krueger et al., 2016). The Dropout (Srivastava et al., 2014) technique creates an ensemble out of a single model by "dropping" - or zeroing - random sets of hidden nodes during each mini-batch. At test time, no nodes are dropped, and each node is scaled by the probability of surviving during training. Srivastava et al. claim that Dropout reduces overfitting by preventing the co-adaptation of nodes. An alternative explanation is that this mechanism creates an exponential number of networks with shared weights during training, which are then implicitly ensembled at test time. DropConnect (Wan et al., 2013) uses a similar trick to create ensembles at test time by dropping connections (weights) during training instead of nodes. The recently proposed Stochastic Depth technique (Huang et al., 2016b) randomly drops layers during training to create an implicit ensemble of networks with varying depth at test time. Finally, Swapout (Singh et al., 2016) is a stochastic training method that generalizes Dropout and Stochastic Depth. From the perspective of model ensembling, Swapout creates diversified network structures for model averaging. Our proposed method similarly trains only a single model; however, the resulting ensemble is "explicit" in that the models do not share weights. Furthermore, our method can be used in conjunction with any of these implicit ensembling techniques.</p>
<p>Several recent publications focus on reducing the test time cost of ensembles, by transferring the "knowledge" of cumbersome ensembles into a single model (Bucilu et al., 2006; Hinton et al., 2015). Hinton et al. (2015) propose to use an ensemble of multiple networks as the target of a single (smaller) network. Our proposed method is complementary to these works as we aim to reduce the training cost of ensembles rather than the test-time cost.</p>
<p>Perhaps most similar to our work is that of Swann \&amp; Allinson (1998) and Xie et al. (2013), who explore creating ensembles from slices of the learning trajectory. Xie et al. introduce the horizontal and vertical ensembling method, which combines the output of networks within a range of training epochs. More recently, Jean et al. (2014) and Sennrich et al. (2016) show improvement by ensembling the intermediate stages of model training. Laine \&amp; Aila (2016) propose a temporal ensembling method for semi-supervised learning, which achieves consensus among models trained with different regularization and augmentation conditions for better generalization performance. Finally, Moghimi et al. (2016) show that boosting can be applied to convolutional neural networks to create strong ensembles. Our work differs from these prior works in that we force the model to visit multiple local minima, and we take snapshots only when the model reaches a minimum. We believe this key insight allows us to leverage more power from our ensembles.</p>
<p>Our work is inspired by the recent findings of Loshchilov \&amp; Hutter (2016) and Smith (2016), who show that cyclic learning rates can be effective for training convolutional neural networks. The authors show that each cycle produces models which are (almost) competitive to those learned with traditional learning rate schedules while requiring a fraction of training iterations. Although model performance temporarily suffers when the learning rate cycle is restarted, the performance eventually surpasses the previous cycle after annealing the learning rate. The authors suggest that cycling perturbs the parameters of a converged model, which allows the model to find a better local minimum. We build upon these recent findings by (1) showing that there is significant diversity in the local minima visited during each cycle and (2) exploiting this diversity using ensembles. We are not concerned with speeding up or improving the training of a single model; rather, our goal is to extract an ensemble of classifiers while following the optimization path of the final model.</p>
<h2>3 Snapshot Ensembling</h2>
<p>Snapshot Ensembling produces an ensemble of accurate and diverse models from a single training process. At the heart of Snapshot Ensembling is an optimization process which visits several local minima before converging to a final solution. We take model snapshots at these various minima, and average their predictions at test time.</p>
<p>Ensembles work best if the individual models (1) have low test error and (2) do not overlap in the set of examples they misclassify. Along most of the optimization path, the weight assignments of a neural network tend not to correspond to low test error. In fact, it is commonly observed that the validation error drops significantly only after the learning rate has been reduced, which is typically done after several hundred epochs. Our approach is inspired by the observation that training neural networks for fewer epochs and dropping the learning rate earlier has minor impact on the final test error (Loshchilov &amp; Hutter, 2016). This seems to suggest that local minima along the optimization path become promising (in terms of generalization error) after only a few epochs.</p>
<p>Cyclic Cosine Annealing. To converge to multiple local minima, we follow a cyclic annealing schedule as proposed by Loshchilov &amp; Hutter (2016). We lower the learning rate at a very fast pace, encouraging the model to converge towards its first local minimum after as few as 50 epochs. The optimization is then continued at a larger learning rate, which perturbs the model and dislodges it from the minimum. We repeat this process several times to obtain multiple convergences. Formally, the learning rate α has the form:</p>
<p>$$
\alpha(t) = f\left(\bmod\left(t - 1, \left\lceil T/M \right\rceil\right)\right), \tag{1}
$$</p>
<p>where ( t ) is the iteration number, ( T ) is the total number of training iterations, and ( f ) is a monotonically decreasing function. In other words, we split the training process into ( M ) cycles, each of which starts with a large learning rate, which is annealed to a smaller learning rate. The large learning rate α = ( f(0) ) provides the model enough energy to escape from a critical point, while the small learning rate α = ( f(\left\lceil T/M \right\rceil) ) drives the model to a well behaved local minimum. In our experiments, we set ( f ) to be the shifted cosine function proposed by Loshchilov &amp; Hutter (2016):</p>
<p>$$
\alpha(t) = \frac{\alpha_0}{2} \left( \cos\left(\frac{\pi \bmod(t - 1, \left\lceil T/M \right\rceil)}{\left\lceil T/M \right\rceil} \right) + 1 \right), \tag{2}
$$</p>
<p>where ( \alpha_0 ) is the initial learning rate. Intuitively, this function anneals the learning rate from its initial value ( \alpha_0 ) to ( f(\left\lceil T/M \right\rceil) \approx 0 ) over the course of a cycle. Following (Loshchilov &amp; Hutter, 2016), we update the learning rate at each iteration rather than at every epoch. This improves the convergence of short cycles, even when a large initial learning rate is used.</p>
<p>Snapshot Ensembling. Figure 2 depicts the training process using cyclic and traditional learning rate schedules. At the end of each training cycle, it is apparent that the model reaches a local minimum with respect to the training loss. Thus, before raising the learning rate, we take a "snapshot" of the model weights (indicated as vertical dashed black lines). After training ( M ) cycles, we have ( M ) model snapshots, ( f_1 \ldots f_M ), each of which will be used in the final ensemble. It is important to highlight that the total training time of the ( M ) snapshots is the same as training a model with a standard schedule (indicated in blue). In some cases, the standard learning rate schedule achieves lower training loss than the cyclic schedule; however, as we will show in the next section, the benefits of ensembling outweigh this difference.</p>
<p>Ensembling at Test Time. The ensemble prediction at test time is the average of the last ( m ) ( ( m \leq M )) model's softmax outputs. Let ( \mathbf{x} ) be a test sample and let ( h_i(\mathbf{x}) ) be the softmax score of snapshot ( i ). The output of the ensemble is a simple average of the last ( m ) models: ( h_{\text{Ensemble}} = \frac{1}{m} \sum_{0}^{m-1} h_{M-i}(\mathbf{x}) ). We always ensemble the last ( m ) models, as these models tend to have the lowest test error.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Training loss of 100-layer DenseNet on CIFAR10 using standard learning rate (blue) and ( M = 6 ) cosine annealing cycles (red). The intermediate models, denoted by the dotted lines, form an ensemble at the end of training.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">C10</th>
<th style="text-align: center;">C100</th>
<th style="text-align: center;">SVHN</th>
<th style="text-align: center;">Tiny ImageNet</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ResNet-110</td>
<td style="text-align: center;">Single model</td>
<td style="text-align: center;">5.52</td>
<td style="text-align: center;">28.02</td>
<td style="text-align: center;">1.96</td>
<td style="text-align: center;">46.50</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NoCycle Snapshot Ensemble</td>
<td style="text-align: center;">5.49</td>
<td style="text-align: center;">26.97</td>
<td style="text-align: center;">1.78</td>
<td style="text-align: center;">43.69</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SingleCycle Ensembles</td>
<td style="text-align: center;">6.66</td>
<td style="text-align: center;">24.54</td>
<td style="text-align: center;">1.74</td>
<td style="text-align: center;">42.60</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Snapshot Ensemble ( $\alpha_{0}=0.1$ )</td>
<td style="text-align: center;">5.73</td>
<td style="text-align: center;">25.55</td>
<td style="text-align: center;">1.63</td>
<td style="text-align: center;">40.54</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Snapshot Ensemble ( $\alpha_{0}=0.2$ )</td>
<td style="text-align: center;">5.32</td>
<td style="text-align: center;">24.19</td>
<td style="text-align: center;">1.66</td>
<td style="text-align: center;">39.40</td>
</tr>
<tr>
<td style="text-align: center;">Wide-ResNet-32</td>
<td style="text-align: center;">Single model</td>
<td style="text-align: center;">5.43</td>
<td style="text-align: center;">23.55</td>
<td style="text-align: center;">1.90</td>
<td style="text-align: center;">39.63</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Dropout</td>
<td style="text-align: center;">4.68</td>
<td style="text-align: center;">22.82</td>
<td style="text-align: center;">1.81</td>
<td style="text-align: center;">36.58</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NoCycle Snapshot Ensemble</td>
<td style="text-align: center;">5.18</td>
<td style="text-align: center;">22.81</td>
<td style="text-align: center;">1.81</td>
<td style="text-align: center;">38.64</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SingleCycle Ensembles</td>
<td style="text-align: center;">5.95</td>
<td style="text-align: center;">21.38</td>
<td style="text-align: center;">1.65</td>
<td style="text-align: center;">35.53</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Snapshot Ensemble ( $\alpha_{0}=0.1$ )</td>
<td style="text-align: center;">4.41</td>
<td style="text-align: center;">21.26</td>
<td style="text-align: center;">1.64</td>
<td style="text-align: center;">35.45</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Snapshot Ensemble ( $\alpha_{0}=0.2$ )</td>
<td style="text-align: center;">4.73</td>
<td style="text-align: center;">21.56</td>
<td style="text-align: center;">1.51</td>
<td style="text-align: center;">32.90</td>
</tr>
<tr>
<td style="text-align: center;">DenseNet-40</td>
<td style="text-align: center;">Single model</td>
<td style="text-align: center;">$5.24^{*}$</td>
<td style="text-align: center;">$24.42^{*}$</td>
<td style="text-align: center;">1.77</td>
<td style="text-align: center;">39.09</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Dropout</td>
<td style="text-align: center;">6.08</td>
<td style="text-align: center;">25.79</td>
<td style="text-align: center;">$1.79^{*}$</td>
<td style="text-align: center;">39.68</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NoCycle Snapshot Ensemble</td>
<td style="text-align: center;">5.20</td>
<td style="text-align: center;">24.63</td>
<td style="text-align: center;">1.80</td>
<td style="text-align: center;">38.51</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SingleCycle Ensembles</td>
<td style="text-align: center;">5.43</td>
<td style="text-align: center;">22.51</td>
<td style="text-align: center;">1.87</td>
<td style="text-align: center;">38.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Snapshot Ensemble ( $\alpha_{0}=0.1$ )</td>
<td style="text-align: center;">4.99</td>
<td style="text-align: center;">23.34</td>
<td style="text-align: center;">1.64</td>
<td style="text-align: center;">37.25</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Snapshot Ensemble ( $\alpha_{0}=0.2$ )</td>
<td style="text-align: center;">4.84</td>
<td style="text-align: center;">21.93</td>
<td style="text-align: center;">1.73</td>
<td style="text-align: center;">36.61</td>
</tr>
<tr>
<td style="text-align: center;">DenseNet-100</td>
<td style="text-align: center;">Single model</td>
<td style="text-align: center;">$3.74^{*}$</td>
<td style="text-align: center;">$19.25^{*}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Dropout</td>
<td style="text-align: center;">3.65</td>
<td style="text-align: center;">18.77</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NoCycle Snapshot Ensemble</td>
<td style="text-align: center;">3.80</td>
<td style="text-align: center;">19.30</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SingleCycle Ensembles</td>
<td style="text-align: center;">4.52</td>
<td style="text-align: center;">18.38</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Snapshot Ensemble ( $\alpha_{0}=0.1$ )</td>
<td style="text-align: center;">3.57</td>
<td style="text-align: center;">18.12</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Snapshot Ensemble ( $\alpha_{0}=0.2$ )</td>
<td style="text-align: center;">3.44</td>
<td style="text-align: center;">17.41</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 1: Error rates (\%) on CIFAR-10 and CIFAR-100 datasets. All methods in the same group are trained for the same number of iterations. Results of our method are colored in blue, and the best result for each network/dataset pair are bolded. ${}^{*}$ indicates numbers which we take directly from Huang et al. (2016a).</p>
<h1>4 EXPERIMENTS</h1>
<p>We demonstrate the effectiveness of Snapshot Ensembles on several benchmark datasets, comparing with competitive baselines. We run all experiments with Torch 7 (Collobert et al., 2011) ${ }^{2}$.</p>
<h3>4.1 DATASETS</h3>
<p>CIFAR. The two CIFAR datasets (Krizhevsky \&amp; Hinton, 2009) consist of colored natural images sized at $32 \times 32$ pixels. CIFAR-10 (C10) and CIFAR-100 (C100) images are drawn from 10 and 100 classes, respectively. For each dataset, there are 50,000 training images and 10,000 images reserved for testing. We use a standard data augmentation scheme (Lin et al., 2013; Romero et al., 2014; Lee et al., 2015; Springenberg et al., 2014; Srivastava et al., 2015; Huang et al., 2016b; Larsson et al., 2016), in which the images are zero-padded with 4 pixels on each side, randomly cropped to produce $32 \times 32$ images, and horizontally mirrored with probability 0.5 .
SVHN. The Street View House Numbers (SVHN) dataset (Netzer et al., 2011) contains $32 \times 32$ colored digit images from Google Street View, with one class for each digit. There are 73,257 images in the training set and 26,032 images in the test set. Following common practice (Sermanet et al., 2012; Goodfellow et al., 2013; Huang et al., 2016a), we withhold 6,000 training images for validation, and train on the remaining images without data augmentation.
Tiny ImageNet. The Tiny ImageNet dataset ${ }^{3}$ consists of a subset of ImageNet images (Deng et al., 2009). There are 200 classes, each of which has 500 training images and 50 validation images. Each image is resized to $64 \times 64$ and augmented with random crops, horizontal mirroring, and RGB intensity scaling (Krizhevsky et al., 2012).
ImageNet. The ILSVRC 2012 classification dataset (Deng et al., 2009) consists of 1000 images classes, with a total of 1.2 million training images and 50,000 validation images. We adopt the same</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: DenseNet-100 Snapshot Ensemble performance on CIFAR-10 and CIFAR-100 with restart learning rate $\alpha_{0}=0.1$ (left two) and $\alpha_{0}=0.2$ (right two). Each ensemble is trained with $M=6$ annealing cycles (50 epochs per each).
data augmentation scheme as in (He et al., 2016a; Huang et al., 2016a) and apply a $224 \times 224$ center crop to images at test time.</p>
<h1>4.2 Training Setting</h1>
<p>Architectures. We test several state-of-the-art architectures, including residual networks (ResNet) (He et al., 2016a), Wide ResNet (Zagoruyko \&amp; Komodakis, 2016) and DenseNet (Huang et al., 2016a). For ResNet, we use the original 110-layer network introduced by He et al. (2016a). Wide-ResNet is a 32-layer ResNet with 4 times as many convolutional features per layer as a standard ResNet. For DenseNet, our large model follows the same setup as (Huang et al., 2016a), with depth $L=100$ and growth rate $k=24$. In addition, we also evaluate our method on a small DenseNet, with depth $L=40$ and $k=12$. To adapt all these networks to Tiny ImageNet, we add a stride of 2 to the first layer of the models, which downsamples the images to $32 \times 32$. For ImageNet, we test the 50-layer ResNet proposed in (He et al., 2016a). We use a mini batch size of $64 .^{4}$
Baselines. Snapshot Ensembles incur the training cost of a single model; therefore, we compare with baselines that require the same amount of training. First, we compare against a Single Model trained with a standard learning rate schedule, dropping the learning rate from 0.1 to 0.01 halfway through training, and then to 0.001 when training is at $75 \%$. Additionally, to compare against implicit ensembling methods, we test against a single model trained with Dropout. This baseline uses the same learning rate as above, and drops nodes during training with a probability of 0.2 .
We then test the Snapshot Ensemble algorithm trained with the cyclic cosine learning rate as described in (2). We test models with the max learning rate $\alpha_{0}$ set to 0.1 and 0.2 . In both cases, we divide the training process into learning rate cycles. Model snapshots are taken after each learning rate cycle. Additionally, we train a Snapshot Ensemble with a non-cyclic learning rate schedule. This NoCycle Snapshot Ensemble, which uses the same schedule as the Single Model and Dropout baselines, is meant to highlight the impact of cyclic learning rates for our method. To accurately compare with the cyclic Snapshot Ensembles, we take the same number of snapshots equally spaced throughout the training process. Finally, we compare against SingleCycle Ensembles, a Snapshot Ensemble variant in which the network is re-initialized at the beginning of every cosine learning rate cycle, rather than using the parameters from the previous optimization cycle. This baseline essentially creates a traditional ensemble, yet each network only has $1 / M$ of the typical training time. This variant is meant to highlight the tradeoff between model diversity and model convergence. Though SingleCycle Ensembles should in theory explore more of the parameter space, the models do not benefit from the optimization of previous cycles.
Training Budget. On CIFAR datasets, the training budget is $B=300$ epochs for DenseNet-40 and DenseNet-100, and $B=200$ for ResNet and Wide ResNet models. Snapshot variants are trained with $M=6$ cycles of $B / M=50$ epochs for DenseNets, and $M=5$ cycles of $B / M=40$ epochs for ResNets/Wide ResNets. SVHN models are trained with a budget of $B=40$ epochs ( 5 cycles of 8 epochs). For Tiny ImageNet, we use a training budget of $B=150$ ( 6 cycles of 25 epochs). Finally, ImageNet is trained with a budget of $B=90$ epochs, and we trained 2 Snapshot variants: one with $M=2$ cycles and one with $M=3$.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>4.3 Snapshot Ensemble results</h1>
<p>Accuracy. The main results are summarized in Table 1. In most cases, Snapshot ensembles achieve lower error than any of the baseline methods. Most notably, Snapshot Ensembles yield an error rate of $17.41 \%$ on CIFAR100 using large DenseNets, far outperforming the record of $19.25 \%$ under the same training cost and architecture (Huang et al., 2016a). Our method has the most success on CIFAR-100</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Val. Error (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Single model</td>
<td style="text-align: center;">24.01</td>
</tr>
<tr>
<td style="text-align: center;">Snapshot Ensemble $(M=2)$</td>
<td style="text-align: center;">23.33</td>
</tr>
<tr>
<td style="text-align: center;">Snapshot Ensemble $(M=3)$</td>
<td style="text-align: center;">23.96</td>
</tr>
</tbody>
</table>
<p>Table 2: Top-1 error rates (\%) on ImageNet validation set using ResNet-50 with varying number of cycles.
and Tiny ImageNet, which is likely due to the complexity of these datasets. The softmax outputs for these datasets are high dimensional due to the large number of classes, making it unlikely that any two models make the same predictions. Snapshot Ensembling is also capable of improving the competitive baselines for CIFAR-10 and SVHN as well, reducing error by $1 \%$ and $0.4 \%$ respectively with the Wide ResNet architecture.</p>
<p>The NoCycle Snapshot Ensemble generally has little effect on performance, and in some instances even increases the test error. This highlights the need for a cyclic learning rate for useful ensembling. The SingleCycle Ensemble has similarly mixed performance. In some cases, e.g., DenseNet-40 on CIFAR-100, the SingleCycle Ensemble is competitive with Snapshot Ensembles. However, as the model size increases to 100 layers, it does not perform as well. This is because it is difficult to train a large model from scratch in only a few epochs. These results demonstrate that Snapshot Ensembles tend to work best when utilizing information from previous cycles. Effectively, Snapshot Ensembles strike a balance between model diversity and optimization.</p>
<p>Table 2 shows Snapshot Ensemble results on ImageNet. The Snapshot Ensemble with $M=2$ achieves $23.33 \%$ validation error, outperforming the single model baseline with $24.01 \%$ validation error. It appears that 2 cycles is the optimal choice for the ImageNet dataset. Provided with the limited total training budget $B=90$ epochs, we hypothesize that allocating fewer than $B / 2=45$ epochs per training cycle is insufficient for the model to converge on such a large dataset.
Ensemble Size. In some applications, it may be beneficial to vary the size of the ensemble dynamically at test time depending on available resources. Figure 3 displays the performance of DenseNet-40 on the CIFAR-100 dataset as the effective ensemble size, $m$, is varied. Each ensemble consists of snapshots from later cycles, as these snapshots have received the most training and therefore have likely converged to better minima. Although ensembling more models generally gives better performance, we observe significant drops in error when the second and third models are added to the ensemble. In most cases, an ensemble of two models outperforms the baseline model.
Restart Learning Rate. The effect of the restart learning rate can be observed in Figure 3. The left two plots show performance when using a restart learning rate of $\alpha_{0}=0.1$ at the beginning of each cycle, and the right two plots show $\alpha_{0}=0.2$. In most cases, ensembles with the larger restart learning rate perform better, presumably because the strong perturbation in between cycles increases the diversity of local minima.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$M$</th>
<th style="text-align: center;">Test Error (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">22.92</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">22.07</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">21.93</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">21.89</td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">22.16</td>
</tr>
</tbody>
</table>
<p>Table 3: Error rates of a DenseNet-40 Snapshot Ensemble on CIFAR-100, varying $M$-the number of models (cycles) used in the ensemble.</p>
<p>Varying Number of Cycles. Given a fixed training budget, there is a trade-off between the number of learning rate cycles and their length. Therefore, we investigate how the number of cycles $M$ affects the ensemble performance, given a fixed training budget. We train a 40-layer DenseNet on the CIFAR-100 dataset with an initial learning rate of $\alpha_{0}=0.2$. We fix the total training budget $B=300$ epochs, and vary the value of $M \in{2,4,6,8,10}$. As shown in Table 3, our method is relatively robust with respect to different values of $M$. At the extremes, $M=2$ and $M=10$, we find a slight degradation in performance, as the cycles are either too few or too short. In practice, we find that setting $M$ to be $4 \sim 8$ works reasonably well.
Varying Training Budget. The left and middle panels of Figure 4 show the performance of Snapshot Ensembles and SingleCycle Ensembles as a function of training budget (where the number of cycles is fixed at $M=6$ ). We train a 40-layer DenseNet on CIFAR-10 and CIFAR-100, with an initial learning rate of $\alpha_{0}=0.1$, varying the total number of training epochs from 60 to 300 . We observe</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Snapshot Ensembles under different training budgets on (Left) CIFAR-10 and (Middle) CIFAR-100. Right: Comparison of Snapshot Ensembles with true ensembles.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Interpolations in parameter space between the final model (sixth snapshot) and all intermediate snapshots. $\lambda=0$ represents an intermediate snapshot model, while $\lambda=1$ represents the final model. Left: A Snapshot Ensemble, with cosine annealing cycles ( $\alpha_{0}=0.2$ every $B / M=50$ epochs). Right: A NoCycle Snapshot Ensemble, (two learning rate drops, snapshots every 50 epochs).
that both Snapshot Ensembles and SingleCycle Ensembles become more accurate as training budget increases. However, we note that as training budget decreases, Snapshot Ensembles still yield competitive results, while the performance of the SingleCycle Ensembles degrades rapidly. These results highlight the improvements that Snapshot Ensembles obtain when the budget is low. If the budget is high, then the SingleCycle baseline approaches true ensembles and outperforms Snapshot ensembles eventually.
Comparison with True Ensembles. We compare Snapshot Ensembles with the traditional ensembling method. The right panel of Figure 4 shows the test error rates of DenseNet-40 on CIFAR-100. The true ensemble method averages models that are trained with 300 full epochs, each with different weight initializations. Given the same number of models at test time, the error rate of the true ensemble can be seen as a lower bound of our method. Our method achieves performance that is comparable with ensembling of 2 independent models, but with the training cost of one model.</p>
<h1>4.4 DIVERSITY OF MODEL ENSEMBLES</h1>
<p>Parameter Space. We hypothesize that the cyclic learning rate schedule creates snapshots which are not only accurate but also diverse with respect to model predictions. We qualitatively measure this diversity by visualizing the local minima that models converge to. To do so, we linearly interpolate snapshot models, as described by Goodfellow et al. (2014). Let $J(\theta)$ be the test error of a model using parameters $\theta$. Given $\theta_{1}$ and $\theta_{2}$ - the parameters from models 1 and 2 respectively - we can compute the loss for a convex combination of model parameters: $J\left(\lambda\left(\theta_{1}\right)+(1-\lambda)\left(\theta_{2}\right)\right)$, where $\lambda$ is a mixing coefficient. Setting $\lambda$ to 1 results in a parameters that are entirely $\theta_{1}$ while setting $\lambda$ to 0 gives the parameters $\theta_{2}$. By sweeping the values of $\lambda$, we can examine a linear slice of the parameter space. Two models that converge to a similar minimum will have smooth parameter interpolations, whereas models that converge to different minima will likely have a non-convex interpolation, with a spike in error when $\lambda$ is between 0 and 1 .
Figure 5 displays interpolations between the final model of DenseNet-40 (sixth snapshot) and all intermediate snapshots. The left two plots show Snapshot Ensemble models trained with a cyclic learning rate, while the right two plots show NoCycle Snapshot models. $\lambda=0$ represents a model which is entirely snapshot parameters, while $\lambda=1$ represents a model which is entirely the parameters of the final model. From this figure, it is clear that there are differences between cyclic and</p>
<p>non-cyclic learning rate schedules. Firstly, all of the cyclic snapshots achieve roughly the same error as the final cyclical model, as the error is similar for $\lambda=0$ and $\lambda=1$. Additionally, it appears that most snapshots do not lie in the same minimum as the final model. Thus the snapshots are likely to misclassify different samples. Conversely, the first three snapshots achieve much higher error than the final model. This can be observed by the sharp minima around $\lambda=1$, which suggests that mixing in any amount of the snapshot parameters will worsen performance. While the final two snapshots achieve low error, the figures suggests that they lie in the same minimum as the final model, and therefore likely add limited diversity to the ensemble.</p>
<p>Activation space. To further explore the diversity of models, we compute the pairwise correlation of softmax outputs for every pair of snapshots. Figure 6 displays the average correlation for both cyclic snapshots and non-cyclical snapshots. Firstly, there are large correlations between the last 3 snapshots of the non-cyclic training schedule (right). These snapshots are taken after dropping the learning rate, suggesting that each snapshot has converged to the same minimum. Though there is more diversity amongst the earlier snapshots, these snapshots have much higher error rates and are therefore not ideal for ensembling. Conversely, there is less correlation between all cyclic snapshots (left). Because all snapshots have similar accuracy (as can be seen in Figure 5), these differences in predictions can be exploited to create effective ensembles.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Pairwise correlation of softmax outputs between any two snapshots for DenseNet-100. Left: A Snapshot Ensemble, with cosine annealing cycles (restart with $\alpha_{0}=0.2$ every 50 epochs). Right: A NoCycle Snapshot Ensemble, (two learning rate drops, snapshots every 50 epochs).</p>
<h1>5 DISCUSSION</h1>
<p>We introduce Snapshot Ensembling, a simple method to obtain ensembles of neural networks without any additional training cost. Our method exploits the ability of SGD to converge to and escape from local minima as the learning rate is lowered, which allows the model to visit several weight assignments that lead to increasingly accurate predictions over the course of training. We harness this power with the cyclical learning rate schedule proposed by Loshchilov \&amp; Hutter (2016), saving model snapshots at each point of convergence. We show in several experiments that all snapshots are accurate, yet produce different predictions from one another, and therefore are well suited for test-time ensembles. Ensembles of these snapshots significantly improve the state-of-the-art on CIFAR-10, CIFAR-100 and SVHN. Future work will explore combining Snapshot Ensembles with traditional ensembles. In particular, we will investigate how to balance growing an ensemble with new models (with random initializations) and refining existing models with further training cycles under a fixed training budget.</p>
<h2>ACKNOWLEDGEMENTS</h2>
<p>We thank Ilya Loshchilov and Frank Hutter for their insightful comments on the cyclic cosineshaped learning rate. The authors are supported in part by the, III-1618134, III-1526012, IIS1149882 grants from the National Science Foundation, US Army Research Office W911NF-14-1-0477, and the Bill and Melinda Gates Foundation.</p>
<h2>REFERENCES</h2>
<p>Léon Bottou. Large-scale machine learning with stochastic gradient descent. In COMPSTAT. 2010.
Cristian Bucilu, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In KDD, 2006.
Rich Caruana, Alexandru Niculescu-Mizil, Geoff Crew, and Alex Ksikes. Ensemble selection from libraries of models. In ICML, 2004.</p>
<p>Ronan Collobert, Koray Kavukcuoglu, and Clément Farabet. Torch7: A matlab-like environment for machine learning. In BigLearn, NIPS Workshop, 2011.</p>
<p>Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In NIPS, 2014.</p>
<p>Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009.</p>
<p>John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.</p>
<p>Ian J Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout networks. In ICML, 2013.</p>
<p>Ian J Goodfellow, Oriol Vinyals, and Andrew M Saxe. Qualitatively characterizing neural network optimization problems. arXiv preprint arXiv:1412.6544, 2014.</p>
<p>Lars Kai Hansen and Peter Salamon. Neural network ensembles. IEEE transactions on pattern analysis and machine intelligence, 12:993-1001, 1990.</p>
<p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016a.</p>
<p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In ECCV, 2016b.</p>
<p>Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.</p>
<p>Gao Huang, Zhuang Liu, and Kilian Q Weinberger. Densely connected convolutional networks. arXiv preprint arXiv:1608.06993, 2016a.</p>
<p>Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Weinberger. Deep networks with stochastic depth. In ECCV, 2016b.</p>
<p>Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICCV, 2015.</p>
<p>Sbastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. On using very large target vocabulary for neural machine translation. arXiv preprint arXiv:1412.2007, 2014.</p>
<p>Kenji Kawaguchi. Deep learning without poor local minima. arXiv preprint arXiv:1605.07110, 2016.</p>
<p>Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016.</p>
<p>Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.</p>
<p>Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.</p>
<p>Anders Krogh, Jesper Vedelsby, et al. Neural network ensembles, cross validation, and active learning. In NIPS, volume 7, 1995.</p>
<p>David Krueger, Tegan Maharaj, János Kramár, Mohammad Pezeshki, Nicolas Ballas, Nan Rosemary Ke, Anirudh Goyal, Yoshua Bengio, Hugo Larochelle, Aaron Courville, et al. Zoneout: Regularizing rnns by randomly preserving hidden activations. arXiv preprint arXiv:1606.01305, 2016 .</p>
<p>Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. arXiv preprint arXiv:1610.02242, 2016.</p>
<p>Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. Fractalnet: Ultra-deep neural networks without residuals. arXiv preprint arXiv:1605.07648, 2016.</p>
<p>Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen Tu. Deeplysupervised nets. In AISTATS, 2015.</p>
<p>Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400, 2013.</p>
<p>Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with restarts. arXiv preprint arXiv:1608.03983, 2016.</p>
<p>Mohammad Moghimi, Mohammad Saberian, Jian Yang, Li-Jia Li, Nuno Vasconcelos, and Serge Belongie. Boosted convolutional neural networks. 2016.</p>
<p>Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning, 2011. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011.</p>
<p>Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014.</p>
<p>Rico Sennrich, Barry Haddow, and Alexandra Birch. Edinburgh neural machine translation systems for wmt 16. arXiv preprint arXiv:1606.02891, 2016.</p>
<p>Pierre Sermanet, Soumith Chintala, and Yann LeCun. Convolutional neural networks applied to house numbers digit classification. In $I C P R, 2012$.</p>
<p>Saurabh Singh, Derek Hoiem, and David Forsyth. Swapout: Learning an ensemble of deep architectures. arXiv preprint arXiv:1605.06465, 2016.</p>
<p>Leslie N. Smith. No more pesky learning rate guessing games. CoRR, abs/1506.01186, 2016. URL http://arxiv.org/abs/1506.01186.</p>
<p>Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014.</p>
<p>Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929-1958, 2014.</p>
<p>Rupesh Kumar Srivastava, Klaus Greff, and Jürgen Schmidhuber. Highway networks. arXiv preprint arXiv:1505.00387, 2015.</p>
<p>A Swann and N Allinson. Fast committee learning: Preliminary results. Electronics Letters, 34(14): 1408-1410, 1998.</p>
<p>Li Wan, Matthew Zeiler, Sixin Zhang, Yann L Cun, and Rob Fergus. Regularization of neural networks using dropconnect. In ICML, 2013.</p>
<p>Jingjing Xie, Bing Xu, and Zhang Chuang. Horizontal and vertical ensemble with deep representation for classification. arXiv preprint arXiv:1306.2759, 2013.</p>
<p>Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.</p>
<h1>SUPPLEMENTARY</h1>
<h2>A. Single model and Snapshot Ensemble performance over time</h2>
<p>In Figures 7-9, we compare the test error of Snapshot Ensembles with the error of individual model snapshots. The blue curve shows the test error of a single model snapshot using a cyclic cosine learning rate. The green curve shows the test error when ensembling model snapshots over time. (Note that, unlike Figure 3, we construct these ensembles beginning with the earliest snapshots.) As a reference, the red dashed line in each panel represents the test error of single model trained for 300 epochs using a standard learning rate schedule. Without Snapshot Ensembles, in about half of the cases, the test error of final model using a cyclic learning rate-the right most point in the blue curve-is no better than using a standard learning rate schedule.
One can observe that under almost all settings, complete Snapshot Ensembles-the right most points of the green curves-outperform the single model baselines. In many cases, ensembles of just 2 or 3 model snapshots are able to match the performance of the single model trained with a standard learning rate. Not surprisingly, the ensembles of model snapshots consistently outperform any of its members, yielding a smooth curve of test error over time.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Single model and Snapshot Ensemble performance over time (part 1).</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Single model and Snapshot Ensemble performance over time (part 2).</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Single model and Snapshot Ensemble performance over time (part 3).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ Exceptions: ResNet-110 and Wide-ResNet are trained with batch size 128 on Tiny ImageNet. The ImageNet model is trained with batch size 256.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
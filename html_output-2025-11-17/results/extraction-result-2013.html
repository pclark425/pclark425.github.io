<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2013 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2013</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2013</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-47.html">extraction-schema-47</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experiments, studies, or results related to curriculum learning for compositional tasks, compositional generalization performance, primitive skill training, composition depth effects, and generalization gaps between trained and novel compositions.</div>
                <p><strong>Paper ID:</strong> paper-277634325</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.06618v1.pdf" target="_blank">Human-like compositional learning of visually-grounded concepts using synthetic environments</a></p>
                <p><strong>Paper Abstract:</strong> The compositional structure of language enables humans to decompose complex phrases and map them to novel visual concepts, showcasing flexible intelligence. While several algorithms exhibit compositionality, they fail to elucidate how humans learn to compose concept classes and ground visual cues through trial and error. To investigate this multi-modal learning challenge, we designed a 3D synthetic environment in which an agent learns, via reinforcement, to navigate to a target specified by a natural language instruction. These instructions comprise nouns, attributes, and critically, determiners, prepositions, or both. The vast array of word combinations heightens the compositional complexity of the visual grounding task, as navigating to a blue cube above red spheres is not rewarded when the instruction specifies navigating to"some blue cubes below the red sphere". We first demonstrate that reinforcement learning agents can ground determiner concepts to visual targets but struggle with more complex prepositional concepts. Second, we show that curriculum learning, a strategy humans employ, enhances concept learning efficiency, reducing the required training episodes by 15% in determiner environments and enabling agents to easily learn prepositional concepts. Finally, we establish that agents trained on determiner or prepositional concepts can decompose held-out test instructions and rapidly adapt their navigation policies to unseen visual object combinations. Leveraging synthetic environments, our findings demonstrate that multi-modal reinforcement learning agents can achieve compositional understanding of complex concept classes and highlight the efficacy of human-like learning strategies in improving artificial systems' learning efficiency.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2013.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2013.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experiments, studies, or results related to curriculum learning for compositional tasks, compositional generalization performance, primitive skill training, composition depth effects, and generalization gaps between trained and novel compositions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Determiner Curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Curriculum learning experiments in the Determiner (D) environment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reinforcement-learning agents trained to ground eight determiner concepts (A, Few, Some, Many, This, That, These, Those) in a synthetic 3D visually-grounded navigation task; compared end-to-end training vs staged curricula (2→4→8 and 4→8). Curriculum reduced total training episodes and produced strong zero-shot compositional generalization to held-out color-shape combinations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>study_domain</strong></td>
                            <td>instruction following / visually-grounded navigation in a synthetic 3D environment</td>
                        </tr>
                        <tr>
                            <td><strong>agent_or_model_name</strong></td>
                            <td>Vision-language RL agent (A2C + LSTM; 3-layer conv vision module, one-hot language embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_structure</strong></td>
                            <td>Data-level staged curricula: (1) no curriculum (8D: train all 8 determiners directly), (2) 4D → 8D (learn 4 simpler determiners then all 8), (3) 2D → 4D → 8D (learn 2, then 4, then 8). Simpler concepts selected by fastest learning curves.</td>
                        </tr>
                        <tr>
                            <td><strong>primitive_training_details</strong></td>
                            <td>Primitives: 8 determiners. Trained separately in the Determiner environment until performance criterion (≥80% success over 1000 episodes). Dataset: 200 unique determiner instructions (160 train, 40 test).</td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_range</strong></td>
                            <td>Compositions involve single-object noun phrases with a determiner (i.e., determiner + object); no deeper multi-object relational compositions tested in this experiment (null for deeper ranges)</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_diversity_description</strong></td>
                            <td>Deterministic variation across 5 colors × 5 shapes, randomized placements within 4 room locations and 3×3×3 local offsets; 200 unique determiner instructions (160 train, 40 held-out test) to evaluate zero-shot composition of determiners with unseen color-shape pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_trained_compositions</strong></td>
                            <td>Agents reached the training performance criterion (≥80% success over 1000 episodes) for curriculum conditions. Baseline (8D) required ~0.87M episodes; 4D → 8D reduced total training episodes by ~15% (to ~0.77M); 2D → 4D → 8D produced a modest reduction (~0.82M, ~0.05M savings vs 8D).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_novel_compositions</strong></td>
                            <td>Zero-shot held-out test success rates after training: approx. 76–79% depending on curriculum (8D: 77.5% ±0.014; 4D→8D: 79.0% ±0.062; 2D→4D→8D: 76.0% ±0.248).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_value</strong></td>
                            <td>Approximately 1–4 percentage points absolute (training performance criterion ≥80% vs zero-shot held-out ≈76–79%; typical gap ≈4 percentage points when comparing 80% training to ~76% zero-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_vs_baseline_comparison</strong></td>
                            <td>Curriculum (4D→8D) reduced total training episodes by ~15% relative to no curriculum (8D); 2D→4D→8D gave a small additional savings (≈0.05M episodes). Zero-shot test performance remained high and roughly comparable across curricula (≈76–79%).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_curriculum_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_correlation_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Staged data-level curricula (especially 4→8) speed up learning of determiner grounding (≈15% fewer episodes) while preserving strong zero-shot compositional generalization (~76–79% success on held-out object combinations); primitives (the determiners) were trained to criterion before use in further composition.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports - provides evidence that primitive mastery via curriculum improves compositional generalization and reduces training cost, though a small generalization gap (~few percentage points) remains between trained and held-out novel combinations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2013.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2013.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experiments, studies, or results related to curriculum learning for compositional tasks, compositional generalization performance, primitive skill training, composition depth effects, and generalization gaps between trained and novel compositions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Preposition Curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Curriculum learning experiments in the Preposition (P) environment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Agents learn eight spatial prepositions (Above, Below, In front of, Behind, Beside, On, Between, Among) in the same synthetic 3D navigation task; naive end-to-end training fails to converge, but staged curricula (4→8 and 2→4→8) enable learning at large combinatorial scale (4800 unique instructions).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>study_domain</strong></td>
                            <td>instruction following / visually-grounded navigation in a synthetic 3D environment</td>
                        </tr>
                        <tr>
                            <td><strong>agent_or_model_name</strong></td>
                            <td>Vision-language RL agent (A2C + LSTM; same architecture as determiner experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_structure</strong></td>
                            <td>Data-level staged curricula: (1) no curriculum (8P: train all 8 prepositions directly), (2) 4P → 8P (learn 4 simpler prepositions then all 8), (3) 2P → 4P → 8P (learn 2, then 4, then 8).</td>
                        </tr>
                        <tr>
                            <td><strong>primitive_training_details</strong></td>
                            <td>Primitives: 8 prepositions. Trained separately in Preposition environment until performance criterion (≥80% success over 1000 episodes) where possible. Dataset: 6000 unique instructions (4800 train, 1200 test).</td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_range</strong></td>
                            <td>Most instructions involve two objects (Object A preposition Object B) where A and B are single instances for 6 prepositions and B is 2–9 instances for Between/Among (i.e., 2-object relational compositions); no systematic deeper depths tested.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_diversity_description</strong></td>
                            <td>Large combinatorial diversity: 4800 instruction combinations in P (4800 train, 1200 held-out test) with color/shape variation across 5 colors × 5 shapes, randomized object placement and counts for group prepositions (Between/Among).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_trained_compositions</strong></td>
                            <td>No-curriculum baseline (8P) failed to converge even after >3.5M episodes. Curricula enabled convergence: 4P→8P required ~2.87M episodes total; 2P→4P→8P required ~2.18M episodes total to reach ≥80% training success.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_novel_compositions</strong></td>
                            <td>Zero-shot held-out test success for converged curricula: ~76.1% ±0.992 (4P→8P) and ~76.4% ±0.46 (2P→4P→8P). Non-converged baseline (8P) performed at chance (~25%).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_value</strong></td>
                            <td>Approximately 3–4 percentage points absolute (training ≥80% vs held-out zero-shot ≈76.1–76.4%). For 8P (no convergence) gap is undefined; held-out performance was chance (~25%).</td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_scaling</strong></td>
                            <td>Not explicitly characterized as a function of depth, but combinatorial explosion (4800→160000 when combined) made naive training intractable; curricula ameliorated this.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_vs_baseline_comparison</strong></td>
                            <td>Curriculum was essential: 8P did not converge (>3.5M episodes), while 4P→8P and 2P→4P→8P converged with ~2.87M and ~2.18M episodes respectively, demonstrating large improvements over no curriculum.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_curriculum_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_correlation_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Prepositional (relational) concepts are substantially harder to learn end-to-end; staged curricula are necessary and effective, converting an intractable learning problem (>3.5M and non-convergent) into one solvable within ~2–3M episodes while enabling ~76% zero-shot held-out generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports - shows compositional generalization improves when primitives (prepositions) are learned via curriculum; highlights that failure to master primitives (no curriculum) yields chance-level generalization and intractable training.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2013.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2013.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experiments, studies, or results related to curriculum learning for compositional tasks, compositional generalization performance, primitive skill training, composition depth effects, and generalization gaps between trained and novel compositions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>D+P Few-shot Composition</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Combined Determiner + Preposition (D + P) few-shot compositional adaptation after primitive pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Agents pretrained separately on determiners and prepositions with curricula were fine-tuned in the combined D+P environment (det+obj + preposition + det+obj) for 100k episodes to evaluate few-shot composition across a very large combinatorial space (160,000 combinations); pretraining enabled non-trivial few-shot adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>study_domain</strong></td>
                            <td>instruction following / visually-grounded navigation in a synthetic 3D environment with multi-concept instructions</td>
                        </tr>
                        <tr>
                            <td><strong>agent_or_model_name</strong></td>
                            <td>Vision-language RL agent (A2C + LSTM) pretrained on P (4P→8P) and D separately</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_structure</strong></td>
                            <td>Two-stage approach: (1) pretrain primitives in P (using 4P→8P curriculum) and D separately until criterion, (2) fine-tune in combined D+P environment for only 0.1M (100k) episodes (few-shot composition stage).</td>
                        </tr>
                        <tr>
                            <td><strong>primitive_training_details</strong></td>
                            <td>Primitives trained separately to criterion: determiners and prepositions (see previous entries). The combined environment training used pretrained weights from P (4P→8P) and then limited fine-tuning in D+P for 100k episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_range</strong></td>
                            <td>Combined instructions include two object mentions each with a determiner and attributes connected via one preposition (i.e., determiner+object_A + preposition + determiner+object_B); this represents compositions of two object phrases and one relation.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_diversity_description</strong></td>
                            <td>Extremely high combinatorial diversity: D+P contains 160,000 unique instruction combinations (120,000 train, 40,000 test) arising from combinations of 8 determiners, 8 prepositions, colors, shapes, and object counts/positions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_trained_compositions</strong></td>
                            <td>After pretraining primitives and fine-tuning, the agent (total pretrain + fine-tune ≈2.97M episodes) achieved an average reward of 7.2 and a success rate of 53% over 1,000 testing episodes in the D+P environment (160,000 combinations).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_novel_compositions</strong></td>
                            <td>Reported result is overall success on the large D+P test set (O.O.D. scale): success rate 53% after few-shot fine-tuning; no separate held-out zero-shot figure provided for D+P without fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_value</strong></td>
                            <td>For D+P few-shot adaptation, no direct trained-vs-novel composition split is reported; however, trained primitives met ≥80% individually, whereas combined few-shot success on D+P was 53% (indicating a substantial decrement when composing across modalities without extended training).</td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_scaling</strong></td>
                            <td>Not systematically varied; authors note that increasing combinatorial depth (moving from P's 4,800 to D+P's 160,000 combinations) sharply increases difficulty and required training budget, but no formal depth-vs-performance curve is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_vs_baseline_comparison</strong></td>
                            <td>Baseline naive training in D+P was not attempted due to intractability; comparison is between pretraining+few-shot fine-tuning versus inability to converge from scratch. Pretraining + 0.1M fine-tune produced 53% success where naive end-to-end would be unattainable.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_curriculum_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_correlation_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Pretraining primitives (especially prepositions) with curricula then fine-tuning briefly in the combined D+P environment enables non-trivial few-shot composition across an enormous instruction space (160k combinations) — achieving 53% success after limited fine-tuning — demonstrating that primitive mastery substantially aids composition, but composing across primitives still yields a large performance drop relative to primitive-level success.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>nuances - supports the idea that primitive mastery and curriculum help composition, but highlights a substantial residual composition gap (trained primitives ≥80% vs combined few-shot 53%), indicating that compositional generalization limitations persist when scaling to deeper/more numerous combinations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Curriculum learning <em>(Rating: 2)</em></li>
                <li>Simulating early word learning in situated connectionist agents <em>(Rating: 2)</em></li>
                <li>Curriculum learning for reinforcement learning domains: A framework and survey <em>(Rating: 2)</em></li>
                <li>Compositional learning of visuallygrounded concepts using reinforcement <em>(Rating: 2)</em></li>
                <li>Determinet: A large-scale diagnostic dataset for complex visually-grounded referencing using determiners <em>(Rating: 2)</em></li>
                <li>Curriculum learning: A survey <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2013",
    "paper_id": "paper-277634325",
    "extraction_schema_id": "extraction-schema-47",
    "extracted_data": [
        {
            "name_short": "Determiner Curriculum",
            "name_full": "Curriculum learning experiments in the Determiner (D) environment",
            "brief_description": "Reinforcement-learning agents trained to ground eight determiner concepts (A, Few, Some, Many, This, That, These, Those) in a synthetic 3D visually-grounded navigation task; compared end-to-end training vs staged curricula (2→4→8 and 4→8). Curriculum reduced total training episodes and produced strong zero-shot compositional generalization to held-out color-shape combinations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "study_domain": "instruction following / visually-grounded navigation in a synthetic 3D environment",
            "agent_or_model_name": "Vision-language RL agent (A2C + LSTM; 3-layer conv vision module, one-hot language embeddings)",
            "curriculum_structure": "Data-level staged curricula: (1) no curriculum (8D: train all 8 determiners directly), (2) 4D → 8D (learn 4 simpler determiners then all 8), (3) 2D → 4D → 8D (learn 2, then 4, then 8). Simpler concepts selected by fastest learning curves.",
            "primitive_training_details": "Primitives: 8 determiners. Trained separately in the Determiner environment until performance criterion (≥80% success over 1000 episodes). Dataset: 200 unique determiner instructions (160 train, 40 test).",
            "composition_depth_range": "Compositions involve single-object noun phrases with a determiner (i.e., determiner + object); no deeper multi-object relational compositions tested in this experiment (null for deeper ranges)",
            "compositional_diversity_description": "Deterministic variation across 5 colors × 5 shapes, randomized placements within 4 room locations and 3×3×3 local offsets; 200 unique determiner instructions (160 train, 40 held-out test) to evaluate zero-shot composition of determiners with unseen color-shape pairs.",
            "performance_trained_compositions": "Agents reached the training performance criterion (≥80% success over 1000 episodes) for curriculum conditions. Baseline (8D) required ~0.87M episodes; 4D → 8D reduced total training episodes by ~15% (to ~0.77M); 2D → 4D → 8D produced a modest reduction (~0.82M, ~0.05M savings vs 8D).",
            "performance_novel_compositions": "Zero-shot held-out test success rates after training: approx. 76–79% depending on curriculum (8D: 77.5% ±0.014; 4D→8D: 79.0% ±0.062; 2D→4D→8D: 76.0% ±0.248).",
            "generalization_gap_measured": true,
            "generalization_gap_value": "Approximately 1–4 percentage points absolute (training performance criterion ≥80% vs zero-shot held-out ≈76–79%; typical gap ≈4 percentage points when comparing 80% training to ~76% zero-shot).",
            "composition_depth_scaling": null,
            "curriculum_vs_baseline_comparison": "Curriculum (4D→8D) reduced total training episodes by ~15% relative to no curriculum (8D); 2D→4D→8D gave a small additional savings (≈0.05M episodes). Zero-shot test performance remained high and roughly comparable across curricula (≈76–79%).",
            "adaptive_curriculum_used": false,
            "spurious_correlation_effects": null,
            "negative_transfer_observed": null,
            "key_findings_summary": "Staged data-level curricula (especially 4→8) speed up learning of determiner grounding (≈15% fewer episodes) while preserving strong zero-shot compositional generalization (~76–79% success on held-out object combinations); primitives (the determiners) were trained to criterion before use in further composition.",
            "supports_or_challenges_theory": "supports - provides evidence that primitive mastery via curriculum improves compositional generalization and reduces training cost, though a small generalization gap (~few percentage points) remains between trained and held-out novel combinations.",
            "uuid": "e2013.0"
        },
        {
            "name_short": "Preposition Curriculum",
            "name_full": "Curriculum learning experiments in the Preposition (P) environment",
            "brief_description": "Agents learn eight spatial prepositions (Above, Below, In front of, Behind, Beside, On, Between, Among) in the same synthetic 3D navigation task; naive end-to-end training fails to converge, but staged curricula (4→8 and 2→4→8) enable learning at large combinatorial scale (4800 unique instructions).",
            "citation_title": "here",
            "mention_or_use": "use",
            "study_domain": "instruction following / visually-grounded navigation in a synthetic 3D environment",
            "agent_or_model_name": "Vision-language RL agent (A2C + LSTM; same architecture as determiner experiments)",
            "curriculum_structure": "Data-level staged curricula: (1) no curriculum (8P: train all 8 prepositions directly), (2) 4P → 8P (learn 4 simpler prepositions then all 8), (3) 2P → 4P → 8P (learn 2, then 4, then 8).",
            "primitive_training_details": "Primitives: 8 prepositions. Trained separately in Preposition environment until performance criterion (≥80% success over 1000 episodes) where possible. Dataset: 6000 unique instructions (4800 train, 1200 test).",
            "composition_depth_range": "Most instructions involve two objects (Object A preposition Object B) where A and B are single instances for 6 prepositions and B is 2–9 instances for Between/Among (i.e., 2-object relational compositions); no systematic deeper depths tested.",
            "compositional_diversity_description": "Large combinatorial diversity: 4800 instruction combinations in P (4800 train, 1200 held-out test) with color/shape variation across 5 colors × 5 shapes, randomized object placement and counts for group prepositions (Between/Among).",
            "performance_trained_compositions": "No-curriculum baseline (8P) failed to converge even after &gt;3.5M episodes. Curricula enabled convergence: 4P→8P required ~2.87M episodes total; 2P→4P→8P required ~2.18M episodes total to reach ≥80% training success.",
            "performance_novel_compositions": "Zero-shot held-out test success for converged curricula: ~76.1% ±0.992 (4P→8P) and ~76.4% ±0.46 (2P→4P→8P). Non-converged baseline (8P) performed at chance (~25%).",
            "generalization_gap_measured": true,
            "generalization_gap_value": "Approximately 3–4 percentage points absolute (training ≥80% vs held-out zero-shot ≈76.1–76.4%). For 8P (no convergence) gap is undefined; held-out performance was chance (~25%).",
            "composition_depth_scaling": "Not explicitly characterized as a function of depth, but combinatorial explosion (4800→160000 when combined) made naive training intractable; curricula ameliorated this.",
            "curriculum_vs_baseline_comparison": "Curriculum was essential: 8P did not converge (&gt;3.5M episodes), while 4P→8P and 2P→4P→8P converged with ~2.87M and ~2.18M episodes respectively, demonstrating large improvements over no curriculum.",
            "adaptive_curriculum_used": false,
            "spurious_correlation_effects": null,
            "negative_transfer_observed": null,
            "key_findings_summary": "Prepositional (relational) concepts are substantially harder to learn end-to-end; staged curricula are necessary and effective, converting an intractable learning problem (&gt;3.5M and non-convergent) into one solvable within ~2–3M episodes while enabling ~76% zero-shot held-out generalization.",
            "supports_or_challenges_theory": "supports - shows compositional generalization improves when primitives (prepositions) are learned via curriculum; highlights that failure to master primitives (no curriculum) yields chance-level generalization and intractable training.",
            "uuid": "e2013.1"
        },
        {
            "name_short": "D+P Few-shot Composition",
            "name_full": "Combined Determiner + Preposition (D + P) few-shot compositional adaptation after primitive pretraining",
            "brief_description": "Agents pretrained separately on determiners and prepositions with curricula were fine-tuned in the combined D+P environment (det+obj + preposition + det+obj) for 100k episodes to evaluate few-shot composition across a very large combinatorial space (160,000 combinations); pretraining enabled non-trivial few-shot adaptation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "study_domain": "instruction following / visually-grounded navigation in a synthetic 3D environment with multi-concept instructions",
            "agent_or_model_name": "Vision-language RL agent (A2C + LSTM) pretrained on P (4P→8P) and D separately",
            "curriculum_structure": "Two-stage approach: (1) pretrain primitives in P (using 4P→8P curriculum) and D separately until criterion, (2) fine-tune in combined D+P environment for only 0.1M (100k) episodes (few-shot composition stage).",
            "primitive_training_details": "Primitives trained separately to criterion: determiners and prepositions (see previous entries). The combined environment training used pretrained weights from P (4P→8P) and then limited fine-tuning in D+P for 100k episodes.",
            "composition_depth_range": "Combined instructions include two object mentions each with a determiner and attributes connected via one preposition (i.e., determiner+object_A + preposition + determiner+object_B); this represents compositions of two object phrases and one relation.",
            "compositional_diversity_description": "Extremely high combinatorial diversity: D+P contains 160,000 unique instruction combinations (120,000 train, 40,000 test) arising from combinations of 8 determiners, 8 prepositions, colors, shapes, and object counts/positions.",
            "performance_trained_compositions": "After pretraining primitives and fine-tuning, the agent (total pretrain + fine-tune ≈2.97M episodes) achieved an average reward of 7.2 and a success rate of 53% over 1,000 testing episodes in the D+P environment (160,000 combinations).",
            "performance_novel_compositions": "Reported result is overall success on the large D+P test set (O.O.D. scale): success rate 53% after few-shot fine-tuning; no separate held-out zero-shot figure provided for D+P without fine-tuning.",
            "generalization_gap_measured": true,
            "generalization_gap_value": "For D+P few-shot adaptation, no direct trained-vs-novel composition split is reported; however, trained primitives met ≥80% individually, whereas combined few-shot success on D+P was 53% (indicating a substantial decrement when composing across modalities without extended training).",
            "composition_depth_scaling": "Not systematically varied; authors note that increasing combinatorial depth (moving from P's 4,800 to D+P's 160,000 combinations) sharply increases difficulty and required training budget, but no formal depth-vs-performance curve is provided.",
            "curriculum_vs_baseline_comparison": "Baseline naive training in D+P was not attempted due to intractability; comparison is between pretraining+few-shot fine-tuning versus inability to converge from scratch. Pretraining + 0.1M fine-tune produced 53% success where naive end-to-end would be unattainable.",
            "adaptive_curriculum_used": false,
            "spurious_correlation_effects": null,
            "negative_transfer_observed": null,
            "key_findings_summary": "Pretraining primitives (especially prepositions) with curricula then fine-tuning briefly in the combined D+P environment enables non-trivial few-shot composition across an enormous instruction space (160k combinations) — achieving 53% success after limited fine-tuning — demonstrating that primitive mastery substantially aids composition, but composing across primitives still yields a large performance drop relative to primitive-level success.",
            "supports_or_challenges_theory": "nuances - supports the idea that primitive mastery and curriculum help composition, but highlights a substantial residual composition gap (trained primitives ≥80% vs combined few-shot 53%), indicating that compositional generalization limitations persist when scaling to deeper/more numerous combinations.",
            "uuid": "e2013.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Curriculum learning",
            "rating": 2
        },
        {
            "paper_title": "Simulating early word learning in situated connectionist agents",
            "rating": 2
        },
        {
            "paper_title": "Curriculum learning for reinforcement learning domains: A framework and survey",
            "rating": 2
        },
        {
            "paper_title": "Compositional learning of visuallygrounded concepts using reinforcement",
            "rating": 2
        },
        {
            "paper_title": "Determinet: A large-scale diagnostic dataset for complex visually-grounded referencing using determiners",
            "rating": 2
        },
        {
            "paper_title": "Curriculum learning: A survey",
            "rating": 1
        }
    ],
    "cost": 0.0139565,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>HUMAN-LIKE COMPOSITIONAL LEARNING OF VISUALLY-GROUNDED CONCEPTS USING SYNTHETIC ENVIRONMENTS
9 Apr 2025</p>
<p>Zijun Lin 
Nanyang Technological University</p>
<p>Centre for Frontier AI Research
A*STAR</p>
<p>M Ganesh Kumar 
Harvard University</p>
<p>Centre for Frontier AI Research
A*STAR</p>
<p>Cheston Tan 
Centre for Frontier AI Research
A*STAR</p>
<p>HUMAN-LIKE COMPOSITIONAL LEARNING OF VISUALLY-GROUNDED CONCEPTS USING SYNTHETIC ENVIRONMENTS
9 Apr 2025423F92985E3541B44F0AEA687D6AA2ADarXiv:2504.06618v1[cs.CV]
The compositional structure of language enables humans to decompose complex phrases and map them to novel visual concepts, showcasing flexible intelligence.While several algorithms exhibit compositionality, they fail to elucidate how humans learn to compose concept classes and ground visual cues through trial-anderror.To investigate this multi-modal learning challenge, we designed a 3D synthetic environment in which an agent learns, via reinforcement, to navigate to a target specified by a natural language instruction.These instructions comprise nouns, attributes, and critically, determiners, prepositions, or both.The vast array of word combinations heightens the compositional complexity of the visual grounding task, as navigating to a blue cube above red spheres is not rewarded when the instruction specifies navigating to "some blue cubes below the red sphere".We first demonstrate that reinforcement learning agents can ground determiner concepts to visual targets but struggle with more complex prepositional concepts.Second, we show that curriculum learning-a strategy humans employ-enhances concept learning efficiency, reducing the required training episodes by 15% in determiner environments and enabling agents to easily learn prepositional concepts.Finally, we establish that agents trained on determiner or prepositional concepts can decompose held-out test instructions and rapidly adapt their navigation policies to unseen visual object combinations.Leveraging synthetic environments, our findings demonstrate that multi-modal reinforcement learning agents can achieve compositional understanding of complex concept classes and highlight the efficacy of human-like learning strategies in improving artificial systems' learning efficiency.</p>
<p>INTRODUCTION</p>
<p>Humans describe the world by combining linguistic parts-of-speech (concept classes) such as adjectives and nouns.As visual complexity increases, we use more words (Sun &amp; Firestone, 2022) and additional concept classes to enhance description accuracy (Elsner et al., 2018;Qiao et al., 2020).For example, determiners are essential for describing object quantity and ownership (Lee et al., 2023), while prepositions are critical for expressing spatial and temporal relationships between objects (Agrawal et al., 2023).In contrast, pretrained vision-language models struggle to ground visual objects to complex linguistic concepts and flexibly compose this knowledge to solve novel scenarios (Kamath et al., 2023;Shen et al., 2023).This gap restricts the alignment between human and machine communication for collaboration.</p>
<p>Children typically master the use of determiners and prepositions by age three (Abu-Akel et al., 2004;Brown, 1973), applying these concepts to describe novel situations and generalizing beyond their experiences (Tomasello, 1987;Washington &amp; Naremore, 1978).They learn incrementally, beginning with simple terms and advancing to complex concepts (Valian, 1986;Gleason &amp; Ratner, 2022;Richards, 1984).The ability to compose linguistic elements for detailed descriptions, such as "some red balls above the green box," is a skill even advanced vision-language models lack (Okawa et al., 2024).Tailoring learning curricula to individual pace and knowledge remains an ongoing chal- lenge, both for humans and machine learning systems (Bengio et al., 2009;Graves, 2008;Soviany et al., 2022;Wang et al., 2021;Narvekar et al., 2020).</p>
<p>In this work, we developed several 3D synthetic environments to study the visuo-linguistic grounding problem in a reinforcement learning setting, similar to how children could learn these concepts.</p>
<p>A trial-and-error based learning environment facilitates artificial agents to learn from experience, a requisite to continually improve alignment during human-machine interaction.Specifically, we investigate the influence of a curriculum in improving compositional learning to ground determiner and preposition concept classes to visual objects.Notably, our synthetic environments allow precise control over the generation of training and held-out test sets while ensuring balanced data across classes, a challenge with real-world datasets.Leveraging on synthetic environments, we make the following contributions:</p>
<p>• We demonstrate that reinforcement learning agents, when trained naively to maximize rewards, can ground descriptive attributes (determiners) to single objects but struggle to ground relational attributes between objects (prepositions).</p>
<p>• We show that agents cannot generalize determiners and prepositions to new combinations without incremental curriculum training from simple to complex tasks.</p>
<p>• We establish that agents can reason compositionally about novel I.I.D. and O.O.D. test instructions and navigate to unknown visual objects after learning determiner and preposition concepts.</p>
<p>SYNTHETIC ENVIRONMENTS FOR GROUNDING</p>
<p>This section describes the synthetic environments designed for agents to learn to ground visual objects to determiner and preposition linguistic concepts by trial-and-error learning.</p>
<p>ENVIRONMENT DESIGN</p>
<p>As shown in Fig. 1, we built three synthetic 3D environments to enable the agents to learn four concepts -shape, color, determiner and preposition.While our primary emphasis is on determiner and preposition, the agents also need to acquire knowledge of color and shape throughout the learning process.Specifically, objects come in random colors (red, green, blue, yellow and black) and shapes (capsule, cube, cylinder, prism and sphere), each chosen from a set of 5.</p>
<p>The target object is randomly placed in one of four predetermined locations within a rectangular room, as illustrated in Fig. 1.Additionally, the positions of the individual objects within each location are randomized within a 3×3×3 block.This creates numerous visual representations that are constrained by the description of the instruction.The room layout remains constant and includes fixed visual landmarks like a door, window, shelf, and a reference man.The first-person perspective is simulated through a Unity-based camera, capturing the environmental dynamics in RGB images.</p>
<p>These images serve as the visual input for the agent.Additionally, the environment generates textual instructions describing the target object, which are provided to the agent as language input.Details of each environment follows below.</p>
<p>DETERMINER ENVIRONMENT</p>
<p>In the determiner environment (D), agents learn eight determiners: "A", "Few", "Some", "Many", "This", "That", "These", and "Those".Instructions follow the form "Determiner + Object(s)".For example, "few yellow cubes" or "those green cylinders" (Fig. 1A).The first four determiners describe object quantity: "A" denotes a single object, 2-3 objects as "Few", 4-6 as "Some", and 7-9 as "Many".The last four determiners depend on proximity to a reference man (Fig. 1A): "This" and "These" refer to closer objects (single or multiple 2∼9), while "That" and "Those" refer to farther objects.</p>
<p>Non-targets are designed to test grounding: (1) same determiner, random color/shape; (2) same color/shape, random determiner; (3) random determiner, color, and shape.For example, given the target instruction "Many red cylinder", non-target visual objects could be "Few red cylinder", "Many blue cube", and "A green prism".</p>
<p>PREPOSITION ENVIRONMENT</p>
<p>In the preposition environment (P ), agents learn eight prepositions: "Above", "Below", "In front of", "Behind", "Beside", "On", "Between", and "Among".Instructions follow the form "Object A + Preposition + Object B", where Object A and Object B differ in color and shape.For the first six prepositions, Object A and Object B are single instances (e.g., "green capsule On red cube").For "Between" and "Among", Object A is singular, while Object B consists of 2-9 instances, respectively.</p>
<p>Non-targets are designed similarly: (1) same preposition, random color/shape; (2) same color/shape, random preposition; (3) random preposition, color, and shape.For example, given the target instruction "green cube Above blue cylinder", non-target visual objects could be "green cube In front of blue cylinder", "yellow cube Above black capsule", and "red cylinder Among green cube".</p>
<p>COMBINED DETERMINER AND PREPOSITION ENVIRONMENT</p>
<p>The combined environment (D + P ) integrates determiners and prepositions.Instructions follow the form "Determiner A + Object A + Preposition + Determiner B + Object B".For example, "a red cube Above Many black cubes" (Fig. 1D).</p>
<p>This environment evaluates the influence of curriculum learning and the ability to decompose complex instructions in few-shot.Non-targets are designed to test comprehension of all concepts: (1) swapped color/shape attributes; (2) altered determiners; (3) modified prepositions.For example, given the target instruction "A black cube Above Many red prisms", non-target visual objects could be A "red prism Above Many black cubes", "Many black cubes Above A red prisms", and "A black cube Behind Many red prisms".</p>
<p>EVALUATING COMPOSITIONAL LEARNING AND GENERALIZATION</p>
<p>In each episode, the agent receives specific rewards based on its actions.Navigating to the target object yields a reward of +10, while collisions with non-target objects or walls result in penalties of -3 and -1, respectively.Additionally, the agent incurs a penalty of -10 if it fails to reach the target within the maximum allowed steps (T max = 500).We define the Performance Criterion as the agent achieving a perfect reward of +10 in at least 800 out of 1000 episodes, corresponding to an 80% success rate.The number of training episodes required to meet this criterion in the determiner and preposition environments is detailed in Section 4.1.The results are in Section 4.2.Specifically, D comprises 200 unique instructions (160 train, 40 test), while P comprises a total of 6000 unique instructions (4800 train, 1200 test).</p>
<p>Naively training agents to learn the D + P environment is intractable as D + P contains 160,000 unique combinations (120,000 train, 40,000 test), which is 800 and 27 times more combinations than the D and P environments respectively.Hence, we used the D + P environment to evaluate agents' ability to decompose novel determiner-preposition instructions.This is by first training the agents on the determiner D and prepositions P environment separately, until they reached performance criterion, and subsequently train them in the D + P environment for only 100,000 episodes.</p>
<p>AGENT ARCHITECTURE</p>
<p>Our vision-language agent architecture, adapted from Hill et al. (2020); Lin et al. (2023), processes visual inputs (a 3×128×128 tensor of RGB pixel values) through a vision module consisting of three convolutional layers.In the Determiner environment (D), language instructions are encoded as three one-hot vectors representing the Determiner, Color, and Shape components.For the Preposition environment (P ), five one-hot vectors are used, while in the combined environment (D + P ), seven one-hot vectors represent the instructions.These are passed to a linear layer, then concatenated with the vision module's output to form an embedding, which is processed by a Long Short Term Memory (LSTM) module, whose activity s t informs both the action predictor (actor) and value estimator (critic).The actor derives a probability distribution π(a t |s t ) over the actions (move forward, move backward, turn left, turn right), while the critic estimates the state-value function V (s t ).</p>
<p>The agent's objective is to maximize cumulative discounted rewards (Sutton, 2018;Kumar et al., 2024) by navigating to correct visual targets based on language instructions (Wang et al., 2019;Kumar et al., 2022), while avoiding incorrect options incurring penalties.Training leverages the advantage actor-critic (A2C) algorithm (Mnih et al., 2016;Kumar et al., 2021a), optimized with RMSProp at a learning rate of 2.5 × 10 −4 across all experiments.</p>
<p>EXPERIMENTS AND RESULTS</p>
<p>This section evaluates the effects of a curriculum on compositional learning and generalization in grounding determiner and preposition concepts to visual objects.In experiment 1, we investigate the role of using curriculum to improve learning efficiency.Here, the agent has to ground either eight determiners or prepositions to visual objects in the Determiners or Preposition environments.evaluated by using agents pretrained on the Preposition (P ) environments, to solve the Determiner and Preposition environment (D + P ) in few-shot training.This demonstrates whether agents can rapidly adapt to an environment that contains a significantly greater number of potential combinations of instructions.</p>
<p>EXPERIMENT 1: CURRICULUM IMPROVES LEARNING EFFICIENCY</p>
<p>To emulate human-like learning, we employ a curriculum, where agents progressively learn simpler concepts before advancing to more complex ones.For both the determiner and preposition environment, agents are trained under three setups: (1) learning all eight concepts directly (8D/8P ), (2) learning four simple concepts first, then all eight e.g.(4D → 8D), and (3) learning two, then four, and finally all eight e.g.(2D → 4D → 8D).</p>
<p>The results in Table 1 and Fig. 2 (right) show that curriculum learning reduces the total training episodes by 15% in the 4D → 8D setup compared to learning all eight determiners from scratch.</p>
<p>While the 2D → 4D → 8D setup does not save more episodes than 4D → 8D, it still achieves a modest reduction of 0.05M episodes.These improvements are significant given the relatively small number of training instruction combinations in the determiner environment (160 combinations).Simple concepts were determined based on the fastest learning curves for each concept.</p>
<p>In contrast, the preposition environment (P ) presents a much greater challenge, with 4800 instruction combinations-30 times more than the determiner environment.As shown in Table 2, agents trained directly on all eight prepositions (8P ) fail to converge even after 3.5M episodes.This aligns with expectations, as the complexity of the 8P environment would require approximately 21M episodes to learn, based on the 0.87M episodes needed for the determiner environment.However, curriculum learning significantly improves learning efficiency: the 4P → 8P and 2P → 4P → 8P setups reduce the required training episodes to 2.87M and 2.18M, respectively.This demonstrates that curriculum learning is essential for tackling environments with high combinatorial complexity.We evaluated the I.I.D. zero-shot capabilities of agents on held-out test instruction combinations in the Determiner (D) and Preposition (P ) environments.Although agents were exposed to all determiner or preposition instructions, they were trained only on a subset of determiner/prepositionshape combinations (Tables 4 and 5) to achieve an 80% success rate.As shown in  The RL agents are tested in the held-out instructions right after reaching the success rate of 80%, except for 8P , which does not converge even after 3.5 million training episodes.The closer the results approach 80%, the better compositionality the agents show.</p>
<p>The combined Determiner and Preposition environment (D + P ) presents a greater challenge, with 160,000 O.O.D. instruction combinations-a scale 25 times larger than the preposition environment and 750 times larger than the determiner environment.To address this complexity, we implemented two strategies: (1) pretraining the agent in the Preposition (P ) environment using a 4P → 8P curriculum, and (2) fine-tuning the agent for compositional learning in the D + P environment for only 0.1M episodes.Without these strategies, agent convergence was unattainable.</p>
<p>Remarkably, with only 2.97M training episodes, the agent achieved an average reward of 7.2 and a success rate of 53% over 1,000 testing episodes in the D + P environment, which has 160,000 instruction combinations.This contrasts with naive training in the 8P environment with just 4,800 instruction combinations, where agents failed to converge even after 3.5M episodes.This demonstrates that once foundational concepts are learned, agents can rapidly adapt to new combinations and solve increasingly complex environments with significantly fewer episodes.Additionally, this highlights the importance of learning to compose concepts using a curriculum to tackle environments with extreme combinatorial complexity.</p>
<p>CONCLUSION, LIMITATIONS AND FUTURE WORK</p>
<p>We developed several 3D synthetic environments to illustrate the impact of curriculum learning on instruction based navigation tasks and demonstrated the compositional capabilities of reinforcement learning agents.Notably, we are the first to showcase the feasibility of grounding RL agents in complex instructions involving determiners and prepositions.Our findings reveal that agents can decompose and recompose instructions, akin to human intelligence, allowing them to effectively solve previously unseen I.I.D. and O.O.D. test cases in zero-shot and few-shot respectively.This work marks a significant step towards aligning human and machine interaction (e.g.collaborative robots, autonomous vehicles), as real-life referring expressions often extend beyond simple word forms or adjective-noun combinations to include numerical references and spatial relationships.The unity scripts and code for the environments and agents will be made publicly available upon acceptance.</p>
<p>The 3D environments crafted for this study utilized simple geometric shapes, such as "capsule" and "prism", raising concerns about the model's generalization to more realistic objects (Hill et al., 2020).Additionally, the absence of obstacles results in straightforward navigation, suggesting that agents trained in these environments may struggle with more complex navigation scenarios that involve obstacles (Anderson et al., 2018;Gu et al., 2022;Kumar et al., 2021b).Future research directions include exploring diverse model architectures and integrating pre-trained text and vision encoders (Shah et al., 2023).Investigating the optimal combination of determiners or prepositions in the curriculum, such as using 2 or 4 determiners to expedite learning, is also worthwhile.A thorough analysis of the model's representations is expected to provide insights into how concepts are embedded (Kumar et al., 2024;Lee et al., 2023), enhancing the model's generalization capabilities across diverse scenarios.Moreover, evaluating whether pretrained large language models can effectively ground visual objects to concept classes using these 3D environments offers another avenue of exploration.This could involve using LLMs to make inferences at each time step (Wang et al., 2021) or generating code to predict actions based on visual frames (Cloos et al., 2024).</p>
<p>instructions containing determiners or prepositions.Our environment dataset and analysis aims to provide deeper insights into how agents learn spatial relationships and acquire object counting capabilities in a human-like learning setting.</p>
<p>C.2 CURRICULUM LEARNING FOR REINFORCEMENT LEARNING Curriculum learning can be divided the into two categories namely model-level curriculum learning and data-level curriculum learning (Soviany et al., 2022).The former is to dynamically adjust the model's complexity or structure as it learns, potentially making it more adept at handling progressively challenging tasks.The latter focuses on defining a difficulty criterion for the training data or task.The model begins learning from simpler tasks and gradually progress to more challenging ones.This entails organizing the training dataset in a way that facilitates a smooth transition from easy to difficult examples.The underlying principle is to guide the learning process by presenting the model with tasks of increasing complexity over time.</p>
<p>Inspired by how human learns and the non-convex optimization properties of reinforcement learning, curriculum learning has been proposed to improve policy convergence if the task order is well curated (Soviany et al., 2022;Wang et al., 2021;Narvekar et al., 2020).However, optimizing the dataset or task sequence is still not well understood and hence a specific curriculum may not always result in successful learning outcome.Hill et al. (2020) demonstrated that the data-level curriculum learning approach reduced the number of training episodes needed for a reinforcement learning agent to ground 40 visual objects to its attribute-noun combination.However, a relatively small task as such still required about 600,000 training episodes for convergence.Furthermore, compositional generalization to held-out instruction was not evaluated.Comparatively, the number of instruction combinations involving either determiners and prepositions in our grounding task are 200 and 4800 respectively.We seek to establish if curriculum learning can expedite the concept grounding process and ensure compositional generalization with a task complexity that are orders of magnitude higher.</p>
<p>C.3 MODELS FOR COMPOSITIONAL LEARNING</p>
<p>Compositional learning involves breaking down information into fundamental elements or concepts and then integrating these elements to address new and unfamiliar combinations in a few-shot or zero-shot setting.(Lake et al., 2015;Xu et al., 2021;De Beule &amp; Bergen, 2006).</p>
<p>Compositional learning has primarily been investigated in the realm of object detection, where visual models are trained on pairs of object and attribute information.These models leverage learned invariances to effectively handle unseen test sets (Kato et al., 2018;Purushwalkam et al., 2019;Anwaar et al., 2021).Another approach involves using the loss function to encourage networks to break down information into generalized features (Stone et al., 2017;Tolooshams et al., 2020).</p>
<p>Recent advancements include models that can recognize or parse objects in images using bounding boxes (Lee et al., 2023) or segmentation masks (Kirillov et al., 2023), enabling them to tackle novel tasks.</p>
<p>Recent multi-modal models learn to align visual inputs to language inputs (Radford et al., 2021;Ma et al., 2023;Yuksekgonul et al., 2022) to solve the task of compositional reasoning (Lu et al., 2023) such as Visual-Question Answering (VQA) (Johnson et al., 2017), Referring Expressions (Lee et al., 2023), or augment images using instructions (Gupta &amp; Kembhavi, 2023).There are few models that integrate compositional learning across vision, language, and action domains.An example is to train reinforcement learning agents that are grounded to visual inputs and language queries.Although these agents require millions of training episodes in diverse simulated environments, they demonstrate impressive proficiency in solving instruction-based tasks (Team et al., 2021).</p>
<p>Nevertheless, how these models ground vision-language-action representations for compositional learning, what the individual concepts are, and how these concepts are recomposed to solve novel combinations remains elusive.Only recently, has the compositional generalization abilities of reinforcement learning agents trained to ground visual objects to nouns and attributes have been explored (Lin et al., 2023).In this work, we aim to demonstrate the compositionality of agents in the 3D navigation task given the complex language instructions containing determiners and prepositions.</p>
<p>Figure 1 :
1
Figure 1: Example environments, each with four target options.A reward of +10 is given when the agent navigates to the target matching the instruction.Punishments of -1, -3, and -10 are incurred for hitting a wall, reaching the wrong target, or failing to reach the correct target, respectively.A: Agent's view of the determiner (D) environment.B: Agent's view of the preposition (P ) environment.C: Close-up view of the target object in P .D: Agent's view of the combined determiner and preposition (D + P ) environment.</p>
<p>Figure 2 :
2
Figure 2: Left: Agent architecture.Red arrows or boxes represent trainable weights, while black arrows or boxes represent frozen weights.Right: Success rates of the agents with and without curriculum learning in D.</p>
<ol>
<li>2
2
EXPERIMENT 2: COMPOSITIONAL LEARNING FOR I.I.D. AND O.O.D. GENERALIZATION</li>
</ol>
<p>Table 4
4
and Table 5 (supplementary material)show the train-test split setting for the D and P environments respectively.After training the agents on the training combinations, the agent's were evaluated on its zero-shot compositional generalization ability on the held-out test combinations.</p>
<p>Table 1 :
1
Curriculum learning (4P → 8P and 2P → 4P → 8P ) reduces the total number of episodes needed to learn the Determiner (D) environment, compared to without (8P ) a curriculum.Values in the table indicate the number of episodes (in millions) needed to achieve a success rate ≥ 80% (performance criterion) over 1000 episodes.Lower values indicate faster learning.
DeterminerTrainingTotalEnvironmentEpisodes (M)Episodes (M)8D0.870.874D → 8D0.66 → 0.110.772D → 4D → 8D0.38 → 0.33 → 0.110.82PrepositionTrainingTotalEnvironmentEpisodes (M)Episodes (M)8P&gt; 3.5&gt; 3.54P → 8P1.57 → 1.32.872P → 4P → 8P0.9 → 0.3 → 0.982.18
Experiment 2 examines the I.I.D. and O.O.D. generalization capabilities of agents.Agents' I.I.D. generalization capabilities are examined on held-out test instruction combinations in either the Determiners (D) or Preposition (P ) environments in zero-shot (no training).O.O.D. generalization is</p>
<p>Table 2 :
2
Curriculum learning (4P → 8P and 2P → 4P → 8P ) reduces the total number of episodes needed to learn the Prepositions (P ) environment, compared to without (8P ) a curriculum.Values in the table indicate the number of episodes (in millions) needed to achieve a success rate ≥ 80% (performance criterion) over 1000 episodes.Lower values indicate faster learning.</p>
<p>Table 3
3EnvironmentSuccess Rate (%)8D77.5 ± 0.014D → 8D79.0 ± 0.062D → 4D → 8D76.0 ± 0.248P24.8 ± 0.854P → 8P76.1 ± 0.992P → 4P → 8P76.4 ± 0.46
, agents demonstrated consistent zero-shot performance on held-out test instructions, with success rates of at least 76%.In contrast, agents that did not converge during training (e.g., 8P ) exhibited chancelevel performance (25%).These findings indicate that agents have acquired the ability to decompose and recompose concepts, enabling generalization to new I.I.D. instructions and visual targets during held-out tests.</p>
<p>Table 3 :
3
Zero-shot generalization in D and P on held-out instructions.Values in the table indicate the mean and standard deviation of the success rate of the agents in the test environment over 3 iterations (N = 3) in percent.</p>
<p>A AGENT ARCHITECTUREFor vision input, RGB pixel values are passed into the vision module, which contains three convolutional layers, and the output flattened into a 3136 (a 64×7×7 tensor) dimensional embedding.In D, the language module takes in three one-hot vector embedding of the instructions, each representing the Determiner, Color and Shape.For example, an instruction in D such as "Those Green Capsules" is represented by three one-hot vectors respectively.Followed by the same rule, instructions in P are represented by five one-hot vectors considering that the instructions contain five words and D + P adopts seven one-hot vectors.These one-hot vectors are fully connected to a 128 unit linear embedding layer.The 3136-D vector from the vision module and the 128-D vector from the language module are concatenated and fed into a 256-D linear mixing layer.A Long Short Term Memory (LSTM) module takes the 256-dimensional embeddings as input.Its activity s t is passed to both the action predictor (actor) and value estimator (critic).The action predictor maps the LSTM's activity to a probability distribution π(a t |s t ) over four possible actions, i.e., move forward, move backward, turn left and turn right.Meanwhile, the value estimator computes a scalar approximation of the agent's state-value function V (s t ).B ENVIRONMENTAL DESIGNAs shown in Table4and 5, 75% of color-shape combinations were used to train the agents on the relevant tasks and 25% held-out test combinations were used to evaluate the agents' zero-shot ability to use the rules learned for unseen combinations.For example, during the training phase, the agents are trained on the instructions such as "Few blue cylinder" and "A black cube".Once the agents reach the performance criterion in training environment, they are tested whether they could combine the concept "Few" and "black cube" to accurately navigate to "Few black cube".C RELATED WORK C.1 DETERMINER AND PREPOSITIONLarge vision-language models, excel in zero-shot classification(Radford et al., 2021) and text-toimage generation(Tao et al., 2023), yet they face documented challenges in object counting(Paiss et al., 2023)and spatial relationship understanding(Lewis et al., 2023).Despite attempts to address these limitations through additional loss functions or fine-tuning(Jiang et al., 2023), performance improvements remain modest and fall short of perfection.In the realm of agent navigation, instructions incorporating determiners (e.g., Few, Many) or prepositions (e.g., Above, Below) have received limited attention.Although these words are prominently used by humans since age three and play a fundamental role in realising human-AI interaction, there is a scarcity of studies in this area.Our research seeks to explore the generalization capabilities of agents trained by reinforcement learning to successfully reach target objects when provided with
Describing the acquisition of determiners in english: A growth modeling approach. Ahmad Abu-Akel, Alison L Bailey, Yeow-Meng Thum, Journal of Psycholinguistic Research. 332004</p>
<p>Stupd: A synthetic dataset for spatial and temporal relation reasoning. Palaash Agrawal, Haidi Azaman, Cheston Tan, arXiv:2309.066802023arXiv preprint</p>
<p>Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, Anton Van Den, Hengel, Proceedings of the IEEE. the IEEE2018</p>
<p>Compositional learning of image-text query for image retrieval. Muhammad Umer Anwaar, Egor Labintcev, Martin Kleinsteuber, Proceedings of the IEEE/CVF Winter conference on Applications of Computer Vision. the IEEE/CVF Winter conference on Applications of Computer Vision2021</p>
<p>Curriculum learning. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, Jason Weston, Proceedings of the 26th annual international conference on machine learning. the 26th annual international conference on machine learning2009</p>
<p>A first language: The early stages. Roger Brown, 1973Harvard University Press</p>
<p>Generating and validating agent and environment code for simulating realistic personality profiles with large language models. Nathan Cloos, Ganesh Kumar, Adam Manoogian, Christopher J Cueva, Shawn A Rhoads, NeurIPS 2024 Workshop on Behavioral Machine Learning. 2024</p>
<p>On the emergence of compositionality. Joachim De, Beule , Benjamin K Bergen, The Evolution of Language. World Scientific2006</p>
<p>Visual complexity and its effects on referring expression generation. Micha Elsner, Alasdair Clarke, Hannah Rohde, Cognitive science. 422018</p>
<p>Kathleen Graves. The language curriculum: A social contextual perspective. Language teaching. Jean Berko, Gleason , Nan Bernstein Ratner, 2022. 2008Plural Publishing41The development of language</p>
<p>Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. Jing Gu, Eliana Stefani, Qi Wu, Jesse Thomason, Xin Eric, Wang , arXiv:2203.12667Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022. 2023arXiv preprintVision-and-language navigation: A survey of tasks, methods, and future directions</p>
<p>Simulating early word learning in situated connectionist agents. Felix Hill, Stephen Clark, Phil Blunsom, Karl Moritz Hermann, Annual Meeting of the Cognitive Science Society. 2020</p>
<p>Ruixiang Jiang, Lingbo Liu, Changwen Chen, arXiv:2305.07304Clip-count: Towards text-guided zero-shot object counting. 2023arXiv preprint</p>
<p>Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, Ross Girshick, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>What's" up" with vision-language models? investigating their struggle with spatial reasoning. Amita Kamath, Jack Hessel, Kai-Wei Chang, arXiv:2310.197852023arXiv preprint</p>
<p>Compositional learning for human object interaction. Keizo Kato, Yin Li, Abhinav Gupta, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)2018SynData@ICLR2025</p>
<p>. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, arXiv:2304.026432023Segment anything. arXiv preprint</p>
<p>One-shot learning of paired associations by a reservoir computing model with hebbian plasticity. Cheston Ganesh Kumar, Camilo Tan, Libedinsky, Andrew Shih-Cheng Yen, -Yi Yong, Tan, arXiv:2106.035802021aarXiv preprint</p>
<p>One-shot learning of paired association navigation with biologically plausible schemas. Cheston Ganesh Kumar, Camilo Tan, Libedinsky, Andrew Shih-Cheng Yen, -Yi Yong, Tan, arXiv:2106.035802021barXiv preprint</p>
<p>A nonlinear hidden layer enables actor-critic agents to learn multiple paired association navigation. Cheston Kumar, Camilo Tan, Libedinsky, Andrew Yy Shih-Cheng Yen, Tan, Cerebral Cortex. 32182022</p>
<p>A model of place field reorganization during reward maximization. Blake Ganesh Kumar, Jacob A Bordelon, Cengiz A Zavatone-Veth, Pehlevan, bioRxiv. 2024</p>
<p>Human-level concept learning through probabilistic program induction. Ruslan Brenden M Lake, Joshua B Salakhutdinov, Tenenbaum, Science. 35062662015</p>
<p>Determinet: A large-scale diagnostic dataset for complex visually-grounded referencing using determiners. Clarence Lee, Cheston Ganesh Kumar, Tan, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Martha Lewis, Nihal V Nayak, Peilin Yu, Qinan Yu, Jack Merullo, Stephen H Bach, Ellie Pavlick, arXiv:2212.10537[cs.LG]Does clip bind concepts? probing compositionality in large image models. 2023</p>
<p>Compositional learning of visuallygrounded concepts using reinforcement. Zijun Lin, Haidi Azaman, M Ganesh Kumar, Cheston Tan, arXiv:2309.045042023arXiv preprint</p>
<p>Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Jianfeng Gao, arXiv:2304.09842Chameleon: Plug-and-play compositional reasoning with large language models. 2023arXiv preprint</p>
<p>Crepe: Can vision-language foundation models reason compositionally. Zixian Ma, Jerry Hong, Mustafa Omer Gul, Mona Gandhi, Irena Gao, Ranjay Krishna, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Asynchronous methods for deep reinforcement learning. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu, International conference on machine learning. PMLR2016</p>
<p>Curriculum learning for reinforcement learning domains: A framework and survey. Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E Taylor, Peter Stone, The Journal of Machine Learning Research. 2112020</p>
<p>Compositional abilities emerge multiplicatively: Exploring diffusion models on a synthetic task. Maya Okawa, S Ekdeep, Robert Lubana, Hidenori Dick, Tanaka, Advances in Neural Information Processing Systems. 202436</p>
<p>Roni Paiss, Ariel Ephrat, Omer Tov, Shiran Zada, Inbar Mosseri, Michal Irani, Tali Dekel, arXiv:2302.12066Teaching clip to count to ten. 2023arXiv preprint</p>
<p>Task-driven modular networks for zero-shot compositional learning. Senthil Purushwalkam, Maximilian Nickel, Abhinav Gupta, Marc'aurelio Ranzato, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2019</p>
<p>Referring expression comprehension: A survey of methods and datasets. Yanyuan Qiao, Chaorui Deng, Qi Wu, IEEE Transactions on Multimedia. 232020</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, 2021</p>
<p>Language curriculum development. C Jack, Richards, RELC journal. 1511984</p>
<p>Lm-nav: Robotic navigation with large pretrained models of language, vision, and action. Dhruv Shah, Błażej Osiński, Sergey Levine, Conference on robot learning. PMLR2023</p>
<p>Scaling vision-language models with sparse mixture of experts. Sheng Shen, Zhewei Yao, Chunyuan Li, Trevor Darrell, Kurt Keutzer, Yuxiong He, arXiv:2303.072262023arXiv preprint</p>
<p>Curriculum learning: A survey. Petru Soviany, Tudor Radu, Paolo Ionescu, Nicu Rota, Sebe, International Journal of Computer Vision. 13062022</p>
<p>Teaching compositionality to cnns. Austin Stone, Huayan Wang, Michael Stark, Yi Liu, Dileep Scott Phoenix, George, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>Seeing and speaking: How verbal "description length" encodes visual complexity. Zekun Sun, Chaz Firestone, Journal of Experimental Psychology: General. 1511822022</p>
<p>Reinforcement learning: An introduction. Richard S Sutton, 2018A Bradford Book</p>
<p>Galip: Generative adversarial clips for text-to-image synthesis. Ming Tao, Bing-Kun Bao, Hao Tang, Changsheng Xu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Open-ended learning leads to generally capable agents. Adam Team, Anuj Stooke, Catarina Mahajan, Charlie Barros, Jakob Deck, Jakub Bauer, Maja Sygnowski, Max Trebacz, Michael Jaderberg, Mathieu, arXiv:2107.12808Open Ended Learning. 2021arXiv preprint</p>
<p>Convolutional dictionary learning based auto-encoders for natural exponential-family distributions. Bahareh Tolooshams, Andrew Song, Simona Temereanca, Demba Ba, International Conference on Machine Learning. PMLR2020</p>
<p>Learning to use prepositions: A case study. Michael Tomasello, Journal of child Language. 1411987</p>
<p>Syntactic categories in the speech of young children. Virginia Valian, Developmental psychology. 2245621986</p>
<p>Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation. Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang, Wang , Lei Zhang, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>A survey on curriculum learning. Xin Wang, Yudong Chen, Wenwu Zhu, IEEE Transactions on Pattern Analysis and Machine Intelligence. 4492021</p>
<p>Children's use of spatial prepositions in two-and threedimensional tasks. S Dora, Rita C Washington, Naremore, Journal of Speech and Hearing Research. 2111978</p>
<p>Guangyue Xu, Parisa Kordjamshidi, Joyce Y Chai, arXiv:2107.05176Zero-shot compositional concept learning. 2021arXiv preprint</p>
<p>When and why vision-language models behave like bags-of-words, and what to do about it?. Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, James Zou, The Eleventh International Conference on Learning Representations. 2022</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1559 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1559</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1559</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-30.html">extraction-schema-30</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-258762577</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.10626v3.pdf" target="_blank">Language Models Meet World Models: Embodied Experiences Enhance Language Models</a></p>
                <p><strong>Paper Abstract:</strong> While large language models (LMs) have shown remarkable capabilities across numerous tasks, they often struggle with simple reasoning and planning in physical environments, such as understanding object permanence or planning household activities. The limitation arises from the fact that LMs are trained only on written text and miss essential embodied knowledge and skills. In this paper, we propose a new paradigm of enhancing LMs by finetuning them with world models, to gain diverse embodied knowledge while retaining their general language capabilities. Our approach deploys an embodied agent in a world model, particularly a simulator of the physical world (VirtualHome), and acquires a diverse set of embodied experiences through both goal-oriented planning and random exploration. These experiences are then used to finetune LMs to teach diverse abilities of reasoning and acting in the physical world, e.g., planning and completing goals, object permanence and tracking, etc. Moreover, it is desirable to preserve the generality of LMs during finetuning, which facilitates generalizing the embodied knowledge across tasks rather than being tied to specific simulations. We thus further introduce the classical (EWC) for selective weight updates, combined with low-rank adapters (LoRA) for training efficiency. Extensive experiments show our approach substantially improves base LMs on 18 downstream tasks by 64.28% on average. In particular, the small LMs (1.3B, 6B, and 13B) enhanced by our approach match or even outperform much larger LMs (e.g., ChatGPT).</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1559.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1559.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>E2WM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Finetuning with Embodied Experiences from World Models (E2WM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training paradigm that collects embodied experiences from a physics-based world model (VirtualHome) via goal-oriented planning and random exploration, then compiles these experiences into supervised fine-tuning tasks for pretrained language models while preserving generality using EWC-LoRA regularization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GPT-Neo / GPT-J / OPT / LLaMA</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Autoregressive transformer language models (pretrained LMs) finetuned with supervised objectives constructed from simulator experiences; tuning is parameter-efficient using LoRA and regularized with Elastic Weight Consolidation (EWC).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td>1.3B, 6B, 13B</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>VirtualHome</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A multi-agent 3D household simulator (Unity-based) that exposes atomic executable actions (e.g., [Grab] <apple>) and world-state predicates (e.g., ON(apple, table)); supports multi-agent interactions, object manipulation, and tracking of object locations and relations over time.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>household tasks / embodied commonsense procedures</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Plan generation (e.g., set up table, watch TV), Activity recognition (infer activity name from plan or final state), Counting (number of objects in a location after actions), Object path tracking (sequence of rooms where an object appears).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Tasks are compositional: activities are defined as sets of goal predicates and decompose into ordered sequences of primitive actions (plans); object-path tasks are ordered sequences of location appearances; goal achievement is defined by satisfying predicate sets, and planners (MCTS) search sequences of primitives to satisfy multiple predicates.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td>The paper reports generalization to unseen activities and benchmarks: plan generation Rouge-L improved (example: 34.31 → 51.23), counting accuracy improved (30.41% → 67.01%), object path tracking LCS improved (33.86 → 98.67) for GPT-J finetuned with E2WM; finetuned smaller models often match or outperform much larger models such as ChatGPT on many tasks; bAbI tasks related to embodied reasoning also see large gains (e.g., GPT-J finetuned surpasses ChatGPT on several bAbI tasks). Perplexity on pretraining data is preserved (negligible increase: e.g., GPT-J 3.443 → 3.537 on Pile subset).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>No explicit curriculum learning strategy is used; instead, effectiveness arises from (1) two complementary data-collection modes — goal-oriented planning (MCTS to produce goal-focused multi-step plans defined by predicate sets) and random exploration (to produce object permanence/tracking experiences); (2) compiling these diverse experiences into multiple supervised tasks (plan generation, activity recognition, counting, object path tracking) so that each training task teaches a specific embodied skill; (3) using EWC-LoRA (combining Elastic Weight Consolidation with low-rank adapters) to preserve pretrained language capabilities while allowing targeted parameter updates, mitigating catastrophic forgetting; ablations show removing a specific training task degrades performance on the corresponding downstream task, indicating task-specific contributions rather than a staged curriculum.</td>
                        </tr>
                        <tr>
                            <td><strong>additional_notes</strong></td>
                            <td>The paper does not present an explicit curriculum ordering (e.g., easy→hard or prerequisite sequencing) or any comparisons between curriculum strategies and non-curriculum training; training examples are collected procedurally but presented to the model in supervised fine-tuning (with task weights), not via an explicit curriculum schedule.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models Meet World Models: Embodied Experiences Enhance Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are zero-shot planners: Extracting actionable knowledge for embodied agents <em>(Rating: 2)</em></li>
                <li>Inner monologue: Embodied reasoning through planning with language models <em>(Rating: 2)</em></li>
                <li>Grounding large language models in interactive environments with online reinforcement learning <em>(Rating: 2)</em></li>
                <li>Pre-trained language models for interactive decision-making <em>(Rating: 2)</em></li>
                <li>Mind's eye: Grounded language model reasoning through simulation <em>(Rating: 1)</em></li>
                <li>Do as i can, not as i say: Grounding language in robotic affordances <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1559",
    "paper_id": "paper-258762577",
    "extraction_schema_id": "extraction-schema-30",
    "extracted_data": [
        {
            "name_short": "E2WM",
            "name_full": "Finetuning with Embodied Experiences from World Models (E2WM)",
            "brief_description": "A training paradigm that collects embodied experiences from a physics-based world model (VirtualHome) via goal-oriented planning and random exploration, then compiles these experiences into supervised fine-tuning tasks for pretrained language models while preserving generality using EWC-LoRA regularization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GPT-Neo / GPT-J / OPT / LLaMA",
            "agent_description": "Autoregressive transformer language models (pretrained LMs) finetuned with supervised objectives constructed from simulator experiences; tuning is parameter-efficient using LoRA and regularized with Elastic Weight Consolidation (EWC).",
            "agent_size": "1.3B, 6B, 13B",
            "environment_name": "VirtualHome",
            "environment_description": "A multi-agent 3D household simulator (Unity-based) that exposes atomic executable actions (e.g., [Grab] &lt;apple&gt;) and world-state predicates (e.g., ON(apple, table)); supports multi-agent interactions, object manipulation, and tracking of object locations and relations over time.",
            "procedure_type": "household tasks / embodied commonsense procedures",
            "procedure_examples": "Plan generation (e.g., set up table, watch TV), Activity recognition (infer activity name from plan or final state), Counting (number of objects in a location after actions), Object path tracking (sequence of rooms where an object appears).",
            "compositional_structure": "Tasks are compositional: activities are defined as sets of goal predicates and decompose into ordered sequences of primitive actions (plans); object-path tasks are ordered sequences of location appearances; goal achievement is defined by satisfying predicate sets, and planners (MCTS) search sequences of primitives to satisfy multiple predicates.",
            "uses_curriculum": false,
            "curriculum_name": null,
            "curriculum_description": null,
            "curriculum_ordering_principle": null,
            "task_complexity_range": null,
            "performance_with_curriculum": null,
            "performance_without_curriculum": null,
            "has_curriculum_comparison": false,
            "alternative_curriculum_performance": null,
            "transfer_generalization": "The paper reports generalization to unseen activities and benchmarks: plan generation Rouge-L improved (example: 34.31 → 51.23), counting accuracy improved (30.41% → 67.01%), object path tracking LCS improved (33.86 → 98.67) for GPT-J finetuned with E2WM; finetuned smaller models often match or outperform much larger models such as ChatGPT on many tasks; bAbI tasks related to embodied reasoning also see large gains (e.g., GPT-J finetuned surpasses ChatGPT on several bAbI tasks). Perplexity on pretraining data is preserved (negligible increase: e.g., GPT-J 3.443 → 3.537 on Pile subset).",
            "key_findings": "No explicit curriculum learning strategy is used; instead, effectiveness arises from (1) two complementary data-collection modes — goal-oriented planning (MCTS to produce goal-focused multi-step plans defined by predicate sets) and random exploration (to produce object permanence/tracking experiences); (2) compiling these diverse experiences into multiple supervised tasks (plan generation, activity recognition, counting, object path tracking) so that each training task teaches a specific embodied skill; (3) using EWC-LoRA (combining Elastic Weight Consolidation with low-rank adapters) to preserve pretrained language capabilities while allowing targeted parameter updates, mitigating catastrophic forgetting; ablations show removing a specific training task degrades performance on the corresponding downstream task, indicating task-specific contributions rather than a staged curriculum.",
            "additional_notes": "The paper does not present an explicit curriculum ordering (e.g., easy→hard or prerequisite sequencing) or any comparisons between curriculum strategies and non-curriculum training; training examples are collected procedurally but presented to the model in supervised fine-tuning (with task weights), not via an explicit curriculum schedule.",
            "uuid": "e1559.0",
            "source_info": {
                "paper_title": "Language Models Meet World Models: Embodied Experiences Enhance Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are zero-shot planners: Extracting actionable knowledge for embodied agents",
            "rating": 2
        },
        {
            "paper_title": "Inner monologue: Embodied reasoning through planning with language models",
            "rating": 2
        },
        {
            "paper_title": "Grounding large language models in interactive environments with online reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Pre-trained language models for interactive decision-making",
            "rating": 2
        },
        {
            "paper_title": "Mind's eye: Grounded language model reasoning through simulation",
            "rating": 1
        },
        {
            "paper_title": "Do as i can, not as i say: Grounding language in robotic affordances",
            "rating": 1
        }
    ],
    "cost": 0.0093195,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Language Models Meet World Models: Embodied Experiences Enhance Language Models
28 Oct 2023</p>
<p>Jiannan Xiang 
Tianhua Tao 
Yi Gu 
TianminShu ♢△ 
Zirui Wang 
Zichao Yang 
Zhiting Hu 
San Diego 
♣ Uiuc 
♢ Mit 
△ Jhu 
♡ Cmu </p>
<p>ChatGPT ChatGPT ChatGPT LM</p>
<p>LM LM</p>
<p>Language Models Meet World Models: Embodied Experiences Enhance Language Models
28 Oct 20232DF9ACECA91BCE47E38CC99BE16A1096arXiv:2305.10626v3[cs.CL]Plan Generation Track &amp; Count Track &amp; Permanence 1Walk to living room2Sit on sofa
While large language models (LMs) have shown remarkable capabilities across numerous tasks, they often struggle with simple reasoning and planning in physical environments, such as understanding object permanence or planning household activities.The limitation arises from the fact that LMs are trained only on written text and miss essential embodied knowledge and skills.In this paper, we propose a new paradigm of enhancing LMs by finetuning them with world models, to gain diverse embodied knowledge while retaining their general language capabilities.Our approach deploys an embodied agent in a world model, particularly a simulator of the physical world (VirtualHome), and acquires a diverse set of embodied experiences through both goal-oriented planning and random exploration.These experiences are then used to finetune LMs to teach diverse abilities of reasoning and acting in the physical world, e.g., planning and completing goals, object permanence and tracking, etc.Moreover, it is desirable to preserve the generality of LMs during finetuning, which facilitates generalizing the embodied knowledge across tasks rather than being tied to specific simulations.We thus further introduce the classical elastic weight consolidation (EWC) for selective weight updates, combined with low-rank adapters (LoRA) for training efficiency.Extensive experiments show our approach substantially improves base LMs on 18 downstream tasks by 64.28% on average.In particular, the small LMs (1.3B, 6B, and 13B) enhanced by our approach match or even outperform much larger LMs (e.g., ChatGPT). 1 * Equal contribution. 1 The code is available at https://github.com/szxiangjn/world-model-for-language-model. 2 Based on GPT-3.5-turbo.37th Conference on Neural Information Processing Systems (NeurIPS 2023).</p>
<p>Introduction</p>
<p>Language Models (LMs) have demonstrated impressive performance on a wide range of natural language processing tasks [34,48,4,7,54].In particular, recent studies show that LMs can assist decision-making for embodied tasks [1,18,25,45,19], demonstrating a certain level of understanding of the physical world.However, such understanding is not robust enough for many reasoning and planning tasks in physical environments.As shown in Figure 1, even the latest large LMs like ChatGPT 2 can still make mistakes in seemingly simple inquiries, such as counting objects in a location.We hypothesize that this is because current LMs trained merely with large-scale text corpora are devoid of embodied experiences such as navigating in an environment, interacting with objects, and sensing as well as tracking the world state.Consequently, they lack robust and comprehensive embodied knowledge necessary for reasoning and planning associated with physical environments.A related line of research finetunes LMs in order to improve specific embodied tasks, resulting in task-specialized models [6,58,21,57].In this paper, we aim to inject diverse fundamental embodied knowledge and skills into pretrained LMs, while retaining the models' generality.We introduce a novel training paradigm for LMs -finetuning with Embodied Experiences from World Models (E2WM).Here, world models are embodied simulators that emulate physical interactions in real-world environments (e.g., VirtualHome [36]).They provide LMs with the opportunity to comprehend object interactions within the environment and to execute actions, thus enabling a level of active engagement previously unattainable.These world models serve as a simplified and cost-effective replica of our real world that can significantly augment the conventional pretraining paradigm.We anticipate that finetuning LMs on embodied experiences gathered from world models can enhance their embodied knowledge-and, with the preserved model generality-consequently strengthen their abilities to solve a broad range of embodied tasks.</p>
<p>In this work, we consider a diverse range of fundamental knowledge and skills for embodied tasks, including tracking objects, planning to complete given goals, recognizing other agents' behaviors, etc.To this end, we introduce two ways to collect embodied experiences from world models that give rise to the desired knowledge and skills: goal-oriented planning and random exploration (Figure 1).Specifically, goal-oriented planning aims to gather experiences associated with planning and goal-oriented agent behaviors, while random exploration focuses on accumulating experiences that involve object and world state tracking.In goal-oriented planning, models are given the goal (e.g., IN(dust, trash can)) for a specific activity (e.g., Clean Floor), and is supposed to generate a plan to complete it.To find the plan, we devise Monte Carlo Tree Search (MCTS) [5,44] to explore the world model.Then the process will be stored as an embodied experience.In random exploration, one or more agents are deployed in the world model to execute random actions, while the locations and the movements of all the objects are tracked simultaneously.</p>
<p>After collecting the embodied experiences, we use them to construct a set of fine-tuning tasks (e.g., plan generation, activity recognition, and tracking).Crucially, to finetune LMs on the collected embodied experiences while retaining their original general knowledge and capabilities, we propose to incorporate the classical Elastic Weight Consolidation (EWC) [22] into our training paradigm.By regularizing the finetuning loss, EWC aims to preserve the important LM parameters from pretraining.We show that EWC is substantially more effective than the popular KL regularization [35,28,31].We further introduce efficient low-rank updates by harmonizing the recent Low-Rank Adaptation (LoRA) [16] with the EWC regularizer.This results in the new EWC-LoRA update rule that greatly reduces training costs and makes our E2WM paradigm accessible to cheap hardware (GPUs).</p>
<p>We instantiate a world model using a virtual household simulator, VirtualHome [36,37], and apply our method to GPT-Neo-1.3B[3], GPT-J-6B [49], OPT-13B [59], and LLaMA-13B [48] models.To test the generalizability of the finetuned LMs, we evaluate them on a variety of unseen tasks which demand similar embodied knowledge required to solve the training tasks.Additionally, we assess the models' performance on the original pretraining data to determine the extent to which their core language modeling abilities are retained.Experiments show that our method significantly improves the baselines on both seen and unseen tasks (e.g., 34.31 → 51.23 Rouge-L on plan generation task, 30.41% → 67.01%accuracy on counting task), without suffering performance degradation on the pretraining dataset (3.443 → 3.537 perplexity on Pile test subset [12]).Moreover, the small GPT-J-6B, OPT-13B, and LLaMA-13B models finetuned with our E2WM paradigm even outperforms ChatGPT on many of the tasks.The experimental results demonstrate the effectiveness of E2WM as a promising fine-tuning mechanism to enhance pretrained LMs with generalizable embodied knowledge and skills.</p>
<p>Related Work</p>
<p>World Model.The term "world model" generally refer to a computational representation of the physical world, capable of simulating changes in the world's state in response to various actions.For instance, humans possess an internal world model that aids in predicting the outcomes of specific actions during the planning process.Recent research induces world models from large LMs for robust human-like reasoning [15].In this work, we employ a simulator equipped with a physics engine to serve as our world model, effectively emulating real-world conditions.In the field of embodied AI, various world models are built to replicate the real world and serve as virtual test environments for assessing robotic agents before real-world deployment.For example, VirtualHome [36,37] is a simulated 3D household environment implemented by Unity3D game engine.AI2-THOR [23] consists of near photo-realistic 3D indoor scenes and has richer object attributes and interaction types.Other indoor household World Models include VRKitchen [13], CHALET [56], MINOS [41], House3D [53], etc. Besides, MineCraft is a more challenging and open-ended world model, which has a large number of objectives and a large-scale task hierarchy [14,27,20].In this paper, we use VirtualHome as our world model.Language Model Grounding.A significant number of recent works focused on grounding language models to world models [1,24,38,47,55].Some of them freeze LMs and leverage certain prompting strategies or specifically-designed modules.For example, Zero-Shot Planner [18] prompts LMs to generate activity plans and translate them into admissible actions.Mind's eye [29] prompts LMs to do simulations with physical engines to answer physical reasoning questions.SayCan [1] uses a learned affordance function to assist LMs in selecting valid actions.DEPS [50] prompts LMs to describe, explain and generate action plans, incorporated with a learned selector module to choose the most efficient path.There are also other works finetuning LMs towards better downstream task performance.For example, Li et al. [25] finetune LMs with supervised learning for interactive decision making, and Carta et al. [6] ground LMs with online reinforcement learning.Different from these works aiming to optimize LMs for specific tasks in the target environments, our work instead focuses on improving the language model itself by acquiring knowledge from world models.</p>
<p>Language Model Regularization.To facilitate the acquisition of new knowledge and skills without losing LMs' language modeling abilities, regularization is often introduced during finetuning.One popular method is adding KL penalty [35,28,46,52,33,60], which leverages KL divergence between the output probability of the currently trained model and the original model to regularize the LM in an RL manner, i.e., by computing policy gradients.For example, InstructGPT uses KL penalty to mitigate over-optimization of the reward model [35], and Liu et al. [28] add KL regularization for training a commonsense knowledge generator.In this work, we instead use elastic weight consolidation (EWC) for regularization.Our empirical results demonstrate that EWC is more effective than applying KL penalty for retaining language modeling abilities and generality of LMs.</p>
<p>Approach</p>
<p>In this work, we propose a new training paradigm, namely finetuning with Embodied Experiences from World Models (E2WM), to inject embodied knowledge into LMs without sacrificing its generality and language modeling abilities.The world model we use is VirtualHome [36,37], a multi-agent simulator for househould activities.In VirtualHome, an executable action step can be simplified as the format of [action] <arg>, e.g., [Grab] <apple> .The world state of VirtualHome consists of objects and their relations (e.g., apple on table).Details about VirtualHome can be found in Appendix  3 Take pillow  A.1.We first describe how to gather embodied experiences in the world model in Section 3.1.Then in Section 3.2 we demonstrate how to finetune LMs by utilizing collected experiences, as well as our proposed method EWC-LoRA for efficient knowledge generalization.</p>
<p>Collecting Embodied Experiences from World Model</p>
<p>LMs pretrained on large scale human-written text corpus often have difficulties in solving basic reasoning and planning in physical environments.This is because that LMs lack necessary embodied knowledge and experiential understanding of the physical world.To address the problem, we propose to leverage world models to collect diverse embodied experiences for enhancing LMs.Specifically, to inject different types of embodied knowledge into LMs, we introduce two ways to gather experiences: goal-oriented planning and random exploration.Figure 2 illustrates the two methods.</p>
<p>Goal-oriented Planning.One important embodied skill is to plan and complete a specific goal, e.g., placing tableware properly to set up the table.To endow LMs with this ability, we propose goal-oriented planning.The approach aims to generate experiences that are goal-oriented, thus are useful to facilitate the acquisition of skills and task planning abilities for executing a range of activities in the world model.To do that, we collect various activities and their corresponding goals.Formally, the goal for an activity in the world model is defined as a set of predicates describing the target world state.For instance, an activity can be set up table, and its goal can be ON(fork, table );ON(plate, table ), which means that fork and plate should be put on the table to fulfill the activity.More details about predicates and goal definitions can be found in Appendix A.2.As shown in Figure 2, in goal-oriented planning, we devise a Monte Carlo Tree Search (MCTS) planner to search through the action space and find a plan, i.e., a sequence of actions, to achieve the goal.</p>
<p>The key to successful MCTS is the reward design.At each time step, if at least one goal predicate is satisfied, the MCTS planner will get a reward of +2, and the achieved goal predicates will be removed from the goal.This ensures that the planner does not repeatedly execute the same action to receive rewards, but rather focuses on achieving the remaining unfulfilled goals.Besides, it will get a -0.1 penalty after each time step to discourage the planner from doing actions irrelevant to fulfilling the goals.Finally, we store the planning process as an embodied experience.</p>
<p>Random Exploration.In real-world scenarios, humans not only acquire new knowledge by finishing tasks, but also learn by just randomly exploring the surroundings, e.g., randomly observing/tracking objects and knowing their properties.To mimic this learning process, we propose another approach, namely random exploration.By simply exploring in the world model, embodied experiences emerge that involve advanced cognitive abilities including object permanence and tracking, as agents observe and track the consistent existence of objects even when they are out of sight.Then the experiences are gathered for finetuning LMs later.Specifically, the approach deploys one or multiple agents in the world model wandering aimlessly and randomly executing actions.As illustrated in Figure 2, multiple agents are in the same environment, interacting with each other or executing different actions on the same objects, which simulates complex situations.During the exploration, the moving paths and the final locations of all the objects in the world model are recorded.Then the whole process is captured as an embodied experience.</p>
<p>Finetuning LMs with Embodied Experiences</p>
<p>There are multiple ways to utilize collected embodied experiences for LMs finetuning, such as supervised learning and reinforcement learning.In this work, we use them with supervised learning for simplicity and efficiency.Specifically, goal-oriented planning experiences are compiled into data examples in two formats: plan generation and activity recognition.As shown in Figure 2, in plan generation, the model is required to generate a stepwise action sequence to fulfill an activity, given the state of some relevant objects as the initial condition.In activity recognition, the model needs to recognize the activity name given its plan.Experiences obtained from random exploration are also transformed into two self-supervision tasks: counting and object path tracking.Examples of the two tasks can be seen in Figure 2. Specifically, for counting, the LM is tasked with identifying the number and name of the objects at a specific location after the agents performed relevant and irrelevant actions and arranged objects randomly.In object path tracking, the model is tasked to output the moving path of an object that is picked up by different agents and moved to different rooms at different times.All the tasks are trained with cross-entropy loss.Suppose that x is the input (e.g., the initial condition in plan generation) and y = {y 1 , ..., y M } is the label (e.g., the stepwise action sequence), we finetune LMs by assigning different weights to different tasks:
L V = v∈V α v M m=1 log P (y m |y &lt;m , x), (1)
where L is the loss function; V is the task set; and α v is the weight for task v. Following Flan-T5 [8],</p>
<p>x is a prompt formatted to contain a task instruction and sampled in-context demonstrations.We provide all prompts in Appendix A.3</p>
<p>Efficient Finetuning with Preserved Generality.However, there are two key problems for simply finetuning LMs.The first one is that LMs will easily overfit to the downstream tasks, leading to performance degradation on other tasks.This deviates from our goal that the model should generalize acquired knowledge across various tasks.Another problem is that finetuning the entire LM is resource-intensive and time-consuming, especially when the LM is extremely large.To overcome the problem and facilitate continual and efficient knowledge acquisition with world models, we propose to finetune only a small number of weights using low-rank adaptors (LoRA) [16] with elastic weight consolidation (EWC) [22], which we refer to as EWC-LoRA.</p>
<p>EWC is a regularization-based method typically used in the area of continual learning [9].It calculates a fisher matrix [11] to estimate the importance of each parameter for a task and then uses it to regularize the training on a new task.The regularization term helps to constrain the parameter updates for the new task to avoid forgetting the previous knowledge.Let U be the pretraining task set, and V be the finetuning task set.Following [2], we have:
F i,i = 1 N N j=1 ∂L (j) U ∂θ * U,i 2 ,(2)L(θ) = L V (θ) + λ i F i,i (θ i − θ * U,i ) 2 , (3)
where L is the loss function, F is the fisher matrix, λ is the hyperparameter, i and j are the indices for parameters and data samples, respectively, and θ and θ * U are currently trained parameters and frozen task U parameters, respectively.Notice that the first term L V (θ) in Equation 3 is calculated in Equation 1, and the second term is the EWC regularizer.In Equation 2, the fisher matrix is calculated by averaging the sum of squares of the gradients from the task U , which indicates the significance of each parameter to the task U .Then the matrix is used in Equation 3 to weigh the shift of model parameters when training on V .By using EWC, the LM learns to adapt to new tasks without catastrophic forgetting on the pretraining task, which forces it to understand and digest new knowledge from the finetuning tasks instead of overfitting to them.However, EWC is both time-and memory-inefficient.First, it requires finetuning the entire set of large LM's parameters.Moreover, the approach involves creating a frozen original model and a fisher matrix that is the same size as the LM, leading to a memory overhead of three times the original size.This makes it particularly challenging to apply to larger models.To alleviate the problem, we propose to combine EWC with low-rank adaptors (LoRA), a parameter-efficient tuning method.LoRA freezes the pretrained model weights and injects two trainable low-rank matrices into each layer of the model.Suppose that W, W * ∈ R r×d are the trained weight matrix and frozen weight matrix, respectively; and B ∈ R r×k , A ∈ R k×d are two low-rank matrices with k ≪ min(r, d).Then the formula for LoRA can be written as W = W * + BA.Suppose that H is flattened BA.Notably, we found that θ i in Equation 3 is the element of W , and θ * U,i is that of W * .Therefore, θ i − θ * U,i is the element of H.We can thus transform Equation 3 into the final formula of EWC-LoRA method:
L(θ) = L V (θ) + λ i F i,i h 2 i ,(4)
where h i = θ i − θ * U,i is the i-th element of H.One of the benefits of this rewriting is that we no longer need to store the trained LM weight matrixes as what vanilla EWC does, which saves plenty of memory space.Besides, we only need to update B and A during the finetuning, which also lowers memory requirements and leads to much faster training speed.Surprisingly, as shown later, we empirically found that adding LoRA into EWC can further mitigate the issue of catastrophic forgetting and overfitting.This aligns with the previous conclusion that limiting the dimension of the optimization problem can alleviate catastrophic forgetting [32].</p>
<p>Experiments</p>
<p>Training Details.For goal-oriented planning, we collected activities and their corresponding target goals with data from RobotHow [36], a housework activity knowledge base created in VirtualHome.We applied our method to GPT-Neo-1.3B[3], GPT-J-6B [49], OPT-13B [59], and LLaMA-13B [48].To save computing resources, we use Int8 technique [10].Both of the models were trained with the AdamW optimizer [30].All the hyperparameters are chosen according to the performance on a held out set.We used one NVIDIA GeForce RTX 3090 for training.More details can be found in Appendix A.4.</p>
<p>Downstream Evaluation Tasks</p>
<p>We developed various downstream evaluation tasks for each type of embodied knowledge, including both the training tasks as well as novel tasks unseen during training used for generalization evaluation.Additionally, we evaluate our models on bAbI [51], a dataset for testing multiple types of knowledge and abilities including embodied knowledge, logic reasoning, linguistic knowledge, etc.We select the bAbI tasks related to embodied knowledge for our evaluation.We evaluate all the unseen tasks including bAbI under few-shot settings, specifically 2-10 shots, by providing a few in-context exemplars in the prompts.We discuss more details of the tasks below.</p>
<p>Plan Generation.To evaluate planning ability, we construct downstream tasks using human-written plans from RobotHow.Specifically, we have three tasks:</p>
<p>• Plan Generation Evaluation.In this task, the model needs to generate a plan for a housework activity.It is similar to the training task but uses human-written plans as the ground truth instead of the collected experiences.We include activities unseen during training to test the generalizability of the model.Inspired by the previous study showing that LMs can easily be distracted by irrelevant context [43], we also created samples having states of unrelated objects in the context (e.g., TV is on for activity Make Coffee) to confuse LMs.In summary, this results in four settings: Vanilla Seen, Vanilla Unseen, Confusing Seen, and Confusing Unseen.We have 175/54/135/43 examples for four settings, respectively.We use Rouge-L [26] as the metric.</p>
<p>• Housework QA.This is a multi-choice QA task, which is unseen during training.It asks which choice is the relevant object to finish a household activity, e.g., which object is relevant to making coffee?It has 261 examples in total, and we use accuracy as the metric.When evaluating, we provide 10 in-context exemplars in the prompts, so this task is evaluated as a 10-shot learning task.</p>
<p>• Negation Housework QA.This is similar to Housework QA but inquires about the irrelevant object, e.g., which object is irrelevant to making coffee?It is more challenging than the vanilla QA because LMs that simply memorize the word co-occurrence in the training data may succeed in the vanilla QA but will fail in the negation QA.This task has 162 examples and uses accuracy as the metric.We provide 10 in-context exemplars in the prompts.</p>
<p>Activity Recognition.We developed two multi-choice QA tasks with the human-written plans and the state changes from RobotHow to test the knowledge gained from activity recognition:</p>
<p>• Activity Recognition QA.In this task, a human-written plan is given and the model needs to choose the correct activity name.An example of the question is "Given a plan: Walk to living room.Sit on sofa.Turn on TV.What is the name of this activity?".And the answer should be Watch TV.The task has 549 examples.We use accuracy as the metric.</p>
<p>• Activity Inference QA.In this task, we use the final state of the world model as input for the model to infer the activity name.For example, the input can be "Tom is sitting on the sofa and facing the TV.The TV is on.What is a possible activity he is doing?", and the answer is "Watch TV".We have 262 examples for this task and use accuracy to measure the performance.We provide 10 in-context exemplars in the prompts.</p>
<p>Counting.We gathered random exploration experiences to construct Counting QA for evaluating skills learned from the counting task.The model is required to answer the number of objects in a specific location.For example, a query can be "Tom put an apple on the table.Tom turned on the TV.Tom put a cup on the table.How many objects are there on the table?".We can see that there will be irrelevant actions like turn on TV to confuse the model and make the question more challenging.We collected 194 samples for the task and used accuracy as the evaluation metric.We provide 5 in-context exemplars in the input prompts.</p>
<p>Object Path Tracking.We developed two downstream tasks for the object path tracking training task, namely Object Path Tracking Evaluation and Object Location QA.</p>
<p>• Object Path Tacking Evaluation.This evaluation task is the same as the training task, where the model is required to generate the full moving path of an object.An example is "Tom walked to the kitchen.Tom grabbed the apple.Mary walked to the bedroom.Tom walked to the living room.What is the order of the rooms where the apple appears?".This question typically includes multiple agents and many irrelevant actions, which makes it difficult to track the object.This task contains 200 examples.Following Huang et al. [18], we evaluate the performance by calculating the length of the longest common subsequence (LCS) between the ground truth and the generated path, normalized by the maximum length of the two.</p>
<p>• Object Location QA.In this task, the model is asked about the location of an object before/after it moves to another location, e.g., where is the apple before/after the kitchen?This task has 200 examples with accuracy as the metric.We provide 2 in-context exemplars in the prompts.</p>
<p>A previous study on prompting multiple QA questions [39] introduces two prompting methods, multiple choice prompt and cloze prompt, and two normalization methods, length, and unconditioned normalization.For all the multi-choice QA tasks, we choose the combination of prompting and normalization methods which yields the best performance on a held out set.</p>
<p>To further verify the effectiveness of our method, we evaluate our finetuned GPT-Neo and GPT-J on the bAbI dataset.Specifically.we select 8 test sets from bAbI that align with the abilities covered in our collected embodied experiences.We include the description of each test set in Appendix A.5.For all the bAbI tasks, we do 2-shot learning by providing 2 in-context exemplars in the input prompts.</p>
<p>Besides downstream tasks, we also want to ensure that our approach does not hurt language modeling performance of the models.Therefore, following previous work [42], we evaluate the perplexity on a subset of Pile [12] test set, which is the pretraining dataset for GPT-Neo and GPT-J.We sampled 5000 examples from Pile test set for evaluation.</p>
<p>Results</p>
<p>Constructed Evaluation Tasks.Results for all the downstream evaluation tasks are shown in Figure 3 and Figure 4.For all the models, we compare the results obtained after finetuning with world model against those of the original base models.For GPT-J, we also include a finetuned model without EWC-LoRA as a baseline.Detailed numbers of the results can be found in Appendix A.6.We also conduct human evaluations for GPT-J on the plan generation task, which can be found in Appendix A.7.In general, the models trained with the world model significantly outperform the baselines on various downstream tasks.Our method is not only effective for small 1.3B model, but can also scale to larger 6B and 13B models.Specifically, our finetuned GPT-J and LLaMA-13B GPT-Neo (Ours) GPT-J (Ours) GPT-Neo (Base) GPT-J (Base) GPT-J (FT) ChatGPT</p>
<p>Figure 3: Experimental results of GPT-Neo and GPT-J on our constructed downstream tasks.GPT-J (FT) refers to the finetuned GPT-J without EWC-LoRA.Our approach surpasses baselines on all of the 11 tasks, and outperforms ChatGPT on 7 of them.For example, our GPT-J model achieves 98.67 LCS on object path tracking, which is significantly better than 33.86 of base GPT-J and 59.53 of ChatGPT.This demonstrates that our model absorbs the knowledge for goal-oriented planning and random exploration instead of memorizing the seen experiences.Specifically, the better plan generation performance under the "Confusing" setting indicates that the world model improves the ability of LMs to avoid being interfered with by irrelevant contexts.On both Housework QA and Negation Housework QA, our models surpass the baselines, showing that our models also acquire knowledge about the necessary objects for completing a housework activity.Results on other downstream tasks also prove the effectiveness of our method.For example, on both Activity Recognition Evaluation and Activity Inference, our approach improves over the baselines significantly.Moreover, improvements can be observed in the downstream tasks regarding random exploration.On Counting and Object Location QA tasks, our LLaMA-13B trained with the world model even surpasses ChatGPT.</p>
<p>bAbI Tasks.To further verify the effectiveness of our method, we evaluate our finetuned models on the bAbI dataset.The results are shown in Figure 5.We can see that our approach significantly outperforms the base LMs.Notably, after finetuned with VirtualHome experiences, GPT-J surpasses the much stronger ChatGPT on the most challenging tasks.Specifically, it outperforms ChatGPT on the Three Supporting Fact task, where the model is required to use three supporting facts from the context to answer a question like "where was the apple before the kitchen?", and Lists/Sets task, which asks the model to give the answers in the form of a list, e.g., the answer for "What is Daniel holding?" is "apple, milk".These results prove that our approach enables LMs to acquire the knowledge and skills inherent in embodied experiences, rather than simply overfitting to the training environment.</p>
<p>Language Modeling.In addition to verifying improved performance on the downstream tasks, we also report results on Pile test subset to ensure the preservation of the generality and language modeling abilities of LMs.From the experimental results shown in Table 1, we can see that our approach only causes a negligible increase in perplexity over the base models.This demonstrates the effectiveness of EWC-LoRA to preserve the generality and linguistic competence of LMs.To verify the generality on other NLP tasks, we also include the results on SuperGLUE [40] in Appendix A.8.</p>
<p>Comparison of Different Regularization Methods</p>
<p>We compare our proposed EWC-LoRA with EWC and LoRA.Besides, we also include the baseline using KL penalty as regularization.The experimental results are shown in Table 2.We also include the results of four evaluation tasks.Notice that we do not include the results of GPT-J with pure EWC and KL, since they are overly memory-intensive or time-consuming.EWC requires an original model and a fisher matrix other than the trained model, which triples the memory usage, making  [32].EWC-LoRA further decreases perplexity, making it extremely close to the original perplexity, while achieving comparable performance with LoRA on downstream tasks.This demonstrates the effectiveness of EWC-LoRA.Besides, We can find that combing LoRA with KL will greatly increase perplexity while not achieving better downstream performance.Overall, our proposed EWC-LoRA achieves the best trade-off between the perplexity and the downstream performance, which outperforms baselines significantly while almost not increasing the perplexity on the pretraining dataset.</p>
<p>Ablation Studies</p>
<p>To study the contribution of each training task, we conducted an ablation study by removing one training task every time.We use GPT-Neo-1.3Bas the base model.We include the results on tasks seen during training in Table 3. Results on all the tasks can be found in Appendix A. We can see that the removal of a training task with similar ability leads to a notable decrease in the model's performance on downstream tasks.For example, the performance of plan generation drops significantly when plan generation is removed from the training tasks.Similarly, the removal of activity recognition or object path tracking from the training tasks leads to a decline in performance in their respective downstream tasks.We conclude that our gathered embodied experience has a tremendous contribution to teaching the corresponding reasoning ability by finetuning.Interestingly, Counting QA performance shows an increase when counting is omitted from the training tasks, possibly because the ability of counting can be inferred from other training tasks.</p>
<p>Conclusion &amp; Future Work</p>
<p>We proposed a new training framework that uses world models to enhance language models.It first collects embodied experiences from world models through both goal-oriented planning and random exploration.The experiences are then compiled into appropriate formats for LMs finetuning.We further introduce EWC-LoRA, which not only facilitates parameter-efficient tuning but also alleviates catastrophic forgetting and enables knowledge generalization.We show the strong performance of our method on a large number of downstream evaluation tasks.</p>
<p>This work demonstrates the advantage of panoramic learning with all forms of experience [17].On the other hand, the present work is limited to a single household environment as the world model.In the future, we intend to study how to integrate embodied experiences from different world models and generalize knowledge learned from each world model to different domains.</p>
<p>A.</p>
<p>Figure 1 :
1
Figure 1: Examples of tasks requiring embodied knowledge (upper), and an overview of our approach (bottom).In the task examples, blue text indicates the useful information for answering the question.</p>
<p>to watch TV? TV and sofa is in living room… Answer: Walk to living room.Sit on sofa.Turn on TV.Question: Given a plan: Walk to living room.Sit on sofa.Turn on TV.What is the task?Answer: Watch TV.</p>
<p>Figure 2 :
2
Figure 2: The illustration of goal-oriented planning (left) and random exploration (right) in our training paradigm.In MCTS, the path in orange represents the final plan generated by the planner.</p>
<p>Figure 4 :
4
Figure4: Experimental results of OPT-13B and LLaMA-13B on our constructed downstream tasks.Our approach applied on LLaMA-13B outperforms ChatGPT on 8 of them.</p>
<p>GPT-Neo (Ours)GPT-J (Ours) GPT-Neo (Base) GPT-J (Base) ChatGPT</p>
<p>Figure 5 :
5
Figure5: Experimental results on bAbI.Our approach outperforms base LMs on all the tasks except for the Two Supporting Fact task.</p>
<p>They are pillow and apple.
1 Grab pillow2 Give pillow to4 Grab apple5 Walk to living room6 Put apple on table7Walk to bathroom8Walk to bedroom9Put pillow on tableQuestion:Tom grabbed pillow. Tomgave pillow to … How manyobjects are on the table?Answer:Two. Plan Generation Activity RecognitionCountingObject Path Tracking
Question: Tom grabbed pillow.Tom walked to kitchen … What is the order of rooms where pillow appears?Answer: Bedroom, kitchen, living room Goal-Oriented Planning Random Exploration</p>
<p>Walk to bathroom Grab toothpaste Walk to bedroom Walk to living room Sit on Sofa Walk to table Grab apple Turn on TV Walk to living room MCTS
-0.1-0.1+1.9-0.1+1.9+1.9-0.1-0.1+1.9</p>
<p>Table 1 :
1
Perplexity on Pile test subset, showing the proposed finetuning with world model manages to preserve the LMs' language modeling capability.
GPT-NeoGPT-JOPT-13BLLaMA-13BBaseOursBaseOursBaseOursBaseOurs4.120 4.193 3.443 3.537 4.077 4.358 3.036 3.069TaskBaseGPT-Neo EWC LoRA LoRA &amp; KL EWC-LoRABaseGPT-J LoRA EWC-LoRAPlan Gen21.25 48.56 51.2445.9949.7034.31 51.2351.23Act Recog 69.22 89.98 87.9881.4285.4387.98 90.1688.52Count22.68 55.67 27.8449.4828.8730.41 63.9267.01Obj PT30.80 95.96 87.2863.5985.9133.86 97.2298.67Perplexity 4.120  *  4.995 4.3605.0294.1933.443  *  3.6753.537</p>
<p>Table 2 :
2
Results of different regularization methods.The abbreviations in the Task column stand for the corresponding evaluation tasks for four training tasks.We use asterisk * to mark the perplexity of base models.with world model even achieve better performance than ChatGPT as a much larger LM on most of the 11 tasks.Besides, we can see the world model improves LMs on both seen and unseen tasks.</p>
<p>Table 3 :
3
Ablation experimental results on training tasks.We use the same abbreviations as Table2.ithard to be applied to large models like GPT-J-6B.Besides, KL penalty term is computed byL KL = E (x,y)∼P θ * [− log (P θ * (y|x)/P θ (y|x))],thus it requires sampling from the model output probability, which is time-consuming.On the contrary, EWC-LoRA is both memory-and timeefficient.In Table2, we can see that EWC-LoRA achieves the lowest perplexity compared to other methods, while still significantly outperforming the base LMs.Compared with pure EWC, applying pure LoRA greatly decreases perplexity, which is consistent with the previous conclusion that limiting the dimension of the optimization problem can mitigate catastrophic forgetting
GPT-NeoBaseOurs -w/o Plan Gen -w/o Act Recog -w/o Count -w/o Obj PTPlan Gen21.25 49.7014.4849.3849.8550.06Act Recog 69.22 85.4385.9748.6385.2584.34Count22.68 28.8718.5625.2635.0532.99Obj PT30.80 85.9192.1384.1786.4629.90Perplexity 4.120  *  4.1934.1714.1514.1624.164</p>
<p>3.4Activity Recognition Tom was at home.He grabbed an apple and put it on the bookshelf.He then walked to the kitchen and srcub a plate.He went back to bookshelf and put the plate on it.Given a sequence of actions in a house, and a question about what items are located in a specific place.Answer the number of items and list the items.Q: {{ movement }} How many items are there on the {{ location }}? A: Ther are {{ number }} itmes on the {{ location }}.They are {{ items }} Tom was at home.He grabbed an apple and put it on the bookshelf.He then walked to the kitchen and srcub a plate.He went back to bookshelf and put the plate on it.Tom went to the kitchen.Mary walked into the dining room.Tom grabbed a plate.Tom travelled to the living room.Mary moved to the living room.Tom put the plate on the table.Mary grabbed the plate.Mary journeyed to the bedroom.Tom went to the kitchen.Mary walked into the dining room.Tom grabbed a plate.Tom travelled to the living room.Mary moved to the living room.Tom put the plate on the table.Mary grabbed the plate.Mary journeyed to the bedroom.
A.3.7 Counting QAData Example Data ExampleKey KeyValue Valueplan choices activity movement locationWalk to living room. Sit on sofa. Watch TV. [watch TV, make coffee, sleep, brush teeth] watch TV bookshelfnumber2In-context ExemplarIn-context ExemplarGiven a task plan: {{ plan }} Q: {{ movement }} How many items are there on the {{ location }}? Question: what is the name of this task? Answer: {{ answer }} A: {{ number }}A.3.5 Activity Inference A.3.8 Object Path TrackingData Example Data ExampleKeyValueKey state choices activity movement objectValue Tom is sitting on the sofa. Tom is facing the TV. [watch TV, make coffee, sleep, brush teeth] watch TV platepathkitchen, living room, bedroomIn-context ExemplarIn-context Exemplar{{ state }}Question: given the above state, a possible activity could be {{ movement }} Answer: {{ answer }} Question: What is the order of the rooms where the {{ object }} appeared?Answer: {{ path }}A.3.6 CountingData Example A.3.9 Object Location QAData ExampleKeyValueKey movement location movementbookshelf Valuenumber2items objectapple, plate platereference_room living roomprepositionbeforeanswerkitchenIn-context ExemplarIn-context Exemplar
{{ movement }} Question: Where is the {{ object }} {{ preposition }} the {{ reference_room }}? Answer: {{ answer }}</p>
<p>Acknowledgements.This project is partially supported by DARPA ECOLE HR00112390063.A AppendixA.1 VirtualHomeThe complete format of an executable action step in VirtualHome is <char{char_id}> [Action] <Object> (Object_id).Specifically, char_id specifies which agent to execute the action when multiple agents are in the world model at the same time.Action should be a supported atomic action in VirtualHome.Object is the object with which the agent interacts.Each object in the environment is assigned an Object_id to distinguish it from others of the same object class.We designed a template for each action to transform them into natural text for LMs finetuing.The full list of executable actions can be found in Table4.Note that in the list, we omit <char{char_id}> and (Object_id) for simplicity.A.2 Acitivity Goal And PredicateThe goal of an household activity in VirtualHome consists of several predicates.Each predicate represents a condition of one object or a relation between two objects.For example, OPEN(coffe maker) means the coffee maker is open, and ON(apple, table) means an apple is on the table.The goal is only achieved when all the predicates are achieved.We collected activities and goals from RobotHow.A.3 Data Format and PromptsFollowing Chung et al.[8], we use instructions with in-context exemplars as prompts.Specifically, the instruction, the question context, and the answer will be provided in each exemplar, and the full prompt will contain multiple such exemplars for in-context learning.The format of the data and the exemplar for each task is provided below.A.In-context ExemplarQuestion: To {{ activity }}, an unrelated item could be Answer: {{ answer }}A.4 HyperparametersFor both GPT-Neo-1.3Band GPT-J-6B, we use a learning rate of 8 × 10 −5 and a batch size of 20.The weights for plan generation, activity recognition, counting, and object path tracking are 1.0, 0.7, 1.0, and 1.0, respectively.We trained GPT-Neo-1.3Bfor 3 epochs with the EWC coefficient λ = 0.5 in Equation4. For GPT-J-6B, we trained it for 5 epochs with λ = 2.With our approach, it takes 40 minutes to train a GPT-Neo and 220 minutes to train a GPT-J.We used a rank of 8 and coefficient of 32 for LoRA's hyperparameters.A.5 bAbI DatasetWe include 8 tasks from bAbI that test embodied knowledge.They are: One Supporting Fact, Two Supporting Fact, Three Supporting Fact, Counting, Lists/Sets, Simple Negation, Time Reasoning, Positional Reasoning.Examples for each task are shown in Table5.Task 1: Single Supporting Fact Task 2: Two Supporting Facts Mary went to the bathroom.John is in the playground.John moved to the hallway.John picked up the football.Mary travelled to the office.Bob went to the kitchen.Where is Mary? A:officeWhere is the football?A:playground Task 3: Three Supporting Facts Task 4: Counting John picked up the apple.Daniel picked up the football.John went to the office.Daniel dropped the football.John went to the kitchen.Daniel got the milk.John dropped the apple.Daniel took the apple.A: office Where was the apple before the kitchen?A:office How many objects is Daniel holding?A: two Task 5: Lists/Sets Task 6: Simple Negation Daniel picks up the football.Sandra travelled to the office.Daniel drops the newspaper.Fred is no longer in the office.Daniel picks up the milk.Is Fred in the office?A:no What is Daniel holding? milk, football Is Sandra in the office?A:yes Task 7: Time Reasoning Task 8: Positional Reasoning In the afternoon Julie went to the park.The triangle is to the right of the blue square.Yesterday Julie was at school.The red square is on top of the blue square.Julie went to the cinema this evening.The red sphere is to the right of the blue square.Where did Julie go after the park?A:cinema Is the red sphere to the right of the blue square?A:yes Where was Julie before the park?A:school Is the red square to the left of the triangle?A:yes Table5: Examples for bAbI tasks.A.6 Results of Main Experiments and Ablation StudiesExperimental results on our constructed downstream tasks are shown in Table6, and the results on bAbI are shown in Table7.We also show the results of ablation studies in Table8.A.7 Human EvaluationsWe conduct human evaluations on plan generation for GPT-J model.Following Huang et al.[18]we asked 3 people to annotate whether each task can be completed using a generated plan.We randomly sampled 150 tasks and asked each person to annotate 50 of them.The Results show that the base GPT-J model can only achieve 24.0% accuracy, while the finetuned model can achieve 62.4%.The higher planning accuracy demonstrates the superior task planning ability of our model.A.8 SuperGLUE ResultsWe evaluate the base GPT-J model and our finetuned model on appropriate SuperGLUE tasks, e.g., that can be formulated as a multi-choice QA task without prompt engineering.Our model's performance matches and even outperforms the baseline, showing our model retains the general language capability.A.9 Broader ImpactLike other generation systems, the language model trained by our approach is susceptible to producing unintended output when confronted with harmful input, such as unethical text or input intended for adversarial attacks.Therefore, we strongly advise against utilizing our approach outside of controlled research environments until these risks have been mitigated.It is important to note that a thoughtless deployment of our method could potentially enable malicious exploitation of the underlying language models.Thus, precautions, such as implementing a filtering mechanism, must be taken.
Anthony Michael Ahn, Noah Brohan, Yevgen Brown, Omar Chebotar, Byron Cortes, Chelsea David, Keerthana Finn, Karol Gopalakrishnan, Alex Hausman, Herzog, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</p>
<p>An empirical investigation towards efficient multi-domain language model pre-training. Kristjan Arumae, Qing Sun, Parminder Bhatia, arXiv:2010.007842020arXiv preprint</p>
<p>GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow. Sid Black, Leo Gao, Phil Wang, Connor Leahy, Stella Biderman, March 2021</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>A survey of monte carlo tree search methods. Edward Cameron B Browne, Daniel Powley, Simon M Whitehouse, Peter I Lucas, Philipp Cowling, Stephen Rohlfshagen, Diego Tavener, Spyridon Perez, Simon Samothrakis, Colton, IEEE Transactions on Computational Intelligence and AI in games. 412012</p>
<p>Thomas Carta, Clément Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, Pierre-Yves Oudeyer, arXiv:2302.02662Grounding large language models in interactive environments with online reinforcement learning. 2023arXiv preprint</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Scaling instruction-finetuned language models. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, arXiv:2210.114162022arXiv preprint</p>
<p>A continual learning survey: Defying forgetting in classification tasks. Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Aleš Leonardis, Gregory Slabaugh, Tinne Tuytelaars, IEEE transactions on pattern analysis and machine intelligence. 4472021</p>
<p>Tim Dettmers, Mike Lewis, Younes Belkada, Luke Zettlemoyer, arXiv:2208.073398-bit matrix multiplication for transformers at scale. 2022arXiv preprintint8(</p>
<p>Science from Fisher information. Frieden Roy, 2004Citeseer974</p>
<p>Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, Connor Leahy, arXiv:2101.00027The Pile: An 800gb dataset of diverse text for language modeling. 2020arXiv preprint</p>
<p>Vrkitchen: an interactive 3d virtual environment for task-oriented learning. Xiaofeng Gao, Ran Gong, Tianmin Shu, Xu Xie, Shu Wang, Song-Chun Zhu, arXiv, abs/1903.057572019</p>
<p>Minerl: a large-scale dataset of minecraft demonstrations. Brandon William H Guss, Nicholay Houghton, Phillip Topin, Cayden Wang, Manuela Codel, Ruslan Veloso, Salakhutdinov, Proceedings of the 28th International Joint Conference on Artificial Intelligence. the 28th International Joint Conference on Artificial Intelligence2019</p>
<p>. Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu, Reasoning with Language Model is Planning with World Model. NeurIPS. 2023</p>
<p>J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021arXiv preprint</p>
<p>Toward a 'Standard Model' of Machine Learning. Zhiting Hu, Eric P Xing, Harvard Data Science Review. 44oct 27 2022</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, International Conference on Machine Learning. PMLR2022</p>
<p>Inner monologue: Embodied reasoning through planning with language models. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Tomas Jackson, Noah Brown, Linda Luu, Sergey Levine, Karol Hausman, Brian Ichter, Proceedings of The 6th Conference on Robot Learning. Karen Liu, Dana Kulic, Jeff Ichnowski, The 6th Conference on Robot LearningPMLRDec 2023205</p>
<p>Minerl diamond 2021 competition: Overview, results, and lessons learned. NeurIPS 2021 Competitions and Demonstrations Track. Anssi Kanervisto, Stephanie Milani, Karolis Ramanauskas, Nicholay Topin, Zichuan Lin, Junyou Li, Jianing Shi, Deheng Ye, Qiang Fu, Wei Yang, 2022</p>
<p>Housekeep: Tidying virtual households using commonsense reasoning. Yash Kant, Arun Ramachandran, Sriram Yenamandra, Igor Gilitschenski, Computer Vision -ECCV 2022: 17th European Conference. Tel Aviv, Israel; Berlin, HeidelbergSpringer-VerlagOctober 23-27, 2022. 2022Proceedings, Part XXXIX</p>
<p>Overcoming catastrophic forgetting in neural networks. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Proceedings of the national academy of sciences. the national academy of sciences2017114</p>
<p>Ai2-thor: An interactive 3d environment for visual ai. Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli Vanderbilt, Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu, arXiv:1712.054742017arXiv preprint</p>
<p>Liunian Harold, Li , Mark Yatskar, Cho-Jui Da Yin, Kai-Wei Hsieh, Chang, arXiv:1908.03557Visualbert: A simple and performant baseline for vision and language. 2019arXiv preprint</p>
<p>Pre-trained language models for interactive decision-making. Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang, Ekin Akyürek, Anima Anandkumar, Advances in Neural Information Processing Systems. 202235</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. 2004</p>
<p>Juewu-mc: Playing minecraft with sample-efficient hierarchical reinforcement learning. Zichuan Lin, Junyou Li, Jianing Shi, Deheng Ye, Qiang Fu, Wei Yang, arXiv:2112.049072021arXiv preprint</p>
<p>Rainier: Reinforced knowledge introspector for commonsense question answering. Jiacheng Liu, Skyler Hallinan, Ximing Lu, Pengfei He, Sean Welleck, Hannaneh Hajishirzi, Yejin Choi, arXiv:2210.030782022arXiv preprint</p>
<p>Mind's eye: Grounded language model reasoning through simulation. Ruibo Liu, Jason Wei, Shixiang Shane Gu, Te-Yen Wu, Soroush Vosoughi, Claire Cui, Denny Zhou, Andrew M Dai, arXiv:2210.053592022arXiv preprint</p>
<p>Ilya Loshchilov, Frank Hutter, arXiv:1711.05101Decoupled weight decay regularization. 2017arXiv preprint</p>
<p>Quark: Controllable text generation with reinforced unlearning. Ximing Lu, Sean Welleck, Jack Hessel, Liwei Jiang, Lianhui Qin, Peter West, Prithviraj Ammanabrolu, Yejin Choi, Advances in neural information processing systems. 352022</p>
<p>Catastrophic interference in connectionist networks: The sequential learning problem. Michael Mccloskey, Neal J Cohen, Psychology of learning and motivation. Elsevier198924</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, arXiv:2112.09332Browser-assisted question-answering with human feedback. 2021arXiv preprint</p>
<p>. ArXiv, abs/2303.08774OpenAI. Gpt-4 technical report. 2023</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Virtualhome: Simulating household activities via programs. Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, Antonio Torralba, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2018</p>
<p>Watch-and-help: A challenge for social perception and human-{ai} collaboration. Xavier Puig, Tianmin Shu, Shuang Li, Zilin Wang, Yuan-Hong Liao, Joshua B Tenenbaum, Sanja Fidler, Antonio Torralba, International Conference on Learning Representations. 2021</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. PMLR2021</p>
<p>Leveraging large language models for multiple choice question answering. Joshua Robinson, Christopher Michael Rytting, David Wingate, arXiv:2210.123532022arXiv preprint</p>
<p>Superglue: Learning feature matching with graph neural networks. Paul-Edouard Sarlin, Daniel Detone, Tomasz Malisiewicz, Andrew Rabinovich, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020</p>
<p>Manolis Savva, X Angel, Alexey Chang, Thomas Dosovitskiy, Vladlen Funkhouser, Koltun, Minos, arXiv:1712.03931Multimodal indoor simulator for navigation in complex environments. 2017arXiv preprint</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, arXiv:2302.047612023arXiv preprint</p>
<p>Large language models can be easily distracted by irrelevant context. Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Chi, Nathanael Schärli, Denny Zhou, arXiv:2302.000932023arXiv preprint</p>
<p>Mastering the game of go with deep neural networks and tree search. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den, Julian Driessche, Ioannis Schrittwieser, Veda Antonoglou, Marc Panneershelvam, Lanctot, nature. 52975872016</p>
<p>Progprompt: Generating situated robot task plans using large language models. Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, Animesh Garg, Workshop on Language and Robotics at CoRL 2022. 2022</p>
<p>Learning to summarize with human feedback. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul F Christiano, Advances in Neural Information Processing Systems. 202033</p>
<p>Embodied bert: A transformer model for embodied, language-guided visual task completion. Alessandro Suglia, Qiaozi Gao, Jesse Thomason, Govind Thattai, Gaurav Sukhatme, arXiv:2108.049272021arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Ben Wang, Aran Komatsuzaki, Gpt-J-6b, A 6 Billion Parameter Autoregressive Language Model. May 2021</p>
<p>Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, Yitao Liang, arXiv:2302.015602023arXiv preprint</p>
<p>Towards ai-complete question answering: A set of prerequisite toy tasks. Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart Van Merriënboer, Armand Joulin, Tomas Mikolov, ICLR 20164th International Conference on Learning Representations. 2016</p>
<p>Jeff Wu, Long Ouyang, M Daniel, Nisan Ziegler, Ryan Stiennon, Jan Lowe, Paul Leike, Christiano, arXiv:2109.10862Recursively summarizing books with human feedback. 2021arXiv preprint</p>
<p>Building generalizable agents with a realistic and rich 3d environment. Yi Wu, Yuxin Wu, Georgia Gkioxari, Yuandong Tian, arXiv:1801.022092018arXiv preprint</p>
<p>ASDOT: Any-shot data-to-text generation with pretrained language models. Jiannan Xiang, Zhengzhong Liu, Yucheng Zhou, Eric Xing, Zhiting Hu, Findings of the Association for Computational Linguistics: EMNLP 2022. Abu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDecember 2022</p>
<p>Learning to stop: A simple yet effective approach to urban vision-language navigation. Jiannan Xiang, Xin Wang, William Yang, Wang , Findings of the Association for Computational Linguistics: EMNLP 2020. 2020</p>
<p>Chalet: Cornell house agent learning environment. Claudia Yan, Dipendra Misra, Andrew Bennnett, Aaron Walsman, Yonatan Bisk, Yoav Artzi, arXiv:1801.073572018arXiv preprint</p>
<p>ReAct: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, International Conference on Learning Representations (ICLR). 2023</p>
<p>PIGLeT: Language grounding through neuro-symbolic interaction in a 3d world. Rowan Zellers, Ari Holtzman, Matthew E Peters, Roozbeh Mottaghi, Aniruddha Kembhavi, Ali Farhadi, Yejin Choi, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing20211</p>
<p>Opt: Open pre-trained transformer language models. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, arXiv:2205.010682022arXiv preprint</p>
<p>Nisan Daniel M Ziegler, Jeffrey Stiennon, Tom B Wu, Alec Brown, Dario Radford, Paul Amodei, Geoffrey Christiano, Irving, arXiv:1909.08593Fine-tuning language models from human preferences. 2019arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
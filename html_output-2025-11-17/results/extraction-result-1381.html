<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1381 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1381</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1381</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-bb430ec2f25e4a1513073a2a4098cbb942c2e3e0</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/bb430ec2f25e4a1513073a2a4098cbb942c2e3e0" target="_blank">Recurrent Environment Simulators</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This work addresses the issue of computationally inefficiency with a model that does not need to generate a high-dimensional image at each time-step and can be used to improve exploration and is adaptable to many diverse environments.</p>
                <p><strong>Paper Abstract:</strong> Models that can simulate how environments change in response to actions can be used by agents to plan and act efficiently. We improve on previous environment simulators from high-dimensional pixel observations by introducing recurrent neural networks that are able to make temporally and spatially coherent predictions for hundreds of time-steps into the future. We present an in-depth analysis of the factors affecting performance, providing the most extensive attempt to advance the understanding of the properties of these models. We address the issue of computationally inefficiency with a model that does not need to generate a high-dimensional image at each time-step. We show that our approach can be used to improve exploration and is adaptable to many diverse environments, namely 10 Atari games, a 3D car racing environment, and complex 3D mazes.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1381.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1381.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Oh2015 baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Action-conditional video prediction (Oh et al., 2015) recurrent simulator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior action-conditional recurrent video-prediction model used as the baseline: an RNN that encodes frames with convolutions, uses a state transition that depends on encoded frames, and decodes predicted frames with deconvolutions where actions influence decoding via multiplicative interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Action-conditional video prediction using deep networks in Atari games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Oh et al. (2015) action-conditional recurrent simulator</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Recurrent neural-network video predictor: frames are encoded with a convolutional encoder C, state s_t = f(s_{t-1}, C(I(xhat_{t-1}, x_{t-1}))) (i.e. state does not directly take action), and predicted frames are produced by decoding D(s_t, a_{t-1}) where the action enters via multiplicative interaction with the state before deconvolution; trained by minimising MSE between predicted and observed frames.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>neural simulator (recurrent conv-deconv RNN)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari 2600 games (video prediction), used as a baseline for other visual environment simulation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Mean squared error (MSE) on next-frame / multi-step prediction averaged over many sequences (paper reports a specific prediction error defined as (1/(3*10,000)) sum ||x - x_hat||^2 and plots MSE vs time-step), plus qualitative human-play and visual inspection of generated frames</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Reported via curves in paper: baseline achieves good short-term accuracy in some games but worse long-term coherence compared to the proposed architectures; no single global scalar MSE is provided in text for all games. Baseline performance is the reference for the authors' improvements (see plotted prediction-error curves).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Black-box neural representation: internal state s_t is a learned vector; interpretability is limited to visualizing predicted frames and inspecting failure modes (e.g., missing objects or blurriness).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of predicted frames and videos; qualitative human interactive tests; analysis of prediction error vs time-step. No explicit latent-variable or object-level interpretability methods reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Decoding and encoding high-dimensional images at every timestep (decode from state to 210x160x3 and re-encode each step) — high FLOP cost per timestep; model sizes and exact FLOPs depend on encoder/decoder but are materially larger than the prediction-independent variant due to image projection at every step.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Less efficient than the paper's prediction-independent simulator because it must project to/from high-dimensional image space at every step; the paper's PI variant claims ~200 million FLOP savings per timestep by avoiding per-step image decoding/encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Serves as baseline: can produce multi-step predictions but often fails at long-term coherence for many Atari games; the authors build on and improve this baseline with action-conditioned transitions and training-scheme changes.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Useful for short-horizon rollouts under the training-policy distribution but tends to degrade long-horizon; observation-dependent transitions can help short-term accuracy but hurt long-term generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Baseline trades per-step visual fidelity for action incorporation — actions only influence decoding, which can limit how well the model captures dynamics; scheduled-sampling-like observation-dependent transitions improve short-term but hurt long-term.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Convolutional encoder/decoder pair, multiplicative action influence in decoder, prediction-dependent vs observation-dependent transitions considered during training (scheduled phases), truncated BPTT during training.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Used as the main prior comparison; authors find that direct action-conditioning of the state transition and alternative training schemes can outperform this baseline on long-term prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests that training close to usage (more prediction-dependent transitions) improves long-term accuracy compared to heavy use of observation-dependent transitions used in the Oh et al. training schedule.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Recurrent Environment Simulators', 'publication_date_yy_mm': '2017-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1381.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1381.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PD-sim</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prediction-dependent recurrent environment simulator (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LSTM-based, action-conditioned recurrent world model that conditions state updates on the previous predicted frame or observed frame (prediction-dependent transitions) and on the previous action, with convolutional encoder and deconvolutional decoder to produce pixel-space predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Prediction-dependent recurrent environment simulator</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Action-conditioned LSTM whose state update f(s_{t-1}, a_{t-1}, C(I(xhat_{t-1}, x_{t-1}))) combines an encoded frame embedding with action information (action fused multiplicatively into gate computations). A convolutional encoder C maps frames to a vector (2816 in Atari experiments), LSTM state (h_t,c_t) has dimension 1024, and a decoder D(h_t) (possibly combined with action in some variants) produces pixel-frame predictions via deconvolutional layers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>neural simulator (action-conditioned recurrent latent model / video-prediction RNN)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>High-dimensional visual environment prediction / simulation for Atari 2600 games, TORCS (3D car racing), and first-person 3D mazes; used for model-based exploration and interactive human play.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Mean squared error (MSE) between predicted and observed frames averaged over sequences (prediction error averaged over 10,000 sequences in experiments); also qualitative metrics: visual coherence over 100–200+ time-steps and success in human interactive play.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Qualitatively: shows substantial long-term coherence in many Atari games and excellent multi-hundred-step accuracy in TORCS; quantitative MSE curves are presented per-game (no single scalar summary reported). Empirically: long-term prediction error generally decreases as the percentage of consecutive prediction-dependent transitions increases; accurate predictions up to 100–200 timesteps in several Atari games and several hundred timesteps in TORCS/3D mazes.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Predictions are inspected visually (videos) to assess object dynamics and coherence; internal latent states are not claimed to be directly interpretable (black-box vector states). Some qualitative interpretation comes from seeing which objects are preserved or vanish in generated frames.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of generated frames and video rollouts; human interactive tests; analysis of failure modes across games to infer what dynamics are captured (e.g., ball/paddle dynamics, background generation). No explicit disentanglement or latent-variable interpretability techniques used.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>State dimension 1024, encoder output 2816 in Atari setup; decoding to 210x160x3 images each timestep (deconvolutions) is computationally heavy. Training uses truncated BPTT (backprop over subsequences up to length 20), mini-batch size 16, RMSProp (learning rate 1e-5). Training datasets: ~5M frames for training and 1M for test in Atari experiments. Exact parameter counts are not provided, but per-step decoding incurs large FLOPs (order of hundreds of millions for full conv-deconv passes).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Compared to prediction-independent variant, PD model is less efficient because it decodes/encodes images each step. Compared to Oh et al. baseline, PD with direct action conditioning on state and adjusted training schemes yields better long-term accuracy in many domains.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Enables improved long-term visual prediction in many Atari games (outperforming prior work in several metrics/visual coherence), supports human-interactive play for hundreds of frames in Pong, Breakout and TORCS, and improves model-based exploration in 3D mazes (see model-based exploration result).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High long-term prediction accuracy improves utility for planning-like tasks (Model-based exploration, interactive simulation) but can reduce short-term sharpness; improvements in predictive fidelity often translate to better exploration (the paper's explorer covers ~50% more maze area after 900 steps than random), though overfitting to training-policy can reduce robustness to novel action distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Increasing consecutive prediction-dependent transitions increases long-term accuracy but reduces short-term sharpness and sometimes convergence speed; higher prediction horizon T and more PDT improves global dynamics but can degrade per-frame visual detail. Models optimized for long-term MSE can become fragile out-of-distribution (less robust to human-play styles).</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Key choices: action fused multiplicatively into LSTM gates (global direct action influence), high-dimensional LSTM state (1024), convolutional encoder/decoder (encoder output 2816), varied training schemes controlling percentage and consecutiveness of prediction-dependent transitions, truncated BPTT across subsequences to increase effective T, warm-up tau typically 10.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared experimentally to (a) Oh et al. baseline (the authors' PD variants with direct action-conditioning generally outperform in long-term accuracy), (b) mixing vs pure PDT training schedules (pure PDT gives best long-term accuracy but may hurt short-term), and (c) prediction-independent variants (PD gives more stable long-term predictions in many games though PI can be more efficient).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends training schemes that increase consecutive prediction-dependent transitions to encourage learning global dynamics for better long-term accuracy; pure 100% PDT often yields best long-term MSE but mixed schemes are preferable for very complex games where short-term detail matters; choose T and consecutive PDT fraction according to trade-off between long-term coherence and short-term sharpness; use truncated BPTT subsequences to increase effective T.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Recurrent Environment Simulators', 'publication_date_yy_mm': '2017-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1381.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1381.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PI-sim</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prediction-independent recurrent environment simulator (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant world model whose state transition is independent of the predicted frame (s_t = f(s_{t-1}, a_{t-1})), allowing the simulator to evolve state without decoding to pixel space at each step and thereby saving substantial computation during rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Prediction-independent recurrent environment simulator</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Action-conditioned state-transition model where s_t = f(s_{t-1}, a_{t-1}) and the encoding z_t used during warm-up is set to h_t (the latent state), enabling the model to run multiple action-steps by evolving latent state without invoking the expensive encoder/decoder per timestep; only when visual output is needed is decoder invoked. Underlying transition implemented with action-conditioned LSTM-like updates and separate warm-up/prediction parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>neural simulator (action-conditioned latent-only recurrent model / efficient rollout model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari games and other high-dimensional visual environments where agents need multi-step rollouts of action sequences (e.g., for planning or exploration), also tested on Atari and 3D environments.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Same MSE prediction-error measures as PD model when decoded frames are produced for evaluation; fidelity assessed as long-term prediction MSE when using truncated BPTT and subsequence training, plus visual inspection of decoded long-rollout frames.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Qualitatively more sensitive to state-transition and training-scheme choices; for Atari, PI-sim required different hyperparameters (e.g., T=30 via truncated BPTT) to approach PD performance; in some difficult games PI outperformed PD, but overall PI tended to give worse long-term visual quality (e.g., fish of wrong size in Fishing Derby) unless carefully tuned. No single global MSE scalar is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latent-only rollout increases opacity (latent states are not interpreted), and interpretability relies on visualizing decoded frames when the decoder is applied; no additional interpretability mechanisms were introduced.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visual inspection of decoded frames after latent rollouts; analysis of sensitivity to training schemes and state-transition architectures. No explicit latent probing or disentanglement.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Significant per-step savings by avoiding per-step conv-deconv operations; authors estimate ~200 million FLOP saved per timestep in Atari by not decoding/encoding images each step. Training still requires learning encoder/decoder during warm-up and occasional decoding, and truncated BPTT/backprop to some warm-up steps was used (they backprop to time-step five in PI models).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>More computationally efficient at inference/rollout time than prediction-dependent simulators because it avoids per-step high-dimensional image projections; claimed ~200M FLOP per timestep reduction in Atari experiments. Efficiency enables faster multi-step rollouts for planning/exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Enabled faster model-based exploration (used for rollouts in 3D mazes) and could match or exceed PD in specific difficult games when hyperparameters adjusted; however, it was more sensitive to architecture and training choices and often required more careful tuning to reach PD accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>PI-sim is especially useful when the agent wants to simulate long action sequences without producing intermediate images (e.g., for planning), trading some predictive fidelity/robustness for big computational savings; for downstream tasks that require per-step visual accuracy PI-sim may be inferior unless carefully tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>PI-sim gains large computational efficiency at the cost of increased sensitivity to state-transition parametrization and training scheme; in many games PI requires longer prediction lengths and truncated BPTT strategies (e.g., T=30 split into subsequences) to approach PD fidelity. Some long-term visual artifacts (e.g., object-size errors) observed without careful tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Remove dependence of s_t on encoded predicted frame; use separate parameters for warm-up and prediction phases; backpropagate to a subset of warm-up timesteps during training; use truncated BPTT across subsequences to increase effective T while controlling memory.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared against PD variant: PI provides large inference-time FLOP reductions and faster rollouts, but typically needs different training hyperparameters and can be less robust; compared to baseline Oh et al. it can be far more efficient and sometimes equally accurate when tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests PI benefits from longer effective prediction lengths via truncated BPTT (e.g., T=30 via subsequences) and careful backprop into warm-up steps; hyperparameter tuning and choice of state-transition architecture are more critical for PI than PD.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Recurrent Environment Simulators', 'publication_date_yy_mm': '2017-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Action-conditional video prediction using deep networks in Atari games <em>(Rating: 2)</em></li>
                <li>Unsupervised learning of video representations using LSTMs <em>(Rating: 2)</em></li>
                <li>Embed to control: A locally linear latent dynamics model for control from raw images <em>(Rating: 1)</em></li>
                <li>From pixels to torques: Policy learning with deep dynamical models <em>(Rating: 1)</em></li>
                <li>Model regularization for stable sample rollouts <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1381",
    "paper_id": "paper-bb430ec2f25e4a1513073a2a4098cbb942c2e3e0",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "Oh2015 baseline",
            "name_full": "Action-conditional video prediction (Oh et al., 2015) recurrent simulator",
            "brief_description": "A prior action-conditional recurrent video-prediction model used as the baseline: an RNN that encodes frames with convolutions, uses a state transition that depends on encoded frames, and decodes predicted frames with deconvolutions where actions influence decoding via multiplicative interactions.",
            "citation_title": "Action-conditional video prediction using deep networks in Atari games",
            "mention_or_use": "use",
            "model_name": "Oh et al. (2015) action-conditional recurrent simulator",
            "model_description": "Recurrent neural-network video predictor: frames are encoded with a convolutional encoder C, state s_t = f(s_{t-1}, C(I(xhat_{t-1}, x_{t-1}))) (i.e. state does not directly take action), and predicted frames are produced by decoding D(s_t, a_{t-1}) where the action enters via multiplicative interaction with the state before deconvolution; trained by minimising MSE between predicted and observed frames.",
            "model_type": "neural simulator (recurrent conv-deconv RNN)",
            "task_domain": "Atari 2600 games (video prediction), used as a baseline for other visual environment simulation tasks",
            "fidelity_metric": "Mean squared error (MSE) on next-frame / multi-step prediction averaged over many sequences (paper reports a specific prediction error defined as (1/(3*10,000)) sum ||x - x_hat||^2 and plots MSE vs time-step), plus qualitative human-play and visual inspection of generated frames",
            "fidelity_performance": "Reported via curves in paper: baseline achieves good short-term accuracy in some games but worse long-term coherence compared to the proposed architectures; no single global scalar MSE is provided in text for all games. Baseline performance is the reference for the authors' improvements (see plotted prediction-error curves).",
            "interpretability_assessment": "Black-box neural representation: internal state s_t is a learned vector; interpretability is limited to visualizing predicted frames and inspecting failure modes (e.g., missing objects or blurriness).",
            "interpretability_method": "Visualization of predicted frames and videos; qualitative human interactive tests; analysis of prediction error vs time-step. No explicit latent-variable or object-level interpretability methods reported.",
            "computational_cost": "Decoding and encoding high-dimensional images at every timestep (decode from state to 210x160x3 and re-encode each step) — high FLOP cost per timestep; model sizes and exact FLOPs depend on encoder/decoder but are materially larger than the prediction-independent variant due to image projection at every step.",
            "efficiency_comparison": "Less efficient than the paper's prediction-independent simulator because it must project to/from high-dimensional image space at every step; the paper's PI variant claims ~200 million FLOP savings per timestep by avoiding per-step image decoding/encoding.",
            "task_performance": "Serves as baseline: can produce multi-step predictions but often fails at long-term coherence for many Atari games; the authors build on and improve this baseline with action-conditioned transitions and training-scheme changes.",
            "task_utility_analysis": "Useful for short-horizon rollouts under the training-policy distribution but tends to degrade long-horizon; observation-dependent transitions can help short-term accuracy but hurt long-term generalization.",
            "tradeoffs_observed": "Baseline trades per-step visual fidelity for action incorporation — actions only influence decoding, which can limit how well the model captures dynamics; scheduled-sampling-like observation-dependent transitions improve short-term but hurt long-term.",
            "design_choices": "Convolutional encoder/decoder pair, multiplicative action influence in decoder, prediction-dependent vs observation-dependent transitions considered during training (scheduled phases), truncated BPTT during training.",
            "comparison_to_alternatives": "Used as the main prior comparison; authors find that direct action-conditioning of the state transition and alternative training schemes can outperform this baseline on long-term prediction.",
            "optimal_configuration": "Paper suggests that training close to usage (more prediction-dependent transitions) improves long-term accuracy compared to heavy use of observation-dependent transitions used in the Oh et al. training schedule.",
            "uuid": "e1381.0",
            "source_info": {
                "paper_title": "Recurrent Environment Simulators",
                "publication_date_yy_mm": "2017-04"
            }
        },
        {
            "name_short": "PD-sim",
            "name_full": "Prediction-dependent recurrent environment simulator (this paper)",
            "brief_description": "An LSTM-based, action-conditioned recurrent world model that conditions state updates on the previous predicted frame or observed frame (prediction-dependent transitions) and on the previous action, with convolutional encoder and deconvolutional decoder to produce pixel-space predictions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Prediction-dependent recurrent environment simulator",
            "model_description": "Action-conditioned LSTM whose state update f(s_{t-1}, a_{t-1}, C(I(xhat_{t-1}, x_{t-1}))) combines an encoded frame embedding with action information (action fused multiplicatively into gate computations). A convolutional encoder C maps frames to a vector (2816 in Atari experiments), LSTM state (h_t,c_t) has dimension 1024, and a decoder D(h_t) (possibly combined with action in some variants) produces pixel-frame predictions via deconvolutional layers.",
            "model_type": "neural simulator (action-conditioned recurrent latent model / video-prediction RNN)",
            "task_domain": "High-dimensional visual environment prediction / simulation for Atari 2600 games, TORCS (3D car racing), and first-person 3D mazes; used for model-based exploration and interactive human play.",
            "fidelity_metric": "Mean squared error (MSE) between predicted and observed frames averaged over sequences (prediction error averaged over 10,000 sequences in experiments); also qualitative metrics: visual coherence over 100–200+ time-steps and success in human interactive play.",
            "fidelity_performance": "Qualitatively: shows substantial long-term coherence in many Atari games and excellent multi-hundred-step accuracy in TORCS; quantitative MSE curves are presented per-game (no single scalar summary reported). Empirically: long-term prediction error generally decreases as the percentage of consecutive prediction-dependent transitions increases; accurate predictions up to 100–200 timesteps in several Atari games and several hundred timesteps in TORCS/3D mazes.",
            "interpretability_assessment": "Predictions are inspected visually (videos) to assess object dynamics and coherence; internal latent states are not claimed to be directly interpretable (black-box vector states). Some qualitative interpretation comes from seeing which objects are preserved or vanish in generated frames.",
            "interpretability_method": "Visualization of generated frames and video rollouts; human interactive tests; analysis of failure modes across games to infer what dynamics are captured (e.g., ball/paddle dynamics, background generation). No explicit disentanglement or latent-variable interpretability techniques used.",
            "computational_cost": "State dimension 1024, encoder output 2816 in Atari setup; decoding to 210x160x3 images each timestep (deconvolutions) is computationally heavy. Training uses truncated BPTT (backprop over subsequences up to length 20), mini-batch size 16, RMSProp (learning rate 1e-5). Training datasets: ~5M frames for training and 1M for test in Atari experiments. Exact parameter counts are not provided, but per-step decoding incurs large FLOPs (order of hundreds of millions for full conv-deconv passes).",
            "efficiency_comparison": "Compared to prediction-independent variant, PD model is less efficient because it decodes/encodes images each step. Compared to Oh et al. baseline, PD with direct action conditioning on state and adjusted training schemes yields better long-term accuracy in many domains.",
            "task_performance": "Enables improved long-term visual prediction in many Atari games (outperforming prior work in several metrics/visual coherence), supports human-interactive play for hundreds of frames in Pong, Breakout and TORCS, and improves model-based exploration in 3D mazes (see model-based exploration result).",
            "task_utility_analysis": "High long-term prediction accuracy improves utility for planning-like tasks (Model-based exploration, interactive simulation) but can reduce short-term sharpness; improvements in predictive fidelity often translate to better exploration (the paper's explorer covers ~50% more maze area after 900 steps than random), though overfitting to training-policy can reduce robustness to novel action distributions.",
            "tradeoffs_observed": "Increasing consecutive prediction-dependent transitions increases long-term accuracy but reduces short-term sharpness and sometimes convergence speed; higher prediction horizon T and more PDT improves global dynamics but can degrade per-frame visual detail. Models optimized for long-term MSE can become fragile out-of-distribution (less robust to human-play styles).",
            "design_choices": "Key choices: action fused multiplicatively into LSTM gates (global direct action influence), high-dimensional LSTM state (1024), convolutional encoder/decoder (encoder output 2816), varied training schemes controlling percentage and consecutiveness of prediction-dependent transitions, truncated BPTT across subsequences to increase effective T, warm-up tau typically 10.",
            "comparison_to_alternatives": "Compared experimentally to (a) Oh et al. baseline (the authors' PD variants with direct action-conditioning generally outperform in long-term accuracy), (b) mixing vs pure PDT training schedules (pure PDT gives best long-term accuracy but may hurt short-term), and (c) prediction-independent variants (PD gives more stable long-term predictions in many games though PI can be more efficient).",
            "optimal_configuration": "Paper recommends training schemes that increase consecutive prediction-dependent transitions to encourage learning global dynamics for better long-term accuracy; pure 100% PDT often yields best long-term MSE but mixed schemes are preferable for very complex games where short-term detail matters; choose T and consecutive PDT fraction according to trade-off between long-term coherence and short-term sharpness; use truncated BPTT subsequences to increase effective T.",
            "uuid": "e1381.1",
            "source_info": {
                "paper_title": "Recurrent Environment Simulators",
                "publication_date_yy_mm": "2017-04"
            }
        },
        {
            "name_short": "PI-sim",
            "name_full": "Prediction-independent recurrent environment simulator (this paper)",
            "brief_description": "A variant world model whose state transition is independent of the predicted frame (s_t = f(s_{t-1}, a_{t-1})), allowing the simulator to evolve state without decoding to pixel space at each step and thereby saving substantial computation during rollouts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Prediction-independent recurrent environment simulator",
            "model_description": "Action-conditioned state-transition model where s_t = f(s_{t-1}, a_{t-1}) and the encoding z_t used during warm-up is set to h_t (the latent state), enabling the model to run multiple action-steps by evolving latent state without invoking the expensive encoder/decoder per timestep; only when visual output is needed is decoder invoked. Underlying transition implemented with action-conditioned LSTM-like updates and separate warm-up/prediction parameters.",
            "model_type": "neural simulator (action-conditioned latent-only recurrent model / efficient rollout model)",
            "task_domain": "Atari games and other high-dimensional visual environments where agents need multi-step rollouts of action sequences (e.g., for planning or exploration), also tested on Atari and 3D environments.",
            "fidelity_metric": "Same MSE prediction-error measures as PD model when decoded frames are produced for evaluation; fidelity assessed as long-term prediction MSE when using truncated BPTT and subsequence training, plus visual inspection of decoded long-rollout frames.",
            "fidelity_performance": "Qualitatively more sensitive to state-transition and training-scheme choices; for Atari, PI-sim required different hyperparameters (e.g., T=30 via truncated BPTT) to approach PD performance; in some difficult games PI outperformed PD, but overall PI tended to give worse long-term visual quality (e.g., fish of wrong size in Fishing Derby) unless carefully tuned. No single global MSE scalar is reported.",
            "interpretability_assessment": "Latent-only rollout increases opacity (latent states are not interpreted), and interpretability relies on visualizing decoded frames when the decoder is applied; no additional interpretability mechanisms were introduced.",
            "interpretability_method": "Visual inspection of decoded frames after latent rollouts; analysis of sensitivity to training schemes and state-transition architectures. No explicit latent probing or disentanglement.",
            "computational_cost": "Significant per-step savings by avoiding per-step conv-deconv operations; authors estimate ~200 million FLOP saved per timestep in Atari by not decoding/encoding images each step. Training still requires learning encoder/decoder during warm-up and occasional decoding, and truncated BPTT/backprop to some warm-up steps was used (they backprop to time-step five in PI models).",
            "efficiency_comparison": "More computationally efficient at inference/rollout time than prediction-dependent simulators because it avoids per-step high-dimensional image projections; claimed ~200M FLOP per timestep reduction in Atari experiments. Efficiency enables faster multi-step rollouts for planning/exploration.",
            "task_performance": "Enabled faster model-based exploration (used for rollouts in 3D mazes) and could match or exceed PD in specific difficult games when hyperparameters adjusted; however, it was more sensitive to architecture and training choices and often required more careful tuning to reach PD accuracy.",
            "task_utility_analysis": "PI-sim is especially useful when the agent wants to simulate long action sequences without producing intermediate images (e.g., for planning), trading some predictive fidelity/robustness for big computational savings; for downstream tasks that require per-step visual accuracy PI-sim may be inferior unless carefully tuned.",
            "tradeoffs_observed": "PI-sim gains large computational efficiency at the cost of increased sensitivity to state-transition parametrization and training scheme; in many games PI requires longer prediction lengths and truncated BPTT strategies (e.g., T=30 split into subsequences) to approach PD fidelity. Some long-term visual artifacts (e.g., object-size errors) observed without careful tuning.",
            "design_choices": "Remove dependence of s_t on encoded predicted frame; use separate parameters for warm-up and prediction phases; backpropagate to a subset of warm-up timesteps during training; use truncated BPTT across subsequences to increase effective T while controlling memory.",
            "comparison_to_alternatives": "Compared against PD variant: PI provides large inference-time FLOP reductions and faster rollouts, but typically needs different training hyperparameters and can be less robust; compared to baseline Oh et al. it can be far more efficient and sometimes equally accurate when tuned.",
            "optimal_configuration": "Paper suggests PI benefits from longer effective prediction lengths via truncated BPTT (e.g., T=30 via subsequences) and careful backprop into warm-up steps; hyperparameter tuning and choice of state-transition architecture are more critical for PI than PD.",
            "uuid": "e1381.2",
            "source_info": {
                "paper_title": "Recurrent Environment Simulators",
                "publication_date_yy_mm": "2017-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Action-conditional video prediction using deep networks in Atari games",
            "rating": 2
        },
        {
            "paper_title": "Unsupervised learning of video representations using LSTMs",
            "rating": 2
        },
        {
            "paper_title": "Embed to control: A locally linear latent dynamics model for control from raw images",
            "rating": 1
        },
        {
            "paper_title": "From pixels to torques: Policy learning with deep dynamical models",
            "rating": 1
        },
        {
            "paper_title": "Model regularization for stable sample rollouts",
            "rating": 1
        }
    ],
    "cost": 0.013676749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>RECURRENT ENVIRONMENT SIMULATORS</h1>
<p>Silvia Chiappa, Sébastien Racaniere, Daan Wierstra \&amp; Shakir Mohamed<br>DeepMind, London, UK<br>{csilvia, sracaniere, wierstra, shakir}@google.com</p>
<h4>Abstract</h4>
<p>Models that can simulate how environments change in response to actions can be used by agents to plan and act efficiently. We improve on previous environment simulators from high-dimensional pixel observations by introducing recurrent neural networks that are able to make temporally and spatially coherent predictions for hundreds of time-steps into the future. We present an in-depth analysis of the factors affecting performance, providing the most extensive attempt to advance the understanding of the properties of these models. We address the issue of computationally inefficiency with a model that does not need to generate a highdimensional image at each time-step. We show that our approach can be used to improve exploration and is adaptable to many diverse environments, namely 10 Atari games, a 3D car racing environment, and complex 3D mazes.</p>
<h2>1 INTRODUCTION</h2>
<p>In order to plan and act effectively, agent-based systems require an ability to anticipate the consequences of their actions within an environment, often for an extended period into the future. Agents can be equipped with this ability by having access to models that can simulate how the environments changes in response to their actions. The need for environment simulation is widespread: in psychology, model-based predictive abilities form sensorimotor contingencies that are seen as essential for perception (O'Regan \&amp; Noë, 2001); in neuroscience, environment simulation forms part of deliberative planning systems used by the brain (Niv, 2009); and in reinforcement learning, the ability to imagine the future evolution of an environment is needed to form predictive state representations (Littman et al., 2002) and for Monte Carlo planning (Sutton \&amp; Barto, 1998).</p>
<p>Simulating an environment requires models of temporal sequences that must possess a number of properties to be useful: the models should make predictions that are accurate, temporally and spatially coherent over long time periods; and allow for flexibility in the policies and action sequences that are used. In addition, these models should be general-purpose and scalable, and able to learn from high-dimensional perceptual inputs and from diverse and realistic environments. A model that achieves these desiderata can empower agent-based systems with a vast array of abilities, including counterfactual reasoning (Pearl, 2009), intuitive physical reasoning (McCloskey, 1983), model-based exploration, episodic control (Lengyel \&amp; Dayan, 2008), intrinsic motivation (Oudeyer et al., 2007), and hierarchical control.</p>
<p>Deep neural networks have recently enabled significant advances in simulating complex environments, allowing for models that consider high-dimensional visual inputs across a wide variety of domains (Wahlström et al., 2015; Watter et al., 2015; Sun et al., 2015; Patraucean et al., 2015). The model of Oh et al. (2015) represents the state-of-the-art in this area, demonstrating high long-term accuracy in deterministic and discrete-action environments.</p>
<p>Despite these advances, there are still several challenges and open questions. Firstly, the properties of these simulators in terms of generalisation and sensitivity to the choices of model structure and training are poorly understood. Secondly, accurate prediction for long time periods into the future remains difficult to achieve. Finally, these models are computationally inefficient, since they require the prediction of a high-dimensional image each time an action is executed, which is unnecessary in situations where the agent is interested only in the final prediction after taking several actions.</p>
<p>In this paper we advance the state-of-the-art in environment modelling. We build on the work of Oh et al. (2015), and develop alternative architectures and training schemes that significantly improve performance, and provide in-depth analysis to advance our understanding of the properties of these</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Graphical model representing (a) the recurrent structure used in Oh et al. (2015) and (b) our recurrent structure. Filled and empty nodes indicate observed and hidden variables respectively.
models. We also introduce a simulator that does not need to predict visual inputs after every action, reducing the computational burden in the use of the model. We test our simulators on three diverse and challenging families of environments, namely Atari 2600 games, a first-person game where an agent moves in randomly generated 3D mazes, and a 3D car racing environment; and show that they can be used for model-based exploration.</p>
<h1>2 RECURRENT ENVIRONMENT SIMULATORS</h1>
<p>An environment simulator is a model that, given a sequence of actions $a_{1}, \ldots, a_{\tau-1} \equiv a_{1: \tau-1}$ and corresponding observations $\mathbf{x}<em _tau:="\tau:" _tau_tau_prime="\tau+\tau^{\prime">{1: \tau}$ of the environment, is able to predict the effect of subsequent actions $a</em>}-1}$, such as forming predictions $\hat{\mathbf{x}<em _tau_1:="\tau+1:" _tau_tau_prime="\tau+\tau^{\prime">{\tau+1: \tau+\tau^{\prime}}$ or state representations $\mathbf{s}</em>$ of the environment.}</p>
<p>Our starting point is the recurrent simulator of Oh et al. (2015), which is the state-of-the-art in simulating deterministic environments with visual observations (frames) and discrete actions. This simulator is a recurrent neural network with the following backbone structure:</p>
<p>$$
\mathbf{s}<em t-1="t-1">{t}=f\left(\mathbf{s}</em>}, \mathcal{C}\left(\mathbb{I}\left(\hat{\mathbf{x}<em t-1="t-1">{t-1}, \mathbf{x}</em>}\right)\right)\right), \quad \hat{\mathbf{x}<em t="t">{t}=\mathcal{D}\left(\mathbf{s}</em>\right)
$$}, a_{t-1</p>
<p>In this equation, $\mathbf{s}<em t-1="t-1">{t}$ is a hidden state representation of the environment, and $f$ a non-linear deterministic state transition function. The symbol $\mathbb{I}$ indicates the selection of the predicted frame $\hat{\mathbf{x}}</em>}$ or real frame $\mathbf{x<em t="t">{t-1}$, producing two types of state transition called prediction-dependent transition and observation-dependent transition respectively. $\mathcal{C}$ is an encoding function consisting of a series of convolutions, and $\mathcal{D}$ is a decoding function that combines the state $\mathbf{s}</em>}$ with the action $a_{t-1}$ through a multiplicative interaction, and then transforms it using a series of full convolutions to form the predicted frame $\hat{\mathbf{x}<em _tau_1:="\tau+1:" _tau_tau_prime="\tau+\tau^{\prime">{t}$.
The model is trained to minimise the mean squared error between the observed time-series $\mathbf{x}</em>}}$, corresponding to the evolution of the environment, and its prediction. In a probabilistic framework, this corresponds to maximising the log-likelihood in the graphical model depicted in Fig. 1(a). In this graph, the link from $\hat{\mathbf{x}<em t="t">{t}$ to $\mathbf{x}</em>}$ represents stochastic dependence, as $\mathbf{x<em t="t">{t}$ is formed by adding to $\hat{\mathbf{x}}</em>$ a Gaussian noise term with zero mean and unit variance, whilst all remaining links represent deterministic dependences. The dashed lines indicate that only one of the two links is active, depending on whether the state transition is prediction-dependent or observation-dependent.
The model is trained using stochastic gradient descent, in which each mini-batch consists of a set of segments of length $\tau+T$ randomly sub-sampled from $\mathbf{x}_{1: \tau+\tau^{\prime}}$. For each segment in the minibatch, the model uses the first $\tau$ observations to evolve the state and forms predictions of the last $T$ observations only. Training comprises three phases differing in the use of prediction-dependent or observation-dependent transitions (after the first $\tau$ transitions) and in the value of the prediction length $T$. In the first phase, the model uses observation-dependent transitions and predicts for $T=10$ time-steps. In the second and third phases, the model uses prediction-dependent transitions and predicts for $T=3$ and $T=5$ time-steps respectively. During evaluation or usage, the model can only use prediction-dependent transitions.</p>
<h1>Action-Dependent State Transition</h1>
<p>A strong feature of the model of Oh et al. (2015) described above is that the actions influence the state transitions only indirectly through the predictions or the observations. Allowing the actions to condition the state transitions directly could potentially enable the model to incorporate action information more effectively. We therefore propose the following backbone structure:</p>
<p>$$
\mathbf{s}<em t-1="t-1">{t}=f\left(\mathbf{s}</em>}, a_{t-1}, \mathcal{C}\left(\mathbb{I}\left(\hat{\mathbf{x}<em t-1="t-1">{t-1}, \mathbf{x}</em>}\right)\right)\right), \quad \hat{\mathbf{x}<em t="t">{t}=\mathcal{D}\left(\mathbf{s}</em>\right)
$$</p>
<p>In the graphical model representation, this corresponds to replacing the link from $a_{t-1}$ to $\hat{\mathbf{x}}<em t-1="t-1">{t}$ with a link from $a</em>$ as in Fig. 1(b).}$ to $\mathbf{s}_{t</p>
<h2>Short-Term versus Long-Term Accuracy</h2>
<p>The last two phases in the training scheme of Oh et al. (2015) described above are used to address the issue of poor accuracy that recurrent neural networks trained using only observation-dependent transitions display when asked to predict several time-steps ahead. However, the paper does not analyse nor discuss alternative training schemes.</p>
<p>In principle, the highest accuracy should be obtained by training the model as closely as possible to the way it will be used, and therefore by using a number of prediction-dependent transitions which is as close as possible to the number of time-steps the model will be asked to predict for. However, prediction-dependent transitions increase the complexity of the objective function such that alternative schemes are most often used (Talvitie, 2014; Bengio et al., 2015; Oh et al., 2015). Current training approaches are guided by the belief that using the observation $\mathbf{x}<em t-1="t-1">{t-1}$, rather than the prediction $\hat{\mathbf{x}}</em>$ has the effect of reducing the propagation of the errors made in the predictions, which are higher at earlier stages of the training, enabling the model to correct itself from the mistakes made up to time-step $t-1$. For example, Bengio et al. (2015) introduce a scheduled sampling approach where at each time-step the type of state transition is sampled from a Bernoulli distribution, with parameter annealed from an initial value corresponding to using only observationdependent transitions to a final value corresponding to using only prediction-dependent transitions, according to a schedule selected by validation.}$, to form the state $\mathbf{s}_{t</p>
<p>Our analysis of different training schemes on Atari, which considered the interplay among warm-up length $\tau$, prediction length $T$, and number of prediction-dependent transitions, suggests that, rather than as having a corrective effect, observation-dependent transitions should be seen as restricting the time interval in which the model considers its predictive abilities, and therefore focuses resources. Indeed we found that, the higher the number of consecutive prediction-dependent transitions, the more the model is encouraged to focus on learning the global dynamics of the environment, which results in higher long-term accuracy. The highest long-term accuracy is always obtained by a training scheme that uses only prediction-dependent transitions even at the early stages of the training. Focussing on learning the global dynamics comes at the price of shifting model resources away from learning the precise details of the frames, leading to a decrease in short-term accuracy. Therefore, for complex games for which reasonable long-term accuracy cannot be obtained, training schemes that mix prediction-dependent and observation-dependent transitions are preferable. It follows from this analysis that percentage of consecutive prediction-dependent transitions, rather than just percentage of such transitions, should be considered when designing training schemes.</p>
<p>From this viewpoint, the poor results obtained in Bengio et al. (2015) when using only predictiondependent transitions can be explained by the difference in the type of the tasks considered. Indeed, unlike our case in which the model is tolerant to some degree of error such as blurriness in earlier predictions, the discrete problems considered in Bengio et al. (2015) are such that one prediction error at earlier time-steps can severely affect predictions at later time-steps, so that the model needs to be highly accurate short-term in order to perform reasonably longer-term. Also, Bengio et al. (2015) treated the prediction used to form $\mathbf{s}<em t-1="t-1">{t}$ as a fixed quantity, rather than as a function of $\mathbf{s}</em>$, and therefore did not perform exact maximum likelihood.</p>
<h2>Prediction-Independent State Transition</h2>
<p>In addition to potentially enabling the model to incorporate action information more effectively, allowing the actions to directly influence the state dynamics has another crucial advantage: it allows to consider the case of a state transition that does not depend on the frame, i.e. of the form $\mathbf{s}<em t-1="t-1">{t}=f\left(\mathbf{s}</em>}, a_{t-1}\right)$, corresponding to removing the dashed links from $\hat{\mathbf{x}<em t-1="t-1">{t-1}$ and from $\mathbf{x}</em>$ in}$ to $\mathbf{s}_{t</p>
<p>Fig. 1(b). We shall call such a model prediction-independent simulator, referring to its ability to evolve the state without using the prediction during usage. Prediction-independent state transitions for high-dimensional observation problems have also been considered in Srivastava et al. (2015).</p>
<p>A prediction-independent simulator can dramatically increase computational efficiency in situations is which the agent is interested in the effect of a sequence of actions rather than of a single action. Indeed, such a model does not need to project from the lower dimensional state space into the higher dimensional observation space through the set of convolutions, and vice versa, at each time-step.</p>
<h1>3 PREDICTION-DEPENDENT SIMULATORS</h1>
<p>We analyse simulators with state transition of the form $\mathbf{s}<em t-1="t-1">{t}=f\left(\mathbf{s}</em>}, a_{t-1}, \mathcal{C}\left(\mathbb{I}\left(\hat{\mathbf{x}<em t-1="t-1">{t-1}, \mathbf{x}</em>\right)\right)\right)$ on three families of environments with different characteristics and challenges, namely Atari 2600 games from the arcade learning environment (Bellemare et al., 2013), a first-person game where an agent moves in randomly generated 3D mazes (Beattie et al., 2016), and a 3D car racing environment called TORCS (Wymann et al., 2013). We use two evaluation protocols. In the first one, the model is asked to predict for 100 or 200 time-steps into the future using actions from the test data. In the second one, a human uses the model as an interactive simulator. The first protocol enables us to determine how the model performs within the action policy of the training data, whilst the second protocol enables us to explore how the model generalises to other action policies.</p>
<p>As state transition, we used the following action-conditioned long short-term memory (LSTM) (Hochreiter \&amp; Schmidhuber, 1997):</p>
<p>Encoding: $\mathbf{z}<em t-1="t-1">{t-1}=\mathcal{C}\left(\mathbb{I}\left(\hat{\mathbf{x}}</em>}, \mathbf{x<em t="t">{t-1}\right)\right)$,
Action fusion: $\mathbf{v}</em>}=\mathbf{W}^{h} \mathbf{h<em t-1="t-1">{t-1} \otimes \mathbf{W}^{a} \mathbf{a}</em>$,
Gate update: $\mathbf{i}<em t="t">{t}=\sigma\left(\mathbf{W}^{i v} \mathbf{v}</em>}+\mathbf{W}^{i z} \mathbf{z<em t="t">{t-1}\right), \mathbf{f}</em>}=\sigma\left(\mathbf{W}^{f v} \mathbf{v<em t-1="t-1">{t}+\mathbf{W}^{f z} \mathbf{z}</em>\right)$,</p>
<p>$$
\mathbf{o}<em t="t">{t}=\sigma\left(\mathbf{W}^{o v} \mathbf{v}</em>\right)
$$}+\mathbf{W}^{o z} \mathbf{z}_{t-1</p>
<p>Cell update: $\mathbf{c}<em t="t">{t}=\mathbf{f}</em>} \otimes \mathbf{c<em t="t">{t-1}+\mathbf{i}</em>} \otimes \tanh \left(\mathbf{W}^{c v} \mathbf{v<em t-1="t-1">{t}+\mathbf{W}^{c z} \mathbf{z}</em>\right)$,
State update: $\mathbf{h}<em t="t">{t}=\mathbf{o}</em>} \otimes \tanh \left(\mathbf{c<em t-1="t-1">{t}\right)$,
where $\otimes$ denotes the Hadamard product, $\sigma$ the logistic sigmoid function, $\mathbf{a}</em>}$ is a one-hot vector representation of $a_{t-1}$, and $\mathbf{W}$ are parameter matrices. In Eqs. (2)-(5), $\mathbf{h<em t="t">{t}$ and $\mathbf{c}</em>}$ are the LSTM state and cell forming the model state $\mathbf{s<em t="t">{t}=\left(\mathbf{h}</em>}, \mathbf{c<em t="t">{t}\right)$; and $\mathbf{i}</em>}, \mathbf{f<em t="t">{t}$, and $\mathbf{o}</em>}$ are the input, forget, and output gates respectively (for simplicity, we omit the biases in their updates). The vectors $\mathbf{h<em t="t">{t}$ and $\mathbf{v}</em>$ for the three families of environments can be found in Appendix B.1, B. 2 and B.3. We used a warm-up phase of length $\tau=10$ and we did not backpropagate the gradient to this phase.}$ had dimension 1024 and 2048 respectively. Details about the encoding and decoding functions $\mathcal{C}$ and $\mathcal{D</p>
<h3>3.1 ATARI</h3>
<p>We considered the 10 Atari games Freeway, Ms Pacman, Qbert, Seaquest, Space Invaders, Bowling, Breakout, Fishing Derby, Pong, and Riverraid. Of these, the first five were analysed in Oh et al. (2015) and are used for comparison. The remaining five were chosen to better test the ability of the model in environments with other challenging characteristics, such as scrolling backgrounds (Riverraid), small/thin objects that are key aspects of the game (lines in Fishing Derby, ball in Pong and Breakout), and sparse-reward games that require very long-term predictions (Bowling). We used training and test datasets consisting of five and one million $210 \times 160$ RGB images respectively, with actions chosen from a trained DQN agent (Mnih et al., 2015) according to an $\epsilon=0.2$-greedy policy. Such a large number of training frames ensured that our simulators did not strongly overfit to the training data (see training and test lines in Figs. 2 and 3, and the discussion in Appendix B.1).</p>
<h2>Short-Term versus Long-Term Accuracy</h2>
<p>Below we summarise our results on the interplay among warm-up length $\tau$, prediction length $T$, and number of prediction-dependent transitions - the full analysis is given in Appendix B.1.1.</p>
<p>The warm-up and prediction lengths $\tau$ and $T$ regulate degree of accuracy in two different ways. 1) The value of $\tau+T$ determines how far into the past the model can access information - this is the case irrespectively of the type of transition used, although when using prediction-dependent transitions</p>
<p>information about the last $T$ time-steps of the environment would need to be inferred. Accessing information far back into the past can be necessary even when the model is used to perform one-step ahead prediction only. 2) The higher the value of $T$ and the number of prediction-dependent transitions, the more the corresponding objective function encourages long-term accuracy. This is achieved by guiding the one-step ahead prediction error in such a way that further predictions will not be strongly affected, and by teaching the model to make use of information from the far past. The more precise the model is in performing one-step ahead prediction, the less noise guidance should be required. Therefore, models with very accurate convolutional and transition structures should need less encouragement.</p>
<p>Increasing the percentage of consecutive prediction-dependent transitions increases long-term accuracy, often at the expense of short-term accuracy. We found that using only observationdependent transitions leads to poor performance in most games. Increasing the number of consecutive prediction-dependent transitions produces an increase in long-term accuracy, but also a decrease in short-term accuracy usually corresponding to reduction in sharpness. For games that are too complex, although the lowest long-term prediction error is still achieved with using only predictiondependent transitions, reasonable long-term accuracy cannot be obtained, and training schemes that mix prediction-dependent and observation-dependent transitions are therefore preferable.</p>
<p>To illustrate these results, we compare the following training schemes for prediction length $T=15$ :</p>
<ul>
<li>0\% PDT: Only observation-dependent transitions.</li>
<li>33\% PDT: Observation and prediction-dependent transitions for the first 10 and last 5 time-steps respectively.</li>
<li>0\%-20\%-33\% PDT: Only observation-dependent transitions in the first 10,000 parameter updates; observation-dependent transitions for the first 12 time-steps and prediction-dependent transitions for the last 3 time-steps for the subsequent 100,000 parameters updates; observation-dependent transitions for the first 10 time-steps and prediction-dependent transitions for the last 5 time-steps for the remaining parameter updates (adaptation of the training scheme of Oh et al. (2015) to $T=15$ ).</li>
<li>46\% PDT Alt.: Alternate between observation-dependent and prediction-dependent transitions from a time-step to the next.</li>
<li>46\% PDT: Observation and prediction-dependent transitions for the first 8 and last 7 time-steps respectively.</li>
<li>67\% PDT: Observation and prediction-dependent transitions for the first 5 and last 10 time-steps respectively.</li>
<li>0\%-100\% PDT: Only observation-dependent transitions in the first 1000 parameter updates; only prediction-dependent transitions in the subsequent parameter updates.</li>
<li>100\% PDT: Only prediction-dependent transitions.</li>
</ul>
<p>For completeness, we also consider a training scheme as in Oh et al. (2015), which consists of three phases with $T=10, T=3, T=5$, and 500,000, 250,000, 750,000 parameter updates respectively. In the first phase $\mathbf{s}<em t-1="t-1">{t}$ is formed by using the observed frame $\mathbf{x}</em>}$, whilst in the two subsequent phases $\mathbf{s<em t-1="t-1">{t}$ is formed by using the predicted frame $\hat{\mathbf{x}}</em>$.
In Figs. 2 and 3 we show the prediction error averaged over 10,000 sequences $^{1}$ for the games of Bowling ${ }^{2}$, Fishing Derby, Pong and Seaquest. More specifically, Fig. 2(a) shows the error for predicting up to 100 time-steps ahead after the model has seen 200 million frames (corresponding to half million parameter updates using mini-batches of 16 sequences), using actions and warm-up frames from the test data, whilst Figs. 2(b)-(c) and 3 show the error at time-steps 5, 10 and 100 versus number of frames seen by the model.</p>
<p>These figures clearly show that long-term accuracy generally improves with increasing number of consecutive prediction-dependent transitions. When using alternating ( $46 \%$ PDT Alt.), rather than consecutive ( $46 \%$ PDT), prediction-dependent transitions long-term accuracy is worse, as we are effectively asking the model to predict at most two time-steps ahead. We can also see that</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Prediction error averaged over 10,000 sequences on (a)-(b) Bowling and (c) Fishing Derby for different training schemes. The same color and line code is used in all figures. (a): Prediction error vs time-steps after the model has seen 200 million frames. (b)-(c): Prediction error vs number of frames seen by the model at time-steps 10 and 100 .
using more prediction-dependent transitions produces lower short-term accuracy and/or slower short-term convergence. Finally, the figures show that using a training phase with only observationdependent transitions that is too long, as in Oh et al. (2015), can be detrimental: the models reaches at best a performance similar to the $46 \%$ PDT Alt. training scheme (the sudden drop in prediction error corresponds to transitioning to the second training phase), but is most often worse.
By looking at the predicted frames we could notice that, in games containing balls and paddles, using only observation-dependent transitions gives rise to errors in reproducing the dynamics of these objects. Such errors decrease with increasing prediction-dependent transitions. In other games, using only observation-dependent transitions causes the model to fail in representing moving objects, except for the agent in most cases. Training schemes containing more prediction-dependent transitions encourage the model to focus more on learning the dynamics of the moving objects and less on details that would only increase short-term accuracy, giving rise to more globally accurate but less sharp predictions. Finally, in games that are too complex, the strong emphasis on long-term accuracy produces predictions that are overall not sufficiently good.
More specifically, from the videos available at ${ }^{3}$ PDTvsODT, we can see that using only observationdependent transitions has a detrimental effect on long-term accuracy for Fishing Derby, Ms Pacman,</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Prediction error on (a) Pong and (b) Seaquest for different training schemes.</p>
<p>Qbert, Riverraid, Seaquest and Space Invaders. The most salient features of the videos are: consistent inaccuracy in predicting the paddle and ball in Breakout; reset to a new life after a few time-steps in Ms Pacman; prediction of background only after a few time-steps in Qbert; no generation of new objects or background in Riverraid; quick disappearance of existing fish and no appearance of new fish from the sides of the frame in Seaquest. For Bowling, Freeway, and Pong, long-term accuracy is generally good, but the movement of the ball is not always correctly predicted in Bowling and Pong and the chicken sometimes disappears in Freeway. On the other hand, using only predictiondependent transitions results in good long-term accuracy for Bowling, Fishing Derby, Freeway, Pong, Riverraid, and Seaquest: the model accurately represents the paddle and ball dynamics in Bowling and Pong; the chicken hardly disappears in Freeway, and new objects and background are created and most often correctly positioned in Riverraid and Seaquest.</p>
<p>The trading-off of long for short-term accuracy when using more prediction-dependent transitions is particularly evident in the videos of Seaquest: the higher the number of such transitions, the better the model learns the dynamics of the game, with new fish appearing in the right location more often. However, this comes at the price of reduced sharpness, mostly in representing the fish.</p>
<p>This trade-off causes problems in Breakout, Ms Pacman, Qbert, and Space Invaders, so that schemes that also use observation-dependent transitions are preferable for these games. For example, in Breakout, the model fails at representing the ball, making the predictions not sufficiently good. Notice that the prediction error (see Fig. 15) is misleading in terms of desired performance, as the $100 \%$ PDT training scheme performs as well as other mixing schemes for long-term accuracy - this highlights the difficulties in evaluating the performance of these models.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Prediction error vs number of frames seen by the model (excluding warm-up frames) for (a) Pong and (b) Seaquest, using prediction lengths $T=10,15$, and 20, and training schemes $0 \%$ PDT, $67 \%$ PDT, and $100 \%$ PDT.</p>
<p>Increasing the prediction length $T$ increases long-term accuracy when using predictiondependent transitions. In Fig. 4, we show the effect of using different prediction lengths $T \leq 20$ on the training schemes $0 \%$ PDT, $67 \%$ PDT, and $100 \%$ PDT for Pong and Seaquest. In Pong, with the $0 \%$ PDT training scheme, using higher $T$ improves long-term accuracy: this is a game for which this scheme gives reasonable accuracy and the model is able to benefit from longer history. This is however not the case for Seaquest (or other games as shown in Appendix B.1.1). On the other hand, with the $100 \%$ PDT training scheme, using higher $T$ improves long-term accuracy in most games (the difference is more pronounced between $T=10$ and $T=15$ than between $T=15$ and $T=20$ ), but decreases short-term accuracy. Similarly to above, reduced short-term accuracy corresponds to reduced sharpness: from the videos available at $T \leq 20$ we can see, for example, that the moving caught fish in Fishing Derby, the fish in Seaquest, and the ball in Pong are less sharp for higher $T$.</p>
<p>Truncated backpropagation still enables increase in long-term accuracy. Due to memory constraints, we could only backpropagate gradients over sequences of length up to 20 . To use $T&gt;20$, we split the prediction sequence into subsequences and performed parameter updates separately for each subsequence. For example, to use $T=30$ we split the prediction sequence into two successive subsequences of length 15 , performed parameter updates over the first subsequence, initialised the state of the second subsequence with the final state from the first subsequence, and then performed parameter updates over the second subsequence. This approach corresponds to a form of truncated backpropagation through time (Williams \&amp; Zipser, 1995) - the extreme of this strategy (with $T$ equal to the length of the whole training sequence) was used by Zaremba et al. (2014).</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Prediction error vs number of frames seen by the model (excluding warm-up frames) for (a) Pong and (b) Seaquest, using BPTT(15, 1), BPTT(15, 2), and BTT(15, 5), and training schemes 0\%PDT, 33\%PDT, and 100\%PDT.</p>
<p>In Fig. 5, we show the effect of using 2 and 5 subsequences of length 15 (indicated by BPTT(15, 2) and BTT(15, 5)) on the training schemes 0\%PDT, 33\%PDT, and 100\%PDT for Pong and Seaquest. We can see that the 0\%PDT and 33\%PDT training schemes display no difference in accuracy for different values of $T$. On the other hand, with the 100\%PDT training scheme, using more than one subsequence improves long-term accuracy (the difference is more pronounced between $T=15$ and $T=30$ than between $T=30$ and $T=75$ ), but decreases short-term accuracy (the difference is small at convergence between $T=15$ and $T=30$, but big between $T=30$ and $T=75$ ). The decrease in accuracy with 5 subsequences is drastic in some games.</p>
<p>For Riverraid, using more than one subsequence with the 33\%PDT and 100\%PDT training schemes improves long-term accuracy dramatically, as shown in Fig. 6, as it enables correct prediction after a jet loss. Interestingly, for the 100\%PDT training scheme, using $\tau=25$ with prediction length $T=15$ (black line) does not give the same amount of gain as when using BPTT(15, 2), even if history length $\tau+T$ is the same. This would seem to suggest that some improvement in BPTT(15, 2) is due to encouraging longer-term accuracy, indicating that this can be achieved even when not fully backpropagating the gradient.</p>
<p>From the videos available at $T&gt;20$, we can see that with $T=75$ the predictions in some of the Fishing Derby videos are faded, whilst in Pong the model can suddenly switch from one dynamics to another for the ball and the opponent's paddle.</p>
<p>In conclusion, using higher $T$ through truncated backpropagation can improve performance. However, in schemes that use many prediction-dependent transitions, a high value of $T$ can lead to poor predictions.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Prediction error vs number of frames seen by the model for Riverraid, using BPTT(15, 1), BPTT(15, 2), and BTT(15, 5), and training schemes 0\%PDT, 33\%PDT, and 100\%PDT. The black line is obtained with the $100 \%$ PDT training scheme.</p>
<h1>Evaluation through Human Play</h1>
<p>Whilst we cannot expect our simulators to generalise to structured sequences of actions never chosen by the DQN and that are not present in the training data, such as moving the agent up and down the alley in Bowling, it is reasonable to expect some degree of generalisation in the action-wise simple environments of Breakout, Freeway and Pong.
We tested these three games by having humans using the models as interactive simulators. We generally found that models trained using only prediction-dependent transitions were more fragile to states of the environment not experienced during training, such that the humans were able to play these games for longer with simulators trained with mixing training schemes. This seems to indicate that models with higher long-term test accuracy are at higher risk of overfitting to the training policy.</p>
<p>In Fig. 7(a), we show some salient frames from a game of Pong played by a human for 500 time-steps (the corresponding video is available at Pong-HPlay). The game starts with score (2,0), after which the opponent scores five times, whilst the human player scores twice. As we can see, the scoring is updated correctly and the game dynamics is accurate. In Fig. 7(b), we show some salient fames from a game of Breakout played by a human for 350 time-steps (the corresponding video is available at Breakout-HPlay). As for Pong, the scoring is updated correctly and the game dynamics is accurate. These images demonstrate some degree of generalisation of the model to a human style of play.</p>
<h2>Evaluation of State Transitions Structures</h2>
<p>In Appendix B.1.2 and B.1.3 we present an extensive evaluation of different action-dependent state transitions, including convolutional transformations for the action fusion, and gate and cell updates, and different ways of incorporating action information. We also present a comparison between action-dependent and action-independent state transitions.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Salient frames extracted from (a) 500 frames of Pong and (b) 350 frames of Breakout generated using our simulator with actions taken by a human player (larger versions can be found in Figs. 47 and 48).</p>
<p>Some action-dependent state transitions give better performance than the baseline (Eqs. (1)-(5)) in some games. For example, we found that increasing the state dimension from 1024 to the dimension of the convolved frame, namely 2816 , might be preferable. Interestingly, this is not due to an increase in the number of parameters, as the same gain is obtained using convolutions for the gate and cell updates. These results seem to suggest that high-dimensional sparse state transition structures could be a promising direction for further improvement. Regarding different ways of incorporation action information, we found that using local incorporation such as augmenting the frame with action information and indirect action influence gives worse performance that direct and global action influence, but that there are several ways of incorporating action information directly and globally that give similar performance.</p>
<h1>3.2 3D ENVIRONMENTS</h1>
<p>Both TORCS and the 3D maze environments highlight the need to learn dynamics that are temporally and spatially coherent: TORCS exposes the need to learn fast moving dynamics and consistency under motion, whilst 3D mazes are partially-observed and therefore require the simulator to build an internal representation of its surrounding using memory, as well learn basic physics, such as rotation, momentum, and the solid properties of walls.</p>
<p>TORCS. The data was generated using an artificial agent controlling a fast car without opponents (more details are given in Appendix B.2).</p>
<p>When using actions from the test set (see Fig. 49 and the corresponding video at TORCS), the simulator was able to produce accurate predictions for up to several hundreds time-steps. As the car moved around the racing track, the simulator was able to predict the appearance of new features in the background (towers, sitting areas, lamp posts, etc.), as well as model the jerky motion of the car caused by our choices of random actions. Finally, the instruments (speedometer and rpm) were correctly displayed.</p>
<p>The simulator was good enough to be used interactively for several hundred frames, using actions provided by a human. This showed that the model had learnt well how to deal with the car hitting the wall on the right side of the track. Some salient frames from the game are shown in Fig. 8 (the corresponding video can be seen at TORCS-HPlay).</p>
<p>3D Mazes. We used an environment that consists of randomly generated 3D mazes, containing textured surfaces with occasional paintings on the walls: the mazes were all of the same size, but</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Salient frames highlighting coherence extracted from 700 frames of TORCS generated using our simulator with actions taken by a human player.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Predicted (left) and real (right) frames at time-steps $1,25,66,158$ and 200 using actions from the test data.
differed in the layout of rooms and corridors, and in the locations of paintings (see Fig. 11(b) for an example of layout). More details are given in Appendix B.3.
When using actions from the test set, the simulator was able to very reasonably predict frames even after 200 steps. In Fig. 9 we compare predicted frames to the real frames at several time-steps (the corresponding video can be seen at 3DMazes). We can see that the wall layout is better predicted when walls are closer to the agent, and that corridors and far away-walls are not as long as they should be. The lighting on the ceiling is correct on all the frames shown.
When using the simulator interactively with actions provided by a human, we could test that the simulator had learnt consistent aspects of the maze: when walking into walls, the model maintained their position and layout (in one case we were able to walk through a painting on the wall - paintings are rare in the dataset and hence it is not unreasonable that they would not be maintained when stress testing the model in this way). When taking $360^{\circ}$ spins, the wall configurations were the same as previously generated and not regenerated afresh, and shown in Fig. 10 (see also 3DMazes-HPLay). The coherence of the maze was good for nearby walls, but not at the end of long-corridors.</p>
<h1>3.3 Model-based Exploration</h1>
<p>The search for exploration strategies better than $\epsilon$-greedy is an active area of research. Various solutions have been proposed, such as density based or optimistic exploration (Auer et al., 2002). Oh et al. (2015) considered a memory-based approach that steers the agent towards previously unobserved frames. In this section, we test our simulators using a similar approach, but select a group of actions rather than a single action at a time. Furthermore, rather than a fixed 2D environment, we consider the more challenging 3D mazes environment. This also enables us this present a qualitative analysis, as we can exactly measure and plot the proportion of the maze visited over time. Our aim is to be quantitatively and qualitatively better than random exploration (using dithering of 0.7 , as this lead to the best possible random agent).
We used a 3D maze simulator to predict the outcome of sequences of actions, chosen with a hardcoded policy. Our algorithm (see below) did $N$ Monte-Carlo simulations with randomly selected sequences of actions of fixed length $d$. At each time-step $t$, we stored the last 10 observed frames in an episodic memory buffer and compared predicted frames to those in memory.</p>
<div class="codehilite"><pre><span></span><code><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">t</span><span class="o">=</span><span class="nt">1</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">episodeLength</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">d</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">    </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">n</span><span class="o">=</span><span class="nt">1</span><span class="o">,</span><span class="w"> </span><span class="nt">N</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">        </span><span class="nt">Choose</span><span class="w"> </span><span class="nt">random</span><span class="w"> </span><span class="nt">actions</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">A</span><span class="o">^</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="o">=</span><span class="nt">a_</span><span class="p">{</span><span class="n">t</span><span class="p">:</span><span class="w"> </span><span class="n">t</span><span class="o">+</span><span class="n">d-1</span><span class="p">}</span><span class="w"> </span><span class="o">;</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="nt">Predict</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">hat</span><span class="p">{</span><span class="err">\mathbf{x</span><span class="p">}</span><span class="err">}</span><span class="nt">_</span><span class="p">{</span><span class="err">t+1:</span><span class="w"> </span><span class="err">t+d</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">end</span>
<span class="w">    </span><span class="nt">Follow</span><span class="w"> </span><span class="nt">actions</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">A</span><span class="o">^</span><span class="p">{</span><span class="err">n_{0</span><span class="p">}</span><span class="err">}\</span><span class="o">)</span><span class="w"> </span><span class="nt">where</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="nt">n_</span><span class="p">{</span><span class="err">0</span><span class="p">}</span><span class="o">=</span><span class="err">\</span><span class="nt">operatorname</span><span class="p">{</span><span class="err">argmax</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">min</span><span class="w"> </span><span class="nt">_</span><span class="p">{</span><span class="err">j=0,10</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="err">\</span><span class="o">|</span><span class="err">\</span><span class="nt">hat</span><span class="p">{</span><span class="err">\mathbf{x</span><span class="p">}</span><span class="err">}</span><span class="nt">_</span><span class="p">{</span><span class="err">t+d</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="nt">-</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">x</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">t-j</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="err">\</span><span class="o">|</span><span class="err">\</span><span class="o">)</span>
<span class="nt">end</span>
</code></pre></div>

<p>Our method (see Fig. 11(a)) covered $50 \%$ more of the maze area after 900 time-steps than random exploration. These results were obtained with 100 Monte-Carlo simulations and sequences of 6 actions (more details are given in Appendix B.4). Comparing typical paths chosen by the random explorer and by our explorer (see Fig. 11(b)), we see the our explorer has much smoother trajectories.</p>
<p>This is a good local exploration strategy that leads to faster movement through corridors. To</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Salient frames highlighting wall-layout memory after $360^{\circ}$ spin generated using our simulator with actions taken by a human player.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: (a) Average ratio over 10 mazes (shaded is the $68 \%$ confidence interval) of area visited by the random agent and an agent using our model. (b) Typical example of paths followed by (left) the random agent and (right) our agent (see the Appendix for more examples).
transform this into a good global exploration strategy, our explorer would have to be augmented with a better memory in order to avoid going down the same corridor twice. These sorts of smooth local exploration strategies could also be useful in navigation problems.</p>
<h1>4 PREDICTION-INDEPENDENT SIMULATORS</h1>
<p>A prediction-independent simulator has state transitions of the form $\mathbf{s}<em t-1="t-1">{t}=f\left(\mathbf{s}</em>$ - in the used structure this enables saving around 200 million flops at each time-step.
For the state transition, we found that a working structure was to use Eqs. (1)-(5) with $\mathbf{z}}, a_{t-1}\right)$, which therefore do not require the high-dimensional predictions. In the Atari environment, for example, this avoids having to project from the state space of dimension 1024 into the observation space of dimension 100,800 $(210 \times 160 \times 3)$ through the decoding function $\mathcal{D}$, and vice versa through the encoding function $\mathcal{C<em t="t">{t}=\mathbf{h}</em>$.}$ and with different parameters for the warm-up and prediction phases. As for the prediction-dependent simulator, we used a warm-up phase of length $\tau=10$, but we did backpropagate the gradient back to time-step five in order to learn the encoding function $\mathcal{C</p>
<p>Our analysis on Atari (see Appendix C) suggests that the prediction-independent simulator is much more sensitive to changes in the state transition structure and in the training scheme than the predictiondependent simulator. We found that using prediction length $T=15$ gave much worse long-term accuracy than with the prediction-dependent simulator. This problem could be alleviated with the use of prediction length $T=30$ through truncated backpropagation.
Fig. 12 shows a comparison of the prediction-dependent and prediction-independent simulators using $T=30$ through two subsequences of length 15 (we indicate this as BPTT(15, 2), even though in the prediction-independent simulator we did backpropagate the gradient to the warm-up phase).
When looking at the videos available at PI-Simulators, we can notice that the prediction-independent simulator tends to give worse type of long-term prediction. In Fishing Derby for example, in the long-term the model tends to create fish of smaller dimension in addition to the fish present in the real frames. Nevertheless, for some difficult games the prediction-independent simulator achieves better performance than the prediction-dependent simulator. More investigation about alternative</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12: Prediction error vs number of frames seen by the model (excluding warm-up frames) for the prediction-dependent and prediction-independent simulators using BPTT(15, 2) for (a) Bowling, Freeway, Pong and (b) Breakout, Fishing Derby, Ms Pacman, Qbert, Seaquest, Space Invaders (the prediction-dependent simulator is trained with the $0 \%-100 \%$ PDT training scheme).
state transitions and training schemes would need to be performed to obtain the same overall level of accuracy as with the prediction-dependent simulator.</p>
<h1>5 DISCUSSION</h1>
<p>In this paper we have introduced an approach to simulate action-conditional dynamics and demonstrated that is highly adaptable to different environments, ranging from Atari games to 3D car racing environments and mazes. We showed state-of-the-art results on Atari, and demonstrated the feasibility of live human play in all three task families. The system is able to capture complex and long-term interactions, and displays a sense of spatial and temporal coherence that has, to our knowledge, not been demonstrated on high-dimensional time-series data such as these.</p>
<p>We have presented an in-deep analysis on the effect of different training approaches on short and longterm prediction capabilities, and showed that moving towards schemes in which the simulator relies less on past observations to form future predictions has the effect on focussing model resources on learning the global dynamics of the environment, leading to dramatic improvements in the long-term predictions. However, this requires a distribution of resources that impacts short-term performance, which can be harmful to the overall performance of the model for some games. This trade-off is also causing the model to be less robust to states of the environment not seen during training. To alleviate this problem would require the design of more sophisticated model architectures than the ones considered here. Whilst it is also expected that more ad-hoc architectures would be less sensitive</p>
<p>to different training approaches, we believe that guiding the noise as well as teaching the model to make use of past information through the objective function would still be beneficial for improving long-term prediction.</p>
<p>Complex environments have compositional structure, such as independently moving objects and other phenomena that only rarely interact. In order for our simulators to better capture this compositional structure, we may need to develop specialised functional forms and memory stores that are better suited to dealing with independent representations and their interlinked interactions and relationships. More homogeneous deep network architectures such as the one presented here are clearly not optimal for these domains, as can be seen in Atari environments such as Ms Pacman where the system has trouble keeping track of multiple independently moving ghosts. Whilst the LSTM memory and our training scheme have proven to capture long-term dependencies, alternative memory structures are required in order, for example, to learn spatial coherence at a more global level than the one displayed by our model in the 3D mazes in oder to do navigation.</p>
<p>In the case of action-conditional dynamics, the policy-induced data distribution does not cover the state space and might in fact be nonstationary over an agent lifetime. This can cause some regions of the state space to be oversampled, whereas the regions we might actually care about the most - those just around the agent policy state distribution - to be underrepresented. In addition, this induces biases in the data that will ultimately not enable the model learn the environment dynamics correctly. As verified from the experiments in this paper, both on live human play and model-based exploration, this problem is not yet as pressing as might be expected in some environments. However, our simulators displayed limitations and faults due to the specificities of the training data, such as for example predicting an event based on the recognition of a particular sequence of actions always co-occurring with this event in the training data rather than on the recognition of the real causes.</p>
<p>Finally, a limitation of our approach is that, however capable it might be, it is a deterministic model designed for deterministic environments. Clearly most real world environments involve noisy state transitions, and future work will have to address the extension of the techniques developed in this paper to more generative temporal models.</p>
<h1>ACKNOWLEDGMENTS</h1>
<p>The authors would like to thank David Barber for helping with the graphical model interpretation, Alex Pritzel for preparing the DQN data, Yori Zwols and Frederic Besse for helping with the implementation of the model, and Oriol Vinyals, Yee Whye Teh, Junhyuk Oh, and the anonymous reviewers for useful discussions and feedback on the manuscript.</p>
<h2>REFERENCES</h2>
<p>P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem. Machine Learning, 47:235-256, 2002.
C. Beattie, J. Z. Leibo, D. Teplyashin, T. Ward, M. Wainwright, H. Küttler, A. Lefrancq, S. Green, V. Valdés, A. Sadik, J. Schrittwieser, K. Anderson, S. York, M. Cant, A. Cain, A. Bolton, S. Gaffney, H. King, D. Hassabis, S. Legg, and S. Petersen. Deepmind lab. CoRR, abs/1612.03801, 2016. URL http://arxiv. org/abs/1612.03801.
M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The Arcade Learning Environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253-279, 2013.
S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. In Advances in Neural Information Processing Systems 28 (NIPS), pp. 1171-1179. 2015.
A. Graves. Generating sequences with recurrent neural networks. 2013. URL http://arxiv.org/abs/ 1308.0850 .
S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735-1780, 1997.
M. Lengyel and P. Dayan. Hippocampal contributions to control: The third way. In Advances in Neural Information Processing Systems 20 (NIPS), pp. 889-896, 2008.
M. L. Littman, R. S. Sutton, and S. Singh. Predictive representations of state. In Advances in Neural Information Processing Systems 14 (NIPS), pp. 1555-1561. 2002.
M. McCloskey. Intuitive physics. Scientific American, 248(4):122-130, 1983.
V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540): 529-533, 02 2015. URL http://dx.doi.org/10.1038/nature14236.</p>
<p>V. Mnih, A. Puigdomènech Badia, M. Mirza, A. Graves, T. P Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Proceedings of the 33rd International Conference on Machine Learning (ICML), 2016.
Y. Niv. Reinforcement learning in the brain. Journal of Mathematical Psychology, 53(3):139-154, 2009.
J. Oh, X. Guo, H. Lee, R. L. Lewis, and S. P. Singh. Action-conditional video prediction using deep networks in Atari games. In Advances in Neural Information Processing Systems 28 (NIPS), pp. 2863-2871. 2015. URL http://arxiv.org/abs/1507.08750.
J. K. O’Regan and A. Noë. A sensorimotor account of vision and visual consciousness. Behavioral and brain sciences, 24(05):939-973, 2001.
P.-Y. Oudeyer, F. Kaplan, and V. V. Hafner. Intrinsic motivation systems for autonomous mental development. Evolutionary Computation, IEEE Transactions on, 11(2):265-286, 2007.
V. Patraucean, A. Handa, and R. Cipolla. Spatio-temporal video autoencoder with differentiable memory. CoRR, abs/1511.06309, 2015. URL http://arxiv.org/abs/1511.06309.
J. Pearl. Causality. Cambridge University Press, 2009.
N. Srivastava, E. Mansimov, and R. Salakhutdinov. Unsupervised learning of video representations using LSTMs. In Proceedings of the 32nd International Conference on Machine Learning (ICML), pp. 843-852, 2015.
W. Sun, A. Venkatraman, B. Boots, and J. A. Bagnell. Learning to filter with predictive state inference machines. CoRR, abs/1512.08836, 2015. URL http://arxiv.org/abs/1512.08836.
R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT Press, 1998.
E. Talvitie. Model regularization for stable sample rollouts. In Proceedings of the Thirtieth Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI-14), pp. 780-789, 2014.
N. Wahlström, T. B. Schön, and M. P. Deisenroth. From pixels to torques: Policy learning with deep dynamical models. CoRR, abs/1502.02251, 2015. URL http://arxiv.org/abs/1502.02251.
M. Watter, J. Springenberg, J. Boedecker, and M. Riedmiller. Embed to control: A locally linear latent dynamics model for control from raw images. In Advances in Neural Information Processing Systems 28 (NIPS), pp. $2728-2736,2015$.
R. J. Williams and D. Zipser. Gradient-based learning algorithms for recurrent networks and their computational complexity. Bibliometrics, pp. 433-486, 1995.
B. Wymann, E. Espié, C. Guionneau, C. Dimitrakakis, R. Coulom, and A. Sumner. Torcs: The open racing car simulator, v1.3.5. 2013. URL http://www.torcs.org.
B. Xu, N. Wang, T. Chen, and M. Li. Empirical evaluation of rectified activations in convolutional network. 2015.
W. Zaremba, I. Sutskever, and O. Vinyals. Recurrent neural network regularization. CoRR, abs/1409.2329, 2014. URL http://arxiv.org/abs/1409.2329.</p>
<h1>A Data, Preprocessing and Training Algorithm</h1>
<p>When generating the data, each selected action was repeated for 4 time-steps and only the 4th frame was recorded for the analysis. The RGB images were preprocessed by subtracting mean pixel values (calculated separately for each color channel and over an initial set of 2048 frames only) and by dividing each pixel value by 255 .
As stochastic gradient algorithm, we used centered RMSProp (Graves, 2013) with learning rate ${ }^{4}$ 1e-5, epsilon 0.01 , momentum 0.9 , decay 0.95 , and mini-batch size 16 . The model was implemented in Torch, using the default initialization of the parameters. The state $\mathbf{s}_{1}$ was initialized to zero.</p>
<h2>B Prediction-Dependent Simulators</h2>
<p>As baseline for the single-step simulators we used the following state transition:
Encoding: $\mathbf{z}<em t-1="t-1">{t-1}=\mathcal{C}\left(\mathbb{I}\left(\dot{\mathbf{x}}</em>}, \mathbf{x<em t="t">{t-1}\right)\right)$,
Action fusion: $\mathbf{v}</em>}=\mathbf{W}^{h} \mathbf{h<em t-1="t-1">{t-1} \otimes \mathbf{W}^{o} \mathbf{a}</em>$,
Gate update: $\mathbf{i}<em t="t">{t}=\sigma\left(\mathbf{W}^{i v} \mathbf{v}</em>}+\mathbf{W}^{i z} \mathbf{z<em t="t">{t-1}\right), \mathbf{f}</em>}=\sigma\left(\mathbf{W}^{f v} \mathbf{v<em t-1="t-1">{t}+\mathbf{W}^{f s} \mathbf{z}</em>\right)$,</p>
<p>$$
\mathbf{o}<em t="t">{t}=\sigma\left(\mathbf{W}^{o v} \mathbf{v}</em>\right)
$$}+\mathbf{W}^{o z} \mathbf{z}_{t-1</p>
<p>Cell update: $\mathbf{c}<em t="t">{t}=\mathbf{f}</em>} \otimes \mathbf{c<em t="t">{t-1}+\mathbf{i}</em>} \otimes \tanh \left(\mathbf{W}^{c v} \mathbf{v<em t-1="t-1">{t}+\mathbf{W}^{c z} \mathbf{z}</em>\right)$,
State update: $\mathbf{h}<em t="t">{t}=\mathbf{o}</em>} \otimes \tanh \left(\mathbf{c<em t-1="t-1">{t}\right)$,
with vectors $\mathbf{h}</em>$ of dimension 1024 and 2048 respectively.}$ and $\mathbf{v}_{t</p>
<h2>B. 1 ATARI</h2>
<p>We used a trained DQN agent (the scores are given in the table on the right) to generate training and test datasets consisting of 5,000,000 and 1,000,000 $(210 \times 160)$ RGB images respectively, with actions chosen according to an $\epsilon=0.2$-greedy policy. Such a large number of training frames was necessary to prevent our simulators from strongly overfitting to the training data. This would be the case with, for example, one million training frames, as shown in Fig. 13 (the corresponding video can be seen at MSPacman). The ghosts are in frightened mode at time-step 1 (first image), and have returned to chase mode at time-step 63 (second image). The simulator is able to predict the exact time of return to the chase mode without sufficient history, which suggests that the sequence was memorized.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Game Name</th>
<th style="text-align: left;">DQN Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Bowling</td>
<td style="text-align: left;">51.84</td>
</tr>
<tr>
<td style="text-align: left;">Breakout</td>
<td style="text-align: left;">396.25</td>
</tr>
<tr>
<td style="text-align: left;">Fishing Derby</td>
<td style="text-align: left;">19.30</td>
</tr>
<tr>
<td style="text-align: left;">Freeway</td>
<td style="text-align: left;">33.38</td>
</tr>
<tr>
<td style="text-align: left;">Ms Pacman</td>
<td style="text-align: left;">2963.31</td>
</tr>
<tr>
<td style="text-align: left;">Pong</td>
<td style="text-align: left;">20.88</td>
</tr>
<tr>
<td style="text-align: left;">Qbert</td>
<td style="text-align: left;">$14,865.43$</td>
</tr>
<tr>
<td style="text-align: left;">Riverraid</td>
<td style="text-align: left;">$13,593.49$</td>
</tr>
<tr>
<td style="text-align: left;">Seaquest</td>
<td style="text-align: left;">$17,250.31$</td>
</tr>
<tr>
<td style="text-align: left;">Space Invaders</td>
<td style="text-align: left;">2952.09</td>
</tr>
</tbody>
</table>
<p>The encoding consisted of 4 convolutional layers with $64,32,32$ and 32 filters, of size $8 \times 8,6 \times 6$, $6 \times 6$, and $4 \times 4$, stride 2 , and padding $0,1,1,0$ and $1,1,1,0$ for the height and width respectively. Every layer was followed by a randomized rectified linear function (RReLU) (Xu et al., 2015) with parameters $l=1 / 8, u=1 / 3$. The output tensor of the convolutional layers of dimension $32 \times 11 \times 8$ was then flattened into the vector $\mathbf{z}_{t}$ of dimension 2816 . The decoding consisted of one fully-connected layer with 2816 hidden units followed by 4 full convolutional layers with the inverse symmetric structure of the encoding transformation: $32,32,32$ and 64 filters, of size $4 \times 4,6 \times 6$, $6 \times 6$, and $8 \times 8$, stride 2 , and padding $0,1,1,0$ and $0,1,1,1$. Each full convolutional layer (except the last one) was followed by a RReLU.</p>
<p>In Fig. 14, we show one example of successful prediction at time-steps 100 and 200 for each game.</p>
<h2>B.1.1 Short-Term Versus Long-Term Accuracy</h2>
<p>In Figures 15-19, we show the prediction error obtained with the training schemes described in Sec. 3.1 for all games. Below we discuss the main findings for each game.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 13: Prediction that demonstrates overfitting of the model when trained on one million frames.</p>
<p>Bowling. Bowling is one of the easiest games to model. A simulator trained using only observationdependent transitions gives quite accurate predictions. However, using only prediction-dependent transitions reduces the error in updating the score and predicting the ball direction.</p>
<p>Breakout. Breakout is a difficult game to model. A simulator trained with only predictiondependent transitions predicts the paddle movement very accurately but almost always fails to represent the ball. A simulator trained with only observation-dependent transitions struggles much less to represent the ball but does not predict the paddle and ball positions as accurately, and the ball also often disappears after hitting the paddle. Interestingly, the long-term prediction error (bottomright of Fig. 15(b)) for the $100 \%$ PDT training scheme is the lowest, as when not representing the ball the predicted frames look closer to the real frames than when representing the ball incorrectly. A big improvement in the model ability to represent the ball could be obtained by pre-processing the frames with max-pooling as done for DQN, as this increases the ball size. We believe that a more sophisticated convolutional structure would be even more effective, but did not succeed in discovering such a structure.</p>
<p>Fishing Derby. In Fishing Derby, long-term accuracy is disastrous with the 0\%PDT training scheme and good with the $100 \%$ PDT training scheme. Short-term accuracy is better with schemes using more observation-dependent transitions than in the $100 \%$ or $0 \%-100 \%$ PDT training schemes, especially at low numbers of parameter updates.</p>
<p>Freeway. With Bowling, Freeway is one of the easiest games to model, but more parameter updates are required for convergence than for Bowling. The $0 \%$ PDT training scheme gives good accuracy, although sometimes the chicken disappears or its position is incorrectly predicted - this happens extremely rarely with the $100 \%$ PDT training scheme. In both schemes, the score is often wrongly updated in the warning phase.</p>
<p>Ms Pacman. Ms Pacman is a very difficult game to model and accurate prediction can only be obtained for a few time-steps into the future. The movement of the ghosts, especially when in frightened mode, is regulated by the position of Ms Pacman according to complex rules. Furthermore, the DQN $\epsilon=0.2$-greedy policy does not enable the agent to explore certain regions of the state space. As a result, the simulator can predict well the movement of Ms Pacman, but fails to predict long-term the movement of the ghosts when in frightened mode or when in chase mode later in the episodes.</p>
<p>Pong. With the $0 \%$ PDT training scheme, the model often incorrectly predicts the direction of the ball when hit by the agent or by the opponent. Quite rarely, the ball disappears when hit by the agent. With the $100 \%$ PDT training scheme, the direction the ball is much more accurately predicted, but the ball more often disappears when hit by the agent, and the ball and paddles are generally less sharp.</p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 14: One example of 200 time-step ahead prediction for each of the 10 Atari games. Displayed are predicted (left) and real (right) frames at time-steps 100 and 200.</p>
<p>Qbert. Qbert is a game for which the $0 \%$ PDT training scheme is unable to predict accurately beyond very short-term, as after a few frames only the background is predicted. The more predictiondependent transitions are used, the less sharply the agent and the moving objects are represented.</p>
<p>Riverraid. In Riverraid, prediction with the $0 \%$ PDT training scheme is very poor, as this scheme causes no generation of new objects or background. With all schemes, the model fails to predict the frames that follow a jet loss - that's why the prediction error increases sharply after around time-step 13 in Fig. 18(b). The long-term prediction error is lower with the $100 \%$ PDT training scheme, as with this scheme the simulator is more accurate before, and sometimes after, a jet loss. The problem of incorrect prediction after a jet loss disappears when using BBTT(15,2) with prediction-dependent transitions.</p>
<p>Seaquest. In Seaquest, with the $0 \%$ PDT training scheme, the existing fish disappears after a few time-steps and no new fish ever appears from the sides of the frame. The higher the number of prediction-dependent transitions the less sharply the fish is represented, but the more accurately its dynamics and appearance from the sides of the frame can be predicted.</p>
<p>Space Invaders Space Invaders is a very difficult game to model and accurate prediction can only be obtained for a few time-steps into the future. The $0 \%$ PDT training scheme is unable to predict accurately beyond very short-term. The $100 \%$ PDT training scheme struggles to represent the bullets.</p>
<p>In Figs. 20-24 we show the effect of using different prediction lengths $T \leq 20$ with the training schemes $0 \%$ PDT, $67 \%$ PDT, and $100 \%$ PDT for all games.</p>
<p>In Figs. 25-29 we show the effect of using different prediction lengths $T&gt;20$ through truncated backpropagation with the training schemes $0 \%$ PDT, $33 \%$ PDT, and $100 \%$ PDT for all games.</p>
<p><img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure 15: Prediction error (average over 10,000 sequences) for different training schemes on (a) Bowling and (b) Breakout. Number of frames is in millions.</p>
<p><img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Figure 16: Prediction error for different training schemes on (a) Fishing Derby and (b) Freeway.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ We found that using a higher learning rate value of $2 \mathrm{e}-5$ would generally increase convergence speed but cause major instability issues, suggesting that gradient clipping would need to be used.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
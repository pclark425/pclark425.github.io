<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4763 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4763</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4763</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-105.html">extraction-schema-105</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <p><strong>Paper ID:</strong> paper-266191124</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.07622v3.pdf" target="_blank">Mathematical Language Models: A Survey</a></p>
                <p><strong>Paper Abstract:</strong> — In recent years, there has been remarkable progress in leveraging Language Models (LMs), encompassing Pre-trained Language Models (PLMs) and Large-scale Language Models (LLMs), within the domain of mathematics. This paper conducts a comprehensive survey of mathematical LMs, systematically categorizing pivotal research endeavors from two distinct perspectives: tasks and methodologies. The landscape reveals a large number of proposed mathematical LLMs, which are further delineated into instruction learning, tool-based methods, fundamental CoT techniques, and advanced CoT methodologies. In addition, our survey entails the compilation of over 60 mathematical datasets, including training datasets, benchmark datasets, and augmented datasets. Addressing the primary challenges and delineating future trajectories within the field of mathematical LMs, this survey is positioned as a valuable resource, poised to facilitate and inspire future innovation among researchers invested in advancing this domain.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4763.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4763.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (ensemble of reasoning paths with majority voting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble-style decoding strategy that samples multiple chain-of-thought (CoT) reasoning paths from an LLM and chooses the final answer by majority (consensus) voting over the sampled answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large LLMs (e.g., PaLM-class / general LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied to large-scale transformer LLMs in the CoT/in-context learning setting; the survey references use with very large models such as PaLM (540B) in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-Consistency (multiple sampled CoTs + majority vote)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Generate multiple independent chain-of-thought traces by sampling (stochastic decoding) and aggregate the final answers via majority vote to pick the most consistent answer across diverse reasoning trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Math reasoning benchmarks (e.g., GSM8K referenced in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-step grade-school math word problems used to evaluate CoT reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Survey: qualitative improvement reported when using multiple sampled reasoning paths and majority voting; the survey does not supply a numeric metric for Self-Consistency itself (original papers show notable accuracy gains on GSM8K).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Single-shot or single-chain CoT (single deterministic chain) — survey reports that aggregating multiple chains (Self-Consistency) yields better performance than relying on a single sampled chain, but gives no numeric values in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Sampling multiple reasoning chains and taking a majority vote (Self-Consistency) reduces the impact of individual stochastic mistakes and typically improves accuracy compared to relying on a single chain-of-thought; the survey presents Self-Consistency as a leading approach in ensemble/diverse strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Survey notes limitations of simple voting approaches (they may ignore useful information present in unsuccessful reasoning chains) and that voting alone does not exploit the internal content of failed chains to improve answers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mathematical Language Models: A Survey', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4763.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4763.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Diversity-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Diversity-of-Thought (prompt-variation to produce diverse reasoning chains)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that intentionally produces diverse reasoning paths by altering prompts or demonstration selection so that the LLM explores varied solution strategies, then aggregates answers (e.g., by majority vote).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Diversity of thought improves reasoning abilities of large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large LLMs (general)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Survey cites application of this idea to large LLMs; the approach modifies prompts/demonstrations rather than model weights to induce varied reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Diversity-of-Thought (prompt variation + aggregation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Create diversity by changing prompts, few-shot exemplars, or sampling strategies to elicit multiple qualitatively different CoT traces for the same problem; aggregate outputs (commonly by majority voting) to choose final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Complex reasoning tasks referenced in survey (e.g., arithmetic, commonsense, symbolic reasoning benchmarks such as GSM8K/MATH mentioned broadly)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmarks that require multi-step reasoning and can benefit from exploring multiple solving strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Survey: reported as beneficial — generating diverse reasoning paths and aggregating typically improves reasoning robustness; no single numeric performance value provided in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Less-diverse or single-prompt CoT — survey reports that diversity-of-thought aggregation typically outperforms relying on a single prompt/chain, though exact metrics are not supplied in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Inducing diversity in reasoning (via prompt variation or different exemplars) and aggregating results improves LLM reasoning performance vs. using a single, similar reasoning chain; diversity is an effective lever for robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Survey comments that simple voting can miss useful partial information in unsuccessful chains; diversity alone requires appropriate aggregation (voting or more advanced consolidation) to realize gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mathematical Language Models: A Survey', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4763.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4763.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DIVERSE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DIVERSE (verify-and-vote style approach described in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A verify-and-vote style method that independently verifies each reasoning step and aggregates via a weighted voting mechanism to filter incorrect answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs with step verification capability (e.g., GPT-4 Code Interpreter referenced elsewhere in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Survey refers to methods that combine LLM step verification with aggregation; specific model instances are left to cited works.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>DIVERSE (per-step verification + weighted voting)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Verify each step of independently generated reasoning chains and perform a weighted vote that favors chains with more verified steps or higher per-step confidence, thereby filtering incorrect answers more robustly than simple majority voting.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Complex deductive reasoning tasks (general reasoning benchmarks referenced in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where stepwise correctness can be inferred or checked (e.g., multi-step mathematical reasoning requiring each step to depend logically on previous steps).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Survey: described as reducing incorrect answers by verifying individual steps and using weighted voting; no numeric performance reported in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Simple majority-vote over final answers or unverified chains — DIVERSE is reported to be more effective by using per-step verification, though the survey provides no quantitative comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Per-step verification combined with weighted aggregation can improve robustness beyond naive majority vote by down-weighting chains with unverifiable or incorrect steps.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Survey warns that sophisticated voting still may not exploit the full information in unsuccessful chains and that verifying entire deductive processes remains challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mathematical Language Models: A Survey', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4763.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4763.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Voting / Ensemble-based Methods (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ensemble-based aggregation (voting, ranking, averaging over multiple reasoning outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of methods that generate multiple reasoning trajectories (via sampling or prompt variations) and aggregate them using majority vote, weighted vote, or learned rankers to improve final-answer reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various LLMs (PaLM, GPT-family, LLaMA-class etc. as discussed in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Ensemble aggregation is applied on top of LLM outputs regardless of backbone; survey references use across PaLM, GPT series and open models.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Ensemble aggregation (majority vote, weighted vote, ranking)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Produce multiple answers/reasoning chains (via sampling, prompt perturbation, or different demonstrations) and combine them with majority voting, weighted voting, or learned ranking/discriminator to choose the final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Mathematical and multi-step reasoning benchmarks (GSM8K, MATH, and other reasoning tasks referenced generally)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmarks that benefit from robustness gained by aggregating multiple candidate solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Survey: ensemble/voting approaches are reported to yield more substantial performance enhancements than single-run outputs (qualitative); no unified numeric metric reported in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Single-sample / single-chain outputs — ensembles typically outperform single outputs according to the survey, although exact numbers depend on model and task.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Ensembling multiple reasoning outputs and aggregating (via voting or ranking) reliably improves performance relative to single-chain outputs; ranking or discriminators can further improve over naive voting by selecting higher-quality chains.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Simple voting can ignore partial correctness in non-majority chains and fails to integrate useful substeps from losing chains; some methods therefore go beyond voting to consolidate facts across chains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mathematical Language Models: A Survey', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4763.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4763.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Single-chain CoT (baseline similar reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Single Chain-of-Thought (single deterministic reasoning trace)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The baseline approach where the model produces a single chain-of-thought (often deterministically) and returns the final answer without aggregating multiple traces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (GPT-family, PaLM, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Typically applied in in-context learning or few-shot CoT prompting; the LLM outputs one reasoning trace per prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Single chain-of-thought</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Produce a single step-by-step chain-of-thought (via deterministic decoding or single-sample generation) and use its final answer as output; lacks ensemble diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Math word problems and symbolic reasoning (GSM8K and similar datasets referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-step problems where a single reasoning path must be correct end-to-end.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Survey: single-chain CoT improves over zero-shot single-step prompting but is generally outperformed by diverse/ensemble approaches (as reported qualitatively in the survey); no numeric metrics provided in survey for direct comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Ensemble/diverse methods such as Self-Consistency or Diversity-of-Thought — survey reports ensembles generally improve performance over single-chain CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A single CoT can be effective but is vulnerable to stochastic errors in reasoning; aggregating multiple chains or inducing diversity tends to be more robust.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Survey notes that increasing complexity or length of a single CoT is not always sufficient — methods that sample and aggregate can better mitigate random erroneous steps.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mathematical Language Models: A Survey', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4763.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4763.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Check / MCR / Consolidation methods</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Check and MCR (methods that integrate reasoning steps across chains)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that go beyond simple voting by incorporating reasoning steps into the aggregation process (self-check) or consolidating facts across multiple chains (MCR) to improve answer reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-check: Using llms to zero-shot check their own step-by-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (general)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Methods operate over outputs of LLMs and typically use additional LLM calls or discriminators to inspect and integrate reasoning content.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-Check / MCR (consolidation over multiple chains)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Self-Check: incorporate step-by-step reasoning into the verification/aggregation so the voting accounts for internal consistency; MCR: consolidate facts extracted from multiple chains to build a more complete evidence set used for final decision.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Complex multi-step reasoning tasks referenced in survey</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Problems where intermediate steps contain informative partial results that can be combined or validated to reach a correct final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Survey: presented as improvements over simple voting by using internal chain content; the survey provides qualitative descriptions but no numeric performance values.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Simple majority-vote over final answers — Self-Check and MCR reported to be more reliable because they use step-level information rather than only final-answer votes.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Incorporating step-level verification or consolidating intermediate facts across chains can address shortcomings of naive ensemble voting and yield better reliability and interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Survey cautions that verifying whole deductive chains is still hard and these methods depend on accurate verification components; improvements are qualitative in the survey and task- and model-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mathematical Language Models: A Survey', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-consistency improves chain of thought reasoning in language models. <em>(Rating: 2)</em></li>
                <li>Diversity of thought improves reasoning abilities of large language models. <em>(Rating: 2)</em></li>
                <li>Verify-and-edit: A knowledge-enhanced chain-of-thought framework. <em>(Rating: 1)</em></li>
                <li>Self-check: Using llms to zero-shot check their own step-by-step reasoning. <em>(Rating: 2)</em></li>
                <li>Answering questions by meta-reasoning over multiple chains of thought. <em>(Rating: 1)</em></li>
                <li>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. <em>(Rating: 1)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4763",
    "paper_id": "paper-266191124",
    "extraction_schema_id": "extraction-schema-105",
    "extracted_data": [
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency (ensemble of reasoning paths with majority voting)",
            "brief_description": "An ensemble-style decoding strategy that samples multiple chain-of-thought (CoT) reasoning paths from an LLM and chooses the final answer by majority (consensus) voting over the sampled answers.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models.",
            "mention_or_use": "mention",
            "model_name": "Large LLMs (e.g., PaLM-class / general LLMs)",
            "model_description": "Applied to large-scale transformer LLMs in the CoT/in-context learning setting; the survey references use with very large models such as PaLM (540B) in related work.",
            "reasoning_method_name": "Self-Consistency (multiple sampled CoTs + majority vote)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Generate multiple independent chain-of-thought traces by sampling (stochastic decoding) and aggregate the final answers via majority vote to pick the most consistent answer across diverse reasoning trajectories.",
            "task_name": "Math reasoning benchmarks (e.g., GSM8K referenced in survey)",
            "task_description": "Multi-step grade-school math word problems used to evaluate CoT reasoning performance.",
            "performance": "Survey: qualitative improvement reported when using multiple sampled reasoning paths and majority voting; the survey does not supply a numeric metric for Self-Consistency itself (original papers show notable accuracy gains on GSM8K).",
            "comparison_with_other_method": true,
            "performance_other_method": "Single-shot or single-chain CoT (single deterministic chain) — survey reports that aggregating multiple chains (Self-Consistency) yields better performance than relying on a single sampled chain, but gives no numeric values in the survey.",
            "key_findings": "Sampling multiple reasoning chains and taking a majority vote (Self-Consistency) reduces the impact of individual stochastic mistakes and typically improves accuracy compared to relying on a single chain-of-thought; the survey presents Self-Consistency as a leading approach in ensemble/diverse strategies.",
            "counter_examples_or_negative_results": "Survey notes limitations of simple voting approaches (they may ignore useful information present in unsuccessful reasoning chains) and that voting alone does not exploit the internal content of failed chains to improve answers.",
            "uuid": "e4763.0",
            "source_info": {
                "paper_title": "Mathematical Language Models: A Survey",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Diversity-of-Thought",
            "name_full": "Diversity-of-Thought (prompt-variation to produce diverse reasoning chains)",
            "brief_description": "A method that intentionally produces diverse reasoning paths by altering prompts or demonstration selection so that the LLM explores varied solution strategies, then aggregates answers (e.g., by majority vote).",
            "citation_title": "Diversity of thought improves reasoning abilities of large language models.",
            "mention_or_use": "mention",
            "model_name": "Large LLMs (general)",
            "model_description": "Survey cites application of this idea to large LLMs; the approach modifies prompts/demonstrations rather than model weights to induce varied reasoning.",
            "reasoning_method_name": "Diversity-of-Thought (prompt variation + aggregation)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Create diversity by changing prompts, few-shot exemplars, or sampling strategies to elicit multiple qualitatively different CoT traces for the same problem; aggregate outputs (commonly by majority voting) to choose final answer.",
            "task_name": "Complex reasoning tasks referenced in survey (e.g., arithmetic, commonsense, symbolic reasoning benchmarks such as GSM8K/MATH mentioned broadly)",
            "task_description": "Benchmarks that require multi-step reasoning and can benefit from exploring multiple solving strategies.",
            "performance": "Survey: reported as beneficial — generating diverse reasoning paths and aggregating typically improves reasoning robustness; no single numeric performance value provided in the survey text.",
            "comparison_with_other_method": true,
            "performance_other_method": "Less-diverse or single-prompt CoT — survey reports that diversity-of-thought aggregation typically outperforms relying on a single prompt/chain, though exact metrics are not supplied in the survey.",
            "key_findings": "Inducing diversity in reasoning (via prompt variation or different exemplars) and aggregating results improves LLM reasoning performance vs. using a single, similar reasoning chain; diversity is an effective lever for robustness.",
            "counter_examples_or_negative_results": "Survey comments that simple voting can miss useful partial information in unsuccessful chains; diversity alone requires appropriate aggregation (voting or more advanced consolidation) to realize gains.",
            "uuid": "e4763.1",
            "source_info": {
                "paper_title": "Mathematical Language Models: A Survey",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "DIVERSE",
            "name_full": "DIVERSE (verify-and-vote style approach described in survey)",
            "brief_description": "A verify-and-vote style method that independently verifies each reasoning step and aggregates via a weighted voting mechanism to filter incorrect answers.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LLMs with step verification capability (e.g., GPT-4 Code Interpreter referenced elsewhere in survey)",
            "model_description": "Survey refers to methods that combine LLM step verification with aggregation; specific model instances are left to cited works.",
            "reasoning_method_name": "DIVERSE (per-step verification + weighted voting)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Verify each step of independently generated reasoning chains and perform a weighted vote that favors chains with more verified steps or higher per-step confidence, thereby filtering incorrect answers more robustly than simple majority voting.",
            "task_name": "Complex deductive reasoning tasks (general reasoning benchmarks referenced in survey)",
            "task_description": "Tasks where stepwise correctness can be inferred or checked (e.g., multi-step mathematical reasoning requiring each step to depend logically on previous steps).",
            "performance": "Survey: described as reducing incorrect answers by verifying individual steps and using weighted voting; no numeric performance reported in the survey.",
            "comparison_with_other_method": true,
            "performance_other_method": "Simple majority-vote over final answers or unverified chains — DIVERSE is reported to be more effective by using per-step verification, though the survey provides no quantitative comparison.",
            "key_findings": "Per-step verification combined with weighted aggregation can improve robustness beyond naive majority vote by down-weighting chains with unverifiable or incorrect steps.",
            "counter_examples_or_negative_results": "Survey warns that sophisticated voting still may not exploit the full information in unsuccessful chains and that verifying entire deductive processes remains challenging.",
            "uuid": "e4763.2",
            "source_info": {
                "paper_title": "Mathematical Language Models: A Survey",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Voting / Ensemble-based Methods (general)",
            "name_full": "Ensemble-based aggregation (voting, ranking, averaging over multiple reasoning outputs)",
            "brief_description": "A class of methods that generate multiple reasoning trajectories (via sampling or prompt variations) and aggregate them using majority vote, weighted vote, or learned rankers to improve final-answer reliability.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Various LLMs (PaLM, GPT-family, LLaMA-class etc. as discussed in survey)",
            "model_description": "Ensemble aggregation is applied on top of LLM outputs regardless of backbone; survey references use across PaLM, GPT series and open models.",
            "reasoning_method_name": "Ensemble aggregation (majority vote, weighted vote, ranking)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Produce multiple answers/reasoning chains (via sampling, prompt perturbation, or different demonstrations) and combine them with majority voting, weighted voting, or learned ranking/discriminator to choose the final answer.",
            "task_name": "Mathematical and multi-step reasoning benchmarks (GSM8K, MATH, and other reasoning tasks referenced generally)",
            "task_description": "Benchmarks that benefit from robustness gained by aggregating multiple candidate solutions.",
            "performance": "Survey: ensemble/voting approaches are reported to yield more substantial performance enhancements than single-run outputs (qualitative); no unified numeric metric reported in the survey.",
            "comparison_with_other_method": true,
            "performance_other_method": "Single-sample / single-chain outputs — ensembles typically outperform single outputs according to the survey, although exact numbers depend on model and task.",
            "key_findings": "Ensembling multiple reasoning outputs and aggregating (via voting or ranking) reliably improves performance relative to single-chain outputs; ranking or discriminators can further improve over naive voting by selecting higher-quality chains.",
            "counter_examples_or_negative_results": "Simple voting can ignore partial correctness in non-majority chains and fails to integrate useful substeps from losing chains; some methods therefore go beyond voting to consolidate facts across chains.",
            "uuid": "e4763.3",
            "source_info": {
                "paper_title": "Mathematical Language Models: A Survey",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Single-chain CoT (baseline similar reasoning)",
            "name_full": "Single Chain-of-Thought (single deterministic reasoning trace)",
            "brief_description": "The baseline approach where the model produces a single chain-of-thought (often deterministically) and returns the final answer without aggregating multiple traces.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LLMs (GPT-family, PaLM, etc.)",
            "model_description": "Typically applied in in-context learning or few-shot CoT prompting; the LLM outputs one reasoning trace per prompt.",
            "reasoning_method_name": "Single chain-of-thought",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Produce a single step-by-step chain-of-thought (via deterministic decoding or single-sample generation) and use its final answer as output; lacks ensemble diversity.",
            "task_name": "Math word problems and symbolic reasoning (GSM8K and similar datasets referenced)",
            "task_description": "Multi-step problems where a single reasoning path must be correct end-to-end.",
            "performance": "Survey: single-chain CoT improves over zero-shot single-step prompting but is generally outperformed by diverse/ensemble approaches (as reported qualitatively in the survey); no numeric metrics provided in survey for direct comparisons.",
            "comparison_with_other_method": true,
            "performance_other_method": "Ensemble/diverse methods such as Self-Consistency or Diversity-of-Thought — survey reports ensembles generally improve performance over single-chain CoT.",
            "key_findings": "A single CoT can be effective but is vulnerable to stochastic errors in reasoning; aggregating multiple chains or inducing diversity tends to be more robust.",
            "counter_examples_or_negative_results": "Survey notes that increasing complexity or length of a single CoT is not always sufficient — methods that sample and aggregate can better mitigate random erroneous steps.",
            "uuid": "e4763.4",
            "source_info": {
                "paper_title": "Mathematical Language Models: A Survey",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Self-Check / MCR / Consolidation methods",
            "name_full": "Self-Check and MCR (methods that integrate reasoning steps across chains)",
            "brief_description": "Approaches that go beyond simple voting by incorporating reasoning steps into the aggregation process (self-check) or consolidating facts across multiple chains (MCR) to improve answer reliability.",
            "citation_title": "Self-check: Using llms to zero-shot check their own step-by-step reasoning.",
            "mention_or_use": "mention",
            "model_name": "LLMs (general)",
            "model_description": "Methods operate over outputs of LLMs and typically use additional LLM calls or discriminators to inspect and integrate reasoning content.",
            "reasoning_method_name": "Self-Check / MCR (consolidation over multiple chains)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Self-Check: incorporate step-by-step reasoning into the verification/aggregation so the voting accounts for internal consistency; MCR: consolidate facts extracted from multiple chains to build a more complete evidence set used for final decision.",
            "task_name": "Complex multi-step reasoning tasks referenced in survey",
            "task_description": "Problems where intermediate steps contain informative partial results that can be combined or validated to reach a correct final answer.",
            "performance": "Survey: presented as improvements over simple voting by using internal chain content; the survey provides qualitative descriptions but no numeric performance values.",
            "comparison_with_other_method": true,
            "performance_other_method": "Simple majority-vote over final answers — Self-Check and MCR reported to be more reliable because they use step-level information rather than only final-answer votes.",
            "key_findings": "Incorporating step-level verification or consolidating intermediate facts across chains can address shortcomings of naive ensemble voting and yield better reliability and interpretability.",
            "counter_examples_or_negative_results": "Survey cautions that verifying whole deductive chains is still hard and these methods depend on accurate verification components; improvements are qualitative in the survey and task- and model-dependent.",
            "uuid": "e4763.5",
            "source_info": {
                "paper_title": "Mathematical Language Models: A Survey",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models.",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Diversity of thought improves reasoning abilities of large language models.",
            "rating": 2,
            "sanitized_title": "diversity_of_thought_improves_reasoning_abilities_of_large_language_models"
        },
        {
            "paper_title": "Verify-and-edit: A knowledge-enhanced chain-of-thought framework.",
            "rating": 1,
            "sanitized_title": "verifyandedit_a_knowledgeenhanced_chainofthought_framework"
        },
        {
            "paper_title": "Self-check: Using llms to zero-shot check their own step-by-step reasoning.",
            "rating": 2,
            "sanitized_title": "selfcheck_using_llms_to_zeroshot_check_their_own_stepbystep_reasoning"
        },
        {
            "paper_title": "Answering questions by meta-reasoning over multiple chains of thought.",
            "rating": 1,
            "sanitized_title": "answering_questions_by_metareasoning_over_multiple_chains_of_thought"
        },
        {
            "paper_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks.",
            "rating": 1,
            "sanitized_title": "program_of_thoughts_prompting_disentangling_computation_from_reasoning_for_numerical_reasoning_tasks"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models.",
            "rating": 1,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        }
    ],
    "cost": 0.01869025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Mathematical Language Models: A Survey</p>
<p>Wentao Liu 
School of Computer Science and Technology
East China Normal University
ShanghaiChina</p>
<p>Hanglei Hu 
Department of Educational Information Technology
East China Normal University
ShanghaiChina</p>
<p>Jie Zhou jzhou@cs.ecnu.edu.cn 
School of Computer Science and Technology
East China Normal University
ShanghaiChina</p>
<p>Yuyang Ding 
School of Computer Science and Technology
East China Normal University
ShanghaiChina</p>
<p>Junsong Li 
School of Computer Science and Technology
East China Normal University
ShanghaiChina</p>
<p>Jiayi Zeng 
School of Computer Science and Technology
East China Normal University
ShanghaiChina</p>
<p>Mengliang He 
School of Computer Science and Technology
East China Normal University
ShanghaiChina</p>
<p>Qin Chen 
School of Computer Science and Technology
East China Normal University
ShanghaiChina</p>
<p>Bo Jiang 
Department of Educational Information Technology
East China Normal University
ShanghaiChina</p>
<p>Lab of Artificial Intelligence for Education
East China Normal University
ShanghaiChina</p>
<p>Aimin Zhou amzhou@cs.ecnu.edu.cn 
School of Computer Science and Technology
East China Normal University
ShanghaiChina</p>
<p>Lab of Artificial Intelligence for Education
East China Normal University
ShanghaiChina</p>
<p>Liang He 
School of Computer Science and Technology
East China Normal University
ShanghaiChina</p>
<p>Mathematical Language Models: A Survey
971D1023609C85290564CBCA94F34F91MathematicsLanguage ModelsPre-trainedLLMsSurvey Mathematical Tasks Mathematical Calculation ( §a) Arithmetic Representation DigitRNN [55]DigitCNN [56]GenBERT [25]NumBERT [57] Arithmetic Calculation ScratchpadGPT [58]Goat [39]MathGLM [38]Mathematical Reasoning ( §b) Math Word Problem Solving MathPrompter [59]MetaMath [60]WizardMath [61]MathAttack [62]LLEMMA [49] Math Question Answering GEOS [63]DROP [64]Mathematics [65]Lila [66] Theorem Proving DeepMath [67]ASTactic [68]NaturalProofs [69]INT [70]
In recent years, there has been remarkable progress in leveraging Language Models (LMs), encompassing Pre-trained Language Models (PLMs) and Large-scale Language Models (LLMs), within the domain of mathematics.This paper conducts a comprehensive survey of mathematical LMs, systematically categorizing pivotal research endeavors from two distinct perspectives: tasks and methodologies.The landscape reveals a large number of proposed mathematical LLMs, which are further delineated into instruction learning, tool-based methods, fundamental CoT techniques, and advanced CoT methodologies.In addition, our survey entails the compilation of over 60 mathematical datasets, including training datasets, benchmark datasets, and augmented datasets.Addressing the primary challenges and delineating future trajectories within the field of mathematical LMs, this survey is positioned as a valuable resource, poised to facilitate and inspire future innovation among researchers invested in advancing this domain.</p>
<p>I. INTRODUCTION</p>
<p>M athematics is the queen of sciences.</p>
<p>-Carl Friedrich Gauss Mathematics stands as a foundational skill integral to human intelligence, wielding significance across diverse fields such as natural sciences, engineering, medicine, finance, computer science, and social sciences.Within the field of natural language processing (NLP), the development of computer models aimed at autonomously resolving mathematical word problems has captivated researchers since as early as 1963 [1,2,3,4].We believe that addressing this problem is a potential way toward illuminating pathways for general reasoning mechanisms, consequently advancing the pursuit of general artificial intelligence (AI).</p>
<p>One main traditional solution to math word problems is statistical learning-based methodologies [5,6,7,8].Notably, machine learning techniques [6,9,10] alongside semantic parsing methods [11,12] have been employed to address this challenge, showcasing promising outcomes on certain datasets.The evolution of deep learning has prompted considerable interest in crafting neural networks capable of resolving mathematical problems [13,14].Notably, recent years have witnessed significant advancements in the domain of mathematical AI [15,16], propelled by the emergence of powerful language models (LMs) [17,18].These language models, comprising pre-trained language models (PLMs) and large-scale language models (LLMs), have assumed a central role in reshaping the landscape of mathematical exploration and practical applications.The paramount focus of this comprehensive survey lies in assessing the impact of these models on the field of mathematics.The survey endeavors to provide a thorough overview of extant research and its consequential implications.</p>
<p>PLMs such as BERT [19], RoBERTa [20], BART [21], GPT-1 [22] and GPT-2 [23] undergo pre-training on extensive textual corpora to assimilate worldly knowledge.To enhance mathematical performance, certain endeavors focus on either pre-training or fine-tuning PLMs using mathematical datasets [24,25,26].For instance, methodologies like Gen-BERT [25], NF-NSM [26], MathBERT [27] and LISA [28] incorporate numerical data or mathematical formulas into PLMs to augment their capabilities.Moreover, specialized modules or tailored loss functions are devised, leveraging existing PLMs to learn mathematical reasoning and operations [29,30,31,32].</p>
<p>The recent advent of LLMs, exemplified by OpenAI's GPT-4 [33], has catalyzed an unforeseen surge in innovation, underscoring the multifaceted potential of AI within the domain of mathematics.These models ( [18,34]) have demonstrated remarkable success across diverse Natural Language Processing (NLP) tasks, leveraging in-context learning [35,36,37] and instruction learning [38,39].Recent studies by Wang et al. [40] indicate that LLMs featuring over 100 billion parameters (e.g., GPT-3 [36] with 175 billion, PaLM [41] with 540 billion) exhibit the capability to address intricate tasks by employing a chain-ofthought (CoT) mechanism [15] when furnished with a limited set of reasoning examples as demonstrations.Advancements in CoT frameworks [42,43,44,45] have been tailored to enhance mathematical performance, incorporating tools and programs [46,47,48].An exemplar in this domain is LLEMMA [49], an openly accessible LLM specifically designed for mathematical tasks.LLEMMA showcases the capacity to generate self-contained textual solutions for mathematical problems and formulate formal proofs.</p>
<p>We mainly review the most related surveys about mathematical reasoning and LLMs.Lu et al. [50] delineate the landscape of deep learning applications specifically pertaining to mathematical reasoning.Qiao et al. [51] predominantly delve into reasoning mechanisms centered around language models, encompassing arithmetic, commonsense, logical, symbolic, and multi-modal reasoning.Chu et al. [52] offer an exploration of studies focused on chain-of-thought (CoT) reasoning.Additionally, Qiu el al. [53] and Naveed et al. [54] contribute comprehensive reviews elucidating the realm of existing PLMs and LLMs, respectively.</p>
<p>Diverging from these perspectives, our work delves into the burgeoning domain of mathematical language models, systematically summarizing the diverse range of studies and innovations in this field.In light of the demonstrated proficiency of LMs in comprehending, generating, and manipulating mathematical expressions, a multitude of advanced methodologies have emerged within the field of mathematics leveraging LMs.By shedding light on the current landscape, tasks, datasets, challenges, and future prospects, we aim to inspire and inform those who aspire to harness the power of language models to revolutionize mathematics and its myriad applications.</p>
<p>Specifically, our categorization of mathematical tasks ( §II) contain two primary domains: mathematical calculation ( §a), consisting of arithmetic representation and arithmetic calculation, and mathematical reasoning ( §b), consisting of problem-solving and theorem proving.Furthermore, our taxonomy of existing algorithms segregates them into PLMsbased approaches ( §III), including autoregression ( §a) and non-autoregression ( §b) LMs, and LLMs-based methodologies ( §IV), which include instruction learning ( §a), toolbased strategies ( §b), fundamental CoT techniques ( §c), and advanced CoT methodologies ( §d).Moreover, we provide a comprehensive compilation of over 60 mathematical datasets ( §V), systematically categorized into training ( §a), benchmark ( §b), and augmented datasets ( §c).This classification aims to facilitate research by delineating the utility and applicability of these datasets within distinct research contexts.Lastly, our survey meticulously addresses the primary challenges and explores the future directions of this domain ( §VI), including faithfulness, multi-modal, uncertainty, evaluation, creation, application, and data scarcity.In conclusion ( §VII), we believe that this comprehensive survey will serve as a valuable asset for researchers and practitioners alike, seeking to push the boundaries of innovation in the field of mathematical language models.</p>
<p>II. MATHEMATICAL TASKS</p>
<p>In this section, we summarize the existing mathematical tasks into mathematical calculation and mathematical reasoning (Figure 1).</p>
<p>a. Mathematical Calculation</p>
<p>The advent of Language Models (LMs) has ushered in a new era of exploration into their computational capabilities, particularly in arithmetic.Initially, these models demonstrated basic computational abilities by representing numbers in textual formats.With the increasing capabilities of LMs, it has been observed that they can acquire arithmetic skills through methods such as fine-tuning, even without carefully crafted numeric representations.</p>
<p>Arithmetic Representation</p>
<p>In the early stages of the research, numerical values were either omitted or oversimplified, treated as ordinary text, or categorized as "unknown" (UNK).This approach, however, proved inadequate for tasks.For instance, BERT performs five times worse when the answer is a number rather than a textual span over the DROP question-answering benchmark [64].</p>
<p>Recent literature proposes various methods for numerical representation.Geva et al. [25] introduce GenBERT, which tokenizes numbers at the digit level and undergoes fine-tuning with arithmetic word problems and simple arithmetic tasks.Zhang et al. [71] experiment with a retrained BERT model by converting numbers into scientific notation (e.g., 314.1 as 3141[EXP]2).Spithourakis and Riedel [55], along with Wallace et al. [56], explore the integration of digit embeddings into a singular embedding that represents the entire number.Berg-Kirkpatrick and Spokoyny [72] propose a method with digit-RNN and exponent embeddings.This method specifically emphasizes the exponent while disregarding the mantissa.Goat [39] introduces Consistent Tokenization, enhancing the relationships between similar numerical values.</p>
<p>Arithmetic Calculation</p>
<p>There has been significant research into the arithmetic capabilities of LLMs.Nogueira et al. [73] and Wang et al. [74] assessed addition and subtraction tasks.Muffo et al. [75] evaluated two-digit multiplication.Yuan et al. [76] evaluated the arithmetic operation capabilities of different models, including GPT-4 [33], Galactica [77], and LLaMA [18].</p>
<p>Traditionally, it was presumed that LLMs could not accurately perform complex arithmetic, especially multiplication involving more than eight digits.However, recent approaches challenge this assumption.Zhou et al. [78] apply specialized prompt engineering to enhance addition capabilities but note limitations in multiplication beyond seven digits.Jelassi et al. [79] investigate length generalization in basic arithmetic tasks using techniques like relative position embeddings and training set priming.ScratchpadGPT [58] demonstrates the effectiveness of pre-generating a Chain of Thought (CoT) before producing an answer in 8-digit addition tasks.Goat [39] utilizes supervised instruction for fine- tuning elementary arithmetic operations with large integers.MathGLM [38] excels in intricate arithmetic tasks by pretraining on datasets that decompose complex arithmetic expressions into simpler steps.Through this process, it gradually generates answers and learns the rules of computation.</p>
<p>b. Mathematical Reasoning</p>
<p>Mathematical reasoning is a pivotal aspect of artificial intelligence that facilitates the understanding and solving of complex mathematical problems.The integration of LLMs in this domain has been significant, owing to their ability to interpret, process, and generate complex natural text.This section delves into the current research and developments within this field, focusing on two main areas: math problem solving and theorem proving.</p>
<p>Math Problem Solving</p>
<p>In the field of math problem solving in artificial intelligence, math problem solving refers to the process of using algorithms, computational models, and increasingly LLMs to understand, explain, and solve mathematical problems.This approach to solving problems spans all levels from basic arithmetic to advanced mathematics, including but not limited to algebra, geometry, statistics, and calculus.In this subsection, we condense it into Math Word Problems (MWPs) and Math Question Answering (MQA).</p>
<p>Math Word Problem Solving.Math Word Problems (MWPs) refer to problems that present mathematical concepts and calculations in the form of written descriptions.Such questions usually include one or more story situations describing a series of events or conditions from which the solver needs to extract relevant mathematical information and apply appropriate mathematical principles to solve the problem.Over the years, using various efficient and intelligent algorithms to solve MWPs has also been a research focus in the fields of computer science and artificial intelligence [2].</p>
<p>Recent research has highlighted the growing capabilities of LLMs in the field of mathematical word problem solving, emphasizing the trend toward more nuanced and sophisticated AI-driven mathematical analysis.MathPrompter [59] uses LLM called GPT3 DaVinci to solve MWPs with excellent results, demonstrating the potential of LLMs to not only explain but also generate complex mathematical reasoning, reflecting a human-like understanding of complex problem sets.</p>
<p>Yuan et al. [80] investigated the interaction between various factors such as pre-training loss, amount of supervised data, and augmented data to improve the mathematical inference performance of LLMs.They found that pre-training loss is a more reliable performance metric and empirically found a log-linear relation between data amount and model performance, but it is worth mentioning that this improvement diminishes with better pre-training models.At the same time, they proposed Rejection sampling Fine-Tuning (RFT) to enhance performance by using supervised models to generate and collect correct reasoning paths, thereby achieving better inference generalization and more significant improvements.Meanwhile, the proficiency with which LLMs perform basic arithmetic tasks is critical to establishing the baseline capabilities of these models [76].This assessment is integral to understanding the limitations and potential of LLMs in future work when faced with mathematical challenges that require precise and logical calculations.</p>
<p>MetaMath [60] further extends the practicality of the LLMs, proposing a paradigm whereby the LLMs generate their mathematical problems, thereby creating a selfsustaining learning environment that encourages continuous improvement of the model's problem-solving acuity.Similarly, WizardMath [61] explores new ways to enhance LLMs' mathematical reasoning capabilities by reinforcing evolutionary instructions, indicating an important step towards autonomous self-improvement of artificial intelligence models.Instead, MathAttack [62] introduces a critical perspective by exploring the sensitivity of LLMs to specialized adversarial input designed to test their mathematical problem-solving abilities.Such investigations are crucial to developing more resilient and reliable LLMs that can withstand and adapt to a variety of problem-solving situations.LLEMMA [49], an open language model dedicated to mathematics, outlines ongoing efforts to make LLMs accessible and adaptable to a wide range of mathematical inquiries, from basic problem solving to advanced theorem proving.</p>
<p>The present findings confirm that these studies comprehensively depict the current research hotspots and future development potential directions of LLMs in the MWPs field.They emphasize that LLMs can have a transformative impact on the field of mathematics, providing innovative tools for educators, students, and researchers.As these models continue to develop, they are expected to revolutionize the way we approach and solve mathematical problems, ushering in a new era of mathematical reasoning and assisted education through artificial intelligence.</p>
<p>Math Question Answering.Math Question Answering (MQA) refers to the computational task where a system is required to understand and automatically solve math-related questions presented in natural language.These questions can range from simple arithmetic problems to complex high school or college-level mathematics, including algebra, calculus, and geometry.The challenge for MQA systems is to accurately interpret the text, convert it into an appropriate mathematical representation, perform the necessary computations, and generate a correct and often step-by-step solution, mimicking the problem-solving process of a human.</p>
<p>Sachan et al. [65] introduce a framework for evaluating neural architectures through a suite of math problems, highlighting the importance of algebraic generalization.This is complemented by GEOS [63], an innovative system that solves SAT geometry problems by integrating text understanding with diagram interpretation.The introduction of datasets and systems like Inter-GPS [81], IconQA [82], and PGDP5K [83,84,85] further advances the field, offering new benchmarks for abstract diagram understanding and visual language reasoning.Additionally, Scharpf et al. [86] indicate that unsupervised methods for formula labeling contribute to the automated understanding of mathematical documents.Collectively, these works demonstrate the evolving landscape of MQA, where the synergy between natural language processing, computer vision, and symbolic reasoning opens new avenues for educational technology and AI research.</p>
<p>Theorem Proving</p>
<p>Theorem proving (TP) refers to the process of demonstrating that a statement is correct based on existing axioms and facts, which is a long-term challenge in AI [87].In the context of LLMs, people began to try to use the huge knowledge base of LLMs and some manual guidance to solve this task.LLMs like GPT-3 [36] can process natural text to understand the premises of the theorem, and apply certain logical reasoning rules to complete the proof requirements.This can then be done by calling relevant tools or using manual inspection to check the correctness of the process.The advanced approach aims to leverage LLM's superior computing power and vast knowledge base relative to humans to automate the theoremproving process, a task that previously required professional mathematicians.</p>
<p>In the field of AI, research on theorem proving has progressed from data sets containing large numbers of humanwritten proofs (such as CoqGym [68]) to complex models that can autonomously generate proof strategies (such as AS-Tactic [68]).This improvement is reflected in the application of some language models that are based on Transformer [88] to TP tasks, especially in recent research such as GPT-f [89].Most notably, some of the proofs generated by these models have been formally recognized by the mathematical community.Jiang et al. [90] further refined this process by using informal proofs to guide TP models.NaturalProofs [69] extends these capabilities by leveraging the language of natural mathematics to create a rich corpus of model training and evaluation.At the same time, DeepMath [67] and INT [70] have promoted the development of this field by demonstrating the effectiveness of neural sequence models for premise selection and evaluating the LLMs' generalization ability in theorem proving.</p>
<p>Ongoing research is delving into the integration of language models and interactive proof assistants within the realm of mathematics.This involves generating proofs through tactic prediction [91,92,93,94], automating formalization processes [95,90], and developing unified tools [96].Due to the substantial computational demands of exploration, language models applied in this domain have traditionally been limited in size.However, recent advancements have showcased potential in employing larger models [97,90].LLEMMA [49] demonstrates few-shot proof autoformalization and tactic prediction, offering a vast dataset of formal mathematics and an openly accessible model for further investigation in these directions.These endeavors establish crucial theoretical and practical frameworks for automating and enhancing tactic prediction.</p>
<p>The above series of progressive research work shows that utilizing the powerful computing power and extensive knowledge base of LLMs is the main development direction for future TP tasks.The more excellent models (such as GPT-4 [33]) obviously have better reasoning capabilities without any human intervention, but we also need to be clear about one thing, that is the inevitable hallucination problem in neural text generation models [98].The hallucination problem refers to the generated content that is nonsensical or unfaithful to the provided source content [99].The occurrence of hallucinations at any step in the reasoning process will lead to errors in the final reasoning results.Therefore, solving the hallucination problem is also particularly important in future work related to TP.</p>
<p>III. PLMS-BASED METHODS</p>
<p>Based on the transformer architecture [88] with self-attention mechanisms, the "pre-training and fine-tuning" paradigm has revolutionized the field of natural language processing.Within the context of Pre-trained Language Models (PLMs), numerous approaches have been proposed to tackle text generation problems [100].Autoregressive LMs (ALMs, e.g., GPT-1 [22] and T5 [101]) and Non-Autoregressive LMs (NALMs, e.g., BERT [17] and Roberta [20]) are the two primary strategies for mathematics (Figure 2).</p>
<p>a. Autoregression LMs</p>
<p>The distinctions between Autoregression LMs (ALMs) and Non-Autoregressive LMs (NALMs) in sequence generation tasks are depicted in Figure 3. Furthermore, ALMs can be categorized into two architectures: causal decoder (e.g., GPT-1 [22] and GPT-2 [23]) and encoder-decoder (e.g., T5 [101]).The generation mechanisms of these two structures for sequences are depicted on the left and in the middle of Figure 3, respectively.These two architectures have not only significantly propelled the advancement of PLMs but have also become the predominant foundational architectures for LLMs.</p>
<p>To enhance the performance of PLMs in mathematical problems, many improved methods based on ALMs have been proposed.GPT-f [89] presents an automated prover and proof assistant to explore the performance of LMs to GPT-f [89], EPT [102] , Generate &amp; Rank [103] , Thor [94] , HTPS [104] , Galactica [77] , MATH-PLM [105] , LISA [28] PACT [92] , Minerva [106] , LIME [107] Methods Socratic Models [158], SOCRATIC QUESTIONING [159], Socratic prompt [160], SocratiQ [161] multi-turn Socratic advice [162] Fig. 2: Taxonomy of language models for mathematic.automated theorem proving.To resolve challenges related to expression fragmentation and operand-context separation, Kim et al. [102] introduce the Expression-Pointer Transformer (EPT), a pure neural model that employs both an 'Expression' token and operand-context pointers during the generation of solution equations.Generate &amp; Rank [103] introduces a novel ranking task for MWPs, a multi-task framework built upon a generative PLM, effectively addressing the challenge of minor mistakes in mathematical expressions.</p>
<p>Thor [94] introduces a framework that integrates language models and automated TP.The latter is employed to selectively choose relevant premises from an extensive library to assist the language model in TP.HTPS [104] presents Hy-perTree Proof Search, a novel algorithm leveraging online training from past proof searches, enhancing model performance and surpassing the previous SOTA GPT-f .Galactica [77] proposes a working memory token approach that can achieve strong performance over existing methods on mathematical MMLU [163] and MATH [105] benchmarks.Furthermore, MATH-PLM [105] introduces a new dataset named MATH, consisting of challenging competition mathematics problems.It is found that relying solely on increasing budgets and model parameter counts would be impractical for achieving robust mathematical reasoning.LISA [28] gathers an extensive collection of lemmas and theorems extracted from the Isabelle standard library and the Archive of Formal Proofs (AFP).Using this vast corpus, it builds LMs that prove theorems effectively within the AFP.PACT [92] extracts abundant self-supervised data from kernel-level proof terms, enabling joint training alongside the standard tactic prediction objective.This effectively enhances the model's ability for TP.Minerva [106]</p>
<p>b. Non-Autoregression LMs</p>
<p>Unlike ALMs, NALMs enable the model to generate all parts of a sequence simultaneously, without depending on previously generated content, as depicted on the right side of Figure 3. Notably, models such as BERT [17], Roberta [20], and related architectures utilize masked word representations, leveraging pre-trained context-aware embeddings as comprehensive semantic features.This significantly elevates the performance benchmarks of NLP tasks.</p>
<p>In the field of mathematics, researchers have proposed various methods for designing models to address mathematical reasoning and computational problems.For example, Aristo [24] fine-tunes BERT using scientific curriculum data, yielding promising results on science exams.GenBERT [25] and NF-NSM [26] enhance the numerical reasoning capabilities of models by incorporating numerical data into the training process of PLMs.MWP-BERT [108] further enhances the model's capacity to represent and calculate numerical values by incorporating numeric attributes into symbol placeholders.MathBERT [27] employs additional joint training of text and formulas to effectively capture the semantic-level structural information of formulas.TAGOP [29], MT2Net [30], and DeductReasoner [31] utilize BERT or RoBERTa to extract the fundamental arithmetic relationships between quantities, enabling mathematical reasoning and operations.BERT-TD [32] utilizes semantic encoding and contrastive learning to cluster problems with similar prototype equations, thereby enhancing the understanding of MWP patterns.</p>
<p>IV. LLMS-BASED METHODS</p>
<p>Large-scale language models (LLMs) are designed for processing and generating text akin to human communication [18,33].Mathematics is also a form of language, that communicates complex concepts and relationships through a structured system of symbols and notation, akin to the rules of a spoken language.Thus, a language model that grasps these mathematical rules can "speak" the language of mathematics, proving to be a valuable asset for mathematicians.In numerous ways, a sophisticated language model like GPT-4 [33] becomes an invaluable tool in the field of mathematics [49,61].We classify the existing studies into four parts: instruction learning, tool-based methods, fundamental CoT methods, and advanced CoT methods (Figure 2).</p>
<p>a. Instruction Learning</p>
<p>Numerous approaches have been proposed to enhance the mathematical performance of models by instruction learning.These approaches are categorically delineated as instruction building, instruction tuning, and in-context learning, based on their distinctive characteristics.</p>
<p>Instruction Building.A semi-supervised approach, as presented in Auto-explanation [109], leverages LLMs to create datasets for automating the scoring of mathematical selfexplanations in the context of mathematics education.A challenging benchmark ProofNet [110] is introduced to build a system of automated theorem proving.Meanwhile, prompt retrieval and distilled back translation methods are introduced for statement autoformalization.WizardLM [111] presents a groundbreaking method called Evol-Instruct, designed to autonomously generate high-quality instructions by LLMs themselves.The conceptual flow is depicted in Fig- ure 4. The instructional process commences with a straightforward "initial instruct" and progresses into diverse forms, deepening, increase reasoning, add constraints, concretizing, complicate Input (formula), in-Breadth evolving, etc.Furthermore, Wizardmath [61] proposes a reinforced Evolinstruct method (RLEIF) to build more complexity of instruct dataset.This method effectively enhances the mathematical challenges.The whole annotating process is extremely expensive and time-consuming [18][19][20][21].On the other hand, the difficulty level distribution of human-created instructions is skewed towards being easy or moderate, with fewer difficult ones (according to the difficulty statistics of ShareGPT [22] from Figure 7a).Possible reasons for this are that the proportion of experts among annotators is low and creating complex instructions demands a lot of mental effort.Human annotators are prone to fatigue and cannot sustain high-intensity work to produce a sufficient proportion of high-difficulty instructions [23][24][25][26].Based on these issues, developing an automatic method that can mass-produce open-domain instructions (especially the more difficult ones) at a relatively low cost becomes the key to further advancing instruction-tuned language models [27][28][29][30].
1 + 1 = ? What is the value of x, if x^3 + 2x + 3=7?</p>
<p>If you have one apple and someone gives you another banana, how many fruits do you have?</p>
<p>In what situation does 1+1 not equal to 2?
How to prove 1 + 1 = 2 in the Goldbach Conjecture? 1/(sqrt(2) + 4^2) = ?
What is the speed of light in a vacuum?</p>
<p>How many times faster is light than sound in a vacuum?</p>
<p>How is the speed of light in a vacuum measured and defined?</p>
<p>Please fill in the table below with the approximate values of the speed of light in each medium.</p>
<p>Medium Speed of light (km/s)</p>
<p>Air Water Glass import math import random # choose a random integer between 1 and 10 x = random.randint(1,10) 1/(math.sqrt(x)+ x^2) =?</p>
<p>The process of plant photosynthesis is commonly written as: 6CO2 + 6H2O → C6H12O6 + 6O2 Please explain the main role of chlorophyll in above formula.</p>
<p>Complicate Input (Formula)</p>
<p>In-Breadth Evolving  In this work, we introduce Evol-Instruct, a novel method using LLMs instead of humans to automatically mass-produce open-domain instructions of various difficulty levels, to improve the performance of LLMs. Figure 1 shows the running examples of Evol-Instruct.Starting from a simple initial instruction "1+1=?", our method randomly selects In-depth Evolving (blue direction line) or In-breadth Evolving (red direction line) to upgrade the simple instruction to a more complex one or create a new one (to increase diversity).The In-depth Evolving includes five types of operations: add constraints, deepening, concretizing, increase reasoning steps, and complicate input.The In-breadth Evolving is mutation, i.e., generating a completely new instruction based on the given instruction.These six 4 https://chat.openai.com/ 2 Fig. 4: Example of Evol-Instruct taken from WizardLM [111].</p>
<p>Model</p>
<p>Number Tokenization LLaMA 74815 Fig. 5: Consistent tokenization for similar number in Goat [39].</p>
<p>reasoning capabilities of Llama-2 by supervised fine-tuning and PPO training [164].</p>
<p>Instruction Tuning.Instruction tuning stands as a potent method to elevate the prowess of large models, aiding in steering the model towards generating outputs that align with human intent [165].Compared to the pre-training phase of LLMs, instruction tuning demands notably less instruction data and computational resources, making it a prevalent strategy for enhancing a model's domain-specific capabilities.For instance, MathGLM [38] showcases that even a language model with 2 billion parameters can adeptly execute multi-digit arithmetic operations with limited training data.</p>
<p>Goat [39] discerns patterns in the tokenization of different numbers and proposes a consistent approach for their tokenization.Utilizing the LLaMA model, Goat ensures consistent tokenization for various numbers, illustrated in Figure 5. Similarly, Calculon [75] refines the model's arithmetic skills by employing a digit decomposition technique to construct a fine-tuning dataset, as depicted in Figure 6.</p>
<p>In PaLM 2-L-Math [112], three fine-tuning strategies were explored, revealing key insights: 1) The quality and style of step-by-step solutions significantly influence model performance; 2) Combining solution re-ranking and majority</p>
<p>Approach Observation Calculon</p>
<p>Compute with pipeline 1201 plus 1302.Translate from number to decomposition: 1201 = 1 units, 0 tens, 2 hundreds, 1 thousands.Translate from number to decomposition: 1302 = 2 units, 0 tens, 3 hundreds, 1 thousands.Sum 1 units, 0 tens, 2 hundreds, 1 thousands + 2 units, 0 tens, 3 hundreds, 1 thousands = 3 units, 0 tens, 5 hundreds, 2 thousands.Translate from decomposition to number: 3 units, 0 tens, 5 hundreds, 2 thousands = 2503</p>
<p>Baseline</p>
<p>Data and training details</p>
<p>For the addition and subtraction operations, we generate training sets of 12000 observations each.In particular for each N ∈ {3, 4, 5} we randomly sample 3000 couples of integer numbers (n1, n2)i, with (n1, n2)i ∈ {10 N −1 , . . ., 10 N − 1} 2 , ∀i ∈ {1, . . ., 3000}.Similarly, for N = 2 we randomly sample 3000 couples of numbers (n1, n2)i ∈ {0, . . ., 99} 2 (one-digit numbers are included).We then join all the couples created (obtaining a set of 12000 couples) and we compute the results of the operations.At the end of this step, we obtain two vectors of results, r+ and r−, where r+,i = n1,i +n2,i and r−,i = n1,i −n2,i, ∀i ∈ {1, . . ., 12000}.Finally, given a triplet (n1, n2, r)i, we generate a string according to the procedures described in section 3, depending on the selected approach.For the multiplication, we generate training sets following the same procedure explained above but, instead of sampling 12000 couples, we sample 3000 couples of numbers from the set {0, . . ., 99} 2 because we will only test the multiplications between two-digit numbers.At the end of this procedure we obtain 9 training sets, each of which corresponding to a combination operation-approach (e.g.addition-decomposition), that we use to fine-tune as many Language Models.Now, we want to underline some points relative to the generated training sets.First, by fixing the operation and varying the approach, the same couples of numbers are used to generate strings, so that couples of numbers are sampled once for each operation.Second, none of the couples present in a training set is in the test set relative to the same operation.The test sets used to evaluate our fine-tuned Language Models are the same used to evaluate GPT-3 in the arithmetic tasks 4 (Brown et al., 2020). 3A full GPT-3 input prompt reported in Appendix A 4 Test sets publicly available at https://github.com/openai/gpt-3</p>
<p>The GPT-2 models fine-tuned in our experiments are GPT-2 Small architectures, which count ∼117M parameters.The GPT-3 model evaluated in our experiments corresponds to the biggest architecture proposed in Brown et al. (2020), which counts ∼175B parameters.We fine-tune each GPT-2 Language Model for 25 epochs with an initial learning rate of 10 −4 and a batch size of 32, using Adam (Kingma and Ba, 2017) as optimizer.For the experiments on GPT-3, due to limited resources available on the dedicated API, we evaluate the model only on the first 100 observations of each test set.We adopt a greedy decoding strategy for the GPT-2 models and a random sampling strategy with tempera-ture=0.7 for the GPT-3 generations.</p>
<p>Results and discussion</p>
<p>In table 2 we show the results obtained with the experiments described in section 3. The GPT-2 model fine-tuned without decomposition (Baseline row) obtains low accuracy scores in all tasks except two-digit addition, where achieves 53.35 accuracy.In particular, in the 4 and 5 addition and subtraction tasks it achieves zero or near-zero accuracy.This demonstrates that, without decomposing numbers, a GPT-2 Language Model is not able to learn to perform computations, especially between numbers with a higher number of digits.On the other hand, Calculon obtains high accuracy scores in all the tasks tested with the exception of 2Dx.This demonstrates that finetuning using the proposed decomposition pipeline effectively makes possible for a transformer Language Model to learn to do calculations.Here, we underline again that none of the couples of numbers composing the training set are in the relative test set, and hence we can conclude that Calculon has effectively learned to sum units with units, tens with tens, and so on and manage to perform arithmetic operations between unseen numbers.However, the results in the two digit multiplication task are poor, suggesting that number decomposition is not sufficient to solve this task and probably higher reasoning capabilities are needed to multiply numbers.</p>
<p>Fig. 6: Number decomposition taken from Calculon [75].</p>
<p>he goal of both problems is for the neural network to learn to emulate a mic" in the sense that it can be represented by a short program, such as ation, from input-output behavior.In neural algorithm induction, the ithm, and each training example gives a single input and desired output fore, the training data is D = {x i , f (x i )} N i=1 .For learning to execute, e the result of a program, represented as source code, on some input.If a program f i , then the training data is
D = {(π i , x i , f i (x i ))} N i=1
(it is multiple input-output examples, but we omit this to lighten notation).</p>
<p>is that to solve a given algorithmic task, we simply encode the interm as text and train the model to emit them to a buffer that we call a et us consider the algorithmic induction task of learning long addition.o 57, a training example may look like the text in Figure 2, where the g addition algorithm are written out explicitly.n be encoded in a dd the source code d, and desired outing example for a own in Figure 1.</p>
<p>will be given the d likelihood-based odel will be given uired to predict the r temperature samence model could rk, we choose to er language models could be effecer models ( voting yields a more substantial performance enhancement compared to using them individually.Additionally, multitask fine-tuning, separating solution generation and evaluation tasks, surpasses the baseline achieved by solution finetuning alone.Moving beyond the enhancement of mathematical capabilities through fine-tuning, LLEMMA [49] introduces the Proof-Pile-2 dataset, an amalgamation of mathematical texts and code.Continual pre-training with Code Llama empowers the model to leverage Python interpreters and formal theorem provers, demonstrating impressive performance on the MATH benchmark.</p>
<p>In-context Learning.In-context Learning (ICL) [36,166] empowers LLMs to execute target tasks by presenting specific task examples as conditions during inference, without updating model parameters.Inspired by this, Scratch-padGPT [58] enhances its proficiency in multi-step computations by mandating the model to output intermediate computation steps into a designated "scratchpad."Codex-math [47] refines Codex by generating programs through code and integrates few-shot learning to automatically create programs for solving mathematical problems.This method has substantially improved the previous state-of-the-art accuracy in automatic solutions by 73.1% across various benchmarks like MIT's mathematics courses, Columbia University's Computational Linear Algebra, and the MATH benchmark [105].Notably, few-shot learning contributed to a 10% accuracy boost.Recent studies also highlight the variability in incontext learning performance across different chosen examples [113].Fu et al. [45] and Zhang et al. [44] selected intricate and diverse examples to enhance reasoning performance.</p>
<p>b. Tool-based Methods</p>
<p>LLMs are designed to use tools, such as codes and calculators, to enhance their problem-solving abilities [115,120].</p>
<p>Single-tool Methods.To improve the performance of mathematical reasoning, math-specific tools such as symbolic solvers and programs are utilized for LLMs [48,42].For example, SymbLLM [48] solved math word problems by combining language models with symbolic solvers.They focus on automatically generating high-quality, step-by-step solutions to mathematical word problems, especially those encountered in mathematical applications.Furthermore, previous studies have explored the process of converting mathematical problems into code [42,47].PoT [42] proposes a fusion of CoT with programs, while Drori et al. [47] showcased the effectiveness of a neural network pre-trained on text and fine-tuned on code, particularly leveraging OpenAI's Codex transformer.This approach successfully solves, explains and generates math problems at the university level.MathPrompter [59] employs a zero-shot chain-of-thought prompting technique to generate multiple algebraic expressions or Python functions in varied ways to solve the same math problem, enhancing reliance on output results.PAL [46] introduces an innovative approach to bolster the performance of pre-trained language models (PLMs) in mathematical problem-solving.This involves utilizing LLMs to comprehend natural language problems and generate programs as intermediate reasoning steps.</p>
<p>In addition to solving math problems, programs can also play a role in tutoring math.For instance, Upadhyay explores the role of large language models in tutoring systems [114], introducing a "code scratchpad" alongside the traditional "language scratchpad" to enhance the model's performance in tutoring steps, particularly using a grade school mathematics dataset.</p>
<p>Multi-tool Methods.To facilitate the seamless integration of LLMs with diverse tools, several multi-tool approaches have been proposed to enable LLMs to learn how to use multiple tools simultaneously.Toolformer [115] adopts a self-supervised training approach to enable the utilization of different tools such as search engines, calculators and translation systems via simple API calls.This is accomplished by fine-tuning on a vast collection of sampled API calls and filtering based on their ability to reduce perplexity on subsequent tokens.Chameleon [116], on the other hand, enhances LLMs with plug-and-play modules designed for compositional reasoning.It creates programs by combining various tools, including LLMs, off-the-shelf vision models, web search engines, Python functions, and heuristic-based modules, to tackle complex reasoning tasks.The Automatic Reasoning and Tool-use (ART) framework [117] leverages frozen LLMs to automatically generate intermediate reasoning steps as a program.This seamlessly incorporates external tools to support computations that surpass the core capabilities of LLMs.Meanwhile, ToolkenGPT [118] adopts a strategy of learning "toolken" embeddings to represent each tool as a token.This empowers LLMs to effortlessly utilize tools similar to generating word tokens.It caters to a broader range of tools and utilizes extensive demonstration data to learn toolkit embeddings.</p>
<p>Furthermore, CRITIC [119] significantly enhances the outputs of LLMs by enabling them to verify and self-correct through interactions with external tools.Inspired by human MATHEMATICAL LANGUAGE MODEL: A SURVEY LIU et al.</p>
<p>cognition and critical thinking, CRITIC continuously refines text generated by LLMs through iterative interactions with tools like search engines and code interpreters.Tool Augmented Language Models (TALM) [120] seamlessly integrate language models with non-differentiable tools.They employ an iterative "self-play" method to bootstrap performance based on a limited number of tool demonstrations.In contrast, Tool-Documentation [121] opted for tools based on documentation rather than relying on demonstrations.Current practices involve teaching LLMs through a few-shot demonstration of tool usage, a process prone to bias and overfitting.The alternative proposed by Tool-Documentation, utilizing tool documentation that provides descriptions of individual tool usage, is argued to be a more effective approach.ToRA [167] employs a set of Tool-integrated Reasoning Agents, which integrate computational libraries and symbolic solvers, among other mathematical tools, to effectively solve complex mathematical problems.</p>
<p>c. Fundamental CoT Methods</p>
<p>A multitude of fundamental Chain-of-Thought (CoT) methods integrated with LLMs have been proposed to enhance the mathematical reasoning abilities of LLMs.</p>
<p>Foundation of CoT.In the initial stages of research, a limited body of work has leveraged the principles of chainof-thought (CoT) to enhance the mathematical capabilities of language models.Notably, Ling et al [122] propose a methodology involving the generation of a series of concise steps, termed "Answer Rationales," to guide the resolution of algebraic word problems.MathQA [123] suggests decomposing Math Word Problems into multiple steps corresponding to programmatic operations for resolution.</p>
<p>Utilizing the in-context learning of LLMs, CoT [15] explicitly introduces the concept of the chain of thought (CoT) for the first time, and substantiates its efficacy in enhancing reasoning abilities.A CoT example is shown in Figure 8, where the text highlighted in blue and green guides to the ultimately correct answer.This validation is also corroborated in Complexity-based CoT [45].Following a similar line of thought, LVSS [124] presents a Process Reward Model (PRM) and compares it with the Outcome Reward Model (ORM).Empirical findings affirm that process supervision outperforms outcome supervision, leading to notable performance improvements on the MATH [105] benchmarks.Consequently, several methods [42,80,125,126,127,128] leverage CoT methodologies to enhance the mathematical performance of models.</p>
<p>RFT [80] proposes the application of rejection sampling finetuning (RFT) to gather more optimal reasoning paths, thereby enhancing mathematical reasoning performance.Thought Propagation [125] considers gaining insights from analogous problems to assist in addressing the current issue.Wang et al. [126] proposed a Fine-Tuning (AFT) paradigm that aligns the model to prioritize the generation of CoT with superior performance.Additionally, PoT [42] introduces a Program of Thoughts, combining CoT and programming, and AOT [127] enhances reasoning abilities through algorithmic-style demonstrations of CoT.Furthermore, MAmmoTH [128] integrates CoT and PoT [42] ra- tionales to instruct large language models in utilizing code tools for solving mathematical problems.</p>
<p>Construction of CoT.To streamline the process of CoT creation, various approaches [43,44,45,126,129,130,131,127,125] have been introduced for its automatic generation.Zero-shot CoT [129] introduces a simple CoT example, "Let's think step by step," which effectively enhances the model's reasoning capabilities.Auto-CoT [44] selects representative questions through clustering and answers them using "Let's think step by step," concatenating the questionanswer pairs one by one to automatically generate CoT.In the case of Complexity-based CoT [45], the demonstration is chosen by simply selecting the reasoning chain with the highest number of inference steps sampled from the model.Using reinforcement learning, PromptPG-CoT [43] trains a model with policy gradient to assist GPT-3 in selecting suitable CoT demonstrations.Similarly, AutoMate CoT [130] employs a variance-reduced policy gradient strategy to estimate the significance of each example in a black box language model, thereby selecting more effective examples.BoostedPrompt [131] presents an iterative prompt ensemble method that enhances prompts when the current demonstration faces challenges in handling specific problems.</p>
<p>d. Advanced CoT Methods</p>
<p>To further enhance the capability of CoT in LLMs, advanced CoT methods have been proposed, including Verify-based Methods, Ensemble-based Methods, Planning-based Methods, and Socratic Teaching Methods.</p>
<p>Verify-based Methods.LLMs often produce incorrect reasoning steps, which can lead to a series of cascading errors in their solutions.Implementing verification and refining the reasoning process based on feedback can significantly reduce these errors.This approach is akin to human reflection and involves critically evaluating each step of the reasoning process.Various verify-based methods [132,133,134,135,136,137] have been proposed to address these issues.</p>
<p>Zhou et al. [132] propose a code-based self-verification approach, utilizing a zero-shot prompt to encourage the GPT-4 Code Interpreter to employ code for self-verifying its responses, enhancing its mathematical reasoning capabilities.To mitigate the challenge of validating the entire deductive reasoning process, VerifyCoT [133] introduces a deductive reasoning form, ensuring that each reasoning step strictly relies on the preceding steps.Furthermore, DIVERSE [134] independently verifies each reasoning step and a voting mechanism to eliminate incorrect answers.Both Verify-and-Edit [135] and Retrieval-CoT [136] utilizes external retrieval tools to support the model in validating reasoning rationales.The key difference is the former edits rationales using retrieved information, while the latter helps the model self-rethink to improve performance on complex reasoning tasks.Both methods effectively reduce factual mistakes during the reasoning process.Shridhar et al. [137] summarize the use of the revisions approach in the SCREWS framework, which includes a selection module to choose between the original and modified reasoning steps.</p>
<p>Ensemble-based Methods.Due to the inherent stochastic nature of language models, which output probability distributions over predicted words, they may randomly generate incorrect reasoning steps and outcomes.To tackle this challenge, some methods [40,138,45,134,139,140,141,142] leverage the concept of ensemble learning, employing techniques such as voting and ranking to eliminate uncertainties in the reasoning process.</p>
<p>Self-Consistency [40] employs multiple reasoning paths and selects the final response through a simple majority vote.Similarly, Diversity-of-Thought [138] generates diverse reasoning paths by altering prompts and aggregates the responses via the majority vote.Complexity-based CoT [45] favors answers derived from more intricate reasoning paths.DIVERSE [134] uses a weighted voting mechanism to filter out incorrect answers.Nevertheless, these voting-based methods often overlook the potentially useful information within unsuccessful CoT reasoning and lack an efficient integration of multiple reasoning chains to improve performance.Self-check [139] addresses this by incorporating reasoning steps into the voting mechanism, ensuring both consistent answers and reliable reasoning.MCR [140] takes a step further by consolidating information across various reasoning chains, acquiring the most relevant facts, facilitating a more comprehensive analysis to make successful reasoning.</p>
<p>Additionally, there are a few methods based on ranking.Rank-verifier [141] proposes using a ranking system to judge the correctness of model completions.Furthermore, GRACE [142] leverages a discriminator trained via contrastive learning to rank each reasoning step.</p>
<p>Planing-based Methods.The original structure of the Chain of Thought (CoT) is sequential, facing limitations when handling highly complex problems and lacking the ability for retrospective correction.To make the CoT chain structure more systematic and intricate, certain planningbased methods [143,144,145,146,147,148,149,150,151,152,153,154,155,156,157] have been proposed.These methods achieve this by altering the organizational structure of reasoning steps or incorporating mechanisms for refinement and reflection.</p>
<p>The Tree-of-Thought (ToT) [143] proposes organizing reasoning paths into a tree structure, where each node represents a reasoning step, and edges denote dependencies between nodes.During the inference process, ToT can use self-assessment to determine future actions.This structure facilitates both forward and backward exploration, employing either deep-first or breadth-first search techniques.TouT [144] effectively employs Monte Carlo Dropout to assess uncertainty scores associated with diverse local responses of language models at intermediate steps, enhancing response precision through integration with global search algorithms.Long et al. [145] introduced a multi-module ToT system, where the state of the ToT thought chain process can be stored and retraced through its storage module.Furthermore, Yao et al. [146], Besta et al. [147], and Lei et al. [148] introduced a novel thought structure called the Graph of Thought (GoT).Similar to ToT, each graph node represents a distinct thought step or state, with edges indicating dependencies between nodes.These methods can effectively integrate diverse reasoning thoughts, fostering collaborative outcomes and improving reasoning by incorporating feedback loops.This contributes to an overall enhancement in the inference capacity of the thought network.Additionally, RE-SPROMPT [149] introduces Residual Connection Prompting, which converts the input prompt into complex reasoning graphs.</p>
<p>Inspired by Monte Carlo tree search, RAP [150] utilizes the Monte Carlo tree search algorithm to traverse tree structures during the reasoning process.This approach effectively balances global and local searches to obtain a high-reward reasoning path.On the other hand, LATS [151] employs LLMs as agents, value functions, and optimizers, repurposing their inherent strengths to enhance decision-making.Liu et al. [152] and Dagan et al. [153] introduced LLM+P and LLM+DP, incorporating the generation of Planning Domain Definition Language (PDDL) to break down complex problems and execute plans using specialized models.</p>
<p>Furthermore, the self-refine approach, as proposed by Madaan et al. [154] and Sun et al. [155], focuses on error correction and summarizing past experiences.Specifically, Self-Refine performs pre-execution validation, providing feedback to the model for refinement in case of errors.Zhou et al. [156] then took a step further by merging the iterative self-refine with PDDL in ISR-LLM.Additionally, Shinn et al. [157] introduced re-flexion to rectify errors stemming from previous actions.Socratic Teaching Methods.The Socratic teaching method [168], derived from the philosophies of ancient Greek thinker Socrates, provides a unique perspective when considering the integration of LMs in mathematics education [169].This method, emphasizing questioning and dialogue, aims to guide learners toward profound thinking [170,171].In realm of mathematics education, employing this method can aid students in grasping concepts with clarity, fostering more rigorous logical application, and promoting reflection and self-discovery.</p>
<p>For LLMs, research shows that the Socratic teaching method can also improve their abilities to a certain extent.For example, the framework of Socratic Models (SMs) [158] significantly broadens the scope of interactive capabilities.SMs facilitate the amalgamation of multiple LLMs to tackle novel multi-modal tasks using structured Socratic dialogues.In Chang et al. [160], techniques including defi-nition, dialectics, cross-examination, induction, and counterfactual reasoning are utilized.These techniques contribute to enhancing the LLMs' comprehension of user intentions, thereby improving model's effectiveness in addressing user queries.The inductive, deductive, and analogical reasoning techniques in the Socratic teaching method establish a comprehensive analytical framework for solving mathematical problems [160,172], ensuring a strong and concise ability to address the complexities of mathematical problem-solving.Moreover, contributions from Ang et al. [162] and Al et al. [161] provided invaluable datasets and evaluation methodologies for Socratic questioning.These contributions underscore the practical applications and potential impact of Socratic methods in counseling and coaching, showcasing their diverse applicability.</p>
<p>Overall, the integration of Socratic teaching techniques in LLMs offers novel perspectives and tools for mathematics education.Unlike CoT [15], Socratic questioning explicitly guides the thinking process, stimulating effective recursive thinking and displaying greater resilience to errors in the thinking process [159].Ranging from basic conversational guidance to intricate problem-solving, these methods demonstrate the effective fusion of modern technology with classical ideas to bolster the learning and comprehension abilities of LLMs.</p>
<p>V. DATASETS</p>
<p>To train and evaluate the arithmetic and math ability of language models, various math word problems datasets [65,105,141,6] are designed.In this paper, we organize the frequently used datasets for mathematical language models by dividing them into training, benchmark and data augmentation datasets (Table 1).</p>
<p>a. Training Datasets</p>
<p>Mathematical Calculation</p>
<p>Several studies have introduced datasets aiming to identify numerical information within text [174,55,175], specifically targeting the prediction of attribute values.The Clinical Data [174,55] consists of clinical records sourced from the London Chest Hospital.Each patient record includes a text report and structured KB tuples detailing 20 potential numeric attributes like age and gender.Scientific Data [55] encompasses paragraphs extracted from Cornell's ARXIV repository, featuring over half a million converted papers across 37 scientific sub-fields.Additionally, Elazar et al. [175] proposed the Distributions over Quantities (DoQ) dataset, comprising empirical counts of scalar attribute values linked to more than 350K nouns, adjectives, and verbs across 10 different attributes.</p>
<p>Furthermore, certain studies consider the relational knowledge between various numbers.For instance, the VERB-PHYSICS [173] dataset aggregates crowdsourced information about actions and objects, encompassing the relative knowledge of grounded object pairs and implications of actions associated with those objects.More intricately, the DROP [64] dataset necessitates Discrete Reasoning Over the content of Paragraphs.In this benchmark of 55,000 adversarially-created questions, a system is tasked with re-solving references within a question, potentially spanning multiple input positions, and performing discrete operations like addition, counting, or sorting.</p>
<p>Math Word Problems</p>
<p>A large number of datasets are proposed for MWPs.Hosseini et al. [5] curated the AddSub dataset, primarily focusing on addition and subtraction problems (Figure 9).This dataset identifies relevant variables and their respective values to translate sentences into problem statements, represented as equations.Another dataset, SingleOp [9], encompasses a wider range of mathematical operations including multiplication and division.Its purpose is to facilitate reasoning about quantities articulated in natural language.DRAW [177] comprises 1000 algebra word problems semiautomatically annotated for evaluating automatic solvers.It features gold coefficient alignments crucial for uniquely identifying equation system derivations.The Alg514 [6] dataset comprises 514 linear algebra problems structured around 28 equation templates.Meanwhile, MultiArith [10] is designed to handle arithmetic problems involving multiple steps and operations, without predefined templates.</p>
<p>Several studies have introduced methods focused on predicting equations through semantic parsing.The SingleEq [12] dataset revolves around grade-school algebra word problems.It comprises samples that can be correlated to a single equation involving multiple mathematical operations on one variable, structured as a parsing tree (Figure 10).In contrast, the Dolphin1878 [11] dataset contains 1,878 word problems in mathematics, wherein each sample might encompass multiple equations and variables depicted through multiple trees.AllArith [178] offers a concise representation of dependencies among number units mentioned in a given problem, referred to as Unit Dependency Graphs (UDGs).To augment dataset size, Math23k [13] gathers 23,161 problems labeled with structured equations and corresponding answers.GSM8K [141] comprises 8.5K high-quality grade school math problems crafted by human problem writers.These problems typically involve 2 to 8 steps for resolution, predominantly requiring a sequence of elementary calculations using basic arithmetic operations to arrive at the final solution.</p>
<p>HMWP [180] aims to enhance dataset diversity by extracting math word problems from a Chinese K12 problem bank.This effort sought to validate the universality of math word problem solvers and expand research in MWPs to better align with real-world scenarios.The dataset encompasses three types of MWPs: arithmetic word problems, equation set problems, and non-linear equation problems.In total, it comprises 5,491 MWPs, including 2,955 single-variable linear MWPs, 1,636 two-variable linear MWPs, and 900 singlevariable nonlinear MWPs.Additionally, MathQA [123] presents a diverse collection of 37,000 English multiplechoice math word problems spanning various mathematical domains (Figure 11).</p>
<p>Existing research endeavors to enhance the comprehensibility and precision of intricate reasoning by providing finegrained annotations of reasoning processes [179,122,183,124,30]. DRAW-1K [179]    This paper investigates the task of learning to solve such problems by mapping the verbs in the problem text into categories that describe their impact on the world state.While the verbs category is crucial (e.g., what happens if "give" is replaced by "receive" in Figure 1?), some elements of the problem are irrelevant.For instance, the fact that three kittens have spots is immaterial to the solution.Thus, ARIS has to determine what information is relevant to solving the problem.</p>
<p>To abstract from the problem text, ARIS maps the text to a state representation which consists of 523 Fig. 9: An example of AddSub, taken from [5].</p>
<p>ic Word Problems into Equations Kedziorski, Hannaneh Hajishirzi, † , Oren Etzioni † , and Siena Dumas Ang Washington, † Allen Institute for AI aang}@uw.edu,{ashishs,orene}@allenai.org  sentences into a coherent mental model.In contrast, the challenge for an NLP system is to "make sense" of the narrative, which may refer to arbitrary activities like renting bikes, collecting coins, or eating cookies.</p>
<p>Previous work coped with the open-domain aspect of algebraic word problems by relying on deterministic state transitions based on verb categorization (Hosseini et al., 2014) or by learning templates that cover equations of particular forms (Kushman et al., 2014).We have discovered, however, that both approaches are brittle, particularly as training data is scarce in this domain, and the space of equations grows exponentially with the number of quantities mentioned in the math problem.</p>
<p>We introduce ALGES, 1 which maps an unseen multi-sentence algebraic word problem into a set of possible equation trees.Figure 1 shows an equation tree alongside the word problem it represents.</p>
<p>ALGES generates the space of trees via Integer Linear Programming (ILP), which allows it to con- 1 The code and data is publicly available at https:// gitlab.cs.washington.edu/ALGES/TACL2015 .with derivations.AQUA [122] structures each question into four components: problem description, multiple-choice answer options, rationale, and correct option label (Figure 12).FinQA [183] presents an expert-annotated dataset comprising 8,281 financial QA pairs, meticulously annotated with numerical reasoning processes to ensure comprehensive explication.PRM800K [124] assigns labels (positive, negative, or neutral) to each step in solving MATH problems sampled by a large-scale generator.The training set encompasses 800K step-level labels across 75K solutions to 12K problems.</p>
<p>Additionally, Patel et al. [176] introduced the SVAMP challenge set to establish a more robust evaluation framework for methods designed to tackle elementary-level math word problems.Alghamdi et al. [181] contributed the inaugural large-scale dataset, ArMATH, for Arabic MWPs.This dataset comprises 6,000 samples of primary-school math problems written in Modern Standard Arabic.Kalyan et al. [184] proposed two datasets, REALFP and SYNTHFP, for a novel task called Fermi Problems (FPs).FPs entail questions whose answers can only be approximately estimated due to the impracticality or impossibility of precise computation.This task necessitates the amalgamation of various reasoning abilities, including suitable abstractions, commonsense knowledge, and the creative synthesis of problem-solving strategies.</p>
<p>Recently, mathematical reasoning has incorporated multiple input modalities such as textual and tabular data [43, 29, 1 University of Washington 2 Allen Institute for AI {amini91, skgabrie, linsh, kedzior, yejin, hannaneh}@cs.washington.edu</p>
<p>Abstract</p>
<p>We introduce a large-scale dataset of math word problems and an interpretable neural math problem solver that learns to map problems to operation programs.Due to annotation challenges, current datasets in this domain have been either relatively small in scale or did not offer precise operational annotations over diverse problem types.We introduce a new representation language to model precise operation programs corresponding to each math problem that aim to improve both the performance and the interpretability of the learned models.Using this representation language, our new dataset, MathQA, significantly enhances the AQuA dataset with fully-specified operational programs.We additionally introduce a neural sequence-to-program model enhanced with automatic problem categorization.Our experiments show improvements over competitive baselines in our MathQA as well as the AQuA datasets.The results are still significantly lower than human performance indicating that the dataset poses new challenges for future research.Our dataset is available at: https: //math-qa.github.io/math-QA/.</p>
<p>Introduction</p>
<p>Answering math word problems poses unique challenges for logical reasoning over implicit or explicit quantities expressed in text.Math wordproblem solving requires extraction of salient information from natural language narratives.Automatic solvers must transform the textual narratives into executable meaning representations, a process that requires both high precision and, in the case of story problems, significant world knowledge.</p>
<p>As shown by the geometry question in Figure 1, math word problems are generally narratives describing the progress of actions and relations over some entities and quantities.The operation pro- gram underlying the problem in Figure 1 highlights the complexity of the problem-solving task.Here, we need the ability to deduce implied constants (pi) and knowledge of domain-specific formulas (area of the square).</p>
<p>In this paper, we introduce a new operationbased representation language for solving math word problems.We use this representation language to construct MathQA 1 , a new large-scale, diverse dataset of 37k English multiple-choice math word problems covering multiple math domain categories by modeling operation programs corresponding to word problems in the AQuA dataset (Ling et al., 2017).We introduce a neural model for mapping problems to operation programs with domain categorization.30].For instance, TabMWP [43] presents a new dataset comprising 38,431 open-domain grade-level problems that necessitate mathematical reasoning across both textual and tabular data.Zhu et al. [29] introduced a large-scale QA dataset, TAT-QA, encompassing both tabular and textual data.This dataset often requires numerical reasoning-such as addition, subtraction, multiplication, division, counting, comparison/sorting, and their combinations-based on real financial reports to infer answers.Additionally, MultiHiertt [30] is a proposed dataset consisting of QA pairs derived from Multi Hierarchical Tabular and Textual data (Figure 13).Each document in this dataset contains multiple tables and longer unstructured texts, accompanied by fine-grained annotations of reasoning processes and supporting facts to elucidate complex numerical reasoning.</p>
<p>Theorem Proving</p>
<p>Current research delves into exploring mathematical theorem datasets (e.g., INT [70], Feit-Thompson [187], and IsarStep [189]) to develop novel machine learning-based theoremproving strategies.For instance, the format of IsarStep is shown in 14.Huang et al. [187] introduced the Feit-Thompson dataset, encompassing 1,602 lemmas, expanding into 83,478 proof states for the Feit-Thompson theorem [202].IsarStep [189]   reasoning over hybrid data containing both textual and tabular content (Zhu et al., 2021;Chen et al., 2021) has attracted much attention.For example, Fig. 13: An example of MultiHiertt, taken from [30].</p>
<p>tracted from the largest repository of proofs handwritten by human experts in a theorem prover.Furthermore, Natural-Proofs [69] comprises 32,000 theorem statements and proofs, 14,000 definitions, and 2,000 other types of pages (e.g., axioms, corollaries).This dataset draws from three domains: broad-coverage data sourced from ProofWiki, deep-coverage data from the Stacks project, and low-resource data extracted from mathematics textbooks.</p>
<p>Several formal mathematical libraries [203,204,185] encompass a range of formal languages tailored for theorem proving (TP).Examples include MML [185], Coq [205], Lean [92], Isabelle [206], and Hol light [207].For instance, MML [185], also known as the Mizar Mathematical Library, was established to systematically construct a centralized, reusable knowledge base reflecting the standard foundations of mathematics, namely classical first-order logic and set theory.Yang et al. [68] compiled the CoqGym dataset, comprising 71,000 human-written proofs from 123 projects developed using the Coq proof assistant.LeanStep [92] serves as the tactic proof dataset for the Lean theorem prover, featuring high-level human-written tactics alongside kernel-level proof terms.LISA [28], one of the largest proof corpora Published as a conference paper at ICLR 2021  Therefore it usually cannot be simply solved by pattern matching and rewriting.To succeed in this task, a model is required to learn the meaning of important mathematical concepts (e.g.determinant in linear algebra, residue in complex analysis), how they are related to each other through theorems, and how they are utilised in proof derivations.Solving the IsaStep task will be potentially helpful for improving the automation of theorem provers, because proposing a valid intermediate proposition will help reduce their search space significantly.It is also a first step towards the long-term goal of sketching complete human-readable proofs automatically.</p>
<p>We have built the IsarStep dataset by mining arguably the largest publicly-hosted repository of mechanised proofs: the Achieve of Formal Proofs (AFP). 1 The AFP is checked by the Isabelle proof assistant (Paulson, 1994) and contains 143K lemmas.Combining the AFP with the standard library of Isabelle/HOL yields a dataset of 204K formally-proved lemmas.The dataset covers a broad spectrum of subjects, including foundational logic (e.g.Gödel's incompleteness theorems), advanced analysis (e.g. the Prime Number Theorem), computer algebra, cryptographic frameworks, and various data structures.A nice property of the mined formal proofs is that they are mostly declarative proofs, a proof style very close to human prose proofs.2Fig. 1 illustrates the proof of irrationality of √ 2 in Isabelle.We can see that the proof is actually legible (even to people who are not familiar with the system) and and it captures high-level structures like those in human proofs.</p>
<p>We further explore the reasoning capabilities of neural models.We frame the proposed task as a sequence-to-sequence (seq2seq) prediction problem.Beyond evaluating the existing neural seq2seq model baselines-the seq2seq with attention (Bahdanau et al., 2015), the transformer (Vaswani et al., 2017)-we also propose a new architecture, the hierarchical transformer ( §4).The architecture is motivated by the way humans reason about propositions; it consists of a set of local transformer layers, modelling the representation of each proposition, and a set of global layers, modelling the correlation across propositions.Experiments ( §5) show that these neural models can solve 15-25% of problems on the test set, and the hierarchical transformer achieves the best result.Further analysis ( §6) on the output of these models shows that while the proposition synthesis task is hard, the neural models can indeed capture mathematical reasoning.We find that the embeddings of closely related mathematical concepts are close in cosine space; models can reason about the relation between set, subset, and member, and perform more complex multi-step reasoning that is even hard for humans.</p>
<p>Our contributions are summarised as follows:</p>
<ol>
<li>We mine a large non-synthetic dataset of formal proofs and propose a task for evaluating neural models' mathematical reasoning abilities.The dataset contains 820K training examples with a vocabulary size of 30K.</li>
</ol>
<p>Fig. 14: An example of IsarStep, taken from [189].</p>
<p>for interactive theorem provers, contains 183,000 theorems and 2.16 million proof steps.This dataset is extracted from the Archive of Formal Proofs and the standard library of Isabelle.Kaliszyk et al. [186] introduced the HolStep dataset rooted in Higher-Order Logic (HOL) proofs.HOList [188] includes almost 30,000 theorems and proofs across three corpora: core, complex, and flyspeck.The core corpus contains fundamental theorems necessary for defining tactics, while the complex corpus consists of theorems related to complex calculus.Flyspeck contains the majority of the lemmas and theorems related to the Kepler conjecture [208].</p>
<p>b. Benchmark Datasets</p>
<p>Various benchmark datasets have been introduced to assess the performance of different mathematical algorithms, such as MAWPS [192].Huang et al. [191] developed the Dol-phin18K dataset, a large-scale and diverse collection containing over 18,000 annotated math word problems.The Mathematics dataset [65] encompasses problems spanning arithmetic, algebra, probability, and calculus.These problems involve sequential questions and answers presented in a freeform textual input/output format.For assessing arithmetic understanding, Mishra et al. [190] introduced NumGLUE, a multi-task benchmark that evaluates AI systems across eight distinct tasks.miniF2F [196] is a dataset designed to create a unified cross-system benchmark for neural theorem proving.It comprises 488 formal Olympiad-level mathematics problem statements, encompassing Metamath, Lean, Isabelle, and HOL Light.Several datasets consider varying difficulty levels across educational stages, ranging from elementary school [193,194] to high school [105].Miao et al. [193] introduced the ASDiv corpus, comprising 2,305 math word problems (MWPs) categorized by problem type and elementary school grade level.The Multilingual Grade School Math Benchmark (MGSM) [194] leverages problems from GSM8K, translating them into 10 languages with the assistance of human annotators.The MATH dataset [105] contains 12,500 problems sourced from high school math competitions.Each problem within MATH is equipped with a step-by-step solution, enabling models to learn to generate answer derivations and explanations.</p>
<p>Furthermore, several datasets serve to evaluate foundation models [195,76].AGIEval [195] functions as a humancentric benchmark tailored to assess foundation models' general abilities in tasks related to human cognition and problem-solving.This dataset is collected from standardized human-centric exams like college entrance exams, law school admission tests, math competitions, and lawyer qualification tests.Yuan et al. [76] introduced the arithmetic dataset MATH 401, designed to test the latest large language models such as GPT-4, ChatGPT, InstrctGPT, Galactica, and LLaMA.It features various arithmetic expressions and offers a comprehensive analysis of the capabilities of large language models.</p>
<p>c. Augmented Datasets</p>
<p>Augmented datasets serve to enhance existing datasets by incorporating additional samples or information.Roy et al. [197] introduced Aggregate by augmenting the AllArith dataset with 661 word problems from Perturb.Perturb contains problems that were manually pruned either for not yielding the desired solution a or being too dissimilar from the input problem p. Large-scale language models like ChatGPT and GPT-4 have been utilized to expand datasets.For instance, MetaMathQA [60] rephrases questions from multiple perspectives without introducing extra knowledge to bootstrap mathematical questions.The Math5K dataset [199] comprises 50,000 problem-solution pairs generated using GPT-4.These datasets leverage advanced language models to enrich the existing collection of problems and solutions.</p>
<p>Several studies have expanded existing datasets by including supplementary information, such as code programs [198,66] and explanations [200].MathQA-Python [198] is a Python adaptation of the MathQA benchmark, comprising 23,914 problems.This dataset aims to evaluate models' capabilities in synthesizing code from intricate textual descriptions.Lila [66] is a comprehensive mathematical reasoning benchmark composed of 23 diverse tasks.It extends 20 existing datasets by gathering task instructions and solutions as Python programs, offering not only correct answers but also explainable solutions.PEN [200] focuses on providing plausible and accurate explanations for solving algebraic word problems present in three benchmark datasets: ALG514, DRAW-1K, and MAWPS.Moreover, some studies have contributed statements and proofs based on existing datasets for theorem proving [90,201].For example, miniF2F+informal [90] is a dataset consisting of manually curated informal statements and proofs aligned with formal statements from the miniF2F dataset.NaturalProofs-Gen [201] adapts data from NATURALPROOFS, including theorem statements, proofs, definitions, and additional pages sourced from ProofWiki.Each proof follows a multi-step structure and references a variable number of sources.</p>
<p>VI. CHALLENGES AND FURTHER DIREC-TIONS</p>
<p>Faithfulness.Mathematical LLMs reside in the phenomena of hallucination and faithfulness, where the generated output may lack factual accuracy or logical grounding [209,99].This phenomenon leads to the production of erroneous or misleading mathematical results, undermining the reliability of the model's outputs.Some studies explored to address this problem by integrating extra knowledge [136], reinforcement learning from human feedback [124], tools [115,120], and verify-based methods [132,133,134,135,136,137].</p>
<p>However, the improvement of the hallucination phenomena is limited, which influences the trustworthiness and utility of mathematical language models in practical applications and scholarly pursuits.</p>
<p>Multi-modal.In particular, math problems (e.g., geometry problems [210]) involve not only textual information but also various modalities such as diagrams, graphs, or mathematical symbols [82].While existing models excel in processing textual information, interpreting and reasoning across multiple modalities concurrently remains a formidable task.Few multi-modal mathematical datasets and methods are proposed for this task [30,82].Bridging the gap between textual representations and visual/mathematical elements necessitates robust mechanisms that can effectively capture and synthesize information from disparate sources, ultimately leading to comprehensive and accurate problem-solving capabilities.In fact, the multi-modal information in mathematics is much more complex than general multi-modal tasks like vision question answering [211] and image captioning [212].Achieving proficiency in handling multi-modal mathematical problems stands as a pivotal yet intricate objective in advancing the competency of mathematical LLMs.</p>
<p>Uncertainty.The uncertainty of LLMs [213,214] leads to the ambiguity and variability problem of mathematical problems.While these models excel in deterministic tasks, their handling of uncertainty, such as probabilistic reasoning or dealing with incomplete or vague information, poses a significant challenge.Mathematical problems often entail nuanced interpretations, fuzzy constraints, or scenarios where a single precise solution might not exist.Several studies investigated this problem via controlled generation technologies [215].However, ensuring that LLMs can navigate and appropriately account for these uncertainties while providing accurate and contextually relevant solutions remains a complex task.</p>
<p>Evaluation.It is still a challenge to evaluate mathematical LMs with robust and comprehensive evaluation metrics that adequately capture the models' performance across various mathematical tasks.Traditional evaluation metrics in natural language processing might not effectively capture the intricacies of mathematical reasoning and problem-solving.Designing evaluation benchmarks [65,105,141,6] and metrics [216,217,218,76] that encompass a wide spectrum of mathematical tasks, spanning from basic arithmetic to complex theorem proving, while accounting for linguistic fluency and mathematical accuracy, remains a significant challenge.Addressing these challenges is crucial to ascertain the reliability, efficacy, and generalizability of mathematical LMs, fostering advancements in this burgeoning field.</p>
<p>Creation.While previous models exhibit remarkable capabilities in understanding and manipulating existing mathematical concepts, their ability to autonomously devise and rigorously prove entirely new theorems presents a formidable hurdle.The development of novel mathematical theorems demands not only profound mathematical reasoning but also creative and insightful problem-solving abilities, aspects that necessitate a deeper understanding of abstract mathematical concepts beyond what the models have been trained on.Davies et al. [169] applied machine learning techniques to discover potential patterns and relationships among mathematical entities.Their approach involved employing attribution techniques to comprehend these connections, leveraging these insights to foster intuition and put forward conjectures.Bridging the gap between the models' proficiency in assimilating existing mathematical knowledge and their capability to generate novel, verifiable, and impactful theorems represents a significant frontier in leveraging LLMs for the advancement of mathematical knowledge and discovery.</p>
<p>Application.While mathematical LMs exhibit promising potential in autonomously solving math problems, their deployment in educational settings as teaching aids or tutors necessitates addressing several pivotal challenges.Tailoring these models to serve as effective educational tools demands not only mathematical proficiency but also adeptness in pedagogy and instructional methodologies.Few studies applied Socratic questioning for mathematical teaching [158,219].Customizing LLMs to cater to diverse learning styles, adapting explanations to suit different proficiency levels, and fostering an interactive and engaging learning environment is interesting.</p>
<p>Data scarcity.The training data significantly influences the performance of language models, particularly in LLMs [220].High-quality and diverse training data can assist the model in enhancing its mathematical reasoning capabilities [61].As discussed in Section a, while there have been limited studies on constructing instruction data through LLMs, these efforts have only considered building from a small set of mathematical reasoning datasets, such as GSM8k [141] and MATH [105].High-quality and diverse mathematical instruction data remains scarce.There is a necessity to explore and investigate additional forms and construction methods for mathematical train data.Additionally, the generation of mathematical training datasets in a multimodal context is also a promising direction.</p>
<p>VII. CONCLUSIONS</p>
<p>The survey elucidates the pivotal role of mathematical LMs in reshaping the landscape of mathematical problem-solving, leveraging a spectrum of models, from pre-trained language models (PLMs) to large-scale language models (LLMs), to address diverse mathematical tasks.Our taxonomical delineation of mathematical tasks and methods provides a systematic framework for comprehending the intricacies of LMsbased methodologies, distinguishing between arithmetic calculation, mathematical reasoning, and various algorithmic approaches employed in these models.The compilation of over 60 diverse mathematical datasets, categorized meticulously into training, benchmark, and augmented datasets, underscores the pivotal role of data in advancing mathematical research, facilitating informed research endeavors within distinct mathematical contexts.Moreover, by critically addressing challenges such as faithfulness, multi-modality, uncertainty, evaluation, theorem creation, application, and Data scarcity, this survey paves the way for future investigations aimed at refining and advancing mathematical LMs capabilities.By shedding light on the current state-of-the-art, challenges, and avenues for future exploration, we envision this comprehensive overview to be a cornerstone in driving innovation and shaping the trajectory of mathematical LMs research, ultimately contributing to the evolving landscape of mathematics and artificial intelligence.</p>
<p>arXiv:2312.07622v2 [cs.CL] 14 Dec 2023 MATHEMATICAL LANGUAGE MODEL: A SURVEY LIU et al.</p>
<p>Fig. 1 :
1
Fig. 1: Taxonomy of mathematical tasks.</p>
<p>Computing, Engineering and Applied Mathematics ACADEMIC JOURNAL ON COMPUTING, ENGINEERING AND APPLIED MATHEMATICS, VOL.XX, NO.XX,</p>
<p>Fig. 3 :
3
Fig. 3: The distinctions between Autoregression LMs andNon-Autoregression LMs.</p>
<p>MATHEMATICAL LANGUAGE MODEL: A SURVEY LIU et al.</p>
<p>Figure 1 :
1
Figure 1: Running Examples of Evol-Instruct.</p>
<p>Computing, Engineering and Applied Mathematics ACADEMIC JOURNAL ON COMPUTING, ENGINEERING AND APPLIED MATHEMATICS, VOL.XX, NO.XX, Month 20XX 293</p>
<p>Compute 1201 plus 1302 .
1302
Final result = 2503 Spaced Compute 1201 plus 1302. 1 2 0 1 plus 1 3 0 2 = 2 5 0 3. Final result = 2503</p>
<p>Figure 2 :
2
Figure 2: Example of input and target for addition with a scratchpad.The carry is recorded in the digit following "C:".Comments (marked by #) are added for clarity and are not part of the target.</p>
<p>Figure 1 :Fig. 8 :
18
Figure1: Chain-of-thought prompting enables large language models to tackle complex arithmetic, commonsense, and symbolic reasoning tasks.Chain-of-thought reasoning processes are highlighted.</p>
<p>Computing, Engineering and Applied Mathematics ACADEMIC JOURNAL ON COMPUTING, ENGINEERING AND APPLIED MATHEMATICS, VOL.XX, NO.XX, Month 20XX</p>
<p>Arithmetic word ProblemLiz had 9 black kittens.She gave some of her kittens to Joan.Joan now has 11 kittens.Liz has 5 kittens left and 3 have spots.How many kittens did Joan get?Liz gave some of her kittens to Joan.</p>
<p>Figure 1 :
1
Figure 1: Example problem and solution.make sense of multiple sentences, as shown in Figure 2, without a priori restrictions on the syntax or vocabulary used to describe the problem.Figure 1 shows an example where ARIS is asked to infer how many kittens Joan received based on facts and constraints expressed in the text, and represented by the state diagram and corresponding equation.While the equation is trivial, the text could have involved assembling toy aircraft, collecting coins, eating cookies, or just about any activity involving changes in the quantities of discrete objects.This paper investigates the task of learning to solve such problems by mapping the verbs in the problem text into categories that describe their impact on the world state.While the verbs category is crucial (e.g., what happens if "give" is replaced by "receive" in Figure1?), some elements of the problem are irrelevant.For instance, the fact that three kittens have spots is immaterial to the solution.Thus, ARIS has to determine what information is relevant to solving the problem.To abstract from the problem text, ARIS maps the text to a state representation which consists of</p>
<p>Figure 1 :
1
Figure 1: Example problem and solution</p>
<p>Fig. 10 :
10
Fig.10: An example of SingleEQ, taken from[9].</p>
<p>Figure 1 :
1
Figure 1: Example of a math word problem aligned with representation language by crowd-sourced annotation</p>
<p>Fig. 11 :Figure 1 :
111
Fig.11: An example of MathQA, taken from[123].Problem 1: Question: Two trains running in opposite directions cross a man standing on the platform in 27 seconds and 17 seconds respectively and they cross each other in 23 seconds.The ratio of their speeds is: Options: A) 3/7 B) 3/2 C) 3/88 D) 3/8 E) 2/2 Rationale: Let the speeds of the two trains be x m/sec and y m/sec respectively.Then, length of the first train = 27x meters, and length of the second train = 17 y meters.(27x + 17y) / (x + y) = 23 → 27x + 17y = 23x + 23y → 4x = 6y → x/y = 3/2.Correct Option: B Problem 2: Question: From a pack of 52 cards, two cards are drawn together at random.What is the probability of both the cards being kings?Options: A) 2/1223 B) 1/122 C) 1/221 D) 3/1253 E) 2/153 Rationale: Let s be the sample space.Then n(s) = 52C2 = 1326 E = event of getting 2 kings out of 4 n(E) = 4C2 = 6 P(E) = 6/1326 = 1/221 Answer is C Correct Option: C Problem 3: Question: For which of the following does p(a)−p(b) = p(a− b) for all values of a and b?Options:A) p(x) = x 2 , B) p(x) = x/2, C) p(x) = x + 5, D) p(x) = 2x1, E) p(x) =|x| Rationale: To solve this easiest way is just put the value and see that if it equals or not.with option A. p(a) = a 2 and p(b) = b 2 so L.H.S = a 2 − b 2 and R.H.S = (a − b) 2 → a 2 + b 2 − 2ab.so L.H.S not equal to R.H.S with option B. p(a) = a/2 and p(b) = b/2 L.H.S = a/2 − b/2 → 1/2(a − b) R.H.S = (a − b)/2 so L.H.S = R.H.S which is the correct answer.answer:B Correct Option: B Figure 1: Examples of solved math problems.</p>
<p>Fig. 12 :
12
Fig.12: An example of AQUA, taken from[122].</p>
<p>Figure 1 :
1
Figure 1: An example of MULTIHIERTT:The system needs to first locate which segment got the most funds in 2017 in the second hierarchical table, then select relevant numbers from the first hierarchical table and generate the correct reasoning program to get the answer.The annotated supporting facts are highlighted in red, and the hierarchical column and row headers are highlighted in orange and green, respectively.</p>
<p>Figure 1 :
1
Figure 1: Full declarative proof the irrationality of √ 2 in Isabelle/HOL.</p>
<p>Computing, Engineering and Applied Mathematics ACADEMIC JOURNAL ON COMPUTING, ENGINEERING AND APPLIED MATHEMATICS, VOL.XX, NO.XX, Month 20XX</p>
<p>Increase Reasoning Complicate Input (Code) Concretizing Add Constraints Deepening Deepening Increase Reasoning Complicate Input (Table) In-Breadth Evolving Initial Instruction</p>
<p>Table 5 :
5
Comparison of number tokenization of various LLMs.It should be noted that ChatGLM also splits each digit into an individual token.Evaluating ChatGLM's arithmetic abilities will be left as future work.</p>
<p>Table 1 :
1
Examples of addition training observations for the considered approaches.Bold sub-strings represent input prompts provided to LMs at inference time.The same examples for the subtraction and multiplication tasks can be obtained substituting {plus, +, sum} with {minus, -, subtract} and {times, *, multiply} respectively.prompt 3 only 4 few-shot examples with the decomposition pipeline that we introduced.</p>
<p>TABLE 1 :
1
introduces a novel dataset featuring 1,000 general algebra word problems, each annotated THE STATISTICS INFORMATION OF MATHEMATICAL DATASETS.SOLUTION MEANS THE FORMAT OF THE OUTPUT, SUCH AS TEXT, FORMULA AND CODE.
C M A E A J Academic Journal on Computing, Engineering and Applied MathematicsACADEMIC JOURNAL ON COMPUTING, ENGINEERING AND APPLIED MATHEMATICS, VOL. XX, NO. XX, Month 20XXDataset#Train#Val#Test#TotalLanguageTaskTypeSolutionVERBPHYSICS [173]7331,096 1,8283,657ENCalculation TrainingFormulaClinical [174, 55]11,1701,625 3,22016,015ENCalculation TrainingFormulaScientific [55]14,6942,037 4,23120,962ENCalculation TrainingFormulaDoQ [175]5875,418 6,00712,012ENCalculation TrainingFormulaDROP [64]77,4099,536 9,62296,567ENCalculation TrainingTextAddSub [5]---395ENMWPTrainingFormulaSingleOp [9]265107159531ENMWPTrainingFormulaSingleEq [12]---508ENMWPTrainingFormulaMultiArith [10]420-180600ENMWPTrainingFormulaAlg514 [6]---514ENMWPTrainingFormulaMath23k [13]22,162-100023,162CHMWPTrainingFormulaAQuA [122]97,46725425497,975ENMWPTrainingTextMathQA [123]29,8374,475 28,98537,297ENMWPTrainingFormulaGSM8K [141]7,4731,3198,792ENMWPTrainingTextSVAMP [176]7003001,000ENMWPTrainingFormulaDRAW [177]---1,000ENMWPTrainingFormulaDolphin1878 [11]-3741,5041,878ENMWPTrainingFormulaAllArith [178]---831ENMWPTrainingFormulaDRAW-1K [179]---1,000ENMWPTrainingFormulaHMWP [180]---5,470CHMWPTrainingFormulaArMATH [181]---6,000ArabicMWPTrainingFormulaTabMWP [43]---38,431ENMWPTrainingTextApe210K [182] TAL-SCQ5K-CH 1 TAL-SCQ5K-EN 2-3,000 3,000----2,000 2,000210,488 5,000 5,000CH CH ENMWP MWP MWPTraining Training TrainingFormula Text TextFinQA [183]6,2518831,1478,281ENMWPTrainingFormulaREALFP [184]185185558928ENMWPTrainingFormulaSYNTHFP [184]10,0001,000 1,00012,000ENMWPTrainingFormulaTAT-QA [29]---16,552ENMWPTrainingTextMultiHiertt [30]7,8301,044 1,56610,440ENMWPTrainingFormulaMML [185]---57,882ENTPTrainingFormulaHolStep [186]2,013,046 -196,030 2,209,076ENTPTrainingFormulaFeit-Thompson [187]---83,478ENTPTrainingFormulaCoqGym [68]---71,000ENTPTrainingFormulaHOList [188]---29,462ENTPTrainingFormulaIsarStep [189]820,0005,000 5,000830,000ENTPTrainingFormulaLISA [28]---183,000ENTPTrainingFormulaNaturalProofs [69]32,000ENTPTrainingTextLeanStep [92]---21,606,000 ENTPTrainingFormulaNumGLUE [190]---101,835ENCalculation Benchmark TextDophin18k [191]---18,460ENMWPBenchmark TextMAWPS [192]---3,320ENMWPBenchmark FormulaASDiv [193]---2,305ENMWPBenchmark FormulaMATH [105]7,5005,00012,500ENMWPBenchmark TextMGSM [194]----Multilingual MWPBenchmark TextMathematics [65]2,000,000100,000 2,100,000ENMWPBenchmark FormulaMMLU-Math [163]---906ENWMPBenchmark FormulaAGIEval [195]---469/220CH/ENMWPBenchmark FormulaMATH 401 [76]---401ENMWPBenchmark FormulaINT [70]----ENTPBenchmark FormulaminiF2F [196]-244244488ENTPBenchmark FormulaAggregate [197]---1,492ENMWPAugmented FormulaMathQA-Python [198]19,2092,822 2,82223,914ENMWPAugmented CodeMath50k [199]---50,000ENWMPAugmented TextPRM800K [124]---2,868ENWMPAugmented TextMetaMathQA [60]---395,000ENMWPAugmented TextLila [66]---134,000ENMWPAugmented CodePEN [200]---3,581ENMWPAugmented FormulaminiF2F+informal [90]-244244488ENTPAugmented FormulaNaturalProofs-Gen [201] 12,5001,000 1,00014,500ENTPAugmented Text</p>
<p>constitutes a non-synthetic dataset ex-Academic Journal on Computing, Engineering and Applied Mathematics ACADEMIC JOURNAL ON COMPUTING, ENGINEERING AND APPLIED MATHEMATICS, VOL.XX, NO.XX, Month 20XXniversity4Penn State University du 1155124348@link.cuhk.edu.hkheastern.edurmz5227@psu.edu
C M A E A Jntain-finan-atten-isting er hy-each multi-le hi-alyti--scale pairs xtual lth of nique mul-ts; 2) al; 3) ques-an ex-anno-orting meri-novel pplies orting ses a ason-mpre-. TheERTTbase-erfor-codehub..s such asrical rea-g textual; Xie andal., 2021;umerical
thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.A: The cafeteria had 23 apples originally. They used 20 to make lunch. So they had 23 -20 = 3. They bought 6 more apples, so they have 3 + 6 = 9. The answer is 9.Chain-of-Thought PromptingQ: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?A: The answer is 11.Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?A: The answer is 27.
The dataset is available at: https://math-qa. github.io/math-QA/
https://www.isa-afp.org
A comparison of proofs in different systems is available in Wiedijk (2006). The declarative style proof is also available in Mizar(Grabowski et al., 2010), where the style originates.
ACADEMIC JOURNAL ON COMPUTING, ENGINEERING AND APPLIED MATHEMATICS, VOL. XX, NO. XX, Month 20XX</p>
<p>. E A Feigenbaum, J Feldman, Computers and thought. 71963</p>
<p>Natural language input for a computer problem solving system. D Bobrow, 1964</p>
<p>An integrated model of skill in solving elementary word problems. D J Briars, J H Larkin, Cognition and instruction. 131984</p>
<p>Understanding and solving arithmetic word problems: A computer simulation. C R Fletcher, Behavior Research Methods, Instruments, &amp; Computers. 1751985</p>
<p>Learning to solve arithmetic word problems with verb categorization. M J Hosseini, H Hajishirzi, O Etzioni, N Kushman, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. A Moschitti, B Pang, W Daelemans, the 2014 Conference on Empirical Methods in Natural Language ProcessingDoha, QatarAssociation for Computational LinguisticsOct. 2014</p>
<p>Learning to automatically solve algebra word problems. N Kushman, Y Artzi, L Zettlemoyer, R Barzilay, Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Long Papers. K Toutanova, H Wu, the 52nd Annual Meeting of the Association for Computational LinguisticsBaltimore, MarylandAssociation for Computational LinguisticsJun. 20141</p>
<p>Learn to solve algebra word problems using quadratic programming. L Zhou, S Dai, L Chen, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language Processing2015</p>
<p>Learning to use formulas to solve simple arithmetic problems. A Mitra, C Baral, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsPapersLong20161</p>
<p>Reasoning about quantities in natural language. S Roy, T Vieira, D Roth, Transactions of the Association for Computational Linguistics. 32015</p>
<p>Solving general arithmetic word problems. S Roy, D Roth, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language Processing2015</p>
<p>Automatically solving number word problems by semantic parsing and reasoning. S Shi, Y Wang, C.-Y Lin, X Liu, Y Rui, Proceedings of the 2015 conference on empirical methods in natural language processing. the 2015 conference on empirical methods in natural language processing2015</p>
<p>Parsing algebraic word problems into equations. R Koncel-Kedziorski, H Hajishirzi, A Sabharwal, O Etzioni, S D Ang, 2015Transactions of the Association for Computational Linguistics3</p>
<p>Deep neural solver for math word problems. Y Wang, X Liu, S Shi, Proceedings of the 2017 conference on empirical methods in natural language processing. the 2017 conference on empirical methods in natural language processing2017</p>
<p>Large language models and mathematical understanding. J Couperus, 2023Master's thesis</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Learning mathematics with large language models: A comparative study with computer algebra systems and other tools. N Matzakos, S Doukakis, M Moundridou, Liu, MATHEMATICAL LANGUAGE MODEL: A SURVEY. 20231851</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. J D , M.-W C Kenton, L K Toutanova, Proceedings of NAACL-HLT. NAACL-HLT2019</p>
<p>Llama: Open and efficient foundation language models. H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, arXiv:2302.139712023arXiv preprint</p>
<p>BERT: Pretraining of Deep Bidirectional Transformers for Language Understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, May 2019</p>
<p>RoBERTa: A robustly optimized BERT pretraining approach. Y Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, Jul. 2019</p>
<p>Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. M Lewis, Y Liu, N Goyal, M Ghazvininejad, A Mohamed, O Levy, V Stoyanov, L Zettlemoyer, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Improving Language Understanding by Generative Pre-Training. A Radford, K Narasimhan, T Salimans, I Sutskever, Jun. 201812</p>
<p>Language Models are Unsupervised Multitask Learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, Feb. 201924</p>
<p>. P Clark, O Etzioni, D Khashabi, T Khot, B D Mishra, K Richardson, A Sabharwal, C Schoenick, O Tafjord, N Tandon, S Bhakthavatsalam, D Groeneveld, M Guerquin, M Schmitz, Feb. 2021From 'F' to 'a' on the N.Y. regents science exams: An overview of the aristo project</p>
<p>Injecting numerical reasoning skills into language models. M Geva, A Gupta, J Berant, Apr. 2020</p>
<p>Injecting numerical reasoning skills into knowledge base question answering models. Y Feng, J Zhang, X Zhang, L Liu, C Li, H Chen, May 2022</p>
<p>MathBERT: A pre-trained model for mathematical formula understanding. S Peng, K Yuan, L Gao, Z Tang, May 2021</p>
<p>Lisa: Language models of isabelle proofs. A Q Jiang, W Li, J M Han, Y Wu, 6th Conference on Artificial Intelligence and Theorem Proving. 2021</p>
<p>Tat-qa: A question answering benchmark on a hybrid of tabular and textual content in finance. F Zhu, W Lei, Y Huang, C Wang, S Zhang, J Lv, F Feng, T.-S Chua, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing20211</p>
<p>Multihiertt: Numerical reasoning over multi hierarchical tabular and textual data. Y Zhao, Y Li, C Li, R Zhang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Learning to reason deductively: Math word problem solving as complex relation extraction. Z Jie, J Li, W Lu, Sep. 2022</p>
<p>Seeking patterns, not just memorizing procedures: Contrastive learning for solving math word problems. Z Li, W Zhang, C Yan, Q Zhou, C Li, H Liu, Y Cao, Mar. 2022</p>
<p>Gpt-4 technical report. Openai, 2023</p>
<p>Llama 2: Open foundation and fine-tuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Metaicl: Learning to learn in context. S Min, M Lewis, L Zettlemoyer, H Hajishirzi, Proceedings of the 2022 Conference of the North American Chapter. the 2022 Conference of the North American ChapterHuman Language Technologies2022</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 202033</p>
<p>Meta-learning via language model in-context tuning. Y Chen, R Zhong, S Zha, G Karypis, H He, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics2022</p>
<p>Gpt can solve mathematical problems without a calculator. Z Yang, M Ding, Q Lv, Z Jiang, Z He, Y Guo, J Bai, J Tang, arXiv:2309.032412023arXiv preprint</p>
<p>Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks. T Liu, B K H Low, arXiv:2305.14201May 2023</p>
<p>. Online, </p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, Q V Le, E H Chi, S Narang, A Chowdhery, D Zhou, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Palm: Scaling language modeling with pathways. A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, arXiv:2204.023112022arXiv preprint</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. W Chen, X Ma, X Wang, W W Cohen, arXiv:2211.125882022arXiv preprint</p>
<p>Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. P Lu, L Qiu, K.-W Chang, Y N Wu, S.-C Zhu, T Rajpurohit, P Clark, A Kalyan, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Automatic chain of thought prompting in large language models. Z Zhang, A Zhang, M Li, A Smola, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Complexitybased prompting for multi-step reasoning. Y Fu, H Peng, A Sabharwal, P Clark, T Khot, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Pal: Program-aided language models. L Gao, A Madaan, S Zhou, U Alon, P Liu, Y Yang, J Callan, G Neubig, International Conference on Machine Learning. PMLR202310799</p>
<p>A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level. I Drori, S Zhang, R Shuttleworth, L Tang, A Lu, E Ke, K Liu, L Chen, S Tran, N Cheng, Proceedings of the National Academy of Sciences. 11932e21234331192022</p>
<p>Solving math word problems by combining language models with symbolic solvers. J He-Yueya, G Poesia, R E Wang, N D Goodman, arXiv:2304.091022023arXiv preprint</p>
<p>Llemma: An open language model for mathematics. Z Azerbayev, H Schoelkopf, K Paster, M Santos, S Mcaleer, A Q Jiang, J Deng, S Biderman, S Welleck, Minerva. 8164B</p>
<p>A survey of deep learning for mathematical reasoning. P Lu, L Qiu, W Yu, S Welleck, K.-W Chang, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. A Rogers, J Boyd-Graber, N Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJul. 20231</p>
<p>Reasoning with language model prompting: A survey. S Qiao, Y Ou, N Zhang, X Chen, Y Yao, S Deng, C Tan, F Huang, H Chen, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. A Rogers, J Boyd-Graber, N Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJul. 20231</p>
<p>A survey of chain of thought reasoning: Advances, frontiers and future. Z Chu, J Chen, Q Chen, W Yu, T He, H Wang, W Peng, M Liu, B Qin, T Liu, 2023</p>
<p>Pre-trained models for natural language processing: A survey. X Qiu, T Sun, Y Xu, Y Shao, N Dai, X Huang, Science China Technological Sciences. 63102020</p>
<p>H Naveed, A U Khan, S Qiu, M Saqib, S Anwar, M Usman, N Barnes, A Mian, arXiv:2307.06435A comprehensive overview of large language models. 2023arXiv preprint</p>
<p>Numeracy for language models: Evaluating and improving their ability to predict numbers. G Spithourakis, S Riedel, ACL 2018-56th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers). 201856</p>
<p>Do nlp models know numbers? probing numeracy in embeddings. E Wallace, Y Wang, S Li, S Singh, M Gardner, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingEMNLP-IJCNLP2019</p>
<p>Do language embeddings capture scales?. X Zhang, D Ramachandran, I Tenney, Y Elazar, D Roth, Findings of the Association for Computational Linguistics: EMNLP 2020. T Cohn, Y He, Y Liu, Association for Computational LinguisticsNov. 2020</p>
<p>Show Your Work: Scratchpads for Intermediate Computation with Language Models. M Nye, A J Andreassen, G Gur-Ari, H Michalewski, J Austin, D Bieber, D Dohan, A Lewkowycz, M Bosma, D Luan, C Sutton, A Odena, arXiv:2112.00114Nov. 2021</p>
<p>Mathprompter: Mathematical reasoning using large language models. S Imani, L Du, H Shrivastava, arXiv:2303.053982023arXiv preprint</p>
<p>Metamath: Bootstrap your own mathematical questions for large language models. L Yu, W Jiang, H Shi, J Yu, Z Liu, Y Zhang, J T Kwok, Z Li, A Weller, W Liu, arXiv:2309.122842023arXiv preprint</p>
<p>Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. H Luo, Q Sun, C Xu, P Zhao, J Lou, C Tao, X Geng, Q Lin, S Chen, D Zhang, arXiv:2308.095832023arXiv preprint</p>
<p>Mathattack: Attacking large language models towards math solving ability. Z Zhou, Q Wang, M Jin, J Yao, J Ye, W Liu, W Wang, X Huang, K Huang, 2023</p>
<p>Solving geometry problems: Combining text and diagram interpretation. M Seo, H Hajishirzi, A Farhadi, O Etzioni, C Malcolm, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. L Màrquez, C Callison-Burch, J Su, the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalAssociation for Computational LinguisticsSep. 2015</p>
<p>Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. D Dua, Y Wang, P Dasigi, G Stanovsky, S Singh, M Gardner, Proceedings of NAACL-HLT. NAACL-HLT2019</p>
<p>Analysing mathematical reasoning abilities of neural models. D Saxton, E Grefenstette, F Hill, P Kohli, International Conference on Learning Representations. 2019</p>
<p>Lila: A unified benchmark for mathematical reasoning. S Mishra, M Finlayson, P Lu, L Tang, S Welleck, C Baral, T Rajpurohit, O Tafjord, A Sabharwal, P Clark, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Deepmath-deep sequence models for premise selection. G Irving, C Szegedy, A A Alemi, N Eén, F Chollet, J Urban, Advances in neural information processing systems. 201629</p>
<p>Learning to prove theorems via interacting with proof assistants. K Yang, J Deng, International Conference on Machine Learning. PMLR2019</p>
<p>Naturalproofs: Mathematical theorem proving in natural language. S Welleck, J Liu, R Le Bras, H Hajishirzi, Y Choi, K Cho, Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2021</p>
<p>Int: An inequality benchmark for evaluating generalization in theorem proving. Y Wu, A Jiang, J Ba, R B Grosse, International Conference on Learning Representations. 2021</p>
<p>Do language embeddings capture scales. X Zhang, D Ramachandran, I Tenney, Y Elazar, D Roth, Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP. the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP2020</p>
<p>An empirical investigation of contextualized number prediction. T Berg-Kirkpatrick, D Spokoyny, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. B Webber, T Cohn, Y He, Y Liu, the 2020 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsNov. 2020</p>
<p>Investigating the limitations of transformers with simple arithmetic tasks. R Nogueira, Z Jiang, J Lin, arXiv:2102.130192021arXiv preprint</p>
<p>Exploring generalization ability of pretrained language models on arithmetic and logical reasoning. C Wang, B Zheng, Y Niu, Y Zhang, CCF International Conference on Natural Language Processing and Chinese Computing. 2021</p>
<p>Evaluating Transformer Language Models on Arithmetic Operations Using Number Decomposition. M Muffo, A Cocco, E Bertino, </p>
<p>How well do large language models perform in arithmetic tasks. Z Yuan, H Yuan, C Tan, W Wang, S Huang, arXiv:2304.020152023arXiv preprint</p>
<p>Galactica: A Large Language Model for Science. R Taylor, M Kardas, G Cucurull, T Scialom, A Hartshorn, E Saravia, A Poulton, V Kerkez, R Stojnic, arXiv:2211.09085Nov. 2022cs, stat</p>
<p>Teaching algorithmic reasoning via in-context learning. H Zhou, A Nova, H Larochelle, A Courville, B Neyshabur, H Sedghi, Nov. 2022</p>
<p>Length generalization in arithmetic transformers. S Jelassi, S Ascoli, C Domingo-Enrich, Y Wu, Y Li, F Charton, Jun. 2023</p>
<p>Scaling relationship on learning mathematical reasoning with large language models. Z Yuan, H Yuan, C Li, G Dong, C Tan, C Zhou, arXiv:2308.018252023arXiv preprint</p>
<p>Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. P Lu, R Gong, S Jiang, L Qiu, S Huang, X Liang, S.-C Zhu, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing20211</p>
<p>Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning. P Lu, L Qiu, J Chen, T Xia, Y Zhao, W Zhang, Z Yu, X Liang, S.-C Zhu, Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2021</p>
<p>Pgdp5k: A diagram parsing dataset for plane geometry problems. Y Hao, M Zhang, F Yin, L Huang, 2022 26th International Conference on Pattern Recognition (ICPR). 2022</p>
<p>Plane geometry diagram parsing. M.-L Zhang, F Yin, Y.-H Hao, C.-L Liu, Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22. the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-2220227</p>
<p>A multi-modal neural geometric solver with textual clauses parsed from diagram. M.-L Zhang, F Yin, C.-L Liu, IJCAI. 2023</p>
<p>Mining mathematical documents for question answering via unsupervised formula labeling. P Scharpf, M Schubotz, B Gipp, Proceedings of the 22nd ACM/IEEE Joint Conference on Digital Libraries. the 22nd ACM/IEEE Joint Conference on Digital Libraries2022</p>
<p>Empirical explorations of the logic theory machine: A case study in heuristic. A Newell, J C Shaw, H A Simon, 10.1145/1455567.1455605ser. IRE-AIEE-ACM '571957Association for Computing MachineryWestern). New York, NY, USA</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, Advances in neural information processing systems. 201730</p>
<p>Generative language modeling for automated theorem proving. S Polu, I Sutskever, 2020</p>
<p>Draft, sketch, and prove: Guiding formal theorem provers with informal proofs. A Q Jiang, S Welleck, J P Zhou, T Lacroix, J Liu, W Li, M Jamnik, G Lample, Y Wu, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Generative language modeling for automated theorem proving. S Polu, I Sutskever, CoRR. 2009.03393, 2020</p>
<p>Proof artifact cotraining for theorem proving with language models. J M Han, J Rute, Y Wu, E Ayers, S Polu, International Conference on Learning Representations. 2022</p>
<p>Hypertree proof search for neural theorem proving. G Lample, T Lacroix, M.-A Lachaux, A Rodriguez, A Hayat, T Lavril, G Ebner, X Martinet, Advances in Neural Information Processing Systems. 202235</p>
<p>Thor: Wielding hammers to integrate language models and automated theorem provers. A Q Jiang, W Li, S Tworkowski, K Czechowski, T Odrzygóźdź, P Miłoś, Y Wu, M Jamnik, 2022</p>
<p>Autoformalization with large language models. Y Wu, A Q Jiang, W Li, M N Rabe, C Staats, M Jamnik, C Szegedy, NeurIPS. 2022</p>
<p>llmstep: Llm proofstep suggestions in lean. S Welleck, R Saha, 2023</p>
<p>Baldur: whole-proof generation and repair with large language models. E First, M N Rabe, T Ringer, Y Brun, arXiv:2303.049102023arXiv preprint</p>
<p>On faithfulness and factuality in abstractive summarization. J Maynez, S Narayan, B Bohnet, R Mcdonald, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. D Jurafsky, J Chai, N Schluter, J Tetreault, the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJul. 2020</p>
<p>Survey of hallucination in natural language generation. Z Ji, N Lee, R Frieske, T Yu, D Su, Y Xu, E Ishii, Y J Bang, A Madotto, P Fung, ACM Computing Surveys. 55122023</p>
<p>Pretrained language models are symbolic mathematics solvers too. K Noorbakhsh, M Sulaiman, M Sharifi, K Roy, P Jamshidi, arXiv:2110.035012021arXiv preprint</p>
<p>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, </p>
<p>Point to the Expression: Solving Algebraic Word Problems using the Expression-Pointer Transformer Model. B Kim, K S Ki, D Lee, G Gweon, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational Linguistics2020Online</p>
<p>Generate &amp; Rank: A Multi-task Framework for Math Word Problems. J Shen, Y Yin, L Li, L Shang, X Jiang, M Zhang, Q Liu, arXiv:2109.03034Sep. 2021</p>
<p>G Lample, M.-A Lachaux, T Lavril, X Martinet, A Hayat, G Ebner, A Rodriguez, T Lacroix, HyperTree Proof Search for Neural Theorem Proving. </p>
<p>Measuring mathematical problem solving with the math dataset. D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2021</p>
<p>Solving Quantitative Reasoning Problems With Language Models. A Lewkowycz, A Andreassen, D Dohan, E Dyer, H Michalewski, V Ramasesh, A Slone, C Anil, I Schlag, T Gutman-Solo, Y Wu, B Neyshabur, G Gur-Ari, V Misra, </p>
<p>LIME: Learning Inductive Bias for Primitives of Mathematical Reasoning. Y Wu, M Rabe, W Li, </p>
<p>MWP-BERT: Numeracy-augmented pre-training for math word problem solving. Z Liang, J Zhang, L Wang, W Qin, Y Lan, J Shao, X Zhang, May 2022</p>
<p>Enhancing automated scoring of math selfexplanation quality using llm-generated datasets: A semi-supervised approach. R Nakamoto, B Flanagan, T Yamauchi, D Yilling, K Takami, H Ogata, 2023</p>
<p>Proofnet: Autoformalizing and formally proving undergraduate-level mathematics. Z Azerbayev, B Piotrowski, H Schoelkopf, E W Ayers, D Radev, J Avigad, arXiv:2302.124332023arXiv preprint</p>
<p>WizardLM: Empowering Large Language Models to Follow Complex Instructions. C Xu, Q Sun, K Zheng, X Geng, P Zhao, J Feng, C Tao, D Jiang, arXiv:2304.12244Jun. 2023</p>
<p>. Online, </p>
<p>Improving Large Language Model Fine-tuning for Solving Math Problems. Y Liu, A Singh, C D Freeman, J D Co-Reyes, P J Liu, arXiv:2310.10047Oct. 2023</p>
<p>What makes good in-context examples for gpt-3?. J Liu, D Shen, Y Zhang, B Dolan, L Carin, W Chen, Deep Learning Inside Out: 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures. Association for Computational Linguistics2022. 2022</p>
<p>Improving mathematics tutoring with a code scratchpad. S Upadhyay, E Ginsberg, C Callison-Burch, ; , E Kochmar, J Burstein, A Horbach, R Laarmann-Quante, N Madnani, A Tack, V Yaneva, Z , Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023). T Yuan, Zesch, the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)Toronto, CanadaAssociation for Computational LinguisticsJul. 2023</p>
<p>Toolformer: Language models can teach themselves to use tools. T Schick, J Dwivedi-Yu, R Dessì, R Raileanu, M Lomeli, L Zettlemoyer, N Cancedda, T Scialom, 2023</p>
<p>Chameleon: Plug-and-play compositional reasoning with large language models. P Lu, B Peng, H Cheng, M Galley, K.-W Chang, Y N Wu, S.-C Zhu, J Gao, 2023</p>
<p>Art: Automatic multi-step reasoning and tool-use for large language models. B Paranjape, S Lundberg, S Singh, H Hajishirzi, L Zettlemoyer, M T Ribeiro, 2023</p>
<p>Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings. S Hao, T Liu, Z Wang, Z Hu, 2023</p>
<p>Critic: Large language models can self-correct with tool-interactive critiquing. Z Gou, Z Shao, Y Gong, Y Shen, Y Yang, N Duan, W Chen, 2023</p>
<p>Talm: Tool augmented language models. A Parisi, Y Zhao, N Fiedel, 2022</p>
<p>Tool documentation enables zero-shot tool-usage with large language models. C.-Y Hsieh, S.-A Chen, C.-L Li, Y Fujii, A Ratner, C.-Y Lee, R Krishna, T Pfister, 2023</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. W Ling, D Yogatama, C Dyer, P Blunsom, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational Linguistics20171</p>
<p>Mathqa: Towards interpretable math word problem solving with operation-based formalisms. A Amini, S Gabriel, S Lin, R Koncel-Kedziorski, Y Choi, H Hajishirzi, Proceedings of the 2019 Conference of the North American Chapter. Long and Short Papers. the 2019 Conference of the North American Chapterthe Association for Computational Linguistics2019</p>
<p>Let's verify step by step. H Lightman, V Kosaraju, Y Burda, H Edwards, B Baker, T Lee, J Leike, J Schulman, I Sutskever, K Cobbe, 2023</p>
<p>Thought propagation: An analogical approach to complex reasoning with large language models. J Yu, R He, R Ying, arXiv:2310.039652023arXiv preprint</p>
<p>Making large language models better reasoners with alignment. P Wang, L Li, L Chen, F Song, B Lin, Y Cao, T Liu, Z Sui, arXiv:2309.021442023arXiv preprint</p>
<p>Algorithm of thoughts: Enhancing exploration of ideas in large language models. B Sel, A Al-Tawaha, V Khattar, L Wang, R Jia, M Jin, arXiv:2308.103792023arXiv preprint</p>
<p>Mammoth: Building math generalist models through hybrid instruction tuning. X Yue, X Qu, G Zhang, Y Fu, W Huang, H Sun, Y Su, W Chen, arXiv:2309.056532023arXiv preprint</p>
<p>Large Language Models are Zero-Shot Reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, </p>
<p>Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data. K Shum, S Diao, T Zhang, arXiv:2302.12822Feb. 2023</p>
<p>Boosted Prompt Ensembles for Large Language Models. S Pitis, M R Zhang, A Wang, J Ba, arXiv:2304.05970Apr. 2023</p>
<p>Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification. A Zhou, K Wang, Z Lu, W Shi, S Luo, Z Qin, S Lu, A Jia, L Song, M Zhan, arXiv:2308.079212023arXiv preprint</p>
<p>Deductive verification of chain-of-thought reasoning. Z Ling, Y Fang, X Li, Z Huang, M Lee, R Memisevic, H Su, arXiv:2306.038722023arXiv preprint</p>
<p>Making language models better reasoners with step-aware verifier. Y Li, Z Lin, S Zhang, Q Fu, B Chen, J.-G Lou, W Chen, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>Verify-and-edit: A knowledge-enhanced chain-of-thought framework. R Zhao, X Li, S Joty, C Qin, L Bing, arXiv:2305.032682023arXiv preprint</p>
<p>Rethinking with Retrieval: Faithful Large Language Model Inference. H He, H Zhang, D Roth, arXiv:2301.00303Dec. 2022</p>
<p>Screws: A modular framework for reasoning with revisions. K Shridhar, H Jhamtani, H Fang, B Van Durme, J Eisner, P Xia, arXiv:2309.130752023arXiv preprint</p>
<p>Diversity of thought improves reasoning abilities of large language models. R Naik, V Chandrasekaran, M Yuksekgonul, H Palangi, B Nushi, arXiv:2310.070882023arXiv preprint</p>
<p>Selfcheck: Using llms to zero-shot check their own step-by-step reasoning. N Miao, Y W Teh, T Rainforth, arXiv:2308.004362023arXiv preprint</p>
<p>Answering questions by meta-reasoning over multiple chains of thought. O Yoran, T Wolfson, B Bogin, U Katz, D Deutch, J Berant, arXiv:2304.130072023arXiv preprint</p>
<p>Training verifiers to solve math word problems. K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, C Hesse, J Schulman, 2021</p>
<p>Discriminator-guided multi-step reasoning with language models. M Khalifa, L Logeswaran, M Lee, H Lee, L Wang, arXiv:2305.149342023arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, I Shafran, T L Griffiths, Y Cao, K Narasimhan, arXiv:2305.106012023arXiv preprint</p>
<p>Tree of uncertain thoughts reasoning for large language models. S Mo, M Xin, arXiv:2309.076942023arXiv preprint</p>
<p>Large language model guided tree-of-thought. J Long, arXiv:2305.082912023arXiv preprint</p>
<p>Beyond chain-of-thought, effective graph-of-thought reasoning in large language models. Y Yao, Z Li, H Zhao, arXiv:2305.165822023arXiv preprint</p>
<p>Graph of Thoughts: Solving Elaborate Problems with Large Language Models. M Besta, N Blach, A Kubicek, R Gerstenberger, L Gianinazzi, J Gajda, T Lehmann, M Podstawski, H Niewiadomski, P Nyczyk, T Hoefler, arXiv:2308.09687Aug. 2023</p>
<p>. Online, </p>
<p>Boosting logical reasoning in large language models through a new framework: The graph of thought. B Lei, C Liao, C Ding, arXiv:2308.086142023arXiv preprint</p>
<p>Resprompt: Residual connection prompting advances multi-step reasoning in large language models. S Jiang, Z Shakeri, A Chan, M Sanjabi, H Firooz, Y Xia, B Akyildiz, Y Sun, J Li, Q Wang, arXiv:2310.047432023arXiv preprint</p>
<p>Reasoning with language model is planning with world model. S Hao, Y Gu, H Ma, J J Hong, Z Wang, D Z Wang, Z Hu, arXiv:2305.149922023arXiv preprint</p>
<p>Language agent tree search unifies reasoning acting and planning in language models. A Zhou, K Yan, M Shlapentokh-Rothman, H Wang, Y.-X Wang, arXiv:2310.044062023arXiv preprint</p>
<p>Llm+ p: Empowering large language models with optimal planning proficiency. B Liu, Y Jiang, X Zhang, Q Liu, S Zhang, J Biswas, P Stone, arXiv:2304.114772023arXiv preprint</p>
<p>Dynamic planning with a llm. G Dagan, F Keller, A Lascarides, arXiv:2308.063912023arXiv preprint</p>
<p>Selfrefine: Iterative refinement with self-feedback. A Madaan, N Tandon, P Gupta, S Hallinan, L Gao, S Wiegreffe, U Alon, N Dziri, S Prabhumoye, Y Yang, arXiv:2303.176512023arXiv preprint</p>
<p>Adaplanner: Adaptive planning from feedback with language models. H Sun, Y Zhuang, L Kong, B Dai, C Zhang, arXiv:2305.166532023arXiv preprint</p>
<p>Isr-llm: Iterative selfrefined large language model for long-horizon sequential task planning. Z Zhou, J Song, K Yao, Z Shu, L Ma, arXiv:2308.137242023arXiv preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. N Shinn, F Cassano, A Gopinath, K R Narasimhan, S Yao, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Socratic models: Composing zero-shot multimodal reasoning with language. A Zeng, M Attarian, B Ichter, K Choromanski, A Wong, S Welker, F Tombari, A Purohit, M Ryoo, V Sindhwani, arXiv:2204.005982022arXiv preprint</p>
<p>The art of socratic questioning: Recursive thinking with large language models. J Qi, Z Xu, Y Shen, M Liu, Q Dingnan Jin, L Wang, Huang, Conference on Empirical Methods in Natural Language Processing. 2023264935025</p>
<p>Prompting large language models with the socratic method. E Y Chang, 2023 IEEE 13th Annual Computing and Communication Workshop and Conference (CCWC). IEEE2023</p>
<p>Socratic questioning of novice debuggers: A benchmark dataset and preliminary evaluations. E Al-Hossami, R Bunescu, R Teehan, L Powell, K Mahajan, M Dorodchi, Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications. the 18th Workshop on Innovative Use of NLP for Building Educational ApplicationsBEA 2023. 2023</p>
<p>Socratic question generation: A novel dataset, models, and evaluation. B H Ang, S D Gollapalli, S K Ng, Proceedings of the 17th Conference of the European Chapter. the 17th Conference of the European Chapterthe Association for Computational Linguistics2023</p>
<p>Measuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2021</p>
<p>J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Instruction Tuning for Large Language Models: A Survey. S Zhang, L Dong, X Li, S Zhang, X Sun, S Wang, J Li, R Hu, T Zhang, F Wu, G Wang, arXiv:2308.10792Oct. 2023</p>
<p>. Online, </p>
<p>Scaling laws for neural language models. J Kaplan, S Mccandlish, T Henighan, T B Brown, B Chess, R Child, S Gray, A Radford, J Wu, D Amodei, arXiv:2001.083612020arXiv preprint</p>
<p>Tora: A tool-integrated reasoning agent for mathematical problem solving. Z Gou, Z Shao, Y Gong, Y Yang, M Huang, N Duan, W Chen, arXiv:2309.174522023arXiv preprint</p>
<p>The socratic method. L Nelson, Thinking: The Journal of Philosophy for Children. 221980</p>
<p>Advancing mathematics by guiding human intuition with ai. A Davies, P Veličković, L Buesing, S Blackwell, D Zheng, N Tomašev, R Tanburn, P Battaglia, C Blundell, A Juhász, Nature. 60078872021</p>
<p>A dialogue about socratic teaching. P C Davis, E E Steinglass, NYU Rev. L. &amp; Soc. Change. 232491997</p>
<p>Socratic teaching and socratic method. T C Brickhouse, N D Smith, 2009</p>
<p>Can language models employ the socratic method? experiments with code debugging. E Al-Hossami, R Bunescu, J Smith, R Teehan, 2023</p>
<p>Verb physics: Relative physical knowledge of actions and objects. M Forbes, Y Choi, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational Linguistics20171</p>
<p>Numerically grounded language models for semantic error correction. G Spithourakis, I Augenstein, S Riedel, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language Processing2016</p>
<p>How large are lions? inducing distributions over quantitative attributes. Y Elazar, A Mahabal, D Ramachandran, T Bedrax-Weiss, D Roth, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics2019</p>
<p>Are nlp models really able to solve simple math word problems?. A Patel, S Bhattamishra, N Goyal, Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterHuman Language Technologies2021</p>
<p>Draw: A challenging and diverse algebra word problem set. S Upadhyay, M.-W Chang, Citeseer, Tech. Rep. 2015</p>
<p>Unit dependency graph and its application to arithmetic word problem solving. S Roy, D Roth, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201731</p>
<p>Annotating derivations: A new evaluation strategy and dataset for algebra word problems. S Upadhyay, M.-W Chang, the Association for Computational Linguistics. 12017in Proceedings of the 15th Conference of the European Chapter</p>
<p>Semantically-aligned universal tree-structured solver for math word problems. J Qin, L Lin, X Liang, R Zhang, L Lin, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language Processing2020</p>
<p>Armath: a dataset for solving arabic math word problems. R Alghamdi, Z Liang, X Zhang, Proceedings of the Thirteenth Language Resources and Evaluation Conference. the Thirteenth Language Resources and Evaluation Conference2022</p>
<p>Ape210k: A large-scale and template-rich dataset of math word problems. W Zhao, M Shang, Y Liu, L Wang, J Liu, arXiv:2009.115062020arXiv preprint</p>
<p>Finqa: A dataset of numerical reasoning over financial data. Z Chen, W Chen, C Smiley, S Shah, I Borova, D Langdon, R Moussa, M Beane, T.-H Huang, B R Routledge, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>How much coffee was consumed during emnlp 2019? fermi problems: A new reasoning challenge for ai. A Kalyan, A Kumar, A Chandrasekaran, A Sabharwal, P Clark, Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics2021. 2021</p>
<p>Four decades of mizar: Foreword. A Grabowski, A Korniłowicz, A Naumowicz, Journal of Automated Reasoning. 552015</p>
<p>Holstep: A machine learning dataset for higher-order logic theorem proving. C Kaliszyk, F Chollet, C Szegedy, International Conference on Learning Representations. 2017</p>
<p>Gamepad: A learning environment for theorem proving. D Huang, P Dhariwal, D Song, I Sutskever, International Conference on Learning Representations. 2019</p>
<p>Holist: An environment for machine learning of higher order logic theorem proving. K Bansal, S Loos, M Rabe, C Szegedy, S Wilcox, International Conference on Machine Learning. PMLR2019</p>
<p>Isarstep: a benchmark for high-level mathematical reasoning. W Li, L Yu, Y Wu, L C Paulson, International Conference on Learning Representations. 2021</p>
<p>Numglue: A suite of fundamental yet challenging mathematical reasoning tasks. S Mishra, A Mitra, N Varshney, B Sachdeva, P Clark, C Baral, A Kalyan, 60th Annual Meeting of the Association for Computational Linguistics, ACL 2022. Association for Computational Linguistics2022</p>
<p>How well do computers solve math word problems? large-scale dataset construction and evaluation. D Huang, S Shi, C.-Y Lin, J Yin, W.-Y Ma, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsLong Papers20161</p>
<p>Mawps: A math word problem repository. R Koncel-Kedziorski, S Roy, A Amini, N Kushman, H Hajishirzi, Proceedings of the 2016 conference of the north american chapter of the association for computational linguistics: human language technologies. the 2016 conference of the north american chapter of the association for computational linguistics: human language technologies2016</p>
<p>A diverse corpus for evaluating and developing english math word problem solvers. S.-Y Miao, C.-C Liang, K.-Y Su, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Language models are multilingual chain-of-thought reasoners. F Shi, M Suzgun, M Freitag, X Wang, S Srivats, S Vosoughi, H W Chung, Y Tay, S Ruder, D Zhou, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Agieval: A human-centric benchmark for evaluating foundation models. W Zhong, R Cui, Y Guo, Y Liang, S Lu, Y Wang, A Saied, W Chen, N Duan, 2023</p>
<p>minif2f: a cross-system benchmark for formal olympiad-level mathematics. K Zheng, J M Han, S Polu, International Conference on Learning Representations. 2023</p>
<p>Mapping to declarative knowledge for word problem solving. S Roy, D Roth, Transactions of the Association for Computational Linguistics. 62018</p>
<p>Program synthesis with large language models. J Austin, A Odena, M Nye, M Bosma, H Michalewski, D Dohan, E Jiang, C Cai, M Terry, Q Le, arXiv:2108.077322021arXiv preprint</p>
<p>Camel: Communicative agents for" mind" exploration of large language model society. G Li, H A A K Hammoud, H Itani, D Khizbullin, B Ghanem, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Ept-x: An expressionpointer transformer model that generates explanations for numbers. B Kim, K S Ki, S Rhim, G Gweon, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Naturalprover: Grounded mathematical proof generation with language models. S Welleck, J Liu, X Lu, H Hajishirzi, Y Choi, Advances in Neural Information Processing Systems. 202235</p>
<p>A machine-checked proof of the odd order theorem. G Gonthier, A Asperti, J Avigad, Y Bertot, C Cohen, F Garillot, S Le Roux, A Mahboubi, R O'connor, S Ould Biha, International conference on interactive theorem proving. Springer2013</p>
<p>Metamath: a computer language for mathematical proofs. N Megill, D A Wheeler, Lulu. com. 2019</p>
<p>An overview of the mizar project. P Rudnicki, Proceedings of the 1992 Workshop on Types for Proofs and Programs. the 1992 Workshop on Types for Proofs and ProgramsCiteseer1992</p>
<p>Interactive theorem proving and program development: Coq'Art: the calculus of inductive constructions. Y Bertot, P Castéran, 2013Springer Science &amp; Business Media</p>
<p>The isabelle framework. M Wenzel, L C Paulson, T Nipkow, Theorem Proving in Higher Order Logics: 21st International Conference. TPHOLs; Montreal, CanadaSpringer2008. August 18-21, 2008. 2008Proceedings 21</p>
<p>A brief overview of hol4. K Slind, M Norrish, International Conference on Theorem Proving in Higher Order Logics. Springer2008</p>
<p>A formal proof of the kepler conjecture. T Hales, M Adams, G Bauer, T D Dang, J Harrison, H Le Truong, C Kaliszyk, V Magron, S Mclaughlin, T T Nguyen, Forum of mathematics, Pi. 5e22017Cambridge University Press</p>
<p>A survey of hallucination in large foundation models. V Rawte, A Sheth, A Das, arXiv:2309.059222023arXiv preprint</p>
<p>Empirical explorations of the geometry theorem machine. H Gelernter, J R Hansen, D W Loveland, western joint IRE-AIEE-ACM computer conference. May 3-5, 1960. 1960in Papers</p>
<p>Vqa: Visual question answering. S Antol, A Agrawal, J Lu, M Mitchell, D Batra, C L Zitnick, D Parikh, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision2015</p>
<p>A comprehensive survey of deep learning for image captioning. M Z Hossain, F Sohel, M F Shiratuddin, H Laga, ACM Computing Surveys (CsUR). 5162019</p>
<p>A survey of uncertainty in deep neural networks. J Gawlikowski, C R N Tassi, M Ali, J Lee, M Humt, J Feng, A Kruspe, R Triebel, P Jung, R Roscher, Artificial Intelligence Review. 5612023Suppl</p>
<p>Shifting attention to relevance: Towards the uncertainty estimation of large language models. J Duan, H Cheng, S Wang, C Wang, A Zavalny, R Xu, B Kailkhura, K Xu, arXiv:2307.013792023arXiv preprint</p>
<p>Controlled text generation with natural language instructions. W Zhou, Y E Jiang, E Wilcox, R Cotterell, M Sachan, International Conference on Machine Learning, ICML 2023. A Research, E Krause, K Brunskill, B Cho, S Engelhardt, J Sabato, Scarlett, Honolulu, Hawaii, USA, serPMLRJuly 2023. 2023202613Proceedings of Machine Learning</p>
<p>Gpt-j-6b: A 6 billion parameter autoregressive language model. B Wang, A Komatsuzaki, 2021</p>
<p>Scaling instructionfinetuned language models. H W Chung, L Hou, S Longpre, B Zoph, Y Tay, W Fedus, Y Li, X Wang, M Dehghani, S Brahma, arXiv:2210.114162022arXiv preprint</p>
<p>Lamda: Language models for dialog applications. R Thoppilan, D De Freitas, J Hall, N Shazeer, A Kulshreshtha, H.-T Cheng, A Jin, T Bos, L Baker, Y Du, arXiv:2201.082392022arXiv preprint</p>
<p>Automatic generation of socratic subquestions for teaching math word problems. K Shridhar, J Macina, M El-Assady, T Sinha, M Kapur, M Sachan, arXiv:2211.128352022arXiv preprint</p>
<p>Enhancing Chat Language Models by Scaling Highquality Instructional Conversations. N Ding, Y Chen, B Xu, Y Qin, Z Zheng, S Hu, Z Liu, M Sun, B Zhou, arXiv:2305.14233May 2023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2349 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2349</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2349</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-63.html">extraction-schema-63</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <p><strong>Paper ID:</strong> paper-15d739e2c184a6844bdbd9a2550d007de6ddb085</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/15d739e2c184a6844bdbd9a2550d007de6ddb085" target="_blank">Objective-Reinforced Generative Adversarial Networks (ORGAN) for Sequence Generation Models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work builds upon previous results that incorporated GANs and RL in order to generate sequence data and test this model in several settings for the generation of molecules encoded as text sequences and in the context of music generation, showing for each case that it can effectively bias the generation process towards desired metrics.</p>
                <p><strong>Paper Abstract:</strong> In unsupervised data generation tasks, besides the generation of a sample based on previous observations, one would often like to give hints to the model in order to bias the generation towards desirable metrics. We propose a method that combines Generative Adversarial Networks (GANs) and reinforcement learning (RL) in order to accomplish exactly that. While RL biases the data generation process towards arbitrary metrics, the GAN component of the reward function ensures that the model still remembers information learned from data. We build upon previous results that incorporated GANs and RL in order to generate sequence data and test this model in several settings for the generation of molecules encoded as text sequences (SMILES) and in the context of music generation, showing for each case that we can effectively bias the generation process towards desired metrics.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2349.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2349.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ORGAN (molecules)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Objective-Reinforced Generative Adversarial Network for Molecule Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequence-generation framework that combines a SeqGAN-style adversarial discriminator with reinforcement learning rewards that include domain-specific objectives (e.g., solubility, synthesizability, druglikeness) to bias generated SMILES strings toward desired molecular properties while preserving data-likeness and diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>drug discovery / de novo molecular generation</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Generate valid, diverse small-molecule structures (SMILES sequences) biased toward domain-desired properties such as solubility (LogP), synthetic accessibility, and druglikeness while maintaining similarity to training distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>limited: experiments used a random subset of 5,000 molecules drawn from a 134k dataset (Ramakrishnan et al. 2014). Data are unlabeled sequences (SMILES) with computed property labels available via cheminformatics tools (RDKit); training data quality is standard curated small-molecule set but relatively small compared to chemical space.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>discrete text sequences (SMILES) representing molecular graphs; fixed-length padded token sequences (max length 51); alphabet size ~43 tokens. Representations have a formal grammar (SMILES) so many generated strings can be invalid.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>high combinatorial complexity: chemical space extremely large (GDB-17 referenced, ~1e11+ molecules), discrete structured grammar (validity constraints), non-linear mapping from sequence to chemical properties; search-space large, sequence lengths up to 51, moderate dimensionality per token (vocabulary size 43).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>well-established domain with many prior methods (RNNs, VAEs, GANs) and established property estimators (RDKit); mature cheminformatics tools and heuristics exist (e.g., synthetic accessibility scoring), but inverse design remains an active research area.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>medium — chemical validity and interpretable property evaluation are important for downstream utility (synthetic accessibility and physico-chemical properties must be verifiable), but black-box generation is acceptable provided generated molecules can be validated with established cheminformatics and experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>ORGAN (RNN generator + adversarial discriminator + RL objective reward); OR(W)GAN variant (Wasserstein critic)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Generator: LSTM-based RNN policy G_theta producing discrete SMILES tokens; Discriminator: CNN-based text classifier D_phi (or WGAN critic) providing a discriminator reward. Training: pretrain G with MLE for 250 epochs, alternate adversarial training of D and policy-gradient updates of G using REINFORCE with Monte Carlo rollouts to estimate Q-values. Reward is a weighted linear combination R = lambda * D_phi + (1 - lambda) * O_i where O_i are domain-specific heuristic objectives (solubility, synthesizability, druglikeness). Penalization of duplicate sequences by dividing reward by repetition count. Wasserstein-1 loss (OR(W)GAN) used to improve discriminator stability.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>hybrid: generative adversarial networks + reinforcement learning (policy-gradient), unsupervised/semi-supervised sequence generation with objective-guided RL</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>applicable and appropriate: method directly addresses discrete sequence generation with domain objectives and accounts for SMILES grammar issues via discriminator and validity penalties; requires pretraining and careful lambda tuning; constraint: Monte Carlo rollouts increase computational cost and discrete grammar leads to invalid strings which must be penalized.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>When optimizing for druglikeliness: ORGAN achieved ~88.2% validity (vs MLE 75.9%, SeqGAN 80.3%), diversity 0.55 (unit: molecular similarity-based diversity metric), druglikeliness ~0.52 (scaled 0-1). OR(W)GAN and naive RL achieved higher validity in some objective runs (e.g., naive RL validity ~97.1% in one setting) but often at cost of diversity or producing trivial/simple molecules. For synthesizeability/solubility objectives validity values reported ~94-97% for optimized methods. Training used 5k-molecule dataset, sequence length <=51.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>ORGAN successfully biased generation toward desired molecular properties while preserving sample diversity better than naive RL; SeqGAN and MLE captured training distribution but did not improve targeted properties; naive RL could maximize objectives but often produced simplistic or degenerate molecules (e.g., single-atom or repetitive patterns) reducing diversity and interest. WGAN variant improved stability and diversity in some experiments but trained slower.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>high: enables targeted exploration of chemical subspaces by steering generative models toward properties of interest while maintaining diversity, which can accelerate ideation of candidate molecules in drug/materials discovery pipelines; method is general and could be applied to other discrete inverse-design tasks. Practical impact depends on downstream experimental validation and ability to push beyond dataset 'capacity ceilings'.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared against MLE (pretrained RNN), SeqGAN, and Naive RL baselines. ORGAN outperformed MLE and SeqGAN on targeted property metrics and maintained greater diversity than Naive RL; Naive RL sometimes achieved higher raw objective scores but produced less diverse or trivial outputs. OR(W)GAN (Wasserstein variant) tended to give better diversity but slower metric improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>combination of adversarial discriminator (keeps samples close to data distribution) with objective-driven RL reward (forces property optimization); penalization for duplicates to reduce mode-collapse; pretraining with MLE to provide stable initialization; WGAN loss for discriminator stability.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Blending adversarial feedback with domain-specific RL rewards lets a sequence generator steer outputs toward scientific objectives (e.g., molecular properties) while using the discriminator to preserve realism and diversity, though tuning (lambda) and dataset limits constrain absolute performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Objective-Reinforced Generative Adversarial Networks (ORGAN) for Sequence Generation Models', 'publication_date_yy_mm': '2017-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2349.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2349.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ORGAN (music)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Objective-Reinforced Generative Adversarial Network for Melody Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension of SeqGAN that incorporates reinforcement-learning rewards for musical objectives (e.g., tonality measured by number of perfect fifths, ratio of stepwise motion) combined with adversarial regularization to produce melodies that are both data-like and optimized for musical heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>music generation / computational creativity</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Generate discrete token sequences encoding musical melodies that maximize musical heuristics (tonality, ratio of steps) while retaining diversity and similarity to a dataset of folk melodies.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>limited: experiments used a 1,000-sample subset from the EsAC folk dataset (processed), each melody length 36 tokens. Data are unlabeled sequences with computable musical metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>discrete fixed-length token sequences representing 16th-note events (36-token melodies), small vocabulary (~38 tokens: silent/no-event + 36 pitch tokens), sequential/time-series discrete data.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>moderate: discrete sequential prediction over short fixed-length sequences, musical structure and constraints (interval relationships) create non-linear reward landscapes; search space large but much smaller than molecular case due to short sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>established research area for ML sequence generation (RNNs, GANs, RL applied previously), but objectives and aesthetic metrics can be heuristic and subjective.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>low-to-medium — generation quality judged by perceptual/aesthetic metrics where black-box models are acceptable for creative outputs, though interpretability may help in understanding musical structure.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>ORGAN (LSTM generator + CNN discriminator + RL objectives); OR(W)GAN variant</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>LSTM-based RNN generator outputs 36-token music sequences; CNN discriminator trained to distinguish real vs generated melodies; reward R = lambda * D_phi + (1-lambda) * O_i where O_i are musical heuristics (tonality measured via perfect-fifth counts, ratio of steps). REINFORCE used with Monte Carlo rollouts for Q-value estimation; pretraining via MLE (250 epochs); WGAN variant tested to improve training stability.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>hybrid: generative adversarial networks + reinforcement learning (policy-gradient) for sequence generation</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>appropriate: method aligns with discrete sequence generation and allows encoding of arbitrary non-differentiable musical objectives; requires objective design and hyperparameter (lambda) tuning; Monte Carlo rollouts impose computational overhead but are feasible for short sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>On music experiments (averaged over 1,000 generated songs): For the Tonality objective, ORGAN achieved diversity 0.268 and tonality 0.372 (scaled 0-1) versus MLE tonality 0.007 and SeqGAN 0.005; for the Ratio of Steps objective, ORGAN diversity 0.433 and ratio 0.632 compared to Naive RL ratio 0.829 but Naive RL diversity 0.321. OR(W)GAN variants showed some metric trade-offs (e.g., lower tonality but maintained diversity).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>ORGAN improved musical heuristic metrics substantially over MLE and SeqGAN while maintaining higher diversity than naive RL; naive RL often produced higher objective scores but with less diversity and simpler melodies, making them less musically interesting. WGAN variants trained slower and sometimes yielded lower raw objective values but helped diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>moderate: enables controllable generation of melodies with user-specified musical properties and provides a framework for optimizing non-differentiable musical objectives; could support interactive composition tools or data augmentation for music research.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared against MLE, SeqGAN and Naive RL baselines: ORGAN outperforms MLE and SeqGAN on targeted musical objectives and maintains better diversity than Naive RL, which tends to produce degenerate/simple solutions despite high objective scores.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>explicit incorporation of musical heuristics into the reward, adversarial term to maintain data-like structure, pretraining for stability, and tuning of lambda to balance objective optimization vs. realism and diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Combining adversarial training with RL rewards for domain heuristics enables effective steering of discrete-sequence generators toward subjective aesthetic goals while preserving variety; balancing (via lambda) is crucial to avoid degenerate high-scoring but uninteresting outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Objective-Reinforced Generative Adversarial Networks (ORGAN) for Sequence Generation Models', 'publication_date_yy_mm': '2017-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2349.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2349.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SeqGAN (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GAN framework adapted for discrete sequence generation that treats the generator as a stochastic policy optimized with policy-gradient methods (REINFORCE) using discriminator outputs as rewards and Monte Carlo rollouts to estimate action-values.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Seqgan: Sequence generative adversarial nets with policy gradient</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>general sequence generation applied to tasks including molecule SMILES generation and music generation (as baseline comparisons in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Generate discrete sequences (text, SMILES, music) using adversarial training despite non-differentiability of sampling, by framing generator as RL agent and using discriminator-provided reward signals.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>varies by application; in this paper used same limited datasets as ORGAN (5k molecules, 1k melodies) as baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>discrete sequences (text tokens), same as ORGAN applications (SMILES and music tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>same discrete sequence complexity; SeqGAN addresses the nondifferentiability via policy gradients and Monte Carlo rollouts, but can suffer from mode collapse and instability.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>established sequence-GAN method in ML research; prior work used it for text and music.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>low-to-medium — provides black-box generation; lacks inherent interpretability but amenable to analysis via discriminator and reward shaping.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>SeqGAN (policy-gradient GAN for sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Generator modeled as stochastic policy G_theta (RNN/LSTM) trained via REINFORCE to maximize discriminator-provided reward; Monte Carlo rollouts used to estimate Q-values for partial sequences; discriminator is a classifier providing scalar reward. No explicit domain-specific objectives in base SeqGAN.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>hybrid: generative adversarial networks + reinforcement learning (policy-gradient)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>applicable for discrete sequence generation but limited when explicit domain objectives are required; in experiments SeqGAN captured data distribution but did not bias for desired properties as ORGAN did.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>In molecule experiments, SeqGAN validity ~80.3% (vs MLE 75.9%, ORGAN 88.2% in druglikeliness run); in music experiments SeqGAN had low tonality/ratio-of-steps compared to ORGAN.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>SeqGAN can learn data-like sequences but without explicit objective guidance it fails to optimize domain-specific properties; prone to mode-collapse and needs stabilizing techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>useful baseline method for discrete sequence GANs; foundation for extensions (like ORGAN) that add objective rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared directly in experiments: ORGAN (adds objective RL reward) outperforms SeqGAN on targeted property optimization while maintaining diversity; naive RL can exceed SeqGAN on raw objectives but at cost of diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>treating generator as RL policy and using rollouts to estimate future rewards enables application of GANs to discrete sequences, but performance depends on discriminator stability and reward design.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>SeqGAN enables adversarial training for discrete sequences by reframing generation as an RL problem, but without explicit objective rewards it cannot bias outputs toward domain-specific scientific goals effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Objective-Reinforced Generative Adversarial Networks (ORGAN) for Sequence Generation Models', 'publication_date_yy_mm': '2017-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2349.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2349.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Naive RL (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Naive Reinforcement Learning (policy-gradient with domain objective only)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that trains the sequence generator purely to maximize domain-specific heuristic rewards using policy-gradient methods (REINFORCE) without adversarial discriminator regularization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>applied to molecule generation and music generation in this paper as a baseline</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Train sequence generator to directly maximize non-differentiable domain-specific objectives (e.g., solubility, druglikeness, musical heuristics) without constraints from data-likeness discriminator.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>uses same limited datasets for pretraining; but after pretraining, training optimizes only the objective signal, so dependence on labeled data is low beyond availability of reward computation.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>discrete sequences (SMILES, music tokens) as in other experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>optimization landscape can be sparse and deceptive; pure RL optimization can converge to local optima that produce trivial high-reward outputs (e.g., tiny molecules or repetitive tokens) due to simplified reward structures.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>standard RL technique applied as baseline; known issues when used alone for objective-driven sequence generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>low — solution acceptable as black box but produced outputs may be uninterpretable or trivial, requiring domain validation.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Naive Reinforcement Learning (REINFORCE with Monte Carlo rollouts, objective-only reward)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Generator initialized via MLE pretraining, then trained with policy-gradient to maximize a domain-specific reward O_i only (lambda=0 in ORGAN formulation). Monte Carlo rollouts used for Q estimation; no discriminator term to constrain to data distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>applicable for direct objective optimization but often inappropriate alone due to generation of degenerate/trivial high-scoring sequences and loss of diversity; may require adversarial or other regularization.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>In molecule tasks naive RL achieved very high validity in some objectives (e.g., ~97.1% in one druglikeliness run) and high objective scores (e.g., solubility improvements), but tended to reduce diversity; in music Naive RL achieved high Ratio of Steps (0.829) but low diversity (0.321).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Effective at maximizing raw objective metrics but often produces simplistic or repetitive outputs that are less useful or interesting (e.g., single-atom molecules or repetitive token sequences); trade-off between objective maximization and diversity/realism.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>limited as a standalone approach for scientific generation tasks where realism/diversity and validity matter; useful as a component but needs regularization (e.g., discriminator) for practical use.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared against ORGAN and SeqGAN: naive RL attains higher raw objective scores in some cases but yields lower diversity and data-likeness; ORGAN's combined adversarial+objective approach yields more balanced outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>direct reward optimization drives objective gains, but the absence of a discriminator or other constraints leads to mode collapse or trivial solutions; success depends on richness of reward and penalties for degenerate outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Maximizing domain objectives with pure RL can reach high objective values but often at the cost of diversity and realism; adversarial constraints (as in ORGAN) are needed to produce practically useful scientific outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Objective-Reinforced Generative Adversarial Networks (ORGAN) for Sequence Generation Models', 'publication_date_yy_mm': '2017-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Seqgan: Sequence generative adversarial nets with policy gradient <em>(Rating: 2)</em></li>
                <li>Wasserstein GAN <em>(Rating: 2)</em></li>
                <li>Automatic chemical design using a data-driven continuous representation of molecules <em>(Rating: 2)</em></li>
                <li>Optimizing distributions over molecular space. An Objective-Reinforced Generative Adversarial Network for Inverse-design Chemistry. <em>(Rating: 2)</em></li>
                <li>Tuning recurrent neural networks with reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2349",
    "paper_id": "paper-15d739e2c184a6844bdbd9a2550d007de6ddb085",
    "extraction_schema_id": "extraction-schema-63",
    "extracted_data": [
        {
            "name_short": "ORGAN (molecules)",
            "name_full": "Objective-Reinforced Generative Adversarial Network for Molecule Generation",
            "brief_description": "A sequence-generation framework that combines a SeqGAN-style adversarial discriminator with reinforcement learning rewards that include domain-specific objectives (e.g., solubility, synthesizability, druglikeness) to bias generated SMILES strings toward desired molecular properties while preserving data-likeness and diversity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "drug discovery / de novo molecular generation",
            "problem_description": "Generate valid, diverse small-molecule structures (SMILES sequences) biased toward domain-desired properties such as solubility (LogP), synthetic accessibility, and druglikeness while maintaining similarity to training distribution.",
            "data_availability": "limited: experiments used a random subset of 5,000 molecules drawn from a 134k dataset (Ramakrishnan et al. 2014). Data are unlabeled sequences (SMILES) with computed property labels available via cheminformatics tools (RDKit); training data quality is standard curated small-molecule set but relatively small compared to chemical space.",
            "data_structure": "discrete text sequences (SMILES) representing molecular graphs; fixed-length padded token sequences (max length 51); alphabet size ~43 tokens. Representations have a formal grammar (SMILES) so many generated strings can be invalid.",
            "problem_complexity": "high combinatorial complexity: chemical space extremely large (GDB-17 referenced, ~1e11+ molecules), discrete structured grammar (validity constraints), non-linear mapping from sequence to chemical properties; search-space large, sequence lengths up to 51, moderate dimensionality per token (vocabulary size 43).",
            "domain_maturity": "well-established domain with many prior methods (RNNs, VAEs, GANs) and established property estimators (RDKit); mature cheminformatics tools and heuristics exist (e.g., synthetic accessibility scoring), but inverse design remains an active research area.",
            "mechanistic_understanding_requirements": "medium — chemical validity and interpretable property evaluation are important for downstream utility (synthetic accessibility and physico-chemical properties must be verifiable), but black-box generation is acceptable provided generated molecules can be validated with established cheminformatics and experiments.",
            "ai_methodology_name": "ORGAN (RNN generator + adversarial discriminator + RL objective reward); OR(W)GAN variant (Wasserstein critic)",
            "ai_methodology_description": "Generator: LSTM-based RNN policy G_theta producing discrete SMILES tokens; Discriminator: CNN-based text classifier D_phi (or WGAN critic) providing a discriminator reward. Training: pretrain G with MLE for 250 epochs, alternate adversarial training of D and policy-gradient updates of G using REINFORCE with Monte Carlo rollouts to estimate Q-values. Reward is a weighted linear combination R = lambda * D_phi + (1 - lambda) * O_i where O_i are domain-specific heuristic objectives (solubility, synthesizability, druglikeness). Penalization of duplicate sequences by dividing reward by repetition count. Wasserstein-1 loss (OR(W)GAN) used to improve discriminator stability.",
            "ai_methodology_category": "hybrid: generative adversarial networks + reinforcement learning (policy-gradient), unsupervised/semi-supervised sequence generation with objective-guided RL",
            "applicability": "applicable and appropriate: method directly addresses discrete sequence generation with domain objectives and accounts for SMILES grammar issues via discriminator and validity penalties; requires pretraining and careful lambda tuning; constraint: Monte Carlo rollouts increase computational cost and discrete grammar leads to invalid strings which must be penalized.",
            "effectiveness_quantitative": "When optimizing for druglikeliness: ORGAN achieved ~88.2% validity (vs MLE 75.9%, SeqGAN 80.3%), diversity 0.55 (unit: molecular similarity-based diversity metric), druglikeliness ~0.52 (scaled 0-1). OR(W)GAN and naive RL achieved higher validity in some objective runs (e.g., naive RL validity ~97.1% in one setting) but often at cost of diversity or producing trivial/simple molecules. For synthesizeability/solubility objectives validity values reported ~94-97% for optimized methods. Training used 5k-molecule dataset, sequence length &lt;=51.",
            "effectiveness_qualitative": "ORGAN successfully biased generation toward desired molecular properties while preserving sample diversity better than naive RL; SeqGAN and MLE captured training distribution but did not improve targeted properties; naive RL could maximize objectives but often produced simplistic or degenerate molecules (e.g., single-atom or repetitive patterns) reducing diversity and interest. WGAN variant improved stability and diversity in some experiments but trained slower.",
            "impact_potential": "high: enables targeted exploration of chemical subspaces by steering generative models toward properties of interest while maintaining diversity, which can accelerate ideation of candidate molecules in drug/materials discovery pipelines; method is general and could be applied to other discrete inverse-design tasks. Practical impact depends on downstream experimental validation and ability to push beyond dataset 'capacity ceilings'.",
            "comparison_to_alternatives": "Compared against MLE (pretrained RNN), SeqGAN, and Naive RL baselines. ORGAN outperformed MLE and SeqGAN on targeted property metrics and maintained greater diversity than Naive RL; Naive RL sometimes achieved higher raw objective scores but produced less diverse or trivial outputs. OR(W)GAN (Wasserstein variant) tended to give better diversity but slower metric improvements.",
            "success_factors": "combination of adversarial discriminator (keeps samples close to data distribution) with objective-driven RL reward (forces property optimization); penalization for duplicates to reduce mode-collapse; pretraining with MLE to provide stable initialization; WGAN loss for discriminator stability.",
            "key_insight": "Blending adversarial feedback with domain-specific RL rewards lets a sequence generator steer outputs toward scientific objectives (e.g., molecular properties) while using the discriminator to preserve realism and diversity, though tuning (lambda) and dataset limits constrain absolute performance.",
            "uuid": "e2349.0",
            "source_info": {
                "paper_title": "Objective-Reinforced Generative Adversarial Networks (ORGAN) for Sequence Generation Models",
                "publication_date_yy_mm": "2017-05"
            }
        },
        {
            "name_short": "ORGAN (music)",
            "name_full": "Objective-Reinforced Generative Adversarial Network for Melody Generation",
            "brief_description": "An extension of SeqGAN that incorporates reinforcement-learning rewards for musical objectives (e.g., tonality measured by number of perfect fifths, ratio of stepwise motion) combined with adversarial regularization to produce melodies that are both data-like and optimized for musical heuristics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "music generation / computational creativity",
            "problem_description": "Generate discrete token sequences encoding musical melodies that maximize musical heuristics (tonality, ratio of steps) while retaining diversity and similarity to a dataset of folk melodies.",
            "data_availability": "limited: experiments used a 1,000-sample subset from the EsAC folk dataset (processed), each melody length 36 tokens. Data are unlabeled sequences with computable musical metrics.",
            "data_structure": "discrete fixed-length token sequences representing 16th-note events (36-token melodies), small vocabulary (~38 tokens: silent/no-event + 36 pitch tokens), sequential/time-series discrete data.",
            "problem_complexity": "moderate: discrete sequential prediction over short fixed-length sequences, musical structure and constraints (interval relationships) create non-linear reward landscapes; search space large but much smaller than molecular case due to short sequences.",
            "domain_maturity": "established research area for ML sequence generation (RNNs, GANs, RL applied previously), but objectives and aesthetic metrics can be heuristic and subjective.",
            "mechanistic_understanding_requirements": "low-to-medium — generation quality judged by perceptual/aesthetic metrics where black-box models are acceptable for creative outputs, though interpretability may help in understanding musical structure.",
            "ai_methodology_name": "ORGAN (LSTM generator + CNN discriminator + RL objectives); OR(W)GAN variant",
            "ai_methodology_description": "LSTM-based RNN generator outputs 36-token music sequences; CNN discriminator trained to distinguish real vs generated melodies; reward R = lambda * D_phi + (1-lambda) * O_i where O_i are musical heuristics (tonality measured via perfect-fifth counts, ratio of steps). REINFORCE used with Monte Carlo rollouts for Q-value estimation; pretraining via MLE (250 epochs); WGAN variant tested to improve training stability.",
            "ai_methodology_category": "hybrid: generative adversarial networks + reinforcement learning (policy-gradient) for sequence generation",
            "applicability": "appropriate: method aligns with discrete sequence generation and allows encoding of arbitrary non-differentiable musical objectives; requires objective design and hyperparameter (lambda) tuning; Monte Carlo rollouts impose computational overhead but are feasible for short sequences.",
            "effectiveness_quantitative": "On music experiments (averaged over 1,000 generated songs): For the Tonality objective, ORGAN achieved diversity 0.268 and tonality 0.372 (scaled 0-1) versus MLE tonality 0.007 and SeqGAN 0.005; for the Ratio of Steps objective, ORGAN diversity 0.433 and ratio 0.632 compared to Naive RL ratio 0.829 but Naive RL diversity 0.321. OR(W)GAN variants showed some metric trade-offs (e.g., lower tonality but maintained diversity).",
            "effectiveness_qualitative": "ORGAN improved musical heuristic metrics substantially over MLE and SeqGAN while maintaining higher diversity than naive RL; naive RL often produced higher objective scores but with less diversity and simpler melodies, making them less musically interesting. WGAN variants trained slower and sometimes yielded lower raw objective values but helped diversity.",
            "impact_potential": "moderate: enables controllable generation of melodies with user-specified musical properties and provides a framework for optimizing non-differentiable musical objectives; could support interactive composition tools or data augmentation for music research.",
            "comparison_to_alternatives": "Compared against MLE, SeqGAN and Naive RL baselines: ORGAN outperforms MLE and SeqGAN on targeted musical objectives and maintains better diversity than Naive RL, which tends to produce degenerate/simple solutions despite high objective scores.",
            "success_factors": "explicit incorporation of musical heuristics into the reward, adversarial term to maintain data-like structure, pretraining for stability, and tuning of lambda to balance objective optimization vs. realism and diversity.",
            "key_insight": "Combining adversarial training with RL rewards for domain heuristics enables effective steering of discrete-sequence generators toward subjective aesthetic goals while preserving variety; balancing (via lambda) is crucial to avoid degenerate high-scoring but uninteresting outputs.",
            "uuid": "e2349.1",
            "source_info": {
                "paper_title": "Objective-Reinforced Generative Adversarial Networks (ORGAN) for Sequence Generation Models",
                "publication_date_yy_mm": "2017-05"
            }
        },
        {
            "name_short": "SeqGAN (baseline)",
            "name_full": "SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient",
            "brief_description": "A GAN framework adapted for discrete sequence generation that treats the generator as a stochastic policy optimized with policy-gradient methods (REINFORCE) using discriminator outputs as rewards and Monte Carlo rollouts to estimate action-values.",
            "citation_title": "Seqgan: Sequence generative adversarial nets with policy gradient",
            "mention_or_use": "use",
            "scientific_problem_domain": "general sequence generation applied to tasks including molecule SMILES generation and music generation (as baseline comparisons in this paper)",
            "problem_description": "Generate discrete sequences (text, SMILES, music) using adversarial training despite non-differentiability of sampling, by framing generator as RL agent and using discriminator-provided reward signals.",
            "data_availability": "varies by application; in this paper used same limited datasets as ORGAN (5k molecules, 1k melodies) as baselines.",
            "data_structure": "discrete sequences (text tokens), same as ORGAN applications (SMILES and music tokens).",
            "problem_complexity": "same discrete sequence complexity; SeqGAN addresses the nondifferentiability via policy gradients and Monte Carlo rollouts, but can suffer from mode collapse and instability.",
            "domain_maturity": "established sequence-GAN method in ML research; prior work used it for text and music.",
            "mechanistic_understanding_requirements": "low-to-medium — provides black-box generation; lacks inherent interpretability but amenable to analysis via discriminator and reward shaping.",
            "ai_methodology_name": "SeqGAN (policy-gradient GAN for sequences)",
            "ai_methodology_description": "Generator modeled as stochastic policy G_theta (RNN/LSTM) trained via REINFORCE to maximize discriminator-provided reward; Monte Carlo rollouts used to estimate Q-values for partial sequences; discriminator is a classifier providing scalar reward. No explicit domain-specific objectives in base SeqGAN.",
            "ai_methodology_category": "hybrid: generative adversarial networks + reinforcement learning (policy-gradient)",
            "applicability": "applicable for discrete sequence generation but limited when explicit domain objectives are required; in experiments SeqGAN captured data distribution but did not bias for desired properties as ORGAN did.",
            "effectiveness_quantitative": "In molecule experiments, SeqGAN validity ~80.3% (vs MLE 75.9%, ORGAN 88.2% in druglikeliness run); in music experiments SeqGAN had low tonality/ratio-of-steps compared to ORGAN.",
            "effectiveness_qualitative": "SeqGAN can learn data-like sequences but without explicit objective guidance it fails to optimize domain-specific properties; prone to mode-collapse and needs stabilizing techniques.",
            "impact_potential": "useful baseline method for discrete sequence GANs; foundation for extensions (like ORGAN) that add objective rewards.",
            "comparison_to_alternatives": "Compared directly in experiments: ORGAN (adds objective RL reward) outperforms SeqGAN on targeted property optimization while maintaining diversity; naive RL can exceed SeqGAN on raw objectives but at cost of diversity.",
            "success_factors": "treating generator as RL policy and using rollouts to estimate future rewards enables application of GANs to discrete sequences, but performance depends on discriminator stability and reward design.",
            "key_insight": "SeqGAN enables adversarial training for discrete sequences by reframing generation as an RL problem, but without explicit objective rewards it cannot bias outputs toward domain-specific scientific goals effectively.",
            "uuid": "e2349.2",
            "source_info": {
                "paper_title": "Objective-Reinforced Generative Adversarial Networks (ORGAN) for Sequence Generation Models",
                "publication_date_yy_mm": "2017-05"
            }
        },
        {
            "name_short": "Naive RL (baseline)",
            "name_full": "Naive Reinforcement Learning (policy-gradient with domain objective only)",
            "brief_description": "A baseline that trains the sequence generator purely to maximize domain-specific heuristic rewards using policy-gradient methods (REINFORCE) without adversarial discriminator regularization.",
            "citation_title": "",
            "mention_or_use": "use",
            "scientific_problem_domain": "applied to molecule generation and music generation in this paper as a baseline",
            "problem_description": "Train sequence generator to directly maximize non-differentiable domain-specific objectives (e.g., solubility, druglikeness, musical heuristics) without constraints from data-likeness discriminator.",
            "data_availability": "uses same limited datasets for pretraining; but after pretraining, training optimizes only the objective signal, so dependence on labeled data is low beyond availability of reward computation.",
            "data_structure": "discrete sequences (SMILES, music tokens) as in other experiments.",
            "problem_complexity": "optimization landscape can be sparse and deceptive; pure RL optimization can converge to local optima that produce trivial high-reward outputs (e.g., tiny molecules or repetitive tokens) due to simplified reward structures.",
            "domain_maturity": "standard RL technique applied as baseline; known issues when used alone for objective-driven sequence generation.",
            "mechanistic_understanding_requirements": "low — solution acceptable as black box but produced outputs may be uninterpretable or trivial, requiring domain validation.",
            "ai_methodology_name": "Naive Reinforcement Learning (REINFORCE with Monte Carlo rollouts, objective-only reward)",
            "ai_methodology_description": "Generator initialized via MLE pretraining, then trained with policy-gradient to maximize a domain-specific reward O_i only (lambda=0 in ORGAN formulation). Monte Carlo rollouts used for Q estimation; no discriminator term to constrain to data distribution.",
            "ai_methodology_category": "reinforcement learning",
            "applicability": "applicable for direct objective optimization but often inappropriate alone due to generation of degenerate/trivial high-scoring sequences and loss of diversity; may require adversarial or other regularization.",
            "effectiveness_quantitative": "In molecule tasks naive RL achieved very high validity in some objectives (e.g., ~97.1% in one druglikeliness run) and high objective scores (e.g., solubility improvements), but tended to reduce diversity; in music Naive RL achieved high Ratio of Steps (0.829) but low diversity (0.321).",
            "effectiveness_qualitative": "Effective at maximizing raw objective metrics but often produces simplistic or repetitive outputs that are less useful or interesting (e.g., single-atom molecules or repetitive token sequences); trade-off between objective maximization and diversity/realism.",
            "impact_potential": "limited as a standalone approach for scientific generation tasks where realism/diversity and validity matter; useful as a component but needs regularization (e.g., discriminator) for practical use.",
            "comparison_to_alternatives": "Compared against ORGAN and SeqGAN: naive RL attains higher raw objective scores in some cases but yields lower diversity and data-likeness; ORGAN's combined adversarial+objective approach yields more balanced outputs.",
            "success_factors": "direct reward optimization drives objective gains, but the absence of a discriminator or other constraints leads to mode collapse or trivial solutions; success depends on richness of reward and penalties for degenerate outputs.",
            "key_insight": "Maximizing domain objectives with pure RL can reach high objective values but often at the cost of diversity and realism; adversarial constraints (as in ORGAN) are needed to produce practically useful scientific outputs.",
            "uuid": "e2349.3",
            "source_info": {
                "paper_title": "Objective-Reinforced Generative Adversarial Networks (ORGAN) for Sequence Generation Models",
                "publication_date_yy_mm": "2017-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Seqgan: Sequence generative adversarial nets with policy gradient",
            "rating": 2
        },
        {
            "paper_title": "Wasserstein GAN",
            "rating": 2
        },
        {
            "paper_title": "Automatic chemical design using a data-driven continuous representation of molecules",
            "rating": 2
        },
        {
            "paper_title": "Optimizing distributions over molecular space. An Objective-Reinforced Generative Adversarial Network for Inverse-design Chemistry.",
            "rating": 2
        },
        {
            "paper_title": "Tuning recurrent neural networks with reinforcement learning",
            "rating": 1
        }
    ],
    "cost": 0.014971,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Objective-Reinforced Generative Adversarial Networks (ORGAN) for Sequence Generation Models</h1>
<p>Gabriel Guimaraes ${ }^{<em>}$, Benjamin Sanchez-Lengeling</em> ${ }^{<em>}$ Carlos Outeiral ${ }^{</em>}$, <em><br>Pedro Luis Cunha Farias</em> Alán Aspuru-Guzik<em>, ${ }^{</em>}$<br>* Harvard University<br>$\dagger$ Pagedraw<br>$\S$ The University of Manchester<br>$\ddagger$ Canadian Institute for Advanced Research (CIFAR) Senior Fellow</p>
<h4>Abstract</h4>
<p>In sequence-based generative models, besides the generation of samples likely to have been drawn from a data distribution, it is often desirable to finetune the samples towards some domain-specific metrics. This work proposes a method to guide the structure and quality of samples utilizing a combination of adversarial training and expert-based rewards with reinforcement learning. Building on SeqGAN, a sequence based Generative Adversarial Network (GAN) framework modeling the data generator as a stochastic policy in a reinforcement learning setting, we extend the training process to include domain-specific objectives additional to the discriminator reward. The mixture of both types of rewards can be controlled via a tune-able parameter. To improve training stability we utilize the Wasserstein distance as loss function for the discriminator. We demonstrate the effectiveness of this approach in two tasks: generation of molecules encoded as text sequences and musical melodies. The experimental results demonstrate the models can generate samples which maintain information originally learned from data, retain sample diversity, and show improvement in the desired metrics.</p>
<h2>1 Introduction</h2>
<p>Unsupervised generation of data is a dynamic area of machine learning and a very active research frontier in areas ranging from language processing and music generation to materials and drug discovery.</p>
<p>In any of these fields, it is often advantageous to guide the generative model towards some desirable characteristics, while ensuring that the samples resemble the initial distribution. In music generation, for example, it might be expected that pleasant melodic patterns prevail over more dissonant ones [Jaques et al., 2016]. In natural language processing, a given sentiment might be emphasized, maybe for producing movie reviews [Radford et al., 2017]. Finally, in materials discovery, the aim is often to optimize some properties for a particular application, for example in organic solar cells [Hachmann et al., 2011], OLEDs [Gómez-Bombarelli et al., 2016a] or new drugs. The generation of discrete
data using Recurrent Neural Networks (RNNs), in particular, Long Short-Term Memory cells [Hochreiter and Schmidhuber, 1997] and maximum likelihood estimation has been shown to work well in practice. However, this often suffers from the so-called exposure bias, and might lack some of the multi-scale structures or salient features of the data. Meanwhile Generative Adversarial Networks (GANs) [Goodfellow et al., 2014], an approach where a generative model competes against a discriminate model, one trying to generate likely data while the other trying to distinguish false from real data. GANs have shown remarkable results at generation of data that imitates a data distribution, however they can suffer from several issues, among these mode-collapse [Arjovsky and Bottou, 2017]. Where the generator learns to produce samples with low variety.</p>
<p>Although GANs were not initially applicable to discrete data due to non-differentiability, approaches such as SeqGAN [Yu et al., 2017], MaliGAN [Che et al., 2017] and BGAN [Hjelm et al., 2017] have arisen to deal with this issue.</p>
<p>Furthermore methods from Reinforcement Learning (RL) have shown great success at solving problems where continuous feedback from an environment is needed [Hjelm et al., 2017].</p>
<p>In this paper, we introduce a novel approach to optimize the properties of a distribution of sequences, increase the diversity of the samples while maintaining the likeliness of the data distribution. In our approach, the generator is trained to maximize a weighted average of two types of rewards: the objective, domain-specific metrics, and the discriminator, which is trained along with the generator in an adversarial fashion. While the objective component of the reward function ensures that the model selects for traits that maximize the specified heuristic, the discriminator incentives the samples to stay within boundaries of the initial data distribution. Diversity is additionally promoted by reducing rewards of non-unique and less diverse sequences.</p>
<p>In order to implement the above idea, we build on SeqGAN, a recent work that successfully combines GANs and RL to apply the GAN framework to sequential data [Yu et al., 2017] and extend it towards domain-specific rewards. To increase the stability of the adversarial training, we test Wasserstein-GANs [Arjovsky et al., 2017] in this framework.</p>
<p>We test our model in the context of molecular and music generation, optimizing several domain-specific metrics.</p>
<p>Our results show that ORGAN is able to tune the quality and structure of samples. We compare our results with the maximum likelihood estimation (MLE), SeqGAN and a RL approach.</p>
<h2>2 Related work</h2>
<p>Previous work has relied on specific modifications of the objective function to reach the desired properties. For example, <em>Jaques et al. (2016)</em> introduce penalties to unrealistic sequences, in absence of which RL can easily get stuck around local maxima which can be very far from the global maximum reward. Related applications by <em>Ranzato et al. (2015)</em> and <em>Li et al. (2016)</em> apply reinforcement learning to sequence generation in a NLP setting.</p>
<p>In the last two years, many methodologies have been proposed for de novo molecular generation. <em>Ertl et al. (2017)</em> and <em>Segler et al. (2017)</em> trained recurrent neural networks to generate drug-like molecules. [Gómez-Bombarelli et al. (2016b)] employed a variational autoencoder to build a latent, continuous space where property optimization can be made through surrogate optimization. Finally, <em>Kadurin et al. (2017)</em> presented a GAN model for drug generation. Additionally, the approach presented in this paper has recently been applied to molecular design [Sanchez-Lengeling et al. (2017)].</p>
<p>In the field of music generation, <em>Lee et al. (2017)</em> built a SeqGAN model employing an efficient representation of multi-channel MIDI to generate polyphonic music. <em>Chen et al. (2017)</em> presented Fusion GAN, a dual-learning GAN model that can fuse two data distributions. <em>Jaques et al. (2017)</em> employ deep Q-learning with a cross-entropy reward to optimize the quality of melodies generated from an RNN.</p>
<p>In adversarial training, <em>Pfau and Vinyals (2016)</em> recontextualizes GANs in the actor-critic setting. This connection is also explored with the Wasserstein-1 distance in WGANs [Arjovsky et al. (2017)]. Minibatch discrimination and feature mapping were used to promote diversity in GANs [Salimans et al. (2016)]. Another approach to avoid mode collapse was shown with Unrolled GANs <em>Metz et al. (2016)</em>. Issues and convergence of GANs has been studied in <em>Mescheder et al. (2017)</em>.</p>
<h2>3 Background</h2>
<p>In this section, we elaborate on the GAN and RL setting based on SeqGAN <em>Yu et al. (2017)</em>.</p>
<p>$G_{\theta}$ is a generator parametrized by $\theta$, that is trained to produce high-quality sequences $Y_{1:T}=\left(y_{1},...,y_{T}\right)$ of length $T$ and a discriminator model $D_{\phi}$ parametrized by $\phi$, trained to classify real and generated sequences. $G_{\theta}$ is trained to deceive $D_{\phi}$, and $D_{\phi}$ to classify correctly. Both models are trained in alternation, following a minimax game:</p>
<p>$\min_{\phi}\mathbb{E}<em _text_data="\text{data">{Y\sim p</em>}}(Y)}[\log D(Y)]+\mathbb{E<em G__theta="G_{\theta">{Y\sim p</em>[\log(1-D(Y))]$ (1)}}(Y)</p>
<p>For discrete data, the sampling process is not differentiable. However, $G_{\theta}$ can be trained as an agent in a reinforcement learning context using the REINFORCE algorithm [Williams (1992)]. Let $R(Y_{1:T})$ be the reward function defined for full length sequences. Given an incomplete sequence $Y_{1:t}$, also to be referred to as state $s_{t}$, $G_{\theta}$ must produce an action $a$, along with the next token $y_{t+1}$.</p>
<p>The agent’s stochastic policy is given by $G_{\theta}\left(y_{t}|Y_{1:t-1}\right)$ and we wish to maximize its expected long term reward</p>
<p>$J(\theta)=E[R(Y_{1:T})|s_{0},\theta]=\sum_{y_{t}\in Y} G_{\theta}\left(y_{1}|s_{0}\right)\cdot Q(s_{0},y_{1})$ (2)</p>
<p>where $s_{0}$ is a fixed initial state. $Q(s,a)$ is the action-value function that represents the expected reward at state $s$ of taking action $a$ and following our current policy $G_{\theta}$ to complete the rest of the sequence. For any full sequence $Y_{1:T}$, we have $Q(s=Y_{1:T-1},a=y_{T})=R(Y_{1:T})$ but we also wish to calculate $Q$ for partial sequences at intermediate timesteps, considering the expected future reward when the sequence is completed. In order to do so, we perform $N$-time Monte Carlo search with the canonical rollout policy $G_{\theta}$ represented as</p>
<p>$\text{MC}^{G_{\theta}}(Y_{1:t} ; N)={Y_{1:T}^{1},...,Y_{1:T}^{N}}$ (3)</p>
<p>where $Y_{1:t}^{n}=Y_{1:t}$ and $Y_{t+1:T}^{n}$ is stochastically sampled via the policy $G_{\theta}$. Now $Q(s,a)$ becomes</p>
<p>[ Q(Y_{1:t-1},y_{t})=\left{\begin{array} {l} \frac{1}{N} \sum_{n=1..N} R(Y_{1:T}^{n}), \text{with} \ Y_{1:T}^{n} \in \text{MC}^{G_{\theta}}(Y_{1:t} ; N), \text{ if } t&lt;T . \ R(Y_{1:T}), \text{ if } t=T .\end{array}\right. \quad \text{ (4)} ]</p>
<p>An unbiased estimation of the gradient of $J(\theta)$ can be derived as</p>
<p>[ \nabla_{\theta}J(\theta) \simeq \frac{1}{T} \sum_{t=1,\ldots,T} \mathbb{E}<em t="t">{y</em> [ \
\nabla_{\theta} \log G_{\theta}\left(y_{t}|Y_{1:t-1}\right) \cdot Q\left(Y_{1:t-1},y_{t}\right)] \quad (5)
]} \sim G_{\theta}\left(y_{t}|Y_{1:t-1}\right)</p>
<p>Finally in SeqGAN the reward function is provided by $D_{\phi}$.</p>
<h2>4 ORGAN</h2>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Schema for ORGAN. Left: $D$ is trained as a classifier receiving as input a mix of real data and generated data by $G$. Right: $G$ is trained by RL where the reward is a combination of $D$ and the objectives, and is passed back to the policy function via Monte Carlo sampling. We penalize non-unique sequences.</p>
<p>Figure 1 illustrates the main idea of ORGAN. To take into account domain-specific desired objectives $O_{i}$, we extend the</p>
<p>reward function for a particular sequence $Y_{1: t}$ to a linear combination of $D_{\phi}$ and $O_{i}$, parametrized by $\lambda$ :</p>
<p>$$
R\left(Y_{1: T}\right)=\lambda \cdot D_{\phi}\left(Y_{1: T}\right)+(1-\lambda) \cdot O_{i}\left(Y_{1: T}\right)
$$</p>
<p>If $\lambda=0$ the model ignores $D$ and becomes a "naive" RL algorithm, whereas if $\lambda=1$ it is simply a SeqGAN model. It should be noted that, if chosen, the objective function can vary based on the current iteration of adversarial training, leading to alternating rewards between several objectives and the discriminator.</p>
<p>An additional mechanism to prevent mode collapse is to penalize non-unique sequences by dividing the reward of a repeated sequence by it's the number of copies. The more a sequence gets repeated, the more it will have diminishing rewards. Alternatively, domain-specific similarity metrics could be used to penalize.</p>
<p>To improve the stability of learning, and avoid of problems of GAN convergence like "perfect discriminator", we also implemented the Wasserstein-1 $W$ distance, also known as earth mover's distance, for $D_{\phi}$ [Arjovsky et al., 2017]. Although the computation of this distance is intractable due to an infimum, it can be transformed via the Kantorovich-Rubinstein duality:
$W\left(p_{\text {data }}, p_{G}\right)=\frac{1}{K} \sup <em Y="Y" _dan="{dan" _sim="\sim" p__text="p_{\text">{|D| \geq K} \mathbb{E}</em>}}}[D(Y)]-\mathbb{E<em G="G">{Y \sim p</em>[D(Y)]$
Under $W, D$ is no longer meant to classify data samples, but now trained and converged to learn $\phi$ such that $D_{\phi}$ is K-Lipschitz continuous and used to compute the Wasserstein distance. Intuitively the cost of moving the generated distribution to the data. In this context, $D$ can now be considered as a critic in an actor-critic setting.}</p>
<h3>4.1 Implementation Details</h3>
<p>$G_{\theta}$ is a RNN with LSTM cells, while $D_{\phi}$ is Convolutional Neural Network (CNN) designed specifically for text classification tasks [Kim, 2014].</p>
<p>To avoid over-fitting with the CNN, we optimized its architecture on classification task between different datasets for each experiment. In the molecule generation task, we utilized a set of drug-like and nondrug-like molecules from the ZINC database [Irwin and Shoichet, 2005]. In the music task, we discriminated between a set of folk and videogame tunes scraped from the internet. We utilize a dropout layer at $75 \%$ and also $L 2$ regularization on the network weights. All the gradient descent steps are done using the Adam algorithm [Kingma and Ba, 2014].</p>
<p>Molecular metrics are implemented using the RDKit chemoinformatics package [Landrum, 2016]. Music metrics employ the MIDI frequencies. The code for ORGAN, including metrics for each experiment, can be found at http: //github.com/gablgl/ORGAN ${ }^{1}$.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>5 Experimental results</h2>
<p>In this section, we will test the performance of ORGAN in two scenarios: the generation of molecules encoded as text sequences and musical melodies. Our objective is to show that ORGAN can generate samples that fulfill some desired objectives while promoting diversity. For purposes of interpretation, the range of each objective has been mapped to $[0,1]$ range, where 0 corresponds to an undesirable property and 1 to a very desirable property. Each generator model was pre-trained for 250 epochs using MLE, and the discriminator was trained for 10 epochs.</p>
<p>To measure diversity we use domain-specific measures. In both fields, there are multiple ways of quantifying the notion of diversity so we tried utilizing more widely used metrics.</p>
<p>We compare ORGAN and the Wasserstein variant ( $W$ ) with three other methods of training RNNs: SeqGAN, Naive RL, and Maximum Likelihood Estimation (MLE). Unless specified, $\lambda$ is assumed to be 0.5 . All training methods involve a pre-training step of 250 epochs of MLE for $G_{\theta}$, and 10 epochs for $D_{\phi}$. The MLE baseline simply stops right after pre-training, while the other methods proceed to further train the model using the different approaches, up to 100 epochs.</p>
<p>For each dataset, we first build a dictionary mapping the vocabulary - the set of all characters present in the dataset to integers. The dataset is then preprocessed by transforming each sequence into a fixed sized integer sequence of length $N$ where $N$ is the maximum length of a string present in the dataset (in the case of molecules, along with around $10 \%$ more characters to increase flexibility and allow generation of larger samples of data). Every string with a length smaller than $N$ is padded with " " characters. Thus the input to our model becomes a list of fixed sized integer sequences.</p>
<h3>5.1 Experiment: Molecules</h3>
<p>Here we test the effectiveness of ORGAN for generating molecules with desirable properties in a pharmaceutical context of drug discovery.</p>
<p>Molecules can be encoded as text sequences by using the SMILES representation [Weininger, 1988] of a molecule. This representation encodes the topological information of a molecule based on common chemical bonding rules. For example, the 6 -carbon ringed molecule benzene can be encoded as ' $\mathrm{C} 1=\mathrm{CC}=\mathrm{CC}=\mathrm{C} 1$ '. Each C represents a carbon atom, the ' $=$ ' symbolizes a double bond and ' 1 ' the start and closing of a cycle/ring, hydrogen atoms can be deduced via simple rules.</p>
<p>The SMILES representation has predefined grammar rules, and as such, it is possible to have invalid expressions that cannot be decoded back to a valid molecule. Therefore desired property on a generative algorithm is to have a high percentage of valid expression. Invalid expressions get penalized. Additionally, we also penalize the generation of duplicate molecules.</p>
<p>Recent generative models ([Gómez-Bombarelli et al., 2016a], [Kusner et al., 2017]) have reported valid expression rates between $4 \%$ up to $80 \%$. It should be noted that there are common uninteresting ways to generate valid expressions by alternating "C" and "O" characters such as 'CCCCCCCC'</p>
<p>and 'COCCCCOC', the combinatorial possibilities of such permutations is already huge.</p>
<p>For training, we utilized a random subset of 5k molecules from the set of 134 thousand stable small molecules [Ramakrishnan et al., 2014]. This is a subset of all molecules with up to nine heavy atoms (CONF) out of the GDB-17 universe of 166 billion organic molecules [Ramakrishnan et al., 2014]. The maximum sequence length is 51 and the alphabet size is 43.</p>
<p>When choosing objectives we picked qualities that are normally desired for small molecule drug discovery:</p>
<ul>
<li><strong>Solubility</strong>: a property that measures how likely a molecule is able to mix with water, also known as the water-octanol partition coefficient (LogP). Computed via RDKit's Crippen function [Landrum, 2016].</li>
<li><strong>Synthetizability</strong>: estimates how hard (0) or how easy (1) it is to synthesize a given molecule [Ertl and Schuffenhauer, 2009].</li>
<li><strong>Druglikeness</strong>: how likely a molecule is a viable candidate for a drug, an estimate that captures the abstract notion of aesthetics in medicinal chemistry [Bickerton et al., 2012]. This property is correlated to the previous two metrics.</li>
</ul>
<p>To estimate the diversity of our generated samples we can utilize the notion of molecular similarity to construct a measure of how similar or dissimilar a molecule is with respect to a dataset. This measure is based on molecular fingerprints and their Jaccard distance [Sanchez-Lengeling et al., 2017]. More concretely, Diversity measures the average similarity of a molecule with respect to a set, in this case, a random subset of molecules from the training set. A value of 1 would indicate the molecule is likely to be considered a diverse member of this set, 0 would indicate it has many repeated substructures with respect to the set.</p>
<p>Table 1 shows quantitative results comparing ORGAN to other methods and three different optimization scenarios. MLE and SeqGAN are able to capture the distribution of properties of the training set with minimal alteration in their metrics. While the metric optimized methods excelled in all metrics above the non-optimized methods, effectively showing that they are able to bias the generation process. The Wasserstein variant of ORGAN also seemed to give better diversity properties.</p>
<p>In our experiments, we also noted that naive RL has different failure scenarios. For instance, this approach excelled particularly in the task of Solubility, this particular task rewards very simple sequences such as for the single atom molecule "N" or monotonous patterns like "CCCCCCC" or "CCOCOCCCC" positively. It seems for the other approaches, the GAN/WGAN setting is enforcing more diversity and so punishes these types of patterns, providing highly soluble molecules with more complex features.</p>
<h3>Capacity ceiling</h3>
<p>We did notice a form of capacity ceiling in our generation tasks in two forms. The GAN models tended to generate sequences that had the same average sequence length as the training set (15.42). With RL we did not observe this constraint, either it went quite low with synthesizability (9.4) or high (21.3) with druglikeliness. This might be advantageous or detrimental based on the setting. Optimizing a property that relates to sequence length, for example, molecular size might change this.</p>
<p>The other ceiling is illustrated in figure 2, where the upper limits in Druglikeliness for the data and the best performing approach match. While OR(W)GAN tends to generate more druglike molecules, they do not reach the highest value of 1. This might be property and dataset dependent.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Violinplots of Druglikeliness for molecules from the baseline <em>Dataset</em> (n=5000) and optimized OR(W)GAN (n=5440).</p>
<h3>Multi-objective training programs</h3>
<p>We also experimented with alternating objectives during training. By training for one epoch each objective in rotation until 99 epochs (33 epochs per objective) we arrive to figure 3.</p>
<p>Surprisingly by alternating the objectives, as seen in the last row of table 1, the gains in each metric are quite high and almost comparable with the best models in each individually trained objective. Although it can also be appreciated in the slight fluctuating behavior of the graphs that there might be limits to the gains that can be achieved. Further work is warranted in this direction.</p>
<h3>5.2 Experiment: Musical melodies</h3>
<p>To further demonstrate the applicability of ORGAN, we extend our study to music sequences. We employ the notation introduced by [Jaques et al., 2017], where each token corresponds to a sixteenth of a bar of music. The first two tokens are reserved as 0, which is silent, and 1, which means no event; the other 36 tokens encode three octaves of music, from C3 (MIDI pitch 48) to B5. We use a 1k random sample from the Essen Associative Code (EsAC) folk dataset as processed by [Chen et al., 2017], where every melody has a duration of 36 tokens (2.25 music bars). We generate songs optimizing two different metrics:</p>
<ul>
<li><strong>Tonality</strong>: This measures how many perfect fifths are in the music that is generated. A perfect fifth is defined as a</li>
</ul>
<table>
<thead>
<tr>
<th>Objective</th>
<th>Algorithm</th>
<th>Validity (%)</th>
<th>Diversity</th>
<th>Druglikeliness</th>
<th>Synthesizability</th>
<th>Solubility</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>MLE</td>
<td>75.9</td>
<td>0.64</td>
<td>0.48</td>
<td>(0%)</td>
<td>0.23</td>
</tr>
<tr>
<td></td>
<td>SeqGAN</td>
<td>80.3</td>
<td>0.61</td>
<td>0.49</td>
<td>(2%)</td>
<td>0.25</td>
</tr>
<tr>
<td>Druglikeliness</td>
<td>ORGAN</td>
<td>88.2</td>
<td>0.55</td>
<td>0.52</td>
<td>(8%)</td>
<td>0.32</td>
</tr>
<tr>
<td></td>
<td>OR(W)GAN</td>
<td>85.0</td>
<td>0.95</td>
<td>0.60</td>
<td>(25%)</td>
<td>0.54</td>
</tr>
<tr>
<td></td>
<td>Naive RL</td>
<td>97.1</td>
<td>0.8</td>
<td>0.57</td>
<td>(19%)</td>
<td>0.53</td>
</tr>
<tr>
<td>Synthesizability</td>
<td>ORGAN</td>
<td>96.5</td>
<td>0.92</td>
<td>0.51</td>
<td>(6%)</td>
<td>0.83</td>
</tr>
<tr>
<td></td>
<td>OR(W)GAN</td>
<td>97.6</td>
<td>1.00</td>
<td>0.20</td>
<td>(-59%)</td>
<td>0.75</td>
</tr>
<tr>
<td></td>
<td>Naive RL</td>
<td>97.7</td>
<td>0.96</td>
<td>0.52</td>
<td>(8%)</td>
<td>0.83</td>
</tr>
<tr>
<td>Solubility</td>
<td>ORGAN</td>
<td>94.7</td>
<td>0.76</td>
<td>0.50</td>
<td>(4%)</td>
<td>0.63</td>
</tr>
<tr>
<td></td>
<td>OR(W)GAN</td>
<td>94.1</td>
<td>0.90</td>
<td>0.42</td>
<td>(-12%)</td>
<td>0.66</td>
</tr>
<tr>
<td></td>
<td>Naive RL</td>
<td>92.7</td>
<td>0.75</td>
<td>0.49</td>
<td>(3%)</td>
<td>0.70</td>
</tr>
<tr>
<td>All/Alternated</td>
<td>ORGAN</td>
<td>96.1</td>
<td>92.3</td>
<td>0.52</td>
<td>(9%)</td>
<td>0.71</td>
</tr>
</tbody>
</table>
<p>Table 1: Evaluation of metrics, on several generative algorithms and optimized for different objectives for molecules. Reported values are mean values of valid generated molecules. The percentage of improvement over the MLE baseline is reported in parenthesis. Values shown in bold indicate significant improvement. Shaded cell indicates direct optimized objectives.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Plots of each objective across the training epochs. Objectives were trained for one epoch, and then switched for another.</p>
<p>Musical interval whose frequencies have a ratio of approximately 3:2. These provide what is generally considered pleasant note sequences due to their high consonance.</p>
<p>Ratio of Steps. A step is an interval between two consecutive notes of a scale. An interval from C to D, for example, is a step. A skip, on the other hand, is a longer interval. An interval from C to G, for example, is a skip. By maximizing the ratio of steps in our music, we are adhering to the conjunct melodic motion. Our rationale here is that by increasing the number of steps in our songs we make our melodic leaps rarer and more memorable [Bonds, 2013].</p>
<p>Moreover, we calculate diversity as the average pairwise edit distance of the generated data [Habrard <em>et al.</em>, 2008]. We do not attempt to maximize this metric explicitly but we keep track of it to shed light on the trade-off between metric optimization and sample diversity in the ORGAN framework. Table 2 shows quantitative results comparing ORGAN to other baseline methods optimizing for three different metrics. ORGAN outperforms SeqGAN and MLE in all of the three metrics. Naive RL achieves a higher score than ORGAN for the Ratio of Steps metric, but it under-performs in terms of diversity, as Naive RL would likely generate very simple rather than diverse songs. In this sense, similar to the molecule case, although the Naive RL ratio of steps score is higher than ORGAN's, the actual generated songs can be deemed much less interesting.</p>
<p>We note that the Ratio of Steps and Tonality have an inverse relationship. This is because two consecutive notes - what qualifies as a step - do not have the frequency ratio of a perfect fifth, which are responsible for increasing tonality. In addition, although the usage of the Wasserstein metric seems to decrease the metrics value, this can be explained as the result of slower training.</p>
<h3>Effect of λ</h3>
<p>By tweaking λ, the ORGAN approach allows one to explore the trade-off between maximizing the desired objective and maintaining likelihood to the data distribution.</p>
<p>Figure 4 shows the distribution of tonality and diversity sampled from ORGAN and OR(W)GAN for several λ values. This showcases that there exists an optimal value for λ which maximizes simultaneously the reward and diversity. This value is dependent on the model, dataset and metric, therefore a parameter search would be advantageous to maximize objectives.</p>
<h2>6 Conclusions and future work</h2>
<p>In this work, we have presented ORGAN, a novel framework to optimize an arbitrary object in a sequence generation task. We have built on recent advances in GANs, particularly SeqGAN, and extended them with reinforcement learning to control properties of generated samples.</p>
<p>We have shown that ORGAN can improve certainly desired metrics, achieving better results than recurrent neural</p>
<table>
<thead>
<tr>
<th>Objective</th>
<th>Algorithm</th>
<th>Diversity</th>
<th>Tonality</th>
<th>Ratio of Steps</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>MLE</td>
<td>0.221</td>
<td>0.007</td>
<td>0.010</td>
</tr>
<tr>
<td></td>
<td>SeqGAN</td>
<td>0.187</td>
<td>0.005</td>
<td>0.010</td>
</tr>
<tr>
<td>Tonality</td>
<td>Naive RL</td>
<td>0.100</td>
<td>0.478</td>
<td>2.9E-05</td>
</tr>
<tr>
<td></td>
<td>ORGAN</td>
<td>0.268</td>
<td>0.372</td>
<td>1.78E-04</td>
</tr>
<tr>
<td></td>
<td>OR(W)GAN</td>
<td>0.268</td>
<td>0.177</td>
<td>2.4E-04</td>
</tr>
<tr>
<td>Ratio of Steps</td>
<td>Naive RL</td>
<td>0.321</td>
<td>0.001</td>
<td>0.829</td>
</tr>
<tr>
<td></td>
<td>ORGAN</td>
<td>0.433</td>
<td>0.001</td>
<td>0.632</td>
</tr>
<tr>
<td></td>
<td>OR(W)GAN</td>
<td>0.134</td>
<td>5.95E-05</td>
<td>0.622</td>
</tr>
</tbody>
</table>
<p>Table 2: Evaluation of metrics, on several generative algorithms and optimized for different objectives for melodies. Each measure is averaged over a set of 1000 generated songs. Values shown in bold indicate significant improvement over MLE baseline. Shaded cell indicates directly optimized objectives.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Plots of Diversity and Tonality rewards (the latter re-scaled to the $[0,1]$ interval) after 80 epochs of training on the music generation task. The upper plot employs the classical GAN loss, while the lower displays a WGAN. The values have been averaged over 1000 samples.
networks trained via either MLE or SeqGAN. Even more importantly, data generation can be made subject to a domainspecific reward function while still using the adversarial setting to guarantee the production of non-repetitious samples. Moreover, ORGAN possesses a natural advantage as a black box compared to similar objective-optimization models, since it is not necessary to introduce multiple domain-specific penalties to the reward function: many times a simple objective "hint" will suffice.</p>
<p>As evidenced with the experiments, the RL component is the one of the major drivers for the property optimization and promotion of diversity. Values tended to be higher when RL was present in the architecture of the models.</p>
<p>Future work should investigate how the choice of heuristic can affect the performance of the model. There are also other formulations of GANs for discrete sequences [Che et al., 2017],[Hjelm et al., 2017] that could be extended with a RL component in order to fine-tune the generation processes.</p>
<p>One area of improvement as seen from figure 2 is to push the boundaries of the datasets in certain properties. In some domains, outliers might be more valuable such as in the case of drug and materials discovery.</p>
<p>Finally, forthcoming research should extend ORGANs to work with non-sequential data, such as images or audio. This requires framing the GAN setup as a reinforcement learning problem in order to add an arbitrary (not necessarily differentiable) objective function. We believe this extension to be quite promising since real-valued GANs are currently better understood than sequence data GANs.</p>
<h2>References</h2>
<p>[Arjovsky and Bottou, 2017] Martin Arjovsky and Léon Bottou. Towards Principled Methods for Training Generative Adversarial Networks. 2017.
[Arjovsky et al., 2017] Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein GAN. 2017.
[Bickerton et al., 2012] G Richard Bickerton et al. Quantifying the chemical beauty of drugs. Nature chemistry, 4(2), 2012.
[Bonds, 2013] Mark Evan. Bonds. A history of music in Western culture. Pearson Education, 2013.</p>
<p>[Che et al., 2017] Tong Che et al. Maximum-Likelihood Augmented Discrete Generative Adversarial Networks. 2017.
[Chen et al., 2017] Zhiqian Chen et al. Learning to fuse music genres with generative adversarial dual learning. arXiv preprint arXiv:1712.01456, 2017.
[Ertl and Schuffenhauer, 2009] Peter Ertl and Ansgar Schuffenhauer. Estimation of synthetic accessibility score of drug-like molecules. J. Cheminform., 1(1), 2009.
[Ertl et al., 2017] Peter Ertl et al. In silico generation of novel, drug-like chemical matter using the lstm neural network. arXiv preprint arXiv:1712.07449, 2017.
[Gómez-Bombarelli et al., 2016a] Rafael GómezBombarelli et al. Design of efficient molecular OLEDs by a high-throughput virtual screening and experimental approach. Nat. Mater., 15(10), 2016.
[Gómez-Bombarelli et al., 2016b] Rafael GómezBombarelli et al. Automatic chemical design using a data-driven continuous representation of molecules. arXiv, 2016.
[Goodfellow et al., 2014] Ian J. Goodfellow et al. Generative Adversarial Networks. arXiv, 2014.
[Habrard et al., 2008] Amaury Habrard et al. Melody Recognition with Learned Edit Distances. Springer, Berlin, Heidelberg, 2008.
[Hachmann et al., 2011] Johannes Hachmann et al. LargeScale Computational Screening and Design of Organic Photovoltaics on the World Community Grid. J. Phys. Chem. Lett., 2(17), 2011.
[Hjelm et al., 2017] R Devon Hjelm et al. Boundary-Seeking Generative Adversarial Networks. 2017.
[Hochreiter and Schmidhuber, 1997] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Comput., 9(8), 1997.
[Irwin and Shoichet, 2005] John J Irwin and Brian K Shoichet. J. Chem. Inf. Model, (December 2004), 2005.
[Jaques et al., 2016] Natasha Jaques et al. Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control. arXiv, 2016.
[Jaques et al., 2017] Natasha Jaques et al. Tuning recurrent neural networks with reinforcement learning. 2017.
[Kadurin et al., 2017] Artur Kadurin et al. drugan: de novo generation of new molecules with desired molecular properties in silico. Molecular pharmaceutics, 14(9), 2017.
[Kim, 2014] Yoon Kim. Convolutional Neural Networks for Sentence Classification. Proc. 2014 Conf. Empir. Methods Nat. Lang. Process. (EMNLP 2014), 2014.
[Kingma and Ba, 2014] Diederik P Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. ICLR, 2014.
[Kusner et al., 2017] Matt J. Kusner, Brooks Paige, and José Miguel Hernández-Lobato. Grammar Variational Autoencoder. 2017.
[Landrum, 2016] Greg Landrum. RDKit, 2016.
[Lee et al., 2017] Sang-gil Lee et al. A seqgan for polyphonic music generation. arXiv preprint arXiv:1710.11418, 2017.
[Li et al., 2016] Jiwei Li et al. Deep reinforcement learning for dialogue generation. arXiv preprint arXiv:1606.01541, 2016.
[Mescheder et al., 2017] Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The Numerics of GANs. 2017.
[Metz et al., 2016] Luke Metz et al. Unrolled Generative Adversarial Networks. 2016.
[Pfau and Vinyals, 2016] David Pfau and Oriol Vinyals. Connecting Generative Adversarial Networks and ActorCritic Methods. 2016.
[Radford et al., 2017] Alec Radford, Rafal Jozefowicz, and Ilya Sutskever. Learning to generate reviews and discovering sentiment. arXiv preprint arXiv:1704.01444, 2017.
[Ramakrishnan et al., 2014] Raghunathan Ramakrishnan et al. Quantum chemistry structures and properties of 134 kilo molecules. Sci. Data, 1, 2014.
[Ranzato et al., 2015] Marc'Aurelio Ranzato et al. Sequence level training with recurrent neural networks. arXiv preprint arXiv:1511.06732, 2015.
[Salimans et al., 2016] Tim Salimans et al. Improved Techniques for Training GANs. NIPS, 2016.
[Sanchez-Lengeling et al., 2017] Benjamin SanchezLengeling et al. Optimizing distributions over molecular space. An Objective-Reinforced Generative Adversarial Network for Inverse-design Chemistry. 2017.
[Segler et al., 2017] Marwin HS Segler et al. Generating focused molecule libraries for drug discovery with recurrent neural networks. ACS Central Science, 2017.
[Weininger, 1988] David Weininger. SMILES, a chemical language and information system. J. Chem. Inf. Model., 28(1), 1988.
[Williams, 1992] R J Williams. Simple Statistical GradientEstimating Algorithms for Connectionist Reinforcement Learning. Mach. Learn., 8(3), 1992.
[Yu et al., 2017] Lantao Yu et al. Seqgan: Sequence generative adversarial nets with policy gradient. In AAAI, 2017.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Repo soon to be updated (May'18)&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
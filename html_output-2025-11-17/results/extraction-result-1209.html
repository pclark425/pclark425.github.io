<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1209 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1209</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1209</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-26.html">extraction-schema-26</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <p><strong>Paper ID:</strong> paper-266362372</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.11690v1.pdf" target="_blank">Agent-based Learning of Materials Datasets from Scientific Literature</a></p>
                <p><strong>Paper Abstract:</strong> Advancements in machine learning and artificial intelligence are transforming materials discovery. Yet, the availability of structured experimental data remains a bottleneck. The vast corpus of scientific literature presents a valuable and rich resource of such data. However, manual dataset creation from these resources is challenging due to issues in maintaining quality and consistency, scalability limitations, and the risk of human error and bias. Therefore, in this work, we develop a chemist AI agent, powered by large language models (LLMs), to overcome these challenges by autonomously creating structured datasets from natural language text, ranging from sentences and paragraphs to extensive scientific research articles. Our chemist AI agent, Eunomia, can plan and execute actions by leveraging the existing knowledge from decades of scientific research articles, scientists, the Internet and other tools altogether. We benchmark the performance of our approach in three different information extraction tasks with various levels of complexity, including solid-state impurity doping, metal-organic framework (MOF) chemical formula, and property relations. Our results demonstrate that our zero-shot agent, with the appropriate tools, is capable of attaining performance that is either superior or comparable to the state-of-the-art fine-tuned materials information extraction methods. This approach simplifies compilation of machine learning-ready datasets for various materials discovery applications, and significantly ease the accessibility of advanced natural language processing tools for novice users in natural language. The methodology in this work is developed as an open-source software on https://github.com/AI4ChemS/Eunomia.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1209.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1209.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Eunomia</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Eunomia (chemist AI agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A zero-shot, LLM-based autonomous chemist agent introduced in this paper that extracts structured materials datasets from unstructured scientific literature by planning and using domain-specific tools (Doc Search, Chain-of-Verification, Dataset Search, CSV Generator).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Eunomia</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An autonomous, tool-augmented AI agent with an LLM (GPT-4, temperature 0) as its core, using the ReAct planning architecture and LangChain for orchestration. It leverages document retrieval (Doc Search with text-ada-002 embeddings + FAISS + MMR), a user-customizable Chain-of-Verification (CoV) module, dataset lookup tools, and CSV/JSON output generation to perform planning, multi-step reasoning, information extraction, and iterative self-checking over sentences, paragraphs, and full research articles.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Materials science / materials data extraction (materials NLP)</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>Eunomia autonomously reads scientific text (sentences, paragraphs, full papers) and produces structured datasets for downstream materials discovery tasks. Demonstrated tasks include: (1) extracting host-to-many dopant relations from single sentences; (2) extracting MOF chemical formulas and guest species from multi-sentence paragraphs; (3) identifying MOFs in full articles and classifying/predicting their water stability while providing exact in-context supporting sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td>The paper does not label Eunomia's outputs as 'incremental' or 'transformational' or use other discovery-category terminology; it frames Eunomia as a method that simplifies and accelerates dataset creation and achieves parity or superiority with fine-tuned models, but does not classify the resulting scientific discoveries themselves.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Quantitative NLP metrics applied to the extraction tasks: precision, recall, and F1 (word-level exact-match as defined in Dunn et al.) for Case Studies 1 and 2; ternary accuracy and confusion matrices for Case Study 3 (Stable / Unstable / Not provided). Yield (fraction of ground-truth MOFs recovered) is reported for Case Study 3. Comparisons are made against a fine-tuned LLM baseline (LLM-NERRE). Embedding choices, retrieval parameters (k, MMR λ=0.5), and the use/absence of CoV are explicitly evaluated. Hand-labeled ground truth by expert chemists is used as the evaluation reference for the water-stability case study.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Validation against human-curated ground-truth datasets (e.g., three expert chemists labeled 101 articles containing 371 MOFs for water stability); direct benchmark comparison to a fine-tuned LLM method (LLM-NERRE/Dunn et al.); internal verification using the Chain-of-Verification module that re-checks extracted claims against user-specified chemistry-informed criteria; reporting of confusion matrices and yields; human-in-the-loop review of outputs (encouraged by the authors).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>Novelty is judged by the paper on two axes: (1) methodological: integration of an LLM agent with domain-specific tools (Doc Search, CoV) to enable zero-shot extraction without fine-tuning; (2) empirical: matching or exceeding performance of fine-tuned approaches on benchmark extraction tasks. The authors argue the zero-shot, tool-augmented agent workflow is novel relative to labor-intensive fine-tuning pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td>Reported numeric outcomes used to quantify impact include: Case Study 1 (hosts/dopants): Eunomia + CoV hosts F1 = 0.905 vs LLM-NERRE hosts F1 = 0.883; Eunomia + CoV dopants F1 = 0.920 vs LLM-NERRE dopants F1 = 0.821. Case Study 2 (MOF formula): Eunomia F1 = 0.606 vs LLM-NERRE F1 = 0.424; (guest species) Eunomia recall = 0.923 precision = 0.429 (F1 = 0.585) vs LLM-NERRE F1 = 0.606. Case Study 3 (water stability): ternary accuracy with CoV = 0.91 and yield = 86.20% (without CoV accuracy = 0.86, yield = 82.70%). These metrics are used as the primary measures of efficacy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td>The paper compares Eunomia's extraction performance against a fine-tuned model (LLM-NERRE) rather than to human 'discoveries'. For benchmark tasks, Eunomia+CoV outperforms LLM-NERRE on host/dopant F1 and MOF formula extraction; guest species extraction results are mixed (high recall but lower precision for Eunomia, attributed to exact-word matching evaluation). For water-stability extraction, Eunomia's outputs are validated against human-curated labels (three expert chemists) and show high ternary accuracy and yield. Exact numeric comparisons are reported in Tables 2 and 3 and in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Success measured per task: Case Study 1 (host/dopant): Eunomia + CoV F1 scores: hosts 0.905, dopants 0.920. Case Study 2: MOF formula F1 = 0.606; guest species F1 = 0.585 (recall 0.923, precision 0.429). Case Study 3 (MOF water stability): ternary accuracy 0.91 with CoV and yield 86.20% (without CoV: accuracy 0.86, yield 82.70%). The agent is described as 'cautious' and often outputs 'Not provided' rather than making uncertain predictions, which affects recall/precision trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Key limitations identified: (1) hallucination risk from LLMs (mitigated but not eliminated by CoV); (2) exact-word matching evaluation metric (from Dunn et al.) penalizes multi-word and semantically-correct but lexically-different predictions, underestimating true performance; (3) difficulty in formalizing chemistry-informed CoV rules for some tasks (authors omitted CoV for Case Study 2 for this reason); (4) occasional skipping of CoV by the agent; (5) computational/token cost of LLM queries; (6) retrieval/repeat-reference behavior (agent sometimes repeatedly refers back to documents); (7) dependence on embedding quality and retrieval hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Agent-based Learning of Materials Datasets from Scientific Literature', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1209.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1209.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-NERRE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-NERRE (fine-tuned LLM named-entity & relation extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fine-tuned LLM approach (reported by Dunn et al.) that uses supervised fine-tuning on 100–1000 manually annotated examples to perform named entity recognition and relation extraction in materials science text, used here as a benchmark baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Structured information extraction from complex scientific text with fine-tuned large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-NERRE</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A fine-tuned large language model (GPT-3 in the referenced work) trained on hundreds of manually annotated examples to perform NER and relation extraction tailored to materials science text; evaluated using exact-word (word-level) matching metrics as described in Dunn et al.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Materials information extraction / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>Used as a supervised baseline for extracting entities and relations (host-dopant, MOF formula, guest species) from materials science text. In the referenced work the model is trained on labeled examples and then applied to extraction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td>The paper treats LLM-NERRE as a baseline method and does not characterize any discoveries made by it as incremental or transformational.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Word-level exact-match precision, recall, and F1 scores as defined in Dunn et al.; trained on annotated examples (413 sentences in one setting) and tested on held-out sentences (e.g., 77 sentence test set for host-dopant).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Supervised training on human-annotated data and evaluation against test sets with the exact-word matching scoring procedure; direct numerical comparison to Eunomia on the same benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>Presented in the literature as a supervised fine-tuned baseline; not novel within this paper—used for benchmarking.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td>Reported baseline metrics in this paper: Case Study 1 hosts F1 = 0.883, dopants F1 = 0.821; Case Study 2 MOF formula F1 = 0.424; guest species F1 = 0.606.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td>LLM-NERRE is compared numerically to Eunomia; Eunomia (especially with CoV) achieves higher F1 on host/dopant extraction and MOF formula extraction, while guest species extraction shows mixed results attributed to evaluation metric issues.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>As above: hosts F1 0.883; dopants F1 0.821; MOF formula F1 0.424; guest species F1 0.606 on the reported benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Requires manually labeled training data and fine-tuning (100–1000 examples), which is labor-intensive and less flexible; performance sensitive to training set size; evaluated using an exact-word matching metric that can misrepresent semantic correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Agent-based Learning of Materials Datasets from Scientific Literature', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1209.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1209.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoV (Eunomia)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Verification (Eunomia implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative verification module integrated into Eunomia that generates verification queries and independently re-evaluates the agent's preliminary responses against user-defined, domain-specific criteria to reduce hallucinations and improve extraction reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Chain-of-Verification (CoV) - Eunomia implementation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An implemented verification tool inspired by prior CoV methods that: (1) takes an agent's preliminary answers; (2) generates iterative, independent verification queries (user-customizable) tailored to the domain; (3) has the agent respond to these queries independently; and (4) produces a final verified response. It is applied to confirm logical connection between supporting sentence(s) and extracted property claims (e.g., water stability criteria).</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>LLM-output verification / materials NLP validation</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>Not a discovery engine per se, but a validation subsystem that detects and corrects mistaken extractions (e.g., distinguishing thermal stability from water stability) and thereby increases the trustworthiness of claims produced by Eunomia.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td>The CoV tool is presented as a validation mechanism; the paper does not ascribe 'incremental' or 'transformational' discovery labels to its outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Measured by delta in primary task metrics when CoV is enabled vs disabled. Specific reported improvements: Case Study 1 and Case Study 3 metrics show higher F1 and ternary accuracy when CoV is used. Confusion matrices are used to inspect error modes.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Iterative independent checks executed by the LLM and comparison to domain-defined criteria (e.g., explicit water-stability rules); human-in-the-loop review is enabled; the CoV process filters predictions that do not satisfy verification queries.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>Paper claims novelty in permitting user-customized, domain-informed verification queries (in contrast to prior CoV implementations that auto-generate queries), enabling more tailored fact-checking for materials tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td>Quantified improvements: e.g., Case Study 3 ternary accuracy improved from 0.86 (no CoV) to 0.91 (with CoV); yield improved from 82.70% to 86.20%. Case Study 1 F1 scores also improved with CoV (hosts from 0.760 to 0.905, dopants from 0.822 to 0.920 in the Eunomia comparisons reported).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td>Compared to Eunomia without CoV: CoV reduces hallucinations and incorrect assignments (e.g., mislabeling thermal stability as water stability) and yields measurable metric improvements (see numbers above). The paper notes occasional skipping of CoV by the agent in some runs.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Effectiveness demonstrated by metric improvements summarized above; CoV successfully filtered some incorrect predictions and improved accuracy/yield in the water-stability case.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Difficult to define chemistry-specific verification rules for some tasks (authors omitted CoV on Case Study 2 for this reason); CoV was sometimes skipped by the agent for unknown reasons; extra computational/query cost; limited to checks that can be encoded as verification queries.</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Agent-based Learning of Materials Datasets from Scientific Literature', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1209.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1209.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large pre-trained foundation model (used as the LLM core of Eunomia) applied in zero-shot mode (temperature = 0) to enable planning, tool use, and multi-step reasoning in the agent framework.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A large autoregressive transformer-based language model provided by OpenAI used as the 'brain' of Eunomia, responsible for reasoning, planning (via ReAct prompts), and executing tool calls; used with deterministic sampling (temperature 0) in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>General LLM / applied to materials NLP in this work</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>Enables Eunomia to perform zero-shot information extraction across complex NLP tasks (NER, co-reference resolution, relation extraction, argument mining) by interacting with retrieval and verification tools; the model itself is not reported to autonomously 'discover' new materials but enables extraction that supports downstream discovery workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td>The paper does not classify outputs produced by GPT-4 (within Eunomia) as incremental or transformational discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>GPT-4's contribution is evaluated indirectly through the agent's downstream performance metrics (precision, recall, F1, ternary accuracy, yield) when used as the core LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Validated via Eunomia's benchmark comparisons to human-labeled ground truth and to fine-tuned baselines; authors report ablations (with/without CoV) but do not report alternative LLM backbones in main results (noted as a hyperparameter).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>Use of GPT-4 in a tool-augmented agent is consistent with contemporary LLM-agent practices; novelty in this paper is the overall agent/tool integration rather than GPT-4 itself.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td>Indirect: contributed to the numeric results of Eunomia (see Eunomia's reported F1, accuracy, yield figures).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td>No direct comparison of GPT-4-generated 'discoveries' to human discoveries is presented; comparisons are made at the task-performance level versus fine-tuned models and human-labeled ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>As the LLM core, GPT-4 is implicated in Eunomia's success rates (see Eunomia success_rate field); no separate standalone success metrics for GPT-4 are reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Intrinsic hallucination risk; cost of use at scale; sensitivity to prompts (authors suggest future prompt-sensitivity analysis); deterministic setting (temperature=0) used to reduce variability.</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Agent-based Learning of Materials Datasets from Scientific Literature', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Structured information extraction from complex scientific text with fine-tuned large language models <em>(Rating: 2)</em></li>
                <li>Chain-of-verification reduces hallucination in large language models <em>(Rating: 2)</em></li>
                <li>Paperqa: Retrieval-augmented generative agent for scientific research <em>(Rating: 2)</em></li>
                <li>Emergent autonomous scientific research capabilities of large language models <em>(Rating: 2)</em></li>
                <li>Chatgpt chemistry assistant for text mining and prediction of mof synthesis <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1209",
    "paper_id": "paper-266362372",
    "extraction_schema_id": "extraction-schema-26",
    "extracted_data": [
        {
            "name_short": "Eunomia",
            "name_full": "Eunomia (chemist AI agent)",
            "brief_description": "A zero-shot, LLM-based autonomous chemist agent introduced in this paper that extracts structured materials datasets from unstructured scientific literature by planning and using domain-specific tools (Doc Search, Chain-of-Verification, Dataset Search, CSV Generator).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Eunomia",
            "system_description": "An autonomous, tool-augmented AI agent with an LLM (GPT-4, temperature 0) as its core, using the ReAct planning architecture and LangChain for orchestration. It leverages document retrieval (Doc Search with text-ada-002 embeddings + FAISS + MMR), a user-customizable Chain-of-Verification (CoV) module, dataset lookup tools, and CSV/JSON output generation to perform planning, multi-step reasoning, information extraction, and iterative self-checking over sentences, paragraphs, and full research articles.",
            "discovery_domain": "Materials science / materials data extraction (materials NLP)",
            "discovery_description": "Eunomia autonomously reads scientific text (sentences, paragraphs, full papers) and produces structured datasets for downstream materials discovery tasks. Demonstrated tasks include: (1) extracting host-to-many dopant relations from single sentences; (2) extracting MOF chemical formulas and guest species from multi-sentence paragraphs; (3) identifying MOFs in full articles and classifying/predicting their water stability while providing exact in-context supporting sentences.",
            "discovery_type": "",
            "discovery_type_justification": "The paper does not label Eunomia's outputs as 'incremental' or 'transformational' or use other discovery-category terminology; it frames Eunomia as a method that simplifies and accelerates dataset creation and achieves parity or superiority with fine-tuned models, but does not classify the resulting scientific discoveries themselves.",
            "evaluation_methods": "Quantitative NLP metrics applied to the extraction tasks: precision, recall, and F1 (word-level exact-match as defined in Dunn et al.) for Case Studies 1 and 2; ternary accuracy and confusion matrices for Case Study 3 (Stable / Unstable / Not provided). Yield (fraction of ground-truth MOFs recovered) is reported for Case Study 3. Comparisons are made against a fine-tuned LLM baseline (LLM-NERRE). Embedding choices, retrieval parameters (k, MMR λ=0.5), and the use/absence of CoV are explicitly evaluated. Hand-labeled ground truth by expert chemists is used as the evaluation reference for the water-stability case study.",
            "validation_approaches": "Validation against human-curated ground-truth datasets (e.g., three expert chemists labeled 101 articles containing 371 MOFs for water stability); direct benchmark comparison to a fine-tuned LLM method (LLM-NERRE/Dunn et al.); internal verification using the Chain-of-Verification module that re-checks extracted claims against user-specified chemistry-informed criteria; reporting of confusion matrices and yields; human-in-the-loop review of outputs (encouraged by the authors).",
            "novelty_assessment": "Novelty is judged by the paper on two axes: (1) methodological: integration of an LLM agent with domain-specific tools (Doc Search, CoV) to enable zero-shot extraction without fine-tuning; (2) empirical: matching or exceeding performance of fine-tuned approaches on benchmark extraction tasks. The authors argue the zero-shot, tool-augmented agent workflow is novel relative to labor-intensive fine-tuning pipelines.",
            "impact_metrics": "Reported numeric outcomes used to quantify impact include: Case Study 1 (hosts/dopants): Eunomia + CoV hosts F1 = 0.905 vs LLM-NERRE hosts F1 = 0.883; Eunomia + CoV dopants F1 = 0.920 vs LLM-NERRE dopants F1 = 0.821. Case Study 2 (MOF formula): Eunomia F1 = 0.606 vs LLM-NERRE F1 = 0.424; (guest species) Eunomia recall = 0.923 precision = 0.429 (F1 = 0.585) vs LLM-NERRE F1 = 0.606. Case Study 3 (water stability): ternary accuracy with CoV = 0.91 and yield = 86.20% (without CoV accuracy = 0.86, yield = 82.70%). These metrics are used as the primary measures of efficacy.",
            "comparison_to_human_discoveries": false,
            "comparison_details": "The paper compares Eunomia's extraction performance against a fine-tuned model (LLM-NERRE) rather than to human 'discoveries'. For benchmark tasks, Eunomia+CoV outperforms LLM-NERRE on host/dopant F1 and MOF formula extraction; guest species extraction results are mixed (high recall but lower precision for Eunomia, attributed to exact-word matching evaluation). For water-stability extraction, Eunomia's outputs are validated against human-curated labels (three expert chemists) and show high ternary accuracy and yield. Exact numeric comparisons are reported in Tables 2 and 3 and in the text.",
            "success_rate": "Success measured per task: Case Study 1 (host/dopant): Eunomia + CoV F1 scores: hosts 0.905, dopants 0.920. Case Study 2: MOF formula F1 = 0.606; guest species F1 = 0.585 (recall 0.923, precision 0.429). Case Study 3 (MOF water stability): ternary accuracy 0.91 with CoV and yield 86.20% (without CoV: accuracy 0.86, yield 82.70%). The agent is described as 'cautious' and often outputs 'Not provided' rather than making uncertain predictions, which affects recall/precision trade-offs.",
            "challenges_limitations": "Key limitations identified: (1) hallucination risk from LLMs (mitigated but not eliminated by CoV); (2) exact-word matching evaluation metric (from Dunn et al.) penalizes multi-word and semantically-correct but lexically-different predictions, underestimating true performance; (3) difficulty in formalizing chemistry-informed CoV rules for some tasks (authors omitted CoV for Case Study 2 for this reason); (4) occasional skipping of CoV by the agent; (5) computational/token cost of LLM queries; (6) retrieval/repeat-reference behavior (agent sometimes repeatedly refers back to documents); (7) dependence on embedding quality and retrieval hyperparameters.",
            "has_incremental_transformational_comparison": false,
            "uuid": "e1209.0",
            "source_info": {
                "paper_title": "Agent-based Learning of Materials Datasets from Scientific Literature",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "LLM-NERRE",
            "name_full": "LLM-NERRE (fine-tuned LLM named-entity & relation extraction)",
            "brief_description": "A fine-tuned LLM approach (reported by Dunn et al.) that uses supervised fine-tuning on 100–1000 manually annotated examples to perform named entity recognition and relation extraction in materials science text, used here as a benchmark baseline.",
            "citation_title": "Structured information extraction from complex scientific text with fine-tuned large language models",
            "mention_or_use": "mention",
            "system_name": "LLM-NERRE",
            "system_description": "A fine-tuned large language model (GPT-3 in the referenced work) trained on hundreds of manually annotated examples to perform NER and relation extraction tailored to materials science text; evaluated using exact-word (word-level) matching metrics as described in Dunn et al.",
            "discovery_domain": "Materials information extraction / NLP",
            "discovery_description": "Used as a supervised baseline for extracting entities and relations (host-dopant, MOF formula, guest species) from materials science text. In the referenced work the model is trained on labeled examples and then applied to extraction tasks.",
            "discovery_type": "",
            "discovery_type_justification": "The paper treats LLM-NERRE as a baseline method and does not characterize any discoveries made by it as incremental or transformational.",
            "evaluation_methods": "Word-level exact-match precision, recall, and F1 scores as defined in Dunn et al.; trained on annotated examples (413 sentences in one setting) and tested on held-out sentences (e.g., 77 sentence test set for host-dopant).",
            "validation_approaches": "Supervised training on human-annotated data and evaluation against test sets with the exact-word matching scoring procedure; direct numerical comparison to Eunomia on the same benchmarks.",
            "novelty_assessment": "Presented in the literature as a supervised fine-tuned baseline; not novel within this paper—used for benchmarking.",
            "impact_metrics": "Reported baseline metrics in this paper: Case Study 1 hosts F1 = 0.883, dopants F1 = 0.821; Case Study 2 MOF formula F1 = 0.424; guest species F1 = 0.606.",
            "comparison_to_human_discoveries": false,
            "comparison_details": "LLM-NERRE is compared numerically to Eunomia; Eunomia (especially with CoV) achieves higher F1 on host/dopant extraction and MOF formula extraction, while guest species extraction shows mixed results attributed to evaluation metric issues.",
            "success_rate": "As above: hosts F1 0.883; dopants F1 0.821; MOF formula F1 0.424; guest species F1 0.606 on the reported benchmarks.",
            "challenges_limitations": "Requires manually labeled training data and fine-tuning (100–1000 examples), which is labor-intensive and less flexible; performance sensitive to training set size; evaluated using an exact-word matching metric that can misrepresent semantic correctness.",
            "has_incremental_transformational_comparison": false,
            "uuid": "e1209.1",
            "source_info": {
                "paper_title": "Agent-based Learning of Materials Datasets from Scientific Literature",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "CoV (Eunomia)",
            "name_full": "Chain-of-Verification (Eunomia implementation)",
            "brief_description": "An iterative verification module integrated into Eunomia that generates verification queries and independently re-evaluates the agent's preliminary responses against user-defined, domain-specific criteria to reduce hallucinations and improve extraction reliability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Chain-of-Verification (CoV) - Eunomia implementation",
            "system_description": "An implemented verification tool inspired by prior CoV methods that: (1) takes an agent's preliminary answers; (2) generates iterative, independent verification queries (user-customizable) tailored to the domain; (3) has the agent respond to these queries independently; and (4) produces a final verified response. It is applied to confirm logical connection between supporting sentence(s) and extracted property claims (e.g., water stability criteria).",
            "discovery_domain": "LLM-output verification / materials NLP validation",
            "discovery_description": "Not a discovery engine per se, but a validation subsystem that detects and corrects mistaken extractions (e.g., distinguishing thermal stability from water stability) and thereby increases the trustworthiness of claims produced by Eunomia.",
            "discovery_type": "",
            "discovery_type_justification": "The CoV tool is presented as a validation mechanism; the paper does not ascribe 'incremental' or 'transformational' discovery labels to its outcomes.",
            "evaluation_methods": "Measured by delta in primary task metrics when CoV is enabled vs disabled. Specific reported improvements: Case Study 1 and Case Study 3 metrics show higher F1 and ternary accuracy when CoV is used. Confusion matrices are used to inspect error modes.",
            "validation_approaches": "Iterative independent checks executed by the LLM and comparison to domain-defined criteria (e.g., explicit water-stability rules); human-in-the-loop review is enabled; the CoV process filters predictions that do not satisfy verification queries.",
            "novelty_assessment": "Paper claims novelty in permitting user-customized, domain-informed verification queries (in contrast to prior CoV implementations that auto-generate queries), enabling more tailored fact-checking for materials tasks.",
            "impact_metrics": "Quantified improvements: e.g., Case Study 3 ternary accuracy improved from 0.86 (no CoV) to 0.91 (with CoV); yield improved from 82.70% to 86.20%. Case Study 1 F1 scores also improved with CoV (hosts from 0.760 to 0.905, dopants from 0.822 to 0.920 in the Eunomia comparisons reported).",
            "comparison_to_human_discoveries": false,
            "comparison_details": "Compared to Eunomia without CoV: CoV reduces hallucinations and incorrect assignments (e.g., mislabeling thermal stability as water stability) and yields measurable metric improvements (see numbers above). The paper notes occasional skipping of CoV by the agent in some runs.",
            "success_rate": "Effectiveness demonstrated by metric improvements summarized above; CoV successfully filtered some incorrect predictions and improved accuracy/yield in the water-stability case.",
            "challenges_limitations": "Difficult to define chemistry-specific verification rules for some tasks (authors omitted CoV on Case Study 2 for this reason); CoV was sometimes skipped by the agent for unknown reasons; extra computational/query cost; limited to checks that can be encoded as verification queries.",
            "has_incremental_transformational_comparison": false,
            "uuid": "e1209.2",
            "source_info": {
                "paper_title": "Agent-based Learning of Materials Datasets from Scientific Literature",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "A large pre-trained foundation model (used as the LLM core of Eunomia) applied in zero-shot mode (temperature = 0) to enable planning, tool use, and multi-step reasoning in the agent framework.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "GPT-4",
            "system_description": "A large autoregressive transformer-based language model provided by OpenAI used as the 'brain' of Eunomia, responsible for reasoning, planning (via ReAct prompts), and executing tool calls; used with deterministic sampling (temperature 0) in experiments.",
            "discovery_domain": "General LLM / applied to materials NLP in this work",
            "discovery_description": "Enables Eunomia to perform zero-shot information extraction across complex NLP tasks (NER, co-reference resolution, relation extraction, argument mining) by interacting with retrieval and verification tools; the model itself is not reported to autonomously 'discover' new materials but enables extraction that supports downstream discovery workflows.",
            "discovery_type": "",
            "discovery_type_justification": "The paper does not classify outputs produced by GPT-4 (within Eunomia) as incremental or transformational discoveries.",
            "evaluation_methods": "GPT-4's contribution is evaluated indirectly through the agent's downstream performance metrics (precision, recall, F1, ternary accuracy, yield) when used as the core LLM.",
            "validation_approaches": "Validated via Eunomia's benchmark comparisons to human-labeled ground truth and to fine-tuned baselines; authors report ablations (with/without CoV) but do not report alternative LLM backbones in main results (noted as a hyperparameter).",
            "novelty_assessment": "Use of GPT-4 in a tool-augmented agent is consistent with contemporary LLM-agent practices; novelty in this paper is the overall agent/tool integration rather than GPT-4 itself.",
            "impact_metrics": "Indirect: contributed to the numeric results of Eunomia (see Eunomia's reported F1, accuracy, yield figures).",
            "comparison_to_human_discoveries": false,
            "comparison_details": "No direct comparison of GPT-4-generated 'discoveries' to human discoveries is presented; comparisons are made at the task-performance level versus fine-tuned models and human-labeled ground truth.",
            "success_rate": "As the LLM core, GPT-4 is implicated in Eunomia's success rates (see Eunomia success_rate field); no separate standalone success metrics for GPT-4 are reported in the paper.",
            "challenges_limitations": "Intrinsic hallucination risk; cost of use at scale; sensitivity to prompts (authors suggest future prompt-sensitivity analysis); deterministic setting (temperature=0) used to reduce variability.",
            "has_incremental_transformational_comparison": false,
            "uuid": "e1209.3",
            "source_info": {
                "paper_title": "Agent-based Learning of Materials Datasets from Scientific Literature",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Structured information extraction from complex scientific text with fine-tuned large language models",
            "rating": 2,
            "sanitized_title": "structured_information_extraction_from_complex_scientific_text_with_finetuned_large_language_models"
        },
        {
            "paper_title": "Chain-of-verification reduces hallucination in large language models",
            "rating": 2,
            "sanitized_title": "chainofverification_reduces_hallucination_in_large_language_models"
        },
        {
            "paper_title": "Paperqa: Retrieval-augmented generative agent for scientific research",
            "rating": 2,
            "sanitized_title": "paperqa_retrievalaugmented_generative_agent_for_scientific_research"
        },
        {
            "paper_title": "Emergent autonomous scientific research capabilities of large language models",
            "rating": 2,
            "sanitized_title": "emergent_autonomous_scientific_research_capabilities_of_large_language_models"
        },
        {
            "paper_title": "Chatgpt chemistry assistant for text mining and prediction of mof synthesis",
            "rating": 1,
            "sanitized_title": "chatgpt_chemistry_assistant_for_text_mining_and_prediction_of_mof_synthesis"
        }
    ],
    "cost": 0.017225749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>AGENT-BASED LEARNING OF MATERIALS DATASETS FROM SCIENTIFIC LITERATURE
18 Dec 2023</p>
<p>Mehrad Ansari 
Acceleration Consortium
University of Toronto
M5S 3E5TorontoOntarioCanada</p>
<p>Department of Chemical Engineering &amp; Applied Chemistry
University of Toronto
M5S 3E5TorontoOntarioCanada</p>
<p>Seyed Mohamad Moosavi 
Acceleration Consortium
University of Toronto
M5S 3E5TorontoOntarioCanada</p>
<p>Department of Chemical Engineering &amp; Applied Chemistry
University of Toronto
M5S 3E5TorontoOntarioCanada</p>
<p>AGENT-BASED LEARNING OF MATERIALS DATASETS FROM SCIENTIFIC LITERATURE
18 Dec 2023DEC54AC97D66E06D1CA4A68DC3EC19B4arXiv:2312.11690v1[cs.AI]
Advancements in machine learning and artificial intelligence are transforming materials discovery.Yet, the availability of structured experimental data remains a bottleneck.The vast corpus of scientific literature presents a valuable and rich resource of such data.However, manual dataset creation from these resources is challenging due to issues in maintaining quality and consistency, scalability limitations, and the risk of human error and bias.Therefore, in this work, we develop a chemist AI agent, powered by large language models (LLMs), to overcome these challenges by autonomously creating structured datasets from natural language text, ranging from sentences and paragraphs to extensive scientific research articles.Our chemist AI agent, Eunomia, can plan and execute actions by leveraging the existing knowledge from decades of scientific research articles, scientists, the Internet and other tools altogether.We benchmark the performance of our approach in three different information extraction tasks with various levels of complexity, including solid-state impurity doping, metal-organic framework (MOF) chemical formula, and property relations.Our results demonstrate that our zero-shot agent, with the appropriate tools, is capable of attaining performance that is either superior or comparable to the state-of-the-art fine-tuned materials information extraction methods.This approach simplifies compilation of machine learning-ready datasets for various materials discovery applications, and significantly ease the accessibility of advanced natural language processing tools for novice users in natural language.The methodology in this work is developed as an open-source software on https://github.com/AI4ChemS/Eunomia.</p>
<p>Introduction</p>
<p>The past decade's extraordinary achievements in leveraging machine learning for chemical discovery highlight the power of accessible knowledge and structured data [1][2][3].However, a significant portion of chemical knowledge, particularly the experimental ones, is scattered across scientific literature in an unstructured format [4].Researchers face challenges in effectively utilizing existing knowledge for design of experiments, as well as in comprehending the entirety of previous works in a field.Thus, the development of methodologies to extract information from the literature and convert it into structured data will play a fundamental role in advancing the machine learning for molecules and materials.</p>
<p>Natural Language Processing (NLP) is a powerful tool for extracting information from scientific literature.Conventional NLP methods have been used in materials and chemical sciences [5][6][7][8][9][10] for Named Entity Recognition.However, these methods are limited in other NLP tasks that are needed for a general-purpose data extraction tool, including Co-reference Resolution, Relation Extraction, Template Filling, Argument Mining, and Entity Linking.To better understand these NLP terminologies, let us consider an example taken from an abstract of a materials paper [11] in the field of metal-organic frameworks (MOFs):</p>
<p>An isoreticular series of cobalt-adeninate bio-MOFs (bio -MOFs-11-14) is reported.The pores of bio -MOFs-11-14 are decorated with acetate, propionate, butyrate, and valerate, respectively.The nitrogen (N 2 ) and carbon dioxide (CO 2 ) adsorption properties of these materials are studied and compared.The isosteric heats of adsorption for CO 2 are calculated, and the CO 2 : N 2 selectivities for each material are determined.As the lengths of the aliphatic chains decorating the pores in bio -MOFs-11-14 increase, the BET surface areas decrease from 1148 m 2 g -1 to 17 m 2 g -1 while the CO 2 : N 2 selectivities predicted from ideal adsorbed solution theory at 1 bar and 273 K for a 10 : 90 CO 2 : N 2 mixture range from 73 : 1 for bio -MOF-11 to 123 : 1 for bio -MOF-12 and finally to 107 : 1 for bio -MOF-13.At 298 K, the selectivities are 43 : 1 for bio -MOF-11, 52 : 1 for bio -MOF-12, and 40 : 1 for bio -MOF-13.The water stability of bio -MOFs-11-14 increases with increasing aliphatic chain length.</p>
<p>• Named Entity Recognition involves identifying and classifying the specific entities within the text into predefined categories (i.e., chemical compounds: "bio-MOFs-11-14", "acetate", experimental conditions: "1 bar", "273 K", "10 : 90 CO2 : N2 mixture").</p>
<p>• Co-reference Resolution focuses on finding all expressions that refer to the same entity in the text.As an example, phrases like "these materials", "each material" are references that relate back to the bio-MOFs-11-14 mentioned in the first sentence.</p>
<p>• Relation Extraction involves extracting semantic relationships from the text, which usually occur between two or more entities (i.e., the impact of "aliphatic chain lengths" on "BET surface areas" and "CO2 : N2 selectivities").</p>
<p>• Template Filing is an efficient approach to extract and structure complex information from text.As an example: material name: bio-MOFs-11-14.</p>
<p>• Argument Mining focuses on the automatic identification and extracts the reasoning presented within the text.As an example, the "increase in the water stability" of the mentioned MOFs are connected to the "increasing length of the aliphatic chains".</p>
<p>• Entity Linking takes one step further than named entity recognition and distinguishes between similarly named entities (i.e., the term "bio-MOFs" would be linked to databases or literature that describe these materials in detail.</p>
<p>The emergence of Large Language Models (LLMs) or foundation models, shows a great promise in tackling these complex NLP tasks [7,[12][13][14].Huang and Cole [15] fine-tuned a language model (BERT) on battery publications to extract device-level information from a paragraph that contains one device only.Dunn et al. [7] showed that fine-tuned LLMs using 100-1000 data points can perform Relation Extraction as well as template filling, enabling conversion of the extracted information into user-defined output formats.Despite these promising results, these methods require training data, limiting their ease of use and broad applicability.Moreover, LLM based approaches have not been explored for more intricate challenges, such as argument mining and co-reference resolution.These tasks are critical for practically using NLP for automated database development.For example, in one article, multiple materials might be discussed and authors use abbreviations like "compound 1" or simply "1" in the entire research manuscript for referencing after initially defining the chemical compound in the introduction section.Additionally, description of material properties often come with various interpretations, limiting using rigid name entity matching.As implementations of standalone LLMs fall short in addressing these intricate tasks, new methods are needed to enable reliable information extraction.An effective approach is to augment LLMs with domain-specific toolkits.These specialized tools offer precise answers, thus addressing the inherent limitations of LLMs in specific domains, and enhancing their overall performance and applicability [16][17][18][19].</p>
<p>In this work, we introduce an autonomous AI agent, Eunomia, augmented with chemistry-informed tools, designed to extract materials science-relevant information from unstructured text and convert it into structured databases.With an LLM at its core, our AI agent is capable of strategizing and executing actions by tapping into a wealth of knowledge from academic publications, domain-specific experts, the Internet, and other user-defined resources (see Figure 1).We show that this method streamlines data extraction, achieving remarkable accuracy and performance solely with a pre-trained LLM (GPT-4 [20]), eliminating the need for fine-tuning.It offers adaptability by accommodating a variety of information extraction tasks through natural language text prompts for new output schemas and reducing the risk of hallucinations through a chain-of-verification process.This capability extends beyond what a standalone LLM can offer.</p>
<p>Eunomia simplifies the development of tailored datasets from literature for domain experts, eliminating the need for extensive programming, NLP, or machine learning expertise.</p>
<p>This manuscript is organized as follows; Benchmarking and evaluating the model performance on three different materials NLP tasks with varying level of complexity are represented in Section 3.This is followed by Section 4, with a discussion on the implications of our findings, the advantages and limitations of our approach, as well as suggested directions for future work.Finally, in Section 5, we describe our methodology on agent's toolkits and evaluation metrics.</p>
<p>Figure 1: Agent-based learning framework overview.The AI agent equipped with various tools (online dataset search, document search, etc.) is tasked to extract information.The example shows the task of identifying all MOFs from a given research article, and predicting their property (e.g.water stability) by providing the reasoning for its decision.This reasoning is the exact in-context sentence from the paper, which is autonomously re-evaluated via the chain-of-verification tool of the agent to ensure its actual logical connection to the water stability property and reduce likelihood of hallucinations.The agent outputs a customized dataset that can be used to develop supervised or unsupervised machine learning methods.</p>
<p>AI Agent</p>
<p>In the realm of artificial intelligence, an "agent" is an autonomous entity capable of taking action based on its environment.In this work, we developed a chemist AI agent, Eunomia, to autonomously extract information from scientific literature (Figure 1).We use an LLM to serve as the brain of our agent [21].The LLM is equipped with advanced capabilities like planning and tool use to act beyond just a text generator, and act as a comprehensive problem solver, enabling effective interactions with the environment.We use ReAct architecture [22] for planning, enabling both reasoning and action.Our agent can interact with external sources like knowledge bases or environments to obtain more information.These knowledge bases are developed as toolkits (see method section for details) allowing the agent to extract relevant information from research articles, publicly available datasets, and built-in domain-specific chemical knowledge, ensuring its proficiency in playing the role of an expert chemist.We use OpenAI's GPT-4 [20] with a temperature of zero as our LLM and LangChain [23] for the application framework development (note the choice of LLM is only a hyperparameter and other LLMs can be also used with our agent).The application of LLMs in developing autonomous agents is a growing area of research [17,22,[24][25][26][27], with a detailed survey available in Ref. [28] for further insights.</p>
<p>In addition to the standard search and text manipulation tools, we have implemented a Chain-of-Verification (CoV) tool to enhance the robustness of our AI agent against hallucination.Hallucination in a LLM refers to the generation of content that strays from factual reality or includes fabricated information [29].In the CoV approach, the agent iteratively assesses its responses to ensure they remain logically consistent and coherent (see method section 5.1.2for details).This addition helps particularly with eliminating mistakenly extracted data related to semantically similar properties.An illustrative example is the case of stability of materials, where thermal, mechanical, and chemical stabilities might be confused by the agent.Figure 2 illustrates how CoV process works in action: the agent is tasked to identify MOFs and the corresponding water stability data in a paper.The agent initially misclassifies a thermally stable MOF as water-stable, but then it corrects this mistake by a comprehensive review using the CoV tool.This tool improves the performance of the agent and ensures robust data extraction.</p>
<p>Figure 2: Iterative Chain of Verification (CoV).The agent is tasked with reading a materials research article and predicting water stability of any mentioned MOFs by providing reasoning.In the initial run, the agent confuses water stability with thermal stability and mistakenly predicts the second MOF as water-stable.The CoV tool evaluates the agent's decisions in its precious step by validating the justification against the pre-defined water stability criteria and disregards this prediction.</p>
<p>Case studies</p>
<p>We evaluate the performance of our AI agent by benchmarking it across three different materials NLP tasks, with increasing task complexity (Table 1).In our assessment, we include a wide range of text lengths, including sentences, paragraphs and entire manuscript, as well as different NLP tasks discussed in the introduction.The first case study focuses on assessing the agent's performance on NLP tasks of lower complexity, specifically Named Entity Recognition and Relation Extraction.For this, we use our agent to extract host-to-many dopants relationships from a single sentence.The second case study, with medium NLP complexity, involves obtaining MOFs' chemical formula and their corresponding guest species from a paragraph with multiple sentences.Finally, the third case study centers on predicting a given property of MOFs based on the context coming from a materials research paper.The property of interest in our work is water stability.This case study aims to, in addition to Named Entity Recognition and Relation Extraction, evaluate the co-reference resolution and argument mining proficiency of our AI agent, tailored for chemists, which involves a high level of NLP complexity.In all case studies, our chemist AI agent, Eunomia, is a zero-shot learner that is equipped with the Doc Search tool (see Section 5.1.1).We have also conducted additional experiments by equipping Eunomia with the chain-of-verification (CoV) tool, as described in Section 5.1.2.This is referred to as Eunomia + CoV from here on.</p>
<p>To fairly compare the performance of our agent with the state-of-the-art fine-tuned LLM methods, the evaluation methodology for the first two case studies mirrors precisely that of Dunn et al. [7] (see Section 5.2 for details), serving as a benchmark reference.In this study, Dunn et al. [7] is referred to as LLM-NERRE, which involves fine-tuning a pre-trained LLM (GPT-3 [30]) on 100-1000 set of manually annotated examples, and then using the model to extract information from unstructured text.</p>
<p>Case Study 1: Host-to-many Dopants Relation</p>
<p>This case study aims to extract structured information about solid-state impurity doping from a single sentence.The objective is to identify the two entities "host" and "dopant"."Host" refers to the foundational crystal, sample, or category of material characterized by essential descriptors in its proximate context, such as "ZnO2 nanoparticles", "LiNbO3", or "half-Heuslers"."Dopant" means the elements or ions that represent a minority component, deliberately introduced impurities, or specific atomic-scale defects or carriers of electric charge like "hole-doped" or "S vacancies".</p>
<p>A single host can be combined with multiple dopants, through individual doping or simultaneous doping with different species, and one dopant can associate with various host materials.The text may contain numerous dopant-host pairings within the same sentence, and also instances of dopants and hosts that do not interact.</p>
<p>Eunomia shows an excellent performance in this task, demonstrating the effectiveness of our approach in Named Entity Recognition and Relation Extraction.Performance comparison between our chemist AI agent (Eunomia), and LLM-NERRE can be found in Table 2.In this setting, the same above definition of hosts and dopants are passed to Eunomia via the input prompt, while LLM-NERRE is fine-tuned on 413 sentences.The testing set contains 77 sentences.Notably, in both tasks, Eunomia + CoV exceeds the performance of LLM-NERRE in terms of the F1 score.This clearly demonstrates the effectiveness of our approach compared to fine-tuning, which can be labor-intensive and error-prone.We instruct Eunomia not to make up answers, which lead to a more cautious outcome, wherein uncertain or unclear inputs yield no output.As an example in the sentence "An anomalous behavior of the emission properties of alkali halides doped with heavy impurities, stimulated new efforts for its interpretation, invoking delicate and sophisticated mechanisms whose interest transcends the considered specific case.", the ground-truth host materials is "alkali halides".However, due to the nature of exact-word matching metric implemented in Ref. [7] a cautious agent with no predictions for the host entity will be penalized with two false negatives, one for each word in the ground-truth, leading to lower recall score.</p>
<p>Case Study 2: MOF Formula and Guest Species Relation</p>
<p>The goal of this case study is to identify MOF formula and guest species from unstructured text, as a paragraph with multiple sentences.The MOF formula refers to the chemical formula of a MOF, which is an important piece of information for characterizing and identifying MOFs.The guest species, on the other hand, are chemical species that have been incorporated, stored, or adsorbed in the MOF.These species are of interest because MOFs are often used for ion and gas separation, and the presence of specific guest species can affect the performance of the MOF.We limit our method to stand-alone Eunomia without CoV due to the complexity of defining a chemistry-informed CoV verification tool for this specific task.It should be noted that Dunn et al. [7] also included results on the identification of synthesis descriptions and applications pertaining to MOFs.However, as the metric of exact-word matching reported in Dunn et al. [7] does not fairly and adequately reflect the model performance for the multi-word (&gt; 2 words) nature of these outputs, we have limited our benchmarking to the MOF formula and guest species identification only.Table 3 shows the performance comparison between Eunomia and LLM-NERRE on the MOF formula and guest species relation extraction task.While Eunomia shows a superior performance on the MOF formula compared to LLM-NERRE, the relatively low performance of both approaches is related to the nature of the exact word matching.Using semantic similarity would be a more appropriate indicator in this context.On the guest species entity, while Eunomia shows a high recall (0.923), precision is relatively poor (0.429).This can be attributed to how the exact-word matching metrics have been defined in Ref. [7], where precision is majorly lowered by the presence of the extra unmatched predicted words (false positives), while recall remains high because all ground truth items were found in the predicted words.</p>
<p>Case Study 3: MOF Property Relation</p>
<p>This case study aims to mimic a practical scenario of developing datasets from scientific literature, where we evaluate the agent's performance on extracting MOF's water stability.To excel in this goal, the agent must identify all MOFs mentioned within the research paper, evaluate their water stability, and support these evaluations using exact sentences derived from the document.Such tasks are inherently linked to the NLP functions of Named Entity Recognition, Coreference Resolution, Relation Extraction, and Argument Mining.This is particularly a challenging task as researcher report the water stability in various ways, using phrases ranging from "the material remains crystalline in humid conditions" to "the MOF is stable in wide range of pH", or "the material is not soluble in water".</p>
<p>For this case study, we created a hand-labeled dataset based on a selection of 101 materials research papers, which contain a selection of 371 MOFs.Three expert chemists manually read through and review each paper, pinpointing the MOFs referenced within.A portion of these articles are selected considering the original work by Burtch et al. [31], where they developed a dataset of MOF names and their water stability by manually reading 111 research articles.To mimic the practical data extraction scenario, in which the agent is passed many articles, many of which do not contain the desired information, we included articles with no information about water stability.Each MOF in our set is assigned to one of the three classes of "Stable", "Unstable", and "Not provided".Figure 3.a presents the distribution of the classes within this dataset.</p>
<p>For this case study, we have established criteria to characterize water-stable MOFs, drawing from the study by Burtch et al. [31] and our own chemical intuition.A water-stable MOF should meet the following criteria:</p>
<p>• No alteration in properties after being subjected to moisture or steam, or when soaked or boiled in water or an aqueous solution.• Preservation of its porous architecture in liquid (water) environments.</p>
<p>• Sustained crystallinity without loss of structural integrity in aqueous environments.</p>
<p>• Insoluble in water or aqueous solutions.</p>
<p>• Exhibiting a pronounced rise in its water adsorption isotherm.</p>
<p>These water stability guidelines are defined as rules to Eunomia within the input prompt, as well as in its equipped CoV tool.</p>
<p>Eunomia with CoV tool retrieve most (yield of 86.20%) of the reported MOFs and shows an excellent performance (accuracy of 0.91) in inferring their water stability.This high yield and accuracy demonstrates the capability of our approach in extracting desired knowledge from the natural text.As expected, in the absence of CoV, there is a marginal decrease in accuracy to 0.86, along with a yield reduction to 82.70%.Taking into account the confusion matrix in Figure 3.b, it is evident that our agent adopts a cautious approach in its predictions.This is reflected in the substantial number of "Not provided" predictions which, upon comparison with the actual ground-truth class, indicates a propensity of the agent to acknowledge the insufficiency of information for making a definitive prediction, rather than mistakenly categorizing samples into the incorrect "Stable" or "Unstable" classes, and contaminating the agent's resulting dataset with unwanted noise.It is apparent that our agent exercises cautious in its judgments.Specifically, the abundance of "Not provided" predictions, when matched against their actual ground-truth categories, suggests that the agent prefers to concede some uncertainty in instances where making an accurate prediction is not feasible, rather than incorrectly assigning samples to the "Stable" or "Unstable" categories.The ternary accuracy is found to be 0.91 with a yield of 86.20%.</p>
<p>Discussion</p>
<p>We presented a high performing and robust method for extracting domain specific information from complex, unstructured text -ranging from sentences and paragraphs to extensive research articles -using AI agents.Scientists and researchers can use our open-source application to effortlessly develop tailored datasets for their specific areas and use them for downstream predictive tasks [32].While currently the cost of querying large datasets may become expensive, we expect the rapid advancements in LLMs will diminish this cost.</p>
<p>Unlike other methods that follow a pipeline-based or end-to-end approach, our agent-based method could appeal to domain experts due to its minimal demand for programming skills, NLP and machine learning knowledge.Users are not required to rigidly define an output schema or engage in the meticulous task of creating manual annotations for the purpose of fine-tuning.Rather, they can simply prompt the agent with more context and describe how their desired output should be formatted in natural language.Moreover, the agent can easily be extended and equipped with other tools (e.g.Dataset Search, CSV Generator, etc.) to be adapted to other problems.For example, we showed that, by equipping the agent with the chain-of-verification tool (CoV), we can minimize Hallucinations and improve the agent's performance.Similarly, by including reasoning tools, we can ask the agent to explain its reasoning based on the provided context to develop more transparent workflows for the LLM-based methods, and reduce their known "black-box" nature.This, simultaneously, offers a great opportunity for human-in-the-loop oversight, especially for tasks of critical importance.</p>
<p>Our results reveal an important observation: while large language models are few-shot learners [30], AI agents with appropriate tools and instructions are capable of being zero-shot learners.This brings an excellent opportunity to boost the performance of standalone LLMs across various domain-specific tasks without having to go through labor-intensive fine-tuning processes.A future thorough and systematic analysis of prompt sensitivity can provide valuable insights into this observation.</p>
<p>Methods</p>
<p>Agent Toolkits</p>
<p>Doc Search</p>
<p>This tool allows for extracting relevant knowledge materials properties from text, ranging from a single sentence and paragraph to a scientific research paper.The research papers are obtained from various chemistry journals including Royal Society of Chemistry (RSC), American Chemical Society (ACS), Elsevier, Inorganic Chemistry, Structural Chemistry, Coordination Chemistry, Wiley, and Crystallographic Communications as a PDF or in XML format (the XML files are obtained through a legal agreement between University of Toronto and ACS).Inspired by the paper-qa Python package (https://github.com/whitead/paper-qa),this tool aims at obtaining the most relevant context (sentences) from the papers to a given input query.This involves embedding the paper and queries into numerical vectors and identifying top k passages within the document that either mention or can somehow imply the property of interest for a MOF.k is set to 9 in our case studies, and is dynamically adjusted depending on the length of the paper to avoid OpenAI's token limitation error.We use OpenAI's text-ada-002 embeddings [33] to represent texts as high dimensional vectors, which are stored as a vector database using FAISS [34].Note that the choice of embedding is another hyperparameter that can be changed in future studies.For benchmarking purposes, we have also conducted all case studies with the newly released Cohere embed-english-v3.0 embeddings (see Supporting Information).</p>
<p>The semantic similarity search is ranked using Maximum Marginal Relevance (MMR) [35] based on cosine similarity, defined as,
MMR = arg max di∈R\S λ • cos(d i , q) − (1 − λ) • max dj ∈S cos(d i , d j )(1)
where d i is a document from the set of retrieved documents R, S is the set of already selected documents, q is the query.λ is a parameter between 0 and 1 that balances the trade-off between relevance (to the query) and diversity (or novelty with respect to already selected documents).In this work, we use the default value of 0.5.The idea behind MMR is to retrieve or select documents that are not just relevant to the query (or topic of interest), but are also diverse among themselves, thus minimizing redundancy.This tool provides the exact in-context sentence from the paper that provides some reasoning for the agent's choice, allowing for a more methodical evaluation of the agent's decision-making process, and reducing the likelihood of hallucinations or fabrications with the human-in-loop verification of the resulting datasets.It is important to note that in some unsuccessful experiments, we observed that the AI agent repeatedly referred back to the document, even after pinpointing the correct answer.Although this minor issue remained unresolved, we introduced an iteration limit for the agent to avoid unnecessary model running costs.</p>
<p>Chain-of-Verification</p>
<p>Inspired by the Chain-of-Verification (CoV) [36] methodology, this tool entails the following steps: initially, the agent provides a preliminary reply, which is followed by iterative verification queries to authenticate the initial draft.The agent independently responds to these queries to ensure the answers remain impartial and unaffected by other responses, and finally it produces its conclusive, verified response.Our implementation of CoV stands apart from the method described in Dhuliawala et al. [36], specifically in how the verification queries are generated.While in the Dhuliawala et al. [36]'s approach, the LLM produces task-specific queries, our method allows for user customization.This adaptability not only enables broader, more tailored domain-specific fact-checking across various tasks, but also opens up opportunities for human-in-the-loop verification, enhancing the accuracy and relevance of the results.This tool substantially boosts agent efficacy and mitigates the likelihood of hallucinations, especially in the events of completing complex tasks (see Figure 2 for more details).It is important to note that for unknown reasons, we have observed that the CoV tool usage was skipped by the agent on a few occasional instances.</p>
<p>Dataset Search</p>
<p>This tool allows for obtaining the chemical structure of MOFs from publically available datasets, including the Materials Projects [37], Crystallography Open Database (COD) [38][39][40][41][42][43][44][45], Cambridge Structural Database (CSD) [46], and QMOF [47,48].</p>
<p>CSV Generator</p>
<p>This tool stores the output of the agent into a CSV or JSON file.</p>
<p>Evaluation Metrics</p>
<p>Multiple metrics have been defined to assess agent's performance across different case studies.Precision, recall and F1 score are defined as, Precision = T P T P + F P ,</p>
<p>Recall = T P T P + F N ,
F1 Score = 2 × Precision × Recall Precision + Recall ,(3)
where, T P represents true positives, F P stands for false positives, and F N denotes false negatives.Precision measures the accuracy of the positive predictions, recall measures the fraction of actual positives that were correctly identified, and the F1 score is the harmonic mean of precision and recall.Binary classification accuracy is defined as,
Binary accuracy = T P + T N N ,(5)
In Case studies 1 and 2 (Sections 3.1 and 3.2), the evaluation metrics used are precisely those defined in the work of [7].In specific, they assessed named entity relationships on a word-to-word matching basis by initially decomposing an entity E into a collection of k words separated by whitespace, denoted as E = {w 1 , w 2 , w 3 , . . ., w k }.For evaluating entities in Named Entity Recognition exclusively, they enumerated the words that are identical in both the true entity set E true and the test entity set E test as true positives (E true ∩ E test ), and the distinct elements in each set as false positives (E test \ E true ) or false negatives (E true \ E test ).For instance, if the true entity is "Bi 2 Te 3 thin film" and the predicted entity is "Bi 2 Te 3 film sample", they noted two true positive matches ("Bi 2 Te 3 ", "film"), one false negative ("thin"), and one false positive ("sample").An exceptional case arises for formula-type entities critical to material identification, whereby E test must encompass all w i interpreted as stoichiometries to consider any w i ∈ E test as correct.For example, with "Bi 2 Te 3 thin film" as E true and "thin film" as E test , three false negatives would be registered.For more details on the scoring metrics and the case studies, readers are encouraged to refer to Ref. [7].</p>
<p>For our last case study in Section 3.3 (predicting water stability of MOFs), the ternary accuracy is defined as, Ternary accuracy = T P S + T P U + T P N P N ,</p>
<p>where N shows the total number of predictions and S, U , N P denote the three classes "Stable", "Unstable", and "Not provided", respectively.T P i shows then number of instances correctly predicted as class i.Additionally, we evaluate the information recovery capabilities of the agent by defining yield as
Yield = N N GT ,(7)
where N GT is the ground-truth number of MOFs mentioned in the research papers, regardless whether the paper discusses water stability or not.</p>
<p>Figure 3 :
3
Figure 3: Performance of the AI agent in information retrieval.a) Class distribution for water stability in the handlabeled ground-truth dataset of 371 MOFs based on 101 research articles.b) Confusion matrix for ternary classification of water stability property with CoV tool using OpenAI text-ada-002 embeddings.It is apparent that our agent exercises cautious in its judgments.Specifically, the abundance of "Not provided" predictions, when matched against their actual ground-truth categories, suggests that the agent prefers to concede some uncertainty in instances where making an accurate prediction is not feasible, rather than incorrectly assigning samples to the "Stable" or "Unstable" categories.The ternary accuracy is found to be 0.91 with a yield of 86.20%.</p>
<p>Table 1 :
1
Overview of the three case studies based on their context from which data is extracted, NLP tasks and complexity
Case StudyContextNLP TasksTask Complexity
1. Host-to-many Dopants Relation Sentence Named Entity Recognition, Relation Extraction 2. MOF Formula and Guest Species Relation Paragraph Named Entity Recognition, Relation Extraction 3. MOF Property Relation Research paper Named Entity Recognition, Relation Extraction, Template Filing, Argument Mining, Entity Linking</p>
<p>Table 2 :
2
Performance comparison between LLM-NERRE, Eunomia, and Eunomia + CoV on hosts and dopants relation extraction (Case Study 1).Eunomia embeddings are generated using OpenAI's text-ada-002.Best scores for each entity are highlighted in bold text.
ModelEntityPrecision (Exact Match) Recall (Exact Match) F1 Score (Exact Match)LLM-NERRE [7] hosts Eunomia hosts Eunomia + CoV hosts0.892 0.753 0.9640.874 0.768 0.8530.883 0.760 0.905LLM-NERRE [7] dopants Eunomia dopants Eunomia + CoV dopants0.831 0.859 0.9620.812 0.788 0.8820.821 0.822 0.920</p>
<p>Table 3 :
3ModelEntityPrecision (Exact Match) Recall (Exact Match) F1 Score (Exact Match)LLM-NERRE [7] mof formula Eunomia mof formula0.409 0.6230.455 0.5890.424 0.606LLM-NERRE [7] guest species Eunomia guest species0.588 0.4290.665 0.9230.606 0.585
Performance comparison between LLM-NERRE and Eunomia on MOF formula and guest species relation extraction (Case study 2).Eunomia embeddings are generated using OpenAI text-ada-002.Best scores for each entity are highlighted in bold text.</p>
<p>AcknowledgementsResearch reported in this work was supported by the Acceleration Consortium at the University of Toronto.SMM acknowledges the support by the Natural Sciences and Engineering Research Council of Canada (NSERC) under grant number RGPIN-2023-04232.The authors thank Alexander Dunn and Andrew S. Rosen for their assistance on the implementation of LLM-NERRE.The authors also thank Haoning Yuan for assisting in manual curation of the water stability dataset used in Section 3.3.Data and Code AvailabilityAll data (including the dataset used for case study 3) and code used to produce results in this study are publically available in the following GitHub repository: https://github.com/AI4ChemS/Eunomia.The methodology in this work is also developed as an open-source application on https://eunomia.streamlit.app.SUPPORTING INFORMATION FOR AGENT-BASED LEARNING OF MATERIALS DATASETS FROM SCIENTIFIC LITERATUREMehradIt is apparent that our agent exercises cautious in its judgments.Specifically, the abundance of "Not provided" predictions, when matched against their actual ground-truth categories, suggests that the agent prefers to concede some uncertainty in instances where making an accurate prediction is not feasible, rather than incorrectly assigning samples to the "Stable" or "Unstable" categories.The ternary accuracy is found to be 0.88 with a yield of 72.96%, which is lower than performance coming from OpenAI's text-ada-002 embeddings (see Figure4bin the main article).
The role of machine learning in the understanding and design of materials. Mohamad Seyed, Kevin Maik Moosavi, Berend Jablonka, Smit, Journal of the American Chemical Society. 142482020</p>
<p>Machine learning for molecular and materials science. Daniel W Keith T Butler, Hugh Davies, Olexandr Cartwright, Aron Isayev, Walsh, Nature. 55977152018</p>
<p>Inverse molecular design using machine learning: Generative models for matter engineering. Benjamin Sanchez, -Lengeling , Alán Aspuru-Guzik, Science. 36164002018</p>
<p>Quantifying the distribution of materials data types in scientific literature across text, tables, and figures. Wade Hasan M Sayeed, Sterling G Smallwood, Taylor D Baird, Sparks, 2023</p>
<p>Named entity recognition and normalization applied to large-scale information extraction from the materials science literature. Leigh Weston, Vahe Tshitoyan, John Dagdelen, Olga Kononova, Amalie Trewartha, Kristin A Persson, Gerbrand Ceder, Anubhav Jain, Journal of chemical information and modeling. 5992019</p>
<p>Chemdataextractor: a toolkit for automated extraction of chemical information from the scientific literature. C Matthew, Jacqueline M Swain, Cole, Journal of chemical information and modeling. 56102016</p>
<p>Structured information extraction from complex scientific text with fine-tuned large language models. Alexander Dunn, John Dagdelen, Nicholas Walker, Sanghoon Lee, Gerbrand Andrew S Rosen, Kristin Ceder, Anubhav Persson, Jain, arXiv:2212.052382022arXiv preprint</p>
<p>Using machine learning and data mining to leverage community knowledge for the engineering of stable metal-organic frameworks. Aditya Nandy, Chenru Duan, Heather J Kulik, Journal of the American Chemical Society. 143422021</p>
<p>Materials synthesis insights from scientific literature via text extraction and machine learning. Edward Kim, Kevin Huang, Adam Saunders, Andrew Mccallum, Gerbrand Ceder, Elsa Olivetti, Chemistry of Materials. 29212017</p>
<p>Digimof: A database of metal-organic framework synthesis information generated via text mining. Kristian Lawson T Glasby, Rosalee Gubsch, Rama Bence, Kesler Oktavian, Isoko, Mohamad Seyed, Joan L Moosavi, Jason C Cordiner, Peyman Z Cole, Moghadam, Chemistry of Materials. 2023</p>
<p>Systematic modulation and enhancement of co 2: N 2 selectivity and water stability in an isoreticular series of bio-mof-11 analogues. Tao Li, De-Li Chen, Jeanne E Sullivan, Mark T Kozlowski, Karl Johnson, Nathaniel L Rosi, Chemical Science. 442013</p>
<p>Extracting accurate materials data from research papers with conversational language models and prompt engineering. P Maciej, Dane Polak, Morgan, arxiv:2303.053522023arXiv preprint</p>
<p>Chatgpt chemistry assistant for text mining and prediction of mof synthesis. Zhiling Zheng, Oufan Zhang, Christian Borgs, Jennifer T Chayes, Omar M Yaghi, arXiv:2306.112962023arXiv preprint</p>
<p>Image and data mining in reticular chemistry using gpt-4v. Zhiling Zheng, Zhiguo He, Omar Khattab, Nakul Rampal, A Matei, Christian Zaharia, Jennifer T Borgs, Omar M Chayes, Yaghi, arXiv:2312.054682023arXiv preprint</p>
<p>Batterybert: A pretrained language model for battery database enhancement. Shu Huang, Jacqueline M Cole, Journal of Chemical Information and Modeling. 62242022</p>
<p>Assessment of chemistry knowledge in large language models that generate code. Glen M Andrew D White, Heta A Hocky, Mehrad Gandhi, Sam Ansari, Cox, Subarna Geemi P Wellawatte, Ziyue Sasmal, Kangxin Yang, Yuvraj Liu, Singh, Digital Discovery. 222023</p>
<p>Sam Andres M Bran, Andrew D Cox, Philippe White, Schwaller, arXiv:2304.05376Chemcrow: Augmenting large-language models with chemistry tools. 2023arXiv preprint</p>
<p>Emergent autonomous scientific research capabilities of large language models. Robert Daniil A Boiko, Gabe Macknight, Gomes, arXiv:2304.053322023arXiv preprint</p>
<p>Paperqa: Retrieval-augmented generative agent for scientific research. Jakub Lála, O' Odhran, Aleksandar Donoghue, Sam Shtedritski, Samuel G Cox, Andrew D Rodriques, White, arXiv:2312.075592023arXiv preprint</p>
<p>. OpenAI. Gpt-4 technical report. 2023</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022arXiv preprint</p>
<p>. Harrison Chase, Langchain, 10 2022</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, arXiv:2112.09332Browser-assisted question-answering with human feedback. 2021arXiv preprint</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, arXiv:2302.047612023arXiv preprint</p>
<p>Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Jianfeng Gao, arXiv:2304.09842Chameleon: Plug-and-play compositional reasoning with large language models. 2023arXiv preprint</p>
<p>Tool learning with foundation models. Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, arXiv:2304.083542023arXiv preprint</p>
<p>Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, arXiv:2309.07864The rise and potential of large language model based agents: A survey. 2023arXiv preprint</p>
<p>A survey of hallucination in large foundation models. Amit Vipula Rawte, Amitava Sheth, Das, arXiv:2309.059222023arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Water stability and adsorption in metal-organic frameworks. Himanshu Nicholas C Burtch, Krista S Jasuja, Walton, Chemical reviews. 114202014</p>
<p>Leveraging large language models for predictive chemistry. Kevin Maik, Jablonka , Philippe Schwaller, Andres Ortega-Guerrero, Berend Smit, 2023</p>
<p>Ryan Greene, Ted Sanders, Lilian Weng, Arvind Neelakantan, New and improved embedding model. 2022</p>
<p>Billion-scale similarity search with gpus. Jeff Johnson, Matthijs Douze, Hervé Jégou, IEEE Transactions on Big Data. 732019</p>
<p>The use of mmr, diversity-based reranking for reordering documents and producing summaries. Jaime Carbonell, Jade Goldstein, Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval. the 21st annual international ACM SIGIR conference on Research and development in information retrieval1998</p>
<p>Chain-of-verification reduces hallucination in large language models. Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, Jason Weston, arXiv:2309.114952023arXiv preprint</p>
<p>Commentary: The materials project: A materials genome approach to accelerating materials innovation. Anubhav Jain, Ping Shyue, Geoffroy Ong, Wei Hautier, William Davidson Chen, Stephen Richards, Shreyas Dacek, Dan Cholia, David Gunter, Gerbrand Skinner, Ceder, APL materials. 112013</p>
<p>Graph isomorphism-based algorithm for cross-checking chemical and crystallographic descriptions. Andrius Merkys, Antanas Vaitkus, Journal of cheminformatics. 151252023Algirdas Grybauskas, Aleksandras Konovalovas, Miguel Quirós, and Saulius Gražulis</p>
<p>Validation of the crystallography open database using the crystallographic information framework. Antanas Vaitkus, Andrius Merkys, Saulius Gražulis, Journal of applied crystallography. 5422021</p>
<p>Using smiles strings for the description of chemical connectivity in the crystallography open database. Miguel Quirós, Saulius Gražulis, Saulė Girdzijauskaitė, Journal of cheminformatics. 1012018Andrius Merkys, and Antanas Vaitkus</p>
<p>Cod:: Cif:: Parser: an error-correcting cif parser for the perl language. Andrius Merkys, Antanas Vaitkus, Justas Butkus, Mykolas Okulič-Kazarinas, Visvaldas Kairys, Saulius Gražulis, Journal of applied crystallography. 4912016</p>
<p>Antanas Vaitkus, and Mykolas Okulič-Kazarinas. Computing stoichiometric molecular composition from crystal structures. Saulius Gražulis, Andrius Merkys, Journal of applied crystallography. 4812015</p>
<p>Crystallography open database (cod): an open-access collection of crystal structures and platform for world-wide collaboration. Saulius Gražulis, Adriana Daškevič, Andrius Merkys, Daniel Chateigner, Luca Lutterotti, Miguel Quiros, Peter Nadezhda R Serebryanaya, Robert T Moeck, Armel Le Downs, Bail, Nucleic acids research. 40D12012</p>
<p>Crystallography open database-an open-access collection of crystal structures. Saulius Gražulis, Daniel Chateigner, Robert T Downs, Miguel Yokochi, Luca Quirós, Elena Lutterotti, Justas Manakova, Peter Butkus, Armel Le Moeck, Bail, Journal of applied crystallography. 4242009</p>
<p>The american mineralogist crystal structure database. T Robert, Michelle Downs, Hall-Wallace, American Mineralogist. 8812003</p>
<p>The cambridge structural database. Ian J Colin R Groom, Matthew P Bruno, Suzanna C Lightfoot, Ward, Acta Crystallographica Section B: Structural Science, Crystal Engineering and Materials. 7222016</p>
<p>Machine learning the quantum-chemical properties of metal-organic frameworks for accelerated materials discovery. Shaelyn M Andrew S Rosen, Debmalya Iyer, Zhenpeng Ray, Alan Yao, Laura Aspuru-Guzik, Justin M Gagliardi, Randall Q Notestein, Snurr, 2021Matter4</p>
<p>High-throughput predictions of metal-organic framework electronic properties: theoretical challenges, graph neural networks, and data exploration. Victor Andrew S Rosen, Patrick Fung, Huck, T O' Cody, Matthew K Donnell, Donald G Horton, Kristin A Truhlar, Justin M Persson, Randall Q Notestein, Snurr, Computational Materials. 811122022</p>            </div>
        </div>

    </div>
</body>
</html>
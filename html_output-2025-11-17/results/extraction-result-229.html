<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-229 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-229</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-229</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-13.html">extraction-schema-13</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use memory mechanisms to solve text-based games, including details about the memory architecture, the games played, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-1c8cab1cba88b270ddcc23f586aaac90b6741cb7</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1c8cab1cba88b270ddcc23f586aaac90b6741cb7" target="_blank">Situated language learning via interactive narratives</a></p>
                <p><strong>Paper Venue:</strong> Patterns</p>
                <p><strong>Paper TL;DR:</strong> This paper provides a roadmap that explores the question of how to imbue learning agents with the ability to understand and generate contextually relevant natural language in service of achieving a goal, and hypothesizes that two key components in creating such agents are interactivity and environment grounding.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e229.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e229.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use memory mechanisms to solve text-based games, including details about the memory architecture, the games played, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph-based DRL (Ammanabrolu & Riedl 2019)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Playing text-adventure games with graph-based deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deep reinforcement learning approach that constructs and uses graph-structured memory (maps) to represent locations, objects, and their relations in text-adventure games to mitigate partial observability and aid navigation and planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Playing text-adventure games with graph-based deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Graph-based deep RL agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A reinforcement learning agent that augments its policy with a learned graph representation of the game world (graph-based memory) to inform action selection; described as 'graph-based deep reinforcement learning'. The paper (referenced) pairs deep RL with an explicit graph/world representation.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>graph-based memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>An explicit graph/map structure encoding locations, connectivity, objects and relations (structured memory aid such as a map or knowledge graph).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Location nodes and connectivity, objects present at locations (including nested objects), and relations among entities encountered during play.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage</strong></td>
                            <td>Maintains a world-map/knowledge-graph during gameplay to detect previously seen locations, guide navigation, and inform action selection under partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark</strong></td>
                            <td>Text-adventure games / interactive fiction (e.g., Zork and other parser-based IF titles)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Classic parser-based interactive fiction games with textual observations, partial observability, many locations, nested objects, and puzzle-like progression requiring long-range dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>game_complexity</strong></td>
                            <td>High: many locations, nested objects, long-term dependencies; action space can be combinatorially large (paper cites e.g. parser vocabulary leading to ~697^5 possible 5-word actions as an example of text-game action complexity).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>Enables tracking of visited locations and objects, reduces getting lost under partial observability, supports long-range dependency reasoning required for puzzle bottlenecks, and constrains exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper (as cited) argues that constructing structured memory aids (maps/graphs) substantially helps automated agents operate in textual worlds by addressing textual-SLAM-style challenges: detecting revisited locations, tracking objects and nested containers, and supporting navigation and puzzle solving under partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Situated language learning via interactive narratives', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e229.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e229.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use memory mechanisms to solve text-based games, including details about the memory architecture, the games played, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dynamic Knowledge Graphs (Adhikari et al., 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning dynamic knowledge graphs to generalize on textbased games</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that builds and updates knowledge-graph-style memory dynamically during play so agents can generalize better across text-based game states and support decision-making under partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning dynamic knowledge graphs to generalize on textbased games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Dynamic knowledge-graph agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An agent that constructs and updates a dynamic knowledge graph (graph-structured memory) capturing entities and relations from textual observations, integrated with learning to improve generalization in text games.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>reinforcement learning / hybrid (graph-augmented learning)</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>dynamic knowledge graph</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>A knowledge graph updated online during play representing entities (locations, objects, characters) and relations (containment, connectivity, interactions).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Entities encountered, relations between entities (e.g., location connectivity, object containment/ownership), and evolving world-state facts extracted from observations.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage</strong></td>
                            <td>Maintains and updates a structured world-model to support generalization and inform policy decisions under partial observability; used to reason about environment dynamics and guide exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark</strong></td>
                            <td>Text-based games / interactive fiction (general text-game benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Parser-based interactive fiction with complex worlds, many entities, nested object structures, and long-term puzzles requiring persistent world knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>game_complexity</strong></td>
                            <td>High complexity due to many entities/relations and long-range dependencies; necessitates a persistent structured memory to capture world state.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>Provides a persistent, structured representation that aids in generalization across states and games and improves the agent's ability to reason about entities and their relations over long horizons.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The referenced work emphasizes that dynamic knowledge-graph memory helps agents generalize and better handle partial observability by explicitly modeling entities and relations encountered during play.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Situated language learning via interactive narratives', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e229.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e229.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use memory mechanisms to solve text-based games, including details about the memory architecture, the games played, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Commonsense-enhanced agents (Murugesan et al., 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Enhancing text-based reinforcement learning agents with commonsense knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Agents augmented with external commonsense knowledge (semantic memory/knowledge bases) to supply priors about object affordances, typical locations, and commonsense relations useful for text-game reasoning and action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Enhancing text-based reinforcement learning agents with commonsense knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Commonsense-augmented RL agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A text-game agent that augments its decision-making with external commonsense knowledge sources (semantic memory/knowledge base) to provide priors about objects, affordances, and likely relations helpful for exploration and planning.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>reinforcement learning (hybrid with external knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external knowledge base / semantic memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>External commonsense knowledge base or semantic memory (structured facts about objects, affordances, and relations) consulted during play.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Commonsense facts and priors such as typical object locations, affordances (what an object can be used for), and thematic knowledge relevant to genres.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage</strong></td>
                            <td>Augments perception/action selection by providing priors and constraints, helping the agent pick plausible actions and reducing exploration of improbable actions under partial information.</td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark</strong></td>
                            <td>Text-adventure games / interactive fiction</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Games that leverage thematic commonsense (e.g., cooking, fantasy) where prior knowledge about affordances and object roles helps solve quests and puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>game_complexity</strong></td>
                            <td>Complex due to need for thematic commonsense and long-range dependencies; success depends on applying appropriate priors in varying genres.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>Supplies thematic commonsense priors that can guide exploration, reduce nonsensical actions, and help satisfy latent dependencies required for puzzle progression.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The cited work indicates that incorporating commonsense knowledge as an external memory source can improve agents' ability to choose plausible actions and handle domain-specific priors necessary for solving text-game puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Situated language learning via interactive narratives', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e229.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e229.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use memory mechanisms to solve text-based games, including details about the memory architecture, the games played, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph-Constrained RL (Ammanabrolu & Hausknecht 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Constrained Reinforcement Learning for Natural Language Action Spaces</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reinforcement learning method that uses graph-structured constraints/memory to restrict and guide natural language action generation in text-adventure games, reducing combinatorial action complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph Constrained Reinforcement Learning for Natural Language Action Spaces</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Graph-constrained RL agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An RL agent that constrains the natural language action space using a graph-structured representation (memory) of the environment, thereby reducing combinatorial action generation and improving exploration efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>graph-based memory / constraint graph</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Graph structure encoding entities and allowable/compositional actions (constraints derived from a memory graph) used to prune or guide action generation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Entities (objects/locations), possible action templates or constraints tied to entities, and connectivity helping to limit action candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage</strong></td>
                            <td>Uses the graph to constrain or bias the policy's action generation toward contextually valid actions, reducing irrelevant action exploration in large language action spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark</strong></td>
                            <td>Text-adventure games / interactive fiction</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Parser-based games with combinatorially large natural language action spaces where constraining action candidates is necessary for tractable learning and exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>game_complexity</strong></td>
                            <td>Very high action-space complexity (paper cites combinatorial sizes like ~697^5 possible 5-word actions for an example parser vocabulary); requires action-space reduction strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>Reduces the effective action space by constraining generation to graph-consistent actions, improving exploration efficiency and making RL tractable in large natural-language action spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The referenced approach highlights that graph-structured memory/constraints can make natural language action selection tractable by pruning invalid action candidates and focusing exploration on contextually plausible commands.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Situated language learning via interactive narratives', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Playing text-adventure games with graph-based deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Learning dynamic knowledge graphs to generalize on textbased games <em>(Rating: 2)</em></li>
                <li>Enhancing text-based reinforcement learning agents with commonsense knowledge <em>(Rating: 2)</em></li>
                <li>Graph Constrained Reinforcement Learning for Natural Language Action Spaces <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-229",
    "paper_id": "paper-1c8cab1cba88b270ddcc23f586aaac90b6741cb7",
    "extraction_schema_id": "extraction-schema-13",
    "extracted_data": [
        {
            "name_short": "Graph-based DRL (Ammanabrolu & Riedl 2019)",
            "name_full": "Playing text-adventure games with graph-based deep reinforcement learning",
            "brief_description": "A deep reinforcement learning approach that constructs and uses graph-structured memory (maps) to represent locations, objects, and their relations in text-adventure games to mitigate partial observability and aid navigation and planning.",
            "citation_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "mention_or_use": "mention",
            "agent_name": "Graph-based deep RL agent",
            "agent_description": "A reinforcement learning agent that augments its policy with a learned graph representation of the game world (graph-based memory) to inform action selection; described as 'graph-based deep reinforcement learning'. The paper (referenced) pairs deep RL with an explicit graph/world representation.",
            "agent_type": "reinforcement learning",
            "uses_memory": true,
            "memory_type": "graph-based memory",
            "memory_structure": "An explicit graph/map structure encoding locations, connectivity, objects and relations (structured memory aid such as a map or knowledge graph).",
            "memory_content": "Location nodes and connectivity, objects present at locations (including nested objects), and relations among entities encountered during play.",
            "memory_usage": "Maintains a world-map/knowledge-graph during gameplay to detect previously seen locations, guide navigation, and inform action selection under partial observability.",
            "game_benchmark": "Text-adventure games / interactive fiction (e.g., Zork and other parser-based IF titles)",
            "game_description": "Classic parser-based interactive fiction games with textual observations, partial observability, many locations, nested objects, and puzzle-like progression requiring long-range dependencies.",
            "game_complexity": "High: many locations, nested objects, long-term dependencies; action space can be combinatorially large (paper cites e.g. parser vocabulary leading to ~697^5 possible 5-word actions as an example of text-game action complexity).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_benefits": "Enables tracking of visited locations and objects, reduces getting lost under partial observability, supports long-range dependency reasoning required for puzzle bottlenecks, and constrains exploration.",
            "memory_limitations": null,
            "key_findings": "The paper (as cited) argues that constructing structured memory aids (maps/graphs) substantially helps automated agents operate in textual worlds by addressing textual-SLAM-style challenges: detecting revisited locations, tracking objects and nested containers, and supporting navigation and puzzle solving under partial observability.",
            "uuid": "e229.0",
            "source_info": {
                "paper_title": "Situated language learning via interactive narratives",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "Dynamic Knowledge Graphs (Adhikari et al., 2020)",
            "name_full": "Learning dynamic knowledge graphs to generalize on textbased games",
            "brief_description": "An approach that builds and updates knowledge-graph-style memory dynamically during play so agents can generalize better across text-based game states and support decision-making under partial observability.",
            "citation_title": "Learning dynamic knowledge graphs to generalize on textbased games",
            "mention_or_use": "mention",
            "agent_name": "Dynamic knowledge-graph agent",
            "agent_description": "An agent that constructs and updates a dynamic knowledge graph (graph-structured memory) capturing entities and relations from textual observations, integrated with learning to improve generalization in text games.",
            "agent_type": "reinforcement learning / hybrid (graph-augmented learning)",
            "uses_memory": true,
            "memory_type": "dynamic knowledge graph",
            "memory_structure": "A knowledge graph updated online during play representing entities (locations, objects, characters) and relations (containment, connectivity, interactions).",
            "memory_content": "Entities encountered, relations between entities (e.g., location connectivity, object containment/ownership), and evolving world-state facts extracted from observations.",
            "memory_usage": "Maintains and updates a structured world-model to support generalization and inform policy decisions under partial observability; used to reason about environment dynamics and guide exploration.",
            "game_benchmark": "Text-based games / interactive fiction (general text-game benchmarks)",
            "game_description": "Parser-based interactive fiction with complex worlds, many entities, nested object structures, and long-term puzzles requiring persistent world knowledge.",
            "game_complexity": "High complexity due to many entities/relations and long-range dependencies; necessitates a persistent structured memory to capture world state.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_benefits": "Provides a persistent, structured representation that aids in generalization across states and games and improves the agent's ability to reason about entities and their relations over long horizons.",
            "memory_limitations": null,
            "key_findings": "The referenced work emphasizes that dynamic knowledge-graph memory helps agents generalize and better handle partial observability by explicitly modeling entities and relations encountered during play.",
            "uuid": "e229.1",
            "source_info": {
                "paper_title": "Situated language learning via interactive narratives",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "Commonsense-enhanced agents (Murugesan et al., 2020)",
            "name_full": "Enhancing text-based reinforcement learning agents with commonsense knowledge",
            "brief_description": "Agents augmented with external commonsense knowledge (semantic memory/knowledge bases) to supply priors about object affordances, typical locations, and commonsense relations useful for text-game reasoning and action selection.",
            "citation_title": "Enhancing text-based reinforcement learning agents with commonsense knowledge",
            "mention_or_use": "mention",
            "agent_name": "Commonsense-augmented RL agent",
            "agent_description": "A text-game agent that augments its decision-making with external commonsense knowledge sources (semantic memory/knowledge base) to provide priors about objects, affordances, and likely relations helpful for exploration and planning.",
            "agent_type": "reinforcement learning (hybrid with external knowledge)",
            "uses_memory": true,
            "memory_type": "external knowledge base / semantic memory",
            "memory_structure": "External commonsense knowledge base or semantic memory (structured facts about objects, affordances, and relations) consulted during play.",
            "memory_content": "Commonsense facts and priors such as typical object locations, affordances (what an object can be used for), and thematic knowledge relevant to genres.",
            "memory_usage": "Augments perception/action selection by providing priors and constraints, helping the agent pick plausible actions and reducing exploration of improbable actions under partial information.",
            "game_benchmark": "Text-adventure games / interactive fiction",
            "game_description": "Games that leverage thematic commonsense (e.g., cooking, fantasy) where prior knowledge about affordances and object roles helps solve quests and puzzles.",
            "game_complexity": "Complex due to need for thematic commonsense and long-range dependencies; success depends on applying appropriate priors in varying genres.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_benefits": "Supplies thematic commonsense priors that can guide exploration, reduce nonsensical actions, and help satisfy latent dependencies required for puzzle progression.",
            "memory_limitations": null,
            "key_findings": "The cited work indicates that incorporating commonsense knowledge as an external memory source can improve agents' ability to choose plausible actions and handle domain-specific priors necessary for solving text-game puzzles.",
            "uuid": "e229.2",
            "source_info": {
                "paper_title": "Situated language learning via interactive narratives",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "Graph-Constrained RL (Ammanabrolu & Hausknecht 2020)",
            "name_full": "Graph Constrained Reinforcement Learning for Natural Language Action Spaces",
            "brief_description": "A reinforcement learning method that uses graph-structured constraints/memory to restrict and guide natural language action generation in text-adventure games, reducing combinatorial action complexity.",
            "citation_title": "Graph Constrained Reinforcement Learning for Natural Language Action Spaces",
            "mention_or_use": "mention",
            "agent_name": "Graph-constrained RL agent",
            "agent_description": "An RL agent that constrains the natural language action space using a graph-structured representation (memory) of the environment, thereby reducing combinatorial action generation and improving exploration efficiency.",
            "agent_type": "reinforcement learning",
            "uses_memory": true,
            "memory_type": "graph-based memory / constraint graph",
            "memory_structure": "Graph structure encoding entities and allowable/compositional actions (constraints derived from a memory graph) used to prune or guide action generation.",
            "memory_content": "Entities (objects/locations), possible action templates or constraints tied to entities, and connectivity helping to limit action candidates.",
            "memory_usage": "Uses the graph to constrain or bias the policy's action generation toward contextually valid actions, reducing irrelevant action exploration in large language action spaces.",
            "game_benchmark": "Text-adventure games / interactive fiction",
            "game_description": "Parser-based games with combinatorially large natural language action spaces where constraining action candidates is necessary for tractable learning and exploration.",
            "game_complexity": "Very high action-space complexity (paper cites combinatorial sizes like ~697^5 possible 5-word actions for an example parser vocabulary); requires action-space reduction strategies.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_benefits": "Reduces the effective action space by constraining generation to graph-consistent actions, improving exploration efficiency and making RL tractable in large natural-language action spaces.",
            "memory_limitations": null,
            "key_findings": "The referenced approach highlights that graph-structured memory/constraints can make natural language action selection tractable by pruning invalid action candidates and focusing exploration on contextually plausible commands.",
            "uuid": "e229.3",
            "source_info": {
                "paper_title": "Situated language learning via interactive narratives",
                "publication_date_yy_mm": "2021-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Learning dynamic knowledge graphs to generalize on textbased games",
            "rating": 2
        },
        {
            "paper_title": "Enhancing text-based reinforcement learning agents with commonsense knowledge",
            "rating": 2
        },
        {
            "paper_title": "Graph Constrained Reinforcement Learning for Natural Language Action Spaces",
            "rating": 2
        }
    ],
    "cost": 0.0106975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Situated Language Learning via Interactive Narratives</h1>
<p>Prithviraj Ammanabrolu<br>Georgia Institute of Technology<br>raj.ammanabrolu@gatech.edu</p>
<p>Mark O. Riedl<br>Georgia Institute of Technology<br>riedl@cc.gatech.edu</p>
<h4>Abstract</h4>
<p>This paper provides a roadmap that explores the question of how to imbue learning agents with the ability to understand and generate contextually relevant natural language in service of achieving a goal. We hypothesize that two key components in creating such agents are interactivity and environment grounding, shown to be vital parts of language learning in humans, and posit that interactive narratives should be the environments of choice for such training these agents. These games are simulations in which an agent interacts with the world through natural language-"perceiving", "acting upon", and "talking to" the world using textual descriptions, commands, and dialogue-and as such exist at the intersection of natural language processing, storytelling, and sequential decision making. We discuss the unique challenges a text games' puzzle-like structure combined with natural language state-and-action spaces provides: knowledge representation, commonsense reasoning, and exploration. Beyond the challenges described so far, progress in the realm of interactive narratives can be applied in adjacent problem domains. These applications provide interesting challenges of their own as well as extensions to those discussed so far. We describe three of them in detail: (1) evaluating AI system's commonsense understanding by automatically creating interactive narratives; (2) adapting abstract text-based policies to include other modalities such as vision; and (3) enabling multi-agent and human-AI collaboration in shared, situated worlds.</p>
<h2>1 Introduction</h2>
<p>Natural language communication has long been considered a defining characteristic of human intelligence. In humans, this communication is grounded in experience and real world context-"what" we say or do depends on the current context around us and "why" we say or do something draws on commonsense knowledge gained through experience. So how do we imbue learning agents with the ability to understand and generate contextually relevant natural language in service of achieving a goal?</p>
<p>Two key components in creating such agents are interactivity and environment grounding, shown to be vital parts of language learning in humans. Humans learn various skills such as language, vision, motor skills, etc. more effectively through interactive media [Feldman and Narayanan, 2004, Barsalou, 2008]. In the realm of machines, interactive environments have served as cornerstones in the quest to develop more robust algorithms for learning agents across many machine learning sub-communities. Environments such as the Atari Learning Environment [Bellemare et al., 2013] and Minecraft [Johnson et al., 2016] have enabled the development of game agents that perform complex tasks while operating on raw video inputs, and more recently THOR [Kolve et al., 2017] and Habitat [Manolis Savva* et al., 2019] attempt to do the same with embodied agents in simulated 3D worlds.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An excerpt from Zork1, a typical text-based adventure game.</p>
<p>Despite such progress in modern machine learning and natural language processing, agents that can communicate with humans (and other agents) through natural language in pursuit of their goals are still primitive. One possible reason for this is that many datasets and tasks used for NLP are static, not supporting interaction and language grounding [Brooks, 1991, Feldman and Narayanan, 2004, Barsalou, 2008, Mikolov et al., 2016, Gauthier and Mordatch, 2016, Lake et al., 2017] In other words, there has been a void for such interactive environments for purely language-oriented tasks. Building on recent work in this field, we posit that interactive narratives should be the environments of choice for such language-oriented tasks. Interactive Narratives, in general, is an umbrella term, that refers to any form of digital interactive experience in which users create or influence a dramatic storyline through their actions [Riedl and Bulitko, 2013]-i.e. the overall story progression in the game is not pre-determined and is directly influenced by a player's choices. For the purposes of this work, we consider one particular type of interactive narrative, parser-based interactive fiction (or text-adventure) games-though we note that other forms of interactive narrative, including those with visual components, provide closely related challenges.
Figure 1 showcases Zork [Anderson et al., 1979], one of the earliest and most influential text-based interactive narrative. These games are simulations in which an agent interacts with the world through natural language-"perceiving", "acting upon", and "talking to" the world using textual descriptions, commands, and dialogue. The simulations are partially observable, meaning that the agent never has access to the true underlying world state and has to reason about how to act in the world based only on potentially the incomplete textual observations of its immediate surroundings. They provide tractable, situated environments in which to explore highly complex interactive grounded language learning without the complications that arise when modeling physical motor control and vision-situations that voice assistants such as Siri or Alexa might find themselves in when improvising responses. These games are usually structured as puzzles or quests with long-term dependencies in which a player must complete a sequence of actions and/or dialogues to succeed. This in turn requires navigation and interaction with hundreds of locations, characters, and objects. The interactive narrative community is one of the oldest gaming communities and game developers in this genre are quite creative. Put these two things together and we get very large, complex worlds that contain a multitude of puzzles and quests to solve across many different genres-everything from slice of life simulators where the player cooks a recipe in their home to Lovecraftian horror mysteries. The complexity and diversity of topics enable us to build and test agents that go an extra step towards modeling the difficulty of situated human language communication.
As the excerpt of the text-game in Figure 1 shows, humans bring competencies in natural language understanding, commonsense reasoning, and deduction to bear in order to infer the context and objectives of a game. Beyond games, real-world applications such as voice-activated personal as-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: A map of Zork1 by artist ion_bond.
sistants can also benefit from advances in these capabilities at the intersection of natural language understanding, natural language generation, and sequential decision making. These real world applications require the ability to reason with ungrounded natural language (unlike multimodal environments that provide visual grounding for language) and interactive narratives provide an excellent suite of environments to tackle these challenges.</p>
<p>Currently, three primary open-source platforms and baseline benchmarks have been developed so far to help measure progress in this field: Jericho [Hausknecht et al., 2020] ${ }^{1}$ a learning environment for human-made interactive narrative games; TextWorld [Ct et al., 2018] ${ }^{2}$ a framework for procedural generation in text-games; and LIGHT [Urbanek et al., 2019] ${ }^{3}$ a large-scale crowdsourced multi-user text-game for studying situated dialogue.</p>
<h1>2 Challenges</h1>
<p>Interactive narratives exist at the intersection of natural language processing, storytelling, and sequential decision making. Like many NLP tasks, they require natural language understanding, but unlike most NLP tasks, Interactive narratives are sequential decision making problems in which actions change the subsequent world states of the game and choices made early in a game may have long term effects on the eventual endings. Reinforcement Learning [Sutton and Barto, 1998] studies sequential decision making problems and has shown promise in vision-based [Jaderberg et al., 2016] and control-based [OpenAI et al., 2018] environments, but has less commonly been applied in the context of language-based tasks. Text-based games thus pose a different set of challenges than traditional video games such as StarCraft. Their puzzle-like structure coupled with a partially observable state space and sparse rewards require a greater understanding of previous context to enable more effective exploration-an implicit long-term dependency problem not often found in other domains that agents must overcome.</p>
<h3>2.1 Knowledge Representation</h3>
<p>Interactive narratives span many distinct locations, each with unique descriptions, objects, and characters. An example of a world of a interactive fiction game can be seen in Figure 2. Players move between locations by issuing navigational commands like go West.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>This, in conjunction with the inherent partial observability of interactive narratives, gives rise to the Textual-SLAM problem, a textual variant of Simultaneous localization and mapping (SLAM) [Thrun et al., 2005] problem of constructing a map while navigating a new environment. In particular, because connectivity between locations is not necessarily Euclidean, agents need to detect when a navigational action has succeeded or failed and whether the location reached was previously seen or new. Beyond location connectivity, its also helpful to keep track of the objects present at each location, with the understanding that objects can be nested inside of other objects, such as food in a refrigerator or a sword in a chest.</p>
<p>Due to the large number of locations in many games, humans often create structured memory aids such as maps to navigate efficiently and avoid getting lost. The creation of such memory aids has been shown to be critical in helping automated learning agents operate in these textual worlds [Ammanabrolu and Riedl, 2019, Murugesan et al., 2020, Adhikari et al., 2020, Ammanabrolu and Hausknecht, 2020]</p>
<h3>2.2 Acting and Speaking in Combinatorially-sized State-Action Spaces</h3>
<p>Interactive narratives require the agent to operate in the combinatorial action space of natural language. To realize how difficult a game such as Zork1 is for standard reinforcement learning agents, we need to first understand how large this space really is. In order to solve solve a popular IF game such as Zork1 its necessary to generate actions consisting of up to five-words from a relatively modest vocabulary of 697 words recognized by Zorks parser. Even this modestly sized vocabulary leads to $\mathcal{O}(697^{5})=1.64 \times 10^{14}$ possible actions at every stepa dauntingly-large combinatorially-sized action space for a learning agent to explore. In comparison, board games such as chess and Go or Atari video games have branching factors of the order of $\mathcal{O}(10^{2})$.</p>
<p>Some text-games extend this even further by requiring agents to engage in dialogue to progress in a task, increasing the space of possibilities exponentially and bringing text environments closer to real-world situations. An example of such an environmentdesigned explicitly as a research platformis the large-scale crowdsourced fantasy text-adventure game LIGHT [Urbanek et al., 2019], seen in Figure 3, where characters can act and talk while interacting with other characters. It consists of a set of locations, characters, and objects leading to rich textual worlds in addition to quests demonstrations of humans playing these quests providing natural language descriptions in varying levels of abstraction of motivations for a given character in a particular setting.</p>
<p>On top of the other text-game related challenges, the primary core challenge for the agent here is the recognition that dialogue can also be used to change the environment. With dialogue, an agent can now learn to instruct or convince other characters in the world to achieve the goal for ite.g. convince the pirate through dialogue to give you their treasure instead of just stealing it yourself. The agent needs to learn to balance both its ability to speak as well as act in order to effectively achieve its goals [Ammanabrolu et al., 2021].</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The LIGHT [Urbanek et al., 2019] environment.</p>
<h3>2.3 Commonsense Reasoning</h3>
<p>Text-games cover a wide variety of genres, as mentioned earlier this ranges from slice of life simulators where the player makes a recipe in their home to Lovecraftian horror mysteries. In order to effectively convey the core narrative or puzzle, text-adventure games make ample use of prior commonsense knowledge. Everyday example could be something as mundane as the fact that an axe can be used to cut wood, or that swords are weapons. Different genres also have specific knowledge</p>
<p>attached to them that wouldn't normally be found in mundane settings, e.g. in a horror or fantasy game, we know that a coffin is likely to contain a vampire or other undead monster or that kings are royalty and must be treated respectfully. When a human enters a particular domain, they already possess priors regarding the specific knowledge relevant to the situations likely to be encounteredthis is thematic commonsense knowledge that a learning agent must acquire to ensure successful interactions.</p>
<p>This is closely related to the problem of transfer, the problem of acquiring and adapting these priors in novel environments through interaction. In this sense, we can think of commonsense knowledge as priors regarding environment dynamics. This problem space can be explored using text-based games. What commonsense can be transferred between two different environments, for example, a horror game and a mundane slice of life game? How do you unlearn, or choose not to apply, a piece of commonsense that no longer fits with the current world. What if the perceived environment dynamics change in novel ways? E.g. some vampires actually love garlic instead of being allergic to them or you suddenly find out that bread can be made without yeast and is known as sourdoughwhole new categories of recipes are now possible.</p>
<h1>2.4 Exploration</h1>
<p>Most text-adventure games have relatively linear plots in which players must solve a sequence of puzzles to advance the story and gain score. To solve these puzzles, players have freedom to a explore both new areas and previously unlocked areas of the game, collect clues, and acquire tools needed to solve the next puzzle and unlock the next portion of the game. From a Reinforcement Learning perspective, these puzzles can be viewed as bottlenecks that act as partitions between different regions of the state space. Whereas the relatively linear progression through puzzles may seem to make the problem easier, the opposite is true. The bottlenecks set up a situation where agents get stuck because they do not see the right action sequence enough times to be sufficiently reinforced. We contend that existing Reinforcement Learning agents are unaware of such latent structure and are thus poorly equipped for solving these types of problems.</p>
<p>Overcoming bottlenecks is not as simple as selecting the correct action from the bottleneck state. Most bottlenecks have long-range dependencies that must first be satisfied: Zork1 for instance features a bottleneck in which the agent must pass through the unlit Cellar where a monster known as a Grue lurks, ready to eat unsuspecting players who enter without a light source. To pass this bottleneck the player must have previously acquired and lit the lantern. Reaching the Cellar without acquiring the lantern results in the player reaching an unwinnable state-the player is unable to go back and acquire a lantern but also cannot progress further without a way to combat the darkness. Other bottlenecks don't rely on inventory items and instead require the player to have satisfied an external condition such as visiting the reservoir control to drain water from a submerged room before being able to visit it. In both cases, the actions that fulfill dependencies of the bottleneck, e.g. acquiring the lantern or draining the room, are not rewarded by the game. Thus agents must correctly satisfy all latent dependencies, most of which are unrewarded, then take the right action from the correct location to overcome such bottlenecks. Consequently, most existing agents-regardless of whether they use a reduced action space [Zahavy et al., 2018, Yuan et al., 2018, Yin and May, 2019] or the full space [Hausknecht et al., 2020, Ammanabrolu and Hausknecht, 2020]-have failed to consistently clear these bottlenecks. It is only recently that works have begun explicitly accounting for and surpassing such bottlenecks-using a reduced action space and Monte-Carlo Planning [Jang et al., 2021] and full action space and intrinsic motivation-based structured exploration [Ammanabrolu et al., 2020c].</p>
<h2>3 Applications and Future Directions</h2>
<p>Beyond the challenges described so far, progress in the realm of interactive narratives can be applied in adjacent problem domains. These applications provide interesting challenges of their own as well as extensions to those discussed so far. This section will describe three of them in detail: (1) evaluating AI system's commonsense understanding by creating interactive narratives; (2) adapting abstract text-based policies to include other modalities such as vision; and (3) enabling multi-agent and human-AI collaboration in shared, situated worlds.</p>
<h1>3.1 Automated World and Quest Generation</h1>
<p>A key consideration in modeling communication through a general purpose interactive narrative solver is that an agent trained to solve these games is limited by the scenarios described in them. Although the range of scenarios is vast, this brings about the question of what the agent is actually capable of understanding even if it has learned to solve all the puzzles in a particular game. Deep (reinforcement) learning systems tend to learn to generalize from the head of any particular data distribution, the "common" scenarios, and memorize the tail, the rarely seen cases. We contend that a potential way of testing an AI system's understanding of a domain is to use the knowledge it has gained in a novel way and to create more instances of that domain.</p>
<p>From the perspective of interactive narratives, this involves automatically creating such gamesthe flip side of the problem of creating agents that operate in these environments-and requires anticipating how people will interact with these environments and conforming to such expected commonsense norms to make a creative and engaging experience. The core experience in an interactive narrative revolves the quest, consisting of the partial ordering of activities that an agent must engage in to make progress toward the end of the game. Quest generation requires narrative intelligence and commonsense knowledge as a quest must maintain coherence throughout while progressing towards a goal [Ammanabrolu et al., 2020a]. Each step of the quest follows logically from the preceding steps much like the steps of a cooking recipe. A restaurant cannot serve a batch of cookies without first gathering ingredients, preparing cooking instruments, mixing ingredients, etc. in a particular sequence. Any generated quest that doesn't follow such an ordering will appear random or nonsensical to a human, betraying the AI's lack of commonsense understanding.</p>
<p>Maintaining quest coherence also means following the constraints of the given game world. The quest has to fit within the confines of the world in terms of both genre and given affordances-e.g. using magic in a fantasy world, placing kitchens next to living rooms in mundane worlds, etc. This gives rise to the concept of world generation, the second half of the automated game generation problem. This refers to generating the structure of the world, including the layout of rooms, textual description of rooms, objects, and characters-setting the boundaries for how an agent is allowed to interact with the world [Ammanabrolu et al., 2020b]. Similarly to quests, a world violating thematically relevant commonsense structuring rules will appear random to humans, providing us with a metric to measure an AI system's understanding.</p>
<h3>3.2 Transfer across domains and modalities</h3>
<p>Many of the core challenges presented by text games manifest themselves across domains with different modalities and it may be possible to transfer progress between the domains. Take the example of a slice-of-life walking simulator text game where the main quest is to complete a recipe as given before. What happens when we encounter a similar situation with the added modality of vision? Can we take the knowledge we've gained from learning a textbased policy by completing the recipe in the original text game and use that to learn how to do something similar with a visually embodied agent? To test this idea, Shridhar et al. [2021] built ALFWorld, a simulator that lets you first learn text-based policies in the "home" textgame TextWorld [Ct et al., 2018], and then execute them in similarly themed scenarios from the visual environment ALFRED [Shridhar et al., 2020]. They find that commonsense priors-regarding things like common object locations, affordances, and causality-learned while playing text-games can be adapted to help create agents that generalize better in visually
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: ALFWorld [Shridhar et al., 2021].</p>
<p>grounded environments. This indicates that text games are suitable environments to train agents to reason abstractly through text which can then be refined and adapted to specific instances in an embodied setting.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: A wet lab protocol as a text game from the X-WLP dataset [Tamari et al., 2021].</p>
<p>Another such cross-domain transfer experiment was tested by Tamari et al. [2021], where they collected and built X-WLP, a corpus of complex wet lab biochemistry protocols that are framed as a quest and could thus be executed via a text-game engine. The annotations themselves are collected using a text-game-like interface, reducing overall data collection cost. Tamari et al. [2019] discuss automatically extracting these protocols from raw lab texts and also training deep reinforcement learning agents on the resulting text-game quest. The ability to automatically frame wet lab experiments in the form of text game quests and leverage the latest text-game agent advances to interactively train agents to perform them has implications for significantly improving procedural text understanding [Levy et al., 2017] and in the reproducibility of scientific experiments [Mehr et al., 2020].</p>
<h1>3.3 Multi-agent and Human-AI Collaboration</h1>
<p>Current work on teaching agents to act and speak in situated, shared worlds such as LIGHT opens the doors for exploring multi-agent communication using natural language, i.e. through dialogue. It has been shown how to teach agents to act and talk in pursuit of a goal in this world leads to them learning multiple ways of achieve the goal: acting to do it themselves, or convincing a partner agent to do it for them. We envision this situated learning paradigm extended to to a multi-agent setting, where there are multiple agents progressing through a world in pursuit of their own motivations that learn to communicate with each other, figuring out what others can do for them. This gives rise to a dynamic world within the bounds of a unified decision making framework, a situation autonomous agents are likely to find themselves in. A village led by an ambitious chief seeking expansion will expand into a town via environment dynamics, or narrative, emerging from this multiagent communication. Agents can further be taught which other agents they should cooperate with and which they should compete with on the basis of the alignment of their motivations. A dragon terrorizing a kingdom and a knight may perhaps be at odds, but the kingdom's ruler will have cause to cooperate and explicitly aid the knight in slaying the dragon. A not-so-fantastic example would be two small clothing businesses cooperating and pooling resources to compete against an encroaching large corporation.
A human-AI collaborative system is an instance of such a multi-agent system where one or more of the agents are humans. These works thus have direct implications for human-AI collaborative systems: from agents that act and talk in multi-user worlds, to improvisational and collaborative storytelling, and creative writing assistants for human authors.</p>
<h2>4 Conclusion</h2>
<p>Interactive narratives provide tractable, situated environments in which to explore highly complex interactive grounded language learning without the complications that arise when modeling physical motor control and vision. The unique challenges a text games' puzzle-like structure combined with natural language state-and-action spaces provides is: knowledge representation, commonsense reasoning, and exploration. These challenges create an implicit long-term dependency problem not often found in other domains that agents must overcome. Text-based games thus pose a different set of challenges than traditional video games such as StarCraft. Beyond the challenges described so far, we have seen how progress in the realm of interactive narratives can be applied in adjacent problem domains, specifically: (1) structured environment creation; (2) transfer to other modalities and domains; and (3) enabling multi-agent and human-AI collaboration in shared, situated worlds.</p>
<h1>Acknowledgements</h1>
<p>We thank Matthew Hausknecht, Xingdi Yuan, and Marc-Alexandre Ct of Microsoft Research for useful discussions on text games and their work on the Jericho and TextWorld platforms. Likewise, thanks to Jack Urbanek, Margaret Li, Arthur Szlam, Tim Rocktschel, and Jason Weston of Facebook AI Research for their efforts and guidance in the work on the LIGHT framework. We also would like to thank the corresponding authors Mohit Shridar of the University of Washington and Ronen Tamari of the Hebrew University of Jerusalem for discussions regarding their respective works ALFWorld and X-WLP and the images within reproduced accordingly.</p>
<h2>References</h2>
<p>A. Adhikari, X. Yuan, M.-A. Ct, M. Zelinka, M.-A. Rondeau, R. Laroche, P. Poupart, J. Tang, A. Trischler, and W. L. Hamilton. Learning dynamic knowledge graphs to generalize on textbased games. arXiv preprint arXiv:2002.09127, 2020.
P. Ammanabrolu and M. Hausknecht. Graph Constrained Reinforcement Learning for Natural Language Action Spaces. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=B1x6w0EtwH.
P. Ammanabrolu and M. O. Riedl. Playing text-adventure games with graph-based deep reinforcement learning. In Proceedings of 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, 2019.
P. Ammanabrolu, W. Broniec, A. Mueller, J. Paul, and M. O. Riedl. Toward automated quest generation in text-adventure games. In International Conference on Computational Creativity (ICCC), 2020a. URL https://arxiv.org/abs/1909.06283.
P. Ammanabrolu, W. Cheung, D. Tu, W. Broniec, and M. O. Riedl. Bringing stories alive: Generating interactive fiction worlds. In Proceedings of the Sixteenth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment (AIIDE-20), 2020b. URL https: //www.aaai.org/ojs/index.php/AIIDE/article/view/7400.
P. Ammanabrolu, E. Tien, M. Hausknecht, and M. O. Riedl. How to avoid being eaten by a grue: Structured exploration strategies for textual worlds. arXiv preprint arXiv:2006.07409, 2020c.
P. Ammanabrolu, J. Urbanek, M. Li, A. Szlam, T. Rocktschel, and J. Weston. How to motivate your dragon: Teaching goal-driven agents to speak and act in fantasy worlds. In Proceedings of 2021 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, 2021. URL https://arxiv.org/ abs/2010.00685.
T. Anderson, M. Blank, B. Daniels, and D. Lebling. Zork. http://ifdb.tads.org/viewgame?id= 4gxk83ja4twckm6j, 1979.
L. W. Barsalou. Grounded cognition. Annual Review of Psychology, 59(1):617-645, 2008. doi: 10. 1146/annurev.psych.59.103006.093639. URL https://doi.org/10.1146/annurev.psych.59.103006. 093639. PMID: 17705682.
M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253-279, jun 2013.
R. A. Brooks. Intelligence without representation. Artificial intelligence, 47(1-3):139-159, 1991.
M.-A. Ct, A. Kdr, X. Yuan, B. Kybartas, T. Barnes, E. Fine, J. Moore, M. Hausknecht, L. E. Asri, M. Adada, W. Tay, and A. Trischler. Textworld: A learning environment for text-based games. CoRR, abs/1806.11532, 2018.
J. Feldman and S. Narayanan. Embodied meaning in a neural theory of language. Brain and language, 89:385-92, 06 2004. doi: 10.1016/S0093-934X(03)00355-9.</p>
<p>J. Gauthier and I. Mordatch. A paradigm for situated and goal-driven language learning. arXiv preprint arXiv:1610.03585, 2016.
M. Hausknecht, P. Ammanabrolu, M.-A. Ct, and X. Yuan. Interactive fiction games: A colossal adventure. In Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI), 2020. URL https: //arxiv.org/abs/1909.05398.
M. Jaderberg, V. Mnih, W. M. Czarnecki, T. Schaul, J. Z. Leibo, D. Silver, and K. Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. CoRR, abs/1611.05397, 2016.
Y. Jang, S. Seo, J. Lee, and K.-E. Kim. Monte-carlo planning and learning with language action value estimates. In International Conference on Learning Representations, 2021. URL https: //openreview.net/forum?id=7_G8JySGecm.
M. Johnson, K. Hofmann, T. Hutton, and D. Bignell. The malmo platform for artificial intelligence experimentation. In IJCAI, IJCAI'16, pages 4246-4247. AAAI Press, 2016. ISBN 978-1-57735-770-4.
E. Kolve, R. Mottaghi, W. Han, E. VanderBilt, L. Weihs, A. Herrasti, D. Gordon, Y. Zhu, A. Gupta, and A. Farhadi. AI2-THOR: An Interactive 3D Environment for Visual AI. arXiv, 2017.
B. M. Lake, T. D. Ullman, J. B. Tenenbaum, and S. J. Gershman. Building machines that learn and think like people. Behavioral and brain sciences, 40, 2017.
O. Levy, M. Seo, E. Choi, and L. Zettlemoyer. Zero-shot relation extraction via reading comprehension. In Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 333-342, Vancouver, Canada, Aug. 2017. Association for Computational Linguistics. doi: 10.18653/v1/K17-1034. URL https://www.aclweb.org/anthology/K17-1034.</p>
<p>Manolis Savva<em>, Abhishek Kadian</em>, Oleksandr Maksymets*, Y. Zhao, E. Wijmans, B. Jain, J. Straub, J. Liu, V. Koltun, J. Malik, D. Parikh, and D. Batra. Habitat: A Platform for Embodied AI Research. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019.
S. H. M. Mehr, M. Craven, A. I. Leonov, G. Keenan, and L. Cronin. A universal system for digitization and automatic execution of the chemical synthesis literature. Science, 370(6512):101-108, 2020. ISSN 0036-8075. doi: 10.1126/science.abc2986. URL https://science.sciencemag.org/ content/370/6512/101.
T. Mikolov, A. Joulin, and M. Baroni. A roadmap towards machine intelligence. In International Conference on Intelligent Text Processing and Computational Linguistics, pages 29-61. Springer, 2016.
K. Murugesan, M. Atzeni, P. Shukla, M. Sachan, P. Kapanipathi, and K. Talamadupula. Enhancing text-based reinforcement learning agents with commonsense knowledge. arXiv preprint arXiv:2005.00811, 2020.</p>
<p>OpenAI, M. Andrychowicz, B. Baker, M. Chociej, R. Jzefowicz, B. McGrew, J. W. Pachocki, J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray, J. Schneider, S. Sidor, J. Tobin, P. Welinder, L. Weng, and W. Zaremba. Learning dexterous in-hand manipulation. CoRR, abs/1808.00177, 2018.
M. O. Riedl and V. Bulitko. Interactive narrative: An intelligent systems approach. Ai Magazine, 34 (1):67-67, 2013.
M. Shridhar, J. Thomason, D. Gordon, Y. Bisk, W. Han, R. Mottaghi, L. Zettlemoyer, and D. Fox. ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. URL https://arxiv.org/ abs/1912.01734.
M. Shridhar, X. Yuan, M.-A. Cote, Y. Bisk, A. Trischler, and M. Hausknecht. {ALFW}orld: Aligning text and embodied environments for interactive learning. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=0IOX0YcCdTn.</p>
<p>R. S. Sutton and A. G. Barto. Introduction to Reinforcement Learning. MIT Press, Cambridge, MA, USA, 1st edition, 1998. ISBN 0262193981.
R. Tamari, H. Shindo, D. Shahaf, and Y. Matsumoto. Playing by the Book: An Interactive Game Approach for Action Graph Extraction from Text. In Proceedings of the Workshop on Extracting Structured Knowledge from Scientific Publications, pages 62-71, Minneapolis, Minnesota, jun 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-2609. URL https:// www.aclweb.org/anthology/W19-2609.
R. Tamari, F. Bai, A. Ritter, and G. Stanovsky. Process-level representation of scientific protocols with interactive annotation. arXiv preprint arXiv:2101.10244, 2021.
S. Thrun, W. Burgard, and D. Fox. Probabilistic Robotics (Intelligent Robotics and Autonomous Agents). The MIT Press, 2005. ISBN 0262201623.
J. Urbanek, A. Fan, S. Karamcheti, S. Jain, S. Humeau, E. Dinan, T. Rocktschel, D. Kiela, A. Szlam, and J. Weston. Learning to speak and act in a fantasy text adventure game. CoRR, abs/1903.03094, 2019.
X. Yin and J. May. Comprehensible context-driven text game playing. CoRR, abs/1905.02265, 2019.
X. Yuan, M. Ct, A. Sordoni, R. Laroche, R. T. des Combes, M. J. Hausknecht, and A. Trischler. Counting to explore and generalize in text-based games. CoRR, abs/1806.11525, 2018.
T. Zahavy, M. Haroush, N. Merlis, D. J. Mankowitz, and S. Mannor. Learn what not to learn: Action elimination with deep reinforcement learning. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 3562-3573. Curran Associates, Inc., 2018.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://github.com/microsoft/jericho
${ }^{2}$ https://github.com/microsoft/textworld
${ }^{3}$ https://parl.ai/projects/light&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
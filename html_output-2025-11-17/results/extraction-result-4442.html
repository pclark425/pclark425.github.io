<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4442 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4442</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4442</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-280996792</p>
                <p><strong>Paper Title:</strong> 32 examples of LLM applications in materials science and chemistry: towards automation, assistants, agents, and accelerated scientific discovery</p>
                <p><strong>Paper Abstract:</strong> Abstract Large language models (LLMs) are reshaping many aspects of materials science and chemistry research, enabling advances in molecular property prediction, materials design, scientific automation, knowledge extraction, and more. Recent developments demonstrate that the latest class of models are able to integrate structured and unstructured data, assist in hypothesis generation, and streamline research workflows. To explore the frontier of LLM capabilities across the research lifecycle, we review applications of LLMs through 32 total projects developed during the second annual LLM hackathon for applications in materials science and chemistry, a global hybrid event. These projects spanned seven key research areas: (1) molecular and material property prediction, (2) molecular and material design, (3) automation and novel interfaces, (4) scientific communication and education, (5) research data management and automation, (6) hypothesis generation and evaluation, and (7) knowledge extraction and reasoning from the scientific literature. Collectively, these applications illustrate how LLMs serve as versatile predictive models, platforms for rapid prototyping of domain-specific tools, and much more. In particular, improvements in both open source and proprietary LLM performance through the addition of reasoning, additional training data, and new techniques have expanded effectiveness, particularly in low-data environments and interdisciplinary research. As LLMs continue to improve, their integration into scientific workflows presents both new opportunities and new challenges, requiring ongoing exploration, continued refinement, and further research to address reliability, interpretability, and reproducibility.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4442.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4442.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MultiAgent-Hypothesis-Verification</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-agent hypothesis generation and verification (RAG + Tree-of-Thoughts + Evaluation agents)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent pipeline that generates hypotheses from retrieved literature, deliberates candidate ideas via tree-of-thoughts reasoning, and automatically evaluates them with specialized evaluator agents for feasibility, utility, and novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Multi-agent hypothesis generation and verification (RAG + Tree-of-Thoughts + evaluation agents)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>The pipeline first builds an embedding-indexed retrieval over a large corpus and uses Retrieval-Augmented Generation (RAG) to supply context to generative agents that propose hypotheses. Candidate hypotheses are then expanded/deliberated using a tree-of-thoughts style search to explore alternative formulations. Separate evaluator agents (often implemented as LLM prompts or chains) score or filter hypotheses along multiple axes (feasibility, utility, novelty). The process is iterative: retrieval informs generation, generation produces candidates, tree-of-thoughts explores alternatives, and evaluators prune/refine outputs. All steps are implemented as LLM-driven agents orchestrated within an agentic framework.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Feasibility (compatibility with existing scientific knowledge and plausibility), Practical utility (applicability or usefulness for a research/engineering problem), Novelty (degree to which the hypothesis is new relative to retrieved literature); implicitly also internal coherence and syntactic validity.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science (example: concrete/cement sustainability in the hackathon study)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Generated research hypotheses for materials design (generative / testable scientific hypotheses)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Applied to ~66,000 abstracts for sustainable concrete design: the system generated 1,000 structured hypotheses; internal automated evaluation classified 243 as 'feasible', 175 as demonstrating 'practical utility', and 12 as 'highly novel'. No quantitative external/expert validation scores were reported in the paper beyond these counts.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated — evaluation conducted by LLM-based evaluator agents (LLM-as-judge style) using retrieved text and prompt-based scoring; no systematic human expert rating dataset reported for those hypotheses in the hackathon description (human-in-the-loop was discussed as desirable for future validation).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Internal filtering/consistency checks using retrieved literature as context; no reported correlation with independent expert judgments or experimental verification in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Relies on retrieval quality and coverage (missing literature biases scores); vulnerable to LLM hallucinations and prompt sensitivity; token/context length and RAG truncation can hide relevant counter-evidence; no external expert or experimental validation reported (so automated feasibility/novelty labels may be noisy); metric thresholds/decision rules for classifying feasibility/novelty not standardized.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>A local embedding index built from ~66,000 abstracts related to concrete (used as background corpus); no standardized benchmark dataset for hypothesis evaluation was used.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '32 examples of LLM applications in materials science and chemistry: towards automation, assistants, agents, and accelerated scientific discovery', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4442.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4442.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-a-judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-a-judge evaluation framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting-based approach in which one or more LLM instances act as evaluators or judges that score or rank candidate hypotheses/explanations according to predefined criteria (e.g., feasibility, novelty).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LLM-as-a-judge</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Candidate hypotheses or explanations are provided to an LLM (often with supporting retrieved evidence) along with an evaluation rubric encoded into the prompt; the LLM returns a classification, score, or justification (e.g., 'feasible/infeasible', novelty estimate, or a numeric score). This can be used as a standalone evaluator or as part of a multi-agent pipeline (e.g., multiple LLM 'judges' or ensemble judgment).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Typically mirrors the rubric supplied in prompts: plausibility/feasibility, empirical support/coherence with literature, novelty relative to retrieved contexts, and sometimes clarity or specificity.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Used generically in the paper's pipelines; applied in examples within materials science (cement/concrete) and other hackathon projects.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Testable research hypotheses and explanatory statements</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>The paper reports use of LLM-as-judge within the multi-agent pipeline to filter hypotheses; quantitative calibration or benchmarking of LLM-judge outputs against human experts is not reported in this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (LLM-based scoring). The approach is amenable to hybrid workflows where human experts audit or re-score selected results, but such hybrid validation was not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Not validated against independent human expert judgments in this work; validation was limited to internal consistency checks and counts of classifier outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>LLM-judge outputs can reflect model biases and overconfidence; without external calibration against human experts or experiments, scores may not correspond to real-world correctness; prompt design and choice of supporting retrieved evidence strongly affect judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>No dedicated benchmark for LLM-judge evaluation reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '32 examples of LLM applications in materials science and chemistry: towards automation, assistants, agents, and accelerated scientific discovery', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4442.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4442.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>F-U-N-eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Feasibility / Utility / Novelty evaluation agents (F-U-N)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of explicit evaluation criteria operationalized as separate evaluator agents that label or score hypotheses on Feasibility, Utility, and Novelty (abbreviated here as F-U-N).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Feasibility / Utility / Novelty (F-U-N) evaluator agents</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>The system runs three (or more) evaluator agents, each focused on one criterion: feasibility (consistency with known science and practical plausibility), utility (potential practical impact or usefulness), and novelty (degree of difference from prior art as determined via retrieval). Each evaluator returns binary labels or graded scores that are then combined to prioritize hypotheses for follow-up.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Feasibility (scientific plausibility and compatibility with current knowledge); Practical utility (application potential, measurability, or impact); Novelty (degree of originality vs. retrieved corpus).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science (demonstrated on concrete sustainability hypotheses), generalizable to other scientific domains.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Testable research hypotheses (mechanistic or design-oriented hypotheses)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>From the hackathon pipeline: out of 1,000 generated hypotheses 243 were flagged as feasible, 175 as practically useful, and 12 as highly novel by these evaluator agents. No further external benchmarking was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (agent-based LLM evaluation); the paper discusses future incorporation of human-in-the-loop validation but does not present it.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Internal classification based on retrieved evidence; no inter-rater reliability or correlation with human experts was presented.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Binary/threshold decisions and scoring rules were not fully described; susceptibility to retrieval omissions and to LLM over/under-estimating novelty; lack of external calibration reduces confidence in automated labels.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>No canonical benchmark; applied to hypotheses generated from an embedding index of ~66k abstracts for the concrete domain.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '32 examples of LLM applications in materials science and chemistry: towards automation, assistants, agents, and accelerated scientific discovery', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4442.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4442.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>G-Peer-T</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>G-Peer-T: LLM Probabilities For Assessing Scientific Novelty and Nonsense</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hackathon project (listed in the paper) that — by its title — aims to use LLM output probabilities to identify scientifically novel claims and flag nonsensical outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LLM probability-based novelty/nonsense scoring (G-Peer-T)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Based on the project title and listing, the approach likely uses token or sequence probabilities (model confidences) from LLMs to estimate how plausible or 'in-distribution' a candidate claim is, flagging low-probability/high-surprise outputs as potential nonsense or novel claims (exact implementation details are not described in the paper's main text).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Model output probability / perplexity as proxy for plausibility; novelty flagged by low prior-probability given model; nonsense detection via low-confidence tokens or incoherent sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Not specified in main text (project listed among diverse hackathon submissions); general-purpose method to detect novelty/nonsense in scientific content.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Assessing candidate assertions/hypotheses or claims for novelty and coherence</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>The paper lists the project in Table 1 but does not provide experimental results or metrics for G-Peer-T within the main manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Presumably automated (probability-based heuristics); no hybrid or human evaluation reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Not described in the paper; no reported correlation with human judgments or empirical tests.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>No details provided in this manuscript; general limitations of probability-based novelty detection include sensitivity to model calibration and difficulty distinguishing true scientific novelty from hallucinated but low-probability nonsense.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Not reported in this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '32 examples of LLM applications in materials science and chemistry: towards automation, assistants, agents, and accelerated scientific discovery', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4442.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4442.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MaSTeA-MaScQA-Eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MaSTeA evaluation on MaScQA (materials science question set) dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evaluation framework that measures LLM performance on a curated undergraduate-level materials science question set (MaScQA) by extracting model outputs, verifying answers (regex/exact/acceptable-range checks), and computing accuracy per question type and topic.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>MaSTeA evaluation pipeline on MaScQA</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>The team curated 650 questions (four structural types: MCQ, MATCH, MCQN, NUM) and evaluated models by parsing model outputs (regular expressions for MCQs), validating numeric responses within exact or tolerance ranges, and computing per-type and per-topic accuracy metrics. Results are presented by model and topic to identify strengths/weaknesses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Accuracy (per-question correctness), broken down by question type (MCQ, match, numeric) and topic (thermodynamics, atomic structure, mechanical behavior, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple models tested (LLAMA3-8B, Claude variants including Opus, GPT-4, Haiku, Sonnet)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Examples include LLAMA3-8B (8B); Claude-3-Opus variant sizes not specified in the paper; GPT-4 (size not specified).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science education / knowledge assessment</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Not full theories — evaluates factual / reasoning answers and stepwise solutions (scientific question-answering and explanations).</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Opus (Claude-3-Opus variant) consistently achieved the highest accuracy across most categories; GPT-4 also showed strong performance in several topics (e.g., material processing and fluid mechanics). Smaller LLaMA3-8B underperformed relative to larger models. Detailed per-topic accuracies are provided in Table 2 of the paper (numerical breakdowns per model and topic).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated scoring against gold answers (regex/exact-match/tolerance checks); no human expert adjudication reported for borderline cases in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Automatic comparison to labeled ground-truth answers in the MaScQA dataset; this provides an objective accuracy metric but does not assess explanation quality beyond correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Dataset covers undergraduate-level questions and may not reflect advanced hypothesis-generation evaluation; automatic parsing of LLM outputs can fail for differently formatted answers; evaluation measures accuracy but not deeper explanatory fidelity or experimental testability.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>MaScQA (650 curated materials science questions) used by the MaSTeA project; dataset and interface produced during hackathon (paper references MaScQA and MaSTeA).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '32 examples of LLM applications in materials science and chemistry: towards automation, assistants, agents, and accelerated scientific discovery', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4442.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4442.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemQA-multimodal-benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemQA: multimodal chemistry question-answering dataset and evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal benchmark (ChemQA) evaluating LLMs on five chemistry QA tasks (atom counting, molecular weight calculation, name conversion, molecule captioning, retrosynthesis) using both images and SMILES text and measuring accuracy across text-only, image-only, and combined modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>ChemQA multimodal accuracy evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Tasks are posed in image, text (SMILES), or combined image+text formats. Models are evaluated on discrete task-specific accuracy metrics (e.g., correct atom counts, correct molecular weight within tolerance, correct name conversion). Experiments compare performance across modalities and across models to quantify multimodal reasoning capability.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Task-specific accuracy (correctness), modality robustness (text vs. image vs. text+image), and comparative model performance.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Evaluated models include Claude-3-Opus, Gemini Pro, GPT-4 Turbo (and others in the broader benchmark work).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry (molecular reasoning and multimodal understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Multimodal reasoning and explanations for chemistry tasks (not full scientific theories but relevant for evaluating ability to produce accurate scientific explanations from visual and textual inputs).</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Models achieved higher accuracy when provided both text and images; image-only performance was substantially worse. Claude-3-Opus performed best on text-only tasks, while Gemini Pro and GPT-4 Turbo performed best in multimodal (text+image) settings. Exact accuracy numbers per task and model are reported in figure 12 of the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated — scoring against task-specific ground truth answers; human evaluation of explanations was not reported in this paper's summary.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Benchmarking against curated ground truth for the five QA tasks; not validated against human expert judgment of explanatory depth.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Image-only reasoning remains weak; current models require multimodal cues for high accuracy; benchmark focuses on correctness but not evaluation of the quality of mechanistic explanations or hypothesis formation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '32 examples of LLM applications in materials science and chemistry: towards automation, assistants, agents, and accelerated scientific discovery', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Scientific hypothesis generation by a large language model: laboratory validation in breast cancer treatment <em>(Rating: 2)</em></li>
                <li>A survey on hypothesis generation for scientific discovery in the era of large language models <em>(Rating: 2)</em></li>
                <li>Advancing the scientific method with large language models: from hypothesis to discovery <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks <em>(Rating: 1)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4442",
    "paper_id": "paper-280996792",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [
        {
            "name_short": "MultiAgent-Hypothesis-Verification",
            "name_full": "Multi-agent hypothesis generation and verification (RAG + Tree-of-Thoughts + Evaluation agents)",
            "brief_description": "A multi-agent pipeline that generates hypotheses from retrieved literature, deliberates candidate ideas via tree-of-thoughts reasoning, and automatically evaluates them with specialized evaluator agents for feasibility, utility, and novelty.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Multi-agent hypothesis generation and verification (RAG + Tree-of-Thoughts + evaluation agents)",
            "evaluation_method_description": "The pipeline first builds an embedding-indexed retrieval over a large corpus and uses Retrieval-Augmented Generation (RAG) to supply context to generative agents that propose hypotheses. Candidate hypotheses are then expanded/deliberated using a tree-of-thoughts style search to explore alternative formulations. Separate evaluator agents (often implemented as LLM prompts or chains) score or filter hypotheses along multiple axes (feasibility, utility, novelty). The process is iterative: retrieval informs generation, generation produces candidates, tree-of-thoughts explores alternatives, and evaluators prune/refine outputs. All steps are implemented as LLM-driven agents orchestrated within an agentic framework.",
            "evaluation_criteria": "Feasibility (compatibility with existing scientific knowledge and plausibility), Practical utility (applicability or usefulness for a research/engineering problem), Novelty (degree to which the hypothesis is new relative to retrieved literature); implicitly also internal coherence and syntactic validity.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Materials science (example: concrete/cement sustainability in the hackathon study)",
            "theory_type": "Generated research hypotheses for materials design (generative / testable scientific hypotheses)",
            "human_comparison": false,
            "evaluation_results": "Applied to ~66,000 abstracts for sustainable concrete design: the system generated 1,000 structured hypotheses; internal automated evaluation classified 243 as 'feasible', 175 as demonstrating 'practical utility', and 12 as 'highly novel'. No quantitative external/expert validation scores were reported in the paper beyond these counts.",
            "automated_vs_human_evaluation": "Automated — evaluation conducted by LLM-based evaluator agents (LLM-as-judge style) using retrieved text and prompt-based scoring; no systematic human expert rating dataset reported for those hypotheses in the hackathon description (human-in-the-loop was discussed as desirable for future validation).",
            "validation_method": "Internal filtering/consistency checks using retrieved literature as context; no reported correlation with independent expert judgments or experimental verification in this paper.",
            "limitations_challenges": "Relies on retrieval quality and coverage (missing literature biases scores); vulnerable to LLM hallucinations and prompt sensitivity; token/context length and RAG truncation can hide relevant counter-evidence; no external expert or experimental validation reported (so automated feasibility/novelty labels may be noisy); metric thresholds/decision rules for classifying feasibility/novelty not standardized.",
            "benchmark_dataset": "A local embedding index built from ~66,000 abstracts related to concrete (used as background corpus); no standardized benchmark dataset for hypothesis evaluation was used.",
            "uuid": "e4442.0",
            "source_info": {
                "paper_title": "32 examples of LLM applications in materials science and chemistry: towards automation, assistants, agents, and accelerated scientific discovery",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "LLM-as-a-judge",
            "name_full": "LLM-as-a-judge evaluation framework",
            "brief_description": "A prompting-based approach in which one or more LLM instances act as evaluators or judges that score or rank candidate hypotheses/explanations according to predefined criteria (e.g., feasibility, novelty).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "LLM-as-a-judge",
            "evaluation_method_description": "Candidate hypotheses or explanations are provided to an LLM (often with supporting retrieved evidence) along with an evaluation rubric encoded into the prompt; the LLM returns a classification, score, or justification (e.g., 'feasible/infeasible', novelty estimate, or a numeric score). This can be used as a standalone evaluator or as part of a multi-agent pipeline (e.g., multiple LLM 'judges' or ensemble judgment).",
            "evaluation_criteria": "Typically mirrors the rubric supplied in prompts: plausibility/feasibility, empirical support/coherence with literature, novelty relative to retrieved contexts, and sometimes clarity or specificity.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Used generically in the paper's pipelines; applied in examples within materials science (cement/concrete) and other hackathon projects.",
            "theory_type": "Testable research hypotheses and explanatory statements",
            "human_comparison": null,
            "evaluation_results": "The paper reports use of LLM-as-judge within the multi-agent pipeline to filter hypotheses; quantitative calibration or benchmarking of LLM-judge outputs against human experts is not reported in this manuscript.",
            "automated_vs_human_evaluation": "Automated (LLM-based scoring). The approach is amenable to hybrid workflows where human experts audit or re-score selected results, but such hybrid validation was not reported here.",
            "validation_method": "Not validated against independent human expert judgments in this work; validation was limited to internal consistency checks and counts of classifier outcomes.",
            "limitations_challenges": "LLM-judge outputs can reflect model biases and overconfidence; without external calibration against human experts or experiments, scores may not correspond to real-world correctness; prompt design and choice of supporting retrieved evidence strongly affect judgments.",
            "benchmark_dataset": "No dedicated benchmark for LLM-judge evaluation reported in this paper.",
            "uuid": "e4442.1",
            "source_info": {
                "paper_title": "32 examples of LLM applications in materials science and chemistry: towards automation, assistants, agents, and accelerated scientific discovery",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "F-U-N-eval",
            "name_full": "Feasibility / Utility / Novelty evaluation agents (F-U-N)",
            "brief_description": "A set of explicit evaluation criteria operationalized as separate evaluator agents that label or score hypotheses on Feasibility, Utility, and Novelty (abbreviated here as F-U-N).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Feasibility / Utility / Novelty (F-U-N) evaluator agents",
            "evaluation_method_description": "The system runs three (or more) evaluator agents, each focused on one criterion: feasibility (consistency with known science and practical plausibility), utility (potential practical impact or usefulness), and novelty (degree of difference from prior art as determined via retrieval). Each evaluator returns binary labels or graded scores that are then combined to prioritize hypotheses for follow-up.",
            "evaluation_criteria": "Feasibility (scientific plausibility and compatibility with current knowledge); Practical utility (application potential, measurability, or impact); Novelty (degree of originality vs. retrieved corpus).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Materials science (demonstrated on concrete sustainability hypotheses), generalizable to other scientific domains.",
            "theory_type": "Testable research hypotheses (mechanistic or design-oriented hypotheses)",
            "human_comparison": false,
            "evaluation_results": "From the hackathon pipeline: out of 1,000 generated hypotheses 243 were flagged as feasible, 175 as practically useful, and 12 as highly novel by these evaluator agents. No further external benchmarking was reported.",
            "automated_vs_human_evaluation": "Automated (agent-based LLM evaluation); the paper discusses future incorporation of human-in-the-loop validation but does not present it.",
            "validation_method": "Internal classification based on retrieved evidence; no inter-rater reliability or correlation with human experts was presented.",
            "limitations_challenges": "Binary/threshold decisions and scoring rules were not fully described; susceptibility to retrieval omissions and to LLM over/under-estimating novelty; lack of external calibration reduces confidence in automated labels.",
            "benchmark_dataset": "No canonical benchmark; applied to hypotheses generated from an embedding index of ~66k abstracts for the concrete domain.",
            "uuid": "e4442.2",
            "source_info": {
                "paper_title": "32 examples of LLM applications in materials science and chemistry: towards automation, assistants, agents, and accelerated scientific discovery",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "G-Peer-T",
            "name_full": "G-Peer-T: LLM Probabilities For Assessing Scientific Novelty and Nonsense",
            "brief_description": "A hackathon project (listed in the paper) that — by its title — aims to use LLM output probabilities to identify scientifically novel claims and flag nonsensical outputs.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "evaluation_method_name": "LLM probability-based novelty/nonsense scoring (G-Peer-T)",
            "evaluation_method_description": "Based on the project title and listing, the approach likely uses token or sequence probabilities (model confidences) from LLMs to estimate how plausible or 'in-distribution' a candidate claim is, flagging low-probability/high-surprise outputs as potential nonsense or novel claims (exact implementation details are not described in the paper's main text).",
            "evaluation_criteria": "Model output probability / perplexity as proxy for plausibility; novelty flagged by low prior-probability given model; nonsense detection via low-confidence tokens or incoherent sequences.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Not specified in main text (project listed among diverse hackathon submissions); general-purpose method to detect novelty/nonsense in scientific content.",
            "theory_type": "Assessing candidate assertions/hypotheses or claims for novelty and coherence",
            "human_comparison": null,
            "evaluation_results": "The paper lists the project in Table 1 but does not provide experimental results or metrics for G-Peer-T within the main manuscript.",
            "automated_vs_human_evaluation": "Presumably automated (probability-based heuristics); no hybrid or human evaluation reported in the paper.",
            "validation_method": "Not described in the paper; no reported correlation with human judgments or empirical tests.",
            "limitations_challenges": "No details provided in this manuscript; general limitations of probability-based novelty detection include sensitivity to model calibration and difficulty distinguishing true scientific novelty from hallucinated but low-probability nonsense.",
            "benchmark_dataset": "Not reported in this manuscript.",
            "uuid": "e4442.3",
            "source_info": {
                "paper_title": "32 examples of LLM applications in materials science and chemistry: towards automation, assistants, agents, and accelerated scientific discovery",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "MaSTeA-MaScQA-Eval",
            "name_full": "MaSTeA evaluation on MaScQA (materials science question set) dataset",
            "brief_description": "An evaluation framework that measures LLM performance on a curated undergraduate-level materials science question set (MaScQA) by extracting model outputs, verifying answers (regex/exact/acceptable-range checks), and computing accuracy per question type and topic.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "MaSTeA evaluation pipeline on MaScQA",
            "evaluation_method_description": "The team curated 650 questions (four structural types: MCQ, MATCH, MCQN, NUM) and evaluated models by parsing model outputs (regular expressions for MCQs), validating numeric responses within exact or tolerance ranges, and computing per-type and per-topic accuracy metrics. Results are presented by model and topic to identify strengths/weaknesses.",
            "evaluation_criteria": "Accuracy (per-question correctness), broken down by question type (MCQ, match, numeric) and topic (thermodynamics, atomic structure, mechanical behavior, etc.).",
            "model_name": "Multiple models tested (LLAMA3-8B, Claude variants including Opus, GPT-4, Haiku, Sonnet)",
            "model_size": "Examples include LLAMA3-8B (8B); Claude-3-Opus variant sizes not specified in the paper; GPT-4 (size not specified).",
            "scientific_domain": "Materials science education / knowledge assessment",
            "theory_type": "Not full theories — evaluates factual / reasoning answers and stepwise solutions (scientific question-answering and explanations).",
            "human_comparison": false,
            "evaluation_results": "Opus (Claude-3-Opus variant) consistently achieved the highest accuracy across most categories; GPT-4 also showed strong performance in several topics (e.g., material processing and fluid mechanics). Smaller LLaMA3-8B underperformed relative to larger models. Detailed per-topic accuracies are provided in Table 2 of the paper (numerical breakdowns per model and topic).",
            "automated_vs_human_evaluation": "Automated scoring against gold answers (regex/exact-match/tolerance checks); no human expert adjudication reported for borderline cases in the main text.",
            "validation_method": "Automatic comparison to labeled ground-truth answers in the MaScQA dataset; this provides an objective accuracy metric but does not assess explanation quality beyond correctness.",
            "limitations_challenges": "Dataset covers undergraduate-level questions and may not reflect advanced hypothesis-generation evaluation; automatic parsing of LLM outputs can fail for differently formatted answers; evaluation measures accuracy but not deeper explanatory fidelity or experimental testability.",
            "benchmark_dataset": "MaScQA (650 curated materials science questions) used by the MaSTeA project; dataset and interface produced during hackathon (paper references MaScQA and MaSTeA).",
            "uuid": "e4442.4",
            "source_info": {
                "paper_title": "32 examples of LLM applications in materials science and chemistry: towards automation, assistants, agents, and accelerated scientific discovery",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "ChemQA-multimodal-benchmark",
            "name_full": "ChemQA: multimodal chemistry question-answering dataset and evaluation",
            "brief_description": "A multimodal benchmark (ChemQA) evaluating LLMs on five chemistry QA tasks (atom counting, molecular weight calculation, name conversion, molecule captioning, retrosynthesis) using both images and SMILES text and measuring accuracy across text-only, image-only, and combined modalities.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "ChemQA multimodal accuracy evaluation",
            "evaluation_method_description": "Tasks are posed in image, text (SMILES), or combined image+text formats. Models are evaluated on discrete task-specific accuracy metrics (e.g., correct atom counts, correct molecular weight within tolerance, correct name conversion). Experiments compare performance across modalities and across models to quantify multimodal reasoning capability.",
            "evaluation_criteria": "Task-specific accuracy (correctness), modality robustness (text vs. image vs. text+image), and comparative model performance.",
            "model_name": "Evaluated models include Claude-3-Opus, Gemini Pro, GPT-4 Turbo (and others in the broader benchmark work).",
            "model_size": null,
            "scientific_domain": "Chemistry (molecular reasoning and multimodal understanding)",
            "theory_type": "Multimodal reasoning and explanations for chemistry tasks (not full scientific theories but relevant for evaluating ability to produce accurate scientific explanations from visual and textual inputs).",
            "human_comparison": false,
            "evaluation_results": "Models achieved higher accuracy when provided both text and images; image-only performance was substantially worse. Claude-3-Opus performed best on text-only tasks, while Gemini Pro and GPT-4 Turbo performed best in multimodal (text+image) settings. Exact accuracy numbers per task and model are reported in figure 12 of the paper.",
            "automated_vs_human_evaluation": "Automated — scoring against task-specific ground truth answers; human evaluation of explanations was not reported in this paper's summary.",
            "validation_method": "Benchmarking against curated ground truth for the five QA tasks; not validated against human expert judgment of explanatory depth.",
            "limitations_challenges": "Image-only reasoning remains weak; current models require multimodal cues for high accuracy; benchmark focuses on correctness but not evaluation of the quality of mechanistic explanations or hypothesis formation.",
            "uuid": "e4442.5",
            "source_info": {
                "paper_title": "32 examples of LLM applications in materials science and chemistry: towards automation, assistants, agents, and accelerated scientific discovery",
                "publication_date_yy_mm": "2025-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Scientific hypothesis generation by a large language model: laboratory validation in breast cancer treatment",
            "rating": 2,
            "sanitized_title": "scientific_hypothesis_generation_by_a_large_language_model_laboratory_validation_in_breast_cancer_treatment"
        },
        {
            "paper_title": "A survey on hypothesis generation for scientific discovery in the era of large language models",
            "rating": 2,
            "sanitized_title": "a_survey_on_hypothesis_generation_for_scientific_discovery_in_the_era_of_large_language_models"
        },
        {
            "paper_title": "Advancing the scientific method with large language models: from hypothesis to discovery",
            "rating": 2,
            "sanitized_title": "advancing_the_scientific_method_with_large_language_models_from_hypothesis_to_discovery"
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "rating": 1,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 1,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        }
    ],
    "cost": 0.018617,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>32 examples of LLM applications in materials science and chemistry: towards automation, assistants, agents, and accelerated scientific discovery
29 September 2025</p>
<p>Yoel Zimmermann 0009-0003-1720-4368
Adib Bazgir 0000-0001-6475-8505
Alexander Al-Feghali 0009-0004-8377-7049
Mehrad Ansari 0009-0004-8377-7049
Joshua D Bocarsly 0000-0001-5696-9193
Catherine Brinson 0000-0003-2551-1563
Yuan Chiang 0000-0002-4017-7084
Defne Circi 0000-0002-5761-0198
Min-Hsueh Chiu 0000-0003-0637-7856
Nathan Daelman 0000-0002-7647-1816
Matthew L Evans 0000-0002-7647-1816
Abhijeet S Gangan 0000-0002-1182-9098
Janine George 
Hassan Harb 0000-0001-8907-0336
Ghazal Khalighinejad 0009-0005-2476-8043
Sartaaj Takrim Khan 0009-0005-2476-8043
Sascha Klawohn 0000-0003-4850-776X
Magdalena Lederbauer 0000-0003-4850-776X
Soroush Mahjoubi 0009-0008-0665-1839
Bernadette Mohr 0000-0001-8879-5431
SeyedMohamad Moosavi 0000-0003-0903-0073
Aakash Naik 0000-0002-0357-5729
Aleyna Beste Ozhan 0000-0002-6071-6786
Dieter Plessers 0000-0002-0281-3860
PiyushAritra Roy 0000-0003-0243-9124
Fabian Schöppach 0000-0003-0243-9124
Philippe Schwaller 
Carla Terboven 0000-0003-3046-6576
Katharina Ueltzen 0009-0004-3786-0773
Yue Wu 0009-0003-2967-1182
Shang Zhu 0000-0003-2874-8267
Jan Janssen 0000-0002-8433-8599
Calvin Li 0000-0001-9948-7119
Ian Foster 
Ben Blaiszik blaiszik@uchicago.edu 0000-0002-5326-4902
Janine George Github 
Federico Ottomano 
Elena Patyukova 
Judith Clymo 
Dmytro Antypov 
Chi Zhang 
Ranjan Maharana 
Weijie Zhang 
Xuefeng Liu 
Erik Bitzek Github 
Anastasiia Tsymbal 
Oleksandr Narykov 
Dana O'connor 
Shagun Maheshwari 
Stanley Lo 
Archit Vasan 
Zartashia Afzal 
Kevin Shen Github 
Jan Weinreich 
Ankur K Gupta 
Amirhossein D Naghdi 
Alishba Imran 
Github Llmspectrometry 
Tyler Josephson 
Fariha Agbere 
Kevin Ishimwe 
Colin Jones 
Charishma Puli 
Samiha Sharlin 
Hao Liu Github 
Andres M Bran 
Anna Borisova 
Marcel M Calderon 
Mark Tropin 
Rob Mills 
Philippe Schwaller Github 
Mofs Mehrad Ansari 
Takrim Sartaaj 
Mahyar Khan 
SeyedMohamad Rajabi 
Amro Aswad Moosavi 
Github 
Alessandro Canalicchio 
Alexander Moßhammer 
Tehseen Rug 
Christoph Völker Github 
Yuan Langsim 
Giuseppe Chiang 
Greg Fisicaro 
Sarom Juhasz 
Bernadette Leang 
Utkarsh Mohr 
Francesco Pratiush 
Leopold Ricci 
Pablo A Talirz 
Trung Unzueta 
Gabriel Vo 
Sebastian Vogel 
Jan Pagel 
Janssen Github 
Marcel Schloz 
Jose C Gonzalez Github 
Chiku Parida 
Martin H Petersen Github 
Luis Pinto 
Xuan Vu Nguyen 
Tirtha Vinchurkar 
Pradip Si 
Kuman Suneel 
Mohd Zaki 
Github Llmy-Way 
Ruijie Zhu 
Faradawn Yang 
Andrew Qin 
Suraj Sudhakar 
Jaehee Park 
Victor Chen Github 
Viktoriia Baibakova 
Maryam G Fard 
Teslim Olayiwola 
Olga Taran Github 
Benjamin Charmes 
Vraj Patel 
Github Llmads 
Sarthak Kapoor 
José M Pizarro 
Ahmed Ilyas 
Alvin N Ladines 
Vikrant Chaudhary Github 
Bernadette Mohr Github 
Hampus Näsström 
Julia Schumann 
Michael Götte 
José A Márquez Github 
Marcus Schwarting Github 
Aleyna Beste Ozhan 
Github Activescience </p>
<p>1 ETH Zurich,
Zurich, Switzerland Columbia, MO, United States of America Quebec, Montreal, Canada Toronto, Ontario, Canada Durham, NC, United States of America Berkeley, CA, United States of America Berkeley, CA, United States of America Los Angeles, CA, United States of America Berlin, Germany Louvain-la-Neuve, Belgium Gozée, Belgium Jena, Germany Berlin, Germany Lemont, IL, United States of America Toronto, Ontario, Canada Boston, MA, United States of America Amsterdam, The Netherlands Leuven, Belgium London, United Kingdom Lausanne, Switzerland Berlin, Germany Ann Arbor, MI, United States of America Düsseldorf, Germany Boston, MA, United States of America Chicago, IL, United States of America Los Angeles, Los Angeles, CA, United States of America Houston, TX, United States of America</p>
<p>2 University of Missouri-Columbia,</p>
<p>3 McGill University,</p>
<p>4 Acceleration Consortium,</p>
<p>5 Duke University,</p>
<p>6 University of California at Berkeley,</p>
<p>7 Lawrence Berkeley National Laboratory,</p>
<p>8 University of Southern California,</p>
<p>9 Humboldt University of Berlin,</p>
<p>10 Université catholique de Louvain,</p>
<p>11 Matgenix SRL,</p>
<p>12 Friedrich-Schiller-Universität Jena,</p>
<p>13 Federal Institute of Materials Research and Testing (BAM),</p>
<p>14 Argonne National Laboratory,</p>
<p>15 University of Toronto,</p>
<p>16 Massachusetts Institute of Technology,</p>
<p>17 University of Amsterdam,</p>
<p>18 KU Leuven,</p>
<p>19 London South Bank University,</p>
<p>20 EPFL,</p>
<p>21 Helmholtz-Zentrum Berlin für Materialien und Energie GmbH,</p>
<p>22 University of Michigan-Ann Arbor,</p>
<p>23 Max-Planck Institute for Sustainable Materials,</p>
<p>24 Fum Technologies, Inc.,</p>
<p>25 University of Chicago,</p>
<p>26 University of California,</p>
<p>27 University of Houston,</p>
<p>28 Independent Researcher</p>
<p>MolFoundation: Benchmarking Chemistry LLMs on Predictive Tasks</p>
<p>Multi-Agent Hypothesis Generation and Verification through Tree of Thoughts and Retrieval Augmented Generation</p>
<p>32 examples of LLM applications in materials science and chemistry: towards automation, assistants, agents, and accelerated scientific discovery
29 September 2025BD01650CEC8A092403FBD96F89EA33B810.1088/2632-2153/ae011aRECEIVED 12 May 2025 REVISED 21 July 2025 ACCEPTED FOR PUBLICATION 29 August 2025LLMmaterials sciencechemistrymachine learningAI Harnessing Language Model for Density Functional Theory (DFT) Parameter Suggestion
Large language models (LLMs) are reshaping many aspects of materials science and chemistry research, enabling advances in molecular property prediction, materials design, scientific automation, knowledge extraction, and more.Recent developments demonstrate that the latest class of models are able to integrate structured and unstructured data, assist in hypothesis generation, and streamline research workflows.To explore the frontier of LLM capabilities across the research lifecycle, we review applications of LLMs through 32 total projects developed during the second annual LLM hackathon for applications in materials science and chemistry, a global hybrid event.These projects spanned seven key research areas: (1) molecular and material property</p>
<p>Introduction</p>
<p>The integration of large language models (LLMs) into scientific workflows is reshaping how researchers approach data-driven discovery [1][2][3][4], automation [5][6][7][8], and even scientific reasoning and hypothesis generation [9][10][11][12].In chemistry and materials science, fields characterized by complex data modalities, heterogeneous data formats, sparse experimental datasets, and fragmented knowledge ecosystems, LLMs are emerging as versatile tools capable of bridging gaps between computational methods, experimental data, literature and text sources, and domain expertise [13][14][15][16][17][18][19][20].Early applications have already demonstrated potential applicability in tasks ranging from molecular property prediction [21][22][23] to automated laboratory workflows [7,24], various combined scientific tasks (e.g.literature summarization, hypothesis generation) [25,26], and development of novel user interfaces [5,27].As illustrated in figure 1, we note that there is a significant opportunity for these broad new capabilities to be incorporated throughout the scientific research lifecycle; from initial ideation through experimental execution to communication, learning, and further iteration.</p>
<p>However, the rapidity of change and the nearly constant release of models with higher performance, lower cost, and wider application spaces, and release of other platform capabilities (e.g.agentic tools, deep research modalities) make it challenging to keep pace, necessitating a collaborative and interdisciplinary effort to identify high-impact use cases, address specific limitations, and prototype applications to catalyze deeper study [28][29][30][31][32][33][34].Towards this goal, we believe that accessing the wisdom of the crowd via science hackathons provides a powerful, and dynamic framework for fostering collaboration building, knowledge exchange, innovation, and incentivizing the rapid problem-solving and exploration needed to realize the benefit of these new models for scientific discovery in materials science and chemistry [35][36][37][38].</p>
<p>In this work, we describe and analyze select applications developed as part of the second LLM Hackathon for applications in materials science and chemistry [38], detailing the broad classes of problems addressed by teams and highlighting trends in the approaches taken.We categorize the 32 submissions into seven key research areas and provide an overview of team contributions with highlights drawn from exemplar projects in each research area.We also present a summary table containing team details and code repository links for all submissions to offer a comprehensive view of the innovations demonstrated during the event.</p>
<p>Finally, we discuss the broader conclusions of the hackathon, emphasizing its role in fostering interdisciplinary collaboration, accelerating the adoption of artificial intelligence (AI) in scientific research [35][36][37], and identifying key challenges that require further investigation.By examining these contributions, we provide insight into how structured collaborative frameworks can drive the systematic integration of LLMs into chemistry and materials science to accelerate research, improve researcher efficiency, and shape the future of AI-driven discovery.</p>
<p>Overview of submissions</p>
<p>The hackathon resulted in 32 teams submissions providing detailed descriptions, covering a broad spectrum of materials science and chemistry applications.The submissions and links to the respective source code repositories are listed in figure 1, whereas brief description for each project can be found in the table S1 of the electronic supporting information provided with this article.We categorized projects based on their primary objectives, clustering them into seven key areas, forming a constellation of new capabilities across the research lifecycle: 1.Molecular and material property prediction: predicting chemical and physical properties of molecules and materials using LLMs, particularly excelling in low-data environments and combining structured and unstructured data.2. Molecular and material design: generating and optimizing novel molecules and materials using LLMs, including peptides, metal-organic frameworks (MOFs), and sustainable construction materials.3. Automation and novel interfaces: developing natural language interfaces and LLM-powered automated workflows to simplify complex scientific tasks, making advanced tools and techniques more accessible to researchers.4. Scientific communication and education: enhancing academic communication, automating educational content creation, and supporting learning in materials science and chemistry.5. Research data management (RDM) and automation: streamlining the handling, organization, and processing of scientific data through LLM-powered tools and multimodal agents.6. Hypothesis generation and evaluation: using LLMs to generate, assess, and refine scientific hypotheses, leveraging multiple AI agents and statistical approaches.7. Knowledge extraction and reasoning: extracting structured information from scientific literature and performing sophisticated reasoning about chemical and materials science concepts through knowledge graphs and multimodal approaches.</p>
<p>Collectively, this constellation of capabilities, shown in figure 1, is applicable to long-standing challenges across the research lifecycle, creating a flywheel of improvements that promises to empower researchers with new capabilities and to accelerate the research process.</p>
<p>We next discuss the constellation of capabilities in more detail and highlight exemplar projects across each key application area.</p>
<p>Table 1.Overview of the tools developed by various teams, and links to source code repositories.Full descriptions of the projects can be found in [39].Reproduced from [39].CC BY 4.0.</p>
<p>Molecular and material property prediction</p>
<p>LLMs have rapidly advanced in molecular and material property prediction, employing both textual and numerical data to predict a wide range of properties.Recent studies [1, 3,21,23] show LLMs performing comparably to, or even surpassing, conventional machine learning methods, particularly in low-data environments.The flexibility in processing both structured and unstructured data [40], as well as their general applicability to regression tasks [41], make LLMs a powerful tool for diverse predictive tasks in molecular and materials science.</p>
<p>Leveraging orbital-based bonding analysis information in LLMs for material property predictions</p>
<p>Previous studies have used different strategies to learn material properties using LLMs, such as enriching graph neural network features with LLM embeddings [4], training domain-specific LLMs and customizing model architectures [13][14][15], or fine-tuning general-purpose LLMs [16,22].While exact strategies have differed, existing models predominantly operate on string representations of crystal structures primarily consisting of compositional and structural information commonly found in crystallographic information files.Multiple studies have successfully utilized the text descriptions of structures [15,16,22] that can be generated using the Robocrystallographer package [17].These descriptions consist of structural features like bond lengths, coordination polyhedra, lattice parameters, coordinates, structure type, and other descriptors.</p>
<p>Other studies explored different string representations of compositional and structural information [4,14,22].The team behind this submission emphasizes that, to their knowledge, no previous studies investigated including orbital-based bonding analysis information in LLMs for materials property prediction tasks.Thus, in this pilot study, the team tested including such descriptions in LLMs to predict the highest-frequency peak in their phonon density of states (DOSs) [18,19].This target is relevant to the thermal properties of materials and it is a tracked component of the MatBench benchmark project.A key hypothesis is that the inclusion of the bonding analysis information for this vibrational property will improve the LLM's performance, as previous studies demonstrated the importance of such bonding information for the same target via a Random Forest model [42].</p>
<p>To test this hypothesis, the team fine-tuned multiple Llama-3-8B (4bit) models on the textual description of 1264 crystal structures in the benchmark dataset.The text descriptions were generated using two packages: the Robocrystallographer and LobsterPy package [43].The text descriptions from LobsterPy consist of orbital-based bonding analyzes containing information on covalent bond strengths and antibonding states.The data used here is available on Zenodo [44] and was generated as part of an earlier dataset publication [42].</p>
<p>During the hackathon, one Llama-3-8B (4bit) model was fine-tuned with the Alpaca prompt format using both Robocrystallographer and LobsterPy text descriptions, and another one using solely Robocrystallographer input.Figure 2 depicts the prompt used to fine-tune an LLM to predict the last phonon DOS peak.The train/test/validation split was 0.64/0.2/0.16.The models were trained for 10 epochs with a validation step after each epoch.The textual output was converted back into numerical frequency values for the computation of mean absolute errors (MAEs) and root mean squared errors.The results show that including bonding-based information improved the model's prediction.The results also corroborate the team's previous finding that quantum-chemical bond strengths are relevant for this particular target property.Both model performances (Robocrystallographer: 44 cm −1 , Robocrystallographer + LobsterPy: 38 cm −1 ) are comparable to other models of the MatBench test suite, with MAEs ranging from 29 cm −1 to 68 cm −1 as per the time of writing [45].</p>
<p>Although the preliminary results seem promising, the models have not yet been exhaustively analyzed, validated, or optimized yet.As the prediction of a numerical value and not its text embedding is of interest to the task, further model adaptation might be beneficial.For example, Rubungo et al [15] modified T5, an encoder-decoder model [46], for regression tasks by removing its decoder and adding a linear layer on top of its encoder.Halving the number of model parameters allowed them to fine-tune on longer input sequences, improving model performance.A recently published benchmark for LLMs in materials property prediction also suggests that fine-tuning models with fewer parameters improves the prediction of materials properties [22].</p>
<p>With the available easy-to-use packages like Unsloth, [47] the team was able to integrate their materials data into fine-tuning an LLM for property prediction with very limited resources and time.Since these initial results, the work has been extended to a dataset of bonding-based text descriptions including 13 000 crystalline materials.In the future, the team aims to (1) test these text descriptions further to learn other thermal and elastic material properties like elastic constants and lattice thermal conductivity and (2) to extend further the text descriptions generated with the LobsterPy package to include, e.g.information on computed charges.</p>
<p>Molecular and material design</p>
<p>LLMs have also been applied to molecular and material design, proving capable in both settings [2,[48][49][50][51], especially if pre-trained or fine-tuned with domain-specific data [52].However, despite these advancements, LLMs still face limitations in practical applications [53].</p>
<p>Leveraging AI agents for designing low band gap MOFs</p>
<p>MOFs are known to be excellent candidates for electrocatalysis due to their large surface area, high adsorption capacity at low CO 2 concentrations, and the ability to fine-tune the spatial arrangement of active sites within their crystalline structure [54].Low band gap MOFs are crucial as they efficiently absorb visible light and exhibit higher electrical conductivity, making them suitable for photocatalysis, solar energy conversion, sensors, and optoelectronics.This submission aims at using chemistry-informed ReAct [55] Agents to optimize the band gap property of MOFs.The overview of the workflow is presented 3(a).The agent takes as inputs a textual representation of the initial MOF structure as a simplified molecular input line-entry system (SMILES) [56][57][58] string representation, and a short description of the property optimization task (i.e.reducing band gap), all in natural language.This is followed by an iterative closed-loop suggestion of new MOF candidates with a lower band gap with uncertainty quantification, by adjusting the initial MOF given a set of design guidelines automatically obtained from the scientific literature.This approach aligns with recent developments in multi-agent systems that can automate iterative scientific discovery processes, from hypothesis generation to experimental validation [25].A detailed analysis of this methodology, including its application to various classes of materials such as surfactants, ligands, and peptides can be found in [59], which supports both closed-loop and human-in-the-loop feedback cycles and thus enables real-time property inference for human-AI collaboration in molecular design.</p>
<p>The agent, powered by an LLM, is augmented with a set of tools allowing for chemistry-informed decision-making.These tools are as follows:</p>
<ol>
<li>Retrieval-augmented generation (RAG): this tool allows the agent to obtain design guidelines on how to adapt the MOF structure from unstructured text.Specifically, in this prototype, the agent has access to a fixed set of 7 MOF research papers (see references [60][61][62][63][64][65][66]) as PDFs.This tool is designed to extract the most relevant sentences from papers in response to a given query.It works by embedding both the paper and the query into numerical vectors using OpenAI's text-ada-002 [67], then identifying the top k passages within the document that either explicitly mention or implicitly suggest the adaptations required for the specified band gap property for a MOF.Inspired by the team's earlier work [68], k is set to 9, but is dynamically adjusted based on the relevant context's length to avoid OpenAI's token limitation.2. Surrogate band gap predictor the surrogate model used is a transformer (MOFormer [69]) that takes as input the MOF as a SMILES string.This model is pre-trained using a self-supervised learning technique known as Barlow-Twin [70], where representation learning is done against structure-based embeddings from a crystal graph convolutional neural network [71].This was done against 16 000 BW20K entries [72].The pre-trained weights are then transferred and fine-tuned to predict the band gap labels taken from 7450 entries from the QMOF database [73].From a 5-fold training, an ensemble of five transformers are trained to return the mean band gap and the standard deviation, which is used to assess uncertainty for predictions.For comparison, the team's transformer's MAE is approximately 0.467, whereas MOFormer, which was pre-trained on 400 000 entries, achieves an MAE of approximately 0.387.3. Chemical feasibility evaluator this tool primarily uses RDKit [74] to convert a SMILES string into an RDKit Mol object, and performs several validation steps to ensure chemical feasibility.First, it parses the SMILES string to confirm correct syntax.Next, it validates the atoms and bonds, ensuring they are chemically valid and recognized.It then checks atomic valences to ensure each atom forms a reasonable number of bonds.For ring structures, RDKit verifies the correct ring closure notation.Additionally, it adds implicit hydrogens to satisfy valence requirements and detects aromatic systems, marking relevant atoms and bonds as aromatic.These steps collectively ensure the molecule's basic chemical validity.</li>
</ol>
<p>The team has used OpenAI's GPT-4 [34] with a temperature of 0.1 as the preferred LLM and LangChain [75] for the application framework development (nonetheless, the team confirms that the choice of LLM is only a hyperparameter and other LLMs can drive the agent).</p>
<p>The new MOF candidates and their corresponding inferred band gap are represented in figure 3(b).The agent starts by retrieving the following design guidelines for low band gap MOFs from research papers: 1. Increasing the conjugation in the linker.2. Selecting electron-rich metal nodes.3. Functionalizing the linker with nitro and amino groups.4. Altering linker length.5. Substitute functional groups (i.e.substituting hydrogen with electron-donating groups on the organic linker).Note that the metal node adaptations are restrained by simply changing the system input prompt.The agent iteratively implements the above strategies, makes changes to the initial MOF, and suggests a new SMILES.The new SMILES is validated using the chemical feasibility evaluator tool, and if found invalid, the agent uses a self-correction feedback loop to suggest new candidates, accounting for the extracted design guidelines.After each valid modification, the band gap of the new MOF is then assessed using the fine-tuned ensemble of surrogate MOFormers to ensure a lower band gap.The self-correction feedback loop also handles new MOFs with undesired higher band gaps with respect to the initial MOF, by reverting to the most recent valid MOF candidate with the lowest band gap identified throughout the iterations.</p>
<p>Automation and novel interfaces</p>
<p>LLMs are increasingly important to the modern scientific workflow, enabling the development of more intuitive interfaces for users dealing with complex digital tools.For example, platforms such as ChemCrow [5], RestGPT [76], and HoneyComb [27] allow researchers to input commands in natural language to interact with and analyze complex software and databases.With LLMs, democratized access and dramatically simpler interfaces are possible for programs like specialized computational techniques or command-line interfaces that may previously have required deep expertise.LLMs excel at autonomous planning and task execution in multistep scenarios [24] by breaking complex processes into smaller actions, making experimental or computational workflows controllable by models with less need for direct oversight.Such behavior may include but is not limited to: simple interaction with laboratory robotic systems [7,77], where difficult scientific objectives can be converted into precise, callable commands: the basis of precision and consistency.The integration of LLMs and robotics promises to improve operational efficiency and enable new designs of experimental workflows with increased flexibility.</p>
<p>LangSim-LLM interface for atomistic simulation</p>
<p>LLMs can augment scientists with their common workflows, dramatically simplifying the interactions across systems using natural language input to understand and implement the intent of the user.The LangSim project [78] prototyped an interface to showcase the ability of LLMs to autonomously start atomistic simulations to study material properties on an atomistic scale (figure 4).This provides the LLM with a way to request and then use novel scientific data and insights that were previously not available in published databases.One might imagine, e.g. the on-the-fly calculation of defect properties, e.g. as grain boundary segregation energies remain prohibitive for high-throughput screening given the billions of possible combinations [79].In addition, by integrating the LLM in the active learning cycle of an autonomous materials discovery loop, with the option to calculate different material properties and access existing databases, the LLM becomes an AI scientist on a quest to discover novel materials.In this project, straightforward atomistic simulation and agentic scientific reasoning were explored as a natural language interface to users without programming skills.</p>
<p>The LangSim project implements atomistic simulation agents based on both pyiron [80] and LangChain [75].LangChain enables the LLM to call any kind of Python function and include the output in the thought process of the next iteration.In the case of LangSim, these Python functions represent simulation workflows implemented in the pyiron [80] workflow framework to calculate material properties with atomistic simulations.By restricting the LLM to pre-defined simulation workflows, the risk of hallucination is reduced compared to generative approaches, which request the LLM to define and generate the simulation workflow.Based on the MACE [81] foundation model for atomistic simulation, LangSim was used to predict the binary concentration of solid solution alloy required to match a user-defined bulk modulus, demonstrating an inverse materials design approach to enable application-specific alloy design.</p>
<p>LLMicroscopilot: assisting microscope operations through LLMs</p>
<p>While the state-of-the-art microscopes in materials science are critical for high-resolution imaging and analysis, they are still primarily operated by humans due to their complex interfaces and high cost.Their manipulation requires extensive training and involves delicate tasks, involving precision alignment for optimal performance, and shifting between different operational modes to address various research questions.These qualities have not only slowed down routine experimental procedures but have also created impediments to broadening access and allowing an acceleration of scientific microscopy.With progress in natural language processing, LLMs promise to unlock new capabilities in this landscape.For example, integration of LLMs to the microscope interface will allow complex operations to be done through natural language commands.Similar to modern chatbots, which allow even those with no programming knowledge to generate complex computer programs [82], LLMs can become intuitive intermediaries assisting users in traversing the manifold control procedures of advanced microscopes.Early studies of scanning probe microscopy have shown that LLMs can facilitate remote access [83] and even direct control [84] of these instruments, lessening the workload for expert operators.A promising approach is to use an LLM agent that accesses and operates well-defined external tools.These agents interpret user commands and use observations in near real time to make decisions, reduce hallucinations, or incorrect outputs, that may appear with a standalone LLM.This would streamline the user experience, further relieving researchers from having to navigate through complex, tool-specific APIs, thus broadening the reach of advanced microscopes, especially to non-experts.An illustration of such is the work performed by the LLMicroscopilot team, an LLM-based agent partially automating the operation of a scanning transmission electron microscope.The LLMicroscopilot prototype (figure 5) combines a generally trained foundation model, which is then tailored to specific domains through dedicated control tools.This agent operates by utilizing the API for a microscope experiment simulation tool [85], by performing such tasks as estimating experimental parameters and executing the actual experiments.Thus, these automations reduced dependence on personnel trained in operating such systems, thus increasing the opportunities for wider engagement in materials science due simplified usability.In the future, new developments may involve e.g.integrating open-source microscope hardware control tools [86] and including capabilities for database access.As such, the system will be able to utilize RAG techniques to further inform parameter estimation and aid data analysis.This will allow researchers to integrate LLMs in user interfaces for high-end microscopes and, instead of working on tedious, routine operational tasks, invest their energy in high-level scientific research and innovation, democratizing access to advanced experimental techniques.</p>
<p>Scientific communication and education</p>
<p>LLMs are transforming how scientific and educational content is created and shared, enhancing accessibility and personalized learning [87][88][89][90].By automating tasks like question generation, feedback, and grading, LLMs streamline educational processes, freeing educators to focus on individual learning needs.Additionally, LLMs assist in translating complex scientific findings into accessible formats, broadening public engagement [90].However, technological readiness, transparency, and ethical concerns around data privacy and bias remain critical challenges to address [87,89].</p>
<p>MaSTeA</p>
<p>This team selected 650 questions from the materials science question answering dataset MaScQA [91], requiring undergraduate-level understanding to solve.These questions are classified into four types based on their structure: multiple choice questions (MCQs), match the following (MATCH), numerical questions (NUMs) with given options (MCQN), and NUMs.MCQs are generally conceptual, with four options, where mostly one is correct, though occasionally multiple answers are valid.MATCH questions involve two lists of entities that need to be correctly paired, with four answer choices provided, one of which contains the correct set of matches.MCQN questions present a numerical problem with four answer choices, requiring a solution to identify the correct option, while NUM questions have numerical answers rounded to the nearest integer or floating-point number as specified.The team aimed to automate the evaluation of open-source and proprietary LLMs on MaScQA and develop an interactive interface for students to engage with these questions.Various models, including LLAMA3-8B, Haiku, Sonnet (claude-3-sonnet-20 240 229), GPT-4, and GPT-4 also demonstrated strong performance, particularly in material processing and fluid mechanics.As expected from prior studies, larger models such as OPUS and GPT-4 outperformed the smaller LLAMA3-8B, reinforcing the significance of model size in performance [92].The results suggest that there is significant room for improvement to enhance the accuracy of language models in answering scientific questions.</p>
<p>The evaluation involved:</p>
<p>• Extracting corresponding values: for MCQs, correct choices were identified using regular expressions and compared to model predictions.• Prediction verification: numerical predictions were validated against exact or acceptable ranges, while MCQ responses were matched to correct answer choices.• Calculating accuracy: accuracy was computed per question type and topic, followed by an overall assessment across all questions.</p>
<p>The evaluation results, summarized in figure 2, show that the Opus variant of Claude consistently outperformed other models, achieving the highest accuracy in most categories.GPT-4 also demonstrated strong performance, particularly in material processing and fluid mechanics.As expected from prior studies, larger models such as Opus and GPT-4 outperformed the smaller LLAMA3-8B, reinforcing the significance of model size in performance [92].The results suggest that there is significant room for improvement to enhance the accuracy of language models in answering scientific questions.The interactive web app, MaSTeA, developed using Streamlit, allows easy model testing to identify LLMs' strengths and weaknesses in different materials science subfields.The interface can be seen in figure 6.</p>
<p>With MaSTeA, the team demonstrated the potential of interactive tools to help students practice answering questions and learn the steps to reach the correct solution.By evaluating LLM performance, the goal was to guide future model development and identify areas for improvement.The results suggest that LLMs can benefit from strategies such as self-consistency [93] and RAG [94], which have been shown to reduce hallucinations and increase accuracy.Additionally, integrating advanced reasoning models could further improve performance.Recent advancements in domain-specific LLMs, such as LLaMat [95], highlight the potential of specialized training to enhance scientific reasoning.</p>
<p>RDM and automation</p>
<p>Various submissions were received that attempt to enhance the management, accessibility, and automation of scientific data workflows using LLMs.These efforts, often leveraging multimodal agents, aim to simplify complex data handling, improve reproducibility, and accelerate insights across diverse scientific disciplines.We highlight two exemplar projects: 'yeLLowhaMmer' a multimodal LLM (MLLM)-based data management agent that automates data handling within electronic lab notebooks (ELNs) and laboratory information management systems (LIMS), and 'NOMAD Query Reporter' , an LLM-based agent that uses RAG to generate context-aware summaries from large materials science repositories like NOMAD [96]</p>
<p>YeLLowhaMMer: a multi-modal tool-calling agent for accelerated RDM</p>
<p>As scientific data continues to grow in volume and complexity, there is a need for tools that can simplify the job of managing this data to draw insights, increase reproducibility, and accelerate discovery.Digital systems of record, such as ELNs or LIMS, have been a great advancement in this area.However, capturing data using, e.g.ELNs or LIMS is laborious, or simply impossible, to accomplish using graphical user interfaces alone.Recent advances in AI present an opportunity to augment how researchers interact with their data, improving scientific data management and allowing scientists to ask scientific questions of these data sources in new ways.</p>
<p>YeLLowhaMmer explored how LLMs can be used to simplify and accelerate data handling tasks in order to generate new insights, improve reproducibility, and save time for researchers using the open-source datalab [97] ELN/LIMS.Previously, the team had made progress toward this goal by developing a conversational assistant, Whinchat [38], that allows users to ask questions about their data.However, this assistant was unable to take action with a user's data or seek additional information as is often needed for scientific tasks.Thus, the team developed yeLLowhaMmer (figure 7) as a MLLM-based data management agent capable of taking free-form text and image instructions from users and executing a variety of complex scientific data management tasks.</p>
<p>The agent is powered by commercial MLLMs used within an agentic framework capable of iteratively writing and executing Python code that interacts with datalab instances via the datalab-api package.In typical usage, a yeLLowhaMmer user might instruct the agent: 'Pull up my 10 most recent sample entries and summarize the synthetic approaches used.'In this case, the agent will attempt to write datalab python API code to query for the user's samples in the datalab instance and write a human-readable summary based on the result.If the code it generates gives an error (or does not give sufficient information), the agent can iteratively rewrite the program until the task is accomplished successfully.Importantly, this paradigm is enabled by the presence of a structured API for diverse forms of scientific data; which is provided by datalab in its open-source schemas and API documentation.</p>
<p>In developing yeLLowhaMmer, the team found that simply copying documentation for the new datalab-api package into the system prompt produced poor code.Creating a simplified version with concrete examples and abridged JSON Schema formats proved more effective.The 12 000-character prompt (ca.3200 tokens) works well with modern large context models like Claude 3 Haiku.Future scientific libraries might benefit from maintaining both standard documentation and condensed 'agents.txt'files optimized and standardized for loading the most important information into LLM context.</p>
<p>This work shows the opportunity to integrate more tightly into scientific data management workflows to allow researchers to quickly handle complex tasks and efficiently ask questions of all collected data.An important challenge is to find ways to ensure that data curated or modified by such agents will be appropriately 'credited' by, for example, visually demarcating AI-generated content, and providing UI pathways for human users to verify or relabel such data in an efficient manner.Finally, recent progress in MLLM's ability to handle audio and video content in addition to text and images will allow agents to use audiovisual data in real time to provide even more comprehensive user interfaces.</p>
<p>NOMAD query reporter: automating research data narratives</p>
<p>RDM in materials science includes a wide variety of schemas and data structures.Databases such as NOMAD [96,98] support extensible context-aware schemas.Hence, the results of a single query may in fact contain various schemas, complicating the data analysis process.NOMAD Query Reporter is a proof-of-concept application built to produce a written summary of the common methodological parameters and standout results in a scientific style.These may serve as the first step in an analysis workflow, or as progenitors of a journal article's 'methods' section.</p>
<p>Given the large size -over 19 million entries-and dynamic nature -open public uploads-of the NOMAD database, retraining or fine-tuning strategies are challenging.Instead, this prototype implements a RAG approach, as defined by Gao et al [99], to enrich Llama3 (70B version) model's [100] knowledge base.The team progressively fed data by field into the LLM's chat-completion API as context.Subsequently, the construction of the summary was completed by topic (i.e.properties, techniques, material composition) in a multi-turn conversation style with the 'roles' feature clearly distinguishing the LLM's tasks from the data provided.Alignment with earlier versions of the chat history is enforced both via low-temperature settings as well as prompt engineering.For a step-by-step overview, see figure 8.</p>
<p>This work highlights the ability of LLMs to augment RDM systems via returning information in formats that are easily understandable by users.While the prototype NOMAD Query Reporter as able to manage homogenized hits well, attempts at extending to manually annotated, heterogeneous data from ELNs proved challenging.Thus, follow-up work should consider more performant models and advanced RAG and other strategies to improve model context.</p>
<p>Hypothesis generation and evaluation</p>
<p>LLMs can be leveraged to streamline scientific inquiry, hypothesis generation, and verification.Recent work across psychology, astronomy, and biomedical research demonstrates their capacity to generate novel, validated hypotheses by integrating domain-specific data structures like causal graphs [11,[101][102][103][104].Although still largely untapped in chemistry and materials science, this approach holds substantial promise for accelerating discovery and innovation in these fields [105][106][107].</p>
<p>Multi-agent hypothesis generation and verification through tree of thoughts and retrieval augmented generation</p>
<p>Scientific discovery thrives on the ability to generate and evaluate new hypotheses efficiently.However, the process of forming meaningful and testable hypotheses often requires extensive background research, domain knowledge, and iterative refinement.Advances in LLMs offer an opportunity to assist researchers in streamlining this process, particularly through structured, multi-agent frameworks that systematically generate, evaluate, and refine ideas.</p>
<p>The thoughtful beavers team (Soroush Mahjoubi, Aleyna B. Ozhan) designed a multi-agent system to enhance scientific inquiry in materials science.Similar systems have proven useful in social sciences [108], and the system was adapted specifically for hypothesis generation in the domain of cement and concrete.The system consists of specialized agents that work in tandem: retrieving background knowledge, generating inspirations, formulating hypotheses, and evaluating their feasibility, utility, and novelty.By leveraging a combination of RAG, tree-of-thoughts reasoning [109], and LLM-as-a-judge frameworks, this pipeline, which is illustrated in figure 9, ensures that only the most promising hypotheses emerge from the process.</p>
<p>To test this pipeline, the authors focused on sustainability challenges in concrete design.By processing 66 000 abstracts related to the field, an embedding-based retrieval system was built to extract relevant insights and generate research questions.From this dataset, the approach produced 1000 structured Figure 9. Multi-agent hypothesis generation and verification framework.The system uses retrieval-augmented generation, tree of thoughts, and feasibility, utility, and novelty evaluation agents to generate and refine hypotheses for sustainable concrete design.hypotheses, which were then subjected to rigorous evaluation.The results showed that 243 hypotheses were deemed feasible based on current scientific knowledge, 175 demonstrated practical utility, and 12 stood out as highly novel.</p>
<p>Looking ahead, this framework can be adapted to other material systems or even cross-disciplinary applications.By adjusting the background retrieval process, researchers could apply this method to areas such as ceramics, composites, or biomedical materials.Additionally, cross-pollination of ideas between domains could inspire new lines of research.As LLM capabilities continue to evolve, integrating AI-assisted hypothesis generation with expert validation could significantly accelerate scientific progress while maintaining the critical role of human creativity in innovation.</p>
<p>Knowledge extraction and reasoning</p>
<p>Extraction of structured scientific knowledge from unstructured text using LLMs to assisting researchers in navigating complex academic content is of wide interest [110][111][112][113].These systems streamline tasks like named entity recognition and relation extraction, offering flexible solutions tailored to materials science and chemistry [111].Tool-augmented frameworks help LLMs address complex reasoning by leveraging scientific tools and resources, expanding their utility as assistants in scientific research [114].</p>
<p>ActiveScience</p>
<p>Extracting and refining knowledge in hard sciences is crucial.While LLMs excel in summarization and dialogue generation, they are also prone to generating false information, a phenomenon known as hallucination.This presents a significant challenge for researchers leveraging LLMs in scientific fields.Various strategies exist to mitigate hallucinations.One approach involves fine-tuning models or constructing additional lightweight models after pretraining, but these methods require substantial computational resources, making them impractical in many cases.A more accessible alternative is RAG, which enhances LLMs by incorporating external information.Conceptually, if a fine-tuned model resembles a domain expert with deep knowledge, a pre-trained model is akin to a generalist with broad understanding.By supplying additional context, pre-trained models can generate more accurate and reliable outputs.To address this challenge, Min-Hsueh Chiu introduced an automated framework ActiveScience that leverages LLMs to ingest scientific articles into a knowledge graph and enable natural language queries for domain knowledge extraction.The framework integrates three key components: a data source API, a LLM, and a graph database.While these components can be replaced with equivalent technologies, this work specifically utilizes the ArXiv API [115], GPT-3.5 Turbo [33], and Neo4j [116].</p>
<p>For structured representation of knowledge and relationships, ActiveScience employs an ontology that defines key entities such as application, property, material, element, and metadata.The ontology design is adaptable and scalable to specific use cases.ActiveScience constructs its knowledge graph by extracting relevant triples from scientific articles.Specifically, prompts are generated using the predefined ontology and the introduction sections of articles to produce Cypher import statements containing structured triples, such as (Material: 'Nanowire') -[HAS_ELEMENT] -(Element: 'Aluminum') and (Material: 'Nanowire') -[HAS_FORMULA] -(Formula: 'Al-Si alloy').These triples are then imported into a Neo4j graph database.To facilitate RAG, the GraphCypherQAChain module from LangChain is employed.For instance, given the query, 'Retrieve the top three reference URLs where the property contains 'opti'?',GraphCypherQAChain dynamically generates a cypher query based on the predefined ontology schema, executes it within Neo4j, and returns the relevant results.The processes of query generation and natural language processing are handled by LLMs.The pipeline and output are illustrated in figure 10.</p>
<p>GlossaGen</p>
<p>Academic literature, particularly review articles and grant applications, would substantially benefit from the inclusion of comprehensive glossaries elucidating complex terminology and discipline-specific nomenclature.However, the manual generation of such reference materials is a labor-intensive and redundant process.To address this limitation, Lederbauer et al developed GlossaGen, which leverages LLMs to automate the creation of glossaries for academic articles and grant proposals, eliminating the need for time-consuming manual compilation.To efficiently process PDF or TeX articles, a pre-processing step automatically extracts the title and DOI, and chunks the text into smaller, context-preserving sections for LLM analysis.LLMs such as GPT-3.5-Turbo[33] and GPT-4-Turbo [32] then identify and define scientific terms with the help of Typed Predictors [117] and chain-of-thought [118] prompting, ensuring well-structured, contextually relevant, and accurate outputs.The generated glossary is not merely presented as a list of terms but also as an ontology-based knowledge graph using Neo4J [116] and Graph Maker [119], visualizing the intricate relationships between various technical concepts (figure 11).A user-friendly interface prototype, developed with Gradio [120], enables seamless interaction and customization, making the system accessible to researchers.</p>
<p>Future enhancements could focus on improving glossary output through LLM fine-tuning, integrating RAG, and enabling article image parsing.Additionally, the system can better support users by allowing them to input specific terms for glossary explanations, ensuring comprehensive coverage even when LLMs omit key concepts.Overall, GlossaGen's rapid development and promising capabilities highlight the potential of LLMs to assist researchers in their scientific outreach.</p>
<p>ChemQA</p>
<p>Foundation models exhibit strong capabilities across common chemistry tasks [1] and even reasoning [12], yet their performance across different input modalities-text, images, and their combination, remains underexplored.Building upon prior benchmarks such as IsoBench [121] and ChemLLMBench [122], the VizChem team (Khalighinejad et al) introduced ChemQA [123], a multimodal question-answering dataset designed to assess multimodal chemistry performance in language models.</p>
<p>ChemQA comprises five distinct QA tasks: atom counting, molecular weight calculation, name conversion, molecule captioning, and retrosynthesis planning.Each task is formulated with both molecular images and textual SMILES representations, enabling a systematic study of multimodal reasoning in chemistry.</p>
<p>The evaluation results, shown in figure 12, reveal that the models achieve higher accuracy when provided with both text and images, while the performance drops significantly with image-only inputs.Notably, Claude 3 Opus demonstrates superior performance in text-based tasks, whereas Gemini Pro and GPT-4 Turbo excel in multimodal settings [34,124,125].These findings highlight the limitations of current models in processing visual chemistry data independently.</p>
<p>By introducing ChemQA, the VizChem team underscored the need for enhanced multimodal reasoning in chemistry.Future work should focus on improving the integration of textual and visual representations to advance AI-driven scientific analysis.</p>
<p>Hackathon event overview</p>
<p>The second annual LLM hackathon for applications in materials science and chemistry was held on 9 May, 2024, bringing together a global network of researchers, students, and industry professionals.With 556 registered participants and over 120 active contributors forming 34 teams, the event spanned multiple time zones and research domains, underscoring the broad interest in applying LLMs to scientific discovery (figure 13).This hackathon built on the success of the previous year's event, described in detail in [38].The hybrid format included physical hubs in Toronto, Montreal, San Francisco, Berlin, Lausanne, and Tokyo, fostering interdisciplinary collaboration across institutions and time zones.The event began with a kickoff panel featuring experts Elsa Olivetti (MIT), Jon Reifsneider (Duke), Michael Craig (valence laboratories), and Marwin Segler (Microsoft), who discussed the evolving role of LLMs in scientific research.Beyond technical contributions, the hackathon fostered a global research community, with 483 researchers continuing collaborations via Slack.The event demonstrated the value of structured collaboration in accelerating AI-driven discovery and bridging computational scientists, experimentalists, and AI researchers.</p>
<p>Conclusion</p>
<p>The LLM Hackathon for applications in materials science and chemistry has demonstrated the dual utility and immense promise of LLMs to impact materials science and chemistry research across the entire lifecycle.Together, the projects (1) demonstrate the promise of a new set of tools that together form a cohesive patchwork to perform tasks ranging from hypothesis generation to data extraction, novel interface design, analysis of results, and more; and (2) showcase the ability of LLMs to enable rapid prototyping and exploration of the application space.Participants effectively utilized LLMs to explore solutions to specific challenges while rapidly evaluating their ideas over just a short 24 h period, highlighting compelling abilities to enhance the efficiency and creativity of research processes across many applications.It's important to note that many projects benefited from significant advancements in LLM performance since the previous year's hackathon.That is, the performance across the application space was improved simply via the release of more powerful versions of Gemini, ChatGPT, Claude, Llama, and other models and more easily accessible APIs and examples.If this trend continues, we expect to see even broader applications in subsequent hackathons and in materials science and chemistry more generally.We note that reliance on proprietary APIs raises reproducibility concerns as models evolve or are deprecated, while infrastructure demands for training, fine-tuning, or running inference on models with parameters reaching hundreds of billions require yet more computational resources, leading to significant infrastructure roadblocks to further academic work.</p>
<p>Importantly, the hybrid hackathon format itself proved to be an effective mechanism to foster interdisciplinary collaboration, accelerate the prototyping of AI-driven tools, and create a global community of researchers engaged in exploring LLM applications.The hybrid format, combining physical hubs with virtual participation, facilitated knowledge exchange across continents, highlighting the importance of accessible, multimodal, and scalable approaches to scientific innovation.</p>
<p>Figure 1 .
1
Figure 1.The LLM-Powered Research Constellation.At each stage of the research process, from initial ideation through experimental execution and communication of results, LLMs provide a constellation of capabilities spanning hypothesis generation, property prediction, novel interfaces, education, material design, automation, data management, scientific communication, and more.This constellation demonstrates the possibility of LLMs and multimodal models to drive a more efficient, rapid, and creative scientific discovery process through integrations across the research lifecycle.</p>
<p>Figure 2 .
2
Figure 2. Schematic depicting the prompt for fine-tuning the LLM with Alpaca prompt format.</p>
<p>Figure 3 .
3
Figure 3. Workflow overview.The react agent looks up guidelines for designing low band gap MOFs from research papers and suggests a new MOF (likely with a lower band gap).It then checks the validity of the new SMILES candidate and predicts the band gap with epistemic uncertainty estimation using an ensemble of surrogate fine-tuned MOFormers.b.Band gap predictions for new MOF candidates as a function of agent iterations.</p>
<p>Figure 4 .
4
Figure 4. LangSim framework for atomistic simulation and inverse design.Custom atomistic modeling tools (such as pyiron, ASE python package functions with underlying EMT and MACE-MP-0 forcefields) are integrated using LangChain @tool decorator.Pydantic model is used to exchange atomic information in a structured format between LLM and tools.The emerging agentic capability for inverse alloy design is demonstrated.LLM agent is able to find the target composition of Cu-Au alloy with the desired bulk modulus.</p>
<p>Figure 5 .
5
Figure 5. Schematic overview of the LLMicroscopilot assistant.The microscope user interface allows the user to input queries, which are then processed by the LLM.The LLM executes appropriate tools to provide domain-specific knowledge, support data analysis, or operate the microscope.</p>
<p>Figure 6 .
6
Figure 6.MaSTeA interface demonstrating a numerical question task.The model arrives at the correct answer by reasoning through the problem, providing students with a step-by-step solution if they struggle to solve it independently.</p>
<p>Figure 7 .
7
Figure 7.The yeLLowhaMmer multimodal agent can be used for a variety of data management tasks.Here, it is shown automatically adding an entry into the datalab lab data management system based on an image of a handwritten lab notebook page.</p>
<p>Figure 8 .
8
Figure 8. Flowchart of the query reporter usage, including the back-end interaction with external resources,i.e.NOMAD and Llama.Intermediate steps managing hallucinations or token limits are marked in red and orange, respectively.</p>
<p>Figure 10 .
10
Figure 10.ActiveScience framework for knowledge extraction.The system combines ontology-driven prompts, large language models, and a Neo4j knowledge graph to enable natural language queries and RAG for scientific research insights.Additionally, a code snippet demonstrating the use of LangChain is shown.</p>
<p>Figure 11 .
11
Figure 11.Schematic overview of the GlossaGen project.Textual information is extracted from PDF and LaTeX files and a glossary is generated with terms and their definition.From this, a knowledge graph is created, showing entities and relationships between terms.</p>
<p>Figure 12 .
12
Figure 12.Performance of Gemini Pro, GPT-4 Turbo, and Claude3 Opus on text, visual, and text+visual representations.The plot shows that models achieve higher accuracy with combined text and visual inputs compared to visual-only inputs.</p>
<p>Figure 13 .
13
Figure 13.LLM Hackathon for applications in materials and chemistry hybrid hackathon.Researchers were able to participate from both remote and in-person locations (purple pins).</p>
<p>Table 1 .
1
(Continued.)
ProjectAuthorsLinksG-Peer-T: LLM Probabilities For Assessing ScientificAlexander Al-Feghali, Sylvester ZhangGitHubNovelty and NonsenseKnowledge Extraction and ReasoningChemQAGhazal Khalighinejad, Shang Zhu, Xuefeng LiuGitHubLithiumMind-Leveraging Language Models forXinyi Ni, Zizhang Chen, Rongda Kang, Sheng-LunGitHubUnderstanding Battery PerformanceLiao, Pengyu Hong, Sandeep MadireddyKnowMat: Transforming Unstructured MaterialHasan M. Sayeed, Ramsey Issa, Trupti Mohanty, TaylorGitHubScience Literature into Structured KnowledgeSparksOntosynthesisQianxiang Ai, Jiaru Bai, Kevin Shen, Jennifer D'Souza,GitHubElliot RischKnowledge graph RAG for polymer simulationJiale Shi, Weijie Zhang, Dandan Tang, Chi ZhangGitHubSynthetic Data Generation and Insightful MachineTapashree Pradhan, Devi Dutta BiswajeetGitHubLearning for High Entropy Alloy HydridesChemsense: Are LLMs aligned with human chemicalMarti ño Ríos-García, Nawaf Alampara, MaraGitHubpreference?Schilling-Wilhelmi, Abdelrahman Ibrahim, Kevin MaikJablonkaGlossaGenMagdalena Lederbauer, Dieter Plessers, PhilippeGitHubSchwaller</p>
<p>Table 2 .
2
Accuracy of LLMs for each topic.The highest performance in each category is shown in bold.The evaluation results, summarized in figure2, show that the Opus variant of Claude consistently outperformed other models, achieving the highest accuracy in most categories.
Topic# QuestionsLLaMA-3-8bHaikuSonnetOPUSGPT4Thermodynamics11437.7247.3755.2673.6857.02Atomic structure1003240496459Mechanical behavior9622.9241.6752.0871.8843.75Material manufacturing9143.9657.1456.0480.2268.13Material applications5352.8364.1577.3692.4586.79Phase transition4131.7146.3465.8570.7363.41Electrical properties3633.332555.5672.2244.44Material processing3548.5754.2974.2988.5788.57Transport phenomena2437.570.8358.3387.562.5Magnetic properties1526.6746.6746.6766.6760Material characterization1478.5757.1485.7192.8671.43Fluid mechanics1421.435057.1478.5785.71Material testing977.7866.67100100100Miscellaneous862.562.562.57562.5Opus (claude-3-opus-20 240 229), were evaluated across 14 subject categories, such as characterization,applications, properties, and behavior.
AcknowledgmentsPlanning for this event was supported by NSF Awards #2226419 and #2209892 and NIST 70NANB24H049 / MML24-1001.We would like to thank event sponsors who provided platform credits and prizes for teams, including RadicalAI, Iteratec, Reincarnate, Acceleration Consortium, and Neo4j.Site coordinators include: Brandon Lines, Philippe Schwaller, Pepe Marquez, Mehrad Ansari and Seyed Mohamad Moosavi.Mohamad Moosavi acknowledges support from the Data Science Institute at the University of Toronto for organizing events related to LLMs.Mehrad Ansari acknowledges Mahyar Rajabi, Seyed Mohamad Moosavi, and Amro Aswad for their feedback on the Project.Aakash Naik, Katharina Ueltzen, and Janine George would like to acknowledge the Gauss Centre for Supercomputing e.V. (www.gauss-centre.eu) for funding their work on this Project by providing generous computing time on the GCS Supercomputer SuperMUC-NG at Leibniz Super computing Centre (www.lrz.de)(Project pn73da).Data availability statementAll code and data for the hackathon submissions is provided in the linked repositories described in table 1 and SI.We have also created an archive of all of the submission repositories as of the time of article submission on Zenodo at https://doi.org/10.5281/zenodo.15285515.Conflicts of interestThe authors declare no conflicts of interest.
. Alexander Al-Feghali,  0009-0004-8377-7049</p>
<p>. Mehrad Ansari,  0000-0001-5696-9193</p>
<p>. Joshua Bocarsly,  0000-0002-7523-152X</p>
<p>. Catherine, Brinson  0000-0003-2551-1563</p>
<p>. Yuan Chiang,  0000-0002-4017-7084</p>
<p>. Defne Circi,  0000-0002-5761-0198</p>
<p>. Min-Hsueh , Chiu  0000-0003-0637-7856</p>
<p>. Nathan Daelman,  0000-0002-7647-1816</p>
<p>. L Matthew, Evans,  0000-0002-1182-9098</p>
<p>. Janine George,  0000-0001-8907-0336</p>
<p>. Hassan Harb,  0000-0002-6016-3122</p>
<p>. Takrim Sartaaj, Khan,  0009-0009-2131-9700</p>
<p>. Sascha Klawohn,  0000-0003-4850-776X</p>
<p>. Magdalena Lederbauer,  0009-0008-0665-1839</p>
<p>. Soroush Mahjoubi,  0000-0001-8879-5431</p>
<p>. Bernadette Mohr,  0000-0003-0903-0073</p>
<p>. Seyed Mohamad, Moosavi  0000-0002-0357-5729</p>
<p>. Aleyna Beste, Ozhan  0000-0002-0281-3860</p>
<p>. Aritra Roy,  0000-0003-0243-9124</p>
<p>. Philippe Schwaller,  0000-0003-3046-6576</p>
<p>. Carla Terboven,  0009-0004-3786-0773</p>
<p>. Katharina Ueltzen,  0009-0003-2967-1182</p>
<p>. Yue Wu,  0000-0003-2874-8267</p>
<p>. Ian Foster,  0000-0003-2129-5269</p>
<p>. Ben Blaiszik,  0000-0002-5326-4902</p>
<p>Leveraging large language models for predictive chemistry Nat. K M Jablonka, P Schwaller, A Ortega-Guerrero, B Smit, 10.1038/s42256-023-00788-1Mach. Intell. 62024</p>
<p>Large language models as molecular design engines. D Bhattacharya, H J Cassady, M Hickner, W F Reinhart, 10.1021/acs.jcim.4c01396J. Chem. Inf. Mod. 642024</p>
<p>AtomGPT: atomistic generative pretrained transformer for forward and inverse materials design. K Choudhary, 10.1021/acs.jpclett.4c01126J. Phys. Chem. Lett. 152024</p>
<p>J Yin, A Bose, G Cong, I Lyngaas, Q Anthony, Comparative study of large language model architectures on frontier 2024 IEEE Int. Parallel and Distributed Processing Symp. IEEE2024</p>
<p>Augmenting large language models with chemistry tools Nat. M Bran, S Cox, O Schilter, C Baldassari, A White, P Schwaller, 10.1038/s42256-024-00832-8Mach. Intell. 62024</p>
<p>M Ramos, C Collison, C White, A D , arXiv:2407.0160A review of large language models and autonomous agents in chemistry. 2024</p>
<p>Self-driving laboratories for chemistry and materials science. G Tom, 10.1021/acs.chemrev.4c00055Chem. Rev. 1242024</p>
<p>Toward automated simulation research workflow through LLM prompt engineering design. Z Liu, Y Chai, J Li, 10.1021/acs.jcim.4c01653J. Chem. Inf. Model. 652024</p>
<p>Y Zhang, S A Khan, Mahmud A Yang, H Lavin, A Levin, M Frey, J Dunnmon, J Evans, J Bundy, A Dzeroski, S Tegner, J Zenil, H , arXiv:2505.16477Advancing the scientific method with large language models: from hypothesis to discovery. 2025</p>
<p>A survey on hypothesis generation for scientific discovery in the era of large language models. A Alkan, S Sourav, arXiv:2504.054962025</p>
<p>Y Zhou, H Liu, T Srivastava, Mei H Tan, C , Hypothesis generation with large language models Proc. 1st Workshop on NLP for Science (NLP4Science). Association for Computational Linguistics2024</p>
<p>M Narayanan, J D Braza, R-R Griffiths, A Bou, G Wellawatte, M C Ramos, L Mitchener, S Rodriques, A D White, arXiv:2506.17238Training a scientific reasoning model for chemistry. 2025</p>
<p>Matscibert: a materials domain language model for text mining and information extraction npj Comput. T Gupta, M Zaki, N M A Krishnan, Mausam , 10.1038/s41524-022-00784-w2022Mater8102</p>
<p>Alchembert: exploring lightweight language models for materials informatics ChemRxiv. X Liu, Y Wang, T Yang, X Liu, Wen X , 2025</p>
<p>A N Rubungo, C Arnold, B P Rand, A B Dieng, arXiv:2506.17238Llm-prop: predicting physical and electronic properties of crystalline solids from their text descriptions. 2023</p>
<p>Explainable synthesizability prediction of inorganic crystal polymorphs using large language models. S Kim, J Schrier, Jung Y , 10.1002/anie.202423950Angew. Chem., Int. Ed. 64e2024239502024</p>
<p>Robocrystallographer: automated crystal structure text descriptions and analysis MRS Commun. A Ganose, A Jain, 10.1557/mrc.2019.9420199</p>
<p>Benchmarking materials property prediction methods: the matbench test set and automatminer reference algorithm npj Comput. A Dunn, Q Wang, A Ganose, D Dopp, A Jain, 10.1038/s41524-020-00406-32020Mater6138</p>
<p>High-throughput density-functional perturbation theory phonons for. G Petretto, S Dwaraknath, H P C Miranda, D Winston, M Giantomassi, M J Van Setten, X Gonze, K A Persson, G Hautier, G-M Rignanese, 10.1038/sdata.2018.65inorganic materials Sci. Data. 52018</p>
<p>Matagent: a human-in-the-loop multi-agent llm framework for accelerating the material science discovery cycle AI for Accelerated Materials Design. A Bazgir, ICLR 20252025</p>
<p>Regression with large language models for materials and molecular property prediction. R Jacobs, M P Polak, L E Schultz, H Mahdavi, V Honavar, D Morgan, 2024</p>
<p>A N Rubungo, K Li, J Hattrick-Simpers, A B Dieng, arXiv:2411.00177Llm4mat-bench: benchmarking large language models for materials property prediction. 2024</p>
<p>Can large language models empower molecular property prediction?. C Qian, H Tang, Z Yang, H Liang, Y Liu, 2023</p>
<p>Autonomous chemical research with large language models. D A Boiko, R Macknight, B Kline, G Gomes, 10.1038/s41586-023-06792-0Nature. 6242023</p>
<p>. A E Ghareeb, B Chang, L Mitchener, A Yiu, C J Szostkiewicz, J M Laurent, M T Razzak, A D White, M Hinks, S G Rodriques, 20252505</p>
<p>M D Skarlinski, S Cox, J M Laurent, J D Braza, M Hinks, M J Hammerling, M Ponnapati, S Rodriques, A D White, arXiv:2409.13740Language agents achieve superhuman synthesis of scientific knowledge. 2024</p>
<p>Honeycomb: a flexible llm-based agent system for materials science. H Zhang, Y Song, Z Hou, S Miret, B Liu, 2024</p>
<p>The llama 3 herd of models. A Grattafiori, 2024</p>
<p>Phi-3 technical report: a highly capable language model locally on your phone [30] Anthropic. M Abdin, 2024. 2024The claude 3 model family: opus, sonnet, haiku</p>
<p>A Jiang, OpenAI 2023 Gpt-4-turbo and gpt-4. 2024. February 202532Mixtral of experts</p>
<p>. J Achiam, Gpt-4 technical report. 2024</p>
<p>How to support newcomers in scientific hackathons -an action research study on expert mentoring. A Nolte, L B Hayden, J D Herbsleb, Proc. ACM on Human-Computer Interaction. 42020</p>
<p>Understanding hackathons for science: collaboration, affordances and outcomes. E P P Pe-Than, J D Herbsleb, Lecture Notes in Computer Science. 2019Springer</p>
<p>Hack your organizational innovation: literature review and integrative model for running hackathons. B Heller, A Amir, R Waxman, Y Maaravi, 10.1186/s13731-023-00269-0J. Innov. Entrepreneurship. 1232023</p>
<p>14 examples of how llms can transform materials science and chemistry: a reflection on a large language model hackathon Dig. K Jablonka, 10.1039/D3DD00113JDiscovery. 22023</p>
<p>Y Zimmermann, Reflections from the 2024 large language model (LLM) hackathon for applications in materials science and chemistry. 2025</p>
<p>Language models are few-shot learners. T Brown, 2020</p>
<p>R Vacareanu, V A Negru, V Suciu, M Surdeanu, From words to numbers: Your large language model is secretly a capable regressor when given in-context examples 1st Conf. on Language Modeling. 2024</p>
<p>A quantum-chemical bonding database for solid-state materials Sci. A A Naik, C Ertural, N Dhamrait, P Benner, George J , 10.1038/s41597-023-02477-5Data. 106102023</p>
<p>Lobsterpy: A package to automatically analyze lobster runs J. Open Source Softw. A A Naik, K Ueltzen, C Ertural, A Jackson, George J , 10.21105/joss.06286202496286</p>
<p>A quantum-chemical bonding database for solid-state materials. A A Naik, C Ertural, N Dhamrait, P Benner, George J , JSONS: Part. 12023</p>
<p>Matbench 2024 The matbench test suite, phonon dataset. July 202412</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, Li W Liu, P , J. Mach. Learn. Res. 212020</p>
<p>. M Han, H , 2024. March 2024The unsloth package (available at</p>
<p>Multimodal large language models for inverse molecular design with retrosynthetic planning. G Liu, M Sun, W Matusik, M Jiang, Chen J , 2024</p>
<p>S Jia, C Zhang, V Fung, Autonomous materials discovery with large language models. 2024</p>
<p>Can llms generate diverse molecules? towards alignment with structural diversity. H Jang, Y Jang, Kim J Ahn, S , 2025</p>
<p>Generative design of functional metal complexes utilizing the internal knowledge of large language models. J Lu, Z Song, Q Zhao, Y Du, Y Cao, H Jia, C Duan, 2024</p>
<p>A sober look at LLMs for material discovery: are they actually good for Bayesian optimization over molecules?. A Kristiadi, F Strieth-Kalthoff, M Skreta, P Poupart, Aspuru-Guzik A Pleiss, G ; Salakhutdinov, Kolter, Heller, Weller, J Oliver, F Scarlett, Berkenkamp, Proc. 41st Int. Conf. on Machine Learning (Proc. Machine Learning Research. 41st Int. Conf. on Machine Learning (. Machine Learning ResearchPMLR2024. Jul 2024235</p>
<p>Are llms ready for real-world materials discovery?. S Miret, N M A Krishnan, 2024</p>
<p>Review on applications of metal-organic frameworks for co2 capture and the performance enhancement mechanisms Renew. L Li, H S Jung, J W And Lee, Y Kang, 10.1016/j.rser.2022.112441Sustain. Energy Rev. 1621124412022</p>
<p>S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, ReAct: synergizing reasoning and acting in language models Int. Conf. on Learning Representations (ICLR). 2023</p>
<p>Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. D Weininger, 10.1021/ci00057a005J. Chem. Inf. Comput. Sci. 281988</p>
<p>Smiles. 2. algorithm for generation of unique smiles notation. D Weininger, A Weininger, J L Weininger, 10.1021/ci00062a008J. Chem. Inf. Comput. Sci. 291989</p>
<p>Smiles. 3. depict. graphical depiction of chemical structures. D Weininger, 10.1021/ci00067a005J. Chem. Inf. Comput. Sci. 301990</p>
<p>M Ansari, J Watchorn, C Brown, J S Brown, arXiv:2410.03963dziner: rational inverse design of materials with ai agents. 2024</p>
<p>Semiconductor metal-organic frameworks: future low-bandgap materials. M Usman, S Mendiratta, K-L Lu, 10.1002/adma.201605071Adv. Mater. 2916050712017</p>
<p>Band gap modulations in uio metal-organic frameworks. E Flage-Larsen, A Røyset, J Cavka, K Thorshaug, 10.1021/jp405335qJ. Phys. Chem. C. 1172013</p>
<p>Band gap engineering of paradigm mof-5 Cryst. L-M Yang, G-Y Fang, J Ma, E Ganz, S S Han, 10.1021/cg500243sGrowth Des. 142014</p>
<p>Theoretical investigations on the chemical bonding, electronic structure and optical properties of the metal-organic framework mof-5 Inorg. L-M Yang, P Vajeeston, P Ravindran, H Fjellvag, M Tilset, 10.1021/ic100694wChem. 492010</p>
<p>Recent advancements in mof-based catalysts for applications in electrochemical and photoelectrochemical water splitting: a review Int. M Ali, E Pervaiz, T Noor, O Rabi, R Zahra, M Yang, 10.1002/er.5807J. Energy Res. 452021</p>
<p>Tuning electrical and mechanical properties of metal-organic frameworks by metal substitution. Y Yan, C Wang, Z Cai, Wang X , Xuan F , 10.1021/acsami.3c08470ACS Appl. Mater. Interfaces. 152023</p>
<p>Tunability of band gaps in metal-organic frameworks. C-K Lin, D Zhao, W-Y Gao, Z Yang, J Ye, T Xu, Q Ge, Ma S Liu, D-J , 10.1021/ic301189mInorg. Chem. 512012</p>
<p>. R Greene, T Sanders, L Weng, A Neelakantan, 2022</p>
<p>Agent-based learning of materials datasets from the scientific literature. M Ansari, S M Moosavi, 10.1039/D4DD00252KDigital Discovery. 32024</p>
<p>Moformer: self-supervised transformer model for metal-organic framework property prediction. Z Cao, R Magar, Wang Y Farimani, A B , 10.1021/jacs.2c11420J. Am. Chem. Soc. 1452023</p>
<p>Barlow twins: self-supervised learning via redundancy reduction Int. J Zbontar, L Jing, I Misra, Lecun Y , Deny S , Conf. on Machine Learning (PMLR. 2021</p>
<p>Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties. T Xie, J C Grossman, 10.1103/PhysRevLett.120.145301Phys. Rev. Lett. 1201453012018</p>
<p>Understanding the diversity of the metal-organic framework ecosystem. M Moosavi, A Nandy, K M Jablonka, D Ongari, J P Janet, P G Boyd, Y Lee, B Smit, H J Kulik, 10.1038/s41467-020-17755-8Nat. Commun. 112020</p>
<p>Machine learning the quantum-chemical properties of metal-organic frameworks for accelerated materials discovery Matter. S Rosen, S M Iyer, D Ray, Z Yao, A Aspuru-Guzik, L Gagliardi, J Notestein, R Q Snurr, 10.1016/j.matt.2021.02.01520214</p>
<p>. G Landrum, 2013Rdkit documentation Release 1 4</p>
<p>. H Chase, 2022 Langchain 10</p>
<p>. Y Song, W Xiong, D Zhu, W Wu, H Qian, M Song, H Huang, C Li, K Wang, R Yao, Y Tian, S Li, 2023Restgpt: connecting large language models with real-world restful apis</p>
<p>. K Darvish, 10.1016/j.matt.2024.10.015Organa: a robotic assistant for automated chemistry experimentation and characterization Matter. 81018972025</p>
<p>A machine learning approach to model solute grain boundary segregation npj Comput. L Huber, R Hadian, B Grabowski, J Neugebauer, 10.1038/s41524-018-0122-72018Mater464</p>
<p>pyiron: an integrated development environment for computational materials science. J Janssen, S Surendralal, Y Lysogorskiy, M Todorova, T Hickel, R Drautz, J Neugebauer, 10.1016/j.commatsci.2018.07.043Comput. Mater. Sci. 1632019</p>
<p>I Batatia, arXiv:2401.00096A foundation model for atomistic materials chemistry. 2023</p>
<p>Roadmap on data-centric materials science Modelling Simul. S Bauer, 10.1088/1361-651X/ad4d0dMater. Sci. Eng. 32633012024</p>
<p>Z Diao, H Yamashita, Abe M , arXiv:2405.15490Leveraging large language models and social media for automation in scanning probe microscopy. 2024</p>
<p>Synergizing human expertise and ai efficiency with language model for microscopy operation and automated experiment design. M C Y Liu, M Checa, R K Vasudevan, 10.1088/2632-2153/ad52e9Mach. Learn.: Sci. Technol. 52024</p>
<p>The abtem code: transmission electron microscopy from first principles Open Res. J Madsen, Susi T , 10.12688/openreseurope.13015.1Eur. 1242021</p>
<p>Nion swift: open source image processing software for instrument control, data acquisition, organization, visualization and analysis using python. C Meyer, N Dellby, J A Hachtel, T Lovejoy, A Mittelberger, O Krivanek, 10.1017/S143192761900134XMicrosc. Microanal. 252019</p>
<p>Practical and ethical challenges of large language models in education: a systematic scoping review. L Yan, L Sha, L Zhao, Y Li, R Martinez-Maldonado, G Chen, X Li, Jin Y Gašević, D , 10.1111/bjet.13370Br. J. Educ. Technol. 552023</p>
<p>Large language models for education: a survey and outlook. S Wang, T Xu, H Li, C Zhang, J Liang, J Tang, P Yu, Wen Q , 2024</p>
<p>Chatgpt for good? on opportunities and challenges of large language models for education Learn. E Kasneci, 10.1016/j.lindif.2023.102274Ind. Diff. 1031022742023</p>
<p>The notorious gpt: science communication in the age of artificial intelligence. M Schäfer, 10.5167/uzh-237847J. Sci. Commun. 22Y022023</p>
<p>Mascqa: investigating materials science knowledge of large language models Dig. M Zaki, 10.1039/D3DD00188ADiscovery. 32024</p>
<p>K Seßler, Y Rong, Gözlüklü E Kasneci, E , arXiv:2408.10839Benchmarking large language models for math reasoning tasks. 2024</p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, Q Le, Chi E Zhou, D , Advances in Neural Information Processing Systems. 2022NeurIPS</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks Advances in. P Lewis, Neural Information Processing Systems (NeurIPS). 202033</p>
<p>V Mishra, arXiv:2412.09560Foundational large language models for materials research. 2024</p>
<p>The nomad laboratory: from data sharing to artificial intelligence. C Draxl, M Scheffler, 10.1088/2515-7639/ab13bbJ. Phys. Mater. 2360012019</p>
<p>. M L Evans, J D Bocarsly, 2024 datalab Zenodo</p>
<p>Nomad: a distributed web-based platform for managing materials science research data J. Open Source Softw. M Scheidgen, 10.21105/joss.05388202385388</p>
<p>. Y Gao, Y Xiong, X Gao, K Jia, J Pan, Y Bi, Y Dai, J Sun, Wang H Wang, H , arXiv:2312.109972023augmented generation for large language models: a survey</p>
<p>H Touvron, arXiv:2302.13971Llama: open and efficient foundation language models. 2023</p>
<p>Abdel-Rehim A Zenil, H Orhobor, O Fisher, M Collins, R J Bourne, E Fearnley, G W , Tate E Smith, H X Soldatova, L King, R D , Scientific hypothesis generation by a large language model: laboratory validation in breast cancer treatment. 2024</p>
<p>Automating psychological hypothesis generation with ai: when large language models meet causal graph Human. S Tong, K Mao, Z Huang, Y Zhao, K Peng, 10.1057/s41599-024-03407-5Soc. Sci. Commun. 118962024</p>
<p>Harnessing the power of adversarial prompting and large language models for robust hypothesis generation in astronomy. ˘i Ciuca, Y-S Ting, S Kruk, K Iyer, 2023</p>
<p>Proteinhypothesis: a physics-aware chain of multi-agent rag llm for hypothesis generation in protein science Towards Agentic AI for Science: Hypothesis Generation, Comprehension. A Bazgir, 2025Quantification and Validation</p>
<p>Beyond designer's knowledge: generating materials design hypotheses via large language models. Q Liu, M P Polak, S Y Kim, M A A Shuvo, H S Deodhar, J Han, Morgan D Oh, H , 2024</p>
<p>Towards ai research agents in the chemical sciences ChemRxiv. O Shir, 2024</p>
<p>Agentichypothesis: a survey on hypothesis generation using llm systems Towards Agentic AI for Science: Hypothesis Generation, Comprehension, Quantification and Validation. A Bazgir, 2025</p>
<p>Large language models for automated open-domain scientific hypotheses discovery. Z Yang, X Du, J Li, J Zheng, S Poria, E Cambria, 2024</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, I Shafran, T Griffiths, Y Cao, K Narasimhan, Advances in Neural Information Processing Systems. Oh, T Naumann, A Globerson, K Saenko, M Hardt and S Levine362023Curran Associates, Inc</p>
<p>Large language models for scientific information extraction: an empirical study for virology. M Shamsabadi, D' Souza, J Auer, S , 2024</p>
<p>Structured information extraction from scientific text with large language models. J Dagdelen, A Dunn, S Lee, N Walker, A S Rosen, G Ceder, K Persson, A Jain, 10.1038/s41467-024-45563-xNat. Commun. 1514182024</p>
<p>Large language models for generative information extraction: a survey. D Xu, W Chen, W Peng, C Zhang, T Xu, X Zhao, X Wu, Y Zheng, Wang Y , Chen E , 2024</p>
<p>Generative ai for self-adaptive systems: state of the art and research roadmap ACM. J Li, M Zhang, N Li, D Weyns, Jin Z Tei, K , Trans. Auto. Adaptive Syst. 192024</p>
<p>Sciagent: tool-augmented language models for scientific reasoning. Y Ma, 2024. 10 March 2025</p>
<p>N4J 2024 Neo4j. 242025</p>
<p>Dspy: Compiling declarative language model calls into self-improving pipelines. O Khattab, 2023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, Chi E , Le Q Zhou, D , 2023</p>
<p>Graph maker. G Maker, 2024. February 2025</p>
<p>. D Fu, G Khalighinejad, O Liu, B Dhingra, D Yogatama, R Jia, W Neiswanger, Isobench: benchmarking multimodal foundation models on isomorphic representations. 2024</p>
<p>. T Guo, K Guo, B Nan, Z Liang, Z Guo, N V Chawla, O Wiest, X Zhang, 2023What can large language models do in chemistry? a comprehensive benchmark on eight tasks</p>
<p>Chemqa: a multimodal question-and-answering dataset on chemistry reasoning. S Zhu, X Liu, G Khalighinejad, 2024</p>
<p>Gemini: a family of highly capable multimodal models [125] The claude 3 model family: opus, sonnet, haiku. G Team, 2024</p>            </div>
        </div>

    </div>
</body>
</html>
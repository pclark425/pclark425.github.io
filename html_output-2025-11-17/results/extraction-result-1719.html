<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1719 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1719</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1719</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-271051529</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.06084v1.pdf" target="_blank">3D Vision and Language Pretraining with Large-Scale Synthetic Data</a></p>
                <p><strong>Paper Abstract:</strong> 3D Vision-Language Pre-training (3D-VLP) aims to provide a pre-train model which can bridge 3D scenes with natural language, which is an important technique for embodied intelligence. However, current 3D-VLP datasets are hindered by limited scene-level diversity and insufficient fine-grained annotations (only 1.2K scenes and 280K textual annotations in ScanScribe), primarily due to the labor-intensive of collecting and annotating 3D scenes. To overcome these obstacles, we construct SynVL3D, a comprehensive synthetic scene-text corpus with 10K indoor scenes and 1M descriptions at object, view, and room levels, which has the advantages of diverse scene data, rich textual descriptions, multi-grained 3D-text associations, and low collection cost. Utilizing the rich annotations in SynVL3D, we pre-train a simple and unified Transformer for aligning 3D and language with multi-grained pretraining tasks. Moreover, we propose a synthetic-to-real domain adaptation in downstream task fine-tuning process to address the domain shift. Through extensive experiments, we verify the effectiveness of our model design by achieving state-of-the-art performance on downstream tasks including visual grounding, dense captioning, and question answering.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1719.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1719.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SynFormer3D</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SynFormer3D</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-modal Transformer pretrained on large-scale synthetic 3D scene — language pairs (SynVL3D) with multi-grained auxiliary tasks (MLM, MOM, SSM, ORP, MRWA, VRWA) and a triple-domain discriminator for synthetic-to-real adaptation; fine-tuned for 3D visual grounding, dense captioning and question answering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>SynFormer3D</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A unified Transformer multimodal encoder that concatenates text tokens and 3D object tokens (from a 3D-object encoder) and performs cross-modal fusion; pretrained with standard VL objectives (masked language/object modeling, scene-sentence matching) plus fine-grained 3D-specific tasks: Object Relationship Prediction (ORP), Multi-level Region-Word Alignment (MRWA), and View-aggregated Region-Word Alignment (VRWA). Includes a triple-domain discriminator (vision / language / joint) used during fine-tuning for synthetic-to-real adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>paired 3D scenes and natural-language descriptions (synthetic)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>SynVL3D synthetic dataset: ~10,000 indoor scenes, >1M textual descriptions (template-based + GPT-3 rephrased free-form), 108 object categories, >1M object instances; fine-grained annotations include category, high-quality mesh, precise bounding boxes, segmentation masks, scene graphs (support/spatial/comparative relations), and phrase-region identifiers. Pretraining runs for 100 epochs, batch size 64 on NVIDIA A100, optimizer AdamW lr=1e-4. Pretraining tasks: MLM, MOM, SSM, ORP, MRWA (object/room/house levels), VRWA (V rotated views).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>3D Visual Grounding; 3D Dense Captioning; 3D Question Answering (ScanRefer, Nr3D/Sr3D, Scan2Cap, ScanQA)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Downstream 3D vision-language tasks on scanned real-world indoor scenes: 3D visual grounding (map referring expressions to object bounding boxes/proposals), dense captioning (generate captions describing objects/regions), and 3D question answering (answer free-form questions about a scanned scene). Environments are scanned indoor point-cloud/mesh scenes with object proposals and annotated language.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Not applicable for pretraining — pretraining uses static natural-language descriptions (no action commands or interactive text environment).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Task-dependent discrete prediction outputs (select object proposal / predict bounding box / generate text answers); there is no continuous locomotion or low-level robot control in the evaluated tasks (they are perception and grounding/captioning/QA tasks operating on object proposals and scene point clouds).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Not applicable — there is no mapping from high-level text actions to low-level embodied controls in this work; transfer is representation-level (aligning text tokens with 3D region/object tokens) and then fine-tuning lightweight task decoders.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>3D point clouds (scene point cloud P), object proposals/features from a 3D-object encoder, mesh and segmentation masks available in synthetic data; annotated camera parameters used to generate multi-view rotated point clouds for VRWA. Downstream evaluation uses scanned real scenes (RGB-D/point clouds) and detector-generated object proposals.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Reported improvements and SOTA: Nr3D overall accuracy 67.5%, Sr3D 78.6%; ScanRefer: 52.3% @IoU>0.25 and 46.2% @IoU>0.5; ScanQA EM@1 = 27.6% (with object proposals) / 24.1% (w/o objects). Dense captioning: improved CIDEr@0.25 and several other metrics (e.g., CIDEr@0.25 ~72.1 reported). These numbers are the paper's reported result for SynFormer3D after synthetic pretraining + synthetic-to-real fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Paper compares to prior methods (some without large-scale pretraining or with smaller 3D-VL pretraining). SynFormer3D improves over prior best pretraining methods (e.g., gains reported as +1.1% on Nr3D and +1.5% on Sr3D compared to prior leading methods; explicit baselines include 3D-VLP and 3D-VISTA). Exact baseline numbers vary by dataset and are reported in paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Large-scale, diverse synthetic 3D-text paired data (10k scenes, 1M descriptions); rich fine-grained annotations enabling object/phrase-region alignment; novel fine-grained pretraining tasks (ORP, MRWA, VRWA) that capture relationships, multi-granularity and multi-view alignment; and synthetic-to-real triple-domain adversarial adaptation during fine-tuning to reduce domain gap.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Synthetic-to-real domain gap in visual appearance and in text fluency (templated/GPT-3 generated descriptions differ from human-annotated language); downstream tasks are perception-focused (no low-level control), so methods do not address action-space mismatches for embodied control; any remaining domain gap may limit gains on metrics sensitive to natural language fluency.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretraining on a very large synthetic corpus of 3D scenes paired with rich, phrase-level language (SynVL3D) and using multi-granular, multi-view alignment tasks substantially improves downstream 3D vision-language tasks (grounding, captioning, QA). View-aggregated alignment and object-relationship prediction are especially helpful; explicit synthetic-to-real domain adaptation via separate vision/language/joint discriminators further improves fine-tuning on real scanned data. The work demonstrates that language-aligned 3D pretraining (even from synthetic data) yields positive transfer for 3D perception tasks, but that domain shift and text fluency remain practical limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '3D Vision and Language Pretraining with Large-Scale Synthetic Data', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1719.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1719.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>3D-VISTA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>3D-VISTA (pretrained 3D vision-language model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previously published 3D vision-language pretraining approach (cited in the paper) that uses masked language/object modeling and scene-sentence matching for 3D-text alignment; served as a competitive baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>3D-VISTA</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A 3D vision-language pretraining model that applies standard VL pretraining objectives (masked language/object modeling, scene-sentence matching) to 3D point cloud scenes paired with text, intended to improve downstream 3D-VL tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>paired 3D scenes and textual descriptions (real scanned datasets aggregated into ScanScribe / related corpora)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Uses aggregated real 3D datasets (ScanScribe, etc.) with hundreds of thousands of textual annotations but fewer scenes (ScanScribe ~1.2K scenes, ~278K descriptions) — cited as limited in scale relative to SynVL3D.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>3D Visual Grounding, Dense Captioning, 3D QA (evaluated as baselines in comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Standard 3D-VL perception tasks on scanned indoor scenes (Nr3D/Sr3D, ScanRefer, Scan2Cap, ScanQA).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Not applicable (pretraining uses static descriptions rather than interactive text actions).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Perception outputs (object selection / caption generation / answer generation), no low-level control.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Representation-level transfer via shared multimodal encoder and fine-tuning, not action mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>3D point clouds / object proposals from scanned scenes; language encoding for captions/questions.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Reported in paper tables as a prior strong baseline (specific numbers in the paper's comparison tables; e.g., 3D-VISTA is one of the leading prior methods for several metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Pretraining on paired 3D-text data with masked modeling objectives that align modalities improves downstream 3D-VL tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Smaller dataset scale and fewer fine-grained phrase-region annotations compared to SynVL3D limit performance and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>3D-VISTA demonstrates benefits of 3D-VL pretraining but is constrained by limited real-scene dataset scale and annotation granularity, motivating larger synthetic corpora and more fine-grained tasks as used by SynFormer3D.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '3D Vision and Language Pretraining with Large-Scale Synthetic Data', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1719.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1719.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>3D-VLP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>3D-VLP (context-aware 3D vision-language pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited prior work that introduces context-aware spatial-semantic alignment between point clouds and textual descriptions for 3D-VL pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>3D-VLP</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A model that performs context-aware spatial-semantic alignment between 3D point cloud features and text descriptions using pretraining objectives tailored to 3D data.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>paired 3D point clouds and textual descriptions (real scanned datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Pretraining performed on existing 3D-VL datasets (limited by size and fine-grained annotations as discussed in paper); specific dataset sizes are reported in that prior work (not detailed in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>3D Visual Grounding, Dense Captioning, 3D QA (used as comparative baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Perception and language tasks on scanned indoor scenes (grounding, captioning, QA).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Not applicable (static language descriptions).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Perception outputs; not low-level motor control.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Representation-level alignment; no explicit action mapping described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Point-cloud based perception and language understanding of object descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Reported as competitive baseline in paper comparisons (specific numeric values in the paper's tables).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Context-aware alignment between spatial features and language aids downstream perception tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Limited dataset scale and annotation granularity compared to large synthetic corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>3D-VLP shows that pretraining with spatial-semantic objectives helps 3D-VL tasks, but larger/diverse data and finer-grained annotations (as in SynVL3D) can further improve transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '3D Vision and Language Pretraining with Large-Scale Synthetic Data', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1719.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1719.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP-Guided</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIP-Guided (Parelli et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A comparative method referenced in ScanQA evaluations that uses CLIP-derived vision-language features to guide 3D question answering; leverages image–text pretrained representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>CLIP-Guided</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A method that uses CLIP (image-text contrastive pretrained model) features to guide 3D question answering and related tasks by transferring image-language representations to 3D scene understanding pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>image-text contrastive pretraining (CLIP-style natural image and caption corpora)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not specified in this paper; CLIP is generally pretrained on large web-scale image-caption pairs (original CLIP paper). The CLIP-Guided method references these pretrained representations to inform 3D QA models.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>3D Question Answering (ScanQA)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Answering natural-language questions about scanned indoor scenes using object proposals and multimodal features.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Not applicable (static questions and answers).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Perception / answer generation (no continuous control).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Representation transfer: use image-text pretrained embeddings (CLIP) as guidance/features for 3D QA pipeline; exact mapping details are in the original CLIP-Guided paper (not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Object proposals from Mask3D, 3D point-cloud inputs; relies on aligning 3D features with CLIP-derived language-aligned visual embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Reported in paper Table 4: CLIP-Guided EM@1 = 23.9% / 21.4% (likely with/without objects) — modest improvement over some baselines as shown in comparison table.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>ScanQA baseline reported 23.5% / 20.9% EM@1 in the table (so CLIP-Guided is comparable/slightly better).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Strong image-language alignment in CLIP provides transferable semantic features that can be aligned to 3D scene features for QA.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Domain gap between 2D image appearance and 3D scanned geometry; CLIP's training on 2D images may not capture 3D spatial relations critical for some 3D QA questions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using image-language pretrained models such as CLIP as guidance can yield modest gains for 3D question-answering but is limited by 2D-to-3D domain discrepancies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '3D Vision and Language Pretraining with Large-Scale Synthetic Data', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1719.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1719.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-CLIP (Delitzas et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A comparative method that aggregates CLIP-like multi-view/contrastive vision-language signals for 3D scene question answering tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Multi-CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Method that applies multiple CLIP-based embeddings or multi-view CLIP features to improve 3D QA by leveraging strong image-language pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>image-text contrastive pretraining (CLIP variants)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not specified in this paper; relies on CLIP-pretrained components and possibly multi-view image proxies to construct features for 3D tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>3D Question Answering (ScanQA)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Answering questions about 3D scanned scenes by leveraging CLIP-derived multi-view visual-language embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Perception / answer generation only.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Feature-level transfer from 2D CLIP embeddings to 3D representations; details in the original Multi-CLIP paper.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>3D point-clouds and multi-view projections to obtain CLIP-compatible image crops/features; object proposals.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Reported in the paper's comparison: Multi-CLIP EM@1 = 24.0% / 21.6% (with/without objects) — shows modest improvements relative to some baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Baselines in table include ScanQA reported 23.5% / 20.9% EM@1.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>CLIP's strong cross-modal alignment and multi-view aggregation provide useful priors for 3D QA.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Mismatch between 2D training distributions and 3D scanned geometry plus limited explicit 3D spatial relation modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Multi-CLIP variants leveraging large image-language pretrained models produce modest gains on 3D QA benchmarks, but are outperformed by specialized 3D-VL pretraining that models 3D relations and multi-granular alignments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '3D Vision and Language Pretraining with Large-Scale Synthetic Data', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1719.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1719.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (Language models are few-shot learners)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large language model used in this paper to rephrase templated descriptions into more natural free-form captions for SynVL3D; mentioned as a tool in dataset generation rather than as a transferred agent for 3D tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A large autoregressive transformer language model pretrained on massive text corpora; used in this work to paraphrase and naturalize template-based descriptions while preserving object phrase identifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>massive web text corpora (general natural language)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not specified in this paper beyond the citation; used only as a rephrasing tool to convert template descriptions into free-form natural language while keeping object identifiers intact.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>None in this paper (GPT-3 is not fine-tuned or transferred to embodied 3D tasks here; it is only used for dataset text generation).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>n/a (GPT-3 is a language generator, not an action-producing text-environment agent in this work).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-3 was used as a dataset augmentation tool (paraphrasing templated descriptions) to increase the naturalness of synthetic captions while preserving phrase-region IDs; it was not used as an agent transferred to embodied 3D tasks in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '3D Vision and Language Pretraining with Large-Scale Synthetic Data', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1719.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1719.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EmbodiedGeneralist (ref)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>An embodied generalist agent in 3d world (cited reference)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced paper title that likely describes an embodied agent in 3D worlds; cited in related work but not used or analyzed in detail here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>An embodied generalist agent in 3d world (reference)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Referenced work (title only in bibliography) presumably about an embodied agent trained for 3D interactive environments; the SynFormer3D paper does not provide details or experiment results for it.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '3D Vision and Language Pretraining with Large-Scale Synthetic Data', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>3D-VISTA <em>(Rating: 2)</em></li>
                <li>3D-VLP <em>(Rating: 2)</em></li>
                <li>CLIP-Guided <em>(Rating: 2)</em></li>
                <li>Multi-CLIP <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
                <li>An embodied generalist agent in 3d world <em>(Rating: 2)</em></li>
                <li>ScanScribe <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1719",
    "paper_id": "paper-271051529",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "SynFormer3D",
            "name_full": "SynFormer3D",
            "brief_description": "A multi-modal Transformer pretrained on large-scale synthetic 3D scene — language pairs (SynVL3D) with multi-grained auxiliary tasks (MLM, MOM, SSM, ORP, MRWA, VRWA) and a triple-domain discriminator for synthetic-to-real adaptation; fine-tuned for 3D visual grounding, dense captioning and question answering.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_agent_name": "SynFormer3D",
            "model_agent_description": "A unified Transformer multimodal encoder that concatenates text tokens and 3D object tokens (from a 3D-object encoder) and performs cross-modal fusion; pretrained with standard VL objectives (masked language/object modeling, scene-sentence matching) plus fine-grained 3D-specific tasks: Object Relationship Prediction (ORP), Multi-level Region-Word Alignment (MRWA), and View-aggregated Region-Word Alignment (VRWA). Includes a triple-domain discriminator (vision / language / joint) used during fine-tuning for synthetic-to-real adaptation.",
            "pretraining_data_type": "paired 3D scenes and natural-language descriptions (synthetic)",
            "pretraining_data_details": "SynVL3D synthetic dataset: ~10,000 indoor scenes, &gt;1M textual descriptions (template-based + GPT-3 rephrased free-form), 108 object categories, &gt;1M object instances; fine-grained annotations include category, high-quality mesh, precise bounding boxes, segmentation masks, scene graphs (support/spatial/comparative relations), and phrase-region identifiers. Pretraining runs for 100 epochs, batch size 64 on NVIDIA A100, optimizer AdamW lr=1e-4. Pretraining tasks: MLM, MOM, SSM, ORP, MRWA (object/room/house levels), VRWA (V rotated views).",
            "embodied_task_name": "3D Visual Grounding; 3D Dense Captioning; 3D Question Answering (ScanRefer, Nr3D/Sr3D, Scan2Cap, ScanQA)",
            "embodied_task_description": "Downstream 3D vision-language tasks on scanned real-world indoor scenes: 3D visual grounding (map referring expressions to object bounding boxes/proposals), dense captioning (generate captions describing objects/regions), and 3D question answering (answer free-form questions about a scanned scene). Environments are scanned indoor point-cloud/mesh scenes with object proposals and annotated language.",
            "action_space_text": "Not applicable for pretraining — pretraining uses static natural-language descriptions (no action commands or interactive text environment).",
            "action_space_embodied": "Task-dependent discrete prediction outputs (select object proposal / predict bounding box / generate text answers); there is no continuous locomotion or low-level robot control in the evaluated tasks (they are perception and grounding/captioning/QA tasks operating on object proposals and scene point clouds).",
            "action_mapping_method": "Not applicable — there is no mapping from high-level text actions to low-level embodied controls in this work; transfer is representation-level (aligning text tokens with 3D region/object tokens) and then fine-tuning lightweight task decoders.",
            "perception_requirements": "3D point clouds (scene point cloud P), object proposals/features from a 3D-object encoder, mesh and segmentation masks available in synthetic data; annotated camera parameters used to generate multi-view rotated point clouds for VRWA. Downstream evaluation uses scanned real scenes (RGB-D/point clouds) and detector-generated object proposals.",
            "transfer_successful": true,
            "performance_with_pretraining": "Reported improvements and SOTA: Nr3D overall accuracy 67.5%, Sr3D 78.6%; ScanRefer: 52.3% @IoU&gt;0.25 and 46.2% @IoU&gt;0.5; ScanQA EM@1 = 27.6% (with object proposals) / 24.1% (w/o objects). Dense captioning: improved CIDEr@0.25 and several other metrics (e.g., CIDEr@0.25 ~72.1 reported). These numbers are the paper's reported result for SynFormer3D after synthetic pretraining + synthetic-to-real fine-tuning.",
            "performance_without_pretraining": "Paper compares to prior methods (some without large-scale pretraining or with smaller 3D-VL pretraining). SynFormer3D improves over prior best pretraining methods (e.g., gains reported as +1.1% on Nr3D and +1.5% on Sr3D compared to prior leading methods; explicit baselines include 3D-VLP and 3D-VISTA). Exact baseline numbers vary by dataset and are reported in paper tables.",
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Large-scale, diverse synthetic 3D-text paired data (10k scenes, 1M descriptions); rich fine-grained annotations enabling object/phrase-region alignment; novel fine-grained pretraining tasks (ORP, MRWA, VRWA) that capture relationships, multi-granularity and multi-view alignment; and synthetic-to-real triple-domain adversarial adaptation during fine-tuning to reduce domain gap.",
            "transfer_failure_factors": "Synthetic-to-real domain gap in visual appearance and in text fluency (templated/GPT-3 generated descriptions differ from human-annotated language); downstream tasks are perception-focused (no low-level control), so methods do not address action-space mismatches for embodied control; any remaining domain gap may limit gains on metrics sensitive to natural language fluency.",
            "key_findings": "Pretraining on a very large synthetic corpus of 3D scenes paired with rich, phrase-level language (SynVL3D) and using multi-granular, multi-view alignment tasks substantially improves downstream 3D vision-language tasks (grounding, captioning, QA). View-aggregated alignment and object-relationship prediction are especially helpful; explicit synthetic-to-real domain adaptation via separate vision/language/joint discriminators further improves fine-tuning on real scanned data. The work demonstrates that language-aligned 3D pretraining (even from synthetic data) yields positive transfer for 3D perception tasks, but that domain shift and text fluency remain practical limitations.",
            "uuid": "e1719.0",
            "source_info": {
                "paper_title": "3D Vision and Language Pretraining with Large-Scale Synthetic Data",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "3D-VISTA",
            "name_full": "3D-VISTA (pretrained 3D vision-language model)",
            "brief_description": "A previously published 3D vision-language pretraining approach (cited in the paper) that uses masked language/object modeling and scene-sentence matching for 3D-text alignment; served as a competitive baseline.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_agent_name": "3D-VISTA",
            "model_agent_description": "A 3D vision-language pretraining model that applies standard VL pretraining objectives (masked language/object modeling, scene-sentence matching) to 3D point cloud scenes paired with text, intended to improve downstream 3D-VL tasks.",
            "pretraining_data_type": "paired 3D scenes and textual descriptions (real scanned datasets aggregated into ScanScribe / related corpora)",
            "pretraining_data_details": "Uses aggregated real 3D datasets (ScanScribe, etc.) with hundreds of thousands of textual annotations but fewer scenes (ScanScribe ~1.2K scenes, ~278K descriptions) — cited as limited in scale relative to SynVL3D.",
            "embodied_task_name": "3D Visual Grounding, Dense Captioning, 3D QA (evaluated as baselines in comparisons)",
            "embodied_task_description": "Standard 3D-VL perception tasks on scanned indoor scenes (Nr3D/Sr3D, ScanRefer, Scan2Cap, ScanQA).",
            "action_space_text": "Not applicable (pretraining uses static descriptions rather than interactive text actions).",
            "action_space_embodied": "Perception outputs (object selection / caption generation / answer generation), no low-level control.",
            "action_mapping_method": "Representation-level transfer via shared multimodal encoder and fine-tuning, not action mapping.",
            "perception_requirements": "3D point clouds / object proposals from scanned scenes; language encoding for captions/questions.",
            "transfer_successful": true,
            "performance_with_pretraining": "Reported in paper tables as a prior strong baseline (specific numbers in the paper's comparison tables; e.g., 3D-VISTA is one of the leading prior methods for several metrics).",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Pretraining on paired 3D-text data with masked modeling objectives that align modalities improves downstream 3D-VL tasks.",
            "transfer_failure_factors": "Smaller dataset scale and fewer fine-grained phrase-region annotations compared to SynVL3D limit performance and generalization.",
            "key_findings": "3D-VISTA demonstrates benefits of 3D-VL pretraining but is constrained by limited real-scene dataset scale and annotation granularity, motivating larger synthetic corpora and more fine-grained tasks as used by SynFormer3D.",
            "uuid": "e1719.1",
            "source_info": {
                "paper_title": "3D Vision and Language Pretraining with Large-Scale Synthetic Data",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "3D-VLP",
            "name_full": "3D-VLP (context-aware 3D vision-language pretraining)",
            "brief_description": "A cited prior work that introduces context-aware spatial-semantic alignment between point clouds and textual descriptions for 3D-VL pretraining.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_agent_name": "3D-VLP",
            "model_agent_description": "A model that performs context-aware spatial-semantic alignment between 3D point cloud features and text descriptions using pretraining objectives tailored to 3D data.",
            "pretraining_data_type": "paired 3D point clouds and textual descriptions (real scanned datasets).",
            "pretraining_data_details": "Pretraining performed on existing 3D-VL datasets (limited by size and fine-grained annotations as discussed in paper); specific dataset sizes are reported in that prior work (not detailed in this paper).",
            "embodied_task_name": "3D Visual Grounding, Dense Captioning, 3D QA (used as comparative baselines)",
            "embodied_task_description": "Perception and language tasks on scanned indoor scenes (grounding, captioning, QA).",
            "action_space_text": "Not applicable (static language descriptions).",
            "action_space_embodied": "Perception outputs; not low-level motor control.",
            "action_mapping_method": "Representation-level alignment; no explicit action mapping described in this paper.",
            "perception_requirements": "Point-cloud based perception and language understanding of object descriptions.",
            "transfer_successful": true,
            "performance_with_pretraining": "Reported as competitive baseline in paper comparisons (specific numeric values in the paper's tables).",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Context-aware alignment between spatial features and language aids downstream perception tasks.",
            "transfer_failure_factors": "Limited dataset scale and annotation granularity compared to large synthetic corpora.",
            "key_findings": "3D-VLP shows that pretraining with spatial-semantic objectives helps 3D-VL tasks, but larger/diverse data and finer-grained annotations (as in SynVL3D) can further improve transfer.",
            "uuid": "e1719.2",
            "source_info": {
                "paper_title": "3D Vision and Language Pretraining with Large-Scale Synthetic Data",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "CLIP-Guided",
            "name_full": "CLIP-Guided (Parelli et al., 2023)",
            "brief_description": "A comparative method referenced in ScanQA evaluations that uses CLIP-derived vision-language features to guide 3D question answering; leverages image–text pretrained representations.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_agent_name": "CLIP-Guided",
            "model_agent_description": "A method that uses CLIP (image-text contrastive pretrained model) features to guide 3D question answering and related tasks by transferring image-language representations to 3D scene understanding pipelines.",
            "pretraining_data_type": "image-text contrastive pretraining (CLIP-style natural image and caption corpora)",
            "pretraining_data_details": "Not specified in this paper; CLIP is generally pretrained on large web-scale image-caption pairs (original CLIP paper). The CLIP-Guided method references these pretrained representations to inform 3D QA models.",
            "embodied_task_name": "3D Question Answering (ScanQA)",
            "embodied_task_description": "Answering natural-language questions about scanned indoor scenes using object proposals and multimodal features.",
            "action_space_text": "Not applicable (static questions and answers).",
            "action_space_embodied": "Perception / answer generation (no continuous control).",
            "action_mapping_method": "Representation transfer: use image-text pretrained embeddings (CLIP) as guidance/features for 3D QA pipeline; exact mapping details are in the original CLIP-Guided paper (not detailed here).",
            "perception_requirements": "Object proposals from Mask3D, 3D point-cloud inputs; relies on aligning 3D features with CLIP-derived language-aligned visual embeddings.",
            "transfer_successful": true,
            "performance_with_pretraining": "Reported in paper Table 4: CLIP-Guided EM@1 = 23.9% / 21.4% (likely with/without objects) — modest improvement over some baselines as shown in comparison table.",
            "performance_without_pretraining": "ScanQA baseline reported 23.5% / 20.9% EM@1 in the table (so CLIP-Guided is comparable/slightly better).",
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Strong image-language alignment in CLIP provides transferable semantic features that can be aligned to 3D scene features for QA.",
            "transfer_failure_factors": "Domain gap between 2D image appearance and 3D scanned geometry; CLIP's training on 2D images may not capture 3D spatial relations critical for some 3D QA questions.",
            "key_findings": "Using image-language pretrained models such as CLIP as guidance can yield modest gains for 3D question-answering but is limited by 2D-to-3D domain discrepancies.",
            "uuid": "e1719.3",
            "source_info": {
                "paper_title": "3D Vision and Language Pretraining with Large-Scale Synthetic Data",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Multi-CLIP",
            "name_full": "Multi-CLIP (Delitzas et al., 2023)",
            "brief_description": "A comparative method that aggregates CLIP-like multi-view/contrastive vision-language signals for 3D scene question answering tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_agent_name": "Multi-CLIP",
            "model_agent_description": "Method that applies multiple CLIP-based embeddings or multi-view CLIP features to improve 3D QA by leveraging strong image-language pretraining.",
            "pretraining_data_type": "image-text contrastive pretraining (CLIP variants)",
            "pretraining_data_details": "Not specified in this paper; relies on CLIP-pretrained components and possibly multi-view image proxies to construct features for 3D tasks.",
            "embodied_task_name": "3D Question Answering (ScanQA)",
            "embodied_task_description": "Answering questions about 3D scanned scenes by leveraging CLIP-derived multi-view visual-language embeddings.",
            "action_space_text": "Not applicable.",
            "action_space_embodied": "Perception / answer generation only.",
            "action_mapping_method": "Feature-level transfer from 2D CLIP embeddings to 3D representations; details in the original Multi-CLIP paper.",
            "perception_requirements": "3D point-clouds and multi-view projections to obtain CLIP-compatible image crops/features; object proposals.",
            "transfer_successful": true,
            "performance_with_pretraining": "Reported in the paper's comparison: Multi-CLIP EM@1 = 24.0% / 21.6% (with/without objects) — shows modest improvements relative to some baselines.",
            "performance_without_pretraining": "Baselines in table include ScanQA reported 23.5% / 20.9% EM@1.",
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "CLIP's strong cross-modal alignment and multi-view aggregation provide useful priors for 3D QA.",
            "transfer_failure_factors": "Mismatch between 2D training distributions and 3D scanned geometry plus limited explicit 3D spatial relation modeling.",
            "key_findings": "Multi-CLIP variants leveraging large image-language pretrained models produce modest gains on 3D QA benchmarks, but are outperformed by specialized 3D-VL pretraining that models 3D relations and multi-granular alignments.",
            "uuid": "e1719.4",
            "source_info": {
                "paper_title": "3D Vision and Language Pretraining with Large-Scale Synthetic Data",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-3",
            "name_full": "GPT-3 (Language models are few-shot learners)",
            "brief_description": "A large language model used in this paper to rephrase templated descriptions into more natural free-form captions for SynVL3D; mentioned as a tool in dataset generation rather than as a transferred agent for 3D tasks.",
            "citation_title": "Language models are few-shot learners",
            "mention_or_use": "mention",
            "model_agent_name": "GPT-3",
            "model_agent_description": "A large autoregressive transformer language model pretrained on massive text corpora; used in this work to paraphrase and naturalize template-based descriptions while preserving object phrase identifiers.",
            "pretraining_data_type": "massive web text corpora (general natural language)",
            "pretraining_data_details": "Not specified in this paper beyond the citation; used only as a rephrasing tool to convert template descriptions into free-form natural language while keeping object identifiers intact.",
            "embodied_task_name": "None in this paper (GPT-3 is not fine-tuned or transferred to embodied 3D tasks here; it is only used for dataset text generation).",
            "embodied_task_description": null,
            "action_space_text": "n/a (GPT-3 is a language generator, not an action-producing text-environment agent in this work).",
            "action_space_embodied": null,
            "action_mapping_method": null,
            "perception_requirements": null,
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": null,
            "transfer_failure_factors": null,
            "key_findings": "GPT-3 was used as a dataset augmentation tool (paraphrasing templated descriptions) to increase the naturalness of synthetic captions while preserving phrase-region IDs; it was not used as an agent transferred to embodied 3D tasks in this work.",
            "uuid": "e1719.5",
            "source_info": {
                "paper_title": "3D Vision and Language Pretraining with Large-Scale Synthetic Data",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "EmbodiedGeneralist (ref)",
            "name_full": "An embodied generalist agent in 3d world (cited reference)",
            "brief_description": "A referenced paper title that likely describes an embodied agent in 3D worlds; cited in related work but not used or analyzed in detail here.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_agent_name": "An embodied generalist agent in 3d world (reference)",
            "model_agent_description": "Referenced work (title only in bibliography) presumably about an embodied agent trained for 3D interactive environments; the SynFormer3D paper does not provide details or experiment results for it.",
            "pretraining_data_type": null,
            "pretraining_data_details": null,
            "embodied_task_name": null,
            "embodied_task_description": null,
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": null,
            "perception_requirements": null,
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": null,
            "transfer_failure_factors": null,
            "key_findings": null,
            "uuid": "e1719.6",
            "source_info": {
                "paper_title": "3D Vision and Language Pretraining with Large-Scale Synthetic Data",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "3D-VISTA",
            "rating": 2
        },
        {
            "paper_title": "3D-VLP",
            "rating": 2
        },
        {
            "paper_title": "CLIP-Guided",
            "rating": 2,
            "sanitized_title": "clipguided"
        },
        {
            "paper_title": "Multi-CLIP",
            "rating": 2,
            "sanitized_title": "multiclip"
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "An embodied generalist agent in 3d world",
            "rating": 2,
            "sanitized_title": "an_embodied_generalist_agent_in_3d_world"
        },
        {
            "paper_title": "ScanScribe",
            "rating": 2,
            "sanitized_title": "scanscribe"
        }
    ],
    "cost": 0.019664749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>3D Vision and Language Pretraining with Large-Scale Synthetic Data
8 Jul 2024</p>
<p>Dejie Yang 
Wangxuan Institute of Computer Technology
Peking University</p>
<p>Zhu Xu xuzhu@stu.pku.edu.cn 
Wangxuan Institute of Computer Technology
Peking University</p>
<p>Wentao Mo 
Wangxuan Institute of Computer Technology
Peking University</p>
<p>Qingchao Chen qingchao.chen@pku.edu.cn 
National Institute of Health Data Science
Peking University</p>
<p>National Key Laboratory of General Artificial Intelligence
Peking University</p>
<p>Siyuan Huang huangsiyuan@ucla.edu 
State Key Laboratory of General Artificial Intelligence
BIGAI</p>
<p>Yang Liu yangliu@pku.edu.cn 
Wangxuan Institute of Computer Technology
Peking University</p>
<p>National Key Laboratory of General Artificial Intelligence
Peking University</p>
<p>3D Vision and Language Pretraining with Large-Scale Synthetic Data
8 Jul 202475A699AE8684B9895CBE35F0B6B2B088arXiv:2407.06084v1[cs.CV]
3D Vision-Language Pre-training (3D-VLP) aims to provide a pre-train model which can bridge 3D scenes with natural language, which is an important technique for embodied intelligence.However, current 3D-VLP datasets are hindered by limited scene-level diversity and insufficient finegrained annotations (only 1.2K scenes and 280K textual annotations in ScanScribe), primarily due to the labor-intensive of collecting and annotating 3D scenes.To overcome these obstacles, we construct SynVL3D, a comprehensive synthetic scenetext corpus with 10K indoor scenes and 1M descriptions at object, view, and room levels, which has the advantages of diverse scene data, rich textual descriptions, multi-grained 3D-text associations, and low collection cost.Utilizing the rich annotations in SynVL3D, we pre-train a simple and unified Transformer for aligning 3D and language with multi-grained pretraining tasks.Moreover, we propose a synthetic-to-real domain adaptation in downstream task fine-tuning process to address the domain shift.Through extensive experiments, we verify the effectiveness of our model design by achieving state-of-the-art performance on downstream tasks including visual grounding, dense captioning, and question answering.Codes are available at: https://github.com/idejie/3DSyn.</p>
<p>Introduction</p>
<p>Bridging 3D scenes with natural language represents a pivotal advancement in the pursuit of embodied artificial intelligence [Huang et al., 2023] including 3D vision-language (3D-VL) tasks such as 3D visual grounding [Chen et al., 2020;Achlioptas et al., 2020], dense captioning [Chen et al., 2021], and question answering [Azuma et al., 2022;Mo and Liu, 2024],etc.</p>
<p>Drawing inspiration from the impact of 2D vision-text pretraining models [Radford et al., 2021;Alayrac et al., 2022], recent researches [Zhu et al., 2023;Jin et al., 2023] demonstrate that 3D-VL pre-training can enhance the performance  in subsequent tasks.However, the performance of these methods is limited by the size and diversity of 3D-text data, as 3D data collection largely dependents on the scanning device, introducing inherent complexities and increased costs in contrast to the simpler process of gathering 2D images.There are limitations to the existing 3D-VL datasets: (1) limited diversity of 3D vision, the largest-scale 3D-VL dataset ScanScribe [Zhu et al., 2023] combines multiple existing 3D datasets, but still only contains 1.2K indoor scenes, which causes limited vision diversity for pre-training; (2) insufficient fine-grained 3D-text associations, ScanScribe provides 278K descriptions of objects where only the sentence-level target objects is annotated and ignores the other fine-grained vision-language associations between the spatial region and other objects mentioned in the sentence.(3) labor-intensive data collection procedure, the 3D data collection of a real scene needs 30 minutes with advanced scanners [Yeshwanth et al., 2023] and annotating a scene with the help of the automatic segmentation still requires 1.93 minutes, makes largescale real-world 3D scene infeasible to collect.</p>
<p>To address the limitation of existing 3D-VL datasets, we propose the dataset SynVL3D, which contains the freecollected and easy-to-annotate synthetic 3D data generated by a 3D scene simulator [Deitke et al., 2022], instead of using re-alistic data.As shown in Figure 1, our dataset SynVL3D has four advantages: (1) Diverse scene data.Benefit from the capability of the 3D simulator [Deitke et al., 2022], we collected 10,000 indoor 3D scenes with 108 object categories.And SynVL3D provides semantic (category), visual (high-quality mesh), and fine-grained visual annotation (error-free position, orientation and segmentation mask) of each object .To acquire fine-grained understanding for scenes, we formed a scene graph for each house (scene) with the relationships between objects based on their spatial position, size, and shape.</p>
<p>(2) Rich textual description.We utilize templates, off-theshelf image captioners [Dai et al., 2023], andGPT3 [Brown et al., 2020] to generate appearance and relationship descriptions of objects, and obtain more than 1M template-based or free-form descriptions.(3) Multi-grained association.Based on the above textual descriptions and scene graph, we retrain a distinct identifier for every noun phrase, creating a connection between each identifier and a spatial region.This establishes a robust phrase-region association, enhancing the grounded interpretation and facilitating a more meaningful evaluation.(4) Low collection cost.Based on simulators, we can obtain free 3D scene data, use off-the-shelf models to automatically describe scenes and obtain fine-grained annotations with few manual annotations.With virtual scenes, we also avoid privacy issues compared to real scenes.</p>
<p>Based on the diverse and fine-grained annotations of large scale of 3D scene provided by SynVL3D, we can construct more fine-grained pre-training tasks.The tasks include relationship prediction between objects, multi-level and viewaggregated region-word alignment, beyond scene-text matching and masked language/object modeling that previous pretraining methods [Zhu et al., 2023;Jin et al., 2023] use.The multi-grained 3D VLP tasks enable the model to capture rich 3D vision-language knowledge, which benefits downstream vision-language tasks.However, pre-training based on synthetic 3D data suffers from the domain shift between the synthetic data and the realistic data used in downstream tasks.Thus, we introduce synthetic-to-real adaptation with domain discriminator in the fine-tuning process of downstream tasks to reduce the representation distribution difference between the synthetic and realistic data of our pre-trained model.The efficacy of our model is evaluated across 3D-VL tasks, including visual grounding (e.g., ScanRefer [Chen et al., 2020], Nr3D/Sr3D [Achlioptas et al., 2020]), dense captioning (e.g., Scan2Cap [Chen et al., 2021]), question answering (e.g., ScanQA [Azuma et al., 2022]).</p>
<p>Our main contributions can be summarized as follows: (1) We construct a large-scale synthetic dataset SynVL3D, featuring diverse 3D scenes, textual descriptions, and finegrained 3D region-text associations at a low manual cost, which is a significant improvement in terms of data diversity and scale compared to prior datasets.(2) We introduce a pretraining framework featuring new auxiliary tasks such as pairwise object relationship prediction and multi-level 3D regionphrase alignment to enhance comprehensive understanding.To address the domain shift between synthetic and real 3D data, we introduce a domain adaptation technique, improving the fine-tuning process for downstream tasks.(3) We demonstrate that with the data scale-up and model design, our model SynFormer3D achieves state-of-the-art performance on a variety of 3D-VL tasks, including visual grounding, dense captioning, and question answering.</p>
<p>Related Work</p>
<p>3D Vision-Language Dataset</p>
<p>There has been a growing interest in 3D-VL datasets.[Chen et al., 2020] and [Achlioptas et al., 2020] introduced the ScanRefer and ReferIt3D datasets, which serve as benchmarks for anchoring sentence-level natural language to ground the 3D object.[Abdelreheem et al., 2024] and [Yuan et al., 2022a] extend to provide phrase-level grounding annotations.Scan2Cap [Chen et al., 2021] provides the textual descriptions about the scene in ScanRefer for captioning.ScanQA [Azuma et al., 2022] developed a 3D questionanswering dataset, which annotates the objects mentioned within the questions and answers.ScanScribe [Zhu et al., 2023] endeavors to build a large-scale dataset by combining the aforementioned datasets for pre-training purposes.However, these 3D-VL datasets are constrained by a lack of scenelevel diversity, fine-grained annotations (the largest existing dataset ScanScribe with only 1.2K scenes and 280K textual descriptions, and the laborious process involved in collecting and annotating 3D scenes.To overcome these obstacles, we constructed the dataset SynVL3D, which features freely collected and easily annotated 3D data generated by a 3D scene simulator [Deitke et al., 2022], as opposed to utilizing realistic data.It encompasses diverse 3D scene data (10K scenes), provides 1M rich textual descriptions with low cost.Moreover, compared to the sentence-region association in Scan-Scribe, we provide the phrase-region finegrained association, which means that most noun phrase in description has an association with the spatial region in the 3D scene.</p>
<p>Large-scale 3D-VL Pre-training</p>
<p>In recent years, large-scale pre-training has emerged as a fundamental paradigm in natural language processing (NLP) [Radford et al., 2018;Radford et al., 2019;Brown et al., 2020] and 2D vision-and-language (2D-VL) domains [Lu et al., 2019;Radford et al., 2021].While the pre-training technique has become indispensable in these domains, its application in the 3D-VL field remains relatively uncharted.3D-VISTA [Zhu et al., 2023] seeks to pioneer a pre-training model for 3D-VL tasks, employing strategies such as masked language modeling, masked object modeling, and scenesentence matching.Concurrently, 3D-VLP [Jin et al., 2023] introduces a context-aware spatial-semantic alignment to forge connections between point clouds and the textual descriptions of ground objects.However, the efficacy of these pre-training tasks is often compromised by the lack of detailed annotations in existing datasets.In this paper, we propose three fine-grained pre-training tasks, including object relations prediction, multi-level region word alignment and view-aggregated region word alignment, to enhance the model's perception capability of object relationships, scenes at different levels and from various views.However, there are few works exploring the simulation-toreal domain adaptation in 3D cross-modality applications, where the domain shift exists in both 3D vision and language.</p>
<p>To address this dual and joint domain shift, we propose a taskspecific fine-tuning strategy with domain adaptation in 3D vision, language, and joint 3D-language domains.</p>
<p>3 Dataset Generation: SynVL3D</p>
<p>In this paper, we introduce SynVL3D, the first large-scale dataset of 3D synthetic scene-text pairs designed for 3D-VL pre-training.IAs indicated in Table 1, SynVL3D not only surpasses existing 3D-VL datasets in size but also offers a richer variety of 3D scenes, text, and fine-grained 3D-text annotations.Our dataset SynVL3D showcases key advantages in four aspects:</p>
<p>Diverse 3D Scene Data.We collected mesh data for 10,000 distinct scenes from an off-the-shelf 3D scene simulator [Deitke et al., 2022].It encompasses 108 object categories, totaling over 1 million distinct instances.Besides the variety surpasses the existing datasets as shown in Table 1, we provide following annotation: (1) For each Single Instance, we automatically acquire its semantic category and precise bounding boxes and segmentation masks directly from the object and scene information provided by the simulator [Deitke et al., 2022] at no extra cost.(2) For Multiple Instance relationship, we construct a scene graph for each scene based on densely annotated objects and their attributes, where each node represents an object instance, and edges represent the relationships between an instance and its neighbors.Inspired by [Wald et al., 2020], the scene graph encompasses a rich array of relationships, which can be classified into three types: a) support relationships(e.g., standing, lying); b) spa-tial relationships (e.g., next to, in front of) that consider the relative positions of neighboring instances; and c) comparative relationships (e.g., bigger than, same shape as) that are based on the size and shape of an instance.</p>
<p>Rich Textual Description.We have generated more than 1 million textual descriptions, both templated and free-form, encapsulating detailed appearances and relationships between objects.</p>
<p>(1) Template-based: For object appearances, we utilize an off-the-shelf image captioner [Dai et al., 2023] to generate captions for the front view of each object.We manually reviewed these descriptions and make corrections where necessary.For relationship triplets of the form <object1, relation, object2>, we craft descriptions using the template "the object1 is relation to object2".The description of an instance is structured as a paragraph by merging its appearance description with the relationship descriptions of its neighbors.</p>
<p>(2) Free-form: We leverage GPT-3 [Brown et al., 2020] to rephrase the templated description to enhance the naturalness.These descriptions not only enrich the dataset with diverse visual perspectives but also enhance the semantic understanding of spatial relations and object attributes.</p>
<p>Phrase-Region Association.The grounding annotations in previous datasets [Achlioptas et al., 2020;Chen et al., 2020;Zhu et al., 2023] mainly consist sentence-region association for the single referred target object.In this paper, for 10K scene, we aim to curate grounding annotations of most objects in each description by providing phrase-region explicit association with an identifier of a phrase that refers to an object in a 3D scene.In practice, GPT-3 is tasked with rephrasing the descriptions while maintaining a critical requirement: the preservation of the identifier for each object, formatted as category(id of instance), throughout the rephrasing process.This ensures that the region-word association remains intact, maintaining the accuracy of object referencing in the dataset.</p>
<p>Low Collection Cost.Unlike other datasets in Table 1 that are manually collected and annotated from real-world scenes, our SynVL3D leverages freely synthetic 3D scene data.We utilize off-the-shelf models for automatic scene description and require minimal manual annotation cost.By using virtual scenes, our approach also circumvents privacy issues that can arise from scanning real-world environments.</p>
<p>Methodology</p>
<p>As shown in Figure 2, our SynFormer3D first uses multimodal encoder to extract and fuse 3D and text features.The pipeline mainly consists two steps: synthetic pre-training (Section 4.2) and task-specific fine-tuning (Section 4.3).</p>
<p>Multi-Modal Encoder</p>
<p>3D-Object Encoder</p>
<p>Masked Object Modeling</p>
<p>Cross-Modal Fusion</p>
<p>Multi-Modal Encoder</p>
<p>Masked Language Modeling</p>
<p>Scene-Sentence Matching Object Relation Prediction</p>
<p>Multi-level Region-Word Alignment</p>
<p>View-aggregated Region-Word Alignment</p>
<p>Triple Domain Adaptation Discriminator</p>
<p>Object Token</p>
<p>Synthetic Pre-Training</p>
<p>To align 3D scene and text in self-supervised manner, pretraining tasks Masked Language Modeling (MLM), Masked Object Modeling (MOM), and Scene-Sentence Matching (SSM) are adopted.Further, to achieve better alignment, we propose three fine-grained pre-training tasks based on the fine-grained annotation in SynVL3D: Object Relationship Prediction (ORP), Multi-level Region-Word Alignment (MRWA) and View-aggregated Region-Word Alignment (VRWA), details are as follows:</p>
<p>ORP.The previous methods [Zhu et al., 2023;Jin et al., 2023] neglect the importance of relationship understanding between object pairs, resulting in limitations for downstream tasks.And we introduce object relationship prediction task, which predicting pairwise relationship with prediction head r(•, •), and the relationship annotations in SynVL3D are utilized to form label.
L ORP = − 1 M 2 M i=1 M j=1 R i,j • log(r(O i , O j )),(1)
where i and j denote the indices of object features within the set of object proposal features O, M is the total number of object proposal and R i,j is the label of relations between object i and j.</p>
<p>MRWA. 3D tasks and operations involve multi-level targets with varying granularity, ranging from object, room, to entire scene.To achieve multi-level alignment, we align regions and words at object, room and scene levels respectively:
L(x, z) = − 1 L • M L i=1 M j=1 y i,j • log(p(x i , z j )), (2) L MRWA = l L(W l , O l ),(3)
where L and M are the length of text(word) token sequence and the number of object features respectively.y i,j means the binary label of whether the i-th word x i and j-th region(object) z j is matching.l ∈ {object, room, scene} indicate the levels of a single object, room, and the entire scene, respectively.W l represents the word feature sequence from descriptions at different levels , and O l is the object feature set derived from the point cloud of the corresponding level.p(•, •) is the region-word matching prediction head.The set of objects involved in different levels of textual description is different.At the object level, only objects near the description are mentioned, while room and house levels provide a larger range of objects.Therefore, region-word alignment at different levels considers the relationships between objects from the object set at different levels (the self-attention is conducted on different set of objects/words).</p>
<p>VRWA.The perspective changes when executing operations or object searching in 3D scenes, which makes viewaggregated object-text alignment critical.Following [Guo et al., 2023], we rotate the point cloud of a 3D scene into V to different views using the annotated camera parameters from SynVL3D: P v = R(θ v )P, where R(θ) is the rotation matrix [Guo et al., 2023], P is the scene point cloud and θ v is the rotation angle adopted for v-th view: θ v = 2π V (v − 1), v ∈ {1, . . ., V } for a total of V views.R(θ v ) × P represents rotating the point cloud of whole scene clockwise by θ v degrees along the Z-axis.Then we encode the v-th view features O v , and apply multi-modal encoder and aggregate multi-view information for the i-th object:
G i = 1 V V v=1 O v i .
To achieve precise alignment between regions and words (as nouns in descriptions correspond to object identifiers in Figure 2), we perform region-phrase matching besides aligning scene and text descriptions:
L VRWA = − 1 L • M L i=1 M j=1 z i,j log(s(W i , G j )). (4)
where z i,j is the matching label of i-th word W i and j-th region(object) G j .s(•, •) is the region-word matching prediction head.Final Pre-training Loss.The final pre-training objective combines the losses from the proxy tasks mentioned earlier:
L pre-train =α(L MLM + L MOM + L SSM ) + (1 − α)(L ORP + L MRWA + L VRWA ),(5)
where α is a hyperparameter used to balance the basic auxiliary tasks(the losses L MLM , L MOM , L SSM is following [Zhu et al., 2023]) with our proposed fine-grained pre-training tasks.Note that the pre-training approach we propose is selfsupervised and downstream task-agnostic.</p>
<p>Task-Specific Fine-Tuning</p>
<p>By incorporating lightweight task-specific decoders, the pretrained model can be easily adapted to a variety of 3D-VL tasks.Due to the possible domain shift in our synthetic dataset and downstream task data (real scene scan data), we propose a task-specific fine-tuning strategy for synthetic-toreal domain adaptation to be better generalized to downstream tasks.In particular, we fine-tune the model on the following tasks: 3D Visual Grounding, 3D Dense Captioning, 3D Question Answering.We equip each task a specific lightweight decoder and corresponding fine-tuning losses.Synthetic-to-Real Adaptation.Domain shift from multiple perspectives between synthetic data for pre-training and realistic data used in downstream tasks may hinder the performance on various 3D VL tasks.Vision differences can be observed between the synthesized data the scanned real data as shown in Figure 2. Besides, the descriptions in our dataset are generated by templates or GPT, which also has a gap with artificially annotated natural language descriptions in downstream datasets.Moreover, downstream tasks also involve the use of joint vision language, such as 3D question answering, where this type of domain shift requires a joint adaptation.Therefore, we introduce synthetic-to-real adaptation with a triple domain discriminator (shown in Figure2) in the fine-tuning process of downstream tasks to reduce the representation distribution difference between the synthetic and realistic data of our pre-trained model.The triple domain discriminator consists of three separate discriminator for vision, language and vision-language joint domains, addressing unimodal and cross-modal domain shifts.</p>
<p>A vision domain discriminator D v is placed after multimodal encoder E to predict the domain label of the object features O:
L vision = max E min Dv L BCE (O)(6)
where L BCE denotes the binary cross entropy loss.Gradient Reverse Layers(GRL) is adopted for min-max optimization.</p>
<p>Similarly, a language domain discriminator D l is adopted to predict the domain label of sentence tokens Ŵ:
L lang = max E min D l L BCE ( Ŵ)(7)
where the sentence token Ŵ is the mean of the text token W 1:L in a sentence.In order to reduce the gap of 3D-text alignment in different domains, we also introduce a joint discriminator D joint .And the adaptation loss for shared-domain is:
L joint = max E min Djoint L BCE ([ Ŵi ; O i ])(8)
where [ Ŵi ; O i ] denote i-th data sample (setence and corresponding 3D object) which is the concatenation of the Ŵ and O.</p>
<p>The final synthetic-to-real adaptation loss can be combined as: L align = L vision + L lang + L joint .Final Fine-tune Loss.The final fine-tuning objective combines the losses from the specific downstream task and synthetic-to-real domain adaptation:
L f =βL task + (1 − β)L align ,(9)
where L task is the specific loss of the downstream task mentioned above and where β is a hyperparameter used to balance the downstream task loss and domain-adaptation loss.</p>
<p>Experiments</p>
<p>Implementation Details</p>
<p>The pre-training runs for 100 epochs with a batch size of 64 with NVIDIA A100 GPUs.</p>
<p>We set the balance hyper-parameters α, β as 0.5 and 0.8.We use the AdamW [Loshchilov and Hutter, 2019] optimizer and learning rate is set to 1e −4 .</p>
<p>Downstream Task</p>
<p>3D Visual Grounding.In Table 2, we evaluate our model on Nr3D, Sr3D [Achlioptas et al., 2020] and ScanRefer [Chen et al., 2020].For Nr3D/Sr3D, we report the results as the grounding accuracy.For ScanRefer, we follow [Chen et al., 2020] to use detector-generated object proposals and report the results as Acc@IoU&gt; k.As shown Table 3: Dense Captioning results on Scan2Cap dataset."C" stands for "CIDEr", "B-4" for "BLEU-4", "M" for "METEOR", and "R" for "ROUGE", respectively."@0.25" and "@0.5" represent the 3D IoU between the predicted and annotated box. in Table 2, SynFormer3D obtains an overall accuracy of 67.5% and 78.6% on Nr3D and Sr3D, which has gains of +1.1% and +1.5% compared to the previous leading methods.It indicates that SynFormer3D is trained on our largescale SynVL3D with diverse 3D vision and text can understand complex 3D scenes better.Notably, our approach can be adaptive to different views better(+1.7%and +5.2% on "View-Dep" compared with 3D-VISTA), benefiting from the well-explored view-aggregated region-word alignment during the pre-training stage.Our SynFormer3D obtains an overall accuracy of 52.3% @0.25 and 46.2% @0.5, which outperforms all other methods on ScanRefer.Compared to the pretraining methods [Jin et al., 2023;Zhu et al., 2023], we introduce more fine-grained pre-training tasks benefiting from the scale-up data and rich and accurate annotations of our SynVL3D.</p>
<p>3D Dense Captioning.In Table 3, we evaluate our model on the Scan2cap.</p>
<p>Our method outperforms the second best scores on CIDEr@0.25 (+1.1%),METEOR@0.25 (+0.5%),BLEU-4@0.5 (+2.6%),METEOR@0.5 (+0.9%) and ROUGE@0.5 (+0.5%).While our method achieves stateof-the-art in most metrics, it is only comparable with previous methods on BLEU-4@0.25,ROUGE@0.25 and CIDEr@0.5.The cause could be attributed to the fact that the textual descriptions utilized in pretraining are automatically generated with minimal manual annotations.This may result in gaps and biases about fluency and free-form expression.4) shows our method surpasses existing models on both EM@1 (+0.6%/+1.1%),EM@10(+0.4%/+0.4%)and captioning metric ROUGE(+0.6%/+0.5%).While the textual input of 3D-QA is a question rather than referring expression , our method still benefits the 3D QA task with the multi-modal features from multi-grained alignment learning.</p>
<p>Ablation Studies</p>
<p>We conduct the ablation studies on three tasks in Tables 5  to 7. 'VG' stands for overall accuracy of 3D visual grounding on Nr3D, 'Cap' for the CIDEr@0.25 result on Scan2Cap and 'QA' for the EM@1 results(w/ objects) on ScanQA.</p>
<p>Effectiveness of the fine-grained pre-training tasks.We evaluate the effectiveness of fine-grained alignment tasks in enhancing 3D visual grounding, dense captioning, and question-answering tasks ( between synthetic and real-world down-stream data (Table 6).(1) 3D Vision Domain Adaptation (Row 2): By applying adaptation to the 3D vision domain, we observe performance increases of 9.2%, 6.7%, and 0.8% for 3D visual grounding, dense captioning, and question-answering tasks, respectively.This highlights the significance of 3D domain adaptation.</p>
<p>(2) Language Domain Adaptation (Row 3): Extending adaptation to the language domain further enhances results, with gains of 2.8%, 2.7%, and 1.0%.This step demonstrates the value of refining the language domain.(3) Joint Vision-Language Domain Adaptation (Row 4): Finally, incorporating vision-language joint domain adaptation leads to additional improvements of 2.4%, 4.0%, and 2.1%.This confirms the effectiveness of our comprehensive approach in reducing domain discrepancies and improving overall performance.</p>
<p>Ablation of MRWA levels.We analyze the impact of different MRWA levels (object, room and house) across three tasks (Table 7), by comparing with baseline object-only alignment.</p>
<p>(1) Room-Level Alignment (Row 2): Adding roomlevel alignment to the baseline object-only alignment (Row 1) yields improvements of 0.4%, 1.2%, and 0.4%.This suggests that room-level associations between words and objects enhance the model's understanding of object relationships within rooms.</p>
<p>(2) House-Level Alignment (Row 3): Extending alignment to the house level (whole scene) leads to gains of 0.2%, 0.8%, and 0.2%.This level of alignment assists the model in comprehending the entire scene by providing associations across the house.(3) Combining All Levels (Row 4): Combining alignments at both room and house levels results in the most substantial improvements of 0.8%, 2.3%, and 0.5%.It indicates the effectiveness of multi-level alignment in improving the model's 3D scene perception.</p>
<p>Conclusion</p>
<p>To tackle the scarcity of data and concerns regarding data collection cost for the 3D vision-language pretraining, we employ synthetic data generation using 3D simulators and offthe-shelf models and create a substantial and diverse synthetic dataset SynVL3D.With the rich annotations in SynVL3D, we pre-train a model SynFormer3D aligning 3D and language with diverse auxiliary tasks, including predicting object relationships, multi-level and view-aggregated regionword alignment.A domain adaptation strategy in fine-tuning is adapted to downstream tasks.Extensive comparisons on various benchmarks show the effectiveness of our method.</p>
<p>This is a state-of-theart, black [fridge] that exhibits a sleek, modern aesthetic… it is left by [plant], and close to [painting] … template free-formMulti-Grained AssociationThis is a state-ofthe-art, black [fridge] that exhibits a sleek, modern aesthetic… it is left by[plant]</p>
<p>Figure 1 :
1
Figure 1: Advantages of our proposed dataset SynVL3D.</p>
<p>Following[</p>
<p>Zhu et al., 2023], firstly we employ text encoder to encode words to text tokens {w CLS , w 1:L }, where w CLS denotes the [cls] token and L denotes the length of a description.The 3D-object encoder is to encode M object proposals and extract features from the input point cloud, represented by o 1:M .To achieve multi-modal fusion, we concatenate the text tokens and the 3D object tokens {w CLS , w 1:L , o 1:M } andFine-TunePre-TrainThis is a state-of-the-art, black [fridge] that exhibits a sleek, modern aesthetic… it is left by [plant], and close to [painting] … This is a [trash can] … It is behind the another [trash can].It is on the left side of the [kitchen cabinet]</p>
<p>Figure 2 :
2
Figure 2: The model architecture of our SynFormer3D.The multi-modal encoder includes a 3D-object encoder, text encoder, and cross-modal fusion modules.Compared to previous pre-trained models, our SynFormer3D introduces more fine-grained auxiliary pre-training tasks, which include Object Relation Prediction , Multi-level and View-aggregated Region-Word Alignment.</p>
<p>Table 1 :
1
[Pei et al., 2024;Zhao et al., 2023;Pei et al., 2023;Xu et al., 2022;Chen and Liu, 2020;Chen et al., 2018]tes the textual description size.2.3Simulation-to-RealDomainAdaptationSimulation-to-realdomain adaptation refers to the process of transferring knowledge acquired in a simulated environment to the real world, and is widely applied in 2D vision[Pei et al., 2024;Zhao et al., 2023;Pei et al., 2023;Xu et al., 2022;Chen and Liu, 2020;Chen et al., 2018]and 3D fields like
DatasetTypeText SceneAnnotationNr3D [Achlioptas et al., 2020]Realistic31K707Sentence-RegionScanRefer [Chen et al., 2020]Realistic37K800Sentence-RegionScanQA [Azuma et al., 2022]Realistic41K800Sentence-RegionScanEnts3D [Abdelreheem et al., 2024] Realistic84K705Phrase-RegionPhraseRefer [Yuan et al., 2022a]Realistic88K707Phrase-Region3R-Scan [Wald et al., 2019]Realistic90K478Sentence-RegionSr3D [Achlioptas et al., 2020]Realistic91K707Sentence-RegionScanScribe [Zhu et al., 2023]Realistic 278K 1.2K Sentence-RegionSyn3D(Ours)Synthetic1M10KPhrase-Region
[Chen et al., 2023]2021] et al., 2021b], indoor scenes layout[Ding et al., 2022], and robotics[DeBortoli et al., 2021].Existing methods primarily aim to design a simulation-to-real domain adaptation framework for uni-modal applications, such as the 3D semantic segmentation[Zhao et al., 2021b;  Ding et al., 2022], point cloud object detection[DeBortoli et al., 2021]and point cloud registration[Chen et al., 2023].</p>
<p>Table 2 :
2
Grounding accuracy (%) on Nr3D, Sr3D and ScanRefer.The best and second-best results are in bold and underlined.
Nr3DSr3DScanReferMethodOverall Easy HardView DepView IndepOverall Easy HardView DepView Indepacc 0.25acc 0.53DVG-Trans [Zhao et al., 2021a]40.848.534.834.843.751.454.244.944.651.747.6 34.7TransRefer3D [He et al., 2021]48.056.739.642.550.757.460.550.249.957.7--LAR [Bakr et al., 2022]48.958.442.347.452.159.463.051.250.059.1--SAT [Yang et al., 2021]56.564.948.454.457.657.961.250.049.258.344.5 30.13D-SPS [Luo et al., 2022]51.558.145.148.053.262.656.265.449.263.248.8 37.0MVT [Huang et al., 2022]59.567.452.759.160.364.566.958.858.464.740.8 33.3ViL3DRel [Chen et al., 2022]64.470.257.462.064.572.874.967.963.873.247.9 37.7ScanRefer [Chen et al., 2020]----------41.2 27.43DJCG [Cai et al., 2022]----------49.6 37.33D-VLP[Jin et al., 2023]----------51.4 39.53D-VISTA[Zhu et al., 2023]64.272.156.761.565.176.478.871.358.977.350.6 45.8Ours65.573.256.363.266.177.979.273.364.179.252.3 46.2MethodCB-4@0.25MRCB-4@0.5MRScan2Cap [Chen et al., 2021]53.734.326.155.035.222.421.443.5No Pre-trainX-Trans2Cap[Yuan et al., 2022b] MORE[Jiao et al., 2022]58.8 58.934.2 35.425.8 26.454.1 55.441.5 39.023.8 23.021.9 21.745.0 44.33DJCG [Cai et al., 2022]60.939.727.559.047.731.524.351.83D-VLP [Jin et al., 2023]64.139.827.758.850.031.924.551.5Pre-train3D-VISTA[Zhu et al., 2023]71.036.528.457.666.934.027.154.3Ours72.137.328.958.264.234.628.054.8</p>
<p>Table 4 :
4
The best and second-best results are in bold and underlined.Answer accuracy on ScanQA using object proposals from Mask3D.Each entry denotes "test w/ object" / "test w/o object".The best and second-best results are in bold and underlined.
MethodEM@1EM@10BLEU-4ROUGEMETEORCIDErScanQA [Azuma et al., 2022]23.5 / 20.9 56.5 / 54.1 12.0 / 10.8 34.3 / 31.1 13.6 / 12.6 67.3 / 60.2No Pre-trainCLIP-Guided [Parelli et al., 2023] 23.9 / 21.4-/ -14.6 / 11.7 35.2 / 32.4 13.9 / 13.3 69.5 / 62.8Multi-CLIP [Delitzas et al., 2023] 24.0 / 21.6-/ -12.7 / 12.9 35.5 / 32.6 14.0 / 13.4 68.7 / 63.23D-VLP[Jin et al., 2023]25.2 / 20.4 55.2 / 51.510.5 / 8.735.5 / 29.6 13.8 / 11.6 68.6 / 55.7Pre-train3D-VISTA[Zhu et al., 2023]27.0 / 23.0 57.9 / 53.5 16.0 / 11.9 38.6 / 32.8 15.2 / 12.9 76.6 / 62.6Ours27.6 / 24.1 58.3 / 54.5 16.3 / 12.3 39.2 / 33.3 14.9 / 13.1 76.2 / 62.7</p>
<p>Table 5 :
5
Ablation on Fine-grained Pre-training Tasks.
ORPVRWAMRWAVGCapQA×××60.563.025.1✓××62.965.825.9×✓×63.367.126.2××✓63.166.826.5✓✓×66.769.827.1✓✓✓67.572.127.6visionlangjointVGCapQA×××53.158.723.7✓××62.365.424.5✓✓×65.168.125.5✓✓✓67.572.127.6</p>
<p>Table 6 :
6
Ablation of Synthetic-to-Realistic Adaptation.3D Question Answering.We evaluate our model on the ScanQA dataset [Azuma et al., 2022].The quantitative comparison (Table</p>
<p>Table 5
5
Effectiveness of synthetic-to-realistic domain adaptation.We evaluated the effectiveness of utilising three specific domain adaptation discriminators that address the domain gap
): (1) ORP (Row 2): Com-pared to the baseline (Row 1), ORP achieves improvementsof +2.4%, +2.8%, and +0.8% for 3D visual grounding,dense captioning, and question-answering tasks, respectively.This suggests ORP's effectiveness in understanding object-environment relations in 3D scenes. (2) VRWA (Row 3):VRWA leads to gains of +2.8%, +4.1%, and +1.1%. Itsfocus on viewpoint-based region-word alignment seems toenhance adaptability from various perspectives. (3) MRWA(Row 4): Incorporating MRWA results in +2.6%, +3.8%, and+1.4% improvements. MRWA's multi-granularity alignment(object/room/scene) appears to enhance model perception in3D environments. (4) VRWA and MRWA v.s. ORP: BothVRWA and MRWA show more significant improvements invisual grounding and question-answering than ORP, likelydue to the enhanced perception across multiple perspectivesand granularities from VRWA and MRWA, that is required byVG and QA tasks. (5) Integrating All Tasks: The combinationof ORP, VRWA, and MRWA yields the highest gains: +7.0%,+9.1%, and +2.5%, demonstrating their capabilities.</p>
<p>Table 7 :
7
Ablation of Multi-level Region-Word Alignment .</p>
<p>AcknowledgementsThis work was supported by the grants from the National Natural Science Foundation of China 62372014.
Ahmed Abdelreheem, Fei Xia, Mohamed Elhoseiny, and Leonidas Guibas. Referit3d: Neural listeners for fine-grained 3d object identification in real-world scenes. Abdelreheem, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer VisionSpringer2024. 2024. 2020ECCV</p>
<p>Eslam Mohamed Bakr, Yasmeen Alsaedy, and Mohamed Elhoseiny. Look around and refer: 2d synthetic semantics knowledge distillation for 3d visual grounding. Alayrac, arXiv:2204.14198CVPR. Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D KaplanTom Brown2022. 2022. 2022. 2022. 2022. 202033Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda AskellarXiv preprintNeurIPS. et al. Language models are few-shot learners. NeurIPS</p>
<p>3djcg: A unified framework for joint dense captioning and visual grounding on 3d point clouds. Cai, AAAI Conference on Artificial Intelligence (AAAI). 2022. 2022. 2020CVPR</p>
<p>Re-weighted adversarial adaptation network for unsupervised domain adaptation. Chen , Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Dave Zhenyu, Chen , Angel X Chang, Matthias Nießner, the IEEE Conference on Computer Vision and Pattern RecognitionSpringer2018. 2018. 2020. 2020ECCV</p>
<p>Scan2cap: Context-aware dense captioning in rgb-d scans. Chen , CVPR. 2021. 2021</p>
<p>Language conditioned spatial relation reasoning for 3d object grounding. Chen , 2022. 2022NeurIPS</p>
<p>Sira-pcr: Sim-toreal adaptation for 3d point cloud registration. Chen , Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023. 2023</p>
<p>Multiclip: Contrastive vision-language pre-training for question answering tasks in 3d scenes. Dai, arXiv:2305.06500arXiv:2303.16894Grasp the multi-view knowledge for 3d visual grounding with gpt and prototype guidance. Dailan He2023. 2023. 2021. 2021. 2022. 2022. 2023. 2023. 2022. 2023. 20216arXiv preprintProceedings of the 29th ACM International Conference on Multimedia</p>
<p>Multi-view transformer for 3d visual grounding. Huang, arXiv:2311.12871Baoxiong Jia, and Siyuan Huang. An embodied generalist agent in 3d world. 2022. 2022. 2023. 2023arXiv preprintCVPR</p>
<p>More: Multiorder relation mining for dense captioning in 3d scenes. Jiao, 2022</p>
<p>Context-aware alignment and mutual masking for 3d-language pre-training. Jin , Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionTel Aviv, IsraelIlya Loshchilov and Frank HutterOctober 23-27, 2022. 2022. 2023. 2023. 2019. 2019Computer Vision-ECCV 2022: 17th European Conference. Decoupled weight decay regularization. ICLR</p>
<p>Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Lu, Huaxia Xia, and Si Liu. 3d-sps: Single-stage 3d visual grounding via referred point progressive selection. 2019. 2019. 2022. 202232CVPR</p>
<p>Bridging the gap between 2d and 3d visual question answering: A fusion approach for 3d vqa. Liu Mo, Wentao Mo, Yang Liu, ; Parelli, Maria Parelli, Alexandros Delitzas, Nikolas Hars, Georgios Vlassis, Sotirios Anagnostidis. 2024. 2024. 2023. 2023Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</p>
<p>Uncertaintyinduced transferability representation for source-free unsupervised domain adaptation. Pei , IEEE Transactions on Image Processing. 20232023</p>
<p>Improving language understanding by generative pre-training. Pei , IEEE Transactions on Pattern Analysis and Machine Intelligence. 2024. 2024. 2018. 2018. 201919OpenAI blog</p>
<p>Rio: 3d object instance re-localization in changing indoor environments. Radford, ICML. Johanna Wald, Armen Avetisyan, Nassir Navab, Federico Tombari, Matthias Nießner, PMLR2021. 2021. 2019ICCV</p>
<p>Learning 3d semantic scene graphs from 3d indoor reconstructions. Wald , Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2020. 2020</p>
<p>Delving into the continuous domain adaptation. Xu, Proceedings of the 30th ACM international conference on Multimedia. the 30th ACM international conference on Multimedia2022. 2022</p>
<p>Sat: 2d semantics assisted training for 3d visual grounding. Yang , Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)2021. 2021. 2023. October 2023ICCV</p>
<p>X-trans2cap: Cross-modal knowledge transfer using transformer for 3d dense captioning. Yuan, arXiv:2207.01821Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Daigang Zhao, Lu Cai, Dong Sheng, Xu, the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2022a. 2022. 2022b. June 2022. 2021a. 2021arXiv preprintICCV</p>
<p>epointda: An end-to-end simulation-to-real domain adaptation framework for lidar point cloud segmentation. Zhao, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2021b. 2021</p>
<p>Masked retraining teacher-student framework for domain adaptive object detection. Zhao, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023. 2023. 2023. 2023ICCV. IEEE</p>            </div>
        </div>

    </div>
</body>
</html>
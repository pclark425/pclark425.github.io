<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3022 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3022</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3022</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-74.html">extraction-schema-74</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-265295180</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.11797v1.pdf" target="_blank">Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have dramatically enhanced the field of language intelligence, as demonstrably evidenced by their formidable empirical performance across a spectrum of complex reasoning tasks. Additionally, theoretical proofs have illuminated their emergent reasoning capabilities, providing a compelling showcase of their advanced cognitive abilities in linguistic contexts. Critical to their remarkable efficacy in handling complex reasoning tasks, LLMs leverage the intriguing chain-of-thought (CoT) reasoning techniques, obliging them to formulate intermediate steps en route to deriving an answer. The CoT reasoning approach has not only exhibited proficiency in amplifying reasoning performance but also in enhancing interpretability, controllability, and flexibility. In light of these merits, recent research endeavors have extended CoT reasoning methodologies to nurture the development of autonomous language agents, which adeptly adhere to language instructions and execute actions within varied environments. This survey paper orchestrates a thorough discourse, penetrating vital research dimensions, encompassing: (i) the foundational mechanics of CoT techniques, with a focus on elucidating the circumstances and justification behind its efficacy; (ii) the paradigm shift in CoT; and (iii) the burgeoning of language agents fortified by CoT approaches. Prospective research avenues envelop explorations into generalization, efficiency, customization, scaling, and safety. This paper caters to a wide audience, including beginners seeking comprehensive knowledge of CoT reasoning and language agents, as well as experienced researchers interested in foundational mechanics and engaging in cutting-edge discussions on these topics. A repository for the related papers is available at https://github.com/Zoeyyao27/CoT-Igniting-Agent.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3022.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3022.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Agent-Memory-Concepts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Short-term and Long-term Memory for Language Agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Conceptual distinction and roles of short-term (working) memory and long-term memory in language agents; short-term memory stores recent temporal context (e.g., chat/action history) while long-term memory stores static/procedural/semantic/episodic knowledge often accessed via retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>language agents (general)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>General class of LLM-based agents that interact with environments via perception, planning/reasoning, tool use and that incorporate explicit memory subsystems to support multi-turn and cross-episode behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>short-term memory and long-term memory (conceptual)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Short-term memory is maintained as recent temporal context (e.g., chat/action history, recent rationales) kept in the prompt/context; long-term memory is stored externally or parametrically and accessed via retrieval methods (tree search, text search, vector retrieval), and can include procedural, semantic, and episodic memories.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>multi-step interactive agent tasks (general)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>General multi-turn tasks requiring planning, tool use, and consistency across episodes (e.g., web operation, multi-hop QA, sequential control), where memory supports retention of past actions, plans, and world facts.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>planning / multi-turn interaction / tool use</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Survey defines and motivates short-term vs long-term memory for agents: short-term memory supplies recent explicit context and impacts current state decisions; long-term memory supports cross-episode consistency and stores more abstract/procedural knowledge accessible via retrieval or parametric storage.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Long natural-language memory sequences are inefficient as they grow and exceed context windows; selecting salient information and distilling logs for efficient retrieval is an open challenge; trade-offs between storage format (linear text) and retrieval efficiency exist.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents", 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3022.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3022.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemWalker</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemWalker</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tree-structured memory approach where lengthy context is summarized into a tree of nodes; agents traverse the tree to gather relevant information for a query.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemWalker</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent memory mechanism that converts long context into a hierarchical tree of summary nodes; the agent navigates this tree via iterative prompting to locate and aggregate relevant memories before responding.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>tree-structured memory / hierarchical summaries</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>The long episodic context is preprocessed into a tree of summaries (leaf nodes: base observations; internal nodes: abstracted summaries). On query, the agent searches/navigates the tree to collect relevant nodes and composes a response after gathering sufficient information.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>long-context retrieval / multi-step agent queries (illustrative)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks that require retrieval from long interaction logs or lengthy contexts; main challenge is locating relevant pieces of information within lengthy histories while staying within context length limits.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>memory retrieval / multi-turn interaction</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Tree-structured memory (MemWalker) enables agents to access long context efficiently by organizing summaries hierarchically and searching over them, reducing token consumption compared to linear histories.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires building and maintaining the tree summaries; search/navigation strategy and selecting when to descend or stop are design challenges; potential loss of detail in summaries and costs to construct/maintain the tree.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents", 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3022.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3022.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GITM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GITM (LLM Decomposer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based decomposer that recursively decomposes goals into a sub-goal tree and uses that hierarchy for memory and planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GITM (LLM Decomposer)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses recursive decomposition of a top-level goal into a hierarchical sub-goal tree, storing the structure as part of the agent's memory/planning state to capture relationships between goals and plans.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>tree-structured sub-goal memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>The agent constructs a hierarchical decomposition of goals (sub-goal tree) that encodes relations between goals and actions; the tree serves as an explicit memory/planning artifact to guide retrieval and decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>hierarchical planning / goal decomposition tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks requiring multi-step planning and capturing goal-plan relationships (e.g., multi-step problem solving and complex task execution) where the challenge is to maintain coherent plans across steps.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>planning / hierarchical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Decomposing goals into a sub-goal tree helps explicitly capture relationships between goals and plans, supporting memory-efficient planning and clearer mapping from memories to actions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Construction and maintenance of the decomposition can be computationally costly; correctness of decomposition and alignment with environment changes are open issues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents", 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3022.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3022.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflection-Tree</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflection Tree (Park et al. / Communicative Agent Memory)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A memory organization where trivial observations are periodically abstracted into higher-level thoughts, forming a tree whose leaves are base observations and internal nodes are increasingly abstract reflections.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative agents: Interactive simulacra of human behavior</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reflection Tree (communicative agent)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Memory mechanism for communicative agents that periodically reflects on collected observations to form abstracted memories organized in a tree, enabling higher-level reasoning and socially consistent behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>hierarchical reflective memory / tree of reflections</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>The agent accumulates base observations as leaf nodes; periodic reflection composes abstract higher-level thought nodes above them; this hierarchical memory is used to inform future behavior and maintain coherent persona across interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>multi-turn social/communicative simulations</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks simulating human-like agent interactions where agents must maintain consistent persona and reference past observations; main challenge is aggregating many trivial observations into useful long-term summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>dialogue / social simulation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Reflection trees allow communicative agents to abstract and compress repeated low-level observations into higher-level thoughts, helping consistency and higher-level decision-making in long-running interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Choosing what to reflect on and how often, and ensuring abstractions preserve useful detail, are open challenges; efficiency and scalability for long lifetimes remain concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents", 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3022.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3022.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Agentsims</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agentsims</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source sandbox for LLM evaluation that demonstrates using a vector database to store and retrieve agent long-term memories as embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Agentsims: An open-source sandbox for large language model evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Agentsims</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Sandbox environment and agent implementation that stores daily memories as embeddings in a vector database and retrieves relevant memories to support agent decision consistency over time.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>vector retrieval / embedding database</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Daily memories are embedded and stored in a vector DB; at decision time, the agent retrieves nearest-neighbor embeddings relevant to the current situation to inform responses and maintain behavioral consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>agent evaluation in sandbox environments</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluation tasks in which agents should recall past events or behaviors to act consistently over long periods; main challenge is efficient retrieval of semantically relevant memories from large logs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>evaluation / memory retrieval / long-horizon interaction</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Vector retrieval enables efficient storage and lookup of large volumes of episodic memories as embeddings, supporting consistent agent behavior and scalable long-term memory access.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Quality depends on embedding relevance and retrieval strategy; distilling salient memories and managing vector DB scale are practical challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents", 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3022.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3022.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatDB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatDB: Augmenting LLMs with Databases as Symbolic Memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approach that augments LLMs with a structured database as symbolic memory, enabling structured storage and querying of agent facts and records.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chatdb: Augmenting llms with databases as their symbolic memory.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ChatDB</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Integrates a database backend to serve as symbolic long-term memory for an LLM, allowing deterministic storage and retrieval of facts and structured records to support agent behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external structured database / symbolic memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Agent writes and reads structured entries to/from a database which is used as a canonical memory store; retrievals are deterministic queries rather than embedding nearest-neighbor lookups, supporting precise factual recall.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>structured knowledge retention and retrieval tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks requiring precise recall of facts, records, or structured information across interactions (e.g., user profiles, inventories), where the main challenge is faithful, up-to-date retrieval and integration with reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>knowledge retrieval / dialogue / personalization</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Symbolic database-backed memory supports reliable, up-to-date, and auditable factual recall compared to only using unstructured context passages; useful for tasks needing exact record-keeping.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires schema design and explicit writes; bridging symbolic DB results into fluent LLM reasoning and handling mismatches between retrieved structured data and natural-language contexts are challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents", 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Agentsims: An open-source sandbox for large language model evaluation. <em>(Rating: 2)</em></li>
                <li>Chatdb: Augmenting llms with databases as their symbolic memory. <em>(Rating: 2)</em></li>
                <li>Generative agents: Interactive simulacra of human behavior <em>(Rating: 2)</em></li>
                <li>Walking down the memory maze: Beyond context limit through interactive reading <em>(Rating: 2)</em></li>
                <li>Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3022",
    "paper_id": "paper-265295180",
    "extraction_schema_id": "extraction-schema-74",
    "extracted_data": [
        {
            "name_short": "Agent-Memory-Concepts",
            "name_full": "Short-term and Long-term Memory for Language Agents",
            "brief_description": "Conceptual distinction and roles of short-term (working) memory and long-term memory in language agents; short-term memory stores recent temporal context (e.g., chat/action history) while long-term memory stores static/procedural/semantic/episodic knowledge often accessed via retrieval.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "language agents (general)",
            "agent_description": "General class of LLM-based agents that interact with environments via perception, planning/reasoning, tool use and that incorporate explicit memory subsystems to support multi-turn and cross-episode behavior.",
            "memory_used": true,
            "memory_type": "short-term memory and long-term memory (conceptual)",
            "memory_mechanism_description": "Short-term memory is maintained as recent temporal context (e.g., chat/action history, recent rationales) kept in the prompt/context; long-term memory is stored externally or parametrically and accessed via retrieval methods (tree search, text search, vector retrieval), and can include procedural, semantic, and episodic memories.",
            "task_name": "multi-step interactive agent tasks (general)",
            "task_description": "General multi-turn tasks requiring planning, tool use, and consistency across episodes (e.g., web operation, multi-hop QA, sequential control), where memory supports retention of past actions, plans, and world facts.",
            "task_type": "planning / multi-turn interaction / tool use",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "key_findings": "Survey defines and motivates short-term vs long-term memory for agents: short-term memory supplies recent explicit context and impacts current state decisions; long-term memory supports cross-episode consistency and stores more abstract/procedural knowledge accessible via retrieval or parametric storage.",
            "limitations_or_challenges": "Long natural-language memory sequences are inefficient as they grow and exceed context windows; selecting salient information and distilling logs for efficient retrieval is an open challenge; trade-offs between storage format (linear text) and retrieval efficiency exist.",
            "uuid": "e3022.0",
            "source_info": {
                "paper_title": "Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "MemWalker",
            "name_full": "MemWalker",
            "brief_description": "A tree-structured memory approach where lengthy context is summarized into a tree of nodes; agents traverse the tree to gather relevant information for a query.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "MemWalker",
            "agent_description": "Agent memory mechanism that converts long context into a hierarchical tree of summary nodes; the agent navigates this tree via iterative prompting to locate and aggregate relevant memories before responding.",
            "memory_used": true,
            "memory_type": "tree-structured memory / hierarchical summaries",
            "memory_mechanism_description": "The long episodic context is preprocessed into a tree of summaries (leaf nodes: base observations; internal nodes: abstracted summaries). On query, the agent searches/navigates the tree to collect relevant nodes and composes a response after gathering sufficient information.",
            "task_name": "long-context retrieval / multi-step agent queries (illustrative)",
            "task_description": "Tasks that require retrieval from long interaction logs or lengthy contexts; main challenge is locating relevant pieces of information within lengthy histories while staying within context length limits.",
            "task_type": "memory retrieval / multi-turn interaction",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "key_findings": "Tree-structured memory (MemWalker) enables agents to access long context efficiently by organizing summaries hierarchically and searching over them, reducing token consumption compared to linear histories.",
            "limitations_or_challenges": "Requires building and maintaining the tree summaries; search/navigation strategy and selecting when to descend or stop are design challenges; potential loss of detail in summaries and costs to construct/maintain the tree.",
            "uuid": "e3022.1",
            "source_info": {
                "paper_title": "Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "GITM",
            "name_full": "GITM (LLM Decomposer)",
            "brief_description": "An LLM-based decomposer that recursively decomposes goals into a sub-goal tree and uses that hierarchy for memory and planning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "GITM (LLM Decomposer)",
            "agent_description": "Uses recursive decomposition of a top-level goal into a hierarchical sub-goal tree, storing the structure as part of the agent's memory/planning state to capture relationships between goals and plans.",
            "memory_used": true,
            "memory_type": "tree-structured sub-goal memory",
            "memory_mechanism_description": "The agent constructs a hierarchical decomposition of goals (sub-goal tree) that encodes relations between goals and actions; the tree serves as an explicit memory/planning artifact to guide retrieval and decision-making.",
            "task_name": "hierarchical planning / goal decomposition tasks",
            "task_description": "Tasks requiring multi-step planning and capturing goal-plan relationships (e.g., multi-step problem solving and complex task execution) where the challenge is to maintain coherent plans across steps.",
            "task_type": "planning / hierarchical reasoning",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "key_findings": "Decomposing goals into a sub-goal tree helps explicitly capture relationships between goals and plans, supporting memory-efficient planning and clearer mapping from memories to actions.",
            "limitations_or_challenges": "Construction and maintenance of the decomposition can be computationally costly; correctness of decomposition and alignment with environment changes are open issues.",
            "uuid": "e3022.2",
            "source_info": {
                "paper_title": "Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Reflection-Tree",
            "name_full": "Reflection Tree (Park et al. / Communicative Agent Memory)",
            "brief_description": "A memory organization where trivial observations are periodically abstracted into higher-level thoughts, forming a tree whose leaves are base observations and internal nodes are increasingly abstract reflections.",
            "citation_title": "Generative agents: Interactive simulacra of human behavior",
            "mention_or_use": "mention",
            "agent_name": "Reflection Tree (communicative agent)",
            "agent_description": "Memory mechanism for communicative agents that periodically reflects on collected observations to form abstracted memories organized in a tree, enabling higher-level reasoning and socially consistent behavior.",
            "memory_used": true,
            "memory_type": "hierarchical reflective memory / tree of reflections",
            "memory_mechanism_description": "The agent accumulates base observations as leaf nodes; periodic reflection composes abstract higher-level thought nodes above them; this hierarchical memory is used to inform future behavior and maintain coherent persona across interactions.",
            "task_name": "multi-turn social/communicative simulations",
            "task_description": "Tasks simulating human-like agent interactions where agents must maintain consistent persona and reference past observations; main challenge is aggregating many trivial observations into useful long-term summaries.",
            "task_type": "dialogue / social simulation",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "key_findings": "Reflection trees allow communicative agents to abstract and compress repeated low-level observations into higher-level thoughts, helping consistency and higher-level decision-making in long-running interactions.",
            "limitations_or_challenges": "Choosing what to reflect on and how often, and ensuring abstractions preserve useful detail, are open challenges; efficiency and scalability for long lifetimes remain concerns.",
            "uuid": "e3022.3",
            "source_info": {
                "paper_title": "Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Agentsims",
            "name_full": "Agentsims",
            "brief_description": "An open-source sandbox for LLM evaluation that demonstrates using a vector database to store and retrieve agent long-term memories as embeddings.",
            "citation_title": "Agentsims: An open-source sandbox for large language model evaluation.",
            "mention_or_use": "mention",
            "agent_name": "Agentsims",
            "agent_description": "Sandbox environment and agent implementation that stores daily memories as embeddings in a vector database and retrieves relevant memories to support agent decision consistency over time.",
            "memory_used": true,
            "memory_type": "vector retrieval / embedding database",
            "memory_mechanism_description": "Daily memories are embedded and stored in a vector DB; at decision time, the agent retrieves nearest-neighbor embeddings relevant to the current situation to inform responses and maintain behavioral consistency.",
            "task_name": "agent evaluation in sandbox environments",
            "task_description": "Evaluation tasks in which agents should recall past events or behaviors to act consistently over long periods; main challenge is efficient retrieval of semantically relevant memories from large logs.",
            "task_type": "evaluation / memory retrieval / long-horizon interaction",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "key_findings": "Vector retrieval enables efficient storage and lookup of large volumes of episodic memories as embeddings, supporting consistent agent behavior and scalable long-term memory access.",
            "limitations_or_challenges": "Quality depends on embedding relevance and retrieval strategy; distilling salient memories and managing vector DB scale are practical challenges.",
            "uuid": "e3022.4",
            "source_info": {
                "paper_title": "Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "ChatDB",
            "name_full": "ChatDB: Augmenting LLMs with Databases as Symbolic Memory",
            "brief_description": "Approach that augments LLMs with a structured database as symbolic memory, enabling structured storage and querying of agent facts and records.",
            "citation_title": "Chatdb: Augmenting llms with databases as their symbolic memory.",
            "mention_or_use": "mention",
            "agent_name": "ChatDB",
            "agent_description": "Integrates a database backend to serve as symbolic long-term memory for an LLM, allowing deterministic storage and retrieval of facts and structured records to support agent behavior.",
            "memory_used": true,
            "memory_type": "external structured database / symbolic memory",
            "memory_mechanism_description": "Agent writes and reads structured entries to/from a database which is used as a canonical memory store; retrievals are deterministic queries rather than embedding nearest-neighbor lookups, supporting precise factual recall.",
            "task_name": "structured knowledge retention and retrieval tasks",
            "task_description": "Tasks requiring precise recall of facts, records, or structured information across interactions (e.g., user profiles, inventories), where the main challenge is faithful, up-to-date retrieval and integration with reasoning.",
            "task_type": "knowledge retrieval / dialogue / personalization",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "key_findings": "Symbolic database-backed memory supports reliable, up-to-date, and auditable factual recall compared to only using unstructured context passages; useful for tasks needing exact record-keeping.",
            "limitations_or_challenges": "Requires schema design and explicit writes; bridging symbolic DB results into fluent LLM reasoning and handling mismatches between retrieved structured data and natural-language contexts are challenges.",
            "uuid": "e3022.5",
            "source_info": {
                "paper_title": "Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Agentsims: An open-source sandbox for large language model evaluation.",
            "rating": 2,
            "sanitized_title": "agentsims_an_opensource_sandbox_for_large_language_model_evaluation"
        },
        {
            "paper_title": "Chatdb: Augmenting llms with databases as their symbolic memory.",
            "rating": 2,
            "sanitized_title": "chatdb_augmenting_llms_with_databases_as_their_symbolic_memory"
        },
        {
            "paper_title": "Generative agents: Interactive simulacra of human behavior",
            "rating": 2,
            "sanitized_title": "generative_agents_interactive_simulacra_of_human_behavior"
        },
        {
            "paper_title": "Walking down the memory maze: Beyond context limit through interactive reading",
            "rating": 2,
            "sanitized_title": "walking_down_the_memory_maze_beyond_context_limit_through_interactive_reading"
        },
        {
            "paper_title": "Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory",
            "rating": 1,
            "sanitized_title": "ghost_in_the_minecraft_generally_capable_agents_for_openworld_enviroments_via_large_language_models_with_textbased_knowledge_and_memory"
        }
    ],
    "cost": 0.017738499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents
20 Nov 2023</p>
<p>Zhuosheng Zhang zhangzs@sjtu.edu.cn 
Shanghai Jiao Tong University
 Amazon Web Services</p>
<p> Yale University</p>
<p>Yao Yao yaoyao27@sjtu.edu.cn 
Shanghai Jiao Tong University
 Amazon Web Services</p>
<p> Yale University</p>
<p>Aston Zhang 
Zhiwei He 
Shanghai Jiao Tong University
 Amazon Web Services</p>
<p> Yale University</p>
<p>Yiming Wang 
Shanghai Jiao Tong University
 Amazon Web Services</p>
<p> Yale University</p>
<p>Mark Gerstein mark.gerstein@yale.edu 
Shanghai Jiao Tong University
 Amazon Web Services</p>
<p> Yale University</p>
<p>Rui Wang wangrui12@sjtu.edu.cn 
Gongshen Liu 
Shanghai Jiao Tong University
 Amazon Web Services</p>
<p> Yale University</p>
<p>Hai Zhao zhaohai@cs.sjtu.edu.cn 
Shanghai Jiao Tong University
 Amazon Web Services</p>
<p> Yale University</p>
<p>Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents
20 Nov 202349DFE29291D119C552501C9120792B49arXiv:2311.11797v1[cs.CL]
Instruction: What time is it in Berlin ?Thought: What I see is a searching page with a search bar.I need to click the search bar to type the question.Action: {"action": "click", "item": "search bar"} * Equal contribution.We thank Diyi Yang for providing valuable feedback on the draft.</p>
<p><img id=0 class="IconGoogle" alt="Google Icon"> </img> <img id=1 class="IconX" alt="Close Icon"> </img> <p id=2 class="text" alt="Search">Search</p> <img id=3 class="Search Icon" alt="Search Icon"> </img> <img id=4 class="Voice Icon" alt="Voice Icon"> </img> <p id=5 class="text" alt="68F in Mountain View">68F in Mountain View </p> ... <p id=19 class="text"> 3 Braves free agents who won't be back next season and why </p> Previous Actions: {"step_idx": 0, "action_description": "click [HOME Icon]"} {"step_idx": 1, "action_description": "click [Google Icon]"} Question: What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into ?</p>
<p>Abstract</p>
<p>Large language models (LLMs) have dramatically enhanced the field of language intelligence, as demonstrably evidenced by their formidable empirical performance across a spectrum of complex reasoning tasks.Additionally, theoretical proofs have illuminated their emergent reasoning capabilities, providing a compelling showcase of their advanced cognitive abilities in linguistic contexts.Critical to their remarkable efficacy in handling complex reasoning tasks, LLMs leverage the intriguing chain-of-thought (CoT) reasoning techniques, obliging them to formulate intermediate steps en route to deriving an answer.The CoT reasoning approach has not only exhibited proficiency in amplifying reasoning performance but also in enhancing interpretability, controllability, and flexibility.In light of these merits, recent research endeavors have extended CoT reasoning methodologies to nurture the development of autonomous language agents, which adeptly adhere to language instructions and execute actions within varied environments.This survey paper orchestrates a thorough discourse, penetrating vital research dimensions, encompassing: (i) the foundational mechanics of CoT techniques, with a focus on elucidating the circumstances and justification behind its efficacy; (ii) the paradigm shift in CoT; and (iii) the burgeoning of language agents fortified by CoT approaches.Prospective research avenues envelop explorations into generalization, efficiency, customization, scaling, and safety.We hope to offer readers a comprehensive understanding of prevalent research areas such</p>
<p>Introduction</p>
<p>Language intelligence pertains to the aptitude for comprehending and reasoning through concepts articulated in natural languages (Sternberg et al., 1982;Ryan &amp; Lopez, 2001;Ramsden et al., 2011;Luwel et al., 2013).Spurred by advancements in scale, large language models (LLMs) have achieved remarkable progress in pursuing human-level language intelligence, compellingly evidenced by the strong empirical benchmarking performance in complex reasoning tasks (Wei et al., 2022), as well as theoretical proofs for the emergent reasoning abilities (Prystawski &amp; Goodman, 2023;Wang &amp; Wang, 2023;Bi et al., 2023).</p>
<p>Reasoning, a pivotal research topic within the realm of language intelligence, is characterized as a multi-step process wherein inferences are drawn from discrete pieces of evidence, culminating in the formation of more abstract concepts that are instrumental in facilitating high-level predictions (Morency et al., 2022;Yu et al., 2023a;Huang &amp; Chang, 2023).Recent research revealed that remarkable enhancements in performance could be attained by prompting LLMs to engage in a step-by-step reasoning process, as opposed to generating answers in a direct manner (Nye et al., 2022;Wei et al., 2023b).The way to prompt LLMs to generate a series of intermediate reasoning steps for solving a problem is called chain-of-thought (CoT) prompting (Wei et al., 2023b).</p>
<p>Optimization of CoT prompting techniques has garnered escalating interest, catalyzing notable paradigm shifts within the CoT framework.These shifts encompass three key aspects: (i) prompting pattern: from manual design of in-context learning demonstrations to automatic prompt construction (Zhang et al., 2023c;Zhou et al., 2023f;Yang et al., 2023a);</p>
<p>(ii) reasoning format: from unstructured natural language formats to structured ones (Chen et al., 2022;Ziqi &amp; Lu, 2023;Yao et al., 2023a;c); (iii) application scenario: from singular language settings to multilingual environments (Shi et al., 2023), from a language modality to embracing multimodal approaches (Zhang et al., 2023d), and from complex reasoning tasks to general-purpose tasks (Wang et al., 2022;Li et al., 2022;Wang et al., 2023g;He et al., 2023).</p>
<p>CoT reasoning is a representative emergent ability of LLMs (Wei et al., 2022).It provides a proficient strategy for deconstructing intricate issues into smaller, manageable sub-problems, systematically enabling solutions through a step-by-step approach (Figure 2).Leveraging the reasoning capabilities developed during pre-training (Xie et al., 2022;Wang et al., 2023a), CoT prompting adeptly identifies atomic knowledge components essential for reasoning processes and seamlessly integrates their relationships, thereby constructing intermediate, coherent reasoning steps (Prystawski &amp; Goodman, 2023;Wang &amp; Wang, 2023).In addressing these sub-problems, the reasoning process can be further enhanced by employing knowledge retrieval and verification tools (Gou et al., 2023a;Qin et al., 2023b).By expanding CoT into a comprehensive framework for perception, memory, and reasoning, language agents, powered by LLMs, have been formulated to adeptly adhere to language instructions and execute actions in either real-world or simulated environments (Rawles et al., 2023;Zhang &amp; Zhang, 2023) (Figure 1).These language agents come in two flavors: (i) autonomous agents (Adept, 2022;Richards, 2023;Hong et al., 2023;Nakajima, 2023) and (ii) communicative agents (Park et al., 2023;Wang et al., 2023c;Zhu et al., 2023;Hong et al., 2023).</p>
<p>In this paper, we navigate through research topics that encompass: (i) unraveling the underlying mechanisms of CoT techniques, with a particular focus on discerning when and why CoT is effective; (ii) identifying and analyzing the paradigm shift occurring within CoT; and (iii) investigating the advent of language agents enabled by CoT techniques.The rest of this paper is structured for a coherent and sequential exploration.Initially, we immerse ourselves in the fundamental aspects of CoT reasoning, which encompasses its defining features and the merits arising from employing CoT techniques.Subsequently, we delve deeper into the inherent mechanisms of CoT, striving to elucidate the specific conditions and reasons that determine its functionality.In the ensuing section, we classify paradigm shifts, directing our attention towards various prompting techniques, reasoning formats, and application scenarios.Following that, we explore the emerging landscape of language agents, spotlighting those facilitated by CoT techniques.To conclude, we engage in a discussion about the challenges encountered and future opportunities looming on the horizon.</p>
<p> CoT demonstrates efficacy under two overarching conditions: first, when an LLM with preferably at least 20 billion parameters is employed, and second, when the parametric knowledge within the LLM encompasses knowledge pieces that are (i) pertinent to the task at hand and (ii) maintain strong mutual interconnections (Section 3.1). CoT functions by assisting in the identification of atomic knowledge pieces pivotal for reasoning and seamlessly connecting these components via the formation of intermediate reasoning steps (Section 3.2). CoT techniques have experienced substantial paradigm shifts, embracing alterations in prompting patterns, reasoning formats, and application scenarios (Section 4). CoT has acted as a catalyst in the evolution of LLM-empowered agents capable of understanding language instructions and executing actions in both real-world and simulated environments, specifically augmenting agent capabilities in perception, memory, and reasoning (Section 5). Despite the swift advancement of LLMs, CoT reasoning, and language agents, numerous challenges persist, such as generalization to unseen domains, efficiency amidst redundant interactions, customization of language agents, scaling up of language agents, and ensuring the safety of language agents (Section 6).</p>
<p>Preliminaries of CoT</p>
<p>In this section, we immerse ourselves in the fundamental elements of CoT reasoning.Firstly, we carve out a distinct contrast between CoT reasoning and the traditional approach of direct reasoning.Subsequently, we proffer definitions for the key components within CoT.Finally, we delineate the advantages of adopting CoT.</p>
<p>Definition</p>
<p>The concept of chain-of-thought refers to a series of intermediate reasoning steps that are generated to solve a problem or arrive at an answer (Wei et al., 2023b), in the form of <inputreasoning chain (rationale)output> mappings.This approach is often more effective than traditional direct reasoning, which attempts to tackle the entire problem all at once.For example, standard classification, multiple choice, and question answering problems often leverage direct reasoning in the form of <inputoutput> mappings.</p>
<p>To elucidate CoT, we establish standard definitions for its key components as illustrated in Figure 2. Formally, assuming that the reasoning dataset distribution is D, we denote s = (x, y)  D as a sampling on D, where x and y denote the question (input) and the answer (output), respectively, and they are both in the form of text sequences.We use |x| to denote the length of the sequence x, and p  to denote the pre-trained language model parameterized with .Details of definitions are as follows:</p>
<p>Instruction.Instructions are usually short sentences used to prompt an LLM to generate answers in the desired format.They guide the LLM to think step by step in the reasoning process.We notate the instruction as p, which is set to different text sequences depending on the task requirements.</p>
<p>Rationale.We uniformly refer to the intermediate processes of CoT reasoning as "rationales".Rationales can encompass solutions, intermediate reasoning steps, or any relevant external knowledge pertaining to a question.We</p>
<p>The is closed.</p>
<p>Figure 2: Comparison between CoT reasoning and direct reasoning.CoT refers to a series of intermediate reasoning steps that are generated to solve a problem or arrive at an answer (Wei et al., 2023b).This approach is often more effective than direct reasoning, which attempts to tackle the entire problem all at once.define rationale as r.If r is generated by the LLM, instruction p can be used to obtain r  p  (x, p).If r is written by a human, instruction p can be exempted, and r = f (x), where f () indicates the handwriting operation.</p>
<p>Exemplars.Exemplars are typically presented as desired input-output pairs in few-shot prompting approaches, each of which contains questions, a rationale, and an answer.Exemplars serve as in-context demonstrations of input-output relationships before generating predictions for test-time examples.Exemplars are usually concatenated before input questions.Specifically, assuming the exemplar size of n, exemplars E can be formulated as:
E = [(x 1 , r 1 , y 1 )    (x n1 , r n1 , y n1 )  (x n , r n , y n )],(1)
where  represents concatenation, (x i , y i )  D and r i = f (x i ).</p>
<p>Zero-Shot-CoT.Zero-Shot-CoT does not require users to provide exemplars.Instead, it typically relies on instructions to facilitate the LLM in conducting step-by-step reasoning, thereby generating answers.For example, Kojima et al. (2023) first elicited the LLM to generate the rationale r using the instruction p 1 such as "Let's think step by step", and then use the instruction p 2 such as "The answer is" to obtain the final answer following the question and rationale.Formally, the output y can be computed as follows:
r  |r|  i=1 p  (r i |x, p 1 , r &lt;i ), y  |y|  i=1 p  (y i |x, p 1 , r, p 2 , y &lt;i ).(2)
Few-Shot-CoT.Few-Shot-CoT involves providing a set of exemplars with associated rationales.These exemplars are concatenated with the question to prompt the LLM to generate the rationale and answer.In this setting, the output y is obtained in an end-to-end mode, which can be formulated as:
y  |y|  i=1 p  (y i |E, x, y &lt;i ).
(3)</p>
<p>Benefits of CoT</p>
<p>CoT techniques have shown various kinds of benefits, including improved reasoning performance, interpretability, controllability, and flexibility.We summarize them in detail below.</p>
<p>Improved Reasoning Performance CoT facilitates a step-by-step progression in the reasoning process for LLMs.By breaking down complex, multi-step problems into intermediate stages, CoT minimizes the risk of overlooking crucial details.Moreover, it ensures the efficient allocation of additional computational resources to problems demanding a higher degree of reasoning steps.Numerous studies have conclusively demonstrated the efficacy of CoT across a wide range of domains, encompassing arithmetic reasoning, commonsense reasoning, and symbolic reasoning (Wei et al., 2023b;Kojima et al., 2023;Wang et al., 2023f).</p>
<p>Improved Interpretability CoT offers an interpretable glimpse into the decision-making process of LLMs.Breaking down complex reasoning tasks into a chain of interconnected thoughts makes it easier to understand the underlying logic and reasoning behind a decision or conclusion made by LLM.It sheds light on how the model may have reached a specific answer, offering valuable insights for debugging and pinpointing where the reasoning process may have deviated from the correct path.However, it is important to note that fully characterizing the model's computations supporting an answer still presents an open challenge (Wei et al., 2023b).</p>
<p>Improved Controllability By prompting LLMs to output a chain of interconnected thoughts, users can exert greater influence over the cognitive processes of LLM.Many studies (Yao et al., 2023a;Ling et al., 2023) were dedicated to the identification and rectification of specific thought units where the reasoning path may have gone off track or where additional information is required.This increased controllability allows for more deliberate and accurate answers.</p>
<p>Improved Flexibility</p>
<p>The utilization of CoT reasoning can be easily prompted in adequately large, off-the-shelf LLMs by simply adding instruction at the end of the input question for Zero-Shot-CoT or incorporating CoT exemplars used for Few-Shot-CoT (Wei et al., 2023b).The flexibility of CoT extends beyond the realm of reasoning tasks, making it applicable to a wide range of fields, including classic natural language processing (NLP), scientific applications, and agent-based systems.</p>
<p>Underlying Mechanism of CoT</p>
<p>This section explores the foundational mechanisms of CoT, encompassing the general conditions that determine when and why CoT is effective.</p>
<p>When CoT Works</p>
<p>Although CoT has shown promising benefits, it may not be suitable in any conditions (Kojima et al., 2023;Wei et al., 2023b;Zhang et al., 2023d).We will introduce when CoT works in engineering and theoretical perspectives.Then we summarize the general conditions to suggest the effective application scopes of CoT reasoning.</p>
<p> From an engineering perspective, Wei et al. (2023b) thought that CoT reasoning is helpful under three conditions: (i) an LLM is used; (ii) the task is challenging and requires multi-step reasoning; (iii) the performance of direct prompting does not increase dramatically while scaling the model size.Notably, Tay et al. (2022) further provided evidence that LLMs with 20 billion parameters, pre-trained on a mixture of denoising functions, can also achieve effective CoT reasoning.</p>
<p>1 Otherwise, CoT techniques tend to struggle with smaller LLMs (Wei et al., 2022).It may lead to hallucination because of lacking supportive knowledge in LLMs (Zhang et al., 2023d) and inferior reasoning capabilities (Magister et al., 2022).CoT reasoning is also less effective in simple-step tasks such as matching, sequence labeling (Qin et al., 2023a), and single-choice question (Chen et al., 2023a).</p>
<p> From a theoretical perspective, Prystawski &amp; Goodman (2023) proved that CoT reasoning is helpful when training data (possibly considered as the parametric knowledge in an LLM) consists of local clusters of variables that strongly influence each other.This finding implied that the LLM must have the knowledge related to the task to support CoT reasoning.We call such knowledge as atomic knowledge.</p>
<p>As CoT reasoning is often elicited by in-context learning (ICL), such as Zero-Shot-CoT and Few-Shot-CoT, another line of study tries to understand when CoT works from the perspective of ICL.Zhang et al. (2023c) showed that CoT reasoning works effectively when prompted with diverse exemplars.Wang et al. (2023a) found that rationales being relevant to the query and correctly ordering the reasoning steps are the keys to the effectiveness of CoT prompting.</p>
<p>Besides prompting, introducing reasoning materials and necessary knowledge for LLMs in the training corpus has also exhibited a profound improvement in CoT reasoning ability in LLMs (Yu et al., 2023b).Recent studies found that pre-training with code data (Chung et al., 2022) or fine-tuning (e.g., instruction tuning) with CoT-style data (Yue et al., 2023) is beneficial for effective CoT reasoning.That is, the CoT reasoning in the same LLMs can be improved or the CoT reasoning ability can be induced in smaller models.</p>
<p>Based on the discussion above, CoT demonstrates efficacy under two overarching conditions: first, when an LLM with preferably at least 20 billion parameters is employed, and second, when the parametric knowledge within the LLM encompasses knowledge pieces that are (i) pertinent to the task at hand and (ii) maintain strong mutual interconnections.</p>
<p>Why CoT Works</p>
<p>Recent studies have employed both empirical and theoretical approaches in an effort to comprehend the underlying reasons for the effectiveness of CoT.</p>
<p> Empirically, Wei et al. (2023b) believed that the success of CoT reasoning constitutes a multifaceted phenomenon that likely involves various emergent abilities.Those abilities include semantic understanding, symbol mapping, topic coherence, arithmetic ability, and faithfulness.Interestingly, Zhang et al. (2023c) found that mistakes in exemplar rationales do not lead to significant performance drops.Wang et al. (2023a) reported a similar observation that LLMs can generate coherent reasoning steps and achieve over 80-90% of the performance, though prompted with invalid reasoning steps in the exemplars.Those findings imply that LLMs already have an innate ability to reason after pre-training (Zhang et al., 2023c;Wang &amp; Wang, 2023).CoT prompting specifies an output format that regularizes the model generation to generate step-by-step while being in order and relevant to the query (Wang et al., 2023a).In other words, CoT techniques help compel the model to conduct reasoning rather than teaching it how to accomplish reasoning (Zhang et al., 2023c).</p>
<p> Theoretically, Bayesian inference is a popular way to investigate why CoT works from a theoretical perspective (Prystawski &amp; Goodman, 2023;Wang &amp; Wang, 2023).Prystawski &amp; Goodman (2023) proved that CoT is effective when the training data exhibits a localized structure with respect to dependencies between variables.In the context of LLMs, the proof can be interpreted that the parametric knowledge within the LLM comprises knowledge pieces that are related to the target problem, and those knowledge pieces exert strong mutual connections with each other.To verify the proof, Bi et al. (2023) conducted an empirical study on code data and found that the local structural properties of the data are crucial for improving CoT reasoning abilities.These findings in Prystawski &amp; Goodman (2023) and Bi et al. (2023) compellingly indicated that CoT may help identify the atomic pieces of knowledge used for reasoning and bridge the relationship between the atomic pieces of knowledge with intermediate reasoning steps.Similarly, Wang &amp; Wang (2023) used knowledge graphs for analysis and found that organizing the known facts as "chains", i.e., CoT, can significantly impact the effectiveness of reasoning.By doing so, LLMs are able to accurately deduce previously unseen facts from known ones to answer a given query without explicitly encoding reasoning rules.</p>
<p>Paradigm Shifts of CoT</p>
<p>After elucidating the general conditions determining when and why CoT is effective, we seek to achieve a more profound and intuitive understanding of the improvements in CoT's reasoning capabilities for LLMs.To this end, we compile and summarize the best performances of CoT across seven of the most emblematic reasoning tasks as of October 2023.We compare these performances with those achieved without CoT and present our findings in Figure 3.These seven reasoning tasks span across distinct categories, including: (i) Arithmetic Reasoning: GSM8K (Cobbe et al., 2021), AQuA (Ling et al., 2017), and SVAMP (Patel et al., 2021) (Wang et al., 2023f).).While the first two uses the text-davinci-002 as the LLM engine, the latter allows for the model employed to vary for each task (details in Table 1).For a fair comparison, the performances of "Direct Prompt", "Manual-CoT", "Best CoT w/o SC" and "Best CoT w/ SC" are all based on using the text-davinci-002 as the LLM engine.</p>
<p>2019) and Strategy QA (Geva et al., 2021); (iii) Symbolic Reasoning: Last Letter Concatenation (Wei et al., 2023b), and Coin Flip (Wei et al., 2023b).</p>
<p>Figure 3 clearly illustrates that the benchmark performance in complex reasoning tasks has advanced rapidly, with CoT exerting a significant influence on the reasoning abilities of LLMs across all seven tasks.Notably, apart from commonsense reasoning, the relatively straightforward CoT format Manual-CoT proposed by Wei et al. (2023b) substantially improves overall accuracy compared to the direct prompt in both arithmetic and symbolic reasoning.</p>
<p>The deficiency in CoT's performance regarding commonsense reasoning tasks has been observed both in Manual-CoT (Wei et al., 2023b) and Zero-Shot-CoT Kojima et al. (2023).However, when CoT is integrated with a significantly larger PaLM (540B) model, it consistently enhances commonsense reasoning.Notably, Zero-Shot-CoT also notes that the rationales generated through CoT often exhibit logical correctness or only contain human-understandable errors.This suggests that CoT encourages improved commonsense reasoning, even when the task metrics do not explicitly measure it.While Manual-CoT fails to yield performance gains in commonsense reasoning, the optimization of reasoning techniques, such as self-consistency answer aggregation (Wang et al., 2023f) and automatic exemplar construction (Shum et al., 2023), reveals the potential of CoT to achieve remarkable results generally.</p>
<p>Moreover, for ease of reference and to provide a clear overview of how CoT can achieve top performance on the seven different datasets, we have included the latest models that have achieved the best performance and the specific LLM engine they utilized in Table 1.</p>
<p>In conclusion, we see that compared with the vanilla prompting approach in Wei et al. (2023b), the latest CoT reasoning techniques have been strengthened throughout the full stack of the reasoning process, such as multimodal perception (Zhang et al., 2023d;Yao et al., 2023c;Huang et al., 2023b;Rose et al., 2023), automatic prompting (Zhang et al., 2023c;Diao et al., 2023), reasoning verification (Weng et al., 2022;Lightman et al., 2023;Ling et al., 2023), and consistency-based sampling (Wang et al., 2023f;2022).With the growing interest in CoT, researchers are continually striving to harness its full potential for enhancing LLMs reasoning capabilities.In this section, we will embark on a journey through the realm of CoT research, following the map of CoT overview as illustrated in Figure 4, delving into the comprehensive discussions of advancements made in three key directions: (i) prompting pattern; (ii) reasoning format; and (iii) application scenario.</p>
<p>Prompting Pattern</p>
<p>The prompting pattern can be primarily divided into two components: instruction generation and exemplar generation.</p>
<p>Instruction generation primarily focuses on finding the optimal instructions to prompt LLM, enabling them to engage in step-by-step reasoning instead of directly answering the question.This approach mainly aims to maximize LLM's zero-shot capability.Exemplar generation primarily focuses on finding the best set of input-output demonstration exemplar pairs for Few-Shot-CoT.These exemplars are used to prompt LLMs along with a test input, enabling the model to predict the corresponding output.</p>
<p>Instruction Generation</p>
<p>Instruction generation can be categorized into two distinct methods: manual instruction generation and automatic instruction generation, based on their respective generation processes.</p>
<p>Early efforts primarily involved manual construction of instruction prompts.The earliest and most traditional instruction generation method was Zero-Shot-CoT proposed by Kojima et al. (2023).Zero-Shot-CoT demonstrates that large language models (LLMs) can perform zero-shot reasoning by adding a simple prompt, "Let's think step by step", before each answer.Zero-Shot-CoT outperforms zero-shot LLM performances on various reasoning tasks without the need for hand-crafted few-shot examples, marking the inception of a new era in Zero-Shot-CoT.Wang et al. (2023d) further proposed the Plan-and-Solve (PS) Prompting to address the missing-step errors in Zero-Shot-CoT reasoning.It consists of devising a plan to divide the task into smaller subtasks and carrying out the subtasks according to the plan.PS prompting consists of two stages.In the first stage, the author prompts the LLM using the proposed prompting template "Let's first understand the problem and devise a plan to solve the problem.Then, let's carry out the plan and solve the problem step by step" to generate the reasoning process and the answer.The second stage extracts the answer using an answer prompt (e.g., "Therefore, the answer (arabic numerals) is").</p>
<p>However, manually designing instructions may not always yield the desired results, and users often need to experiment with various prompts to achieve the desired behavior.In response to this challenge, Zhou et al. (2023f) proposed the Automatic Prompt Engineer (APE), a method designed for the automated generation and selection of instructions for LLMs.APE treats instruction generation as a form of natural language program synthesis and optimizes this process by searching through a pool of instruction candidates proposed by an LLM.The primary goal is to maximize a chosen score function.To elaborate further, APE initiates the process by instructing the LLM to generate a set of candidate instructions using manually crafted templates.Subsequently, it utilizes the LLM to infer the most likely instructions with the highest score, based on input-output exemplars.By harnessing the capabilities of LLMs, APE streamlines the prompt engineering process, alleviating extensive human intervention and generating high-quality instructions.Yang et al. (2023a) presented Optimization by PROmpting (OPRO), a straightforward yet highly effective approach that harnesses the power of Language Model (LLM) as optimizers.OPRO represents a groundbreaking method in optimization, utilizing LLMs to their full potential.OPRO initiates the optimization process by presenting a natural</p>
<p>Reasoning Aggregation</p>
<p>CoT Verification</p>
<p>CoT for Classic NLP Task</p>
<p>CoT for Agent</p>
<p>CoT for Science</p>
<p>Let's first understand the problem and devise a plan to solve the problem.Then, let's carry out the plan and solve the problem step by step.</p>
<p>Cot Extension</p>
<p>Cot Formulation language description of both the optimization problem and the optimization trajectory.This trajectory includes prior solutions along with their associated optimization scores.Subsequently, updated solutions are devised and evaluated for their performance and quality.The prompt for the subsequent optimization step incorporates these solutions after thorough examination.As the iterative process unfolds, the solutions undergo progressive refinement, ultimately improving their quality.</p>
<p>Initially, OPRO is applied to address two classic optimization challenges: the linear regression problem and the traveling salesman problem.The study then proceeds to demonstrate that prompts optimized by OPRO surpass human-designed prompts in performance, particularly on tasks such as GSM8K and Big-Bench Hard tasks.OPRO showcases its efficiency in resolving common optimization challenges and enhancing prompts by presenting optimization tasks in natural language for LLMs, consistently generating and refining solutions.</p>
<p>Exemplar Generation</p>
<p>Similar to instruction generation, exemplar generation can also be classified into two categories based on the method of constructing exemplars: manual exemplar generation and automatic exemplar generation.</p>
<p>Few-Shot-CoT reasoning, formally explored by Wei et al. (2023b), represents a discrete prompt learning approach that uses multiple input-output pairs to prompt the LLM to output rationales and obtain the final answer.To provide a clearer distinction, we will refer to their work as Manual-CoT.Manual-CoT follows the traditional manual exemplar generation method.In contrast to the conventional in-context learning, where LLMs are prompted with a list of input-output demonstration pairs alongside a test input to enable the model to predict the output, Manual-CoT involves prompting the model's outputs with manually designed additional logical reasoning procedures in addition to the target output.Diao et al. (2023) took Manual-CoT a step further by optimizing the selection of exemplars and introduced Active-Prompt, which uses task-specific example prompts annotated with human-designed rationales.Active-Prompt exists in a state that falls between manual exemplar generation and automatic exemplar generation.The method selects the most uncertain questions from a pool of task-specific queries using uncertainty-based active learning metrics.Active-Prompt first asks LLM to answer questions multiple times following the Manual-CoT (Wei et al., 2023b).The model then selects the most uncertain questions based on the uncertainty metric (e.g.disagreement, entropy, variance, self-confidence), manually annotates the rationales, and uses the questions and rationales as examples for inference.</p>
<p>To eliminate the need for manual efforts in hand-crafting task-specific demonstrations to generate reasoning chains one by one, Zhang et al. (2023c) proposed Auto-CoT which maintains the diversity of sampled questions and generates reasoning chains to automatically construct demonstrations.Specifically, Auto-CoT consists of two main stages: (i) Problem clustering: divide the given dataset of problems into several clusters; (ii) Demonstration sampling: select a representative problem from each cluster and use Zero-Shot-CoT to generate its reasoning chain.Shum et al. (2023) proposed a strategy called Automate-CoT (Automatic Prompt Augmentation and Selection with Chain-of-Thought) that automates the process of augmenting and selecting rational chains for CoT prompting.The process consists of three steps: augmenting the language model to generate multiple pseudo-chains, pruning the pseudo-chains based on consistency with ground-truth answers, and selecting the most helpful chain-of-thought using a variance-reduced policy gradient strategy.</p>
<p>Reasoning Format</p>
<p>The enhancements in reasoning format primarily encompass three aspects: CoT formulation, reasoning aggregation, and CoT verification.CoT formulation focuses on transforming the sequential CoT into various cognitive structures, such as tree-like, graph-like, or table-like formats, thereby incorporating structural thinking cues.Reasoning aggregation primarily concerns the enhancement of LLM CoT reasoning accuracy through the aggregation of results sampled from the LLM.CoT verification primarily emphasizes the introduction of verification methods to verify and amend the CoT reasoning process.We will elaborate on these three aspects in the following sections.</p>
<p>CoT Formulation</p>
<p>We present five representative CoT formulations in Figure 5.We will progressively delve into the CoT formulation shifts based on this illustration.</p>
<p>Chen et al. ( 2022) introduced Program-of-Thoughts (PoT) for solving complex numerical reasoning tasks.PoT uses language models to generate both text and programming language statements, which can be executed on a program interpreter to decouple complex computation from reasoning and language understanding.Ziqi &amp; Lu (2023) 2023) is dubbed as GoT-rationale).</p>
<p>GoT-rationale models the thought generation process of language models as a graph.The system architecture of GoT comprises multiple interacting modules: (i) Prompter that prepares prompt for the LLM, which are then used to generate responses; (ii) Parser that extracts information from the LLM's responses, which is then used by other modules in the architecture; (iii) Scorer that verifies and scores the LLM's replies to determine their quality and relevance to the task at    (Ziqi &amp; Lu, 2023), (iv) Tree-of-Thoughts (ToT) (Yao et al., 2023a), (v) Graph-of-Thoughts-Rationale (GoT-Rationale) (Besta et al., 2023).</p>
<p>hand; (iv) Controller that process with two elements: the Graph of Operations (GoO) and the Graph Reasoning State (GRS).GoO is a static structure that specifies the graph decomposition of a given task, which means it prescribes the transformations to be applied to LLM thoughts, along with their order and dependencies.GRS is a dynamic structure that maintains the state of the ongoing LLM reasoning process, which includes the history of its thoughts and their states.Experimental results indicate that GoT outperforms state-of-the-art techniques in tasks such as sorting, set operations, keyword counting, and document merging.Lee &amp; Kim (2023) proposed Recursion of Thought (RoT), which empowers language models to recursively generate multiple contexts for problem-solving.In RoT, LLMs are prompted to output special tokens such as GO, THINK, and STOP, which serve to initiate context-related operations.The THINK token indicates the model needs to solve a sub-problem, which triggers a recursive process to generate a new context for that sub-problem.This innovative approach enables the models to effectively handle problems whose solutions exceed the maximum context size by creating and managing multiple nested contexts.</p>
<p>Different from above works that focus on introducing structural information into CoT reasoning, Ning et al. (2023) proposed Skeleton-of-Thought to accelerate the CoT reasoning process.SoT consists of two stages: (i) Skeleton stage: SoT guides the LLM to output a concise skeleton of the answer through a manually designed a skeleton prompt template and extracts points from the skeleton response; (ii) Point-expanding stage: SoT prompts the LLM to expand on each point in parallel through a point expanding template and finally concatenates all the points to get the final answer.</p>
<p>Reasoning Aggregation</p>
<p>Wang et al. (2023f) introduced a novel decoding strategy called self-consistency to replace the greedy decoding strategy in CoT.Self-consistency CoT first prompts the language model following the Manual-CoT (Wei et al., 2023b) and then samples a diverse set of reasoning paths from the language model's decoder.Finally, Self-consistency CoT finds the most consistent answer by taking a majority vote which was found to significantly improve the performance of the CoT.Wang et al. (2022) further developed a unified framework for rationale-augmented ensembles which aims at aggregating over multiple rationales generated from the language model to mitigate the brittleness of the results.The author explores three distinct approaches of rationale-augmented ensembles,each differing in how randomness is introduced into the input or output space: (i) self-consistency (Wang et al., 2023f): the ensembling is based on sampling multiple language model outputs; (ii) prompt-order ensembling: the ensembling process is based on the order of the input exemplars; (iii) input-rationale ensembling: the ensembling is based on sampling multiple input exemplars rationales from LLMs.The author found that regardless of the variation in input or prompt, the best way to improve task performance is sampling rationale in the output space.</p>
<p>CoT Verification</p>
<p>CoT verification initially focused on self-verification through multiple rounds of questioning, enabling models to validate their own responses.Later works involve leveraging external tools for information validation, such as information retrieval, calculators, or program execution.This section explores various methods and strategies within CoT verification, contributing to the enhancement of model reliability and response accuracy.</p>
<p>Weng et al. ( 2022) first proposed and proved that LLMs have self-verification abilities by using the conclusion obtained through CoT as a condition for verifying the original problem.Self-verification consists of two steps: (i) forward reasoning that samples multiple candidate reasoning paths; (ii) backward verification that calculates the verification scores for each candidate's answer by masking the original conditions and predicting their results in turn.The answer with the highest score is selected as the final answer.</p>
<p>Lightman et al. ( 2023) focused on training reward models and conducted a comparison between the outcome supervision reward model (ORM) and process supervision reward model (PRM) for LLM to solve problems from the MATH dataset (Hendrycks et al., 2021b), finding that process supervision significantly outperforms outcome supervision.Outcome supervision is provided without humans, as the MATH dataset has automatically checkable answers.Process supervision, on the other hand, requires human data-labelers to label the correctness of each step in model-generated solutions.The authors also released PRM800K, a complete dataset of 800,000 step-level human feedback labels used to train their best reward model.Ling et al. (2023) proposed a verification process using a "Natural Program" format.Natural Program breaks down the reasoning process into individual steps which is accompanied by its corresponding minimal set of premises.Then, the author employed a 2-phase sequence generation, strategy Unanimity-Plurality Voting, to verify the deductive reasoning process.Unanimity-Plurality Voting first performs deductive validations on sampled reasoning chains and then conducts a majority-based voting among the verified candidate chains to obtain the final answer.</p>
<p>Based on Self-consistency CoT (Wang et al., 2023f), Zhao et al. (2023b) designed the Verify-and-Edit framework to improve the factuality and accuracy of reasoning chains generated by CoT.The framework first passes predictions with lower-than-average consistency to the next stages for further processing.The second step involves producing verifying questions using manually designed prompts to test the factual correctness of the predictions.The framework then retrieves external knowledge from reliable systems (e.g., Wikipedia, Google) and edits the generated rationales with the informed answers obtained from external knowledge.Finally, the framework produces new predictions based on the edited rationales.</p>
<p>Similarly, Gou et al. (2023a) introduced a framework called CRITIC that allows large language models (LLMs) to validate and amend their own outputs through tool-interactive critiquing.CRITIC formulates various external tools into text-to-text functions (e.g., search engines, code interpreters) to integrate external tools into LLMs.Through a manually designed prompt template, the framework starts with an initial output and interacts with appropriate external tools to evaluate certain aspects of the text, revising the output based on the feedback obtained during the validation process.Zou et al. (2023) proposed AuRoRA, an augmented reasoning and refining system with task-adaptive CoT prompting.</p>
<p>AuRoRA has the characteristics of task self-adaptation and process automation.It extracts relevant knowledge from multiple sources, reducing the issue of incorrect information.Knowledge from different sources (e.g., Wikipedia) is then combined, double-checked, and refined to enhance reliability.The system revises the initial CoT using high-quality extracted knowledge to enhance accuracy and logic.</p>
<p>Instead of using a single LLM to refine their outputs based on feedback on their previous outputs, multi-agent debate has been proposed to improve reasoning performance (Du et al., 2023).Liang et al. (2023) identified a degenerationof-thought (DoT) problem-the LLM fails to generate novel thoughts through reflection even if its initial stance is incorrect once the LLM has established confidence in its solutions.The DoT problem can be addressed by allowing divergent thinking using a Multi-Agent Debate (MAD) framework where multiple agents express their arguments and a judge manages the debate process to obtain a final solution.Du et al. (2023) also leveraged multiple instances of an LLM to debate their individual reasoning processes over multiple rounds to arrive at a consistent final answer.</p>
<p>The approach has been shown to improve the factual validity of generated content and reduce fallacious answers and hallucinations.</p>
<p> Can LLMs perform reliable CoT verification?Though CoT verification approaches above have been proposed as a remedy to improve reasoning performance and reliability, the role and efficacy of the verification are questioned.</p>
<p>Recent work has tried to examine the self-verification capabilities of LLMs in reasoning tasks (Valmeekam et al., 2023;Huang et al., 2023a;Stechly et al., 2023).Huang et al. (2023a) identified that the enhancements observed in CoT verification studies were often facilitated by the utilization of oracles, which guided the self-correction process using ground-truth labels, external tools, or feedback from the environment to evaluate the correctness of the responses.However, it is crucial to note that obtaining high-quality external feedback is challenging in real-world applications.In the absence of oracles, LLMs encounter difficulties in rectifying their initial responses solely relying on their inherent capabilities-which we regard as imperfect verification.In the imperfect verification scenario, LLMs tend to nonexistent violations, and over-correct the reasoning process with false positives-walk right over the correct solution especially when there are mistakes in the verification process (Valmeekam et al., 2023).This phenomenon raises concerns about the inherent capability of the LLM to accurately assess the correctness of its reasoning process.It becomes evident that the key to achieving effective CoT verification lies in harnessing external, high-quality feedback for verification.</p>
<p>For instance, integrating external tools such as search engines and calculators into the verification process has shown beneficial (Chen et al., 2022;2023d;Olausson et al., 2023;Pan et al., 2023).</p>
<p>Application Scenarios</p>
<p>Inspired by the latest techniques proposed above to enhance the reasoning capabilities of LLMs, CoT techniques have shown greater impact with the shifts of its application scenarios.The application scenario shifts include the extension from single-language tasks to multilingual tasks, from single-language modality to multimodalities, and from complex reasoning tasks to general-purpose tasks.Shi et al. (2023) extended the CoT to encompass the realm of multilingualism and introduces the Multilingual Grade School Math (MGSM) benchmark, which evaluates the reasoning abilities of large language models in multilingual settings.This benchmark comprises 250 grade-school math problems that have been translated into ten linguistically diverse languages.Furthermore, the authors proposed a concept called "Multi-lingual CoT", which involves prompting LLMs with multilingual exemplars and incorporating English intermediate reasoning steps.This approach has been shown to yield competitive or even superior results.Multi-lingual CoT suggests that employing English chain-of-thought prompting as a baseline could be a valuable strategy for multilingual reasoning research.</p>
<p>From Single Language to Multilingual Scenarios</p>
<p>From Text Modality to Multimodalities</p>
<p>Multimodalities in CoT can be classified into two categories: input multimodalities and output multimodalities, depending on where the multimodal elements are introduced.Figure 6 illustrates these types of multimodalities in CoT.Zhang et al. (2023d) first explored input multimodalities CoT, which enables the CoT to transcend beyond textual information and proposes a multimodal CoT (MM-CoT).Instead of prompting LLMs, MM-CoT focuses on fine-tuning.MM-CoT incorporates language (text) and vision (images) modalities into a two-stage framework: rationale generation and answer inference.MM-CoT fine-tunes smaller LLMs and integrates language and visual modalities using a gated fusion mechanism.The results of this approach have demonstrated that incorporating visual information can enhance the LLM's ability to generate reasoning paths and mitigate the hallucination challenges, resulting in improved performance.</p>
<p>Input Multimodalities</p>
<p>Language Model</p>
<p>Output Multimodalities</p>
<p>Language Model Different from GoT-Rationale (Besta et al., 2023) which models the thought generation process as a graph structure, GoT-Input, on the other hand, centers its attention on modeling thought graphs derived from CoT rationales to enhance the model's reasoning capabilities.In the first stage, the model generates the rationale given the input question and a thought graph built by leveraging open IE systems to extract the sub-verb-obj triplets from the input.In the second stage, the model generates the answer given the question and the generated rationales as inputs and a new thought graph based on the input text.GoT employs different encoders for text, graph, and image (optional) respectively and enhances the deductive reasoning capability through the usage of GNN.GoT then fuses the features using a gated fusion method to generate the final answer.By modeling the non-sequential nature of human thinking within LLMs, GoT proves to enhance the LLMs with deductive reasoning abilities</p>
<p>In Huang et al. (2023b), KOSMOS-1, a multimodal language model capable of processing various modalities, was introduced.The authors explored a multimodal chain-of-thought prompting approach using KOSMOS-1.In the initial stage, when presented with an image, the authors employed the prompt "Introduce this picture in detail:" to generate a detailed description of the image as the rationale.Subsequently, the model was provided with both the rationale and a task-specific prompt to generate the final results.</p>
<p>In contrast to the input multimodalities CoT mentioned above, Visual Chain of Thoughts (VCoT) (Rose et al., 2023) introduces multimodalities into the output space.VCoT initiates the process by generating captions for visual elements and identifying multipoint foveation to maintain input sequence consistency when producing multimodal infillings.Subsequently, it employs a recursive approach to generate multimodal infillings, encompassing both images and image captions.This is achieved through a combination of novelty-driven recursive infilling and consistency-driven visual augmentation.These strategies are employed to enhance interpretability for multi-step reasoning and bridge logical gaps, ultimately contributing to improved downstream task performance.</p>
<p>From Complex Reasoning Rasks to General-Purpose Tasks</p>
<p>The applicability of CoT has expanded from its initial utilization in mathematical, commonsense, and logical reasoning tasks to encompass a wide range of NLP tasks.Wang et al. (2023g) introduced CoT into the realm of summarization and proposed the Summary Chain-of-Thought (SumCoT) technique with the aim of guiding Large Language Models (LLMs) to generate summaries in a step-by-step fashion.This approach enables the integration of more fine-grained details from source documents into the final summaries.SumCoT begins by instructing LLMs to extract core news elements from the source document using manually designed guiding question prompts.Subsequently, it involves integrating the extracted elements along with additional details from the source documents to produce comprehensive and informative summaries.In addition to the aforementioned classical NLP tasks, numerous studies have actively pursued the integration of CoT reasoning within the realm of science the development of automated intelligent agents.Singhal et al. (2022) presented MultiMedQA, a benchmark that combines six existing open question answering datasets and HealthSearchQA, a new free-response dataset of medical questions searched online.Based on the benchmark, the author then proposed instruction prompt tuning to further align Flan-PaLM to the medical domain, producing Med-PaLM.Specifically, the author used the soft prompt as an initial prefix shared across multiple medical datasets, followed by the relevant task-specific manual exemplars or instructions along with the target question.Following the CoT reasoning format, Med-PaLM's answers to consumer medical questions compared favorably with cliniciangenerated answers, demonstrating the effectiveness of instruction prompt tuning.The research provides a glimpse into the opportunities and challenges of applying large language models to the medical domain.</p>
<p>Bran et al. (2023) incorporated</p>
<p>CoT into the field of chemistry and proposed ChemCrow, a chemistry agent powered by LLM.Designed to tackle a wide spectrum of challenges spanning organic synthesis, drug discovery, and materials design, ChemCrow operates within the structured CoT reasoning format.Specifically, ChemCrow initially assembles a toolkit using various chemistry-related packages and software tools.The LLM in ChemCrow, guided by CoT reasoning principles, embarks on an automated and iterative chain-of-thought process.It begins by assessing the current state of the task, considering its alignment with the ultimate objective, planning the next steps and the choice of tools accordingly, and finally, solving the problem.Through the integration of 18 expert-designed tools, the LLM's performance in chemistry-related tasks is significantly improved.By integrating the CoT reasoning format, ChemCrow showcases its capacity to independently plan and execute a range of chemical syntheses, including an insect repellent, three organocatalysts, and even the discovery of a novel chromophore.This exemplifies its effectiveness in automating a diverse array of chemical tasks.</p>
<p>Towards Language Agents</p>
<p>With improved capabilities by the advanced techniques above, CoT reasoning has yielded a broader impact on the AI community, notably fueling the development of autonomous agents in real life.Building intelligent autonomous agents that are capable of learning and acting in a distinct environment is a long-standing goal of artificial intelligence (AI) (Searle, 1969;Wooldridge &amp; Jennings, 1995;Maes, 1995;Hendler, 1999;Wang et al., 2023b;Xi et al., 2023;Zhou et al., 2023d).In light of the swift advancements detailed previously, CoT reasoning approaches have been leveraged for perception, memory, and reasoning, language agents, thereby enabling interaction within increasingly complex  environments.These abilities serve as the foundation for developing autonomous agents that help solve complex tasks through human-agent and agent-agent collaboration.</p>
<p>As a result, LLM-based language agents, empowered by CoT techniques, have emerged in a wide range of research areas, such as engineering (Li et al., 2023a;Mehta et al., 2023;Qian et al., 2023), natural sciences (Bran et al., 2023;Kang &amp; Kim, 2023;Boiko et al., 2023), and social sciences (Aher et al., 2023;Akata et al., 2023;Ma et al., 2023;Dan et al., 2023).Those language agents are capable of following language instructions and executing actions in real-world or simulated environments.Figure 7 illustrates the representative application scenarios of agents for autonomous control (Rawles et al., 2023;Jiang et al., 2022), research (Bran et al., 2023;Boiko et al., 2023), programming (Bairi et al., 2023), andinteraction (Park et al., 2023).A detailed technical comparison of existing agents is presented in Table 2.We will elaborate the technical philosophy in the following parts.</p>
<p> What is new in language agents compared with RL agents?The pursuit of developing generally intelligent agents has been a long-standing goal of AI research.In the early stages, research on agents primarily RL techniques (Wilkins, 2014;Mnih et al., 2015).RL agents are trained to make decisions through iterative interactions with an environment, receiving feedback in the form of rewards or penalties-correct moves are rewarded, while erroneous ones are penalized.This iterative process aims to minimize mistakes and maximize accurate decisions.RL agents possess a key trait: the ability to self-evolve through continuous interactions with their environments (Bai et al., 2023a).However, RL agents face limitations.They heavily rely on expert data and meticulously designed reward functions tailored for specific tasks.Consequently, their effectiveness is often confined to individual tasks, hampering their generalization capabilities to novel tasks or domains (Kim et al., 2023a).Furthermore, the inner workings of RL agents often lack transparency and interpretability (Lundberg &amp; Lee, 2017;Yang et al., 2018).In contrast, language agents distinguish themselves from RL agents by leveraging commonsense priors embedded in LLMs.These priors reduce dependence on human annotation and trial-and-error learning, enabling easy adaptation to new tasks or environments and allowing better interpretability with CoT (Yao et al., 2022;Shah et al., 2023).However, language agents face challenges in evolving their parameters in response to environmental changes, primarily because they are predominantly adapted to environments through prompts or the heavy costs of fine-tuning the LLMs.While recent studies on language agents, such as Retroformer (Yao et al., 2023b), have incorporated RL-like policies to enhance the capabilities of language agents, the focus remains largely limited to language reasoning tasks.It holds promise to see how to bridge the gap between RL agents and language agents to facilitate future architectures that can work generally with strong performance and high interpretability in complex environments.In consideration of the pros and cons of RL agents and Language agents, please refer to Table 3 for more details.</p>
<p>Table 2: A technical comparison of representative agents.Specifically, we classify the memory modules into two main types: short-term memory and long-term memory.As defined in Section 5.2.2, short-term memory is dynamic in nature and can be easily read and written via prompts.The most common form of short-term memory is chat history.Long-term memory, on the other hand, is static and is typically stored in a database, accessible through various retrieval methods, including tree search, text search, and vector retrieval.For the external tools module, we divide the tools into three types: Web search (Web), Code interpreter (Code), and other tools (Other).More details of tool use can be found in Section 5.1.3.Training Process Trained through iterative interactions with the environment, receiving rewards or penalties.</p>
<p>Adaptation to new tasks or environments with reduced dependence on human annotation, primarily through prompts.</p>
<p>Self-Evolution</p>
<p>Possess the ability to self-evolve through continuous interactions with the environment.</p>
<p>Face challenges in evolving parameters in response to environmental changes; Adaptation is mainly through prompts or costly fine-tuning of LLMs.</p>
<p>Limitations</p>
<p>Heavily relies on expert data and task-specific reward functions; Effectiveness often confined to individual tasks.</p>
<p>Challenges in evolving parameters dynamically; Focus on language reasoning tasks, may lack adaptability to broader tasks.</p>
<p>Transparency</p>
<p>Working mechanism often lacks transparency and interpretability.Generally allow better interpretability with commonsense priors, but have challenges in parameter evolution transparency.</p>
<p>Generalization</p>
<p>Limited generalization capabilities to novel tasks or domains.</p>
<p>Facilitates easy adaptation to new tasks or environments, reducing dependence on task-specific training.Primary focus remains on language tasks.</p>
<p>Future Goals</p>
<p>Aim to bridge the gap between RL agents and language agents, facilitating more versatile and adaptable architectures.The following part will introduce the basic concepts of language agents and show how CoT is utilized in those agents.</p>
<p>General Framework</p>
<p>The landscape of language agent frameworks within the existing literature is notably diverse.We outline representative architectures in recent studies and summarize a cohesive and overarching conceptual framework for language agents.Wang et al. (2023b) designed a modulized agent framework with four modules: (i) a profiling module to identify the role of the agent, (ii) a memory module to recall past behaviors, (iii) a planning module to plan future action, and (iv) an action module to translate the agent's decisions into specific outputs.Xi et al. (2023) proposed a conceptual agent framework with three components: (i) brain that undertakes basic tasks like memorizing, thinking, and decision-making, (ii) perception that perceives and processes multimodal information from the external environment, and (iii) action that carries out the execution using tools and influences the surroundings.Zhou et al. (2023d) presented a featurized agent framework for language agents, which supports important features, including planning, memory, tool use, multi-agent communication, and fine-grained symbolic control.Sumers et al. (2023) proposed another conceptual architecture for language agents called CoALA.CoALA organizes agents along three key dimensions: (i) information storage that is divided into working and long-term memories, (ii) action space that is divided into internal and external actions, and (iii) decision-making procedure that is structured as an interactive loop with planning and execution.</p>
<p>Though different architectures have been designed, recent technical research (Yao et al., 2022;2023b;Park et al., 2023;Zhu et al., 2023) tends to follow the line of the conceptual framework by prompting LLMs to imitate the agent processes such as perception, memory, and reasoning.The basic assumption is that LLMs have already captured world knowledge to some extent (Gurnee &amp; Tegmark, 2023), which can be induced by CoT prompting step by step.</p>
<p>Therefore, we summarize a general conceptual framework of language agents in view of technical practice as shown in Figure 8.Given a user instruction (also known as a goal), an agent needs to complete the task with multiple steps of interaction across the environment, possibly operating with tools.Without the loss of generality, we focus on a single agent when introducing the framework.It is worth noting that multiple agents can cooperate or compete with each other in a multi-agent environment.Before diving into the technical discussion, we first present the basic concepts of language agents, i.e., agent, environment, and tool use, as below.</p>
<p>Agent Backbone Model</p>
<p>A language agent can be built upon either a single-modality LLM or a multimodal LLM.Completing a task often comes with multiple steps of interaction.The entire process is called an episode, which is composed of a series of turns.To accomplish the task, the agent needs to plan ahead, make decisions, and execute actions at each turn of the episode.The process of planning, decision-making, and action execution may reflect the reasoning ability of LLMs as LLMs are exposed to real-world or virtual environments that do not exist during the pre-training of LLMs.In such environments, the LLM must perceive the world's knowledge and take action, in which cases we will show that CoT helps bridge the gap between the environment perception and the innate ability of LLMs.</p>
<p>Such agents expand the landscape of language models to compete in specific fields, including application operation, web searching, and web shopping.There are two popular types of language agents: autonomous agents and communicative agents.Typical examples of autonomous agents are AutoGPT (Richards, 2023), BabyAGI (Nakajima, 2023), and AgentGPT (Reworkd, 2023).In contrast, communicative agents are personalized and socialized agents with human behaviors that can communicate (Park et al., 2023;Wang et al., 2023c;Zhu et al., 2023), collaborate (Hong et al., 2023;Qian et al., 2023) and debate (Liang et al., 2023;Du et al., 2023;Xiong et al., 2023a) with each other.They are often deployed in immersive environments.</p>
<p>Environment Interaction</p>
<p>An intrinsic characteristic of language agents is communicating, interacting, and evolving with environments.Such environments include operation systems, third-party applications, webpages, and virtual environments.LLMs handle environments with two kinds of approaches, namely, environment parsing and multimodal perception, depending on whether the LLM has the ability to model the multimodal inputs.Environment parsing refers to those approaches that leverage external tools such as optical character recognition (OCR) and icon detectors (Zhang et al., 2021;Sunkara et al., 2022) to parse the environment into textual elements (e.g., HTML layouts) as inputs to an LLM.In contrast, multimodal perception, also dubbed as first principles thinking (Zhang &amp; Zhang, 2023), refers to using a multimodal LLM to simultaneously process the inputs in different modalities.To build a multimodal LLM, a popular way is to use a simple projection matrix to integrate a pre-trained large vision model (e.g., CLIP (Radford et al., 2021) and BLIP-2 (Li et al., 2023c)) into an LLM (Liu et al., 2023b;Zhang et al., 2023a).More recent studies have also explored modeling the inputs of different modalities into the same vector space, thus resulting in any-to-any representation learning (Huang et al., 2023b;Wu et al., 2023;Moon et al., 2023) and interleaved multimodal representation learning (Li et al., 2023b;Zhao et al., 2023a).</p>
<p>Tool Use</p>
<p>Tool use can be seen as an expansion of a language model's ability boundary, compensating for parametric knowledge for reasoning and grounding the language model's capabilities to interact with environments (Qin et al., 2023b).Tools coming into play include knowledge bases, search engines, code interpreters, online models, applications, databases, and even bespoke tools specially created for specific tasks, overcoming the constraints of generic APIs (Li et al., 2023d;Schick et al., 2023;Cai et al., 2023;Zhou et al., 2023d;Team, 2023).</p>
<p>The purpose of tool use comes with three aspects:</p>
<p> Action execution.The language model is not confined to merely predicting the next action; it has the capability to execute it in the real environment.This includes everything from executing codes or queries through a JavaScript element selection on a webpage (Zhou et al., 2023c), executing programs via code interpreters or compilers (Gur et al., 2023;Ni et al., 2023;Ddac et al., 2023;Ruan et al., 2023a;Gou et al., 2023b), to interacting with online expert models which serve as callable APIs (Shen et al., 2023;Patil et al., 2023;Ge et al., 2023).These steps can be dynamically adjusted with effective scaling of the tool set depending on task requirements and computational capacity (Yuan et al., 2023).</p>
<p> External knowledge acquisition.Retrieval augmentation has been shown so effective that has been regarded as a standard solution to alleviate the factuality drawback (Trivedi et al., 2022;Yao et al., 2022).To empower the CoT process, up-to-date knowledge is accessible through search engines (Khattab et al., 2022;Nakano et al., 2021), while domain-specific through expert candidates (Bran et al., 2023;Ge et al., 2023).The purpose of tool use extends beyond augmenting the language model's scope; they enable language models to adapt to a complex environment or a vast application ecosystem and ensure that the information language models have access to is up-to-date, thereby reducing the propensity to generate non-factual information (Wang et al., 2023b).</p>
<p> Reasoning and verification.In the reasoning process, language models are sometimes prone to errors.Tools that provide accurate, real-time knowledge can help correct reasoning errors and formulate more accurate responses.Pieces of evidence from these tools are used to rewrite the initial output for self-correction (Gou et al., 2023a).Code LLMs can be further verified with execution results from program executors (Ni et al., 2023).Multi-tool and multi-step planning and retrieval strategies, involving depth-first or breadth-first approaches, can be deployed for a deep or diverse range of possible pathways (Liu et al., 2023e;Qin et al., 2023b).</p>
<p>CoT Facilitates Agent Abilities</p>
<p>Language agents are placed in interactive loops with the external environment (Sumers et al., 2023).The interface loops can be elicited in three ways (Figure 1), namely, perception, memory, and reasoning.CoT methods empower the agents from all three perspectives.</p>
<p>Perception as CoT</p>
<p>Prompting the agent to interpret the perception step by step, as a chain of perception, has been shown to improve the action success rate.It enhances the understanding of the environment or the context.Notably, Rawles et al. (2023) found that using the CoT template, "Answer: Let's think step by step.I see <Screen Caption>, I need to ...", substantially improves the action prediction accuracy.As an example shown in Figure 1, the prompt of perception as CoT can be "Let's think step by step.I see unrelated search results in the Google app".Furthermore, Zhang et al. (2023d) and Huang et al. (2023b) leveraged external tools to obtain the image captions as supplemental inputs to help improve the perception of the multimodal environments.The captions are placed in <Screen Caption> to organize the input prompt.</p>
<p>In addition to the one-way interpretation of perception, language agents can benefit significantly from integrating environmental feedback, especially in the context of multi-turn interactions where the environment is subject to alterations (Chen et al., 2023d;Olausson et al., 2023;Jignasu et al., 2023).Effectively integrating this feedback necessitates the implementation of a crucial method: self-correction with environment feedback (Xu et al., 2023d;Zhou et al., 2023a;Yao et al., 2023b;Zhao et al., 2023d).Self-correction entails exposing the model to intricate sequences of operations, encompassing tasks such as executing codes, conducting operations, and controlling robots.These operations can lead to execution failures and generate error messages.In this context, the agent is not only required to comprehend these environmental cues but must also actively engage in iterative error correction processes until the desired outcome is achieved.Consequently, the agent's performance within these dynamic environments serves as a direct indicator of its self-correction proficiency.This proficiency, in turn, showcases the agent's ability to assimilate feedback from the environment effectively.The seamless incorporation of such feedback not only refines the interpretive capacities but also enhances its overall functionality, making it pivotal in the realm of advanced language agents.</p>
<p> Is language-centered perception the future?Multimodal perception stands as one of the key steps toward achieving artificial general intelligence.Current trends, likely inspired by the impressive reasoning capacities of language models, predominantly adopt a language-centered perception approach (Figure 9(a)).Typically, distinct encoders are utilized to process inputs from various modalities, such as images.The resulting encodings are then linked to an existing language model through cross-attention or supplementary adapters, facilitating the integration of multimodal inputs into the language model's embedding space (Alayrac et al., 2022;Liu et al., 2023a;Wu et al., 2023;Driess et al., 2023;Chen et al., 2023c;Bai et al., 2023b;Zhang et al., 2023a).In contrast to this prevailing language-centric modeling, Rust et al. (2023) has proposed an image-centered approach (Figure 9(b)) by rendering text as images, enabling the transfer of representations across languages based on orthographic similarity or the co-activation of pixels.To better align the inputs from different modalities and allow for convenient scaling up model parameters, recent research endeavors have explored a unified approach (Figure 9(c)).For instance, in the context of vision-language modalities, instead of employing a separate image encoder, image patches are treated as tokens and linearly projected into the embedding layer of the transformer.These patches are then fused with the representations of language tokens, allowing for seamless integration (Huang et al., 2023b;Bavishi et al., 2023).</p>
<p>Though various kinds of perception approaches, including language-centered, image-centered, and unified methods, have been proposed in the realm of agent perception, determining the most suitable choice remains a formidable challenge.This difficulty arises due to the involvement of more diverse and complex modalities such as auditory, tactile, and brain signals during interactions between agents and environments.Besides, these modalities often come with imbalanced data scales, complicating the perception process.Additionally, the diversity in types and formats of multimodal data poses challenges related to computation efficiency and the scalability of models.Exploring innovative methods to address these challenges will pave the way for the development of effective and efficient perception frameworks in the future.</p>
<p>Memory as CoT</p>
<p>A language agent is commonly equipped with both long-term memory and short-term memory (Sumers et al., 2023;Wang et al., 2023e).</p>
<p>Short-term memory.Short-term memory is formed as temporal information that may be flexible to change in different steps of episodes (also known as working memory in Sumers et al. (2023).Short-term memory is more temporal-specific, offering explicit, recent context that facilitates the agent.On the one hand, short-term memory shows direct support and closer relations with the exact current state.On the other hand, short-term memory yields a relatively moderate impact on the whole environment.For example, short-term memory can be modeled within an episode of a multi-step task, the chain of action history (Zhang &amp; Zhang, 2023), or the rationales or sub-question in the last several hops of multi-hop question answering (Yao et al., 2022;Khattab et al., 2022).Due to the significant temporal character, short-term memory raises little storage concern.</p>
<p>Long-term memory.Long-term memory provides the agent with the capability to retain and recall static information over episodes (Weng, 2023).In contrast to short-term memory, long-term memory is more general to the task, as a macroscopic and abstract understanding of the whole world.This can include procedural memory that stores the production system itself, semantic memory that stores facts about the world, and episodic memory that stores sequences of the agent's past behavior (Sumers et al., 2023) Long-term memories can rely on both parametric and non-parametric knowledge storage.They can be from the trainable parameters of the language agents or maintained as external knowledge that can be leveraged through retrieval systems.</p>
<p>For example, the earlier hops of former episodes are long-term memories from agent parameters, and the output action formulations are parametric long-term memories.</p>
<p> Towards efficient memory operation.Modeling memory as linear natural language sequences becomes inefficient as sequences lengthen during the agent's interaction with environments.Besides, the context window of LLMs is predetermined to be limited in length.To pursue more efficient memory operations, recent studies have explored two types of approaches, i.e., leveraging (i) tree search and (ii) vector retrieval.</p>
<p>(i) Tree search.Memory can be stored with a tree structure and fetched by searching on the tree.Notably, MemWalker (Chen et al., 2023b) empowered agents to access textual memory information through iterative prompting.In this approach, the agent initially processes the lengthy context into a tree of summary nodes.Upon receiving a query, the agent navigates this tree to search for relevant information and responds after gathering sufficient information.Similarly, GITM (Zhu et al., 2023) proposed an LLM Decomposer that recursively decomposes goals into a sub-goal tree.The hierarchical tree structure helps the model to explicitly capture the relationships between goals and corresponding plans in the memory.Park et al. (2023) proposed the reflection tree to organise the memory of a communicative agent.When facing the trivial observations during the interaction with the environment, the agent periodically reflects on existing memories in a abstract manner, thus forming a reflection tree: "the leaf nodes of the tree represent the base observations, and the non-leaf nodes represent thoughts that become more abstract and higher-level the higher up the tree they are".</p>
<p>(ii) Vector retrieval.The other way to store memory is via vector storage (Hu et al., 2023;Zhou et al., 2023e).Vector database has become a key carrier for storing, managing, and retrieving high-dimensional data, such as the long-term memory of language agents.It can represent complex data types such as text, images, videos, and even structured data.Agentsims (Lin et al., 2023) employed a vector database to enable efficient storage and retrieval within long-term memory.Specifically, it stores daily memories as embeddings within this vector database.When the agent encounters new situations and necessitates the recall of past memories, the long-term memory system adeptly retrieves pertinent information, thereby ensuring the consistency of the agent's behavior.</p>
<p>Reasoning as CoT</p>
<p>Inspired by the success of eliciting LLMs' step-by-step reasoning abilities, CoT has also been applied in inducing the agents to reason via planning or decision-making.More importantly, CoT methods for language agents require careful design to handle the action execution and state observation.</p>
<p>The gap between reasoning and action is bridged by combining interleaving thought, action, and observation (Yao et al., 2022;Khattab et al., 2022;Shinn et al., 2023).By exploring the use of LLMs to generate both CoT traces and task-specific actions in an interleaved manner, it has been found that reasoning and acting achieve mutual promotion.</p>
<p>Reasoning traces help the model make action plans and handle exceptions, while actions allow the LLM to interface with external sources, such as knowledge bases or environments, to gather additional information for knowledge support.(Xu et al., 2023b) detached the reasoning process from external observations to reduce token consumption during multiple steps of CoT.</p>
<p>Similarly, AgentBench (Liu et al., 2023c) compelled language agents to complete tasks via "think" and "Act" steps.Further, Zhang &amp; Zhang (2023) proposed a chain-of-action technique-leveraging a series of intermediate previous action histories and future action plans-to help the agent decide what action to execute, which transforms the decision-making as a CoT reasoning problem.</p>
<p> How to expand the capability of agents?Currently, the mainstream interest is to apply CoT prompting approaches to elicit LLMs' reasoning abilities during the interaction with the environments as discussed above.The basic hypothesis is that LLMs already have the prior knowledge to perform as the language agents for our concerned tasks and CoT prompting approaches are effective in invoking the knowledge.Those prompting techniques have the advantage of flexibility and convenience because it is easy to design and adjust the prompts according to the task requirements and characteristics.However, LLM performance has shown to be sensitive to prompts and there is a lack of evidence that LLM can actually learn domain knowledge from the prompts.Therefore, purely prompting methods may not be adequate to make LLMs generalizable to new domains.To expand the capability boundary of language agents, there is a recent interest in fine-tuning LLMs on curated datasets to build effective agents.Chen et al. (2023a) called for a re-thinking of fine-tuning language models when the target tasks and data formats are known and enough data can be collected (e.g., possibly automatically with GPT-4).The results have revealed that fine-tuning can not only achieve strong generalization and robustness but also improve performance.Gou et al. (2023b) curated interleaved tool-use data composed of natural language CoT with tool-integrated programs.Then, a tool-integrated reasoning agent was trained on those high-quality annotations and achieved substantial performance gains on various mathematical reasoning tasks.</p>
<p>Challenges</p>
<p>Despite the swift advancements in the realms of LLMs, CoT reasoning, and language agents, numerous promising challenges still beckon for deeper exploration, particularly pertaining to generalization to unseen domains, enhancing efficiency amidst redundant interactions, developing customizable agents, scaling up language agents, ensuring the safety of language agents, and capacity evaluation.</p>
<p>Generalization to Unseen Domains</p>
<p>Language agents have found extensive applications in practical fields such as engineering (Li et al., 2023a;Mehta et al., 2023;Qian et al., 2023), natural sciences (Bran et al., 2023;Kang &amp; Kim, 2023;Boiko et al., 2023), and social sciences (Aher et al., 2023;Akata et al., 2023;Ma et al., 2023;Dan et al., 2023).Despite their widespread use, a significant challenge persists: adapting LLMs to specific, especially unseen domains.This challenge is twofold: firstly, determining an efficient method for acquiring domain-specific knowledge, such as employing CoT prompting techniques.The limitations arise from the finite scope of knowledge acquisition during pre-training on textual corpora, lacking substantial interaction with the physical world.Secondly, there is the challenge of effectively adapting LLMs to diverse, unseen domains.Given the substantial variation in action spaces across tasks (e.g., drone control versus web browsing), aligning the model's knowledge with the specific task requirements remains a formidable obstacle.These challenges underscore a critical gap in current research.The need to enhance LLMs' adaptability to novel domains and help LLMs learn from environments is paramount, requiring innovative solutions that address both knowledge acquisition and effective task alignment.</p>
<p>Prompting and fine-tuning are widely used techniques to adapt pre-trained LLMs to new domains.However, it remains an underexplored area of when and how to leverage prompting (e.g., prompting pattern and reasoning format) and fine-tuning (e.g., instruction tuning) techniques to help LLMs generalize to unseen domains.In doing so, researchers can pave the way for more versatile and impactful applications of language agents across a myriad of fields.</p>
<p>Efficiency against Redundant Interactions</p>
<p>Completing a task necessitates intricate, multi-step interactions with the environment.This process results in extensive and repetitive logs, which have been identified as pivotal for task completion (Zhang &amp; Zhang, 2023).However, due to computational constraints, most studies utilize only a limited number of log steps (Park et al., 2023).Although recent advancements have expanded the capacity of LLMs to handle extended contexts (Xiong et al., 2023b), conducting inference based on these logs is hampered by the inherently slow speed of autoregressive LLMs.This issue is exacerbated in multi-agent interaction environments, where numerous agents generate a substantial volume of interaction logs.</p>
<p>To tackle this challenge, one potential solution is to incorporate a memory mechanism for storing and retrieving knowledge from these logs.However, the key challenge lies in exploring effective methods to discern salient knowledge and distill relevant information from the logs.Addressing this challenge is crucial for enhancing the efficiency of inference processes in complex, multi-agent scenarios.</p>
<p>Customizable Language Agents</p>
<p>LLMs are usually supposed to acquire general language ability and common knowledge through pre-training on largescale corpora, and then cater to human preferences following instructions through further alignment tuning, including instruction tuning and reinforcement learning from human feedback.Whereas, users have specialized requirements and individual characteristics.Thus, building a customizable assistant from LLMs is of great importance.</p>
<p>Existing related studies mostly fall into three general methods: (i) customizable prompting, often with role or tool specifications.CAMEL (Li et al., 2023a) prompted LLM with formatted profiles of human-agent pairs to simulate the workflow of diverse groups of internet users or occupations.MetaAgents (Li et al., 2023e) prompted the language agent to play a specific role in some certain social context.ExpertPrompting (Xu et al., 2023a) proposed to prompt LLMs to solve a problem conditioned on an expert identity profile that is best suited for the problem.RoCo (Mandi et al., 2023) assigned robots with an LLM role to talk on their behalves, generating plans for practical tasks.Customizable ChatGPT has also been announced to comply with specified instructions, extra knowledge, and a combination of skills; 2 (ii) customizable training.The gradient updates in the language model can further ensure the customizable alignment.Auto-UI (Zhang &amp; Zhang, 2023) was trained on the Android UI control domain, achieving stable performance as an autonomous agent.For the communicative agent, Character-LLM (Shao et al., 2023) trained the LLMs with profiles and detailed scenes, enabling LLMs to mimic well-known people, like Beethoven; (iii) customizable model editing.Besides training, editing is an alternative to changing stored knowledge in language agents, which improves the factuality and reliability of a customized assistant.ROME (Meng et al., 2022a) and MEMIT (Meng et al., 2022b) used the locate-and-edit method to correct wrong knowledge.Transformer-Patcher (Huang et al., 2023c) further alleviated the error recurrence by real-time sequential editing.Beyond factual knowledge correctness, PersonalityEdit (Mao et al., 2023) changed the model response to match the Big Five personality traits.</p>
<p>Despite the recent progress, the challenge of developing customizable agents still lies within three folds.Firstly, existing studies mainly focus on methods for practical applications in certain, separate domains.However, fewer considerations are oriented to the specific requirements of users.Secondly, language agent customization requires lightweight, efficient, and low-resource consumption, especially for user-level customization.Different from large-scale, general training, customization peruses effective methods that involve fewer data, partial parameters, or only elaborate designed prompts.Thirdly, the balance between customization and information security needs to be maintained.The user's properties and records (such as age, gender, and medical record) may be exposed to an agent, resulting in a risk of privacy leakage.</p>
<p>Scaling up Language Agents</p>
<p>Multi-agent systems have exhibited social phenomena (Park et al., 2023;Wang et al., 2023c;Zhu et al., 2023).Inspired by the observations, recent interest has considered scaling the number of language agents (Li et al., 2023a) to form a large-scale language model society.However, computation overhead is still an obstacle when modeling multi-agent communications (Xi et al., 2023).In the realm of future prospects, the exploration of scaling unveils intriguing possibilities across two pivotal domains.Firstly, there arises a profound curiosity concerning the potential emergence of novel capabilities within a singular agent amidst communication.Secondly, comprehending the implications of scaling, such as personality change and social phenomenon, becomes imperative in empowering language agents to address increasingly complex challenges.Furthermore, this comprehension serves as a linchpin in observing, detecting, and mitigating the risks entailed by potentially harmful behaviors, thereby ensuring the secure and beneficial evolution of these agents for the betterment of society.</p>
<p>Safety of Language Agents</p>
<p>Imagine a near future where intelligent agents are anticipated to seamlessly collaborate with humans and other agents, simplifying daily tasks and interacting with diverse environments.This convenience is accompanied by a significant challenge: ensuring the safety of these agents, especially during prolonged, multi-round interactions.For example, the popular user interface agents designed for web operation (Zhou et al., 2023c) and mobile device control (Zhang &amp; Zhang, 2023) may result in privacy leakage and permission abuse.Shaikh et al. (2022) called for attention to the bias and toxicity in Zero-Shot-CoT reasoning as it tends to significantly induce the model to produce harmful or undesirable output, which may also bring negative effects in language agents.Effectively addressing the safety challenge demands a multifaceted approach.Firstly, the exploration of more robust and controllable model architectures, coupled with an in-depth understanding of their underlying mechanisms, shows great promise.Delving into the intricacies of agent behavior and enhancing the reliability of their responses are pivotal in this endeavor.Secondly, the rapid evolution of attacks tailored for LLMs necessitates a reevaluation of traditional defense techniques.</p>
<p>Existing studies concerning LLM safety mainly focus on content safety, such as offensiveness, fairness, and bias of LLM-generated contents (Zhang et al., 2023b).As language agents are exposed to multi-turn interactions in distinct environments possibly with operating tools, new safety risks may emerge at a systematic level (Xu et al., 2023c;Sato et al., 2023), ranging from instruction input, environment perception, reasoning process, as well as tool use.We summarize three key properties of agent safety risks, including (i) new attacking types, such as operation attacking by environment injection (Liu et al., tool misuse (Fu et al., 2023), jailbreaking (Deng et al., 2023;Wei et al., 2023a), and privacy leakage (Kim et al., 2023b); (ii) new attacking surface during the interaction between agent-human, agent-agent, and agent-environment; (iii) complex types of environments, such as operation systems, third-party applications, webpages, and virtual environment.</p>
<p>However, the safety of language languages has been underexplored.The definition of language agent safety has not yet reached an agreement.Novel attack methods, specifically designed for language agents, present unique challenges.Consequently, innovative defense strategies must be developed to mitigate safety risks induced by these sophisticated attacks, particularly in complex environments.This dual focus on building benchmarking resources, enhancing internal safety measures, and fortifying defenses against external threats is paramount for ensuring the secure integration of intelligent agents into our daily lives.</p>
<p>Evaluation of Language Agents</p>
<p>Early studies in NLP mainly focus on assessing a specific ability of models, for example, machine translation, question answering, and summarization (Chang et al., 2023).The evaluation tends to be dataset-centered, which makes it hard to reflect the model's general ability.In the era of LLMs, more comprehensive benchmark datasets have been released, such as MMLU (Hendrycks et al., 2021a), BIG-Bench (Srivastava et al., 2022), and AGI Eval (Zhong et al., 2023).However, the major focus of those benchmark datasets is on the understanding and reasoning abilities of LLMs.Besides, they are mostly single-turn evaluations, which makes it hard to evaluate the planning and decision-making abilities of LLM in distinct environments.</p>
<p>There is an increasing interest in developing environment-centered evaluation approaches.As language agents are exposed to interactive environments, it remains challenging to evaluate those agents in a volatile environment.Specifically, the measurement of task success might be task-specific and ambiguous.For example, in a system control problem (Rawles et al., 2023), a user instruction can be completed by different trajectories, however, it is hard to annotate all possible ways as gold labels for evaluation.To address the challenge, simulation-based evaluation (Wang et al., 2023e;Yang et al., 2023b;Ruan et al., 2023b) has attracted increasing interest.Execution feedback or external judgment can be used to measure if the task is successful or not.However, execution feedback is not always accessible in every kind of environment, and using external judgment may also include model bias (Wang et al., 2023e).</p>
<p>Besides assessing task success rate, it is also critical to consider safety risks as discussed in Section 6.5.Furthermore, as language agents may evolve in the environments, especially in multi-agent communities, how to track and evaluate the agent properties is also a challenge.</p>
<p>Conclusion</p>
<p>In slightly over a year, CoT techniques have substantially enhanced the reasoning capabilities of LLMs.Going beyond the confines of reasoning tasks in NLP, CoT techniques have been expanded to facilitate the development of language agents.These agents have demonstrated the ability to comprehend language instructions and execute actions in diverse environments.This study meticulously examines the evolution from CoT reasoning to the automation of language agents, offering a comprehensive review and delving into key research topics.These topics include investigating the foundational mechanics underpinning CoT techniques, understanding the paradigm shift associated with CoT, and exploring the emergence of language agents facilitated by CoT techniques.Furthermore, this research delineates several promising avenues for future exploration, including aspects related to generalization, efficiency, customization, scaling, and safety.</p>
<p>Figure 1 :
1
Figure 1: An overview of language agent framework empowered with the chain-of-thought (CoT) mechanism in perception, memory, and reasoning.</p>
<p>Figure 4 :
4
Figure 4: Overview of representative CoT approaches.We delve into the paradigm shifts of CoT techniques in three key directions: (i) prompting pattern (instruction generation and exemplar generation); (ii) reasoning format (CoT formulation, reasoning aggregation, and CoT verification); and (iii) application scenario (multilingualism, multimodality, and general-purpose tasks).</p>
<p>Figure 5 :
5
Figure5: Formulation Shifts of CoT.We illustrate five representative CoT formulations in chronological order: (i) Chain-of-Thoughts (CoT), (ii) Programm-of-Thoughts (PoT)(Chen et al., 2022), (iii) Table-of-Thoughts (Tab-CoT)(Ziqi &amp; Lu, 2023), (iv) Tree-of-Thoughts (ToT)(Yao et al., 2023a), (v) Graph-of-Thoughts-Rationale (GoT-Rationale)(Besta et al., 2023).</p>
<p>Figure 6 :
6
Figure 6: Formulation of multimodalities CoT.We categorized multimodalities in CoT into two types: (i) Input Multimodalities: Various modalities such as text, image (Zhang et al., 2023d), caption (Huang et al., 2023b), and graph Yao et al. (2023c) are incorporated into the model's input; (ii) Output Multimodalities: Multimodalities, including text and image (Rose et al., 2023), are introduced into the model's output.</p>
<p>Li et al. (2023f) proposed Self-Prompting LLMs for Open-Domain QA (ODQA).Self-Prompting consists of two stages: In the first stage, the model tasks LLM with generating a pseudo ODQA dataset by prompting it to automatically construct QA pairs with context paragraphs and explanations.In the second stage, the model dynamically selects a few examples from a pool using a clustering-based retrieval method to serve as context demonstrations.These selected examples aid in understanding and answering specific questions.He et al. (2023) explored the CoT technique in machine translation and introduced Multi-Aspect Prompting and Selection (MAPS).Drawing inspiration from strategies employed by human translators, MAPS breaks down the machine translation process into several steps.It requires the LLM to initially discern the topics and keywords of the sentence awaiting translation, and then to retrieve analogous example sentences.By integrating this extracted knowledge, the LLM produces more accurate translations.</p>
<p>Figure 7 :
7
Figure 7: Representative agents for autonomous control, research, programming, and interaction.The illustrations are adapted from Rawles et al. (2023), Jiang et al. (2022), Bran et al. (2023), Boiko et al. (2023), Bairi et al. (2023), and Park et al. (2023).</p>
<p>Figure 8 :
8
Figure8: General framework of language agents.Language agents are capable of following language instructions and executing actions in real-world or simulated environments.</p>
<p>Figure 9 :
9
Figure 9: Multimodal perception methods including (a) language-centered method; (b) image-centered method; (c) unified method.</p>
<p>; (ii) Commonsense Reasoning: CSQA(Talmor et al.,
Arithmetic ReasoningCommonsense ReasoningSymbolic ReasoningFigure 3: Performance on seven reasoning tasks. "Direct Prompt" refers to the standard few-shot prompting approach,where exemplars are formatted as questions and answers, with the model providing direct answers. "Best CoT w/oSC", "Best CoT w/ SC" and "Best CoT*" represent the highest accuracy (%) achieved as of October 2023 ("SC" standsfor self-consistency</p>
<p>Table 1 :
1
Wang et al. (2023f)reasoning tasks (SC: self-consistency byWang et al. (2023f)).
CategoryDatasetModelBest Acc LLMGSM8KCSV (Zhou et al., 2023b)97.00GPT-4 Code InterpreterArithmetic ReasoningAQuANatural Program (Ling et al., 2023) 70.34ChatgptSVAMPPoT (Chen et al., 2022) + SC89.10Text-davinci-002Commonsense ReasoningCSQA Strategy QAManual-CoT (Wei et al., 2023b) + SC 95.10 Manual-CoT (Wei et al., 2023b) + SC 90.40PaLM 2 PaLM 2Symbolic Reasoninglast letter concatenation Natural Program (Ling et al., 2023) 92.98 Coin Flip Auto-CoT (Zhang et al., 2023c) 99.90Chatgpt Text-davinci-002</p>
<p>Yao et al. (2023a)tural reasoning ablilty for LLMs.They introduce the Tabular Chain of Thought (Tab-CoT), which adopts a table-filling approach to model CoT.In Tab-CoT, an instruction of "| step | subquestion | process | result |" is manually designed to prompt LLMs to generate a table while conducting the reasoning process.The answer is then extracted from the generated table at the end of the process.Tab-CoT showcases robust zero-shot and few-shot capabilities in performing reasoning across multiple dimensions, encompassing both rows and columns.Yao et al. (2023a)proposed Tree-of-Thoughts (ToT) that breaks CoT into thought units and formulates them into tree structure.ToT allows LLMs to explore coherent thought units that serve as intermediate steps toward problem-solving, consider different options and evaluate their decisions.By incorporating different methods, ToT is able to look ahead to determine what to do next or trace-back to correct history decisions.Experiments have demonstrated that ToT significantly elevates the problem-solving capabilities of language models.This improvement is particularly noteworthy in the context of tasks that demand intricate non-trivial planning or search processes</p>
<p>Yao et al. (2023c))et al. (2023)further extended the tree structure into graph structure and propose Graph-of-Thoughts-Rationale (To distinguish from GoT proposed byYao et al. (2023c),Besta et al. (</p>
<p>Table 3 :
3
Comparison between RL agents and language agents.
AspectRL AgentsLanguage AgentsKnowledge Aquisition Primarily use RL techniques.Leverage commonsense priors embedded in LLMs.</p>
<p>. For example, given a goal, upvote the latest post, in the varied environment states, two chains of actions have been observed to accomplish the goal: (i) [opening Instagram, going to home feed, looking at the latest post, upvoting the latest post] and (ii) = [go to the HOME screen, opening Instagram, going to home feed, looking at a post, upvoting the latest post].It can be found that atom actions [opening Instagram, going to home feed, looking at the latest post, upvoting the latest post] can serve as long-term memory for this goal, i.e., a chain of static memory.</p>
<p>It should be noted that recent studies have explored fine-tuning smaller language models to perform CoT reasoning for specific tasks(Magister   et al.,<br />
; Yue et al.,<br />
). Here we only discuss general scenarios where LLMs can achieve effective CoT reasoning per se-better performance than direct reasoning-without additional task-specific fine-tuning on CoT-style training data.
https://openai.com/blog/introducing-gpts.</p>
<p>Act-1: Transformer for actions. Adept. 2022</p>
<p>Using large language models to simulate multiple humans and replicate human subject studies. Rosa I Gati V Aher, Adam Arriaga, Kalai Tauman, International Conference on Machine Learning. PMLR2023</p>
<p>Playing repeated games with large language models. Elif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh, Matthias Bethge, Eric Schulz, abs/2305.168672023ArXiv preprint</p>
<p>Flamingo: a visual language model for few-shot learning. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Advances in Neural Information Processing Systems. 202235</p>
<p>Evolutionary reinforcement learning: A survey. Hui Bai, Ran Cheng, Yaochu Jin, Intelligent Computing. 2252023a</p>
<p>Qwen-vl: A frontier large vision-language model with versatile abilities. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, Jingren Zhou, abs/2308.129662023bArXiv preprint</p>
<p>Codeplan: Repository-level coding using llms and planning. Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, Shashank Ashok, Shet, abs/2309.124992023ArXiv preprint</p>
<p>Introducing our multimodal models. Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, Sagnak Tarlar, 2023</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, Torsten Hoefler, abs/2308.096872023ArXiv preprint</p>
<p>When do program-of-thoughts work for reasoning?. Zhen Bi, Ningyu Zhang, Yinuo Jiang, Shumin Deng, Guozhou Zheng, Huajun Chen, ArXiv preprint, abs/2308.154522023</p>
<p>Emergent autonomous scientific research capabilities of large language models. Robert Daniil A Boiko, Gabe Macknight, Gomes, abs/2304.053322023ArXiv preprint</p>
<p>Chemcrow: Augmenting large-language models with chemistry tools. Sam Andres M Bran, Andrew D Cox, Philippe White, Schwaller, abs/2304.053762023ArXiv preprint</p>
<p>Large language models as tool makers. Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, Denny Zhou, abs/2305.171262023ArXiv preprint</p>
<p>A survey on evaluation of large language models. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, abs/2307.031092023ArXiv preprint</p>
<p>Fireact: Toward language agent fine-tuning. Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, Shunyu Yao, abs/2310.059152023aArXiv preprint</p>
<p>Walking down the memory maze: Beyond context limit through interactive reading. Howard Chen, Ramakanth Pasunuru, Jason Weston, Asli Celikyilmaz, abs/2310.050292023bArXiv preprint</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, abs/2211.125882022ArXiv preprint</p>
<p>Pali-x: On scaling up a multilingual vision and language model. Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, abs/2305.185652023cArXiv preprint</p>
<p>Teaching large language models to self-debug. Xinyun Chen, Maxwell Lin, Nathanael Schrli, Denny Zhou, abs/2304.051282023dArXiv preprint</p>
<p>A survey of chain of thought reasoning: Advances, frontiers and future. Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, Ting Liu, abs/2309.154022023ArXiv preprint</p>
<p>Scaling instruction-finetuned language models. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, abs/2210.114162022ArXiv preprint</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, abs/2110.141682021ArXiv preprint</p>
<p>Educhat: A large-scale language model-based chatbot system for intelligent education. Yuhao Dan, Zhikai Lei, Yiyang Gu, Yong Li, Jianghao Yin, Jiaju Lin, Linhao Ye, Zhiyan Tie, Yougen Zhou, Yilei Wang, abs/2308.027732023ArXiv preprint</p>
<p>Jailbreaker: Automated jailbreak across multiple large language model chatbots. Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, Yang Liu, abs/2307.087152023ArXiv preprint</p>
<p>Active prompting with chain-of-thought for large language models. Shizhe Diao, Pengcheng Wang, Yong Lin, Tong Zhang, abs/2302.122462023ArXiv preprint</p>
<p>Vipergpt: Visual inference via python execution for reasoning. Surs Ddac, Sachit Menon, Carl Vondrick, abs/2303.081282023ArXiv preprint</p>
<p>Palm-e: An embodied multimodal language model. Danny Driess, Fei Xia, S M Mehdi, Corey Sajjadi, Aakanksha Lynch, Brian Chowdhery, Ayzaan Ichter, Jonathan Wahid, Quan Tompson, Tianhe Vuong, Yu, abs/2303.033782023ArXiv preprint</p>
<p>Improving factuality and reasoning in language models through multiagent debate. Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, Igor Mordatch, abs/2305.143252023ArXiv preprint</p>
<p>Misusing tools in large language models with visual adversarial examples. Xiaohan Fu, Zihan Wang, Shuheng Li, Rajesh K Gupta, Niloofar Mireshghallah, Taylor Berg-Kirkpatrick, Earlence Fernandes, 2023</p>
<p>Openagi: When llm meets domain experts. Yingqiang Ge, Wenyue Hua, Jianchao Ji, Juntao Tan, Shuyuan Xu, Yongfeng Zhang, abs/2304.043702023ArXiv preprint</p>
<p>Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant, 10.1162/tacl_a_00370Transactions of the Association for Computational Linguistics. 92021</p>
<p>Critic: Large language models can self-correct with tool-interactive critiquing. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, Weizhu Chen, abs/2305.11738ArXiv preprint. 2023a</p>
<p>Tora: A tool-integrated reasoning agent for mathematical problem solving. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen, 2023b</p>
<p>A realworld webagent with planning, long context understanding, and program synthesis. Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, Aleksandra Faust, abs/2307.128562023ArXiv preprint</p>
<p>Language models represent space and time. Wes Gurnee, Max Tegmark, 2023</p>
<p>Exploring human-like translation strategy with large language models. Zhiwei He, Tian Liang, Wenxiang Jiao, Zhuosheng Zhang, Yujiu Yang, Rui Wang, Zhaopeng Tu, Shuming Shi, Xing Wang, abs/2305.041182023ArXiv preprint</p>
<p>Is there an intelligent agent in your future?. James Hendler, Nature. 111999</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. AustriaMay 3-7, 2021OpenReview.net, 2021a</p>
<p>Measuring mathematical problem solving with the MATH dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021. Joaquin Vanschoren, Sai-Kit Yeung, the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021December 2021, virtual, 2021b</p>
<p>Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka, Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, and Chenglin Wu. Metagpt: Meta programming for multi-agent collaborative framework. 2023</p>
<p>Chatdb: Augmenting llms with databases as their symbolic memory. Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, Hang Zhao, abs/2306.039012023ArXiv preprint</p>
<p>Towards reasoning in large language models: A survey. Jie Huang, Kevin Chen, -Chuan Chang, 10.18653/v1/2023.findings-acl.67Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Large language models cannot self-correct reasoning yet. Jie Huang, Xinyun Chen, Swaroop Mishra, Steven Huaixiu, Adams Wei Zheng, Xinying Yu, Denny Song, Zhou, abs/2310.017982023aArXiv preprint</p>
<p>Language is not all you need: Aligning perception with language models. Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, Furu Wei, abs/2302.140452023bArXiv preprint</p>
<p>Transformer-patcher: One mistake worth one neuron. Zeyu Huang, Yikang Shen, Xiaofeng Zhang, Jie Zhou, Wenge Rong, Zhang Xiong, arXiv:2301.097852023carXiv preprint</p>
<p>Vima: General robot manipulation with multimodal prompts. Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, Linxi Fan, abs/2210.030942022ArXiv preprint</p>
<p>Chinmay Hegde, and Adarsh Krishnamurthy. Towards foundational ai models for additive manufacturing: Language models for g-code debugging, manipulation, and comprehension. Anushrut Jignasu, Kelly Marshall, Baskar Ganapathysubramanian, Aditya Balu, abs/2309.024652023ArXiv preprint</p>
<p>Chatmof: An autonomous ai system for predicting and generating metal-organic frameworks. Yeonghun Kang, Jihan Kim, abs/2308.01423ArXiv preprint. 2023</p>
<p>Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp. Omar Khattab, Keshav Santhanam, Lisa Xiang, David Li, Percy Hall, Christopher Liang, Matei Potts, Zaharia, abs/2212.140242022ArXiv preprint</p>
<p>Language models can solve computer tasks. Geunwoo Kim, Pierre Baldi, Stephen Mcaleer, abs/2303.174912023aArXiv preprint</p>
<p>Propile: Probing privacy leakage in large language models. Siwon Kim, Sangdoo Yun, Hwaran Lee, Martin Gubri, Sungroh Yoon, Seong Joon Oh, abs/2307.018812023bArXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, ( Shixiang, Machel Shane) Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in Neural Information Processing Systems. 202335</p>
<p>Recursion of thought: A divide-and-conquer approach to multi-context reasoning with language models. Soochan Lee, Gunhee Kim, 10.18653/v1/2023.findings-acl.40Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Camel: Communicative agents for" mind" exploration of large scale language model society. Guohao Li, Hasan Abed, Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, Bernard Ghanem, abs/2303.177602023aArXiv preprint</p>
<p>Textbind: Multi-turn interleaved multimodal instruction-following. Huayang Li, Siheng Li, Deng Cai, Longyue Wang, Lemao Liu, Taro Watanabe, Yujiu Yang, Shuming Shi, abs/2309.08637ArXiv preprint. 2023b</p>
<p>Self-prompting large language models for open-domain qa. Junlong Li, Zhuosheng Zhang, Hai Zhao, abs/2212.08635ArXiv preprint. 2022</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, The Fortieth International Conference on Machine Learning. 2023c</p>
<p>Api-bank: A benchmark for tool-augmented llms. Minghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li, Fei Huang, Yongbin Li, abs/2304.08244ArXiv preprint. 2023d</p>
<p>Metaagents: Simulating interactions of human behaviors for llm-based task-oriented coordination via collaborative generative agents. Yuan Li, Yixuan Zhang, Lichao Sun, abs/2310.065002023eArXiv preprint</p>
<p>Prompting large language models for zero-shot domain adaptation in speech recognition. Yuang Li, Yu Wu, Jinyu Li, Shujie Liu, abs/2306.160072023fArXiv preprint</p>
<p>Encouraging divergent thinking in large language models through multi-agent debate. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, Shuming Shi, abs/2305.19118. 2023ArXiv preprint</p>
<p>Let's verify step by step. Vineet Hunter Lightman, Yura Kosaraju, Harrison Burda, John Edwards ; Leike, Ilya Schulman, Karl Sutskever, Cobbe, Jan. abs/2305.20050. 2023Bowen Baker, Teddy LeeArXiv preprint</p>
<p>Agentsims: An open-source sandbox for large language model evaluation. Jiaju Lin, Haoran Zhao, Aochi Zhang, Yiting Wu, Huqiuyue Ping, Qin Chen, abs/2308.040262023ArXiv preprint</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, 10.18653/v1/P17-1015Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics20171</p>
<p>Deductive verification of chain-of-thought reasoning. Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, Hao Su, abs/2306.038722023ArXiv preprint</p>
<p>Improved baselines with visual instruction tuning. Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee, abs/2310.037442023aArXiv preprint</p>
<p>Visual instruction tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, abs/2304.084852023bArXiv preprint</p>
<p>Agentbench: Evaluating llms as agents. Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, abs/2308.036882023cArXiv preprint</p>
<p>Prompt injection attack against llm-integrated applications. Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng, Yang Liu, abs/2306.05499ArXiv preprint. 2023d</p>
<p>Controlllm: Augment language models with tools by searching on graphs. Zhaoyang Liu, Zeqiang Lai, Zhangwei Gao, Erfei Cui, Xizhou Zhu, Lewei Lu, Qifeng Chen, Yu Qiao, Jifeng Dai, Wenhai Wang, abs/2310.177962023eArXiv preprint</p>
<p>A survey of deep learning for mathematical reasoning. Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, Kai-Wei Chang, 10.18653/v1/2023.acl-long.817Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>A unified approach to interpreting model predictions. M Scott, Su-In Lundberg, Lee, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017. Isabelle Guyon, Samy Ulrike Von Luxburg, Hanna M Bengio, Rob Wallach, S V N Fergus, Roman Vishwanathan, Garnett, Long Beach, CA, USADecember 4-9, 2017. 2017</p>
<p>The role of verbal and performance intelligence in children's strategy selection and execution. Koen Luwel, Ageliki Foustana, Patrick Onghena, Lieven Verschaffel, Learning and Individual Differences. 242013</p>
<p>Understanding the benefits and challenges of using large language modelbased conversational agents for mental well-being support. Zilin Ma, Yiyang Mei, Zhaoyuan Su, abs/2307.158102023ArXiv preprint</p>
<p>Agents that reduce work and information overload. Pattie Maes, Readings in human-computer interaction. Elsevier1995</p>
<p>Teaching small language models to reason. Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, Aliaksei Severyn, abs/2212.084102022ArXiv preprint</p>
<p>Roco: Dialectic multi-robot collaboration with large language models. Zhao Mandi, Shreeya Jain, Shuran Song, abs/2307.047382023ArXiv preprint</p>
<p>Shengyu Mao, Ningyu Zhang, Xiaohan Wang, Mengru Wang, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen, arXiv:2310.02168Editing personality for llms. 2023arXiv preprint</p>
<p>Improving grounded language understanding in a collaborative environment by interacting with agents through help feedback. Nikhil Mehta, Milagro Teruel, Patricio Figueroa Sanz, Xin Deng, Ahmed Hassan Awadallah, Julia Kiseleva, abs/2304.107502023ArXiv preprint</p>
<p>Locating and editing factual associations in GPT. Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov, Advances in Neural Information Processing Systems. 2022a35</p>
<p>Mass editing memory in a transformer. Kevin Meng, Sen Arnab, Alex Sharma, Yonatan Andonian, David Belinkov, Bau, arXiv:2210.072292022barXiv preprint</p>
<p>Human-level control through deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, nature. 51875402015</p>
<p>Anymal: An efficient and scalable any-modality augmented language model. Seungwhan Moon, Andrea Madotto, Zhaojiang Lin, Tushar Nagarajan, Matt Smith, Shashank Jain, Chun-Fu Yeh, Prakash Murugesan, Peyman Heidari, Yue Liu, Kavya Srinet, Babak Damavandi, Anuj Kumar, 2023</p>
<p>Advanced multimodal machine learning. Louis-Philippe Morency, Amir Zadeh, Paul Liang, 2022</p>
<p>. Yohei Nakajima, Babyagi, 2023</p>
<p>Webgpt: Browser-assisted question-answering with human feedback. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, abs/2112.093322021ArXiv preprint</p>
<p>Lever: Learning to verify language-to-code generation with execution. Ansong Ni, Srini Iyer, Dragomir Radev, Ves Stoyanov, Wen-Tau Yih, Sida I Wang, Xi Victoria, Lin , Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine Learning2023</p>
<p>Skeleton-of-thought: Large language models can do parallel decoding. Xuefei Ning, Zinan Lin, Zixuan Zhou, Huazhong Yang, Yu Wang, abs/2307.153372023ArXiv preprint</p>
<p>Show your work: Scratchpads for intermediate computation with language models. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, Augustus Odena, Deep Learning for Code Workshop. 2022</p>
<p>Demystifying gpt self-repair for code generation. Jeevana Theo X Olausson, Chenglong Priya Inala, Jianfeng Wang, Armando Gao, Solar-Lezama, abs/2306.098962023ArXiv preprint</p>
<p>Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, William Yang, Wang , abs/2308.03188ArXiv preprint. 2023</p>
<p>Generative agents: Interactive simulacra of human behavior. Sung Joon, Park, C Joseph, Carrie J O'brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, abs/2304.03442ArXiv preprint. 2023</p>
<p>Are NLP models really able to solve simple math word problems?. Arkil Patel, Satwik Bhattamishra, Navin Goyal, 10.18653/v1/2021.naacl-main.168Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational Linguistics2021</p>
<p>Gorilla: Large language model connected with massive apis. G Shishir, Tianjun Patil, Xin Zhang, Joseph E Wang, Gonzalez, abs/2305.153342023ArXiv preprint</p>
<p>Why think step-by-step? reasoning emerges from the locality of experience. Ben Prystawski, Noah D Goodman, abs/2304.038432023ArXiv preprint</p>
<p>Communicative agents for software development. Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, Maosong Sun, abs/2307.079242023ArXiv preprint</p>
<p>Reasoning with language model prompting: A survey. Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Huajun Chen, 10.18653/v1/2023.acl-long.294Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Is chatgpt a general-purpose natural language processing task solver?. Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, Diyi Yang, ArXiv preprint, abs/2302.064762023a</p>
<p>Toolllm: Facilitating large language models to master 16000+ real-world apis. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, abs/2307.167892023bArXiv preprint</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, Proceedings of the 38th International Conference on Machine Learning, ICML 2021. Marina Meila, Tong Zhang, the 38th International Conference on Machine Learning, ICML 2021PMLR18-24 July 2021. 2021139of Proceedings of Machine Learning Research</p>
<p>Verbal and non-verbal intelligence changes in the teenage brain. Sue Ramsden, Fiona M Richardson, Goulven Josse, S C Michael, Caroline Thomas, Clare Ellis, Mohamed L Shakeshaft, Cathy J Seghier, Price, Nature. 47973712011</p>
<p>Android in the wild: A large-scale dataset for android device control. Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, Timothy Lillicrap, abs/2307.100882023. 2023ArXiv preprint</p>
<p>Visual chain of thought: Bridging logical gaps with multimodal infillings. Toran Bruce, Richards ; Daniel Rose, Vaishnavi Himakunthala, Andy Ouyang, Ryan He, Alex Mei, Yujie Lu, Michael Saxon, Chinmay Sonar, Diba Mirza, William Yang, Wang , abs/2305.02317Auto-gpt: An autonomous gpt-4 experiment. 2023. 2023ArXiv preprint</p>
<p>Tptu: Task planning and tool usage of large language model-based ai agents. Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing Du, Shiwei Shi, Hangyu Mao, Xingyu Zeng, Rui Zhao, abs/2308.034272023aArXiv preprint</p>
<p>Identifying the risks of lm agents with an lm-emulated sandbox. Yangjun Ruan, Honghua Dong, Andrew Wang, Silviu Pitis, Yongchao Zhou, Jimmy Ba, Yann Dubois, Chris J Maddison, Tatsunori Hashimoto, abs/2309.158172023bArXiv preprint</p>
<p>Language modelling with pixels. Phillip Rust, Jonas F Lotz, Emanuele Bugliarello, Elizabeth Salesky, Miryam De Lhoneux, Desmond Elliott, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Wechsler adult intelligence scale-iii. J Joseph, Shane J Ryan, Lopez, Understanding psychological assessment. Springer2001</p>
<p>Evaluating language-model agents on realistic autonomous tasks. Megan Kinniment, Lucas Jun, Koba Sato, Haoxing Du, Brian Goodrich, Max Hasin, Lawrence Chan, Luke Harold Miles, Hjalmar Tao R Lin, Joel Wijk, Aaron Burget, Ho, 2023</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dess, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, abs/2302.047612023ArXiv preprint</p>
<p>Speech acts: An essay in the philosophy of language. John R Searle, 1969Cambridge university press626</p>
<p>Navigation with large language models: Semantic guesswork as a heuristic for planning. Dhruv Shah, Michael Equi, Blazej Osinski, Fei Xia, Brian Ichter, Sergey Levine, abs/2310.101032023ArXiv preprint</p>
<p>On second thought, let's not think step by step! bias and toxicity in zero-shot reasoning. Omar Shaikh, Hongxin Zhang, William Held, Michael Bernstein, Diyi Yang, arXiv:2212.080612022arXiv preprint</p>
<p>Character-llm: A trainable agent for role-playing. Yunfan Shao, Linyang Li, Junqi Dai, Xipeng Qiu, abs/2310.101582023ArXiv preprint</p>
<p>Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang, abs/2303.175802023ArXiv preprint</p>
<p>Language models are multilingual chain-of-thought reasoners. Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, Jason Wei, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, RwandaMay 1-5, 2023. 2023OpenReview.net</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, 2023</p>
<p>Automatic prompt augmentation and selection with chain-of-thought from labeled data. Kashun Shum, Shizhe Diao, Tong Zhang, abs/2302.128222023ArXiv preprint</p>
<p>Large language models encode clinical knowledge. Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Kumar Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly, Nathaneal Schrli, Aakanksha Chowdhery, ; R Webster, Gregory S Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, abs/2212.13138Nenad Tomasev, Yun Liu, Alvin Rajkomar, Joelle K. Barral, Christopher Semturs, Alan Karthikesalingam, and Vivek Natarajan2022Philip Andrew Mansfield, Blaise Agera y Arcas, DaleArXiv preprint</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adri Gupta, Garriga-Alonso, abs/2206.046152022ArXiv preprint</p>
<p>Gpt-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems. Kaya Stechly, Matthew Marquez, Subbarao Kambhampati, abs/2310.123972023ArXiv preprint</p>
<p>The nature of verbal comprehension. Robert J Sternberg, Janet S Powell, Daniel B Kaye, org/10.1016/0304-422X(82)90031-6Poetics. 0304-422X1121982</p>
<p>Cognitive architectures for language agents. Theodore Sumers, Shunyu Yao, Karthik Narasimhan, Thomas L Griffiths, abs/2309.02427ArXiv preprint. 2023</p>
<p>Towards better semantic understanding of mobile interfaces. Srinivas Sunkara, Maria Wang, Lijuan Liu, Gilles Baechler, Yu-Chung Hsiao, Jindong Chen, Abhanshu Sharma, James W W Stout, Proceedings of the 29th International Conference on Computational Linguistics. the 29th International Conference on Computational LinguisticsGyeongju, Republic of Korea2022International Committee on Computational Linguistics</p>
<p>CommonsenseQA: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, 10.18653/v1/N19-1421Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Ul2: Unifying language learning paradigms. Yi Tay, Mostafa Dehghani, Xavier Vinh Q Tran, Jason Garcia, Xuezhi Wei, Hyung Won Wang, Dara Chung, Tal Bahri, Steven Schuster, Zheng, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Xagent: An autonomous agent for complex task solving. Xagent Team, 2023</p>
<p>Interleaving retrieval with chainof-thought reasoning for knowledge-intensive multi-step questions. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, Ashish Sabharwal, abs/2212.105092022ArXiv preprint</p>
<p>Can large language models really improve by selfcritiquing their own plans? ArXiv preprint. Karthik Valmeekam, Matthew Marquez, Subbarao Kambhampati, abs/2310.081182023</p>
<p>Towards understanding chain-of-thought prompting: An empirical study of what matters. Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, Huan Sun, 10.18653/v1/2023.acl-long.153Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023a1</p>
<p>Survey on factuality in large language models: Knowledge, retrieval and domain-specificity. Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, Xiangru Tang, Tianhang Zhang, Cheng Jiayang, Yunzhi Yao, Wenyang Gao, Xuming Hu, Zehan Qi, abs/2310.075212023bArXiv preprint</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, abs/2305.162912023cArXiv preprint</p>
<p>Plan-andsolve prompting: Improving zero-shot chain-of-thought reasoning by large language models. Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy , Ka-Wei Lee, Ee-Peng Lim, 10.18653/v1/2023.acl-long.147Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. Anna Rogers, Jordan L Boyd-Graber, Naoaki Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 9-14, 2023. 2023d1ACL 2023</p>
<p>Lei Wang, Jingsen Zhang, Hao Yang, Zhiyuan Chen, Jiakai Tang, Zeyu Zhang, Xu Chen, Yankai Lin, Ruihua Song, Wayne Xin Zhao, Jun Xu, Zhicheng Dou, Jun Wang, Ji-Rong Wen, When large language model based agent meets user behavior analysis: A novel user simulation paradigm. 2023e</p>
<p>Reasoning ability emerges in large language models as aggregation of reasoning paths: A case study with knowledge graphs. Xinyi Wang, William Yang, Wang , Workshop on Efficient Systems for Foundation Models@ ICML2023. 2023</p>
<p>Rationale-augmented ensembles in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Denny Zhou, abs/2207.007472022ArXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, RwandaMay 1-5, 2023. 2023fOpenReview.net</p>
<p>Element-aware summarization with large language models: Expertaligned evaluation and chain-of-thought method. Yiming Wang, Zhuosheng Zhang, Rui Wang, 10.18653/v1/2023.acl-long.482Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023g1</p>
<p>Alexander Wei, Nika Haghtalab, Jacob Steinhardt, Jailbroken, arXiv:2307.02483How does llm safety training fail?. 2023aarXiv preprint</p>
<p>Emergent abilities of large language models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus, Transactions on Machine Learning Research. 2022Survey Certification</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 2023b35</p>
<p>Llm-powered autonomous agents. lilianweng.github.io. Lilian Weng, 2023</p>
<p>Large language models are reasoners with selfverification. Yixuan Weng, Minjun Zhu, Shizhu He, Kang Liu, Jun Zhao, abs/2212.095612022ArXiv preprint</p>
<p>Practical planning: extending the classical AI planning paradigm. David E Wilkins, 2014Elsevier</p>
<p>Intelligent agents: Theory and practice. Michael Wooldridge, Nicholas R Jennings, The knowledge engineering review. 1021995</p>
<p>Next-gpt: Any-to-any multimodal llm. Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, Tat-Seng Chua, abs/2309.055192023ArXiv preprint</p>
<p>The rise and potential of large language model based agents: A survey. Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, abs/2309.078642023ArXiv preprint</p>
<p>An explanation of in-context learning as implicit bayesian inference. Sang Michael Xie, Aditi Raghunathan, Percy Liang, Tengyu Ma, The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event. April 25-29, 2022. 2022OpenReview.net</p>
<p>Examining the inter-consistency of large language models: An in-depth analysis via debate. Kai Xiong, Xiao Ding, Yixin Cao, Ting Liu, Bing Qin, 2023a2305arXiv e-prints</p>
<p>Effective long-context scaling of foundation models. Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, abs/2309.160392023bArXiv preprint</p>
<p>Expertprompting: Instructing large language models to be distinguished experts. Benfeng Xu, An Yang, Junyang Lin, Quan Wang, Chang Zhou, Yongdong Zhang, Zhendong Mao, abs/2305.146882023aArXiv preprint</p>
<p>Rewoo: Decoupling reasoning from observations for efficient augmented language models. Binfeng Xu, Zhiyuan Peng, Bowen Lei, Subhabrata Mukherjee, Yuchen Liu, Dongkuan Xu, abs/2305.183232023bArXiv preprint</p>
<p>Sc-safety: A multi-round open-ended question adversarial safety benchmark for large language models in chinese. Liang Xu, Kangkang Zhao, Lei Zhu, Hang Xue, abs/2310.058182023cArXiv preprint</p>
<p>Lemur: Harmonizing natural language and code for language agents. Yiheng Xu, Hongjin Su, Chen Xing, Boyu Mi, Qian Liu, Weijia Shi, Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, abs/2310.068302023dArXiv preprint</p>
<p>Large language models as optimizers. Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, Xinyun Chen, abs/2309.034092023aArXiv preprint</p>
<p>Learning interactive real-world simulators. Mengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Dale Schuurmans, Pieter Abbeel, abs/2310.061142023bArXiv preprint</p>
<p>Learn to interpret atari agents. Zhao Yang, Song Bai, Li Zhang, Philip Hs, Torr , abs/1812.112762018ArXiv preprint</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Yuan Karthik R Narasimhan, Cao, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, abs/2305.106012023aArXiv preprint</p>
<p>Retroformer: Retrospective large language agents with policy gradient optimization. Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh Murthy, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, abs/2308.021512023bArXiv preprint</p>
<p>Beyond chain-of-thought, effective graph-of-thought reasoning in large language models. Yao Yao, Zuchao Li, Hai Zhao, abs/2305.165822023cArXiv preprint</p>
<p>Nature language reasoning, a survey. Fei Yu, Hongbo Zhang, Benyou Wang, abs/2303.147252023aArXiv preprint</p>
<p>Towards better chain-of-thought prompting strategies: A survey. Zihan Yu, Liang He, Zhen Wu, Xinyu Dai, Jiajun Chen, abs/2310.049592023bArXiv preprint</p>
<p>Craft: Customizing llms by creating and retrieving from specialized toolsets. Lifan Yuan, Yangyi Chen, Xingyao Wang, Yi Ren Fung, Hao Peng, Heng Ji, abs/2309.174282023ArXiv preprint</p>
<p>Mammoth: Building math generalist models through hybrid instruction tuning. Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, abs/2309.056532023ArXiv preprint</p>
<p>Llama-adapter: Efficient fine-tuning of language models with zero-init attention. Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, Yu Qiao, abs/2303.161992023aArXiv preprint</p>
<p>Screen recognition: Creating accessibility metadata for mobile applications from pixels. Xiaoyi Zhang, Lilian De Greef, Amanda Swearngin, Samuel White, Kyle Murray, Lisa Yu, Qi Shan, Jeffrey Nichols, Jason Wu, Chris Fleizach, Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. the 2021 CHI Conference on Human Factors in Computing Systems2021</p>
<p>Safetybench: Evaluating the safety of large language models with multiple choice questions. Zhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun, Yongkang Huang, Chong Long, Xiao Liu, Xuanyu Lei, Jie Tang, Minlie Huang, abs/2309.070452023bArXiv preprint</p>
<p>You only look at screens: Multimodal chain-of-action agents. Zhuosheng Zhang, Aston Zhang, abs/2309.114362023ArXiv preprint</p>
<p>Automatic chain of thought prompting in large language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, The Eleventh International Conference on Learning Representations (ICLR 2023). 2023c</p>
<p>Mmicl: Empowering vision-language model with multi-modal in-context learning. Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, Alex Smola, ; Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, Baobao Chang, abs/2309.07915ArXiv preprint. 2023d. 2023aArXiv preprintMultimodal chain-of-thought reasoning in language models</p>
<p>Verify-and-edit: A knowledge-enhanced chain-of-thought framework. Ruochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei Qin, Lidong Bing, 10.18653/v1/2023.acl-long.320Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. Anna Rogers, Jordan L Boyd-Graber, Naoaki Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 9-14, 2023. 2023b1ACL 2023</p>
<p>A survey of large language models. Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Dong, abs/2303.182232023cArXiv preprint</p>
<p>Large language models as commonsense knowledge for large-scale task planning. Zirui Zhao, Wee Sun Lee, David Hsu, ArXiv preprint, abs/2305.14078, 2023d. </p>
<p>Agieval: A human-centric benchmark for evaluating foundation models. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, Nan Duan, abs/2304.063642023ArXiv preprint</p>
<p>Language agent tree search unifies reasoning acting and planning in language models. Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, Yu-Xiong Wang, abs/2310.044062023aArXiv preprint</p>
<p>Solving challenging math word problems using GPT-4 code interpreter with code-based self-verification. Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, Hongsheng Li, abs/2308.079212023bArXiv preprint</p>
<p>Webarena: A realistic web environment for building autonomous agents. Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, abs/2307.138542023cArXiv preprint</p>
<p>Agents: An open-source framework for autonomous language agents. Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian Zhang, Jing Chen, Ruipu Wu, Shuai Wang, abs/2309.078702023dArXiv preprint</p>
<p>. Xuanhe Zhou, Guoliang Li, Zhiyuan Liu, abs/2308.054812023eLlm as dba. ArXiv preprint</p>
<p>Large language models are human-level prompt engineers. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, RwandaMay 1-5, 2023. 2023fOpenReview.net</p>
<p>Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory. Xizhou Zhu, Yuntao Chen, Chenxin Hao Tian, Weijie Tao, Chenyu Su, Gao Yang, Bin Huang, Lewei Li, Xiaogang Lu, Wang, abs/2305.171442023ArXiv preprint</p>
<p>Tab-cot: Zero-shot tabular chain of thought. Jin Ziqi, Wei Lu, Findings of the Association for Computational Linguistics: ACL 2023. 2023</p>
<p>Aurora: Augmented reasoning and refining with task-adaptive chain-ofthought prompting. Anni Zou, Zhuosheng Zhang, Hai Zhao, 2023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1579 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1579</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1579</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-30.html">extraction-schema-30</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-f8717dd893b150977971cadaf893615770e2a251</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f8717dd893b150977971cadaf893615770e2a251" target="_blank">Generalization in Text-based Games via Hierarchical Reinforcement Learning</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This paper introduces a hierarchical framework built upon the knowledge graph-based RL agent to conduct goal-conditioned reinforcement learning and shows that the proposed method enjoys favorable generalizability.</p>
                <p><strong>Paper Abstract:</strong> Deep reinforcement learning provides a promising approach for text-based games in studying natural language communication between humans and artificial agents. However, the generalization still remains a big challenge as the agents depend critically on the complexity and variety of training tasks. In this paper, we address this problem by introducing a hierarchical framework built upon the knowledge graph-based RL agent. In the high level, a meta-policy is executed to decompose the whole game into a set of subtasks specified by textual goals, and select one of them based on the KG. Then a sub-policy in the low level is executed to conduct goal-conditioned reinforcement learning. We carry out experiments on games with various difficulty levels and show that the proposed method enjoys favorable generalizability.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1579.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1579.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scheduled task sampling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scheduled task sampling (curriculum learning via level-adaptive sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curriculum-like multi-task sampling strategy that adapts the probability of sampling tasks/levels based on per-level training performance, encouraging focus on harder levels as training proceeds.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>H-KGA</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Hierarchical Knowledge Graph-based Agent: a two-level HRL agent using a KG-based observation; a meta-policy selects textual goals (subtasks) and a goal-conditioned sub-policy selects admissible text actions. Graph encoder (R-GCN) and text encoder (single-block transformer) are used; Double DQN with prioritized replay trains both policies.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld (cooking games, rl.0.1 derived sets)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A text-based interactive fiction environment (TextWorld) with cooking-themed games: at each timestep the agent receives textual (here: ground-truth KG) observations and selects text actions from an admissible action set; tasks vary by map layout (rooms), ingredients, and recipe requirements across multiple difficulty levels.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>household tasks (cooking procedures)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Prepare the meal: collect ingredients (e.g., 'find red/purple potato'), process ingredients (e.g., 'cook red potato'), and follow multi-step recipes requiring navigation across rooms and correct ordering of steps.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Complex tasks decompose into textual instruction-like subtasks (goals); solving a game is composed by sequencing goal-conditioned policies that complete available goals (hierarchical task structure).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>Scheduled task sampling (performance-adaptive level sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Maintain per-level training performance v_l; compute sampling probability p_l = softmax(beta - v_l) so levels with lower performance (harder for the agent) receive higher sampling probability; each episode samples a level according to p_l then a game from that level. This encourages allocating more training episodes to harder levels as training proceeds.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>task difficulty (operationalized as current training performance per level; lower performance → higher sampling frequency)</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Levels range from single-room, single-ingredient/simple recipes (S1/US1) to multi-room, multi-ingredient, multi-step recipes (S4/US4) — roughly from 1-step/1-ingredient tasks to multi-step 3-ingredient cooking procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td>H-KGA (with scheduled task sampling + level-aware replay + BeBold exploration) achieves normalized scores: Avg Seen = 0.72 ± 0.04, Avg Unseen = 0.79 ± 0.04, Avg All = 0.76 ± 0.03 (Table 2). Performance improvements are largest on difficult levels (e.g., S4/US4; paper reports marked gains there).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td>Removing scheduled task sampling (H-KGA w/o Sch, i.e., no scheduled sampling but keeping other components) yields: Avg Seen = 0.52 ± 0.07, Avg Unseen = 0.63 ± 0.07, Avg All = 0.57 ± 0.05; removing both scheduled sampling and level-aware replay (H-KGA w/o Sch w/o LR) yields Avg Seen = 0.63 ± 0.14, Avg Unseen = 0.63 ± 0.16, Avg All = 0.63 ± 0.15 (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td>Ablation comparisons in the paper: (1) H-KGA (full) vs H-KGA w/o Sch (scheduled sampling removed) — substantial drop in performance on seen and especially hard levels; (2) H-KGA w/o Sch w/o LR (both scheduled sampling and level-aware replay removed) — different degradation pattern, indicating interactions between sampling and replay strategies. No other curriculum ordering strategies (e.g., strictly increasing difficulty or prerequisite-based curricula) were evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td>Paper reports improved generalization to unseen difficulty levels: H-KGA Avg Unseen = 0.79 ± 0.04 (versus GATA baseline Avg Unseen = 0.62 ± 0.07). The curriculum strategy contributes to better performance on unseen harder levels (noted particularly on complex levels S4/US4).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Adapting sampling probabilities to focus training on levels where agent performance is low (scheduled task sampling) improves learning on difficult levels and overall generalization; scheduled sampling is especially important for hard multi-room, multi-ingredient cooking tasks. The paper also finds that curriculum alone is insufficient without other components (e.g., exploration bonus BeBold), and that improper training schedules can cause forgetting or sample inefficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generalization in Text-based Games via Hierarchical Reinforcement Learning', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1579.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1579.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Level-aware replay buffer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Level-aware replay buffer (level-conditioned caching for multi-task replay)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A replay-buffer strategy that tracks per-level average rewards and only adds cached episode transitions to the level-specific replay buffer if they exceed a level-relative threshold, preventing hard-level transitions from being systematically excluded due to lower absolute rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>H-KGA</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Hierarchical Knowledge Graph-based Agent: a two-level HRL agent using a KG-based observation; a meta-policy selects textual goals and a goal-conditioned sub-policy selects admissible text actions; trained with Double DQN and prioritized replay.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld (cooking games, rl.0.1 derived sets)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Text-based cooking games where interactions are text commands chosen from an admissible set and observations are represented as knowledge-graph triplets (ground-truth KG used in experiments); tasks vary by rooms, ingredients, and recipe complexity across multiple levels.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>household tasks (cooking procedures)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Collect and prepare ingredients (e.g., 'find knife', 'find red potato', 'cook red potato') and execute multi-step recipes across rooms.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Tasks are hierarchical and compositional: games are composed of sequences of available goals/subtasks (textual instructions) that must be completed in order to finish the overall recipe.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>Level-aware replay buffer (complementary MTL strategy)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>When moving episode caches into the replay buffer, store each transition with its level identifier and compare average rewards within the same level; only push cached episodes into the replay buffer if their average return exceeds a threshold relative to that level (cache_avg > τ * buffer_level_avg). This prevents transitions from hard levels (which naturally have lower absolute returns) being rejected compared to easy-level transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>level-awareness / fairness across difficulty levels (ensures representation of hard tasks in replay independent of absolute reward scale)</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Covers same levels as scheduled sampling: from single-room single-ingredient to multi-room multi-ingredient recipes (1- to multi-step cooking procedures across levels S1..S4 and US1..US4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td>With level-aware replay (together with scheduled sampling and BeBold), H-KGA Avg All = 0.76 ± 0.03 (Table 2). The full MTL strategies produce higher sample efficiency and better performance on difficult levels.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td>Removing level-aware replay in addition to scheduled sampling (H-KGA w/o Sch w/o LR) yields Avg All = 0.63 ± 0.15; removing only scheduled sampling (but keeping level-aware replay) gives Avg All = 0.57 ± 0.05, indicating level-aware replay interacts with scheduled sampling to affect final performance.</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td>The paper reports ablations: (a) keep level-aware replay but remove scheduled sampling (H-KGA w/o Sch) — worse performance; (b) remove both (H-KGA w/o Sch w/o LR) — performance differs (Avg All = 0.63) showing level-aware replay helps maintain presence of hard-level transitions and improves training effectiveness when used with scheduled sampling. No standalone alternative replay strategies were evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td>Level-aware replay helps maintain learning signals from hard levels and, combined with scheduled sampling, supports transfer/generalization to unseen harder levels (H-KGA Avg Unseen = 0.79 ± 0.04 vs baselines); ablations removing these strategies degrade unseen-level performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Making the replay buffer level-aware prevents hard-level experiences from being discarded due to lower absolute rewards and improves sample efficiency for multi-level training; it complements scheduled task sampling to ensure the agent receives both more episodes from hard levels and retains those episodes for learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generalization in Text-based Games via Hierarchical Reinforcement Learning', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Curriculum learning <em>(Rating: 2)</em></li>
                <li>Curriculum-guided hindsight experience replay <em>(Rating: 2)</em></li>
                <li>TextWorld: A learning environment for text-based games <em>(Rating: 2)</em></li>
                <li>Language as an abstraction for hierarchical deep reinforcement learning <em>(Rating: 1)</em></li>
                <li>Learning dynamic belief graphs to generalize on text-based games <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1579",
    "paper_id": "paper-f8717dd893b150977971cadaf893615770e2a251",
    "extraction_schema_id": "extraction-schema-30",
    "extracted_data": [
        {
            "name_short": "Scheduled task sampling",
            "name_full": "Scheduled task sampling (curriculum learning via level-adaptive sampling)",
            "brief_description": "A curriculum-like multi-task sampling strategy that adapts the probability of sampling tasks/levels based on per-level training performance, encouraging focus on harder levels as training proceeds.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "H-KGA",
            "agent_description": "Hierarchical Knowledge Graph-based Agent: a two-level HRL agent using a KG-based observation; a meta-policy selects textual goals (subtasks) and a goal-conditioned sub-policy selects admissible text actions. Graph encoder (R-GCN) and text encoder (single-block transformer) are used; Double DQN with prioritized replay trains both policies.",
            "agent_size": null,
            "environment_name": "TextWorld (cooking games, rl.0.1 derived sets)",
            "environment_description": "A text-based interactive fiction environment (TextWorld) with cooking-themed games: at each timestep the agent receives textual (here: ground-truth KG) observations and selects text actions from an admissible action set; tasks vary by map layout (rooms), ingredients, and recipe requirements across multiple difficulty levels.",
            "procedure_type": "household tasks (cooking procedures)",
            "procedure_examples": "Prepare the meal: collect ingredients (e.g., 'find red/purple potato'), process ingredients (e.g., 'cook red potato'), and follow multi-step recipes requiring navigation across rooms and correct ordering of steps.",
            "compositional_structure": "Complex tasks decompose into textual instruction-like subtasks (goals); solving a game is composed by sequencing goal-conditioned policies that complete available goals (hierarchical task structure).",
            "uses_curriculum": true,
            "curriculum_name": "Scheduled task sampling (performance-adaptive level sampling)",
            "curriculum_description": "Maintain per-level training performance v_l; compute sampling probability p_l = softmax(beta - v_l) so levels with lower performance (harder for the agent) receive higher sampling probability; each episode samples a level according to p_l then a game from that level. This encourages allocating more training episodes to harder levels as training proceeds.",
            "curriculum_ordering_principle": "task difficulty (operationalized as current training performance per level; lower performance → higher sampling frequency)",
            "task_complexity_range": "Levels range from single-room, single-ingredient/simple recipes (S1/US1) to multi-room, multi-ingredient, multi-step recipes (S4/US4) — roughly from 1-step/1-ingredient tasks to multi-step 3-ingredient cooking procedures.",
            "performance_with_curriculum": "H-KGA (with scheduled task sampling + level-aware replay + BeBold exploration) achieves normalized scores: Avg Seen = 0.72 ± 0.04, Avg Unseen = 0.79 ± 0.04, Avg All = 0.76 ± 0.03 (Table 2). Performance improvements are largest on difficult levels (e.g., S4/US4; paper reports marked gains there).",
            "performance_without_curriculum": "Removing scheduled task sampling (H-KGA w/o Sch, i.e., no scheduled sampling but keeping other components) yields: Avg Seen = 0.52 ± 0.07, Avg Unseen = 0.63 ± 0.07, Avg All = 0.57 ± 0.05; removing both scheduled sampling and level-aware replay (H-KGA w/o Sch w/o LR) yields Avg Seen = 0.63 ± 0.14, Avg Unseen = 0.63 ± 0.16, Avg All = 0.63 ± 0.15 (Table 2).",
            "has_curriculum_comparison": true,
            "alternative_curriculum_performance": "Ablation comparisons in the paper: (1) H-KGA (full) vs H-KGA w/o Sch (scheduled sampling removed) — substantial drop in performance on seen and especially hard levels; (2) H-KGA w/o Sch w/o LR (both scheduled sampling and level-aware replay removed) — different degradation pattern, indicating interactions between sampling and replay strategies. No other curriculum ordering strategies (e.g., strictly increasing difficulty or prerequisite-based curricula) were evaluated.",
            "transfer_generalization": "Paper reports improved generalization to unseen difficulty levels: H-KGA Avg Unseen = 0.79 ± 0.04 (versus GATA baseline Avg Unseen = 0.62 ± 0.07). The curriculum strategy contributes to better performance on unseen harder levels (noted particularly on complex levels S4/US4).",
            "key_findings": "Adapting sampling probabilities to focus training on levels where agent performance is low (scheduled task sampling) improves learning on difficult levels and overall generalization; scheduled sampling is especially important for hard multi-room, multi-ingredient cooking tasks. The paper also finds that curriculum alone is insufficient without other components (e.g., exploration bonus BeBold), and that improper training schedules can cause forgetting or sample inefficiency.",
            "uuid": "e1579.0",
            "source_info": {
                "paper_title": "Generalization in Text-based Games via Hierarchical Reinforcement Learning",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "Level-aware replay buffer",
            "name_full": "Level-aware replay buffer (level-conditioned caching for multi-task replay)",
            "brief_description": "A replay-buffer strategy that tracks per-level average rewards and only adds cached episode transitions to the level-specific replay buffer if they exceed a level-relative threshold, preventing hard-level transitions from being systematically excluded due to lower absolute rewards.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "H-KGA",
            "agent_description": "Hierarchical Knowledge Graph-based Agent: a two-level HRL agent using a KG-based observation; a meta-policy selects textual goals and a goal-conditioned sub-policy selects admissible text actions; trained with Double DQN and prioritized replay.",
            "agent_size": null,
            "environment_name": "TextWorld (cooking games, rl.0.1 derived sets)",
            "environment_description": "Text-based cooking games where interactions are text commands chosen from an admissible set and observations are represented as knowledge-graph triplets (ground-truth KG used in experiments); tasks vary by rooms, ingredients, and recipe complexity across multiple levels.",
            "procedure_type": "household tasks (cooking procedures)",
            "procedure_examples": "Collect and prepare ingredients (e.g., 'find knife', 'find red potato', 'cook red potato') and execute multi-step recipes across rooms.",
            "compositional_structure": "Tasks are hierarchical and compositional: games are composed of sequences of available goals/subtasks (textual instructions) that must be completed in order to finish the overall recipe.",
            "uses_curriculum": true,
            "curriculum_name": "Level-aware replay buffer (complementary MTL strategy)",
            "curriculum_description": "When moving episode caches into the replay buffer, store each transition with its level identifier and compare average rewards within the same level; only push cached episodes into the replay buffer if their average return exceeds a threshold relative to that level (cache_avg &gt; τ * buffer_level_avg). This prevents transitions from hard levels (which naturally have lower absolute returns) being rejected compared to easy-level transitions.",
            "curriculum_ordering_principle": "level-awareness / fairness across difficulty levels (ensures representation of hard tasks in replay independent of absolute reward scale)",
            "task_complexity_range": "Covers same levels as scheduled sampling: from single-room single-ingredient to multi-room multi-ingredient recipes (1- to multi-step cooking procedures across levels S1..S4 and US1..US4).",
            "performance_with_curriculum": "With level-aware replay (together with scheduled sampling and BeBold), H-KGA Avg All = 0.76 ± 0.03 (Table 2). The full MTL strategies produce higher sample efficiency and better performance on difficult levels.",
            "performance_without_curriculum": "Removing level-aware replay in addition to scheduled sampling (H-KGA w/o Sch w/o LR) yields Avg All = 0.63 ± 0.15; removing only scheduled sampling (but keeping level-aware replay) gives Avg All = 0.57 ± 0.05, indicating level-aware replay interacts with scheduled sampling to affect final performance.",
            "has_curriculum_comparison": true,
            "alternative_curriculum_performance": "The paper reports ablations: (a) keep level-aware replay but remove scheduled sampling (H-KGA w/o Sch) — worse performance; (b) remove both (H-KGA w/o Sch w/o LR) — performance differs (Avg All = 0.63) showing level-aware replay helps maintain presence of hard-level transitions and improves training effectiveness when used with scheduled sampling. No standalone alternative replay strategies were evaluated.",
            "transfer_generalization": "Level-aware replay helps maintain learning signals from hard levels and, combined with scheduled sampling, supports transfer/generalization to unseen harder levels (H-KGA Avg Unseen = 0.79 ± 0.04 vs baselines); ablations removing these strategies degrade unseen-level performance.",
            "key_findings": "Making the replay buffer level-aware prevents hard-level experiences from being discarded due to lower absolute rewards and improves sample efficiency for multi-level training; it complements scheduled task sampling to ensure the agent receives both more episodes from hard levels and retains those episodes for learning.",
            "uuid": "e1579.1",
            "source_info": {
                "paper_title": "Generalization in Text-based Games via Hierarchical Reinforcement Learning",
                "publication_date_yy_mm": "2021-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Curriculum learning",
            "rating": 2
        },
        {
            "paper_title": "Curriculum-guided hindsight experience replay",
            "rating": 2
        },
        {
            "paper_title": "TextWorld: A learning environment for text-based games",
            "rating": 2
        },
        {
            "paper_title": "Language as an abstraction for hierarchical deep reinforcement learning",
            "rating": 1
        },
        {
            "paper_title": "Learning dynamic belief graphs to generalize on text-based games",
            "rating": 1
        }
    ],
    "cost": 0.011992,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Generalization in Text-based Games via Hierarchical Reinforcement Learning</h1>
<p>Yunqiu Xu ${ }^{1}$, Meng Fang ${ }^{2}$, Ling Chen ${ }^{1}$, Yali Du ${ }^{3}$, Chengqi Zhang ${ }^{1}$<br>${ }^{1}$ Australian Artificial Intelligence Institute, University of Technology Sydney, Sydney, Australia<br>Yunqiu.Xu@student.uts.edu.au, {Ling.Chen, Chengqi.Zhang}@uts.edu.au<br>${ }^{2}$ Eindhoven University of Technology, Eindhoven, the Netherlands<br>m.fang@tue.nl<br>${ }^{3}$ King's College London, London, United Kingdom<br>yali.du@kcl.ac.uk</p>
<h4>Abstract</h4>
<p>Deep reinforcement learning provides a promising approach for text-based games in studying natural language communication between humans and artificial agents. However, the generalization still remains a big challenge as the agents depend critically on the complexity and variety of training tasks. In this paper, we address this problem by introducing a hierarchical framework built upon the knowledge graph-based RL agent. In the high level, a meta-policy is executed to decompose the whole game into a set of subtasks specified by textual goals, and select one of them based on the KG. Then a subpolicy in the low level is executed to conduct goal-conditioned reinforcement learning. We carry out experiments on games with various difficulty levels and show that the proposed method enjoys favorable generalizability.</p>
<h2>1 Introduction</h2>
<p>Text-based games are simulated systems where the agent takes textual observations as the input, and interacts with the environment via text commands (Hausknecht et al., 2020). They are suitable test-beds to study natural language understanding, commonsense reasoning and language-informed decision making (Luketina et al., 2019). Reinforcement Learning (RL) based agents (Narasimhan et al., 2015; Zahavy et al., 2018) have been developed to handle challenges such as languagebased representation learning and combinatorial action space. Among them, KG-based agents (Ammanabrolu and Hausknecht, 2020) yield promising performance with the aid of Knowledge Graph (KG), which serves as a belief state to provide structural information.</p>
<p>To design intelligent RL-based agents for textbased games, it is necessary to build agents that automatically learn to solve different games. However, generalization remains as one of the key challenges of RL - the agent tends to overfit the train-
ing environment and fails to generalize to new environments (Cobbe et al., 2019). In the domain of text-based games, the TextWorld (Côté et al., 2018) makes it feasible to study generalizability by creating non-overlapping game sets with customizable domain gaps (e.g., themes, vocabulary sets, difficulty levels and layouts). Most previous works study generalizability either upon games with the same difficulty level but different layouts (Ammanabrolu and Riedl, 2019a), or upon games with a set of multiple levels that have been observed during training (Adolphs and Hofmann, 2020). Although these agents perform well on relatively simple games, they can hardly achieve satisfactory performance on difficult games (Adhikari et al., 2020). In this work, we aim to develop agents that can be generalized to not only games from the same difficulty level while having unseen different layouts, but also games from unseen difficulty levels where both layouts and complexities are different.</p>
<p>While solving a whole game might be difficult due to long-term temporal dependencies, and the learnt strategy might be difficult to be transferred to other games due to large domain gaps, it would be more flexible to treat the game as a sketch of subtasks (Andreas et al., 2017; Oh et al., 2017). This brings two branches of benefits. First, the subtasks would be easier to solve as they have shortterm temporal dependencies. Second, the strategies learnt for solving subtasks may be recomposed to solve an unseen game. Motivated by these insights, we aim to solve a game by decomposing it into subtasks characterized by textual goals, then making decisions conditioned on them. Instead of handcrafting the task sketches, we leverage the hierarchical reinforcement learning (HRL) framework for adaptive goal selection, and exploit the compositional nature of language (Jiang et al., 2019) to improve generalizability.</p>
<p>We develop a two-level framework, Hierarchical</p>
<p>Knowledge Graph-based Agent (H-KGA) ${ }^{1}$, to learn a hierarchy of policies with the aid of KG. In the high level, we use a meta-policy to obtain a set of available goals characterized by texts, and select one of them according to the current KGbased observation. Then, we use a sub-policy for goal-conditioned reinforcement learning. Besides, we design a scheduled training strategy to facilitate learning across multiple levels. We conduct experiments on a series of cooking games across 8 levels, while only 4 levels are available during training. The experimental results show that our method improves generalizability in both seen and unseen levels.</p>
<p>Our contributions are summarised as follows: Firstly, we are the first to study generalizability in text-based games from the aspect of hierarchical reinforcement learning. Secondly, we develop a two-level HRL framework leveraging the KGbased observation for adaptive goal selection and goal-conditioned decision making. Thirdly, we empirically validate the effectiveness of our method in games with both seen and unseen difficulty levels, which show favorable generalizability.</p>
<h2>2 Related work</h2>
<h3>2.1 RL agent for text-based games</h3>
<p>Motivated by the prosperity of deep reinforcement learning techniques in playing games (Silver et al., 2016), robotics (Schulman et al., 2017; Fang et al., 2019a,b) and NLP (Fang et al., 2017), several RLbased game agents have been developed for textbased games (He et al., 2016; Yuan et al., 2018; Jain et al., 2020; Yin and May, 2019; Guo et al., 2020; Xu et al., 2020a). Compared with the non-learning-based agents (Hausknecht et al., 2019; Atkinson et al., 2019), the RL-based agents are more favorable as there is no need to handcraft game playing strategies with huge amounts of expert knowledge. The KG-based agents (Murugesan et al., 2020; Xu et al., 2020b) extend RL-based agents with the knowledge graph, which can be constructed from the raw textual observation via simple rules (Ammanabrolu and Riedl, 2019a), language models (Ammanabrolu et al., 2020) or pretraining tasks (Adhikari et al., 2020). The major benefit of KG is that it serves as a belief state to provide structural and historical information to handle partial observability. While these works focus</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>on constructing KG from the textual observation, we aim at improving generalizability by fully exploiting the KG to design a goal-conditioned HRL. Our work thus complements KG-based agents.</p>
<h3>2.2 Generalization in text-based games</h3>
<p>It may be difficult to study generalization in games initially designed for human players (Hausknecht et al., 2020), as they are so challenging that existing RL agents are still far from being able to solve a large proportion of them even under the single game setting (Yao et al., 2020). Furthermore, these games usually have different themes, vocabularies and logics, making it hard to determine the domain gap (Ammanabrolu and Riedl, 2019b). Compared with these man-made games, the synthetic games (Côté et al., 2018; Urbanek et al., 2019) provide a more natural way to study generalization by generating multiple similar games with customizable domain gaps (e.g., by varying game layouts). Generally, the training and testing game sets in previous works have either the same difficulty level (Ammanabrolu and Riedl, 2019a; Murugesan et al., 2021), or a mixture of multiple levels (Adolphs and Hofmann, 2020; Yin et al., 2020), or both (Adhikari et al., 2020). In this work, we extend the setting of multiple levels to unseen levels. We not only study generalization in games that have the same difficulty level but various layouts, but also consider games where both the layouts and levels are different from those of the training games. In addition, we emphasize on improving the performance on hard levels.</p>
<h3>2.3 Hierarchical reinforcement learning</h3>
<p>The HRL framework (Dayan and Hinton, 1992) has been studied in video games (Kulkarni et al., 2016; Vezhnevets et al., 2017; Shu et al., 2018), robotic control tasks (Nachum et al., 2018), and NLP tasks such as the dialogue system (Peng et al., 2017; Saleh et al., 2020). However, as far as we know, we are the first to introduce the insight into textbased games with KG-based observation. Previous works also considered identifying a task by textual goal specifications (Bahdanau et al., 2019; Fu et al., 2019). In the domain of text-based games, such goal-conditioned RL setting has been studied with the quest generation tasks (Ammanabrolu et al., 2019, 2021). In our work, we specify a subtask by its goal. Different from these works, where a single goal is pre-specified or directly generated from the observation, we introduce a hierarchy by</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The overview of H-KGA. In the high level (red), the meta-policy $\pi^{\text {meta }}$ first obtains the set of goals of available subtasks $\mathcal{G}<em t="t">{t}$ from $o</em>$ and $g$.
disentangling the process of goal set generation and goal selection. By accommodating flexible goal set generation (e.g., by pre-trained language models or human experts), we focus on designing a metapolicy to select the goal in an adaptive manner. By adopting HRL to select a textual-based goal for the sub-policy, our work is similar to HIN (Jiang et al., 2019) which, however, focuses on visual scenarios and separately trains the meta-policy and the sub-policy, leaving joint training as future work. Instead, we consider the domain of text-based games, and develop a framework to enable joint training of meta-policy and sub-policy. We further compare joint and individual training in Sec. 6.}^{\mathrm{KG}}$, then selects a goal $g_{t}$ for the sub-policy $\pi^{\text {sub }}$. In the low level (blue), $\pi^{\text {sub }}$ selects the action $a_{t}$ from the admissible action set $A_{t}$ conditioned on $o_{t}^{\mathrm{KG}</p>
<h2>3 Background</h2>
<h3>3.1 KG-based observation</h3>
<p>Following previous works (Hausknecht et al., 2020), we formulate the text-based games as Partially Observable Markov Decision Processes (POMDPs), where the details is in Appendix A. We discard the raw textual observation and consider only the KG-based observation $o_{t}^{\mathrm{KG}}$ as the observational input at timestep $t$. Fig. 1 shows an example of $o_{t}^{\mathrm{KG}}$. The KG is defined as $G=$ $(V, E)$, where $V$ and $E$ are the node set and the edge set, respectively. $o_{t}^{\mathrm{KG}}$ consists of a set of
triplets, where a triplet is formulated as $\langle$ Subject, Relation, Object $\rangle$, denoting that the Subject $\in V$ has Relation $\in E$ with the Object $\in V$.</p>
<h3>3.2 Problem setting</h3>
<p>We aim to design an RL-based agent that is able to address the generalization in solving text-based games. To reduce the requirement for external knowledge, we consider games sharing similar themes and vocabularies, but varying in their layouts and / or difficulty levels. For example, games of the cooking theme (Côté et al., 2018) share the same overall objective: prepare the meal. To accomplish it, the player has to collect ingredients and prepare them in correct ways. The layout of a game contains the room connectivity and the preparing steps (e.g., the type / location of ingredients). The difficulty of a game depends on the complexity of the map (e.g., the number of rooms) and the recipe (e.g., the number of ingredients), such that two games with different levels are naturally different in their layouts. We follow the multi-task learning setting to consider that the training set and the testing set consist of multiple games from multiple levels. We consider two scenarios of generalization: 1) seen levels, where the training and testing games have the same levels, but different layouts. 2) unseen levels, where the training and</p>
<p>testing games are different in levels and layouts. More examples are provided in Appendix. D.</p>
<h2>4 Methodology</h2>
<h3>4.1 Overview</h3>
<p>Fig. 1 shows the overview of H-KGA, which consists of a hierarchy of two levels of policies. In the high level, a meta-policy $\pi^{\text {meta }}$ first obtains the set of goals of available subtasks $\mathcal{G}<em t="t">{t}$ from $o</em>$.}^{\mathrm{KG}}$, then selects a goal $g_{t}$ for the sub-policy $\pi^{\text {sub }}$. In the low level, $\pi^{\text {sub }}$ selects the action $a_{t}$ from the admissible action set $A_{t}$ conditioned on $o_{t}^{\mathrm{KG}}$ and $g$. We omit the subscript " $t$ " for $g$ because this goal may be selected in the past rather than the current time step. For example, as shown in Fig. 1, once the $g_{t}$ is selected by $\pi^{\text {meta }}$, it will remain unchanged for $N$ time steps until being completed (e.g., accomplished or failed). At each time step from $t$ to $t+N, \pi^{\text {sub }}$ considers the same goal $g_{t</p>
<p>In the following, we illustrate how to design $\pi^{\text {meta }}$ to obtain the available goal set $\mathcal{G}<em t="t">{t}$ and conduct goal selection to obtain $g</em>} \in \mathcal{G<em t="t">{t}$ in Sec. 4.2; how to design $\pi^{\text {sub }}$ to select an action $a</em>$ in Sec. 4.3; and how to train H-KGA with a scheduled curriculum for multi-task learning in Sec. 4.4.} \in A_{t</p>
<h3>4.2 Meta-policy for goal selection</h3>
<p>As discussed before, while a whole game may be difficult to accomplish due to long-term temporal dependency, decomposing it into a sketch of subtasks will make the game easier to be solved (Sohn et al., 2018; Shiarlis et al., 2018). If we consider the solving strategy for a subtask as a skill, the generalizability for an unseen game will also be improved by recomposing the learnt skills. Therefore, inspired by the HRL framework (Sutton et al., 1999), we design a meta-policy $\pi^{\text {meta }}$ to first obtain a set of subtasks, then select one subtask from them. We characterize a subtask by its goal to transform subtask selection into goal selection. We make the goal to be instruction-like textual descriptions (e.g., "find purple potato"), yielding better flexibility and interpretability than using a state as the goal (Andrychowicz et al., 2017). Fig. 1 shows the overview of $\pi^{\text {meta }}$ (in red), which consists of a goal set generator, a graph encoder, a text encoder and a goal scorer. We denote the set containing all required goals for solving a game as $\mathcal{G}$. Then we define a goal as "available" at a time step if no other goals should be accomplished before it. For example, "cook red potato" is not available in</p>
<p>Fig. 1, as another goal "find red potato" should be accomplished first. The goal set generator has two purposes: 1) obtain the set of currently available goals $\mathcal{G}<em t="t">{t} \subseteq \mathcal{G}$, and 2) check whether a goal has been accomplished. Inspired by (Jiang et al., 2019), the goal set generator can be implemented by different approaches, including supervised language models and non-learning-based methods such as human supervisors and functional programs. In our work, we use a non-learning-based method to obtain $\mathcal{G}</em>$ and the details are discussed in Sec 5.3 and Appendix B.</p>
<p>After obtaining $\mathcal{G}<em t="t">{t}, \pi^{\text {meta }}$ will be used to select a goal $g</em>} \in \mathcal{G<em t="t">{t}$. We use a graph encoder to encode $o</em>}^{\mathrm{KG}}$ as state representation $\boldsymbol{s<em t="t">{t}^{\text {meta }}$, and a text encoder to encode $\mathcal{G}</em>$ will be paired with each goal representation, then processed by linear layers to obtain the goal scores. The scores can be treated as either sampling probabilities or Q values, where the goal candidate with the highest Q value will be selected.}$ as a stack of goal representations. Arbitrary graph encoders and text encoders can be used. We implement the graph encoder based on the Relational Graph Convolutional Networks (R-GCNs) (Schlichtkrull et al., 2018) to take both nodes and edges into consideration. For the text encoder, a simple single-block transformer (Vaswani et al., 2017) is sufficient as the goal candidates are short texts. In the goal scorer, we adopt a goal scoring process similar to (He et al., 2016), where $\boldsymbol{s}_{t}^{\text {meta }</p>
<p>Following the Semi-Markov Decision Process (SMDPs) (Sutton et al., 1999), $\pi^{\text {meta }}$ will be reexecuted once a goal is accomplished / failed. $\pi^{\text {meta }}$ receives rewards $r_{t}^{\text {env }}$ from the environment. In a transition for $\pi^{\text {meta }}$, the reward is set as the sum of environment rewards:</p>
<p>$$
r^{\text {meta }}=\sum_{i=1}^{T} r_{t+i}^{\text {env }}
$$</p>
<p>where $T$ denotes time steps for accomplishing $g_{t}$.</p>
<h3>4.3 Sub-policy for action selection</h3>
<p>The sub-policy $\pi^{\text {sub }}$ follows the goal-conditioned RL setting (Kaelbling, 1993) where $a_{t}$ is selected by considering both $o_{t}^{\mathrm{KG}}$ and $g$. Fig. 1 shows the architecture of $\pi^{\text {sub }}$ (in blue), which is similar to $\pi^{\text {meta }}$ except that the state $\boldsymbol{s}<em t="t">{t}^{\text {sub }}$ is constructed based on both $o</em>$, or be reinitialized with new weights. As this work does not aim at handling the combinatorial action space,}^{\mathrm{KG}}$ and $g$. The graph encoder and text encoder in $\pi^{\text {meta }}$ can be re-used in $\pi^{\text {sub }</p>
<p>we consider the admissible action set $A_{t} \subseteq \mathcal{A}$ for each time step. We denote an action as "admissible" if it does not lead to meaningless feedback (e.g., "Nothing happens"). Similar to the goal scorer in $\pi^{\text {meta }}$, the action scorer will pair $s_{t}^{\text {sub }}$ with each candidate $a_{i} \in A_{t}$, followed by linear layers to compute the action scores.</p>
<p>Depending on goal accomplishment, $\pi^{\text {sub }}$ receives binary intrinsic reward $r_{t}^{\text {goal }} \in\left{r_{\min }, r_{\max }\right}$, which in this work can be determined by reusing the goal set generator upon $o_{t+1}^{\mathrm{KG}}$. Take Fig. 1 as an example. If the goal before observing $o_{t}^{\mathrm{KG}}$ is "find knife", the agent will receive $r_{t}^{\text {goal }}=r_{\text {max }}$, as this goal is accomplished at time step $t$. Although the KG can serve as a "map" to provide guidance, such binary reward is insufficient for the agent to accomplish a goal in complex games (e.g., the agent has to go through multiple rooms to find an ingredient). To further improve the performance of $\pi^{\text {sub }}$, we reshape the sub-reward with the countbased intrinsic reward (Bellemare et al., 2016) to encourage exploration. Specifically, we apply the BeBold method (Zhang et al., 2020) to the textbased games domain. During training, we count the visitation of observations within an episode, and the accumulated visitation throughout the training process. The count-based reward $r_{t}^{\text {count }}$ is then defined as the regulated difference of inverse cumulative visitation counts with episodic restriction:</p>
<p>$$
\begin{aligned}
r_{t+1}^{\text {count }}=\max &amp; \left(\frac{1}{\mathrm{~N}<em t="t">{\text {acc }}\left(o</em>}^{\mathrm{KG}}\right)}-\frac{1}{\mathrm{~N<em t_1="t+1">{\text {acc }}\left(o</em>,\right. \
&amp; 0) \cdot \mathbb{I}\left{\mathrm{N}}^{\mathrm{KG}}\right)<em t_1="t+1">{\text {epi }}\left(o</em>\right)=1\right}
\end{aligned}
$$}^{\mathrm{KG}</p>
<p>where $\mathrm{N}<em _epi="{epi" _text="\text">{\text {acc }}$ and $\mathrm{N}</em>:$}}$ denote the accumulated and episodic visitation count, respectively. The $\mathbb{I}$ operation returns 1 if $o_{t+1}^{\mathrm{KG}}$ is visited for the first time in the current episode, otherwise 0 . The reward for $\pi^{\text {sub }}$ can then be obtained by combining $r_{t+1}^{\text {goal }}$ and $r_{t+1}^{\text {count }</p>
<p>$$
r_{t+1}^{\text {sub }}=r_{t+1}^{\text {goal }}+\lambda \cdot r_{t+1}^{\text {count }}
$$</p>
<p>where $\lambda$ is a constant coefficient.</p>
<h3>4.4 Training H-KGA for multi-task learning</h3>
<p>We train H-KGA via Double DQN (Hasselt et al., 2016) with prioritized experience replay (Schaul et al., 2015). Algo. 1 shows the training strategy. We consider a training set $\mathcal{D}<em _text="\text" _train="{train">{\text {train }}$ with $\mathcal{L}$ levels of games. For each episode, we sample a game $x$ from $\mathcal{D}</em>$ to interact with (lines 2-22). A goal $g$ will be terminated if it is accomplished/ failed, or}</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 Training Strategy for H-KGA
Input: game sets \(\left\{\mathcal{D}_{\text {train }}, \mathcal{D}_{\text {val }}\right\}\) , replay buffers \(\left\{B^{\text {meta }}, B^{\text {sub }}\right\}\) ，
    update frequencies \(\left\{F_{\text {up }}^{\text {meta }}, F_{\text {up }}^{\text {sub }}\right\}\) ， validation frequency \(F_{\text {val }}\) ，
    tolerance \(\tau\), coefficients \(\beta, \lambda\), patience \(P\)
    Initialize: counters \(k \leftarrow 1, p \leftarrow 0, N_{\text {acc }} \leftarrow \emptyset, N_{\text {epi }} \leftarrow \emptyset\), best
    validation score \(V_{\text {val }} \leftarrow 0, r^{\text {meta }} \leftarrow 0\), caches \(\left\{C^{\text {meta }}, C^{\text {sub }}\right\}\),
    policies \(\left\{\pi^{\text {meta }}, \pi^{\text {sub }}\right\},\left\{\Pi^{\text {meta }}, \Pi^{\text {sub }}\right\}\)
    for \(e \leftarrow 1\) to NUM_EPISODES do
        \(l \leftarrow\) SampleLevel \(\left(\mathcal{L}, p_{l}\right)\)
        \(x \leftarrow\) SampleGame \(\left(\mathcal{D}_{\text {train }}, l\right)\)
        \(o_{0}^{\mathrm{KG}} \leftarrow\) reset \(x\)
        \(C^{\text {meta }} \leftarrow \emptyset, C^{\text {sub }} \leftarrow \emptyset, N_{\text {epi }} \leftarrow \emptyset\),
        Update \(N_{\text {acc }}, N_{\text {epi }}\) with \(o_{0}^{\mathrm{KG}}\)
        for \(t \leftarrow 0\) to NUM_STEPS do
            \(g \leftarrow \pi^{\text {meta }}\left(g \mid o_{t}^{\mathrm{KG}}\right)\)
            \(r^{\text {meta }} \leftarrow 0\)
            while \(g\) is not terminated do
                \(a_{t} \leftarrow \pi^{\text {sub }}\left(a \mid o_{t}^{\mathrm{KG}}, g\right)\)
            Execute \(a_{t}\), receive \(o_{t+1}^{\mathrm{KG}}, r_{t+1}^{\text {out }}\), obtain \(r_{t+1}^{\text {goal }}\)
            Update \(N_{\text {acc }}, N_{\text {epi }}\) with \(o_{t+1}^{\mathrm{KG}}\)
            Compute \(r_{t+1}^{\text {sub }}\) using Eq. (2) and Eq. (3)
            Store the sub transition into \(C^{\text {sub }}\)
            \(r^{\text {meta }} \leftarrow r^{\text {meta }}+r_{t+1}^{\text {out }}\)
            \(t \leftarrow t+1\)
            \(k \leftarrow k+1\)
            if \(k \% F_{\text {up }}^{\text {meta }}=0\) then
                Update \(\left(\pi^{\text {meta }}, B^{\text {meta }}\right)\)
            if \(k \% F_{\text {up }}^{\text {sub }}=0\) then
                Update \(\left(\pi^{\text {sub }}, B^{\text {sub }}\right)\)
            Store the meta transition into \(C^{\text {meta }}\)
        Update \(p_{l}\) using Eq. (4)
        if \(\operatorname{Avg}\left(r^{\text {meta }} \mid C^{\text {meta }}, l\right)&gt;\tau \cdot \operatorname{Avg}\left(r^{\text {meta }} \mid B^{\text {meta }}, l\right)\) then
            Store all transitions in \(C^{\text {meta }}\) into \(B^{\text {meta }}\)
        if \(\operatorname{Avg}\left(r^{\text {goal }} \mid C^{\text {sub }}, l\right)&gt;\tau \cdot \operatorname{Avg}\left(r^{\text {goal }} \mid B^{\text {sub }}, l\right)\) then
            Store all transitions in \(C^{\text {sub }}\) into \(B^{\text {sub }}\)
        if \(e \% F_{\text {val }}=0\) then
            \(v_{\text {val }} \leftarrow \operatorname{Validate}\left(\pi^{\text {meta }}, \pi^{\text {sub }}, \mathcal{D}_{\text {val }}\right)\)
            if \(v_{\text {val }} \geq V_{\text {val }}\) then
                \(V_{\text {val }} \leftarrow v_{\text {val }}, \Pi^{\text {meta }} \leftarrow \pi^{\text {meta }}, \Pi^{\text {sub }} \leftarrow \pi^{\text {sub }}\)
                \(p \leftarrow 0\), continue
            if \(p&gt;P\) then
                \(\pi^{\text {meta }} \leftarrow \Pi^{\text {meta }}, \pi^{\text {sub }} \leftarrow \Pi^{\text {sub }}, p \leftarrow 0\)
            else
            \(p \leftarrow p+1\)
</code></pre></div>

<p>$t$ exceeds NUM_STEPS. We formulate the meta transition as $\left\langle o_{t}^{\mathrm{KG}}, g, r^{\text {meta }}, o_{t+1}^{\mathrm{KG}}, l\right\rangle$, and the sub transition as $\left\langle\left\langle o_{t}^{\mathrm{KG}}, g\right\rangle, a_{t}, r_{t+1}^{\text {sub }}, r_{t+1}^{\text {goal }},\left\langle o_{t+1}^{\mathrm{KG}}, g\right\rangle, l\right\rangle$, where $l \in \mathcal{L}$ denotes the level of a game. We update $\pi^{\text {meta }}\left(\pi^{\text {sub }}\right)$ per $F_{\text {up }}^{\text {meta }}\left(F_{\text {up }}^{\text {sub }}\right)$ interaction steps, by sampling a batch of transitions from the replay buffer $B^{\text {meta }}\left(B^{\text {sub }}\right)$. In addition, we leverage two strategies empirically effective for previous agents (Adhikari et al., 2020). First, we collect the episodic transitions within a cache, and only push them into the replay buffer when its average reward is greater than $\tau$ times the average reward of the buffer (lines 23-26). Second, we validate the model on a validation set $\mathcal{D}<em _text="\text" _val="{val">{\text {val }}$ per $F</em>$ episodes and keep track of the best score $V$ and the correspond-}</p>
<p>ing policies $\left{\Pi^{\text {meta }}, \Pi^{\text {sub }}\right}$. We load the training policies $\left{\pi^{\text {meta }}, \pi^{\text {sub }}\right}$ back to $\left{\Pi^{\text {meta }}, \Pi^{\text {sub }}\right}$, if the validation performance $v_{\text {val }}$ keeps being worse than $V$ for over $P$ times (lines 27-35).</p>
<p>The training process can be formulated as multitask learning if we treat learning on games from the same level as a task. While the knowledge can be shared across levels, different levels may have different scales of training time and performance. For example, those from hard levels generally require more time to learn and tend to have lower normalized performance. To facilitate such multi-task learning setting, we further propose two strategies to improve Algorithm 1: 1) scheduled task sampling and 2) level-aware replay buffer. The scheduled task sampling is inspired by the curriculum learning (Bengio et al., 2009), where we schedule the tasks based on their difficulties. We track the training performance $v_{l}$ on a level $l$, and compute the sampling probability as:</p>
<p>$$
p_{l}=\frac{\exp \left(\beta-v_{l}\right)}{\sum_{l_{i} \in \mathcal{L}} \exp \left(\beta-v_{l_{i}}\right)}
$$</p>
<p>where $\beta$ is a constant coefficient. For each episode, we first sample a level based on the probabilities, and then sample a training game from this level uniformly (lines 2-3). Compared to level-invariant sampling, this strategy encourages the agent to focus more on hard levels with training going on. Another strategy, level-aware replay buffer, is conducted when moving transitions from cache to the replay buffer (lines 23-26). As the transitions collected from hard games tend to have lower reward, they are not likely to be added to the replay buffer. To alleviate this problem, we make the level as an additional component of transition and record the average reward of each level. Then we compare those belonging to the same level to determine whether to add new transitions.</p>
<h2>5 Experiments</h2>
<h3>5.1 Experiment setting</h3>
<p>We conduct experiments on multiple levels of cooking games (Côté et al., 2018). While previous work (Adhikari et al., 2020) considered either a single level, or a mixture of 4 levels, we extend their setting to 8 levels. Based on the rl.0.1 game set $^{2}$, we build a training game set $\mathcal{D}_{\text {train }}$ with 4 levels, including 100 games per level. We build a</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 1: Game statistics. "#Ings" denotes the number of ingredients, "#Reqs" denotes the requirements, and "#Acts" denotes the admissible actions per time step.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Level</th>
<th style="text-align: center;">#Triplets</th>
<th style="text-align: center;">#Rooms</th>
<th style="text-align: center;">#Olips</th>
<th style="text-align: center;">#Ings</th>
<th style="text-align: center;">#Reqs</th>
<th style="text-align: center;">#Acts</th>
<th style="text-align: center;">MaxScore</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">S1</td>
<td style="text-align: center;">21.44</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">17.09</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">11.54</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: center;">S2</td>
<td style="text-align: center;">21.50</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">17.49</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">11.81</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: center;">S3</td>
<td style="text-align: center;">46.09</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">34.15</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">7.25</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: center;">S4</td>
<td style="text-align: center;">54.54</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">33.41</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">28.38</td>
<td style="text-align: center;">11</td>
</tr>
<tr>
<td style="text-align: center;">US1</td>
<td style="text-align: center;">19.85</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">16.01</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">7.98</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: center;">US2</td>
<td style="text-align: center;">20.74</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">16.69</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">8.87</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: center;">US3</td>
<td style="text-align: center;">31.04</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">24.81</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">7.61</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: center;">US4</td>
<td style="text-align: center;">47.31</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">31.09</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">13.90</td>
<td style="text-align: center;">5</td>
</tr>
</tbody>
</table>
<p>validating game set $\mathcal{D}<em _text="\text" _train="{train">{\text {val }}$ with the same 4 levels of $\mathcal{D}</em>}}$, where each level contains 20 games. We build two testing game sets: $\mathcal{D<em _test="{test" _text="\text">{\text {test }}^{\text {seen }}$, and $\mathcal{D}</em>}}^{\text {unseen }}$, both of which contain 4 levels and 20 games per level. The levels within $\mathcal{D<em _text="\text" _train="{train">{\text {test }}^{\text {seen }}$ have been seen in $\mathcal{D}</em>}}$ and $\mathcal{D<em _test="{test" _text="\text">{\text {val }}$, while there is no overlapping game. The levels within $\mathcal{D}</em>$ are unseen during training. Table 1 shows the game statistics averaged over each level, where "S#" denotes a seen level and "US#" denotes an unseen level.}}^{\text {unseen }</p>
<h3>5.2 Baselines</h3>
<p>We consider the following five models, and compare with more variants in ablation studies:</p>
<ul>
<li>GATA (Adhikari et al., 2020): a powerful KGbased agent and the state-of-the-art on the rl.0.1 game set. However, it does not have hierarchical architecture, and the action selection policy is not goal-conditioned.</li>
<li>GC-GATA: GATA equipped with a goal set generator, a goal-conditioned action selection (sub-)policy, and a non-learnable meta-policy for random goal selection.</li>
<li>H-KGA: the proposed model with both metapolicy and sub-policy.</li>
<li>H-KGA HalfJoint: an H-KGA variant, where during the first half of training process only the sub-policy is trained, then the two policies are jointly trained.</li>
<li>H-KGA Ind: an H-KGA variant, where the two policies are individually trained (the subpolicy for the first half, then the meta-policy).</li>
</ul>
<h3>5.3 Implementation details</h3>
<p>We implement the models based on GATA's released code ${ }^{3}$. In particular, we adopt the version</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>Table 2: The testing performance at the end of training.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Avg Seen</th>
<th>Avg Unseen</th>
<th>Avg All</th>
</tr>
</thead>
<tbody>
<tr>
<td>GATA</td>
<td>$0.47 \pm 0.04$</td>
<td>$0.62 \pm 0.07$</td>
<td>$0.55 \pm 0.06$</td>
</tr>
<tr>
<td>GC-GATA</td>
<td>$0.54 \pm 0.13$</td>
<td>$0.61 \pm 0.12$</td>
<td>$0.58 \pm 0.12$</td>
</tr>
<tr>
<td>H-KGA (ours)</td>
<td>$\mathbf{0 . 7 2} \pm 0.04$</td>
<td>$\mathbf{0 . 7 9} \pm 0.04$</td>
<td>$\mathbf{0 . 7 6} \pm 0.03$</td>
</tr>
<tr>
<td>H-KGA HalfJoint</td>
<td>$0.56 \pm 0.11$</td>
<td>$0.57 \pm 0.07$</td>
<td>$0.57 \pm 0.09$</td>
</tr>
<tr>
<td>H-KGA Ind</td>
<td>$0.70 \pm 0.02$</td>
<td>$0.68 \pm 0.01$</td>
<td>$0.69 \pm 0.02$</td>
</tr>
<tr>
<td>GATA w/o BeBold</td>
<td>$0.54 \pm 0.06$</td>
<td>$0.68 \pm 0.02$</td>
<td>$0.61 \pm 0.03$</td>
</tr>
<tr>
<td>H-KGA w/o BeBold</td>
<td>$0.57 \pm 0.07$</td>
<td>$0.65 \pm 0.09$</td>
<td>$0.61 \pm 0.07$</td>
</tr>
<tr>
<td>H-KGA w/o Sch</td>
<td>$0.52 \pm 0.07$</td>
<td>$0.63 \pm 0.07$</td>
<td>$0.57 \pm 0.05$</td>
</tr>
<tr>
<td>H-KGA w/o Sch w/o LR</td>
<td>$0.63 \pm 0.14$</td>
<td>$0.63 \pm 0.16$</td>
<td>$0.63 \pm 0.15$</td>
</tr>
</tbody>
</table>
<p>GATA-GTF and denote it as GATA for simplicity. GATA-GTF discards textual observations and uses the ground truth full KG as observation, so that there is no information extraction error incurred during KG construction. We design a simple non-learning-based goal set generator to obtain available goals (leaving pre-training-based generators as future work). Please refer to Appendix B for details. All models follow the same architecture of graph encoder (i.e., R-GCNs), text encoder (i.e., single transformer block with single head) and scorers (i.e., linear layers). The encoders in $\pi^{\text {meta }}$ and $\pi^{\text {sub }}$ are initialized separately.</p>
<p>We set the step limit of an episode as 50 for training and 100 for validation / testing. We train the models for 100,000 episodes. All models apply the BeBold reward bonus with $\lambda=0.1$, and the scheduled sampling method with $\beta=1.0$. We set $B^{\text {meta }}$ with size 50,000 and $B^{\text {sub }}$ with size 500,000. We set $F_{\text {up }}^{\text {meta }}$ and $F_{\text {up }}^{\text {sub }}$ as 50 time steps, and the updating starts after 100 episodes with batch size 64. The GC-GATA pre-trained for 50,000 episodes is used for initializing H-KGA HalfJoint and H-KGA Ind. For every 1,000 episodes, we validate the model on $\mathcal{D}<em _test="{test" _text="\text">{\text {val }}$, and report the testing performance on $\mathcal{D}</em>$. The experiments are conducted on a Quadro RTX 6000 GPU. Each experiment is run with 3 random seeds, and each run takes 2-3 days to finish.}}^{\text {seen }}$ and $\mathcal{D}_{\text {test }}^{\text {unseen }</p>
<h3>5.4 Evaluation metrics</h3>
<p>We denote a game's score as the episodic sum of rewards without discount. We use the normalized score, which is defined as the collected score normalized by the maximum available score for this game, to measure the performance. For each testing game set, we report the performance on each level and the performance averaged over levels.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The models' performance on $\mathcal{D}<em _test="{test" _text="\text">{\text {test }}^{\text {seen }}$ ("S4", "Avg Seen") and $\mathcal{D}</em>$ ("US4", "Avg Unseen").}}^{\text {unseen }</p>
<h2>6 Results and discussions</h2>
<h3>6.1 Main results</h3>
<p>Table 2 shows the testing performance at the end of training, and Fig. 2 shows the models' testing performance with respect to the training episodes. Due to space constraint, we present only results on the two most difficult levels, "S4" and "US4", as well as the average performance on $\mathcal{D}<em _test="{test" _text="\text">{\text {test }}^{\text {seen }}$ and $\mathcal{D}</em>$. Please refer to Appendix C for the full results. Our H-KGA outperforms baselines in both seen and unseen levels. Its advantage becomes most significant in the most complex level, "S4", which is with the most number of rooms, ingredients and required preparation steps as shown in Table 1. The performance improvement of our model can be attributed to two aspects: the goal-conditioned subpolicy and the meta-policy for adaptive goal selection. GC-GATA, which can be regarded as H-KGA without the meta-policy, also achieves improvement over GATA, demonstrating the effectiveness of goal-conditioned decision making. Compared to GC-GATA, the use of a learned meta-policy helps to further improve H-KGA.}}^{\text {unseen }</p>
<p>However, we observe that joint training after pretraining the sub-policy leads to performance drop (H-KGA HalfJoint), which could be attributed to the forgetting problem in RL (Vinyals et al., 2019). Another variant, H-KGA Ind, where the pre-trained sub-policy is frozen during training the meta-policy,</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The models' performance with / without the BeBold reward bonus.
performs better and exceeds GC-GATA, but still worse than our H-KGA. While H-KGA Ind might still have space for performance improvement, it requires more training episodes (i.e., collecting more interaction samples), leading to low sample efficiency. Instead, our H-KGA utilizes the training data more efficiently and achieves comparable performance with fewer episodes, making it more favorable for practical applications.</p>
<p>We also observe that learning a good meta-policy helps in solving games from unseen levels. In "US4", where the agent has to navigate through multiple rooms to collect three ingredients, it is more important to learn a strategy to determine the collecting order. In these games, our H-KGA performs better than those without a meta-policy (GATA, GC-GATA), and those with a "not-so-good" metapolicy (H-KGA HalfJoint, H-KGA Ind).</p>
<h3>6.2 The influence of exploration</h3>
<p>In Sec. 4.3, we enhance the sub-policy with the BeBold reward to encourage exploration. We investigate its contribution by comparing models without such rewards. Fig. 3 shows the results. In terms of the average performance, our H-KGA is already better than GATA even without the BeBold reward ("H-KGA w/o BeBold" v.s., "GATA w/o BeBold"). However, the results on "S4" and "US4" show that sufficient exploration is essential for these difficult games, where it's hard for H-KGA without
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The models' performance with / without the multi-task learning strategies.</p>
<p>BeBold to collect over $50 \%$ (40\%) of the scores in "S4" ("US4"). We also find that encouraging exploration only is not sufficient, as there is no obvious improvement for GATA, or even worse performance according to Table 2.</p>
<h3>6.3 The influence of MTL strategies</h3>
<p>In Sec. 4.4, we introduce two strategies to facilitate training H-KGA in the setting of multi-task learning. We then conduct ablation studies to investigate their contributions. Fig. 4 shows the results, where "Sch" denotes the scheduled task sampling and "LR" denotes level-aware replay buffer. Although H-KGA can still achieve comparable average performance in both seen and unseen levels, without scheduled task sampling its performance on difficult levels, which require more training steps to collect more training samples, is limited. Similarly, training without "LR" prevents transitions of difficult levels from being added to the replay buffer, leading to low sample efficiency.</p>
<h2>7 Conclusion</h2>
<p>In this paper, we investigated generalization for reinforcement learning in text-based games. We introduced a two-level hierarchical framework, HKGA, to address this problem. In the high level, a meta-policy is executed to decompose the whole game as subtasks characterized by textual goals,</p>
<p>and select a goal based on the knowledge graphbased observation. In the low level, a sub-policy is executed to select action conditioned on the goal. Experimental results showed that H-KGA achieved favorable performance on games with various difficulty levels. As an ongoing work, we would like to study automatic goal generation methods. We are also interested in extending our work to more complex scenarios .</p>
<h2>Acknowledgement</h2>
<p>This work was supported in part by ARC DP180100966. We thank Xingdi Yuan and MarcAlexandre Côté from Microsoft Research, and anonymous reviewers for suggestions.</p>
<h2>References</h2>
<p>Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre Côté, Mikuláš Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, and Will Hamilton. 2020. Learning dynamic belief graphs to generalize on text-based games. In Advances in Neural Information Processing Systems (NeurIPS), volume 33, pages 30453057.</p>
<p>Leonard Adolphs and Thomas Hofmann. 2020. Ledeepchef: Deep reinforcement learning agent for families of text-based games. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), volume 34, pages 7342-7349.</p>
<p>Prithviraj Ammanabrolu, William Broniec, Alex Mueller, Jeremy Paul, and Mark O Riedl. 2019. Toward automated quest generation in text-adventure games. In Proceedings of the 4th Workshop on Computational Creativity in Language Generation, pages $1-12$.</p>
<p>Prithviraj Ammanabrolu and Matthew Hausknecht. 2020. Graph constrained reinforcement learning for natural language action spaces. In International Conference on Learning Representations (ICLR).</p>
<p>Prithviraj Ammanabrolu and Mark Riedl. 2019a. Playing text-adventure games with graph-based deep reinforcement learning. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), volume 1, pages $3557-3565$.</p>
<p>Prithviraj Ammanabrolu and Mark Riedl. 2019b. Transfer in deep reinforcement learning using knowledge graphs. In Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13), pages 110 .</p>
<p>Prithviraj Ammanabrolu, Ethan Tien, Zhaochen Luo, and Mark O Riedl. 2020. How to avoid being eaten by a grue: Exploration strategies for text-adventure agents. arXiv preprint arXiv:2002.08795.</p>
<p>Prithviraj Ammanabrolu, Jack Urbanek, Margaret Li, Arthur Szlam, Tim Rocktäschel, and Jason Weston. 2021. How to motivate your dragon: Teaching goaldriven agents to speak and act in fantasy worlds. In Proceedings of Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACLHLT), pages 807-833.</p>
<p>Jacob Andreas, Dan Klein, and Sergey Levine. 2017. Modular multitask reinforcement learning with policy sketches. In International Conference on Machine Learning (ICML), volume 70, pages 166-175.</p>
<p>Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. 2017. Hindsight experience replay. In Advances in Neural Information Processing Systems (NeurIPS), volume 30, pages 5048-5058.</p>
<p>Timothy Atkinson, Hendrik Baier, Tara Copplestone, Sam Devlin, and Jerry Swan. 2019. The text-based adventure ai competition. IEEE Transactions on Games, 11(3):260-266.</p>
<p>Dzmitry Bahdanau, Felix Hill, Jan Leike, Edward Hughes, Pushmeet Kohli, and Edward Grefenstette. 2019. Learning to understand goal specifications by modelling reward. In International Conference on Learning Representations (ICLR).</p>
<p>Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. 2016. Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information Processing Systems (NeurIPS), volume 29, pages 14711479 .</p>
<p>Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. 2009. Curriculum learning. In International Conference on Machine Learning (ICML), pages 41-48.</p>
<p>Karl Cobbe, Oleg Klimov, Chris Hesse, Taehoon Kim, and John Schulman. 2019. Quantifying generalization in reinforcement learning. In International Conference on Machine Learning (ICML), volume 97, pages 1282-1289.</p>
<p>Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Ruo Yu Tao, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, and Adam Trischler. 2018. Textworld: A learning environment for textbased games. arXiv preprint arXiv:1806.11532.</p>
<p>Peter Dayan and Geoffrey E Hinton. 1992. Feudal reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), volume 5.</p>
<p>Meng Fang, Yuan Li, and Trevor Cohn. 2017. Learning how to active learn: A deep reinforcement learning approach. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 595-605.</p>
<p>Meng Fang, Cheng Zhou, Bei Shi, Boqing Gong, Jia Xu , and Tong Zhang. 2019a. DHER: Hindsight experience replay for dynamic goals. In International Conference on Learning Representations (ICLR).</p>
<p>Meng Fang, Tianyi Zhou, Yali Du, Lei Han, and Zhengyou Zhang. 2019b. Curriculum-guided hindsight experience replay. In Advances in Neural Information Processing Systems (NeurIPS), volume 32, pages 12602-12613.</p>
<p>Justin Fu, Anoop Korattikara, Sergey Levine, and Sergio Guadarrama. 2019. From language to goals: Inverse reinforcement learning for vision-based instruction following. In International Conference on Learning Representations (ICLR).</p>
<p>Xiaoxiao Guo, Mo Yu, Yupeng Gao, Chuang Gan, Murray Campbell, and Shiyu Chang. 2020. Interactive fiction game playing as multi-paragraph reading comprehension with reinforcement learning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages $7755-7765$.</p>
<p>Hado van Hasselt, Arthur Guez, and David Silver. 2016. Deep reinforcement learning with double q-learning. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), volume 30, pages 2094-2100.</p>
<p>Matthew Hausknecht, Prithviraj Ammanabrolu, MarcAlexandre Côté, and Xingdi Yuan. 2020. Interactive fiction games: A colossal adventure. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), volume 34, pages 7903-7910.</p>
<p>Matthew Hausknecht, Ricky Loynd, Greg Yang, Adith Swaminathan, and Jason D Williams. 2019. Nail: A general interactive fiction agent. arXiv preprint arXiv:1902.04259.</p>
<p>Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari Ostendorf. 2016. Deep reinforcement learning with a natural language action space. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 1621-1630.</p>
<p>Vishal Jain, William Fedus, Hugo Larochelle, Doina Precup, and Marc G Bellemare. 2020. Algorithmic improvements for deep reinforcement learning applied to interactive fiction. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), volume 34, pages 4328-4336.</p>
<p>Yiding Jiang, Shixiang Shane Gu, Kevin P Murphy, and Chelsea Finn. 2019. Language as an abstraction for hierarchical deep reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), volume 32, pages 9419-9431.</p>
<p>Leslie Pack Kaelbling. 1993. Learning to achieve goals. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), pages 1094-1098.</p>
<p>Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum. 2016. Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. Advances in Neural Information Processing Systems (NeurIPS), 29:3675-3683.</p>
<p>Jelena Luketina, Nantas Nardelli, Gregory Farquhar, Jakob Foerster, Jacob Andreas, Edward Grefenstette, Shimon Whiteson, and Tim Rocktäschel. 2019. A survey of reinforcement learning informed by natural language. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), pages 6309-6317.</p>
<p>Keerthiram Murugesan, Mattia Atzeni, Pavan Kapanipathi, Pushkar Shukla, Sadhana Kumaravel, Gerald Tesauro, Kartik Talamadupula, Mrinmaya Sachan, and Murray Campbell. 2021. Text-based rl agents with commonsense knowledge: New challenges, environments and baselines. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), volume 35, pages 9018-9027.</p>
<p>Keerthiram Murugesan, Mattia Atzeni, Pushkar Shukla, Mrinmaya Sachan, Pavan Kapanipathi, and Kartik Talamadupula. 2020. Enhancing text-based reinforcement learning agents with commonsense knowledge. arXiv preprint arXiv:2005.00811.</p>
<p>Ofir Nachum, Shixiang Shane Gu, Honglak Lee, and Sergey Levine. 2018. Data-efficient hierarchical reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), volume 31, pages 3303-3313.</p>
<p>Karthik Narasimhan, Tejas D Kulkarni, and Regina Barzilay. 2015. Language understanding for textbased games using deep reinforcement learning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages $1-11$.</p>
<p>Junhyuk Oh, Satinder Singh, Honglak Lee, and Pushmeet Kohli. 2017. Zero-shot task generalization with multi-task deep reinforcement learning. In International Conference on Machine Learning (ICML), volume 70, pages 2661-2670.</p>
<p>Baolin Peng, Xiujun Li, Lihong Li, Jianfeng Gao, Asli Celikyilmaz, Sungjin Lee, and Kam-Fai Wong. 2017. Composite task-completion dialogue policy learning via hierarchical deep reinforcement learning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2231-2240.</p>
<p>Abdelrhman Saleh, Natasha Jaques, Asma Ghandeharioun, Judy Shen, and Rosalind Picard. 2020. Hierarchical reinforcement learning for open-domain di-</p>
<p>alog. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), volume 34, pages 87418748 .</p>
<p>Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. 2015. Prioritized experience replay. arXiv preprint arXiv:1511.05952.</p>
<p>Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max Welling. 2018. Modeling relational data with graph convolutional networks. In European Semantic Web Conference (ESWC), pages 593-607.</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347.</p>
<p>Kyriacos Shiarlis, Markus Wulfmeier, Sasha Salter, Shimon Whiteson, and Ingmar Posner. 2018. Taco: Learning task decomposition via temporal alignment for control. In International Conference on Machine Learning (ICML), volume 80, pages 46544663 .</p>
<p>Tianmin Shu, Caiming Xiong, and Richard Socher. 2018. Hierarchical and interpretable skill acquisition in multi-task reinforcement learning. In International Conference on Learning Representations (ICLR).</p>
<p>David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. 2016. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484-489.</p>
<p>Sungryull Sohn, Junhyuk Oh, and Honglak Lee. 2018. Hierarchical reinforcement learning for zero-shot generalization with subtask dependencies. In Advances in Neural Information Processing Systems (NeurIPS), volume 31, pages 7156-7166.</p>
<p>Richard S Sutton, Doina Precup, and Satinder Singh. 1999. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181-211.</p>
<p>Jack Urbanek, Angela Fan, Siddharth Karamcheti, Saachi Jain, Samuel Humeau, Emily Dinan, Tim Rocktäschel, Douwe Kiela, Arthur Szlam, and Jason Weston. 2019. Learning to speak and act in a fantasy text adventure game. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 673-683.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems (NeurIPS), volume 30, pages 59986008 .</p>
<p>Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg, David Silver, and Koray Kavukcuoglu. 2017. FeUdal networks for hierarchical reinforcement learning. In International Conference on Machine Learning (ICML), volume 70, pages 3540-3549.</p>
<p>Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. 2019. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350-354.</p>
<p>Yunqiu Xu, Ling Chen, Meng Fang, Yang Wang, and Chengqi Zhang. 2020a. Deep reinforcement learning with transformers for text adventure games. In IEEE Conference on Games (CoG), pages 65-72.</p>
<p>Yunqiu Xu, Meng Fang, Ling Chen, Yali Du, Joey Tianyi Zhou, and Chengqi Zhang. 2020b. Deep reinforcement learning with stacked hierarchical attention for text-based games. In Advances in Neural Information Processing Systems (NeurIPS), volume 33, pages 16495-16507.</p>
<p>Shunyu Yao, Rohan Rao, Matthew Hausknecht, and Karthik Narasimhan. 2020. Keep CALM and explore: Language models for action generation in text-based games. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8736-8754.</p>
<p>Xusen Yin and Jonathan May. 2019. Comprehensible context-driven text game playing. IEEE Conference on Games (CoG), pages 1-8.</p>
<p>Xusen Yin, Ralph Weischedel, and Jonathan May. 2020. Learning to generalize for sequential decision making. In Proceedings of the Conference on Empirical Methods in Natural Language Processing: Findings (EMNLP-Findings), pages 3046-3063.</p>
<p>Xingdi (Eric) Yuan, Marc-Alexandre Côté, Alessandro Sordoni, Romain Laroche, Remi Tachet des Combes, Matthew Hausknecht, and Adam Trischler. 2018. Counting to explore and generalize in textbased games. In European Workshop on Reinforcement Learning (EWRL).</p>
<p>Tom Zahavy, Matan Haroush, Nadav Merlis, Daniel J Mankowitz, and Shie Mannor. 2018. Learn what not to learn: Action elimination with deep reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), volume 31, pages $3562-3573$.</p>
<p>Tianjun Zhang, Huazhe Xu, Xiaolong Wang, Yi Wu, Kurt Keutzer, Joseph E Gonzalez, and Yuandong Tian. 2020. Bebold: Exploration beyond the boundary of explored regions. arXiv preprint arXiv:2012.08621.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://aka.ms/twkg/rl.0.1.zip&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ https://github.com/xingdi-eric-yuan/ GATA-public&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-575 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-575</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-575</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-16.html">extraction-schema-16</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <p><strong>Paper ID:</strong> paper-72edcb3788f9c141a3ed28e6d36f75ca4977d27e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/72edcb3788f9c141a3ed28e6d36f75ca4977d27e" target="_blank">Spatio-temporal Graph Convolutional Neural Network: A Deep Learning Framework for Traffic Forecasting</a></p>
                <p><strong>Paper Venue:</strong> International Joint Conference on Artificial Intelligence</p>
                <p><strong>Paper TL;DR:</strong> This paper proposes a novel deep learning framework, Spatio-Temporal Graph Convolutional Networks (STGCN), to tackle the time series prediction problem in traffic domain, and builds the model with complete convolutional structures, which enable much faster training speed with fewer parameters.</p>
                <p><strong>Paper Abstract:</strong> Timely accurate traffic forecast is crucial for urban traffic control and guidance. Due to the high nonlinearity and complexity of traffic flow, traditional methods cannot satisfy the requirements of mid-and-long term prediction tasks and often neglect spatial and temporal dependencies. In this paper, we propose a novel deep learning framework, Spatio-Temporal Graph Convolutional Networks (STGCN), to tackle the time series prediction problem in traffic domain. Instead of applying regular convolutional and recurrent units, we formulate the problem on graphs and build the model with complete convolutional structures, which enable much faster training speed with fewer parameters. Experiments show that our model STGCN effectively captures comprehensive spatio-temporal correlations through modeling multi-scale traffic networks and consistently outperforms state-of-the-art baselines on various real-world traffic datasets.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e575.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e575.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Spectral Graph Convolution</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spectral Graph Convolution (graph Fourier / spectral filtering)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A convolution operator defined in the graph spectral domain using the graph Laplacian eigenbasis to filter signals on nodes; enables convolution-like localized filtering on irregular graph domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Convolutional neural networks on graphs with fast localized spectral filtering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Spectral graph convolution</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Defines convolution on graphs via multiplication in the graph Fourier domain: Theta *_{G} x = U Theta(Λ) U^T x, where U are eigenvectors of the normalized Laplacian L and Θ(Λ) is a spectral filter (diagonal in eigenbasis). To be computationally tractable and local, the spectral filter is approximated (e.g. by Chebyshev polynomials) or simplified to a 1st-order linear form. In this paper the operator is extended to multi-channel inputs and 3-D tensors (time × nodes × channels) and is applied to road-graph node signals (traffic speeds).</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / machine learning algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>signal processing on graphs / graph machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>traffic forecasting / transportation time-series</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Multiple adaptations: (1) approximated spectral filters using Chebyshev polynomial expansion (K-order) to localize filters and reduce complexity, (2) implemented a 1st-order layer-wise linear approximation (single parameter, renormalized adjacency) as an alternative, (3) extended operator from vector signals to multi-channel matrices and 3-D tensors to handle time-series frames, (4) integrated graph convolution layers inside spatio-temporal 'sandwich' blocks between temporal convolution layers, (5) renormalized adjacency with W+I and used practical kernel sizes (K=1 or 3) tuned by grid search for traffic data.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful - when incorporated into STGCN the adapted spectral graph convolution improved prediction accuracy over baselines and enabled scalable computation; e.g., on PeMSD7(M) 15-min MAE: STGCN(Cheb) 2.25 vs baseline GCGRU 2.37, and STGCN achieved statistically significant better results (two-tailed t-test, α=0.01, p<0.01). The spectral-approximation implementation allowed large-scale training with reduced complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Direct spectral filtering is O(n^2) and requires Laplacian eigendecomposition which is infeasible for large graphs; naive spectral filters are non-local. These computational and memory barriers had to be addressed for traffic-scale graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Existing spectral graph theory and prior algorithmic approximations (Chebyshev expansion, 1st-order linearization), availability of road-graph adjacency and node signals, and GPU-based training frameworks enabled adaptation and scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Requires construction of a weighted adjacency matrix (W), computation or approximation of Laplacian-related quantities (rescaled Laplacian or renormalized adjacency), GPU/machine learning infrastructure, and sufficient historical traffic data to train filter parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>High — authors state STGCN is a universal framework for structured time series and the spectral graph convolution adaptations can apply to other spatio-temporal graph domains (e.g., social networks, recommender systems) with appropriate graph construction.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>theoretical principles and explicit procedural steps (algorithmic knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Spatio-temporal Graph Convolutional Neural Network: A Deep Learning Framework for Traffic Forecasting', 'publication_date_yy_mm': '2017-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e575.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e575.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chebyshev Approximation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chebyshev Polynomial Approximation of Graph Filters</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A technique to approximate spectral graph filters by truncated Chebyshev polynomial expansions of the rescaled Laplacian, yielding K-localized filter operations computed recursively.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Wavelets on graphs via spectral graph theory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Chebyshev polynomial approximation for graph convolution kernels</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Approximates the spectral filter Θ(Λ) by a truncated Chebyshev expansion Θ(Λ) ≈ Σ_{k=0}^{K-1} θ_k T_k(˜Λ) with ˜Λ = 2Λ/λ_max − I, where T_k are Chebyshev polynomials evaluated on the scaled Laplacian ˜L. The recursion for Chebyshev polynomials allows computation of K-localized convolutions in O(K|E|) time without explicit eigendecomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / numerical approximation</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>signal processing on graphs / spectral graph theory</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>traffic forecasting on road graphs</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application with adaptation (parameter choices)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Applied standard Chebyshev truncation as in Defferrard et al.; chose kernel size K=3 for experiments; rescaled Laplacian used with practical assumption λ_max≈2 for simplification; integrated the operation into multi-channel and 3-D tensor inputs within ST-Conv blocks.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful - enabled localized graph filtering with tractable computation and contributed to the best-performing variant STGCN(Cheb) on several benchmarks (e.g., PeMSD7 datasets), while keeping training time low.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Need to choose K and assume/approximate λ_max; improper choices can affect locality and model capacity. Also increases parameterization vs 1st-order approach, impacting memory and compute for very large graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Existing recursive Chebyshev formulation allowing sparse-matrix operations; prior demonstrations in graph CNN literature; sparsity of road graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Requires access to the Laplacian (or adjacency) and ability to perform sparse matrix multiplications; selection/tuning of K and λ_max approximation; GPU for efficient training.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>High within graph-structured ML tasks — applicable to many domains with graph signals where localized filters and scalable computation are needed.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and numerical approximation technique</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Spatio-temporal Graph Convolutional Neural Network: A Deep Learning Framework for Traffic Forecasting', 'publication_date_yy_mm': '2017-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e575.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e575.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>1st-order Approximation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>First-order Graph Laplacian Approximation (Kipf & Welling style)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A layer-wise linear approximation of spectral graph convolution that simplifies filters to a first-order polynomial in the Laplacian leading to a computationally cheap renormalized adjacency multiplication.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Semi-supervised classification with graph convolutional networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>First-order (linear) graph convolution approximation</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Assumes Θ(Λ) can be approximated with two shared parameters and simplifies convolution to Θ *_{G} x ≈ θ (I_n + D^{-1/2} W D^{-1/2}) x after renormalizing W to W+I. This yields a cheap layer operation equivalent to multiplying by a renormalized adjacency, stacked across layers to increase receptive field.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / model approximation</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>graph neural network research (semi-supervised learning on graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>traffic forecasting / spatio-temporal graph models</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Adopted the 1st-order approximation to replace Chebyshev parameterization in an STGCN variant (STGCN(1st)), used renormalized adjacency ˜W = W + I and ˜D accordingly, set K (effective receptive field) via number of stacked graph convolutional layers rather than polynomial order, and integrated into the ST-Conv block design.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful and computationally advantageous - STGCN(1st) achieved comparable predictive performance to STGCN(Cheb) while reducing computational overhead; on PeMSD7(L) it yielded ~20% faster training (STGCN(1st) training time 1554.37s vs STGCN(Cheb) 1926.81s) with satisfactory accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>May be less expressive than higher-order polynomial filters for some datasets; trade-off between parameter economy and representational power needs tuning (number of layers vs receptive field).</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Simplicity of renormalized adjacency multiplication, compatibility with deep stacking to enlarge receptive field, and reduced parameter count enabling larger-batch training.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Requires renormalized adjacency construction, choice of network depth to match desired neighborhood size, and typical ML infrastructure for training.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>High for large-scale graph problems where parameter economy and computational efficiency are important; applicable to other spatio-temporal graph domains.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and theoretical/practical approximation knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Spatio-temporal Graph Convolutional Neural Network: A Deep Learning Framework for Traffic Forecasting', 'publication_date_yy_mm': '2017-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e575.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e575.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gated Temporal Convolution</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gated 1-D Temporal Convolutional Sequence Learning (GLU-style)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fully-convolutional approach to sequence modeling using causal 1-D convolutions followed by gated linear units (GLU) to capture temporal dependencies in parallel instead of using recurrent networks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Convolutional sequence to sequence learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Gated temporal convolutional sequence learning (temporal CNN with GLU)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Uses stacked causal 1-D convolutions along the time axis with kernel size K_t and gated linear units as non-linearity: Γ *_T Y = P ⊙ σ(Q), where convolution maps input sequences to P and Q which are combined by element-wise gating. No time-step recurrence is used, enabling parallel training; residual connections and layer normalization are added for stability.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / sequence modeling technique</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>sequence modeling / natural language processing (convolutional seq2seq)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>traffic time-series forecasting</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Adopted the causal 1-D convolution + GLU architecture for node-wise time series, applied identically in parallel to each graph node, used kernel size K_t=3 in experiments, integrated with graph convolution layers in ST-Conv 'sandwich' blocks (temporal conv → graph conv → temporal conv), applied no padding (sequence shortens by K_t−1 per layer), and used residual connections and layer normalization tailored to traffic data.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful - replacing RNNs with gated temporal convolutions yielded much faster training and competitive or superior accuracy; e.g., STGCN training time was ~272s vs GCGRU 3824s on PeMSD7(M) (≈14× speedup) while achieving better MAE and RMSE metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Temporal convolution reduces sequence length without padding (requires design consideration for input window M), and selection of kernel sizes and layer depth affects receptive field; potential for boundary effects if not carefully designed.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Parallelizability of convolutions on modern GPUs, the local temporal structure of traffic data that can be captured by finite receptive fields, and prior demonstration of GLU effectiveness in sequence tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Requires sufficient historical window length (M) and tuning of K_t and stack depth to cover needed temporal horizons; GPU resources for parallel training; integration with graph convolution operations.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>High for other time-series on graphs or per-node sequences where parallel temporal modeling yields efficiency gains (authors explicitly note applicability to other spatio-temporal forecasting tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and instrumental/technical skills (model architecture design)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Spatio-temporal Graph Convolutional Neural Network: A Deep Learning Framework for Traffic Forecasting', 'publication_date_yy_mm': '2017-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e575.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e575.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ConvLSTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Convolutional LSTM (ConvLSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LSTM variant where the input-to-state and state-to-state transitions use convolutional operations, designed to capture spatio-temporal correlations in grid-structured data (e.g., precipitation nowcasting).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Convolutional lstm network: A machine learning approach for precipitation nowcasting</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Convolutional LSTM (ConvLSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Replaces fully-connected transformations in LSTM gates with convolutional layers, enabling the model to capture both spatial and temporal correlations in structured grid data by performing convolution in the state transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / recurrent neural network extension</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>computer vision / meteorology (precipitation nowcasting) / spatio-temporal modeling</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>traffic forecasting (mentioned as prior art for spatio-temporal tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>mentioned as prior cross-domain application (from precipitation nowcasting to general spatio-temporal tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Not applied in this paper; cited as prior work that applies convolution inside recurrent architectures for spatio-temporal problems.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>mentioned as successful in prior literature for precipitation nowcasting, but not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Authors note that conv-LSTM and recurrent networks generally require iterative training, are computationally heavy, and can accumulate errors over steps — motivating the use of purely convolutional temporal models in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Spatial structure in grid-like data and demonstration of ConvLSTM in meteorology encouraged exploring convolutional mechanisms for other spatio-temporal domains.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Designed for grid-structured spatial domains (images/frames); requires convolutional-capable implementations of LSTM gates and sufficient training data.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Moderate — effective for grid-like spatio-temporal data but limited when spatial domain is irregular graphs unless adapted.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>theoretical principles and explicit procedural steps (model architecture concept)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Spatio-temporal Graph Convolutional Neural Network: A Deep Learning Framework for Traffic Forecasting', 'publication_date_yy_mm': '2017-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e575.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e575.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Diffusion Conv GRU (GCGRU)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Diffusion Convolutional Recurrent Neural Network (Diffusion Conv + GRU, often called GCGRU)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model that combines graph-based diffusion convolution operations with GRU recurrent units to model spatio-temporal diffusion processes for traffic forecasting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Diffusion convolutional recurrent neural network: Data-driven traffic forecasting</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Diffusion convolutional recurrent neural network (GCGRU)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Integrates diffusion-style graph convolution (modeling spatial diffusion on the graph) into recurrent GRU cells so that each recurrent update includes graph convolutional mixing across nodes; designed for traffic forecasting to model spatial diffusion and temporal dynamics jointly.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / hybrid ML model</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>graph-based machine learning / time-series modeling</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>traffic forecasting (transportation)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for specific application (graph diffusion + GRU hybrid)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Not introduced by this paper but used as baseline: GCGRU adapts diffusion graph convolutions to operate inside GRU recurrence. In the paper, authors executed GCGRU with layer sizes (64,64,128) as a comparative baseline under the same forecasting setup.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>used successfully as a competitive baseline in experiments, but STGCN outperformed GCGRU in both accuracy and training efficiency (e.g., PeMSD7(M) 15-min MAE GCGRU 2.37 vs STGCN(Cheb) 2.25; training time GCGRU 3824s vs STGCN 272s).</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>GCGRU (recurrent architecture) was computationally heavy and required more GPU memory; on large dataset PeMSD7(L) GCGRU had to reduce batch size due to memory limits, increasing training time substantially.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Conceptual match between diffusion processes on graphs and traffic propagation dynamics; prior work and libraries implementing diffusion convolutions and GRU cells.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Graph adjacency/distance measures for diffusion convolution, recurrent training infrastructure, and careful tuning for memory/compute trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Moderate to high within spatio-temporal graph forecasting tasks, but recurrent component limits training parallelism on large datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and applied algorithmic knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Spatio-temporal Graph Convolutional Neural Network: A Deep Learning Framework for Traffic Forecasting', 'publication_date_yy_mm': '2017-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Convolutional neural networks on graphs with fast localized spectral filtering <em>(Rating: 2)</em></li>
                <li>Semi-supervised classification with graph convolutional networks <em>(Rating: 2)</em></li>
                <li>Wavelets on graphs via spectral graph theory <em>(Rating: 2)</em></li>
                <li>Convolutional sequence to sequence learning <em>(Rating: 2)</em></li>
                <li>Diffusion convolutional recurrent neural network: Data-driven traffic forecasting <em>(Rating: 2)</em></li>
                <li>Convolutional lstm network: A machine learning approach for precipitation nowcasting <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-575",
    "paper_id": "paper-72edcb3788f9c141a3ed28e6d36f75ca4977d27e",
    "extraction_schema_id": "extraction-schema-16",
    "extracted_data": [
        {
            "name_short": "Spectral Graph Convolution",
            "name_full": "Spectral Graph Convolution (graph Fourier / spectral filtering)",
            "brief_description": "A convolution operator defined in the graph spectral domain using the graph Laplacian eigenbasis to filter signals on nodes; enables convolution-like localized filtering on irregular graph domains.",
            "citation_title": "Convolutional neural networks on graphs with fast localized spectral filtering",
            "mention_or_use": "use",
            "procedure_name": "Spectral graph convolution",
            "procedure_description": "Defines convolution on graphs via multiplication in the graph Fourier domain: Theta *_{G} x = U Theta(Λ) U^T x, where U are eigenvectors of the normalized Laplacian L and Θ(Λ) is a spectral filter (diagonal in eigenbasis). To be computationally tractable and local, the spectral filter is approximated (e.g. by Chebyshev polynomials) or simplified to a 1st-order linear form. In this paper the operator is extended to multi-channel inputs and 3-D tensors (time × nodes × channels) and is applied to road-graph node signals (traffic speeds).",
            "procedure_type": "computational method / machine learning algorithm",
            "source_domain": "signal processing on graphs / graph machine learning",
            "target_domain": "traffic forecasting / transportation time-series",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "Multiple adaptations: (1) approximated spectral filters using Chebyshev polynomial expansion (K-order) to localize filters and reduce complexity, (2) implemented a 1st-order layer-wise linear approximation (single parameter, renormalized adjacency) as an alternative, (3) extended operator from vector signals to multi-channel matrices and 3-D tensors to handle time-series frames, (4) integrated graph convolution layers inside spatio-temporal 'sandwich' blocks between temporal convolution layers, (5) renormalized adjacency with W+I and used practical kernel sizes (K=1 or 3) tuned by grid search for traffic data.",
            "transfer_success": "successful - when incorporated into STGCN the adapted spectral graph convolution improved prediction accuracy over baselines and enabled scalable computation; e.g., on PeMSD7(M) 15-min MAE: STGCN(Cheb) 2.25 vs baseline GCGRU 2.37, and STGCN achieved statistically significant better results (two-tailed t-test, α=0.01, p&lt;0.01). The spectral-approximation implementation allowed large-scale training with reduced complexity.",
            "barriers_encountered": "Direct spectral filtering is O(n^2) and requires Laplacian eigendecomposition which is infeasible for large graphs; naive spectral filters are non-local. These computational and memory barriers had to be addressed for traffic-scale graphs.",
            "facilitating_factors": "Existing spectral graph theory and prior algorithmic approximations (Chebyshev expansion, 1st-order linearization), availability of road-graph adjacency and node signals, and GPU-based training frameworks enabled adaptation and scaling.",
            "contextual_requirements": "Requires construction of a weighted adjacency matrix (W), computation or approximation of Laplacian-related quantities (rescaled Laplacian or renormalized adjacency), GPU/machine learning infrastructure, and sufficient historical traffic data to train filter parameters.",
            "generalizability": "High — authors state STGCN is a universal framework for structured time series and the spectral graph convolution adaptations can apply to other spatio-temporal graph domains (e.g., social networks, recommender systems) with appropriate graph construction.",
            "knowledge_type": "theoretical principles and explicit procedural steps (algorithmic knowledge)",
            "uuid": "e575.0",
            "source_info": {
                "paper_title": "Spatio-temporal Graph Convolutional Neural Network: A Deep Learning Framework for Traffic Forecasting",
                "publication_date_yy_mm": "2017-09"
            }
        },
        {
            "name_short": "Chebyshev Approximation",
            "name_full": "Chebyshev Polynomial Approximation of Graph Filters",
            "brief_description": "A technique to approximate spectral graph filters by truncated Chebyshev polynomial expansions of the rescaled Laplacian, yielding K-localized filter operations computed recursively.",
            "citation_title": "Wavelets on graphs via spectral graph theory",
            "mention_or_use": "use",
            "procedure_name": "Chebyshev polynomial approximation for graph convolution kernels",
            "procedure_description": "Approximates the spectral filter Θ(Λ) by a truncated Chebyshev expansion Θ(Λ) ≈ Σ_{k=0}^{K-1} θ_k T_k(˜Λ) with ˜Λ = 2Λ/λ_max − I, where T_k are Chebyshev polynomials evaluated on the scaled Laplacian ˜L. The recursion for Chebyshev polynomials allows computation of K-localized convolutions in O(K|E|) time without explicit eigendecomposition.",
            "procedure_type": "computational method / numerical approximation",
            "source_domain": "signal processing on graphs / spectral graph theory",
            "target_domain": "traffic forecasting on road graphs",
            "transfer_type": "direct application with adaptation (parameter choices)",
            "modifications_made": "Applied standard Chebyshev truncation as in Defferrard et al.; chose kernel size K=3 for experiments; rescaled Laplacian used with practical assumption λ_max≈2 for simplification; integrated the operation into multi-channel and 3-D tensor inputs within ST-Conv blocks.",
            "transfer_success": "successful - enabled localized graph filtering with tractable computation and contributed to the best-performing variant STGCN(Cheb) on several benchmarks (e.g., PeMSD7 datasets), while keeping training time low.",
            "barriers_encountered": "Need to choose K and assume/approximate λ_max; improper choices can affect locality and model capacity. Also increases parameterization vs 1st-order approach, impacting memory and compute for very large graphs.",
            "facilitating_factors": "Existing recursive Chebyshev formulation allowing sparse-matrix operations; prior demonstrations in graph CNN literature; sparsity of road graphs.",
            "contextual_requirements": "Requires access to the Laplacian (or adjacency) and ability to perform sparse matrix multiplications; selection/tuning of K and λ_max approximation; GPU for efficient training.",
            "generalizability": "High within graph-structured ML tasks — applicable to many domains with graph signals where localized filters and scalable computation are needed.",
            "knowledge_type": "explicit procedural steps and numerical approximation technique",
            "uuid": "e575.1",
            "source_info": {
                "paper_title": "Spatio-temporal Graph Convolutional Neural Network: A Deep Learning Framework for Traffic Forecasting",
                "publication_date_yy_mm": "2017-09"
            }
        },
        {
            "name_short": "1st-order Approximation",
            "name_full": "First-order Graph Laplacian Approximation (Kipf & Welling style)",
            "brief_description": "A layer-wise linear approximation of spectral graph convolution that simplifies filters to a first-order polynomial in the Laplacian leading to a computationally cheap renormalized adjacency multiplication.",
            "citation_title": "Semi-supervised classification with graph convolutional networks",
            "mention_or_use": "use",
            "procedure_name": "First-order (linear) graph convolution approximation",
            "procedure_description": "Assumes Θ(Λ) can be approximated with two shared parameters and simplifies convolution to Θ *_{G} x ≈ θ (I_n + D^{-1/2} W D^{-1/2}) x after renormalizing W to W+I. This yields a cheap layer operation equivalent to multiplying by a renormalized adjacency, stacked across layers to increase receptive field.",
            "procedure_type": "computational method / model approximation",
            "source_domain": "graph neural network research (semi-supervised learning on graphs)",
            "target_domain": "traffic forecasting / spatio-temporal graph models",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "Adopted the 1st-order approximation to replace Chebyshev parameterization in an STGCN variant (STGCN(1st)), used renormalized adjacency ˜W = W + I and ˜D accordingly, set K (effective receptive field) via number of stacked graph convolutional layers rather than polynomial order, and integrated into the ST-Conv block design.",
            "transfer_success": "successful and computationally advantageous - STGCN(1st) achieved comparable predictive performance to STGCN(Cheb) while reducing computational overhead; on PeMSD7(L) it yielded ~20% faster training (STGCN(1st) training time 1554.37s vs STGCN(Cheb) 1926.81s) with satisfactory accuracy.",
            "barriers_encountered": "May be less expressive than higher-order polynomial filters for some datasets; trade-off between parameter economy and representational power needs tuning (number of layers vs receptive field).",
            "facilitating_factors": "Simplicity of renormalized adjacency multiplication, compatibility with deep stacking to enlarge receptive field, and reduced parameter count enabling larger-batch training.",
            "contextual_requirements": "Requires renormalized adjacency construction, choice of network depth to match desired neighborhood size, and typical ML infrastructure for training.",
            "generalizability": "High for large-scale graph problems where parameter economy and computational efficiency are important; applicable to other spatio-temporal graph domains.",
            "knowledge_type": "explicit procedural steps and theoretical/practical approximation knowledge",
            "uuid": "e575.2",
            "source_info": {
                "paper_title": "Spatio-temporal Graph Convolutional Neural Network: A Deep Learning Framework for Traffic Forecasting",
                "publication_date_yy_mm": "2017-09"
            }
        },
        {
            "name_short": "Gated Temporal Convolution",
            "name_full": "Gated 1-D Temporal Convolutional Sequence Learning (GLU-style)",
            "brief_description": "A fully-convolutional approach to sequence modeling using causal 1-D convolutions followed by gated linear units (GLU) to capture temporal dependencies in parallel instead of using recurrent networks.",
            "citation_title": "Convolutional sequence to sequence learning",
            "mention_or_use": "use",
            "procedure_name": "Gated temporal convolutional sequence learning (temporal CNN with GLU)",
            "procedure_description": "Uses stacked causal 1-D convolutions along the time axis with kernel size K_t and gated linear units as non-linearity: Γ *_T Y = P ⊙ σ(Q), where convolution maps input sequences to P and Q which are combined by element-wise gating. No time-step recurrence is used, enabling parallel training; residual connections and layer normalization are added for stability.",
            "procedure_type": "computational method / sequence modeling technique",
            "source_domain": "sequence modeling / natural language processing (convolutional seq2seq)",
            "target_domain": "traffic time-series forecasting",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "Adopted the causal 1-D convolution + GLU architecture for node-wise time series, applied identically in parallel to each graph node, used kernel size K_t=3 in experiments, integrated with graph convolution layers in ST-Conv 'sandwich' blocks (temporal conv → graph conv → temporal conv), applied no padding (sequence shortens by K_t−1 per layer), and used residual connections and layer normalization tailored to traffic data.",
            "transfer_success": "successful - replacing RNNs with gated temporal convolutions yielded much faster training and competitive or superior accuracy; e.g., STGCN training time was ~272s vs GCGRU 3824s on PeMSD7(M) (≈14× speedup) while achieving better MAE and RMSE metrics.",
            "barriers_encountered": "Temporal convolution reduces sequence length without padding (requires design consideration for input window M), and selection of kernel sizes and layer depth affects receptive field; potential for boundary effects if not carefully designed.",
            "facilitating_factors": "Parallelizability of convolutions on modern GPUs, the local temporal structure of traffic data that can be captured by finite receptive fields, and prior demonstration of GLU effectiveness in sequence tasks.",
            "contextual_requirements": "Requires sufficient historical window length (M) and tuning of K_t and stack depth to cover needed temporal horizons; GPU resources for parallel training; integration with graph convolution operations.",
            "generalizability": "High for other time-series on graphs or per-node sequences where parallel temporal modeling yields efficiency gains (authors explicitly note applicability to other spatio-temporal forecasting tasks).",
            "knowledge_type": "explicit procedural steps and instrumental/technical skills (model architecture design)",
            "uuid": "e575.3",
            "source_info": {
                "paper_title": "Spatio-temporal Graph Convolutional Neural Network: A Deep Learning Framework for Traffic Forecasting",
                "publication_date_yy_mm": "2017-09"
            }
        },
        {
            "name_short": "ConvLSTM",
            "name_full": "Convolutional LSTM (ConvLSTM)",
            "brief_description": "An LSTM variant where the input-to-state and state-to-state transitions use convolutional operations, designed to capture spatio-temporal correlations in grid-structured data (e.g., precipitation nowcasting).",
            "citation_title": "Convolutional lstm network: A machine learning approach for precipitation nowcasting",
            "mention_or_use": "mention",
            "procedure_name": "Convolutional LSTM (ConvLSTM)",
            "procedure_description": "Replaces fully-connected transformations in LSTM gates with convolutional layers, enabling the model to capture both spatial and temporal correlations in structured grid data by performing convolution in the state transitions.",
            "procedure_type": "computational method / recurrent neural network extension",
            "source_domain": "computer vision / meteorology (precipitation nowcasting) / spatio-temporal modeling",
            "target_domain": "traffic forecasting (mentioned as prior art for spatio-temporal tasks)",
            "transfer_type": "mentioned as prior cross-domain application (from precipitation nowcasting to general spatio-temporal tasks)",
            "modifications_made": "Not applied in this paper; cited as prior work that applies convolution inside recurrent architectures for spatio-temporal problems.",
            "transfer_success": "mentioned as successful in prior literature for precipitation nowcasting, but not evaluated in this paper.",
            "barriers_encountered": "Authors note that conv-LSTM and recurrent networks generally require iterative training, are computationally heavy, and can accumulate errors over steps — motivating the use of purely convolutional temporal models in this paper.",
            "facilitating_factors": "Spatial structure in grid-like data and demonstration of ConvLSTM in meteorology encouraged exploring convolutional mechanisms for other spatio-temporal domains.",
            "contextual_requirements": "Designed for grid-structured spatial domains (images/frames); requires convolutional-capable implementations of LSTM gates and sufficient training data.",
            "generalizability": "Moderate — effective for grid-like spatio-temporal data but limited when spatial domain is irregular graphs unless adapted.",
            "knowledge_type": "theoretical principles and explicit procedural steps (model architecture concept)",
            "uuid": "e575.4",
            "source_info": {
                "paper_title": "Spatio-temporal Graph Convolutional Neural Network: A Deep Learning Framework for Traffic Forecasting",
                "publication_date_yy_mm": "2017-09"
            }
        },
        {
            "name_short": "Diffusion Conv GRU (GCGRU)",
            "name_full": "Diffusion Convolutional Recurrent Neural Network (Diffusion Conv + GRU, often called GCGRU)",
            "brief_description": "A model that combines graph-based diffusion convolution operations with GRU recurrent units to model spatio-temporal diffusion processes for traffic forecasting.",
            "citation_title": "Diffusion convolutional recurrent neural network: Data-driven traffic forecasting",
            "mention_or_use": "use",
            "procedure_name": "Diffusion convolutional recurrent neural network (GCGRU)",
            "procedure_description": "Integrates diffusion-style graph convolution (modeling spatial diffusion on the graph) into recurrent GRU cells so that each recurrent update includes graph convolutional mixing across nodes; designed for traffic forecasting to model spatial diffusion and temporal dynamics jointly.",
            "procedure_type": "computational method / hybrid ML model",
            "source_domain": "graph-based machine learning / time-series modeling",
            "target_domain": "traffic forecasting (transportation)",
            "transfer_type": "adapted/modified for specific application (graph diffusion + GRU hybrid)",
            "modifications_made": "Not introduced by this paper but used as baseline: GCGRU adapts diffusion graph convolutions to operate inside GRU recurrence. In the paper, authors executed GCGRU with layer sizes (64,64,128) as a comparative baseline under the same forecasting setup.",
            "transfer_success": "used successfully as a competitive baseline in experiments, but STGCN outperformed GCGRU in both accuracy and training efficiency (e.g., PeMSD7(M) 15-min MAE GCGRU 2.37 vs STGCN(Cheb) 2.25; training time GCGRU 3824s vs STGCN 272s).",
            "barriers_encountered": "GCGRU (recurrent architecture) was computationally heavy and required more GPU memory; on large dataset PeMSD7(L) GCGRU had to reduce batch size due to memory limits, increasing training time substantially.",
            "facilitating_factors": "Conceptual match between diffusion processes on graphs and traffic propagation dynamics; prior work and libraries implementing diffusion convolutions and GRU cells.",
            "contextual_requirements": "Graph adjacency/distance measures for diffusion convolution, recurrent training infrastructure, and careful tuning for memory/compute trade-offs.",
            "generalizability": "Moderate to high within spatio-temporal graph forecasting tasks, but recurrent component limits training parallelism on large datasets.",
            "knowledge_type": "explicit procedural steps and applied algorithmic knowledge",
            "uuid": "e575.5",
            "source_info": {
                "paper_title": "Spatio-temporal Graph Convolutional Neural Network: A Deep Learning Framework for Traffic Forecasting",
                "publication_date_yy_mm": "2017-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Convolutional neural networks on graphs with fast localized spectral filtering",
            "rating": 2
        },
        {
            "paper_title": "Semi-supervised classification with graph convolutional networks",
            "rating": 2
        },
        {
            "paper_title": "Wavelets on graphs via spectral graph theory",
            "rating": 2
        },
        {
            "paper_title": "Convolutional sequence to sequence learning",
            "rating": 2
        },
        {
            "paper_title": "Diffusion convolutional recurrent neural network: Data-driven traffic forecasting",
            "rating": 2
        },
        {
            "paper_title": "Convolutional lstm network: A machine learning approach for precipitation nowcasting",
            "rating": 1
        }
    ],
    "cost": 0.015539999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</h1>
<p>Bing Yu ${ }^{<em> 1}$, Haoteng Yin ${ }^{</em> 2,3}$, Zhanxing Zhu ${ }^{13,4}$<br>${ }^{1}$ School of Mathematical Sciences, Peking University, Beijing, China<br>${ }^{2}$ Academy for Advanced Interdisciplinary Studies, Peking University, Beijing, China<br>${ }^{3}$ Center for Data Science, Peking University, Beijing, China<br>${ }^{4}$ Beijing Institute of Big Data Research (BIBDR), Beijing, China<br>{byu, htyin, zhanxing.zhu}@pku.edu.cn</p>
<h4>Abstract</h4>
<p>Timely accurate traffic forecast is crucial for urban traffic control and guidance. Due to the high nonlinearity and complexity of traffic flow, traditional methods cannot satisfy the requirements of mid-and-long term prediction tasks and often neglect spatial and temporal dependencies. In this paper, we propose a novel deep learning framework, Spatio-Temporal Graph Convolutional Networks (STGCN), to tackle the time series prediction problem in traffic domain. Instead of applying regular convolutional and recurrent units, we formulate the problem on graphs and build the model with complete convolutional structures, which enable much faster training speed with fewer parameters. Experiments show that our model STGCN effectively captures comprehensive spatio-temporal correlations through modeling multi-scale traffic networks and consistently outperforms state-of-the-art baselines on various real-world traffic datasets.</p>
<h2>1 Introduction</h2>
<p>Transportation plays a vital role in everybody's daily life. According to a survey in 2015, U.S. drivers spend about 48 minutes on average behind the wheel daily. ${ }^{1}$ Under this circumstance, accurate real-time forecast of traffic conditions is of paramount importance for road users, private sectors and governments. Widely used transportation services, such as flow control, route planning, and navigation, also rely heavily on a high-quality traffic condition evaluation. In general, multiscale traffic forecast is the premise and foundation of urban traffic control and guidance, which is also one of main functions of the Intelligent Transportation System (ITS).</p>
<p>In the traffic study, fundamental variables of traffic flow, namely speed, volume, and density are typically chosen as indicators to monitor the current status of traffic conditions and</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>to predict the future. Based on the length of prediction, traffic forecast is generally classified into two scales: short-term (5 $\sim 30 \mathrm{~min}$ ), medium and long term (over 30 min ). Most prevalent statistical approaches (for example, linear regression) are able to perform well on short interval forecast. However, due to the uncertainty and complexity of traffic flow, those methods are less effective for relatively long-term predictions.</p>
<p>Previous studies on mid-and-long term traffic prediction can be roughly divided into two categories: dynamical modeling and data-driven methods. Dynamical modeling uses mathematical tools (e.g. differential equations) and physical knowledge to formulate traffic problems by computational simulation [Vlahogianni, 2015]. To achieve a steady state, the simulation process not only requires sophisticated systematic programming but also consumes massive computational power. Impractical assumptions and simplifications among the modeling also degrade the prediction accuracy. Therefore, with rapid development of traffic data collection and storage techniques, a large group of researchers are shifting their attention to data-driven approaches.</p>
<p>Classic statistical and machine learning models are two major representatives of data-driven methods. In timeseries analysis, autoregressive integrated moving average (ARIMA) and its variants are one of the most consolidated approaches based on classical statistics [Ahmed and Cook, 1979; Williams and Hoel, 2003]. However, this type of model is limited by the stationary assumption of time sequences and fails to take the spatio-temporal correlation into account. Therefore, these approaches have constrained representability of highly nonlinear traffic flow. Recently, classic statistical models have been vigorously challenged by machine learning methods on traffic prediction tasks. Higher prediction accuracy and more complex data modeling can be achieved by these models, such as $k$-nearest neighbors algorithm (KNN), support vector machine (SVM), and neural networks (NN).</p>
<p>Deep learning approaches have been widely and successfully applied to various traffic tasks nowadays. Significant progress has been made in related work, for instance, deep belief network (DBN) [Jia et al., 2016; Huang et al., 2014], stacked autoencoder (SAE) [Lv et al., 2015; Chen et al., 2016]. However, it is difficult for these dense</p>
<p>networks to extract spatial and temporal features from the input jointly. Moreover, within narrow constraints or even complete absence of spatial attributes, the representative ability of these networks would be hindered seriously.</p>
<p>To take full advantage of spatial features, some researchers use convolutional neural network (CNN) to capture adjacent relations among the traffic network, along with employing recurrent neural network (RNN) on time axis. By combining long short-term memory (LSTM) network [Hochreiter and Schmidhuber, 1997] and 1-D CNN, Wu and Tan [2016] presented a feature-level fused architecture CLTFP for shortterm traffic forecast. Although it adopted a straightforward strategy, CLTFP still made the first attempt to align spatial and temporal regularities. Afterwards, Shi et al. [2015] proposed the convolutional LSTM, which is an extended fullyconnected LSTM (FC-LSTM) with embedded convolutional layers. However, the normal convolutional operation applied restricts the model to only process grid structures (e.g. images, videos) rather than general domains. Meanwhile, recurrent networks for sequence learning require iterative training, which introduces error accumulation by steps. Additionally, RNN-based networks (including LSTM) are widely known to be difficult to train and computationally heavy.</p>
<p>For overcoming these issues, we introduce several strategies to effectively model temporal dynamics and spatial dependencies of traffic flow. To fully utilize spatial information, we model the traffic network by a general graph instead of treating it separately (e.g. grids or segments). To handle the inherent deficiencies of recurrent networks, we employ a fully convolutional structure on time axis. Above all, we propose a novel deep learning architecture, the spatio-temporal graph convolutional networks, for traffic forecasting tasks. This architecture comprises several spatio-temporal convolutional blocks, which are a combination of graph convolutional layers [Defferrard et al., 2016] and convolutional sequence learning layers, to model spatial and temporal dependencies. To the best of our knowledge, it is the first time that to apply purely convolutional structures to extract spatio-temporal features simultaneously from graph-structured time series in a traffic study. We evaluate our proposed model on two realworld traffic datasets. Experiments show that our framework outperforms existing baselines in prediction tasks with multiple preset prediction lengths and network scales.</p>
<h2>2 Preliminary</h2>
<h3>2.1 Traffic Prediction on Road Graphs</h3>
<p>Traffic forecast is a typical time-series prediction problem, i.e. predicting the most likely traffic measurements (e.g. speed or traffic flow) in the next $H$ time steps given the previous $M$ traffic observations as,</p>
<p>$$
\begin{aligned}
&amp; \hat{v}<em t_H="t+H">{t+1}, \ldots, \hat{v}</em>= \
&amp; \quad \underset{v_{t+1}, \ldots, v_{t+H}}{\arg \max } \log P\left(v_{t+1}, \ldots, v_{t+H} \mid v_{t-M+1}, \ldots, v_{t}\right)
\end{aligned}
$$</p>
<p>where $v_{t} \in \mathbb{R}^{n}$ is an observation vector of $n$ road segments at time step $t$, each element of which records historical observation for a single road segment.</p>
<p>In this work, we define the traffic network on a graph and focus on structured traffic time series. The observation $v_{t}$ is
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Graph-structured traffic data. Each $v_{t}$ indicates a frame of current traffic status at time step $t$, which is recorded in a graphstructured data matrix.
not independent but linked by pairwise connection in graph. Therefore, the data point $v_{t}$ can be regarded as a graph signal that is defined on an undirected graph (or directed one) $\mathcal{G}$ with weights $w_{i j}$ as shown in Figure 1. At the $t$-th time step, in graph $\mathcal{G}<em t="t">{t}=\left(\mathcal{V}</em>}, \mathcal{E}, W\right), \mathcal{V<em t="t">{t}$ is a finite set of vertices, corresponding to the observations from $n$ monitor stations in a traffic network; $\mathcal{E}$ is a set of edges, indicating the connectedness between stations; while $W \in \mathbb{R}^{n \times n}$ denotes the weighted adjacency matrix of $\mathcal{G}</em>$.</p>
<h3>2.2 Convolutions on Graphs</h3>
<p>A standard convolution for regular grids is clearly not applicable to general graphs. There are two basic approaches currently exploring how to generalize CNNs to structured data forms. One is to expand the spatial definition of a convolution [Niepert et al., 2016], and the other is to manipulate in the spectral domain with graph Fourier transforms [Bruna et al., 2013]. The former approach rearranges the vertices into certain grid forms which can be processed by normal convolutional operations. The latter one introduces the spectral framework to apply convolutions in spectral domains, often named as the spectral graph convolution. Several followingup studies make the graph convolution more promising by reducing the computational complexity from $\mathcal{O}\left(n^{2}\right)$ to linear [Defferrard et al., 2016; Kipf and Welling, 2016].</p>
<p>We introduce the notion of graph convolution operator " $*_{\mathcal{G}}$ " based on the conception of spectral graph convolution, as the multiplication of a signal $x \in \mathbb{R}^{n}$ with a kernel $\Theta$,</p>
<p>$$
\Theta *_{\mathcal{G}} x=\Theta(L) x=\Theta\left(U \Lambda U^{T}\right) x=U \Theta(\Lambda) U^{T} x
$$</p>
<p>where graph Fourier basis $U \in \mathbb{R}^{n \times n}$ is the matrix of eigenvectors of the normalized graph Laplacian $L=I_{n}-$ $D^{-\frac{1}{2}} W D^{-\frac{1}{2}}=U \Lambda U^{T} \in \mathbb{R}^{n \times n}\left(I_{n}\right.$ is an identity matrix, $D \in \mathbb{R}^{n \times n}$ is the diagonal degree matrix with $D_{i i}=\Sigma_{j} W_{i j}$ ); $\Lambda \in \mathbb{R}^{n \times n}$ is the diagonal matrix of eigenvalues of $L$, and filter $\Theta(\Lambda)$ is also a diagonal matrix. By this definition, a graph signal $x$ is filtered by a kernel $\Theta$ with multiplication between $\Theta$ and graph Fourier transform $U^{T} x$ [Shuman et al., 2013].</p>
<h2>3 Proposed Model</h2>
<h3>3.1 Network Architecture</h3>
<p>In this section, we elaborate on the proposed architecture of spatio-temporal graph convolutional networks (STGCN). As shown in Figure 2, STGCN is composed of several spatiotemporal convolutional blocks, each of which is formed as a "sandwich" structure with two gated sequential convolution layers and one spatial graph convolution layer in between. The details of each module are described as follows.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Architecture of spatio-temporal graph convolutional networks. The framework STGCN consists of two spatio-temporal convolutional blocks (ST-Conv blocks) and a fully-connected output layer in the end. Each ST-Conv block contains two temporal gated convolution layers and one spatial graph convolution layer in the middle. The residual connection and bottleneck strategy are applied inside each block. The input $v_{t-M+1}, \ldots, v_{t}$ is uniformly processed by ST-Conv blocks to explore spatial and temporal dependencies coherently. Comprehensive features are integrated by an output layer to generate the final prediction $\hat{v}$.</p>
<h3>3.2 Graph CNNs for Extracting Spatial Features</h3>
<p>The traffic network generally organizes as a graph structure. It is natural and reasonable to formulate road networks as graphs mathematically. However, previous studies neglect spatial attributes of traffic networks: the connectivity and globality of the networks are overlooked, since they are split into multiple segments or grids. Even with 2-D convolutions on grids, it can only capture the spatial locality roughly due to compromises of data modeling. Accordingly, in our model, the graph convolution is employed directly on graphstructured data to extract highly meaningful patterns and features in the space domain. Though the computation of kernel $\Theta$ in graph convolution by Eq. (2) can be expensive due to $\mathcal{O}\left(n^{2}\right)$ multiplications with graph Fourier basis, two approximation strategies are applied to overcome this issue.</p>
<p>Chebyshev Polynomials Approximation To localize the filter and reduce the number of parameters, the kernel $\Theta$ can be restricted to a polynomial of $\Lambda$ as $\Theta(\Lambda)=\sum_{k=0}^{K-1} \theta_{k} \Lambda^{k}$, where $\theta \in \mathbb{R}^{K}$ is a vector of polynomial coefficients. $K$ is the kernel size of graph convolution, which determines the maximum radius of the convolution from central nodes. Traditionally, Chebyshev polynomial $T_{k}(x)$ is used to approximate kernels as a truncated expansion of order $K-1$ as $\Theta(\Lambda) \approx \sum_{k=0}^{K-1} \theta_{k} T_{k}(\tilde{\Lambda})$ with rescaled $\tilde{\Lambda}=2 \Lambda / \lambda_{\max }-I_{n}$ ( $\lambda_{\max }$ denotes the largest eigenvalue of $L$ ) [Hammond et al., 2011]. The graph convolution can then be rewritten as,</p>
<p>$$
\Theta *<em k="0">{\mathcal{G}} x=\Theta(L) x \approx \sum</em>) x
$$}^{K-1} \theta_{k} T_{k}(\tilde{L</p>
<p>where $T_{k}(\tilde{L}) \in \mathbb{R}^{n \times n}$ is the Chebyshev polynomial of order $k$ evaluated at the scaled Laplacian $\tilde{L}=2 L / \lambda_{\max }-I_{n}$. By recursively computing $K$-localized convolutions through the
polynomial approximation, the cost of Eq. (2) can be reduced to $\mathcal{O}(K|\mathcal{E}|)$ as Eq. (3) shows [Defferrard et al., 2016].
$1^{\text {st }}$-order Approximation A layer-wise linear formulation can be defined by stacking multiple localized graph convolutional layers with the first-order approximation of graph Laplacian [Kipf and Welling, 2016]. Consequently, a deeper architecture can be constructed to recover spatial information in depth without being limited to the explicit parameterization given by the polynomials. Due to the scaling and normalization in neural networks, we can further assume that $\lambda_{\max } \approx 2$. Thus, the Eq. (3) can be simplified to,</p>
<p>$$
\begin{aligned}
\Theta *<em 0="0">{\mathcal{G}} x &amp; \approx \theta</em>\right) x \
&amp; \approx \theta_{0} x-\theta_{1}\left(D^{-\frac{1}{2}} W D^{-\frac{1}{2}}\right) x
\end{aligned}
$$} x+\theta_{1}\left(\frac{2}{\lambda_{\max }} L-I_{n</p>
<p>where $\theta_{0}, \theta_{1}$ are two shared parameters of the kernel. In order to constrain parameters and stabilize numerical performances, $\theta_{0}$ and $\theta_{1}$ are replaced by a single parameter $\theta$ by letting $\theta=\theta_{0}=-\theta_{1} ; W$ and $D$ are renormalized by $\tilde{W}=W+I_{n}$ and $\tilde{D}<em j="j">{i i}=\Sigma</em>$ separately. Then, the graph convolution can be alternatively expressed as,} \tilde{W}_{i j</p>
<p>$$
\begin{aligned}
\Theta *<em n="n">{\mathcal{G}} x &amp; =\theta\left(I</em>\right) x \
&amp; =\theta\left(\tilde{D}^{-\frac{1}{2}} \tilde{W} \tilde{D}^{-\frac{1}{2}}\right) x
\end{aligned}
$$}+D^{-\frac{1}{2}} W D^{-\frac{1}{2}</p>
<p>Applying a stack of graph convolutions with the $1^{\text {st }}$-order approximation vertically that achieves the similar effect as $K$ localized convolutions do horizontally, all of which exploit the information from the $(K-1)$-order neighborhood of central nodes. In this scenario, $K$ is the number of successive filtering operations or convolutional layers in a model instead. Additionally, the layer-wise linear structure is parametereconomic and highly efficient for large-scale graphs, since the order of the approximation is limited to one.</p>
<p>Generalization of Graph Convolutions The graph convolution operator " $*<em i="i">{\mathcal{G}}$ " defined on $x \in \mathbb{R}^{n}$ can be extended to multi-dimensional tensors. For a signal with $C</em>$, the graph convolution can be generalized by,}$ channels $X \in \mathbb{R}^{n \times C_{i}</p>
<p>$$
y_{j}=\sum_{i=1}^{C_{i}} \Theta_{i, j}(L) x_{i} \in \mathbb{R}^{n}, 1 \leq j \leq C_{o}
$$</p>
<p>with the $C_{i} \times C_{o}$ vectors of Chebyshev coefficients $\Theta_{i, j} \in$ $\mathbb{R}^{K}\left(C_{i}, C_{o}\right.$ are the size of input and output of the feature maps, respectively). The graph convolution for 2-D variables is denoted as " $\Theta <em><em i="i">{\mathcal{G}} X$ " with $\Theta \in \mathbb{R}^{K \times C</em>} \times C_{o}}$. Specifically, the input of traffic prediction is composed of $M$ frame of road graphs as Figure 1 shows. Each frame $v_{t}$ can be regarded as a matrix whose column $i$ is the $C_{i}$-dimensional value of $v_{t}$ at the $i^{\text {th }}$ node in graph $\mathcal{G<em i="i">{t}$, as $X \in \mathbb{R}^{n \times C</em>$ in parallel. Thus, the graph convolution can be further generalized in 3-D variables, noted as " $\Theta }}$ (in this case, $C_{i}=1$ ). For each time step $t$ of $M$, the equal graph convolution operation with the same kernel $\Theta$ is imposed on $X_{t} \in \mathbb{R}^{n \times C_{i}</em><em i="i">{\mathcal{G}} \mathcal{X}$ " with $\mathcal{X} \in \mathbb{R}^{M \times n \times C</em>$.}</p>
<h3>3.3 Gated CNNs for Extracting Temporal Features</h3>
<p>Although RNN-based models become widespread in timeseries analysis, recurrent networks for traffic prediction still suffer from time-consuming iterations, complex gate mechanisms, and slow response to dynamic changes. On the contrary, CNNs have the superiority of fast training, simple structures, and no dependency constraints to previous steps. Inspired by [Gehring et al., 2017], we employ entire convolutional structures on time axis to capture temporal dynamic behaviors of traffic flows. This specific design allows parallel and controllable training procedures through multi-layer convolutional structures formed as hierarchical representations.</p>
<p>As Figure 2 (right) shows, the temporal convolutional layer contains a 1-D causal convolution with a width- $K_{t}$ kernel followed by gated linear units (GLU) as a non-linearity. For each node in graph $\mathcal{G}$, the temporal convolution explores $K_{t}$ neighbors of input elements without padding which leading to shorten the length of sequences by $K_{t}-1$ each time. Thus, input of temporal convolution for each node can be regarded as a length- $M$ sequence with $C_{i}$ channels as $Y \in$ $\mathbb{R}^{M \times C_{i}}$. The convolution kernel $\Gamma \in \mathbb{R}^{K_{t} \times C_{i} \times 2 C_{o}}$ is designed to map the input $Y$ to a single output element $[P Q] \in$ $\mathbb{R}^{\left(M-K_{t}+1\right) \times\left(2 C_{o}\right)}$ ( $P, Q$ is split in half with the same size of channels). As a result, the temporal gated convolution can be defined as,</p>
<p>$$
\Gamma *<em t="t">{T} Y=P \odot \sigma(Q) \in \mathbb{R}^{\left(M-K</em>
$$}+1\right) \times C_{o}</p>
<p>where $P, Q$ are input of gates in GLU respectively; $\odot$ denotes the element-wise Hadamard product. The sigmoid gate $\sigma(Q)$ controls which input $P$ of the current states are relevant for discovering compositional structure and dynamic variances in time series. The non-linearity gates contribute to the exploiting of the full input filed through stacked temporal layers as well. Furthermore, residual connections are implemented among stacked temporal convolutional layers. Similarly, the temporal convolution can also be generalized to 3-D variables by employing the same convolution kernel $\Gamma$ to every node $\mathcal{Y}<em i="i">{i} \in \mathbb{R}^{M \times C</em>$ equally, noted as " $\Gamma *}}$ (e.g. sensor stations) in $\mathcal{G<em i="i">{T} \mathcal{Y}$ " with $\mathcal{Y} \in \mathbb{R}^{M \times n \times C</em>$.}</p>
<h3>3.4 Spatio-temporal Convolutional Block</h3>
<p>In order to fuse features from both spatial and temporal domains, the spatio-temporal convolutional block (ST-Conv block) is constructed to jointly process graph-structured time series. The block itself can be stacked or extended based on the scale and complexity of particular cases.</p>
<p>As illustrated in Figure 2 (mid), the spatial layer in the middle is to bridge two temporal layers which can achieve fast spatial-state propagation from graph convolution through temporal convolutions. The "sandwich" structure also helps the network sufficiently apply bottleneck strategy to achieve scale compression and feature squeezing by downscaling and upscaling of channels $C$ through the graph convolutional layer. Moreover, layer normalization is utilized within every ST-Conv block to prevent overfitting.</p>
<p>The input and output of ST-Conv blocks are all 3-D tensors. For the input $v^{l} \in \mathbb{R}^{M \times n \times C^{\prime}}$ of block $l$, the output $v^{l+1} \in$ $\mathbb{R}^{\left(M-2\left(K_{t}-1\right)\right) \times n \times C^{l+1}}$ is computed by,</p>
<p>$$
v^{l+1}=\Gamma_{1}^{l} <em>_{T} \operatorname{ReLU}\left(\Theta^{l} </em><em 0="0">{\mathcal{G}}\left(\Gamma</em>\right)\right)
$$}^{l} *_{T} v^{l</p>
<p>where $\Gamma_{0}^{l}, \Gamma_{1}^{l}$ are the upper and lower temporal kernel within block $l$, respectively; $\Theta^{l}$ is the spectral kernel of graph convolution; $\operatorname{ReLU}(\cdot)$ denotes the rectified linear units function. After stacking two ST-Conv blocks, we attach an extra temporal convolution layer with a fully-connected layer as the output layer in the end (See the left of Figure 2). The temporal convolution layer maps outputs of the last ST-Conv block to a single-step prediction. Then, we can obtain a final output $Z \in \mathbb{R}^{n \times c}$ from the model and calculate the speed prediction for $n$ nodes by applying a linear transformation across $c$-channels as $\hat{v}=Z w+b$, where $w \in \mathbb{R}^{c}$ is a weight vector and $b$ is a bias. We use L2 loss to measure the performance of our model. Thus, the loss function of STGCN for traffic prediction can be written as,</p>
<p>$$
L\left(\hat{v} ; W_{\theta}\right)=\sum_{t}\left|\hat{v}\left(v_{t-M+1}, \ldots, v_{t}, W_{\theta}\right)-v_{t+1}\right|^{2}
$$</p>
<p>where $W_{\theta}$ are all trainable parameters in the model; $v_{t+1}$ is the ground truth and $\hat{v}(\cdot)$ denotes the model's prediction.</p>
<p>We now summarize the main characteristics of our model STGCN in the following,</p>
<ul>
<li>STGCN is a universal framework to process structured time series. It is not only able to tackle traffic network modeling and prediction issues but also to be applied to more general spatio-temporal sequence learning tasks.</li>
<li>The spatio-temporal block combines graph convolutions and gated temporal convolutions, which can extract the most useful spatial features and capture the most essential temporal features coherently.</li>
<li>The model is entirely composed of convolutional structures and therefore achieves parallelization over input with fewer parameters and faster training speed. More importantly, this economic architecture allows the model to handle large-scale networks with more efficiency.</li>
</ul>
<h2>4 Experiments</h2>
<h3>4.1 Dataset Description</h3>
<p>We verify our model on two real-world traffic datasets, BJER4 and PeMSD7, collected by Beijing Municipal Traffic Commission and California Deportment of Transportation, respectively. Each dataset contains key attributes of traffic observations and geographic information with corresponding timestamps, as detailed below.</p>
<p>BJER4 was gathered from the major areas of east ring No. 4 routes in Beijing City by double-loop detectors. There are 12 roads selected for our experiment. The traffic data are aggregated every 5 minutes. The time period used is from 1st July to 31st August, 2014 except the weekends. We select the first month of historical speed records as training set, and the rest serves as validation and test set respectively.</p>
<p>PeMSD7 was collected from Caltrans Performance Measurement System (PeMS) in real-time by over 39, 000 sensor stations, deployed across the major metropolitan areas of California state highway system [Chen et al., 2001]. The dataset is also aggregated into 5-minute interval from 30-second data samples. We randomly select a medium and a large scale among the District 7 of California containing 228 and 1, 026</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: PeMS sensor network in District 7 of California (left), each dot denotes a sensor station; Heat map of weighted adjacency matrix in PeMSD7(M) (right).</p>
<p>stations, labeled as <strong>PeMSD7(M)</strong> and <strong>PeMSD7(L)</strong>, respectively, as data sources (shown in the left of Figure 3). The time range of PeMSD7 dataset is in the weekdays of May and June of 2012. We split the training and test sets based on the same principles as above.</p>
<h3>4.2 Data Preprocessing</h3>
<p>The standard time interval in two datasets is set to 5 minutes. Thus, every node of the road graph contains 288 data points per day. The linear interpolation method is used to fill missing values after data cleaning. In addition, data input are normalized by Z-Score method.</p>
<p>In BJER4, the topology of the road graph in Beijing east No.4 ring route system is constructed by the deployment diagram of sensor stations. By collating affiliation, direction and origin-destination points of each road, the ring route system can be digitized as a directed graph.</p>
<p>In PeMSD7, the adjacency matrix of the road graph is computed based on the distances among stations in the traffic network. The weighted adjacency matrix <em>W</em> can be formed as,</p>
<p>$$w_{ij} = \begin{cases} \exp(-\frac{d_{ij}^2}{\sigma^2}), &amp; i \neq j \text{ and } \exp(-\frac{d_{ij}^2}{\sigma^2}) \ge \epsilon \ 0 &amp; \text{ otherwise. }\end{cases} \tag{10}$$</p>
<p>where <em>wij</em> is the weight of edge which is decided by <em>dij</em> (the distance between station <em>i</em> and <em>j</em>). <em>σ²</em> and <em>ε</em> are thresholds to control the distribution and sparsity of matrix <em>W</em>, assigned to 10 and 0.5, respectively. The visualization of <em>W</em> is presented in the right of Figure 3.</p>
<h3>4.3 Experimental Settings</h3>
<p>All experiments are compiled and tested on a Linux cluster (CPU: Intel(R) Xeon(R) CPU E5-2620 v4 @ 2.10GHz, GPU: NVIDIA GeForce GTX 1080). In order to eliminate atypical traffic, only workday traffic data are adopted in our experiment [Li <em>et al.</em>, 2015]. We execute grid search strategy to locate the best parameters on validations. All the tests use 60 minutes as the historical time window, a.k.a. 12 observed data points (<em>M</em> = 12) are used to forecast traffic conditions in the next 15, 30, and 45 minutes (<em>H</em> = 3, 6, 9).</p>
<p><strong>Evaluation Metric &amp; Baselines</strong> To measure and evaluate the performance of different methods, Mean Absolute Errors (MAE), Mean Absolute Percentage Errors (MAPE), and</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>BJER4 (15/ 30/ 45 min)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>MAE</td>
</tr>
<tr>
<td>HA</td>
<td>5.21</td>
</tr>
<tr>
<td>LSVR</td>
<td>4.24/ 5.23/ 6.12</td>
</tr>
<tr>
<td>ARIMA</td>
<td>5.99/ 6.27/ 6.70</td>
</tr>
<tr>
<td>FNN</td>
<td>4.30/ 5.33/ 6.14</td>
</tr>
<tr>
<td>FC-LSTM</td>
<td>4.24/ 4.74/ 5.22</td>
</tr>
<tr>
<td>GCGRU</td>
<td>3.84/ 4.62/ 5.32</td>
</tr>
<tr>
<td>STGCN(Cheb)</td>
<td>3.78/ 4.45/ 5.03</td>
</tr>
<tr>
<td>STGCN(1st)</td>
<td>3.83/ 4.51/ 5.10</td>
</tr>
</tbody>
</table>
<p>Table 1: Performance comparison of different approaches on the dataset BJER4.</p>
<p>Root Mean Squared Errors (RMSE) are adopted. We compare our framework STGCN with the following baselines: 1). Historical Average (HA); 2). Linear Support Vector Regression (LSVR); 3). Auto-Regressive Integrated Moving Average (ARIMA); 4). Feed-Forward Neural Network (FNN); 5). Full-Connected LSTM (FC-LSTM) [Sutskever <em>et al.</em>, 2014]; 6). Graph Convolutional GRU (GCGRU) [Li <em>et al.</em>, 2018].</p>
<p><strong>STGCN Model</strong> For BJER4 and PeMSD7(M/L), the channels of three layers in ST-Conv block are 64, 16, 64 respectively. Both the graph convolution kernel size <em>K</em> and temporal convolution kernel size <em>Kt</em> are set to 3 in the model STGCN(Cheb) with the Chebyshev polynomials approximation, while the <em>K</em> is set to 1 in the model STGCN(1st) with the 1st-order approximation. We train our models by minimizing the mean square error using RMSprop for 50 epochs with batch size as 50. The initial learning rate is 10⁻³ with a decay rate of 0.7 after every 5 epochs.</p>
<h3>4.4 Experiment Results</h3>
<p>Table 1 and 2 demonstrate the results of STGCN and baselines on the datasets BJER4 and PeMSD7(M/L). Our proposed model achieves the best performance with statistical significance (two-tailed T-test, α = 0.01, <em>P</em> &lt; 0.01) in all three evaluation metrics. We can easily observe that traditional statistical and machine learning methods may perform well for short-term forecasting, but their long-term predictions are not accurate because of error accumulation, memorization issues, and absence of spatial information. ARIMA model performs the worst due to its incapability of handling complex spatio-temporal data. Deep learning approaches generally achieved better prediction results than traditional machine learning models.</p>
<h4>Benefits of Spatial Topology</h4>
<p>Previous methods did not incorporate spatial topology and modeled the time series in a coarse-grained way. Differently, through modeling spatial topology of the sensors, our model STGCN has achieved a significant improvement on short and mid-and-long term forecasting. The advantage of STGCN is more obvious on dataset PeMSD7 than BJER4, since the sensor network of PeMS is more complicated and structured (as illustrated in Figure 3), and our model can effectively utilize spatial structure to make more accurate predictions.</p>
<p>To compare three methods based on graph convolution: GCGRU, STGCN(Cheb) and STGCN(1st), we show their</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">PeMSD7(M) (15/ 30/ 45 min)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">PeMSD7(L) (15/ 30/ 45 min)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MAPE (\%)</td>
<td style="text-align: center;">RMSE</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">MAPE (\%)</td>
<td style="text-align: center;">RMSE</td>
</tr>
<tr>
<td style="text-align: center;">HA</td>
<td style="text-align: center;">4.01</td>
<td style="text-align: center;">10.61</td>
<td style="text-align: center;">7.20</td>
<td style="text-align: center;">4.60</td>
<td style="text-align: center;">12.50</td>
<td style="text-align: center;">8.05</td>
</tr>
<tr>
<td style="text-align: center;">LSVR</td>
<td style="text-align: center;">2.50/ 3.63/ 4.54</td>
<td style="text-align: center;">5.81/ 8.88/ 11.50</td>
<td style="text-align: center;">4.55/ 6.67/ 8.28</td>
<td style="text-align: center;">2.69/ 3.85/ 4.79</td>
<td style="text-align: center;">6.27/ 9.48/ 12.42</td>
<td style="text-align: center;">4.88/ 7.10/ 8.72</td>
</tr>
<tr>
<td style="text-align: center;">ARIMA</td>
<td style="text-align: center;">5.55/ 5.86/ 6.27</td>
<td style="text-align: center;">12.92/ 13.94/ 15.20</td>
<td style="text-align: center;">9.00/ 9.13/ 9.38</td>
<td style="text-align: center;">5.50/ 5.87/ 6.30</td>
<td style="text-align: center;">12.30/ 13.54/ 14.85</td>
<td style="text-align: center;">8.63/ 8.96/ 9.39</td>
</tr>
<tr>
<td style="text-align: center;">FNN</td>
<td style="text-align: center;">2.74/ 4.02/ 5.04</td>
<td style="text-align: center;">6.38/ 9.72/ 12.38</td>
<td style="text-align: center;">4.75/ 6.98/ 8.58</td>
<td style="text-align: center;">2.74/ 3.92/ 4.78</td>
<td style="text-align: center;">7.11/ 10.89/ 13.56</td>
<td style="text-align: center;">4.87/ 7.02/ 8.46</td>
</tr>
<tr>
<td style="text-align: center;">FC-LSTM</td>
<td style="text-align: center;">3.57/ 3.94/ 4.16</td>
<td style="text-align: center;">8.60/ 9.55/ 10.10</td>
<td style="text-align: center;">6.20/ 7.03/ 7.51</td>
<td style="text-align: center;">4.38/ 4.51/ 4.66</td>
<td style="text-align: center;">11.10/ 11.41/ 11.69</td>
<td style="text-align: center;">7.68/ 7.94/ 8.20</td>
</tr>
<tr>
<td style="text-align: center;">GCGRU</td>
<td style="text-align: center;">2.37/ 3.31/ 4.01</td>
<td style="text-align: center;">5.54/ 8.06/ 9.99</td>
<td style="text-align: center;">4.21/ 5.96/ 7.13</td>
<td style="text-align: center;">2.48/ 3.43/ 4.12 *</td>
<td style="text-align: center;">5.76/ 8.45/ 10.51 *</td>
<td style="text-align: center;">4.40/ 6.25/ 7.49 *</td>
</tr>
<tr>
<td style="text-align: center;">STGCN(Cheb)</td>
<td style="text-align: center;">2.25/ 3.03/ 3.57</td>
<td style="text-align: center;">5.26/ 7.33/ 8.69</td>
<td style="text-align: center;">4.04/ 5.70/ 6.77</td>
<td style="text-align: center;">2.37/ 3.27/ 3.97</td>
<td style="text-align: center;">5.56/ 7.98/ 9.73</td>
<td style="text-align: center;">4.32/ 6.21/ 7.45</td>
</tr>
<tr>
<td style="text-align: center;">STGCN(1 ${ }^{\text {st }}$ )</td>
<td style="text-align: center;">2.26/ 3.09/ 3.79</td>
<td style="text-align: center;">5.24/ 7.39/ 9.12</td>
<td style="text-align: center;">4.07/ 5.77/ 7.03</td>
<td style="text-align: center;">2.40/ 3.31/ 4.01</td>
<td style="text-align: center;">5.63/ 8.21/ 10.12</td>
<td style="text-align: center;">4.38/ 6.43/ 7.81</td>
</tr>
</tbody>
</table>
<p>Table 2: Performance comparison of different approaches on the dataset PeMSD7.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Speed prediction in the morning peak and evening rush hours of the dataset PeMSD7.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Test RMSE versus the training time (left); Test MAE versus the number of training epochs (right). (PeMSD7(M))
predictions during morning peak and evening rush hours, as shown in Figure 4. It is easy to observe that our proposal STGCN captures the trend of rush hours more accurately than other methods; and it detects the ending of the rush hours earlier than others. Stemming from the efficient graph convolution and stacked temporal convolution structures, our model is capable of fast responding to the dynamic changes among the traffic network without over-reliance on historical average as most of recurrent networks do.</p>
<h2>Training Efficiency and Generalization</h2>
<p>To see the benefits of the convolution along time axis in our proposal, we summarize the comparison of training time between STGCN and GCGRU in Table 3. In terms of fairness, GCGRU consists of three layers with $64,64,128$ units respectively in the experiment for PeMSD7(M), and STGCN uses the default settings as described in Section 4.3. Our model STGCN only consumes 272 seconds, while RNN-type of model GCGRU spends 3, 824 seconds on PeMSD7(M). This $\mathbf{1 4}$ times acceleration of training speed mainly benefits from applying the temporal convolution instead of recurrent structures, which can achieve fully parallel training rather than exclusively relying on chain structures as RNN</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">Time Consumption (s)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">STGCN(Cheb)</td>
<td style="text-align: center;">STGCN(1 ${ }^{\text {st }}$ )</td>
<td style="text-align: center;">GCGRU</td>
</tr>
<tr>
<td style="text-align: left;">PeMSD7(M)</td>
<td style="text-align: center;">$\mathbf{2 7 2 . 3 4}$</td>
<td style="text-align: center;">271.18</td>
<td style="text-align: center;">3824.54</td>
</tr>
<tr>
<td style="text-align: left;">PeMSD7(L)</td>
<td style="text-align: center;">1926.81</td>
<td style="text-align: center;">$\mathbf{1 5 5 4 . 3 7}$</td>
<td style="text-align: center;">19511.92</td>
</tr>
</tbody>
</table>
<p>Table 3: Time consumptions of training on the dataset PeMSD7.
do. For PeMSD7(L), GCGRU has to use the half of batch size since its GPU consumption exceeded the memory capacity of a single card (results marked as "*" in Table 2); while STGCN only need to double the channels in the middle of ST-Conv blocks. Even though our model still consumes less than a tenth of the training time of model GCGRU under this circumstance. Meanwhile, the advantages of the $1^{\text {st }}$-order approximation have appeared since it is not restricted to the parameterization of polynomials. The model STGCN(1 ${ }^{\text {st }}$ ) speeds up around $20 \%$ on a larger dataset with a satisfactory performance compared with STGCN(Cheb).</p>
<p>In order to further investigate the performance of compared deep learning models, we plot the RMSE and MAE of the test set of PeMSD7(M) during the training process, see Figure 5. Those figures also suggest that our model can achieve much faster training procedure and easier convergences. Thanks to the special designs in ST-Conv blocks, our model has superior performances in balancing time consumption and parameter settings. Specifically, the number of parameters in STGCN $\left(4.54 \times 10^{5}\right)$ only accounts for around two third of GCGRU, and saving over $95 \%$ parameters compared to FC-LSTM.</p>
<h2>5 Related Works</h2>
<p>There are several recent deep learning studies that are also motivated by the graph convolution in spatio-temporal tasks. Seo et al. [2016] introduced graph convolutional recurrent network (GCRN) to identify jointly spatial structures and dynamic variation from structured sequences of data. The key challenge of this study is to determine the optimal combinations of recurrent networks and graph convolution under specific settings. Based on principles above, Li et al. [2018] successfully employed the gated recurrent units (GRU) with graph convolution for long-term traffic forecasting. In contrast to these works, we build up our model completely from convolutional structures; The ST-Conv block is specially designed to uniformly process structured data with residual connection and bottleneck strategy inside; More efficient graph convolution kernels are employed in our model as well.</p>
<h2>6 Conclusion and Future Work</h2>
<p>In this paper, we propose a novel deep learning framework STGCN for traffic prediction, integrating graph convolution and gated temporal convolution through spatio-temporal convolutional blocks. Experiments show that our model outperforms other state-of-the-art methods on two real-world datasets, indicating its great potentials on exploring spatiotemporal structures from the input. It also achieves faster training, easier convergences, and fewer parameters with flexibility and scalability. These features are quite promising and practical for scholarly development and large-scale industry deployment. In the future, we will further optimize the network structure and parameter settings. Moreover, our proposed framework can be applied into more general spatiotemporal structured sequence forecasting scenarios, such as evolving of social networks, and preference prediction in recommendation systems, etc.</p>
<h2>References</h2>
<p>[Ahmed and Cook, 1979] Mohammed S Ahmed and Allen R Cook. Analysis of freeway traffic time-series data by using Box-Jenkins techniques. 1979.
[Bruna et al., 2013] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013.
[Chen et al., 2001] Chao Chen, Karl Petty, Alexander Skabardonis, Pravin Varaiya, and Zhanfeng Jia. Freeway performance measurement system: mining loop detector data. Transportation Research Record: Journal of the Transportation Research Board, (1748):96-102, 2001.
[Chen et al., 2016] Quanjun Chen, Xuan Song, Harutoshi Yamada, and Ryosuke Shibasaki. Learning deep representation from big and heterogeneous data for traffic accident inference. In AAAI, pages 338-344, 2016.
[Defferrard et al., 2016] Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In NIPS, pages 3844-3852, 2016.
[Gehring et al., 2017] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional sequence to sequence learning. arXiv preprint arXiv:1705.03122, 2017.
[Hammond et al., 2011] David K Hammond, Pierre Vandergheynst, and Rémi Gribonval. Wavelets on graphs via spectral graph theory. Applied and Computational Harmonic Analysis, 30(2):129-150, 2011.
[Hochreiter and Schmidhuber, 1997] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735-1780, 1997.
[Huang et al., 2014] Wenhao Huang, Guojie Song, Haikun Hong, and Kunqing Xie. Deep architecture for traffic flow prediction: deep belief networks with multitask learning. IEEE Transactions on Intelligent Transportation Systems, 15(5):2191-2201, 2014.
[Jia et al., 2016] Yuhan Jia, Jianping Wu, and Yiman Du. Traffic speed prediction using deep learning method. In ITSC, pages 1217-1222. IEEE, 2016.
[Kipf and Welling, 2016] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.
[Li et al., 2015] Yexin Li, Yu Zheng, Huichu Zhang, and Lei Chen. Traffic prediction in a bike-sharing system. In SIGSPATIAL, page 33. ACM, 2015.
[Li et al., 2018] Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. Diffusion convolutional recurrent neural network: Data-driven traffic forecasting. In $I C L R, 2018$.
[Lv et al., 2015] Yisheng Lv, Yanjie Duan, Wenwen Kang, Zhengxi Li, and Fei-Yue Wang. Traffic flow prediction with big data: a deep learning approach. IEEE Transactions on Intelligent Transportation Systems, 16(2):865873, 2015.
[Niepert et al., 2016] Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural networks for graphs. In ICML, pages 2014-2023, 2016.
[Seo et al., 2016] Youngjoo Seo, Michaël Defferrard, Pierre Vandergheynst, and Xavier Bresson. Structured sequence modeling with graph convolutional recurrent networks. arXiv preprint arXiv:1612.07659, 2016.
[Shi et al., 2015] Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo. Convolutional lstm network: A machine learning approach for precipitation nowcasting. In NIPS, pages 802-810, 2015.
[Shuman et al., 2013] David I Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega, and Pierre Vandergheynst. The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains. IEEE Signal Processing Magazine, 30(3):83-98, 2013.
[Sutskever et al., 2014] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In NIPS, pages 3104-3112, 2014.
[Vlahogianni, 2015] Eleni I Vlahogianni. Computational intelligence and optimization for transportation big data: challenges and opportunities. In Engineering and Applied Sciences Optimization, pages 107-128. Springer, 2015.
[Williams and Hoel, 2003] Billy M Williams and Lester A Hoel. Modeling and forecasting vehicular traffic flow as a seasonal arima process: Theoretical basis and empirical results. Journal of transportation engineering, 129(6):664-672, 2003.
[Wu and Tan, 2016] Yuankai Wu and Huachun Tan. Shortterm traffic flow forecasting with spatial-temporal correlation in a hybrid deep learning framework. arXiv preprint arXiv:1612.01022, 2016.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Equal contributions.
${ }^{1}$ Corresponding author.
${ }^{1}$ https://aaafoundation.org/american-driving-survey-2014-2015/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
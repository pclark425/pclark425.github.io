<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1451 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1451</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1451</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-26.html">extraction-schema-26</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <p><strong>Paper ID:</strong> paper-cab294fe9d6a33f9876baf557f57b5aa65727b1f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/cab294fe9d6a33f9876baf557f57b5aa65727b1f" target="_blank">Autonomous discovery in the chemical sciences part I: Progress</a></p>
                <p><strong>Paper Venue:</strong> Angewandte Chemie</p>
                <p><strong>Paper TL;DR:</strong> This two-part review examines how automation has contributed to different aspects of discovery in the chemical sciences and describes many case studies of discoveries accelerated by or resulting from computer assistance and automation from the domains of synthetic chemistry, drug discovery, inorganic chemistry, and materials science.</p>
                <p><strong>Paper Abstract:</strong> This two-part review examines how automation has contributed to different aspects of discovery in the chemical sciences. In this first part, we describe a classification for discoveries of physical matter (molecules, materials, devices), processes, and models and how they are unified as search problems. We then introduce a set of questions and considerations relevant to assessing the extent of autonomy. Finally, we describe many case studies of discoveries accelerated by or resulting from computer assistance and automation from the domains of synthetic chemistry, drug discovery, inorganic chemistry, and materials science. These illustrate how rapid advancements in hardware automation and machine learning continue to transform the nature of experimentation and modelling. Part two reflects on these case studies and identifies a set of open challenges for the field.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1451.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1451.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Early computational reasoning frameworks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BACON / BACON.4 / FARENHEIT / ABACUS (family of early inductive discovery systems)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Rule- and heuristic-based programs from the 1970s-1990s designed to formalize inductive scientific discovery by generating and scoring candidate empirical laws from data; demonstrated by rediscovering well-known physical laws under idealized conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BACON-family (BACON, BACON.4, FARENHEIT, ABACUS)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Rule-based inductive discovery systems that enumerate candidate arithmetic or piecewise relationships among input variables and score them against data; use hand-encoded heuristics and operators to propose, modify, and rank hypotheses. Designed to operate on pre-selected variables and (often) noise-free data.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Physics / Chemistry (empirical law discovery, general scientific laws)</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>These systems recovered known empirical relationships (e.g., Ohm's law, Snell's law, conservation laws, Black's specific heat law) by searching combinations of input variables and scoring generated expressions against data; they were applied to idealized datasets with irrelevant variables removed and little/no measurement noise.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td>incremental</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td>The paper describes these systems as recapitulating known laws rather than generating radically new theories; they rediscovered established empirical laws from data, consistent with an incremental or confirmatory role rather than transformational breakthroughs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Comparison of generated candidate laws against known target laws and data fit; success measured by the ability to reproduce recognized analytic relationships (i.e., rediscovery of established laws) under simplified/noiseless conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Comparison to canonical, pre-existing empirical laws and demonstration on curated datasets; validation was largely retrospective (checking whether the program returns known relationships).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>Novelty judged by whether an analytical law not already known was produced; because these programs mostly rediscovered existing relationships, novelty was limited—the systems were viewed as validating inductive procedures rather than producing new theory.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td>Qualitative: list of rediscovered laws and the ability to produce analytic forms; no numerical rates reported in this review beyond enumeration of successful recapitulations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td>The paper contrasts these programs with human-led discovery, noting that the systems largely reproduced what humans already knew and depended on hand-encoded priors; such systems 'fill in the gaps' of existing theory rather than effect paradigm shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Qualitative success in rediscovering multiple canonical laws (Ohm's law, Snell's law, conservation of momentum, Black's specific heat law) under idealized conditions; not presented as a numeric success rate.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Require strong priors and hand-coded heuristics, assume removal of irrelevant variables and low noise, limited ability to handle stoichiometry/phase changes/uncertainty, and thus constrained in producing genuinely novel discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous discovery in the chemical sciences part I: Progress', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1451.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1451.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Schmidt & Lipson symbolic regression</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Symbolic regression for natural law discovery (Schmidt & Lipson)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A symbolic regression framework that generates and scores hypothesized analytical laws from experimental motion-tracking data, shown to rediscover conservation laws and Hamiltonians.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Symbolic regression framework (Schmidt & Lipson style)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Search-based symbolic regression that generates candidate analytic expressions (combinations of variables and operations) and scores them by fit and parsimony to empirical data to identify compact physical laws.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Physics / mechanistic model discovery</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>Applied to motion-tracking datasets to rediscover Hamiltonians, Lagrangians, and geometric conservation laws by producing closed-form expressions that match observed dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td>incremental</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td>The paper presents this work as rediscovery of known physical laws from data; it is characterized as recapitulating established theoretical relationships rather than producing unprecedented theoretical frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Scoring of generated analytical laws by data fit and simplicity; demonstration that top-ranked expressions correspond to known canonical laws.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Comparison of machine-produced expressions to known analytic laws and verification against measured datasets (retrospective validation).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>Novelty limited to the method's capacity to rediscover known laws; novelty would be assessed by generation of previously unknown analytic expressions, which was not the primary outcome discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td>Qualitative: success in matching known canonical forms; no explicit numeric performance metrics provided in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td>Framed as reproducing human-derived laws; demonstrates potential of algorithmic induction but not evidence of surpassing human creativity in producing new theories.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Reported qualitatively as successful in rediscovering multiple canonical laws from motion data; no numeric success rate provided in review.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Relies on selection of appropriate input variables and sufficiently clean data; combinatorial hypothesis space can be large, and scoring schemes must balance fit and parsimony.</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous discovery in the chemical sciences part I: Progress', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1451.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1451.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STAHL / KEKADA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>STAHL (Zytkow & Simon) and KEKADA (knowledge-driven metabolic discovery system)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Logic-based systems that attempted to automate compositional and metabolic model construction (e.g., rediscovering oxygen theory and the Krebs cycle) using inference operators and simulated experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>STAHL / KEKADA</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Heuristic and operator-based AI systems: STAHL infers compositional models from reaction lists by applying logical inference rules; KEKADA uses multiple heuristic operators (hypothesis proposers, generators, modifiers, confidence modifiers) and simulated metabolic experiments to construct and evaluate hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Chemistry / Biochemistry (compositional and metabolic model induction)</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>STAHL produced proposed chemical elements and compound compositions (e.g., rediscovering Lavoisier's oxygen theory analogues) from reaction data; KEKADA was able to reconstruct the Krebs cycle from the same empirical metabolic reaction data available historically.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td>incremental</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td>These programs were judged successful insofar as they replicated known biochemical frameworks (e.g., Krebs cycle) from available empirical data, consistent with incremental rediscovery rather than transformational novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Assessment by whether the system produced models corresponding to known historical discoveries (e.g., Krebs cycle); scoring based on inference plausibility within simulated experimental contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Retrospective comparison to historical known biological pathways and logical consistency checks within simulated experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>Novelty limited because the outputs corresponded to extant scientific knowledge; the systems' interest lies more in formalizing reasoning processes than in reporting new, independently validated discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td>Qualitative demonstration of capability to reconstruct historical discoveries; no quantitative impact metrics reported in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td>Paper highlights that these systems formalize reasoning akin to human scientists but are constrained by their representation and heuristics; lack of uncertainty handling and other limitations make them less flexible than human reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Qualitatively successful in reconstructing the Krebs cycle and compositional inferences in their test cases; no numeric success rates provided.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Inability to consider stoichiometry, phase changes, uncertainty, and competing hypotheses in earlier systems; dependence on hand-encoded operators limits scope and flexibility.</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous discovery in the chemical sciences part I: Progress', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1451.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1451.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reaction Mechanism Generator (RMG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reaction Mechanism Generator (RMG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A template-driven software tool that enumerates elementary reactions and builds detailed kinetic mechanisms for combustion and pyrolysis by combining expert reaction templates with estimated thermodynamic and kinetic parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Reaction Mechanism Generator (RMG)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Enumerative mechanism generator that uses expert-defined reaction templates to propose elementary reactions among input species, and estimates rate constants using first-principles calculations (e.g., DFT) and group additivity regressions to generate kinetic models.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Physical chemistry / combustion chemistry (detailed kinetic mechanism discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>RMG identifies new elementary reactions/pathways and produces detailed kinetic mechanisms for complex reactive systems (e.g., combustion/pyrolysis), enabling exploration of effects such as fuel additives on ignition delay by producing mechanistic proposals and kinetic parameter estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td>incremental</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td>The review frames RMG as an enabling mechanistic enumerator that discovers plausible elementary steps within constrained template/estimation frameworks—advancing mechanistic detail but not characterized as a transformational theory generator.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Validation of generated mechanisms by estimating kinetic/thermodynamic parameters and comparing model predictions (e.g., ignition delay, product distributions) against experimental data or higher-level calculations; plausibility judged by energetics and rate estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Comparison to experimental combustion/pyrolysis data and to first-principles calculations where available; pruning based on calculated transition state energies and kinetic relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>Novelty assessed by whether proposed elementary steps and assembled mechanisms explain observed global chemistry not previously captured; novelty is often incremental—identifying new steps within an established mechanistic framework.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td>Qualitative: ability to generate comprehensive mechanism sets and to predict effects on macroscopic observables (e.g., ignition delay); no single numeric metric provided in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td>RMG augments human mechanistic exploration by systematically enumerating possibilities and estimating parameters; its discoveries are framed as mechanistic hypotheses that require human or experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Combinatorial explosion of possible reactions without heuristics, reliance on template coverage and parameter estimation accuracy, and need for experimental or higher-level computational validation to confirm proposed steps.</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous discovery in the chemical sciences part I: Progress', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1451.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1451.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MECHEM / ReaxFF-guided mechanism search</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MECHEM and ReaxFF-guided mechanistic search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Algorithmic enumerators and force-field-guided search methods that enumerate or guide plausible elementary reactions and pathways in catalytic and reactive systems using reactive potential energy surfaces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MECHEM / ReaxFF-guided search</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Combinatorial enumeration frameworks (MECHEM) and reactive force-field (ReaxFF) guided searches that enumerate possible elementary reactions and steer exploration toward kinetically plausible pathways using approximate potential energy surfaces and pruning heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Catalysis / reaction mechanism discovery</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>These systems generate large sets of elementary reaction candidates and identify multi-step pathways and transition states that could rationalize observed global reactions in catalytic systems by pruning via estimated energetics and kinetics.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td>incremental</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td>Presented as algorithmic aids to explore mechanistic hypothesis space and enumerate plausible steps; they extend mechanistic knowledge but are not portrayed as delivering paradigm-shifting theoretical advances.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Pruning based on estimated transition state energies / kinetic plausibility from ReaxFF or DFT calculations; relevance judged by whether pathways can account for observed macroscopic reactions.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Electronic structure calculations (DFT) and transition state searches for energetic validation; comparison of simulated outcomes to experimental observations when available.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>Novelty arises when enumerated pathways reveal previously unconsidered elementary steps or channels; novelty is usually contextual and requires subsequent computational or experimental confirmation.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td>Qualitative: ability to reduce search space to kinetically feasible pathways; no universal numeric impact metric provided in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td>These methods can find pathways that humans might overlook due to combinatorial complexity and can accelerate hypothesis generation, but human interpretation and higher-fidelity validation remain required.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Huge combinatorial spaces if heuristics are absent; quality depends on force-field fidelity and the ability to correctly prune unphysical pathways; computational cost for high-fidelity validation can be substantial.</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous discovery in the chemical sciences part I: Progress', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1451.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1451.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ab initio nanoreactor</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ab initio nanoreactor molecular dynamics discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reactive molecular dynamics approach that accelerates discovery of chemical reactions by periodically injecting kinetic energy to encourage collisions, enabling observation of rare events and discovery of unexpected products without predefined reaction coordinates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Ab initio nanoreactor</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An MD-based exploratory system that runs ab initio molecular dynamics and periodically perturbs molecules (e.g., pushing them toward the center) to accelerate collisions and rare event sampling, allowing discovery of unexpected reaction products and novel reaction channels.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Prebiotic chemistry / chemical reaction discovery</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>Used to simulate reactive mixtures and discover unexpected products and reaction pathways similar to historic origin-of-life experiments (e.g., Urey–Miller-like chemistry) without reliance on reaction enumeration heuristics or predefined reaction coordinates.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td>incremental</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td>Framed as a method to discover plausible products and reaction channels algorithmically; the review treats this as expanding mechanistic possibilities rather than claiming transformational theoretical breakthroughs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Identification of chemical products and pathways observed in MD trajectories; plausibility judged by energetic and mechanistic consistency of simulated events.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Higher-level electronic structure calculations for mechanistic validation and, where feasible, experimental follow-up to confirm predicted products or pathways.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>Novelty assessed by whether the MD-driven exploration reveals products or mechanisms not previously considered; emphasizes that it does not require prior heuristics and thus may uncover unexpected chemistry.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td>Qualitative demonstration of discovery of unexpected products; no quantitative impact metric provided in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td>Method can find unanticipated chemistry that might evade human intuition by unbiased simulation under perturbed conditions; experimental validation remains key for establishing significance.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Requires significant computation, interpretation of noisy MD trajectories, and subsequent validation; pushing systems to accelerate rare events may bias observed chemistry.</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous discovery in the chemical sciences part I: Progress', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1451.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1451.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CASP frameworks (OCSS / WODCA / neural MCTS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Computer-Aided Synthesis Planning frameworks (OCSS, WODCA, neural-network-guided Monte Carlo Tree Search)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of retrosynthetic planning systems that use rule libraries, value functions, and/or learned policies (including neural networks with MCTS) to propose synthetic routes, recently producing recommendations judged plausible by chemists.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Computer-Aided Synthesis Planning (CASP) systems</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>CASP systems range from heuristic rule-based planners (WODCA, OCSS) to modern systems that extract templates algorithmically and use learned value functions or neural action policies integrated with Monte Carlo tree search to propose retrosynthetic disconnections and full synthetic routes.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Synthetic organic chemistry / synthesis route discovery</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>These systems propose retrosynthetic disconnections and multi-step synthetic routes for target molecules, including suggesting steps that are judged by human experts as comparable to literature routes; they can search large template libraries or learn action policies from reaction databases.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td>incremental</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td>The review positions CASP outputs as synthetic proposals within existing chemistry paradigms; while they can propose novel disconnections, they are framed as practical planning tools rather than transformational scientific discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Evaluation by human expert assessment (including double-blind studies where chemists judged machine-suggested routes as equally plausible to literature ones), route length and estimated synthetic complexity metrics, and synthesizability / starting-material availability heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Expert chemist review, experimental implementation when undertaken, and comparison to known literature pathways; value-function metrics for synthetic tractability are used as proxies.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>Novelty determined by whether suggested disconnections or full routes are unseen in the literature or provide more efficient/feasible pathways; novelty often requires experimental execution to be confirmed.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td>Qualitative human-assessment outcomes (e.g., in a double-blind study, machine recommendations were considered equally plausible to literature pathways); metrics such as estimated synthetic complexity or route length are used as proxies.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td>Double-blind evaluation found machine-generated retrosynthetic suggestions to be judged as plausible as literature routes by chemists; learned policies can outperform simple heuristics in generating shorter or more target-aligned routes.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Validation bottleneck because experimental synthesis is required to fully validate novel retrosynthetic proposals; constrained by template coverage and quality of reaction databases; estimation proxies for synthesizability are imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous discovery in the chemical sciences part I: Progress', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1451.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1451.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gao et al. reaction condition model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Data-driven reaction condition recommendation model (Gao et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model trained on literature reaction data (Reaxys) to propose reagents, catalysts, solvents, and temperatures as suitable reaction conditions, learning continuous embeddings for chemicals reflecting synthetic function.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Gao et al. condition recommender</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A data-driven classification/regression approach that, given reactant and product structures, predicts categorical choices (reagent, catalyst, solvent identities) and continuous variables (e.g., temperature) by learning embeddings from a large literature database.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Synthetic chemistry / reaction condition prediction</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>The model recommends reaction conditions for proposed transformations by learning mappings from reactant/product structures to likely conditions found in the literature, effectively recommending feasible conditions without human-encoded rules.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td>incremental</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td>Characterized as a recommender that learns from existing literature precedents and proposes likely conditions rather than discovering fundamentally new reaction mechanisms or paradigms.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Trained and evaluated on literature data (Reaxys) with classification/regression metrics for condition prediction accuracy; embedding quality inferred by downstream recommendation performance.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Comparison of predicted conditions to reported literature conditions; implied need for experimental validation for novel condition proposals.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>Novelty is in automating condition selection and learning chemical function embeddings; the model's recommendations are judged novel if they propose conditions not commonly reported or if they enable successful new experiments upon testing.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td>Prediction accuracy on literature-conditioned datasets and qualitative downstream usefulness (not quantified in the review).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Depends on literature coverage and biases; when only positive (successful) examples exist, the model treats reported conditions as adequate, which may limit learning of failure modes; experimental validation of novel suggestions remains essential.</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous discovery in the chemical sciences part I: Progress', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1451.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1451.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deep generative models (GANs/VAEs)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep generative molecular design (GANs, VAEs, other generative models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Neural generative architectures that learn latent distributions over chemical structures to sample diverse candidate molecules for downstream screening and synthesis, facilitating exploration of vast chemical space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Deep generative molecular models (GANs / VAEs / related)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Generative adversarial networks, variational autoencoders, and related deep models that learn continuous latent representations of molecules enabling sampling and optimization for desired properties; often coupled to property predictors or optimization routines.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Molecular discovery / materials discovery / drug design</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>Used to propose candidate molecules or materials by sampling learned distributions or by latent-space optimization toward properties of interest; these proposals serve as inputs to virtual or experimental screening for discovery of new functional matter.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td>incremental</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td>Presented as tools to expand and sample chemical space and propose candidates; the review frames them as accelerating candidate generation within existing discovery pipelines rather than as engines of paradigm-shifting theory.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Evaluation often via property-prediction scores, diversity/novelty metrics, synthetic accessibility proxies, and downstream validation success (virtual screening hit rates or experimental confirmation).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Virtual screening followed by experimental synthesis/testing for promising candidates; metrics include hit identification and property confirmation.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>Novelty assessed by structural novelty relative to training data, predicted property extremeness, and whether experimental testing confirms previously unknown high-performing compounds.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td>Proxy metrics include novelty/diversity, predicted property scores, and downstream hit rates upon experimental validation; no universal numeric thresholds provided in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Sampling may produce synthetically inaccessible or unstable molecules; evaluation is often limited by the fidelity of property predictors and the cost of experimental validation; disentangling true discovery from rediscovery of known chemistry can be challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous discovery in the chemical sciences part I: Progress', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1451.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e1451.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Active learning / Bayesian optimization frameworks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Active learning and Bayesian optimization experimental design frameworks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Iterative experimental design strategies that select experiments to maximally improve predictive models or to directly optimize objectives, thereby reducing the number of required experiments compared to brute-force search.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Active learning and Bayesian optimization</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Algorithmic frameworks that use uncertainty-aware models and acquisition functions to select the most informative or promising experiments at each iteration (active learning) or to balance exploration and exploitation for objective optimization (Bayesian optimization), often coupled with automated experimentation.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>General experimental sciences (chemistry, materials, process optimization, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>Applied to optimize reaction conditions, identify new reactions/materials, or learn structure-property models with far fewer experiments than unguided search; can enable closed-loop autonomous discovery when coupled with automated lab hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td>incremental</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td>Described as efficiency and acceleration methods—reducing experimental burden and navigating high-dimensional spaces more intelligently—rather than methods that by themselves produce transformational theoretical breakthroughs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Measured by reduction in number of experiments needed to reach a performance target (e.g., active learning may require only ~20% of experiments compared to exhaustive search as noted in the review), model improvement metrics, and optimization of objective values achieved.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Empirical demonstration in iterative experimental campaigns with comparison to baseline strategies (random or brute-force sampling), and assessment of resource savings and model accuracy improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>Novelty assessed by whether the strategy discovers solutions that would be missed or require far more resources under brute-force search; degree of novelty depends on design space and constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td>Experimental-effort reduction (e.g., percent of experiments saved), model performance improvements, and improved objective values obtained; the review cites an example claim that active strategies might need only ~20% of experiments to find an optimal solution in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Effectiveness depends on model quality, representation, and experimental noise; in continuous or enormously large spaces, brute-force comparisons are not always meaningful; closing the loop requires integrated automated execution which is nontrivial.</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous discovery in the chemical sciences part I: Progress', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1451",
    "paper_id": "paper-cab294fe9d6a33f9876baf557f57b5aa65727b1f",
    "extraction_schema_id": "extraction-schema-26",
    "extracted_data": [
        {
            "name_short": "Early computational reasoning frameworks",
            "name_full": "BACON / BACON.4 / FARENHEIT / ABACUS (family of early inductive discovery systems)",
            "brief_description": "Rule- and heuristic-based programs from the 1970s-1990s designed to formalize inductive scientific discovery by generating and scoring candidate empirical laws from data; demonstrated by rediscovering well-known physical laws under idealized conditions.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "BACON-family (BACON, BACON.4, FARENHEIT, ABACUS)",
            "system_description": "Rule-based inductive discovery systems that enumerate candidate arithmetic or piecewise relationships among input variables and score them against data; use hand-encoded heuristics and operators to propose, modify, and rank hypotheses. Designed to operate on pre-selected variables and (often) noise-free data.",
            "discovery_domain": "Physics / Chemistry (empirical law discovery, general scientific laws)",
            "discovery_description": "These systems recovered known empirical relationships (e.g., Ohm's law, Snell's law, conservation laws, Black's specific heat law) by searching combinations of input variables and scoring generated expressions against data; they were applied to idealized datasets with irrelevant variables removed and little/no measurement noise.",
            "discovery_type": "incremental",
            "discovery_type_justification": "The paper describes these systems as recapitulating known laws rather than generating radically new theories; they rediscovered established empirical laws from data, consistent with an incremental or confirmatory role rather than transformational breakthroughs.",
            "evaluation_methods": "Comparison of generated candidate laws against known target laws and data fit; success measured by the ability to reproduce recognized analytic relationships (i.e., rediscovery of established laws) under simplified/noiseless conditions.",
            "validation_approaches": "Comparison to canonical, pre-existing empirical laws and demonstration on curated datasets; validation was largely retrospective (checking whether the program returns known relationships).",
            "novelty_assessment": "Novelty judged by whether an analytical law not already known was produced; because these programs mostly rediscovered existing relationships, novelty was limited—the systems were viewed as validating inductive procedures rather than producing new theory.",
            "impact_metrics": "Qualitative: list of rediscovered laws and the ability to produce analytic forms; no numerical rates reported in this review beyond enumeration of successful recapitulations.",
            "comparison_to_human_discoveries": true,
            "comparison_details": "The paper contrasts these programs with human-led discovery, noting that the systems largely reproduced what humans already knew and depended on hand-encoded priors; such systems 'fill in the gaps' of existing theory rather than effect paradigm shifts.",
            "success_rate": "Qualitative success in rediscovering multiple canonical laws (Ohm's law, Snell's law, conservation of momentum, Black's specific heat law) under idealized conditions; not presented as a numeric success rate.",
            "challenges_limitations": "Require strong priors and hand-coded heuristics, assume removal of irrelevant variables and low noise, limited ability to handle stoichiometry/phase changes/uncertainty, and thus constrained in producing genuinely novel discoveries.",
            "has_incremental_transformational_comparison": true,
            "uuid": "e1451.0",
            "source_info": {
                "paper_title": "Autonomous discovery in the chemical sciences part I: Progress",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "Schmidt & Lipson symbolic regression",
            "name_full": "Symbolic regression for natural law discovery (Schmidt & Lipson)",
            "brief_description": "A symbolic regression framework that generates and scores hypothesized analytical laws from experimental motion-tracking data, shown to rediscover conservation laws and Hamiltonians.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Symbolic regression framework (Schmidt & Lipson style)",
            "system_description": "Search-based symbolic regression that generates candidate analytic expressions (combinations of variables and operations) and scores them by fit and parsimony to empirical data to identify compact physical laws.",
            "discovery_domain": "Physics / mechanistic model discovery",
            "discovery_description": "Applied to motion-tracking datasets to rediscover Hamiltonians, Lagrangians, and geometric conservation laws by producing closed-form expressions that match observed dynamics.",
            "discovery_type": "incremental",
            "discovery_type_justification": "The paper presents this work as rediscovery of known physical laws from data; it is characterized as recapitulating established theoretical relationships rather than producing unprecedented theoretical frameworks.",
            "evaluation_methods": "Scoring of generated analytical laws by data fit and simplicity; demonstration that top-ranked expressions correspond to known canonical laws.",
            "validation_approaches": "Comparison of machine-produced expressions to known analytic laws and verification against measured datasets (retrospective validation).",
            "novelty_assessment": "Novelty limited to the method's capacity to rediscover known laws; novelty would be assessed by generation of previously unknown analytic expressions, which was not the primary outcome discussed here.",
            "impact_metrics": "Qualitative: success in matching known canonical forms; no explicit numeric performance metrics provided in the review.",
            "comparison_to_human_discoveries": true,
            "comparison_details": "Framed as reproducing human-derived laws; demonstrates potential of algorithmic induction but not evidence of surpassing human creativity in producing new theories.",
            "success_rate": "Reported qualitatively as successful in rediscovering multiple canonical laws from motion data; no numeric success rate provided in review.",
            "challenges_limitations": "Relies on selection of appropriate input variables and sufficiently clean data; combinatorial hypothesis space can be large, and scoring schemes must balance fit and parsimony.",
            "has_incremental_transformational_comparison": true,
            "uuid": "e1451.1",
            "source_info": {
                "paper_title": "Autonomous discovery in the chemical sciences part I: Progress",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "STAHL / KEKADA",
            "name_full": "STAHL (Zytkow & Simon) and KEKADA (knowledge-driven metabolic discovery system)",
            "brief_description": "Logic-based systems that attempted to automate compositional and metabolic model construction (e.g., rediscovering oxygen theory and the Krebs cycle) using inference operators and simulated experiments.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "STAHL / KEKADA",
            "system_description": "Heuristic and operator-based AI systems: STAHL infers compositional models from reaction lists by applying logical inference rules; KEKADA uses multiple heuristic operators (hypothesis proposers, generators, modifiers, confidence modifiers) and simulated metabolic experiments to construct and evaluate hypotheses.",
            "discovery_domain": "Chemistry / Biochemistry (compositional and metabolic model induction)",
            "discovery_description": "STAHL produced proposed chemical elements and compound compositions (e.g., rediscovering Lavoisier's oxygen theory analogues) from reaction data; KEKADA was able to reconstruct the Krebs cycle from the same empirical metabolic reaction data available historically.",
            "discovery_type": "incremental",
            "discovery_type_justification": "These programs were judged successful insofar as they replicated known biochemical frameworks (e.g., Krebs cycle) from available empirical data, consistent with incremental rediscovery rather than transformational novelty.",
            "evaluation_methods": "Assessment by whether the system produced models corresponding to known historical discoveries (e.g., Krebs cycle); scoring based on inference plausibility within simulated experimental contexts.",
            "validation_approaches": "Retrospective comparison to historical known biological pathways and logical consistency checks within simulated experiments.",
            "novelty_assessment": "Novelty limited because the outputs corresponded to extant scientific knowledge; the systems' interest lies more in formalizing reasoning processes than in reporting new, independently validated discoveries.",
            "impact_metrics": "Qualitative demonstration of capability to reconstruct historical discoveries; no quantitative impact metrics reported in the review.",
            "comparison_to_human_discoveries": true,
            "comparison_details": "Paper highlights that these systems formalize reasoning akin to human scientists but are constrained by their representation and heuristics; lack of uncertainty handling and other limitations make them less flexible than human reasoning.",
            "success_rate": "Qualitatively successful in reconstructing the Krebs cycle and compositional inferences in their test cases; no numeric success rates provided.",
            "challenges_limitations": "Inability to consider stoichiometry, phase changes, uncertainty, and competing hypotheses in earlier systems; dependence on hand-encoded operators limits scope and flexibility.",
            "has_incremental_transformational_comparison": true,
            "uuid": "e1451.2",
            "source_info": {
                "paper_title": "Autonomous discovery in the chemical sciences part I: Progress",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "Reaction Mechanism Generator (RMG)",
            "name_full": "Reaction Mechanism Generator (RMG)",
            "brief_description": "A template-driven software tool that enumerates elementary reactions and builds detailed kinetic mechanisms for combustion and pyrolysis by combining expert reaction templates with estimated thermodynamic and kinetic parameters.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Reaction Mechanism Generator (RMG)",
            "system_description": "Enumerative mechanism generator that uses expert-defined reaction templates to propose elementary reactions among input species, and estimates rate constants using first-principles calculations (e.g., DFT) and group additivity regressions to generate kinetic models.",
            "discovery_domain": "Physical chemistry / combustion chemistry (detailed kinetic mechanism discovery)",
            "discovery_description": "RMG identifies new elementary reactions/pathways and produces detailed kinetic mechanisms for complex reactive systems (e.g., combustion/pyrolysis), enabling exploration of effects such as fuel additives on ignition delay by producing mechanistic proposals and kinetic parameter estimates.",
            "discovery_type": "incremental",
            "discovery_type_justification": "The review frames RMG as an enabling mechanistic enumerator that discovers plausible elementary steps within constrained template/estimation frameworks—advancing mechanistic detail but not characterized as a transformational theory generator.",
            "evaluation_methods": "Validation of generated mechanisms by estimating kinetic/thermodynamic parameters and comparing model predictions (e.g., ignition delay, product distributions) against experimental data or higher-level calculations; plausibility judged by energetics and rate estimates.",
            "validation_approaches": "Comparison to experimental combustion/pyrolysis data and to first-principles calculations where available; pruning based on calculated transition state energies and kinetic relevance.",
            "novelty_assessment": "Novelty assessed by whether proposed elementary steps and assembled mechanisms explain observed global chemistry not previously captured; novelty is often incremental—identifying new steps within an established mechanistic framework.",
            "impact_metrics": "Qualitative: ability to generate comprehensive mechanism sets and to predict effects on macroscopic observables (e.g., ignition delay); no single numeric metric provided in the review.",
            "comparison_to_human_discoveries": true,
            "comparison_details": "RMG augments human mechanistic exploration by systematically enumerating possibilities and estimating parameters; its discoveries are framed as mechanistic hypotheses that require human or experimental validation.",
            "success_rate": null,
            "challenges_limitations": "Combinatorial explosion of possible reactions without heuristics, reliance on template coverage and parameter estimation accuracy, and need for experimental or higher-level computational validation to confirm proposed steps.",
            "has_incremental_transformational_comparison": true,
            "uuid": "e1451.3",
            "source_info": {
                "paper_title": "Autonomous discovery in the chemical sciences part I: Progress",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "MECHEM / ReaxFF-guided mechanism search",
            "name_full": "MECHEM and ReaxFF-guided mechanistic search",
            "brief_description": "Algorithmic enumerators and force-field-guided search methods that enumerate or guide plausible elementary reactions and pathways in catalytic and reactive systems using reactive potential energy surfaces.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "MECHEM / ReaxFF-guided search",
            "system_description": "Combinatorial enumeration frameworks (MECHEM) and reactive force-field (ReaxFF) guided searches that enumerate possible elementary reactions and steer exploration toward kinetically plausible pathways using approximate potential energy surfaces and pruning heuristics.",
            "discovery_domain": "Catalysis / reaction mechanism discovery",
            "discovery_description": "These systems generate large sets of elementary reaction candidates and identify multi-step pathways and transition states that could rationalize observed global reactions in catalytic systems by pruning via estimated energetics and kinetics.",
            "discovery_type": "incremental",
            "discovery_type_justification": "Presented as algorithmic aids to explore mechanistic hypothesis space and enumerate plausible steps; they extend mechanistic knowledge but are not portrayed as delivering paradigm-shifting theoretical advances.",
            "evaluation_methods": "Pruning based on estimated transition state energies / kinetic plausibility from ReaxFF or DFT calculations; relevance judged by whether pathways can account for observed macroscopic reactions.",
            "validation_approaches": "Electronic structure calculations (DFT) and transition state searches for energetic validation; comparison of simulated outcomes to experimental observations when available.",
            "novelty_assessment": "Novelty arises when enumerated pathways reveal previously unconsidered elementary steps or channels; novelty is usually contextual and requires subsequent computational or experimental confirmation.",
            "impact_metrics": "Qualitative: ability to reduce search space to kinetically feasible pathways; no universal numeric impact metric provided in the review.",
            "comparison_to_human_discoveries": true,
            "comparison_details": "These methods can find pathways that humans might overlook due to combinatorial complexity and can accelerate hypothesis generation, but human interpretation and higher-fidelity validation remain required.",
            "success_rate": null,
            "challenges_limitations": "Huge combinatorial spaces if heuristics are absent; quality depends on force-field fidelity and the ability to correctly prune unphysical pathways; computational cost for high-fidelity validation can be substantial.",
            "has_incremental_transformational_comparison": true,
            "uuid": "e1451.4",
            "source_info": {
                "paper_title": "Autonomous discovery in the chemical sciences part I: Progress",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "Ab initio nanoreactor",
            "name_full": "Ab initio nanoreactor molecular dynamics discovery",
            "brief_description": "A reactive molecular dynamics approach that accelerates discovery of chemical reactions by periodically injecting kinetic energy to encourage collisions, enabling observation of rare events and discovery of unexpected products without predefined reaction coordinates.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Ab initio nanoreactor",
            "system_description": "An MD-based exploratory system that runs ab initio molecular dynamics and periodically perturbs molecules (e.g., pushing them toward the center) to accelerate collisions and rare event sampling, allowing discovery of unexpected reaction products and novel reaction channels.",
            "discovery_domain": "Prebiotic chemistry / chemical reaction discovery",
            "discovery_description": "Used to simulate reactive mixtures and discover unexpected products and reaction pathways similar to historic origin-of-life experiments (e.g., Urey–Miller-like chemistry) without reliance on reaction enumeration heuristics or predefined reaction coordinates.",
            "discovery_type": "incremental",
            "discovery_type_justification": "Framed as a method to discover plausible products and reaction channels algorithmically; the review treats this as expanding mechanistic possibilities rather than claiming transformational theoretical breakthroughs.",
            "evaluation_methods": "Identification of chemical products and pathways observed in MD trajectories; plausibility judged by energetic and mechanistic consistency of simulated events.",
            "validation_approaches": "Higher-level electronic structure calculations for mechanistic validation and, where feasible, experimental follow-up to confirm predicted products or pathways.",
            "novelty_assessment": "Novelty assessed by whether the MD-driven exploration reveals products or mechanisms not previously considered; emphasizes that it does not require prior heuristics and thus may uncover unexpected chemistry.",
            "impact_metrics": "Qualitative demonstration of discovery of unexpected products; no quantitative impact metric provided in the review.",
            "comparison_to_human_discoveries": true,
            "comparison_details": "Method can find unanticipated chemistry that might evade human intuition by unbiased simulation under perturbed conditions; experimental validation remains key for establishing significance.",
            "success_rate": null,
            "challenges_limitations": "Requires significant computation, interpretation of noisy MD trajectories, and subsequent validation; pushing systems to accelerate rare events may bias observed chemistry.",
            "has_incremental_transformational_comparison": true,
            "uuid": "e1451.5",
            "source_info": {
                "paper_title": "Autonomous discovery in the chemical sciences part I: Progress",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "CASP frameworks (OCSS / WODCA / neural MCTS)",
            "name_full": "Computer-Aided Synthesis Planning frameworks (OCSS, WODCA, neural-network-guided Monte Carlo Tree Search)",
            "brief_description": "A class of retrosynthetic planning systems that use rule libraries, value functions, and/or learned policies (including neural networks with MCTS) to propose synthetic routes, recently producing recommendations judged plausible by chemists.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Computer-Aided Synthesis Planning (CASP) systems",
            "system_description": "CASP systems range from heuristic rule-based planners (WODCA, OCSS) to modern systems that extract templates algorithmically and use learned value functions or neural action policies integrated with Monte Carlo tree search to propose retrosynthetic disconnections and full synthetic routes.",
            "discovery_domain": "Synthetic organic chemistry / synthesis route discovery",
            "discovery_description": "These systems propose retrosynthetic disconnections and multi-step synthetic routes for target molecules, including suggesting steps that are judged by human experts as comparable to literature routes; they can search large template libraries or learn action policies from reaction databases.",
            "discovery_type": "incremental",
            "discovery_type_justification": "The review positions CASP outputs as synthetic proposals within existing chemistry paradigms; while they can propose novel disconnections, they are framed as practical planning tools rather than transformational scientific discoveries.",
            "evaluation_methods": "Evaluation by human expert assessment (including double-blind studies where chemists judged machine-suggested routes as equally plausible to literature ones), route length and estimated synthetic complexity metrics, and synthesizability / starting-material availability heuristics.",
            "validation_approaches": "Expert chemist review, experimental implementation when undertaken, and comparison to known literature pathways; value-function metrics for synthetic tractability are used as proxies.",
            "novelty_assessment": "Novelty determined by whether suggested disconnections or full routes are unseen in the literature or provide more efficient/feasible pathways; novelty often requires experimental execution to be confirmed.",
            "impact_metrics": "Qualitative human-assessment outcomes (e.g., in a double-blind study, machine recommendations were considered equally plausible to literature pathways); metrics such as estimated synthetic complexity or route length are used as proxies.",
            "comparison_to_human_discoveries": true,
            "comparison_details": "Double-blind evaluation found machine-generated retrosynthetic suggestions to be judged as plausible as literature routes by chemists; learned policies can outperform simple heuristics in generating shorter or more target-aligned routes.",
            "success_rate": null,
            "challenges_limitations": "Validation bottleneck because experimental synthesis is required to fully validate novel retrosynthetic proposals; constrained by template coverage and quality of reaction databases; estimation proxies for synthesizability are imperfect.",
            "has_incremental_transformational_comparison": true,
            "uuid": "e1451.6",
            "source_info": {
                "paper_title": "Autonomous discovery in the chemical sciences part I: Progress",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "Gao et al. reaction condition model",
            "name_full": "Data-driven reaction condition recommendation model (Gao et al.)",
            "brief_description": "A model trained on literature reaction data (Reaxys) to propose reagents, catalysts, solvents, and temperatures as suitable reaction conditions, learning continuous embeddings for chemicals reflecting synthetic function.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Gao et al. condition recommender",
            "system_description": "A data-driven classification/regression approach that, given reactant and product structures, predicts categorical choices (reagent, catalyst, solvent identities) and continuous variables (e.g., temperature) by learning embeddings from a large literature database.",
            "discovery_domain": "Synthetic chemistry / reaction condition prediction",
            "discovery_description": "The model recommends reaction conditions for proposed transformations by learning mappings from reactant/product structures to likely conditions found in the literature, effectively recommending feasible conditions without human-encoded rules.",
            "discovery_type": "incremental",
            "discovery_type_justification": "Characterized as a recommender that learns from existing literature precedents and proposes likely conditions rather than discovering fundamentally new reaction mechanisms or paradigms.",
            "evaluation_methods": "Trained and evaluated on literature data (Reaxys) with classification/regression metrics for condition prediction accuracy; embedding quality inferred by downstream recommendation performance.",
            "validation_approaches": "Comparison of predicted conditions to reported literature conditions; implied need for experimental validation for novel condition proposals.",
            "novelty_assessment": "Novelty is in automating condition selection and learning chemical function embeddings; the model's recommendations are judged novel if they propose conditions not commonly reported or if they enable successful new experiments upon testing.",
            "impact_metrics": "Prediction accuracy on literature-conditioned datasets and qualitative downstream usefulness (not quantified in the review).",
            "comparison_to_human_discoveries": false,
            "comparison_details": "",
            "success_rate": null,
            "challenges_limitations": "Depends on literature coverage and biases; when only positive (successful) examples exist, the model treats reported conditions as adequate, which may limit learning of failure modes; experimental validation of novel suggestions remains essential.",
            "has_incremental_transformational_comparison": true,
            "uuid": "e1451.7",
            "source_info": {
                "paper_title": "Autonomous discovery in the chemical sciences part I: Progress",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "Deep generative models (GANs/VAEs)",
            "name_full": "Deep generative molecular design (GANs, VAEs, other generative models)",
            "brief_description": "Neural generative architectures that learn latent distributions over chemical structures to sample diverse candidate molecules for downstream screening and synthesis, facilitating exploration of vast chemical space.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Deep generative molecular models (GANs / VAEs / related)",
            "system_description": "Generative adversarial networks, variational autoencoders, and related deep models that learn continuous latent representations of molecules enabling sampling and optimization for desired properties; often coupled to property predictors or optimization routines.",
            "discovery_domain": "Molecular discovery / materials discovery / drug design",
            "discovery_description": "Used to propose candidate molecules or materials by sampling learned distributions or by latent-space optimization toward properties of interest; these proposals serve as inputs to virtual or experimental screening for discovery of new functional matter.",
            "discovery_type": "incremental",
            "discovery_type_justification": "Presented as tools to expand and sample chemical space and propose candidates; the review frames them as accelerating candidate generation within existing discovery pipelines rather than as engines of paradigm-shifting theory.",
            "evaluation_methods": "Evaluation often via property-prediction scores, diversity/novelty metrics, synthetic accessibility proxies, and downstream validation success (virtual screening hit rates or experimental confirmation).",
            "validation_approaches": "Virtual screening followed by experimental synthesis/testing for promising candidates; metrics include hit identification and property confirmation.",
            "novelty_assessment": "Novelty assessed by structural novelty relative to training data, predicted property extremeness, and whether experimental testing confirms previously unknown high-performing compounds.",
            "impact_metrics": "Proxy metrics include novelty/diversity, predicted property scores, and downstream hit rates upon experimental validation; no universal numeric thresholds provided in the review.",
            "comparison_to_human_discoveries": false,
            "comparison_details": "",
            "success_rate": null,
            "challenges_limitations": "Sampling may produce synthetically inaccessible or unstable molecules; evaluation is often limited by the fidelity of property predictors and the cost of experimental validation; disentangling true discovery from rediscovery of known chemistry can be challenging.",
            "has_incremental_transformational_comparison": true,
            "uuid": "e1451.8",
            "source_info": {
                "paper_title": "Autonomous discovery in the chemical sciences part I: Progress",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "Active learning / Bayesian optimization frameworks",
            "name_full": "Active learning and Bayesian optimization experimental design frameworks",
            "brief_description": "Iterative experimental design strategies that select experiments to maximally improve predictive models or to directly optimize objectives, thereby reducing the number of required experiments compared to brute-force search.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Active learning and Bayesian optimization",
            "system_description": "Algorithmic frameworks that use uncertainty-aware models and acquisition functions to select the most informative or promising experiments at each iteration (active learning) or to balance exploration and exploitation for objective optimization (Bayesian optimization), often coupled with automated experimentation.",
            "discovery_domain": "General experimental sciences (chemistry, materials, process optimization, etc.)",
            "discovery_description": "Applied to optimize reaction conditions, identify new reactions/materials, or learn structure-property models with far fewer experiments than unguided search; can enable closed-loop autonomous discovery when coupled with automated lab hardware.",
            "discovery_type": "incremental",
            "discovery_type_justification": "Described as efficiency and acceleration methods—reducing experimental burden and navigating high-dimensional spaces more intelligently—rather than methods that by themselves produce transformational theoretical breakthroughs.",
            "evaluation_methods": "Measured by reduction in number of experiments needed to reach a performance target (e.g., active learning may require only ~20% of experiments compared to exhaustive search as noted in the review), model improvement metrics, and optimization of objective values achieved.",
            "validation_approaches": "Empirical demonstration in iterative experimental campaigns with comparison to baseline strategies (random or brute-force sampling), and assessment of resource savings and model accuracy improvements.",
            "novelty_assessment": "Novelty assessed by whether the strategy discovers solutions that would be missed or require far more resources under brute-force search; degree of novelty depends on design space and constraints.",
            "impact_metrics": "Experimental-effort reduction (e.g., percent of experiments saved), model performance improvements, and improved objective values obtained; the review cites an example claim that active strategies might need only ~20% of experiments to find an optimal solution in some settings.",
            "comparison_to_human_discoveries": false,
            "comparison_details": "",
            "success_rate": null,
            "challenges_limitations": "Effectiveness depends on model quality, representation, and experimental noise; in continuous or enormously large spaces, brute-force comparisons are not always meaningful; closing the loop requires integrated automated execution which is nontrivial.",
            "has_incremental_transformational_comparison": true,
            "uuid": "e1451.9",
            "source_info": {
                "paper_title": "Autonomous discovery in the chemical sciences part I: Progress",
                "publication_date_yy_mm": "2020-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [],
    "cost": 0.017839,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Autonomous discovery in the chemical sciences part I: Progress</h1>
<p>Connor W. Coley<em>! Natalie S. Eyke, Klavs F. Jensen</em>§</p>
<p>Keywords: automation, chemoinformatics, drug discovery, machine learning, materials science</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>Contents</h1>
<p>1 Abstract ..... 3
2 Introduction ..... 3
3 Defining discovery ..... 4
3.1 Classifications of discoveries ..... 4
3.2 Discovery as a search ..... 6
3.3 The role of validation and feedback ..... 7
4 Elements of autonomous discovery ..... 9
4.1 Assessing autonomy in discovery ..... 9
4.2 Enabling factors ..... 12
5 Examples of (partially) autonomous discovery ..... 15
5.1 Foundational computational reasoning frameworks ..... 15
5.2 Discovery of mechanistic models ..... 17
5.2.1 Discovery of detailed kinetic mechanisms ..... 17
5.3 Noniterative discovery of chemical processes ..... 19
5.3.1 Discovery of new synthetic pathways ..... 19
5.3.2 Discovering models of chemical reactivity ..... 21
5.3.3 Discovery of new chemical reactions from experimental screening ..... 25
5.4 Iterative discovery of chemical processes ..... 25
5.4.1 Discovery of optimal synthesis conditions ..... 25
5.4.2 Discovery of new chemical reactions through an active search ..... 28
5.5 Noniterative discovery of structure-property models ..... 29
5.5.1 Discovery of important molecular features ..... 30
5.5.2 Discovery of models for spectral analysis ..... 31
5.5.3 Discovery of potential energy surfaces and functionals ..... 32
5.5.4 Discovery of models for phase behavior ..... 33
5.6 Noniterative discovery of new physical matter ..... 33
5.6.1 Discovery through brute-force experimentation ..... 34
5.6.2 Discovery through computational screening ..... 36
5.6.3 Discovery through molecular generation ..... 39
5.7 Iterative discovery of new physical matter ..... 42
5.7.1 Discovery for pharmaceutical applications ..... 42
5.7.2 Discovery for materials applications ..... 45
5.8 Brief summary of discovery in other domains ..... 50
6 Conclusion ..... 51
7 Acknowledgements ..... 51</p>
<h1>1 Abstract</h1>
<p>This two-part review examines how automation has contributed to different aspects of discovery in the chemical sciences. In this first part, we describe a classification for discoveries of physical matter (molecules, materials, devices), processes, and models and how they are unified as search problems. We then introduce a set of questions and considerations relevant to assessing the extent of autonomy. Finally, we describe many case studies of discoveries accelerated by or resulting from computer assistance and automation from the domains of synthetic chemistry, drug discovery, inorganic chemistry, and materials science. These illustrate how rapid advancements in hardware automation and machine learning continue to transform the nature of experimentation and modelling.</p>
<p>Part two reflects on these case studies and identifies a set of open challenges for the field.</p>
<h2>2 Introduction</h2>
<p>The prospect of a robotic scientist has long been an object of curiosity, optimism, skepticism, and job-loss fear, depending on who is asked. As computing was becoming mainstream, excitement grew around the potential for logic and reasoning-the underpinnings of the scientific process-to be codified into computer programs; as hardware automation became more robust and cost effective, excitement grew around the potential for a universal synthesis platform to enhance the work of human chemists in the lab; and as data availability and statistical analysis/inference techniques improved, excitement grew around the potential for statistical models (machine learning included) to draw new insights from vast quantities of chemical information $[1-7]$.</p>
<p>The confluence of these factors makes that prospect increasingly realistic. In organic chemistry, we have already seen proof-of-concept examples of the "robo-chemist" [8] able to intelligently select and conduct experiments [9-11]; there have even been strides made toward a universal synthesis platform [12-15], theoretically capable of executing most chemical processes but highly constrained in practice. While there have been fewer success stories in automating drug discovery holistically [16], the excitement around machine learning in this application space is especially apparent, with dozens of start-up companies promising to revolutionize the development of new medicines through artificial intelligence [17].</p>
<p>A more pessimistic view of automated discovery is that machines will never be able to make real "revolutions" in science because they necessarily operate within a specific set of instructions [18]. This attitude is exemplified by Lady Lovelace's objection: "The Analytical Engine has no pretensions to originate anything. It can do whatever we know how to order it to perform. It can follow analysis; but it has no power of</p>
<p>anticipating any analytical relations or truths" [1]. Some have expressed a milder sentiment, perhaps in light of advances in computing, cautioning that an increasing reliance on robotic tools might reduce the odds of a serendipitous discovery [6]. Muggleton is more declarative, stating that "science is an essentially human activity that requires clarity both in the statement of hypotheses and their clear and undeniable refutation through experimentation" [19]. However, there is little disagreement that automation and computation in science has improved productivity through efficiency, reduction of error, and the ability to address large-scale problems [20].</p>
<p>In the remainder of Part 1, we will discuss the different types of discovery typically reported in the chemical sciences and how they can be unified as searches in a high-dimensional design space. Along with this definition comes a recommended set of questions to ask when evaluating the extent to which a discovery can be attributed to automation or autonomy. We will then discuss a number of case studies arranged in terms of the type of discovery being pursued and the nature of the approach used to do so. Part 2 will reflect on these case studies and make explicit what we believe to be the primary obstacles to autonomous discovery.</p>
<h1>3 Defining discovery</h1>
<h3>3.1 Classifications of discoveries</h3>
<p>There is no single definition of what constitutes a scientific discovery. Valdés-Pérez defines discovery as "the generation of novel, interesting, plausible, and intelligible knowledge" [5]. Data-driven knowledge discovery, specifically, has been defined as the "nontrivial extraction of implicit, previously unknown, and potential useful information" [21]. Each of these criteria, however, is inherently subjective. "Novel" is simultaneously ambiguous and considered distinct from "new"; it is generally meant to indicate some level of nonobviousness or, by one definition, a lack of predictability [22]. However, if we artificially limit what we consider to be known and demonstrate a successful extrapolation to a conclusion that really was known, it would be reasonable to argue that this does not constitute a discovery. This connects to the question of what it might mean for a discovery to be "interesting" or "useful", for which we avoid providing a precise definition.</p>
<p>For the purposes of this review, we instead define three broad types of discoveries in the chemical sciences (Figure 1) and provide examples of each.</p>
<p>Physical matter. Often, the ultimate result of a discovery campaign is the identification of a molecule (not discounting macromolecules), material, or device that achieves a desired function. This category encompasses most drug discovery efforts, where the output may be new chemical matter that could later become</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The three broad categories of discovery described in this review: physical matter, processes, and models.
part of a therapeutic, as well as materials discovery for a wide array of applications.
Processes. Discoveries may also take the form of processes. These may be abstract, like the HaberBosch process, pasteurization, and directed evolution. They are more often concrete, like synthetic routes to organic molecules or a specific set of reaction conditions to achieve a chemical transformation.</p>
<p>Models. Our definition of a model includes empirical models (such as those obtained through regression of experimental data), structure-function relationships, symbolic regressions, natural laws, and even conceptual models that provide mechanistic understanding. It is common for models to be part of the discovery of the other two types as surrogates for experiments, as will be seen in many examples below.</p>
<p>The most famous examples of scientific discoveries in chemistry tend to be natural laws or theories that are able to rationalize observed phenomena that previous theories could not. Mendeleev's periodic table of the elements, Thomson's discovery of the electron, Rutherford's discovery of atomic nuclei, the Schrodinger equation, Kekulé's structure of benzene, et cetera. In their time, these represented radical departures from previous frameworks. Though we do consider these to be models, identifying them through computational or algorithmic approaches would require substantially more open-ended hypothesis generation than what is currently possible.</p>
<h1>3.2 Discovery as a search</h1>
<p>We argue that the process of scientific discovery can always be thought of as a search problem, regardless of the nature of that discovery $[7,23,24]$.</p>
<p>Molecular discovery is a search within "chemical space" [25-28]-an enormous combinatorial design space of theoretically-possible molecules. A common estimate of its size, considering only molecules of a limited size made up of CHONS atoms, is $10^{60}$ [29], although for any one application or with reasonable restrictions (e.g., on drug-likeness or synthetic accessibility), the size of the relevant chemical space will be significantly smaller [30, 31]. Biological compounds exist in an even larger space if one considers that there are, e.g., $20^{100}$ theoretically-possible 100 -peptide proteins using only canonical amino acids, although again the number that are foldable and biologically relevant will be significantly smaller. Materials discovery is another combinatorial design space, where structural composition must be defined by both discrete variables (e.g., elemental identities) and continuous variables (e.g., stoichometric ratios) and processing conditions. The design space for a device is even larger, as it compounds the complexity of its constituent components with additional considerations about its geometry.</p>
<p>Discovering a chemical or physical process is the result of searching a design space defined by process variables and/or sequences of operations. For example, optimizing a chemical reaction for its yield might involve changing species' concentrations, the reaction temperature, and the residence time [32]. It may also include selecting the identity of a catalyst as a discrete variable [33], or changing the order of addition [34]. A new research workflow can be thought of as the identification of actions to be taken and their timing, such as the development of split-and-pool combinatorial chemistry for diversity-oriented synthesis [35] or a screening and selection strategy for directed evolution [36].</p>
<p>The majority of models that are "discovered", under our broad definition, are empirical relationships that come from data fitting. In these cases, the search space is well-defined once an input representation (e.g., a set of descriptors or parameters) and a model family (e.g., a linear model, a deep neural network) are selected. While this can present a massive search space when considering all possible values of all learned parameters (e.g., for deep learning regression techniques), the final model is often the result of a simplified, local search from a random initialization (e.g., using stochastic gradient descent). Symbolic regressions are searches in a combinatorial space of input variables and mathematical operations [37]. More abstract models, like mechanistic explanations of natural phenomena, exist in a high-dimensional hypothesis space that is difficult to formalize; automated discovery tools that are able to generate causal explanations do so using simplified terminology and well-defined ontologies [38].</p>
<p>In virtually every case of computer-assisted discovery, the actual search space is significantly larger than</p>
<p>what the program or platform is allowed to explore. We might decide to focus our attention on a specific set of compounds (e.g., a fixed scaffold), a specific class of materials (e.g., perovskites), a specific step in a catalyst synthesis process with a finite number of tunable process variables (e.g., the temperature and time of an annealing step), or a specific hypothesis structure (e.g., categorizing a ligand's effect on a protein as an agonist, antagonist, promoter, etc.). Constraining the search space is one way of integrating domain expertise/intuition into the discovery process. Moreover, it can greatly simplify the search process and mitigate the practical challenges of automated validation and feedback.</p>
<h1>3.3 The role of validation and feedback</h1>
<p>The way that we navigate the search space in a discovery effort is often iterative. Classically, the discovery of physical matter, such as in lead optimization for drug discovery, is divided into stages of design, make, test. An analogous cycle for searching hypothesis space could be described as hypothesize, validate, revise beliefs.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Simplified schematic of a hypothesis-driven (or model-driven) discovery process. When not proceeding iteratively, new information is not used to revise our belief (current knowledge). Lowercase roman numerals (red) correspond to the questions for assessing autonomy in discovery.</p>
<p>This third step-test or revise beliefs-helps to explain the role of validation and feedback in discovery: experiments, physical or computational, serve to support or refute hypotheses. When information is imperfect or insufficient to lead to a confident prediction, it is important to collect new information to improve our understanding of the problem. This might mean taking an empirical regression fit to a small number of data points, evaluating our uncertainty, and performing follow-up experiments to reduce our uncertainty in regions where we would like to have a more confident prediction (Figure 2). Purely virtual screening is not sufficient for drug discovery [39], where experimental validation continues to be essential [40]; Schneider and Clark describe experimental testing of drugs designed using de novo workflows as a "non-negotiable" criterion [41]. In the materials space as well, Halls and Tasaki propose a materials discovery scheme in which synthesis,</p>
<p>characterization, and testing are critical components [42]. The scope of hypotheses that lend themselves to automated validation has limited the scope of discovery tasks that are able to be automated.</p>
<p>Consider a scenario where we have a large data set of molecular structures and a property of interest, like their in vitro binding affinity for a particular protein target. We can perform a statistical regression to correlate the two and represent our understanding of the structure-function landscape. Based on that model, we may propose a new structure-a compound not yet tested-that is predicted to have high activity. Whether that constitutes discovery of the compound is ambiguous. Using scientific publication as the bar, it is reasonable to expect a high degree of confidence, regardless of whether that confidence arises from a statistical analysis of existing data or from confirmation through acquisition of new data. Even with a highly accurate model, performing a large virtual screen could lead to thousands of false positive results [31]. For a philosophical discussion about the nature of knowledge and need for confidence, correctness, and justification, see the description of the Gettier problem in ref. 43.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: One way to visualize the discovery process. The goal definition will implicitly or explicitly define the search space within which we operate. Available structured information can be used to generate or refine a hypothesis within that search space. Often, when we are doing more than pure data analysis, there will be an iterative process of information gathering prior to the final output, or interpretation.</p>
<p>We note here that this hypothesis-first approach to discovery (Figure 3) is more consistent with the philosophy of Popper [44]. This is in contrast to an observation- or experiment-first approach, which is more consistent with the philosophy of Bacon [45]; data mining studies tend to be Baconian [46]. In practice, when discovery proceeds iteratively, the distinction between the two is simply where one enters the cycle. Both are types of model-guided discovery, which is distinct from brute-force screening or approaches relying solely on serendipity that we discuss later.</p>
<h1>4 Elements of autonomous discovery</h1>
<p>It is impossible to imagine conducting research without some degree of machine assistance, defining "machine" broadly. We rely on computers to organize, analyze, and visualize data; analytical instruments to queue samples, perform complex measurements, and convert them into structured data. However, it is important to consider precisely what is facilitated by automation or computer-assistance in terms of the broader discovery process. Many technologies (e.g., NMR sample changers) add a tremendous amount of convenience and reduce the manual burden of experimentation, but provide only a modest acceleration of discovery rather than a fundamental shift in the way we approach these problems. Considering the cognitive burden of experimental design and analysis connects to the distinction between autonomy and automation. A toy slot car that sets its own speed as it proceeds through a fixed track is qualitatively different from a self-driving car in the city, yet each successfully operates within its defined environment. Though there is no precise threshold between automation and autonomy, autonomy generally implies some degree of decision-making and adaptability in response to unexpected outcomes.</p>
<h3>4.1 Assessing autonomy in discovery</h3>
<p>Here, we propose a set of questions to ask when evaluating the extent to which a discovery process or workflow is autonomous: (i) How broadly is the goal defined? (ii) How constrained is the search/design space? (iii) How are experiments for validation/feedback selected? (iv) How superior to a brute force search is navigation of the design space? (v) How are experiments for validation/feedback performed? (vi) How are results organized and interpreted? (vii) Does the discovery outcome contribute to broader scientific knowledge? These questions are mapped onto the schematic for hypothesis-driven discovery in Figure 2.
(i) How broadly is the goal defined? While algorithms can be made to exhibit creativity (e.g., coming up with a unique strategy in Go or Chess [47, 48]), at some level, they do so for the sake of maximizing a human-defined objective. Is the goal defined at the highest level possible (e.g., find an effective therapeutic)? Or is it narrow (e.g., find a molecule that maximizes this black-box property for which we have an assay and preliminary data)? The higher the level at which the mission can be defined, the more compelling the discovery becomes. That requires platforms to understand what experiments can be performed and how they are useful for the task at hand.
(ii) How constrained is the search/design space? An unconstrained search space is one that we operate in as human researchers. There are many ways in which humans can artificially constrain the search space available to an autonomous platform. A maximally constrained search space in the discovery</p>
<p>of physical matter could be a (small) fixed list of candidates over which to screen. Limitations in the experimental and computational capabilities of an autonomous platform have the effect of constraining the search space as well; the scientific process itself has been described by some as a dual search in a hypothesis space and experimental space [24, 49]. How these constraints are defined influences the difficulty of the search process, the likelihood of success, and the significance of the discovery. The fewer the constraints placed on a platform, the greater the degree to which it can be said to be operating autonomously.
(iii) How are experiments for validation/feedback selected? Unconstrained experimental design is a complex process requiring evaluation of local decisions as well as a global strategy for the overall timeframe, coherency, and scientific merit of a proposed experiment [50]. When operating within a restricted experimental space, design can be simplified to local decisions of specific implementation details without these high-level decisions. Cummings and Bruni define a taxonomy for human-automation collaboration in terms of the three primary roles played by a human or computer: moderator (of the overall decision-making process), generator (of feasible solutions), and decision-maker (of which action to take) [51]. Their levels of automation include ones where humans must take all decisions/actions, where the computer narrows down the selection, where the computer executes one if the human approves, and where the computer executes automatically and informs the human if necessary. The second level is typical for the discovery of new physical matter, where computational design algorithms may propose compounds that must be subjected to a manual assessment of synthesizability before being manually synthesized. The smaller the search space and the cheaper the experiments-including considerations of time and risk of failure-the less human intervention is required in selecting experiments.
(iv) How superior to a brute force search is navigation of the design space? This question seeks to identify the extent to which there is "intelligence" in the search strategy. Langley et al.'s notion of discovery as a heuristic search emphasizes this criterion [23]. Whether or not the strategy is more effective than a brute force search depends on the size of the space and how experiments are selected. For example, a high throughput screen of compounds from a fixed library is equivalent to a brute-force search. An active learning strategy designed to promote exploration might require only $20 \%$ of the experiments to find an optimal solution. When dealing with continuous (e.g., process variables) or virtually infinite (e.g., molecular structure) design spaces, it is not possible to quantify meaningfully the number of experiments in a brute-force search.
(v) How are experiments for validation/feedback performed? Being able to automatically gather new information to support/refute a hypothesis is an important aspect of an automated discovery</p>
<p>workflow. At one extreme, experiments are performed entirely by humans (regardless of how they are proposed); in the middle, experiments might be performed semi-automatically but require significant human set-up between experiments; at the other extreme, experiments can be performed entirely without human intervention. This question is tightly coupled to that of who chooses the experiments and the size of the search space. The narrower the experimental design space, the more likely it is that validation/feedback can be automated. In computational studies, it is relatively straightforward to automate simulations if we are willing to discard failures without manual inspection (e.g., DFT simulations that fail to converge).
(vi) How are results organized and interpreted? In an iterative workflow, the results of information gathering (experiments, simulations) are organized as structured information and used to update our prior knowledge and revise our beliefs before the next round of experimental design. Provided that the experiments/simulations can be designed to produce information that is already in a compatible format (e.g., quantifying a reaction yield to build a model of yield as a function of process variables), this is simply a practical step toward closing the loop. In a few specialized workflows, experimental results naturally drive the selection of subsequent experiments, as in directed evolution and phase-assisted continuous evolution [52].
(vii) (optional) Does the outcome contribute to broader scientific knowledge? Though not necessarily related to the concept of autonomy, this question speaks to impact and intelligibility. Does it require extensive interpretation after the fact to evaluate how or what it has learned, or is it selfexplanatory? Intelligibility is one of the criteria for discovery put forward by Valdés-Pérez [5], among others. Describing physical phenomena requires far less domain knowledge than does explaining those phenomena [53]. Especially in empirical modeling, there is often a dichotomy between models built for accurate predictions and models built for explanatory predictions [54, 55]. Turing made note of this at least as early as 1950, saying that "an important feature of a learning machine is that its teacher will often be very largely ignorant of quite what is going on inside, although he may still be able to some extent to predict his pupil's behavior" [1]. The past few years have seen an interest in the transparency, interpretability, and explainability of machine learning models, not just the accuracy [56].</p>
<p>Several of these questions probe the extent to which discovery is "closed loop", which implicitly assumes an iterative process of multiple hypothesize-test-revise beliefs cycles. Iterative refinement is crucial when operating inside poorly-explored design spaces (e.g., using an uncommon scaffold) or with new objective functions (e.g., maximizing binding to a new protein target in vitro). Most of the case studies described in the following sections are better described as "open loop" and involve only certain aspects of the work-</p>
<p>flow in Figure 2. For example, a common paradigm of computer-aided discovery is to define an objective function, perform a large-scale data mining study, propose a solution of new molecule, material, and/or model, and manually validate a small number of those predictions. Waltz and Buchanan describe many early computational discovery programs as merely running calculations, rather than trying to close the loop $[20]$.</p>
<h1>4.2 Enabling factors</h1>
<p>A confluence of improved data availability, computing abilities, and experimental capabilities have brought us substantially closer to autonomous discovery (Figure 4). These improvements contribute to two categories of methodological progress: (1) techniques for navigating the search space more effectively, and (2) techniques for accelerating validation/feedback. Many machine learning techniques, for example, have been used to build empirical models within the search space to enable or accelerate the search; mapping the design space for a molecule, material, device, or process to relevant performance metrics is a prerequisite for any "rational design".
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The factors that have enabled autonomous discovery fall into one of three main categories.</p>
<p>As Claus and Underwood point out, effective discovery requires assimilation of knowledge contained in large quantities of data of diverse types [57]. The quantity of chemical property and process data available in journals, the patent literature, and online databases makes it challenging to analyze by hand. Digitization of organic reaction information into computer-readable databases like Reaxys, SPRESI, CASREACT, and Lowe's USPTO dataset has not just facilitated searching that information, but has enabled new analyses thereof [58]. Millions of bioactivity measurements are found in databases like ChEMBL and PubChem, not to mention the dozens of genomic, metabolomic, and proteomic databases that have emerged in the last decade $[59,60]$. There are also many repositories for experimental and computational properties of materials, which</p>
<p>have facilitated the construction of empirical models to predict new material performance [61, 62]. Gil et al. [63] discusses the utility of AI techniques in searching and synthesizing large amounts of information as part of "discovery informatics" [57, 64, 65]. Even now, an enormous amount of untapped information remains housed in laboratory notebooks and journal articles. For such information to be directly usable, someone must undertake the challenge of compiling the data into an accessible, user-friendly format and overcome any intellectual property restrictions. Image and natural language processing techniques can make this task less burdensome; there is increasing interest in adapting such information extraction algorithms for use in chemistry $[66-71]$.</p>
<p>Autonomous discovery systems rely on a variety of computational tools to generate hypotheses from data without human intervention. This includes both the software that makes the recommendations (e.g., proposes correlations, regresses models, selects experiments) as well as the underlying hardware that makes using the software tractable. Our discussion of the advances in this area focuses on software developments with an emphasis on machine learning algorithms, which has elicited cross-disciplinary excitement [72-75].</p>
<p>Typically, search domains that are of interest for discovery are characterized by high dimensionality (e.g., chemical space). In such domains, the patterns within the available data may be beyond the capacity of humans to infer a priori without years of intuition-building practice. Machine learning and pattern recognition algorithms can be used to discover these regularities automatically, e.g., by using the available data to parameterize a neural network model [76]. Varnek and Baskin and Mitchell provide overviews of machine learning techniques as applicable to common cheminformatics problems [77, 78] and brief tutorials can be found in a number of reviews [79-82]. It is becoming increasingly common to use machine learning to develop empirical quantitative structure-activity/property relationships (QSARs/QSPRs) to score molecules and guide virtual screening as part of broader discovery frameworks [83]. These models can be used to distinguish promising compounds from unpromising ones and prioritize molecules for synthesis and testing (validation), thus facilitating the extrapolation of information about existing molecules to novel molecules that exist only in silico [31].</p>
<p>Algorithms that enable efficient navigation of design spaces represent an important set of computing advances. Even with a model representing our belief about a physical structure-property relationship, an algorithmic framework is needed to apply that belief to experimental design. These frameworks include active learning strategies [84] that aim to maximize the accuracy of predictive models while minimizing the required training data, as well as goal-directed strategies such as Bayesian optimization [85] and genetic algorithms [86]. These iterative techniques can reduce the experimental burden associated with discovery in domains or search spaces where exhaustive testing is not practical.</p>
<p>Algorithms that are capable of directly proposing candidate molecules or materials (physical matter)</p>
<p>as a form of experiment selection are worth special emphasis. Recently, deep generative models [87] such as generative adversarial networks (GANs) [88] and variational autoencoders (VAEs) [89] have attracted a great deal of interest, as they facilitate the creation of diverse molecular libraries without the impossible task of systematically enumerating all potential functional compounds [90-92]. Many case studies that leverage these and related frameworks for the discovery of physical matter are described later.</p>
<p>Experimental advances toward autonomous discovery include automation of well-established laboratory workflows (along with parallelization and miniaturization) as well as entirely novel synthetic and analytical methodologies. Aspects of experimental validation (Figure 5) have existed in an automated format for decades (e.g., addition to and sampling from chemical reactors [93, 94]), and many of the requisite hardware units have been commercialized (e.g., liquid handling platforms and plate readers available through companies such as Beckman, Hamilton, BioTek, and Tecan). However, moving beyond piecemeal automation to the entire experimental burden of discovery workflows is challenging. Each process step, which may include synthesis, purification, assay preparation, and analysis, must be seamlessly integrated for the platform to operate without manual intervention; each interface presents new potential points of failure [95].
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Generic workflow for experimental validation.</p>
<p>The complexity of software required for hardware automation ranges from sequencing commands from a fixed schedule [15], to real-time control and optimization [10], to higher-level scheduling and orchestration [96]; user interface driven software such as LabVIEW [97] can aid the creation of fit-for-purpose control systems with minimal programming experience. Although end-to-end automation of an experimental discovery workflow is uncommon, there are numerous benefits to be gained even from partial automation, chief among these being standardization and increased throughput [98].</p>
<p>In addition to automation, novel experimental methodologies have been developed that lend themselves particularly well to autonomous discovery workflows by facilitating the exploration of broad design spaces, a helpful feature that increases the likelihood of discovery [99]. These include synthesis-focused methodologies, such as DNA-encoded libraries [100] and diversity-oriented synthesis [35, 101], as well as analysis-focused methodologies, such as ambient mass spectrometry [102] and MISER for accelerating liquid chromatographic analysis $[103]$.</p>
<p>The three categories of enabling factors described herein facilitate discovery in different ways: data is leveraged to create models that inform and predict, computational tools are used to create models from data and reason about which experiments to perform next, and physical (or computational) experiments validate hypotheses and facilitate refinement thereof. These factors can be strategically combined to give rise to different types of studies. For example, the experimental capabilities described here, in isolation, can be used for high-throughput, brute-force screening; computational tools can be used for data generation (through, e.g., DFT simulations); virtual screening is achieved through the combination of data and algorithms; and integration of all three is needed for fully autonomous discovery.</p>
<h1>5 Examples of (partially) autonomous discovery</h1>
<p>In this section, we summarize a series of case studies that demonstrate how automation and machine autonomy influence discovery in various research domains. The extent to which techniques in automation and computation have enabled each case varies. Some only benefit from automated laboratory hardware, others learn underlying trends from large or complex data, and still others use computational techniques to efficiently explore high dimensional design spaces.</p>
<p>Specifically, subsection 5.1 describes early computational reasoning frameworks; 5.2 describes the discovery of mechanistic models; 5.3 and 5.4 describe the noniterative and iterative discovery of chemical processes; 5.5 describes the noniterative discovery of property models; 5.6 and 5.7 describe the noniterative and iterative discovery of physical matter; finally, 5.8 provides a brief summary of a few tangentially-related domains.</p>
<h3>5.1 Foundational computational reasoning frameworks</h3>
<p>There has been a long-standing fascination with the philosophical question of whether or not it is possible to codify and automate the process of discovery [104]. In the 1980s and 1990s, several programs were developed to mimic a codifiable approach to discovery and to reproduce specific quintessential discoveries of models, led by Langley and Zytkow [6]. These programs deal with questions of model induction and hypothesis generation (as a form of data analysis) rather than experimental selection and automated validation/feedback.</p>
<p>BACON is a rule-based framework introduced in 1978 to formalize the Baconian method of inductive reasoning to discover empirical laws, supplemented with data-driven heuristics [105]. BACON.4, a later iteration specifically designed for chemical problems, searched for arithmetic combinations of input variables to identify regularities in data (e.g., noting that pressure times volume is invariant for constant temperature in a closed gas system) [7]. This approach was able to recapitulate Ohm's law, Archimedes' law of displacement, Snell's law, conservation of momentum, gravitation, and Black's specific heat law [106]. The search for an</p>
<p>empirical relationship was greatly simplified by excluding any irrelevant variables (i.e., all input variables were known to be important) and eliminating all measurement noise. Extensions of this approach included describing piecewise functions (FARENHEIT [107]) and coping with irrelevant observations and noise (ABACUS [108]). More recently, Schmidt and Lipson demonstrated that using a symbolic regression framework similar to BACON, it is possible to rediscover Hamiltonians, Lagrangians, and geometric conservation laws from empirical motion tracking data [37]. Much like its predecessors, their program uses a two-part process of generating and scoring hypothesized analytical laws.</p>
<p>The STAHL program developed by Zytkow and Simon in the mid-1980s sought to automate the construction of compositional models to, e.g., rediscover Lavoisier's theory of oxygen [109]. It operates on a list of chemical reactions to produce a list of proposed chemical elements and the compounds they make up by making inferences like " $\mathrm{A}+\mathrm{B}+\mathrm{C} \longrightarrow \mathrm{B}+\mathrm{D} " \Longrightarrow$ " D is composed of A and C ". While the program was arguably successful in formalizing a specific form of scientific reasoning, the lack of any consideration for stoichometry, phase changes, and ability to consider uncertainty, competing hypotheses, and request information makes such a logic framework highly limited in utility. The KEKADA program [110] was designed with those abilities in order to replicate the discovery of the Krebs cycle. Using seven heuristic operators (hypothesis proposers, problem generators, problem choosers, expectation setters, hypothesis generators, hypothesis modifiers, and confidence modifiers) and simulated experiments of metabolic reactions, KEKADA was able to rediscover the Krebs cycle from the same empirical data that would have been obtainable at the time.</p>
<p>The knowledge bases for these early programs were comprised of expert-defined relationships, rules, and heuristics designed to reflect prior knowledge and bring the programs up to the level of domain experts. Programs based entirely on user-defined axioms have proved successful in automatic theorem generation in graph theory [111]. However, these rules bring at least two drawbacks in the context of inductive reasoning. The first is that it is more difficult for experts to recapitulate their knowledge through rules than by providing examples from which an algorithm can generalize [112]. The second is that too stringent priors may restrict the model from deviating far enough from existing theory to make a substantial discovery and merely "fill in the gaps" of what is known. Kulkarni and Simon argue that a lack of prior knowledge about allowed/disallowed reactions actually served to benefit Krebs, as a formally trained chemist might not have pursued a hypothesis that was-at the time-believed to be highly unlikely [110].</p>
<h1>5.2 Discovery of mechanistic models</h1>
<h3>5.2.1 Discovery of detailed kinetic mechanisms</h3>
<p>Computer assistance has proved useful in the exploration and simulation of reaction pathways [113-116]. The vast number of possible elementary reactions creates a combinatorial space of hypothesized pathways that is difficult to explore manually in an unbiased manner, making it a prime candidate for algorithmic approaches. One such approach, MECHEM, enumerates elementary reactions in catalytic reaction systems to identify series of mechanistic steps able to rationalize an observed global reaction [117-119]. Ismail et al. have demonstrated a similar approach to identifying multi-step reaction mechanisms for catalytic reactions using a ReaxFF potential energy surface [121] to guide the search toward kinetically-likely pathways [120]. In the absence of heuristics or calculations to drive the search, millions of possible elementary reactions can be generated even with species of just a few atoms [122].</p>
<p>The Reaction Mechanism Generator (RMG) fills a similar role in developing detailed kinetic mechanisms for combustion and pyrolysis processes [123]. Expert-defined reaction templates enumerate potential elementary reactions between a set of user-defined input molecules; rate constants for the forward and reverse reactions are estimated from a combination of first principles calculations (e.g., DFT) and group additivity rules regressed to experimental data. The ability to estimate kinetic and thermodynamic parameters enables the identification of new elementary reactions and pathways and, e.g., exploration of untested fuel additives' effects on ignition delay [124]. An earlier study by Broadbelt et al. used a similar approach to develop detailed kinetic models for pyrolysis reactions [125].</p>
<p>Mechanistic enumerations/searches have been applied extensively to the discovery of transition states and reaction channels [126-128]. These methods represent a search in the $(3 N-6)$-dimensional potential energy surface landscape implicitly defined by an $N$-atom pool of reacting species. Approaches like Berny optimization [129] are used to identify transition state (TS) geometries for the purposes of estimating energetic barrier heights. Double-ended search methods like the freezing string method (FSM [130]) or growing string method (GSM [131]) require knowledge of the product structure and run iterative electronic structure calculations to identify a plausible reaction pathway; these can be applied to the discovery of new elementary reactions by systematically enumerating potential product species [132-134] (Figure 7). Single-ended search methods operate on reactant species only and perturb the geometry along reactive coordinates, including, e.g., the artificial force induced reaction method (AFIR [135]). An alternate approach to reaction discovery is by direct simulation of reactive mixtures using molecular dynamics (MD) [136-138]. Wang et al. describe the use of an " $a b$ initio nanoreactor" to find unexpected products from similar starting materials to the Urey-Miller experiment on the origin of life [136]. Importantly, their approach does not require the use of</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Discovery of detailed kinetic models through iterative selection of important elementary reaction steps. Figure reproduced from Broadbelt et al. [125].
heuristics to define reaction coordinates or enumeration rules to define possible products. Instead, molecules in an MD simulation are periodically pushed toward the center to impart kinetic energy and encourage collisions at a rate that enables the observation of rare events over tractable simulation timescales. In principle, these can be to applied to the prospective prediction of novel reaction types and, ultimately, the development of new synthetic methodologies.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Workflow for identification of reaction networks between known reactants R and known products P through combinatorial enumeration of possible mechanistic steps pruned by calculated transition state energies. Figure reproduced from Kim et al. [134].</p>
<h1>5.3 Noniterative discovery of chemical processes</h1>
<h3>5.3.1 Discovery of new synthetic pathways</h3>
<p>Synthetic pathways are a prerequisite for physically producing a molecule of interest, whether for experimental validation of a predicted property or for production at scale. Retrospective analyses of known single-step chemical reactions can yield hypothesized synthetic pathways as combinations thereof. Gothard et al. describe an analysis of a "Network of Organic Chemistry"-a copy of the Beilstein database with seven million reactions-for the discovery of one-pot reactions; their search space comprised any consecutive sequence of known reactions where the product of one is a reactant of another [139]. Candidate sequences were evaluated using eight filters, including a $322 \times 322$ table of functional groups and their cross-reactivity and a $322 \times 97$ table of their compatibility under 97 categories of reaction conditions. Through application of these expert heuristics to millions of candidate sequences, the authors identified multi-step chemistries that could potentially be run without an intermediate purification, choosing a handful of such pathways for experimental validation. While their filters were all hand-encoded, data mining techniques can also be used to estimate functional group reactivity [140, 141]. Selecting pathways within a search space defined by combinations of known single-step reactions has taken on other forms as well, including the identification of cyclic pathways [142], the optimization of process cost [143], and the optimization of estimated process mass intensity [144].</p>
<p>Generating yet-unseen chemical reactions for a synthesis plan-a necessity for the synthesis of novel molecules-is a harder search problem than when searching within a fixed reaction network [145]. Because the number of states in a naive retrosynthetic expansion will scale as $b^{d}$ for branching factor $b$ and depth $d$, guiding the search is an essential aspect of computer-aided synthesis planning (CASP) programs. The breadth of the search depends on the coverage of the rule sets: abstracted enzymatic reactions tend to number in the hundreds [146], expert transformation rules often number in dozens or hundreds [147, 148] but can extend into the tens of thousands in contemporary programs [149], and algorithmically-extracted templates generally number in the thousands to hundreds of thousands [150-153]. To the extent that reaction rules and synthetic strategies can be codified, synthesis planning is highly conducive to computational assistance [58, 154-158] (Figure 8). CASP approaches that generate retrosynthetic suggestions without the use of pre-extracted template libraries $[159,160]$ still result in a large search space of possible disconnections.</p>
<p>Even the earliest CASP programs emphasized the importance of navigating the search space of possible disconnections [154, 161]. The search in OCSS was guided by five subgoals for structural simplification: reduce internal connectivity, reduce molecular size, minimize functional groups, remove reactive or unstable functional groups, and simplify stereochemistry [161]. Starting material oriented retrosynthesis introduces additional constraints in the search, as the goal state is a specific starting material, rather than one of many</p>
<p>from a database of available compounds [162]. It is only fairly recently that CASP tools have started to be used more widely for discovery of synthetic routes. Development is stymied by the complexities of validation and feedback, which can only occur by experimental implementation [163] or review by expert chemists [164].
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Workflow used by the WODCA program for computer-aided synthesis planning. Figure reproduced from Ihlenfeldt and Gasteiger [154].</p>
<p>There are two main approaches to navigating the search space during retrosynthetic expansion to determine which disconnections are most promising: value functions and action policies. Value functions estimate the synthetic complexity of reactant molecules as a proxy for how close they are to being purchasable [165169]. Despite their limitations, these are widely used in virtual screening libraries as a rapid means of prioritizing compounds that appear more synthetically tractable. While even simple user-defined heuristics that attempt to break a molecule into the smallest possible fragments can be successful in planning full synthetic routes, learned value functions can offer some advantages in finding shorter pathways or being tailored to a user-defined cost function [170]. Action policies directly predict which transformation rule to apply based on literature precedents in a knowledge base; this can be accomplished through a simple nearest-neighbor strategy [171] or through a trained neural network model for classification [153]. The latter approach has been integrated into a Monte Carlo tree search framework to rapidly generate and explore the space of candidate pathways, resulting in recommendations that chemists considered equally plausible to literature pathways in a double-blind study [164]. Less common approaches to navigating the search space include proof-number search [172].</p>
<p>Reaction pathway discovery is relevant in synthetic biology and metabolic engineering contexts as well.</p>
<p>For example, one study by Rangarajan et al. describes the application of Rule Input Network Generator (RING, [174]) to identify plausible production biosynthetic pathways through a heuristic-driven network generation and analysis [173]. Kim et al. review algorithms and heuristics used to explore metabolic networks and find optimal pathways [146]. A broader review of machine learning for biological networks can be found in ref. 175 .</p>
<h1>5.3.2 Discovering models of chemical reactivity</h1>
<p>Identifying synthetic pathways is but one step toward fully automated synthesis. For any theoretical robochemist capable of synthesizing any molecule on demand $[8,14]$, these ideas must be able to be acted upon and executed in the laboratory. Even without automated synthesis, hypothesized synthetic pathways are of little use without experimental validation. This requires additional models of chemical reactivity that can, among other things, propose suitable reaction conditions, estimate the confidence in the reactions it proposes, and have some notion of why one set of substrates might achieve a higher yield than others. Models for these tasks can be trained directly on experimental data using a variety of statistical techniques.</p>
<p>Given a set of combination of successful and unsuccessful reaction examples (i.e., high and low yielding), one can train a binary classifier model to predict whether a proposed set of reaction conditions will be successful [176]. The same task can also be treated as a regression of reaction yields, rather than as a classification, as a function of substrate descriptors; a virtual screen of known conditions as a fixed search space can then propose substrate-dependent optimal conditions [177]. When only successful reaction examples are present, one can treat the selection of reaction conditions as a recommendation problem comprising a classification subproblem (for reagent, catalyst, solvent identity) and a regression subproblem (temperature) under the assumption that the "true" published conditions are adequate. This was Gao et al.'s approach using the Reaxys database to produce a model that is able to propose conditions at the level of species identity and temperature based on reactant and product structures [178]. In the process of learning the relationship between reactants/products and suitable reaction conditions, the model learns a continuous embedding for chemicals that reflects their function in organic synthesis, similar to how semantic meaning is captured by word2vec models [179]. Formulating condition selection as a data-driven classification problem has also been used in a more focused manner as an alternative to expert recommender systems [180], e.g., to choose phosphine ligands for Buchwald-Hartwig aminations [144] or catalysts for deprotections [141].</p>
<p>In some cases, computational prediction of solvation free energies can meaningfully assist in the selection of reaction solvents [181]. To a first approximation, solvation energy can be estimated by a linear model describing potential solute-solvent interactions [182, 183]. When those interaction parameters can be predicted via DFT, one can estimate the performance of a large virtual set of solvents, e.g., to optimize the rate</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Department of Chemical Engineering, Massachusetts Institute of Technology, Cambridge, MA 02139
${ }^{\dagger}$ ccoley@mit.edu
${ }^{\ddagger}$ kfjensen@mit.edu&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
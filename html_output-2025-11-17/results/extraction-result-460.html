<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-460 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-460</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-460</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-271064726</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.06249v3.pdf" target="_blank">CodeUpdateArena: Benchmarking Knowledge Editing on API Updates</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) are increasingly being used to synthesize and reason about source code. However, the static nature of these models' knowledge does not reflect the fact that libraries and API functions they invoke are continuously evolving, with functionality being added or changing. While numerous benchmarks evaluate how LLMs can generate code, no prior work has studied how an LLMs' knowledge about code API functions can be updated. To fill this gap, we present CodeUpdateArena, a benchmark for knowledge editing in the code domain. An instance in our benchmark consists of a synthetic API function update paired with a program synthesis example that uses the updated functionality; our goal is to update an LLM to be able to solve this program synthesis example without providing documentation of the update at inference time. Compared to knowledge editing for facts encoded in text, success here is more challenging: a code LLM must correctly reason about the semantics of the modified function rather than just reproduce its syntax. Our dataset is constructed by first prompting GPT-4 to generate atomic and executable function updates. Then, for each update, we generate program synthesis examples whose code solutions are prone to use the update. Our benchmark covers updates of various types to 54 functions from seven diverse Python packages, with a total of 670 program synthesis examples. Our experiments show that prepending documentation of the update to open-source code LLMs (i.e., DeepSeek, CodeLlama) does not allow them to incorporate changes for problem solving, and existing knowledge editing techniques also have substantial room for improvement. We hope our benchmark will inspire new methods for knowledge updating in code LLMs.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e460.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e460.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UnitTestLiteralization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Literalization of Unit Tests to Remove Dependence on Updated APIs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Unit tests generated during dataset construction initially computed expected answers by calling the (new) updated API, causing false positives for evaluation metrics; the authors detect and remove these dependencies by 'literalizing' expected answers (serializing or replacing API-derived answers with literals) to ensure tests are valid when run against the old implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CodeUpdateArena dataset generation / evaluation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A synthetic-data generation and evaluation pipeline that (1) generates API update specifications, (2) generates updated function implementations, (3) creates program-synthesis problems and unit tests, and (4) evaluates model outputs using tests executed with both old and updated API implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>generated unit test skeletons and update docstrings (automatically produced natural language descriptions used to create tests)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>unit test code executed in Python test harness; updated and old API implementations used at runtime</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>test-dependency mismatch / evaluation contamination</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Many generated unit tests computed their expected_result programmatically by calling the updated API implementation (or otherwise invoking update-dependent code) rather than embedding a literal expected value. This means a candidate solution could appear to pass tests only because the test used the same updated API behavior, producing false positives for the metric that is intended to require use of the updated API by the generated solution.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>evaluation suite / unit test definitions</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>empirical execution and inspection: running tests with both old and new API implementations and observing inflated UPass@k (false positives), plus manual inspection of failing or suspicious cases</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>counting and transforming unit tests that depended on updated API outputs; the authors report literalizing 4,114 unit tests out of 4,221 total (≈97.5%) and include a comparison of evaluation results before vs after literalization (reported in the paper's Table 7) showing inflated UPass@k when literalization is not performed</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Without literalization, UPass@k (the metric counting correct solutions that meaningfully use the updated API) is substantially inflated (false positives). After literalization, metrics better reflect whether models truly used the updated API. The paper notes 'models like Base model has substantially higher UPass@k' when literalization is omitted, indicating a material evaluation bias.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>High in this dataset generation process: 4,114 / 4,221 unit tests required literalization (≈97.5%).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Generation pipeline allowed tests to compute expected answers by invoking the updated API (convenient for automatic generation), producing a mismatch between the natural-language/specification-level test and the intended isolated test that should not depend on the updated implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Literalize unit tests by replacing programmatic answers with literal serialized values (pickle serialization or numpy.array2string), and when serialization failed use a secondary LLM (Claude-3.5) plus manual edits to produce semantically equivalent tests that do not call the updated API.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective: large majority of tests were successfully literalized (4,114/4,221). The authors report that evaluation without literalization produces inflated UPass@k and that literalization corrects this; no precise numeric reduction of false positives is given beyond the qualitative statement and the dataset counts and Table 7 comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / code generation / program synthesis benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CodeUpdateArena: Benchmarking Knowledge Editing on API Updates', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e460.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e460.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DocstringVsInWeightMismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mismatch Between Documentation (Docstrings) and Model Internalization via Fine-Tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Continuing pretraining (fine-tuning) on a natural-language docstring describing an API update did not enable code LLMs to internalize the new semantics for downstream program synthesis; in many cases FT on docstrings failed to improve efficacy and harmed specificity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM knowledge-editing workflows (FT(U) vs Prepend vs FT(PS))</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Experimental setup comparing methods to convey API updates to models: (1) prepend the update docstring at inference time (RAG-like), (2) fine-tune on the docstring alone (FT(U)), and (3) fine-tune on program-synthesis examples that use the updated API (FT(PS)).</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>API documentation / docstring describing the new API semantics</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>model weights after fine-tuning; downstream code generation outputs</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>semantic propagation failure / incomplete specification → implementation mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Although docstrings accurately describe new API semantics in natural language, fine-tuning models on those docstrings (i.e., training them to 'regurgitate' the update description) did not enable the models to generate programmatic code that uses the updated API semantics at inference time. In other words, the natural-language description did not translate into the model internalizing the procedural or functional semantics required for program synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model parameter updates / knowledge representation inside the model (in-weight knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>experimental comparison using evaluation metrics (UPass@k for efficacy and SPass for specificity) across model variants and update methods; statistical testing (paired bootstrap with p < 0.05) was used to verify differences</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>UPass@k (pass@k restricted to solutions that meaningfully use the updated API) and SPass (change in performance on a held-out HumanEval set to measure specificity); comparisons across Prepend, FT(U), and FT(PS) and reporting deltas in these metrics</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>FT(U) (fine-tuning on docstrings) did not improve efficacy (UPass) and in many cases hurt specificity (SPass) relative to the base model. The paper contrasts this with Prepend (in-context docstring) which improved pass@k, and FT(PS) (fine-tuning on examples) which produced larger efficacy gains. Thus relying on docstrings alone as an update mechanism is insufficient and can degrade other capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed consistently across the tested open-source code LLMs in this study (CodeLlama, DS-Coder-v1, DS-Coder-v1.5); characterized as an expected negative result analogous to prior findings in factual knowledge editing literature.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Natural-language descriptions (docstrings) do not provide the direct procedural supervised signal required for the model to internalize how to apply new function semantics in downstream code synthesis; models appear to need example-driven or context-driven signals rather than just textual descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Fine-tune on program-synthesis examples that explicitly demonstrate using the updated API (FT(PS)); alternatively, provide the update docstring in-context at inference time (Prepend) as a retrieval-augmented approach.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>FT(PS) improved efficacy markedly (outperforming Prepend in many settings) but often caused a trade-off by reducing specificity for some open-source models; Prepend reliably improved immediate pass@k (in-context use) but does not represent an in-weight edit. Quantitative deltas and significance tests are reported in the paper's result tables.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / model editing / code generation</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CodeUpdateArena: Benchmarking Knowledge Editing on API Updates', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e460.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e460.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GenToExecAcceptanceGap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generation-to-Execution Gap in Synthetic API Update Implementations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A substantial fraction of synthetic API update specifications could not be paired with a validated executable updated implementation or sufficient program-synthesis examples, leading to filtering and reduced dataset yield.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>API update generation and validation subsystem</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pipeline that prompts an LLM (GPT-4) to propose update specifications and to synthesize code implementations for the updated API, followed by automated unit-test-based validation and human filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>synthetic update specification (natural-language description of update + docstring)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>synthesized Python implementation of the updated API function and reference solutions for program-synthesis tasks</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>generation-quality vs execution-validity gap / incomplete or trivial implementations</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Not all LLM-generated update specifications could be turned into valid, executable updated implementations that sufficiently differed from the old API. The pipeline only kept an updated implementation if it passed >=70% of the update unit tests and passed more tests than the original implementation. Many generated updates were trivial, duplicate, or infeasible to implement/test automatically and were discarded after validation and manual review.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>dataset construction (implementation generation and filtering stage)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>automated test execution (unit tests generated alongside the update) combined with heuristics and subsequent manual inspection and deduplication</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>acceptance / filtering rates: authors report that on average ~41% of update specifications were paired with an updated function implementation (i.e., acceptance rate after implementation attempts); ~53% of examples were removed by manual filtering (duplicates/trivial updates); ~37% of update specifications lacked at least 3 valid program-synthesis examples and were filtered out.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Reduced the number of usable update+PS instances, necessitated manual filtering and heavier engineering effort to ensure dataset quality, and constrained the diversity of updates in the final benchmark. This affects downstream experiments by limiting the set of updates models are tested on.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Substantial: only ~41% of proposed updates yielded acceptable implementations; per-update filtering percentages (53% removed by dedup/trivial filters; 37% lacking PS examples) indicate a nontrivial prevalence of unusable or low-quality generated items.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Automatic synthetic generation of realistic, atomic, and executable API updates is challenging: some proposed updates are trivial, contradictory, require complex environment setups, or are difficult to test automatically (e.g., ML APIs or server-backed functions). LLMs sometimes produce updates that cannot be robustly implemented or tested without human intervention.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use multi-step generation with resampling (up to 3 implementations per specification), automated validation thresholds (>=70% tests passed and improvement over original), plus manual inspection and deduplication to remove low-quality or trivial updates.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective at improving dataset quality (final benchmark contains 670 PS examples over 54 functions), but costly: roughly half of initial candidates were discarded and some updates could not be recovered; authors acknowledge this trade-off and document the manual filtering steps.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>dataset creation for ML benchmarks / program synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CodeUpdateArena: Benchmarking Knowledge Editing on API Updates', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e460.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e460.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MethodAssumptionMismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Non-Applicability of Existing Model-Editing Methods Due to Format Mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Many existing model-editing algorithms assume knowledge items have short knowledge-triplet structure (subject, relation, object) and produce brief textual answers; code API updates do not fit this format (they are procedural, often long, and require reasoning about function semantics), making such methods inapplicable or ineffective.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>model-editing algorithm selection for code LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Survey and experimental consideration of applying prior editing methods (e.g., ROME, MEMIT, REMEDI, MEND) to the setting of updating code-focused LLM knowledge about API semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research-method assumptions / prior-method specifications (natural-language descriptions of editing algorithms)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>parameter-editing procedures for transformer models (weight edits, low-rank adapters, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>assumption mismatch / representation mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Prior editing methods either assume (a) knowledge items are short factual phrases (so adjusting a few parameters suffices), or (b) knowledge has a relation-graph style triplet structure to localize changes. API updates are long, procedural, and require compositional reasoning (reference solutions average ~175 tokens). Consequently, methods optimized for short factual edits fail or are not straightforward to apply in the code/API update scenario.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model-editing method applicability / algorithmic assumptions</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>qualitative analysis of prior methods' assumptions and experimental attempts (authors note these methods are 'not straightforward' to apply; other flexible approaches optimized for regurgitation also underperform on long reference solutions)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>empirical failure modes and poor performance when applying these approaches in preliminary experiments (no single-number metric summarized here beyond general ineffectiveness reported), and argument based on average reference-solution length (≈175 tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Limits the set of off-the-shelf knowledge-editing algorithms that can be used for code API updates and motivates the need for example-driven fine-tuning or new editing methods tailored to procedural/code knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Conceptual and broadly applicable: this mismatch applies to many prior editing methods and is not limited to this dataset—authors cite multiple prior works and negative results in related propagation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Mismatch between the representational assumptions of prior editing techniques (short textual facts, triplet structure) and the nature of code/API updates (procedural, multi-token, semantic changes).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use supervised fine-tuning on program-synthesis examples (FT(PS)) that provide direct behavioral signals, or develop new editing algorithms that handle procedural and compositional updates; also consider retrieval-in-context (Prepend) as an interim workaround.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>FT(PS) showed empirical improvement in efficacy (UPass) for the benchmark, though at potential cost to specificity; the need for new algorithms remains.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / knowledge editing / code models</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CodeUpdateArena: Benchmarking Knowledge Editing on API Updates', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge <em>(Rating: 2)</em></li>
                <li>Propagating knowledge updates to LMs through distillation <em>(Rating: 2)</em></li>
                <li>Evaluating the ripple effects of knowledge editing in language models <em>(Rating: 2)</em></li>
                <li>Fast model editing at scale <em>(Rating: 1)</em></li>
                <li>Editing factual knowledge in language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-460",
    "paper_id": "paper-271064726",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "UnitTestLiteralization",
            "name_full": "Literalization of Unit Tests to Remove Dependence on Updated APIs",
            "brief_description": "Unit tests generated during dataset construction initially computed expected answers by calling the (new) updated API, causing false positives for evaluation metrics; the authors detect and remove these dependencies by 'literalizing' expected answers (serializing or replacing API-derived answers with literals) to ensure tests are valid when run against the old implementation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "CodeUpdateArena dataset generation / evaluation pipeline",
            "system_description": "A synthetic-data generation and evaluation pipeline that (1) generates API update specifications, (2) generates updated function implementations, (3) creates program-synthesis problems and unit tests, and (4) evaluates model outputs using tests executed with both old and updated API implementations.",
            "nl_description_type": "generated unit test skeletons and update docstrings (automatically produced natural language descriptions used to create tests)",
            "code_implementation_type": "unit test code executed in Python test harness; updated and old API implementations used at runtime",
            "gap_type": "test-dependency mismatch / evaluation contamination",
            "gap_description": "Many generated unit tests computed their expected_result programmatically by calling the updated API implementation (or otherwise invoking update-dependent code) rather than embedding a literal expected value. This means a candidate solution could appear to pass tests only because the test used the same updated API behavior, producing false positives for the metric that is intended to require use of the updated API by the generated solution.",
            "gap_location": "evaluation suite / unit test definitions",
            "detection_method": "empirical execution and inspection: running tests with both old and new API implementations and observing inflated UPass@k (false positives), plus manual inspection of failing or suspicious cases",
            "measurement_method": "counting and transforming unit tests that depended on updated API outputs; the authors report literalizing 4,114 unit tests out of 4,221 total (≈97.5%) and include a comparison of evaluation results before vs after literalization (reported in the paper's Table 7) showing inflated UPass@k when literalization is not performed",
            "impact_on_results": "Without literalization, UPass@k (the metric counting correct solutions that meaningfully use the updated API) is substantially inflated (false positives). After literalization, metrics better reflect whether models truly used the updated API. The paper notes 'models like Base model has substantially higher UPass@k' when literalization is omitted, indicating a material evaluation bias.",
            "frequency_or_prevalence": "High in this dataset generation process: 4,114 / 4,221 unit tests required literalization (≈97.5%).",
            "root_cause": "Generation pipeline allowed tests to compute expected answers by invoking the updated API (convenient for automatic generation), producing a mismatch between the natural-language/specification-level test and the intended isolated test that should not depend on the updated implementation.",
            "mitigation_approach": "Literalize unit tests by replacing programmatic answers with literal serialized values (pickle serialization or numpy.array2string), and when serialization failed use a secondary LLM (Claude-3.5) plus manual edits to produce semantically equivalent tests that do not call the updated API.",
            "mitigation_effectiveness": "Effective: large majority of tests were successfully literalized (4,114/4,221). The authors report that evaluation without literalization produces inflated UPass@k and that literalization corrects this; no precise numeric reduction of false positives is given beyond the qualitative statement and the dataset counts and Table 7 comparison.",
            "domain_or_field": "machine learning / code generation / program synthesis benchmarking",
            "reproducibility_impact": true,
            "uuid": "e460.0",
            "source_info": {
                "paper_title": "CodeUpdateArena: Benchmarking Knowledge Editing on API Updates",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "DocstringVsInWeightMismatch",
            "name_full": "Mismatch Between Documentation (Docstrings) and Model Internalization via Fine-Tuning",
            "brief_description": "Continuing pretraining (fine-tuning) on a natural-language docstring describing an API update did not enable code LLMs to internalize the new semantics for downstream program synthesis; in many cases FT on docstrings failed to improve efficacy and harmed specificity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "LLM knowledge-editing workflows (FT(U) vs Prepend vs FT(PS))",
            "system_description": "Experimental setup comparing methods to convey API updates to models: (1) prepend the update docstring at inference time (RAG-like), (2) fine-tune on the docstring alone (FT(U)), and (3) fine-tune on program-synthesis examples that use the updated API (FT(PS)).",
            "nl_description_type": "API documentation / docstring describing the new API semantics",
            "code_implementation_type": "model weights after fine-tuning; downstream code generation outputs",
            "gap_type": "semantic propagation failure / incomplete specification → implementation mismatch",
            "gap_description": "Although docstrings accurately describe new API semantics in natural language, fine-tuning models on those docstrings (i.e., training them to 'regurgitate' the update description) did not enable the models to generate programmatic code that uses the updated API semantics at inference time. In other words, the natural-language description did not translate into the model internalizing the procedural or functional semantics required for program synthesis.",
            "gap_location": "model parameter updates / knowledge representation inside the model (in-weight knowledge)",
            "detection_method": "experimental comparison using evaluation metrics (UPass@k for efficacy and SPass for specificity) across model variants and update methods; statistical testing (paired bootstrap with p &lt; 0.05) was used to verify differences",
            "measurement_method": "UPass@k (pass@k restricted to solutions that meaningfully use the updated API) and SPass (change in performance on a held-out HumanEval set to measure specificity); comparisons across Prepend, FT(U), and FT(PS) and reporting deltas in these metrics",
            "impact_on_results": "FT(U) (fine-tuning on docstrings) did not improve efficacy (UPass) and in many cases hurt specificity (SPass) relative to the base model. The paper contrasts this with Prepend (in-context docstring) which improved pass@k, and FT(PS) (fine-tuning on examples) which produced larger efficacy gains. Thus relying on docstrings alone as an update mechanism is insufficient and can degrade other capabilities.",
            "frequency_or_prevalence": "Observed consistently across the tested open-source code LLMs in this study (CodeLlama, DS-Coder-v1, DS-Coder-v1.5); characterized as an expected negative result analogous to prior findings in factual knowledge editing literature.",
            "root_cause": "Natural-language descriptions (docstrings) do not provide the direct procedural supervised signal required for the model to internalize how to apply new function semantics in downstream code synthesis; models appear to need example-driven or context-driven signals rather than just textual descriptions.",
            "mitigation_approach": "Fine-tune on program-synthesis examples that explicitly demonstrate using the updated API (FT(PS)); alternatively, provide the update docstring in-context at inference time (Prepend) as a retrieval-augmented approach.",
            "mitigation_effectiveness": "FT(PS) improved efficacy markedly (outperforming Prepend in many settings) but often caused a trade-off by reducing specificity for some open-source models; Prepend reliably improved immediate pass@k (in-context use) but does not represent an in-weight edit. Quantitative deltas and significance tests are reported in the paper's result tables.",
            "domain_or_field": "machine learning / model editing / code generation",
            "reproducibility_impact": true,
            "uuid": "e460.1",
            "source_info": {
                "paper_title": "CodeUpdateArena: Benchmarking Knowledge Editing on API Updates",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GenToExecAcceptanceGap",
            "name_full": "Generation-to-Execution Gap in Synthetic API Update Implementations",
            "brief_description": "A substantial fraction of synthetic API update specifications could not be paired with a validated executable updated implementation or sufficient program-synthesis examples, leading to filtering and reduced dataset yield.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "API update generation and validation subsystem",
            "system_description": "Pipeline that prompts an LLM (GPT-4) to propose update specifications and to synthesize code implementations for the updated API, followed by automated unit-test-based validation and human filtering.",
            "nl_description_type": "synthetic update specification (natural-language description of update + docstring)",
            "code_implementation_type": "synthesized Python implementation of the updated API function and reference solutions for program-synthesis tasks",
            "gap_type": "generation-quality vs execution-validity gap / incomplete or trivial implementations",
            "gap_description": "Not all LLM-generated update specifications could be turned into valid, executable updated implementations that sufficiently differed from the old API. The pipeline only kept an updated implementation if it passed &gt;=70% of the update unit tests and passed more tests than the original implementation. Many generated updates were trivial, duplicate, or infeasible to implement/test automatically and were discarded after validation and manual review.",
            "gap_location": "dataset construction (implementation generation and filtering stage)",
            "detection_method": "automated test execution (unit tests generated alongside the update) combined with heuristics and subsequent manual inspection and deduplication",
            "measurement_method": "acceptance / filtering rates: authors report that on average ~41% of update specifications were paired with an updated function implementation (i.e., acceptance rate after implementation attempts); ~53% of examples were removed by manual filtering (duplicates/trivial updates); ~37% of update specifications lacked at least 3 valid program-synthesis examples and were filtered out.",
            "impact_on_results": "Reduced the number of usable update+PS instances, necessitated manual filtering and heavier engineering effort to ensure dataset quality, and constrained the diversity of updates in the final benchmark. This affects downstream experiments by limiting the set of updates models are tested on.",
            "frequency_or_prevalence": "Substantial: only ~41% of proposed updates yielded acceptable implementations; per-update filtering percentages (53% removed by dedup/trivial filters; 37% lacking PS examples) indicate a nontrivial prevalence of unusable or low-quality generated items.",
            "root_cause": "Automatic synthetic generation of realistic, atomic, and executable API updates is challenging: some proposed updates are trivial, contradictory, require complex environment setups, or are difficult to test automatically (e.g., ML APIs or server-backed functions). LLMs sometimes produce updates that cannot be robustly implemented or tested without human intervention.",
            "mitigation_approach": "Use multi-step generation with resampling (up to 3 implementations per specification), automated validation thresholds (&gt;=70% tests passed and improvement over original), plus manual inspection and deduplication to remove low-quality or trivial updates.",
            "mitigation_effectiveness": "Effective at improving dataset quality (final benchmark contains 670 PS examples over 54 functions), but costly: roughly half of initial candidates were discarded and some updates could not be recovered; authors acknowledge this trade-off and document the manual filtering steps.",
            "domain_or_field": "dataset creation for ML benchmarks / program synthesis",
            "reproducibility_impact": true,
            "uuid": "e460.2",
            "source_info": {
                "paper_title": "CodeUpdateArena: Benchmarking Knowledge Editing on API Updates",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "MethodAssumptionMismatch",
            "name_full": "Non-Applicability of Existing Model-Editing Methods Due to Format Mismatch",
            "brief_description": "Many existing model-editing algorithms assume knowledge items have short knowledge-triplet structure (subject, relation, object) and produce brief textual answers; code API updates do not fit this format (they are procedural, often long, and require reasoning about function semantics), making such methods inapplicable or ineffective.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "model-editing algorithm selection for code LLMs",
            "system_description": "Survey and experimental consideration of applying prior editing methods (e.g., ROME, MEMIT, REMEDI, MEND) to the setting of updating code-focused LLM knowledge about API semantics.",
            "nl_description_type": "research-method assumptions / prior-method specifications (natural-language descriptions of editing algorithms)",
            "code_implementation_type": "parameter-editing procedures for transformer models (weight edits, low-rank adapters, etc.)",
            "gap_type": "assumption mismatch / representation mismatch",
            "gap_description": "Prior editing methods either assume (a) knowledge items are short factual phrases (so adjusting a few parameters suffices), or (b) knowledge has a relation-graph style triplet structure to localize changes. API updates are long, procedural, and require compositional reasoning (reference solutions average ~175 tokens). Consequently, methods optimized for short factual edits fail or are not straightforward to apply in the code/API update scenario.",
            "gap_location": "model-editing method applicability / algorithmic assumptions",
            "detection_method": "qualitative analysis of prior methods' assumptions and experimental attempts (authors note these methods are 'not straightforward' to apply; other flexible approaches optimized for regurgitation also underperform on long reference solutions)",
            "measurement_method": "empirical failure modes and poor performance when applying these approaches in preliminary experiments (no single-number metric summarized here beyond general ineffectiveness reported), and argument based on average reference-solution length (≈175 tokens)",
            "impact_on_results": "Limits the set of off-the-shelf knowledge-editing algorithms that can be used for code API updates and motivates the need for example-driven fine-tuning or new editing methods tailored to procedural/code knowledge.",
            "frequency_or_prevalence": "Conceptual and broadly applicable: this mismatch applies to many prior editing methods and is not limited to this dataset—authors cite multiple prior works and negative results in related propagation tasks.",
            "root_cause": "Mismatch between the representational assumptions of prior editing techniques (short textual facts, triplet structure) and the nature of code/API updates (procedural, multi-token, semantic changes).",
            "mitigation_approach": "Use supervised fine-tuning on program-synthesis examples (FT(PS)) that provide direct behavioral signals, or develop new editing algorithms that handle procedural and compositional updates; also consider retrieval-in-context (Prepend) as an interim workaround.",
            "mitigation_effectiveness": "FT(PS) showed empirical improvement in efficacy (UPass) for the benchmark, though at potential cost to specificity; the need for new algorithms remains.",
            "domain_or_field": "machine learning / knowledge editing / code models",
            "reproducibility_impact": true,
            "uuid": "e460.3",
            "source_info": {
                "paper_title": "CodeUpdateArena: Benchmarking Knowledge Editing on API Updates",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge",
            "rating": 2,
            "sanitized_title": "can_lms_learn_new_entities_from_descriptions_challenges_in_propagating_injected_knowledge"
        },
        {
            "paper_title": "Propagating knowledge updates to LMs through distillation",
            "rating": 2,
            "sanitized_title": "propagating_knowledge_updates_to_lms_through_distillation"
        },
        {
            "paper_title": "Evaluating the ripple effects of knowledge editing in language models",
            "rating": 2,
            "sanitized_title": "evaluating_the_ripple_effects_of_knowledge_editing_in_language_models"
        },
        {
            "paper_title": "Fast model editing at scale",
            "rating": 1,
            "sanitized_title": "fast_model_editing_at_scale"
        },
        {
            "paper_title": "Editing factual knowledge in language models",
            "rating": 1,
            "sanitized_title": "editing_factual_knowledge_in_language_models"
        }
    ],
    "cost": 0.0166435,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CodeUpdateArena: BENCHMARKING KNOWLEDGE EDITING ON API UPDATES
3 Apr 2025</p>
<p>Zeyu Leo Liu zliu@cs.utexas.edu 
The University of Texas at Austin</p>
<p>Shrey Pandit 
The University of Texas at Austin</p>
<p>Xi Ye 
The University of Texas at Austin</p>
<p>Eunsol Choi 
The University of Texas at Austin</p>
<p>Greg Durrett 
The University of Texas at Austin</p>
<p>CodeUpdateArena: BENCHMARKING KNOWLEDGE EDITING ON API UPDATES
3 Apr 20250F7EC4869D59AA4FD8FB98C7D4625315arXiv:2407.06249v3[cs.CL]</p>
<p>ABSTRACT</p>
<p>Large language models (LLMs) are increasingly being used to synthesize and reason about source code.The libraries and API functions they invoke are continuously evolving, with functionality being added or changing.Yet, no prior work has studied how an LLM's knowledge about code API functions can be updated.To fill this gap, we present CodeUpdateArena, a benchmark for knowledge editing in the code domain.An instance in our benchmark consists of a synthetic API function update paired with a program synthesis example that uses the updated functionality; our goal is to update an LLM to be able to solve this program synthesis example without providing documentation of the update at inference time.Compared to knowledge editing for facts, success here is more challenging: a code LLM must reason about the semantics of the modified function rather than just reproduce its syntax.Our dataset is constructed by first prompting GPT-4 to generate atomic and executable function updates.Then, for each update, we generate program synthesis examples whose code solutions are prone to use the update.Our benchmark covers updates of various types to 54 functions from seven diverse Python packages, with a total of 670 program synthesis examples.Our experiments show that fine-tuning open-source code LLMs (i.e., DeepSeek, CodeLlama) on documentation of a new update does not allow them to incorporate changes for problem-solving.However, prepending the same information does help, establishing that the information is present, and careful fine-tuning on examples demonstrating the update shows improvement, paving the way for better knowledge editing techniques for code.</p>
<p>[Scenario] You are building a software for an online auction site…</p>
<p>[Problem] Create a function that returns the indices of the bidders in the order of their bids, with the highest bidder first…</p>
<p>INTRODUCTION</p>
<p>Large language models (LLMs) have demonstrated strong abilities to synthesize code to solve problems (Chen et al., 2021;Li et al., 2023;DeepSeek-AI et al., 2024;Guo et al., 2024a).This capability enables them to use external libraries: they can invoke standard libraries for data sciencerelated tasks (Lai et al., 2023), program SMT solvers (Ye et al., 2023), or use external modules for tasks like computer vision (Gupta &amp; Kembhavi, 2023).However, such APIs are not static and adherence to older APIs can cause failures.For example, in a live demo,1 GPT-4 failed to correctly implement a Discord bot due to outdated API knowledge.To be maximally useful, LLMs for code generation need to stay in sync with API updates, even those that occur after pre-training.</p>
<p>A separate line of research studies knowledge editing for LLMs on simple facts.Typical use-cases here are teaching LLMs about new entities (Onoe et al., 2023), updating roles of existing entities like who the British prime minister is now (De Cao et al., 2021;Mitchell et al., 2022), and other such temporally-sensitive knowledge (Zhang &amp; Choi, 2021).A number of techniques have been presented for these settings to efficiently update the parameters of LLMs, such as with a single gradient update (Mitchell et al., 2022;Meng et al., 2023) or with a small number of updates (De Cao et al., 2021;Meng et al., 2022;Padmanabhan et al., 2023;Akyürek et al., 2024;Chen et al., 2023).</p>
<p>These studies suggest a natural parallel in the code setting: can we efficiently update a pre-trained model's knowledge of an API?In this work, we construct a benchmark to evaluate this capability.</p>
<p>Our benchmark instances, shown in Figure 1, consist of a problem setting defined by a synthetic API update, such as an additional boolean flag in a function like numpy.argsort.We choose synthetic updates, as information about any real API function update will likely be used as a pre-training corpus by the next generation of pre-trained models.Then, for each function update, we have a number of program synthesis problems requiring the use of that update.Although there are solutions that do not use the update, the most parsimonious solutions do use the API functionality in question, and models are prompted to do so.</p>
<p>Our evaluation assesses whether LLMs can, after being updated on the synthetic API function update (docstring, example usage, etc.), solve these program synthesis examples using the given API function without being provided the update at inference time.</p>
<p>Our final benchmark, CodeUpdateArena, contains 670 program synthesis tasks, covering 54 functions from 7 Python packages.Our benchmark is synthetically constructed by a carefully designed data generation pipeline driven by , enabling it to be scaled or updated with new instances in the future.We manually filter our generated API updates and conduct a number of additional intrinsic evaluations of dataset quality to establish the correctness of dataset instances.</p>
<p>Our experimental evaluation focuses on how existing small-scale LLMs (e.g., CodeLlama (Rozière et al., 2023)) perform at this update setting when combined with existing knowledge updating techniques.GPT-4 and Claude-3.5 are able to solve program synthesis examples when prompted with the API update in context, with Claude-3.5 outperforming GPT-4 on its own generated data.We then present two intuitive baselines for how practitioners would utilize API update information.The first method, fine-tuning models on a docstring explaining the update does not improve performance.However, fine-tuning on examples of the update being used does lead to improvement, and even outperforms having the API update in context.Through our ablation study, we found that the mix of training examples and learning rate are important for successful fine-tuning, but there is a tradeoff between efficacy and specificity of the update (impact on unrelated settings).We believe our dataset can provide a testbed for developing better methods for code knowledge editing in the future.Our code is released at .</p>
<p>BACKGROUND AND RELATED WORK</p>
<p>Knowledge editing Knowledge editing involves updating a pre-trained model's parameters to contain additional knowledge that was not present in its pre-training corpus.Suppose we have a model M and let (c, u) denote the additional knowledge u that should be returned in context c.Past work has focused on finding a model M ′ such that M ′ ≈ M and M ′ (c) returns u with high probability.For instance, suppose c = "the prime minister of the UK is" and u = "Rishi Sunak"; we want to update the model's knowledge about the UK's prime minister with as little change to other facts (e.g.Eiffel tower is in Rome) as possible.</p>
<p>Prior work quantifies model editing success by measuring whether M ′ can return u when prompted with c.A second goal is to preserve the original M as closely as possible, measured by ensuring that the model's predictions on irrelevant contexts are not changed.The techniques for knowledge editing typically involve gradient updates (De Cao et al., 2021), including meta-learned updates (Mitchell et al., 2022), localized updates leveraging interpretability methods (Meng et al., 2022), and updates on a collection of related examples (Padmanabhan et al., 2023;Akyürek et al., 2024).</p>
<p>A third goal involves knowledge propagation (Onoe et al., 2023;Padmanabhan et al., 2023;Cohen et al., 2024;Powell et al., 2024;Zhong et al., 2023), where an LLM must be able to reason about the injected knowledge in contexts that may seem unrelated on the surface.However, current literature has many negative results for this setting (Cohen et al., 2024;Hua et al., 2024).Our benchmark will allow us to evaluate the state of affairs in the code setting, and whether functional competence around code updates is more easily obtained than functional competence around textual knowledge.</p>
<p>Updates in Source Code Despite a large body of work on knowledge editing (Wang et al., 2023), past work in this space has not explored the ramifications for code language models.Rather than just reproduce an update like in knowledge editing settings (e.g.be able to generate Python 3.12 has lifted restrictions on the usage of f-strings), a user would likely expect a code LLM to be able to generate, debug, or otherwise reason about code containing these updates.</p>
<p>To the best of our knowledge, existing benchmarks mainly focus on general coding capabilities of LLMs rather than their capability in dealing with API updates or historical versions existent in pretraining corpus.Although some recent research has also explored providing documentations of functions (or tools) (Zhou et al., 2022;Su et al., 2024;Zhang et al., 2023b;Hsieh et al., 2023) and code snippets (Su et al., 2024;Zhang et al., 2023a;Phan et al., 2024;Shrivastava et al., 2022) to LLMs in a retrieval-augmented framework (Chen et al., 2017;Guu et al., 2020;Lewis et al., 2020), our main focus is on enabling LLMs to internalize this knowledge in an update (in-weight) and propagate it during program synthesis as opposed to using it in-context.Therefore, our work also relates to more general program synthesis using LLMs (Austin et al., 2021), especially those on developing benchmarks (Chen et al., 2021;Liu et al., 2023;2024;Gu et al., 2024;Jimenez et al., 2024;Ding et al., 2023;Du et al., 2023;Guo et al., 2024b;Xie et al., 2024;Lai et al., 2023).</p>
<p>Defining an update taxonomy The goal of this work is to assess models' abilities to be updated with realistic changes to functions in APIs.Most of the time when new functionality is introduced, the update extends existing methods in an atomic way</p>
<p>TASK: CodeUpdateArena</p>
<p>We define f ← u to be the update made to an existing function f when providing it with new semantics u.Our task involves understanding whether a pretrained code language model M can be updated with f ← u.We assume that some kind of parametric update is made to yield a new model M (f ←u) ; this can be done via various fine-tuning methods that have been proposed for knowledge editing.We will describe exactly how u is conveyed to the language model in Section 6.1; here, we focus on what capabilities we want the updated model M (f ←u) to exhibit.</p>
<p>To evaluate M (f ←u) , we provide a set of program synthesis examples P (f ←u) .Each program synthesis example consists of a problem scenario s i , a problem specification p i , and a set of T unit test cases
T (f ←u) i = {(t i,1 , a i,1 ), (t i,2 , a i,2 ), • • • (t i,T , a i,T )}. P (f ←u) := s i , p i , T (f ←u) i T i=1
Each example scenario and specification is related to the updated semantics u.Let ci ← M (f ←u) (s i , p i ) denote the result of predicting a code solution to problem i for update u.We want to evaluate M (f ←u) for three broad capabilities: (1) edit success: ∀j, ci (t UPass@k Our main evaluation metric captures both edit success and use of f .We define UPass@k as the standard pass@k except that it only counts solutions that meaningfully use the updated function as "correct".</p>
<p>We run a solution against test cases with different function implementations at runtime: a) when executing with the updated function in the environment, the solution must pass all tests.</p>
<p>b) when executing with the old function in the environment, the solution must fail some tests.</p>
<p>Details of how to do this execution are described in Appendix D. The first check is the standard one used in pass@k.This second check ensures that the new functionality of f is leveraged in a nontrivial way.Detecting a call to f is insufficient; if, for example, the update provides a new argument, we want the model to use that new argument rather than use f in its pre-update form.</p>
<p>Our program synthesis examples are designed to be naturally suited to the updated function f ← u.</p>
<p>It is, of course, possible for a code LLM to produce a solution that passes the tests but sidesteps the usage of f altogether; however, in Section 6, we will see that prompted GPT-4 frequently does use the update in successful solutions.</p>
<p>SPass@k captures how well the update is specific in that model's other capabilities are not affected (specificity) before (M) and after (M (f ←u) ) injecting each update.We discuss details in Section 6.1.</p>
<p>4 Update AND Arena GENERATION</p>
<p>We generate our data by prompting GPT-4 (Achiam et al., 2023) to instantiate our proposed task CodeUpdateArena, following recent work on generating synthetic datasets for complex tasks with LLMs (Sprague et al., 2024;Lee et al., 2024;Tang et al., 2024;Yehudai et al., 2024;Oh et al., 2024;Zhao et al., 2024).Each data instance requires an update semantics u and program synthesis examples P (f ←u) to evaluate the integration of the updates.We first generate the update semantics (described in Section 4.1) and generate program synthesis examples (Section 4.2).The output from each generation step is validated through manual inspection and heuristics.Figure 2 outlines our generation process.</p>
<p>Update (NEW API FUNCTION) GENERATION</p>
<p>Step 1: Generate update specification u Given an update type (e.g., add new argument) and a function f (e.g., numpy.argsort),we generate an update u consisting of four pieces:</p>
<p>• a description of the update: e.g., adding a new boolean argument 'reverse', which controls whether the sorting is descending or ascending.</p>
<p>• the new function signature: e.g., numpy.argsort(..., reverse=False)</p>
<p>• a docstring describing expected new behavior  5) result = np.argsort(array,reverse=True) # @ANSWER@ # @ASSERT@ # Unit Test 2 def test_reverse_true(): ...</p>
<p>-# @ANSWER@ + expected_result = old_argsort(-array) -# @ASSERT@ + assert np.equal(result, expected_result) # Unit Test 1 def test_reverse_true(): array = np.random.rand(5)result = np.argsort(array,reverse=True) expected_result = old_argsort(-array) assert np.equal(result, expected_result)</p>
<p>Step 1: Problem Specification Generation Step 2: Unit test generation (shared)  Step 2: Generate a suite of unit tests Once the description of the update is available, we create a set of unit tests to verify the correctness of the updated function f ← u.</p>
<p>To make the tests comprehensive, we ask GPT-4 to generate 10 unit test functions, testing edge cases (e.g., empty input) and interaction with existing arguments (e.g.reverse=True and axis=1).</p>
<p>See Appendix B.5 for the details of the prompt.We first generate unit test "skeletons", unit test function with initialization of the input.Fig. 2 shows an example.Each skeleton takes the format of a unit test function with two placeholders -@ANSWER@ for answer and @ASSERT@ for assertion.Given a unit test skeleton, GPT-4 generates the answer and assertion statement(s).The details of answer and assertion generation can be found in Appendix B.2.</p>
<p>Step 3: Generate an updated function f ← u We now prompt GPT4 to generate the source code for the updated function f ← u given the function f and update specification u.We prompt using the original function implementation (e.g.original argsort) to implement the new version.This typically involves an implementation that wraps the original version of the function; for instance, if a new boolean flag is added, call the function normally in one case and otherwise call it with a transformed input or output.</p>
<p>We validate the generated function with unit tests from the previous step.Specifically, we accept the updated function if (1) it passes 70% of unit tests and (2) it passes more unit tests than the original implementation.2To improve the coverage, we sample up to three implementations if earlier implementation does not satisfy two criteria above.After this process, on average, around 41% of update specifications are paired with an updated function implementation.The rest are discarded.</p>
<p>Step 4: Filtering and deduplication Lastly, to verify the quality of generated data, the authors of this paper manually examine the update specifications and filter duplicates and trivial update specifications (e.g., change the return type from list to tuple).This process removes roughly 53% of examples on average, and the filtering percentage differs per package.We also filter update specifications for which we could not generate at least 3 valid program synthesis examples (37% of update specifications), as described in the next section.2 and more details in Appendix B.6.</p>
<p>Step 1: Problem specification Given the update rationale generated as a part of update specification u, GPT4 generates: (1) a scenario s i that a problem is situated in;</p>
<p>(2) the problem specification p i that a solution function is mean to fulfill; and (3) the solution's function signature, according to the problem specification.See an example at Appendix A.2.</p>
<p>Step 2: Unit tests We then generate a set of unit tests meant to test that the solution to the program synthesis example is correct.Note that these do not necessarily depend on the update, but only on the specification of the problem from Step 1; they do not test whether the function is used.</p>
<p>We allow GPT-4 to include updated function in its generation, in contrast with update generation, where GPT-4 could only call the old function through old_[function name].Other than the difference above, the generation process is identical to Step 2 in update generation.</p>
<p>Step 3: Reference Solution The prompt instructs GPT-4 to solve the problem by using the new function as part of its solution.This helps to ensure that there exists a solution that uses the updated function.We define a threshold δ = 0.6 of a fraction of tests that the implementation must pass in order to be included in the benchmark.We found this quality bar to be high enough given the presence of bad tests, which we discard next.</p>
<p>Step 4: Filtering and Deduplication Finally, we implement several filters of low-quality examples.First, we discard unit tests that generated solution doesn't pass, as well as unit tests checking for exceptions (try/catch behavior).Our inspection of these cases showed that failed unit tests are almost invariably incorrect while the generated reference solutions are correct.Second, for each update, we remove program synthesis cases for which reference solutions are too similar, to avoid GPT-4 generating essentially similar solutions.Example of duplicate reference function in Figure 9. See more detailed description at Appendix B.3</p>
<p>Step 5: Literalize answers in unit tests During generation, many unit tests initially rely on calling updated APIs themselves to produce the correct answer to the test programmatically.However, this causes unintended failures in the unit tests when running the old API updates in the environment, leading to false positives for UPass@k even when the synthesis code does not use the new function.We "literalize" the unit tests to remove these usages of the API; we provide more details in Appendix B.4.</p>
<p>CHARACTERIZING THE DATASET</p>
<p>Solvability</p>
<p>We also demonstrate that our program synthesis examples are solvable: do the problem scenario and specification provide enough detail to actually synthesize the correct code?To test this, we run an experiment prepending the update docstring to GPT-4's context and evaluating pass@k without checking for whether the update was correctly used.As shown in Table 3, GPT-4 achieves pass@5 of 85.1; this means, in most scenarios, GPT-4 is able to provide a correct solution to the program synthesis examples within 5 trials.The performance is reasonably high across all packages in the benchmark.</p>
<p>Human Inspection We conducted manual inspection on predicted solution that GPT-4 fails in Table 3, and categorized sources of error.See details in Appendix C.2.We found errors mostly come from "Wrong Solution" and "Incomplete Solution", meaning failure to handle the edge cases of the problem statement, real mistakes due to misinterpretation of the problem statement, etc.These errors can be avoided by using stronger language models, as we will demonstrate in Section 6.2.We observed relatively few cases of incorrect test cases or bad specifications, indicating that our dataset is of sufficient quality to test knowledge editing methods.We also verify the quality of our generated data by measuring the unit test coverage on our reference solution in Appendix C.3.</p>
<p>EXPERIMENTS</p>
<p>EXPERIMENTAL SETTING</p>
<p>Base LLMs We tested two proprietary models for our prepending experiment: GPT-4 (gpt-4-0613) (Achiam et al., 2023) and Claude-3.5 (claude-3-5-sonnet-20240620) (Anthropic, 2024).For finetuning experiments, we consider three open-source code LLMs that are instruction-tuned: CodeLlama (7B-sized; Rozière et al. ( 2023)), DS-Coder-v1 (6.7B), and DS-Coder-v1.5 (7B; Guo et al. (2024a)).</p>
<p>Evaluation Scenario We evaluate approaches in the single-edit scenario, where we inject one update at a time about a single API.For measuring efficacy (UPass), we consider whether the predicted solution passes all the unit tests with the updated API but fails to do so with the old API (see Section 3 and Appendix D).To measure specificity (SPass), we measure the change in model performance on a random sample of 82 HumanEval (Chen et al., 2021) instances across 25 random single edits.</p>
<p>Knowledge Editing Approaches</p>
<p>• Prepend In this setting, we simply prepend the function update's docstring in-context at inference time (see Prompt E.3).This represents a retrieval-augmented (RAG) setting (Su et al., 2024;Zhou et al., 2022), which leads to higher inference cost and does not represent model updates.This is not considered a knowledge editing approach but establishes the performance of an effective alternative method (Onoe et al., 2023;Padmanabhan et al., 2023).• Fine-tune on update information: FT (U) In this setting, we conduct continued pretraining on the docstring describing the new behavior (Gururangan et al., 2020).This setting captures the scenario where the package designer provides a release note about the updated API function while no examples of the function being used are available.Non-applicability of existing knowledge editing methods Although a number of methods for knowledge editing have been proposed, not all of them are applicable to our setting.A line of methods including ROME, MEMIT, and REMEDI (Meng et al., 2022;2023;Hernandez et al., 2023) assume the injected data follows a strict knowledge triplet format of (subject, relation, object); this triplet structure is required for localization.Applying those methods to CodeUpdateArena is not straightforward, as code entities do not exhibit these knowledge graph-like relations.Other methods designed for similar settings do not assume this structure.However, even these more flexible approaches like MEND (Mitchell et al., 2022) and others (Hartvigsen et al., 2023;Huang et al., 2023) are optimized for models regurgitating the right short phrase response, typically less than 10 tokens.Our reference solutions contain 175.3 tokens on average.Furthermore, these methods have not proven effective in more related natural language settings such as Onoe et al. (2023).
Model Approach @1 (∆) @5 (∆) @1 (∆) @5 (∆) @1 (∆) @5 (∆) GPT-</p>
<p>RESULTS AND DISCUSSIONS</p>
<p>We present the experimental results in Table 4.All of the open-source models perform worse than proprietary models in the Prepend setting.GPT-4 and Claude-3.5 both achieve high performance.Interestingly, Claude-3.5 outperforms GPT-4 despite GPT-4 having been used to generate the dataset; this suggests that GPT-4 is not strongly favored on this benchmark beyond other frontier models.</p>
<p>Similar to the results in entity knowledge editing (Onoe et al., 2023), continuing training on update information (FT (U)) does not improve efficacy and hurts specificity.On the other hand, training the model on program synthesis examples (FT (PS)) works well, outperforming the prepend setting (See Table 4).We observe that, except on DS-Coder-v1, the open models suffer a large drop in performance on specificity.This means that although our method can use the updated API to solve downstream  11. tasks better than other baselines, retaining performance and avoiding catastrophic forgetting remains a key challenge.
UPass (Efficacy) ↑ SPass (Specificity) ↑ Method c r @1 (∆) @5 (∆) @1 (∆) @5 (∆)
Our results echo with prior work in that training models to regurgitate the injected knowledge does not help models to pragmatically use the knowledge for downstream tasks (Zhong et al., 2023) or update the status of related knowledge (Cohen et al., 2024;Jiang et al., 2024;Allen-Zhu &amp; Li, 2024).</p>
<p>In contrast, providing models with a more direct training signal like in our FT (PS) baseline or related context (Padmanabhan et al., 2023;Akyürek et al., 2024) helps with knowledge propagation and utilizing the knowledge pragmatically.</p>
<p>In the following section, we will conduct an ablation study on our methods to understand the importance of different design choices of our methods across different injected models.</p>
<p>ABLATION STUDY</p>
<p>Our method FT (PS) serves as a starting point for future editing methods on our dataset to build on.In this section, we study its design choices and demonstrate the difficulties and the trade-offs to achieve efficacy and specificity at the same time.For efficiency, we follow the same evaluation procedure as Table 4 and but only use 1 random program synthesis example per update (in total, 161) to calculate efficacy.We focus on the DS-Coder-v1, which achieves the best specificity and overall performance, and DS-Coder-v1.5, which achieves the highest efficacy in finetuning experiments.Having the target update is important: when we train the model on only program synthesis examples from random updates, Tables 5 and 11 show little or negative gain from learning only the task format.However, the random synthesis examples also matter: when we exclude them, the performance decreases as well, although to a less extent (see Table 11).</p>
<p>Impact</p>
<p>In a more complete hyperparameter sweep, we found that repeating the examples from target update twice (c = 2) is generally the optimal hyperparameter, beyond which we observe diminishing gains in efficacy and drops in specificity.Second, although different models have different optimal values, we found that larger number of random updates r will continue to decrease models' performances for efficacy and specificity.This is a different observation from prior work (Gangadhar &amp; Stratos, 2024).See more details in Appendix E.4.</p>
<p>Different models have different sensitivity to learning rates Efficacy and specificity often show tradeoffs.We vary the learning rate and plot these in Figure 3 to better understand this relationship.We observe that DS-Coder-v1 and DS-Coder-v1.5 have different sensitivities to the learning rate.First, knowledge injection, even with a learning rate as small as 1e-6, greatly harms DS-Coder-v1.5'sperformance on HumanEval whereas DS-Coder-v1's performance on HumanEval is kept unharmed.</p>
<p>Secondly, both models start to outperform the prepend setting with a learning rate greater than 1e-4.Furthermore, as the learning rate increases, the specificity and efficacy of DS-Coder-v1.5 exhibit a clear tradeoff -when the learning rate increases beyond 1e-4, DS-Coder-v1.5 undergoes a large increase in efficacy and decrease in specificity.In contrast, DS-Coder-v1's efficacy increases even with an improvement in specificity.We verify the gap from the Base model by a paired bootstrap test with p &lt; 0.05.However, we do not observe this increase in both metrics in general (e.g., Figure 12).We believe future work needs to investigate the cause of such differences and take them into account when designing new algorithms.</p>
<p>CONCLUSION</p>
<p>In this paper, we presented CodeUpdateArena, a benchmark of API updates and corresponding program synthesis examples.We demonstrated that our approach to synthesizing these leads to high-quality examples.Across three LLMs, we conduct experiments for two simple baselines.One of the baselines greatly outperforms prepending update information in context, which is different from observation from knowledge editing in entity-driven scenarios.We further conducted a comprehensive ablation study to inform future exploration.We hope our initial exploration could spur future work to develop new knowledge updating methods for code LLMs to benchmark on this setting.</p>
<p>Limitations: One limitation of CodeUpdateArena is that certain APIs are difficult to test with our dataset synthesis framework.For instance, it is difficult to generate unit tests for machine learning APIs, and can be very involved to generate tests if a significant setup is needed (e.g., a mock web server backend).Furthermore, our focus on synthetic API updates is necessary to avoid data contamination, but at the same time decreases the realism of our dataset.It would be ideal to have real software engineers annotate these kinds of updates at scale, but in preliminary experiments, we found it very difficult to come up with creative and realistic updates.Finally, our examples are restricted to Python and English-language descriptions; we believe a multilingual version of the benchmark (both human languages and code languages) would be useful.</p>
<p>Reproducibility Statement: Our dataset and code are available at .The dataset construction procedure is fully automated and documented, enabling future researchers to generate similar or related datasets.For our experiments, we detailed our hyperparameters in Appendix E. Our ablation study (Section 6.3 and Appendix E.4) depicts the model's behavior across different design choices.</p>
<p>A DATASET A.1 UPDATE TAXONOMY</p>
<p>To systematically capture different types of updates, we first create a taxonomy for update types, rooted in updates to functions.</p>
<p>Recall that we define f ← u to be the update made to an existing function f when providing it with new semantics u.We assume f always takes the form of Function([argument1, argument2,
• • • ]) → Output.
We view u as consisting of three independent components: (1) the Action that the update is applying to the API function (e.g., deprecate); (2) the Locus that the action happens at (e.g.argument or output);</p>
<p>(3) and the Aspect that the action is applying at some place for (e.g., name and data type).See Table 6 in the appendix for possible values of each component.We note that we do not focus on Action=deprecate in this work, as techniques for knowledge unlearning are different than those for knowledge editing.</p>
<p>An update type is a tuple with values for each component listed in Table 6.For example, the add-argument-NULL update type means the update is adding a completely new argument to the existing arguments of a function, and modify-argument-name update type means the update is modifying the name of an existing argument (i.e.renaming).We note that not all combination makes sense, e.g.modify-output-name; or some update types might overlap with another e.g., add-function-semantics overlaps with modify-function-semantics. We remove those and obtain 17 update types.A new boolean parameter 'inverse' is added to math.pow() to calculate the inverse power.</p>
<p>Update DocString:</p>
<p>An additional parameter 'inverse' has been introduced to the function signature, which when set to True, will return the inverse power of the numbers, i.e., 1/(x^y).This results in a non-trivially different implementation from the previous one, and the rest of the function behavior stays the same.The new parameter 'inverse' is a boolean parameter with a default value of False.When 'inverse' is set to True, the output of the function is changed to 1/(x^y), and when 'inverse' is set to False or left unspecified, the output remains the same as in the old version, which is x^y.</p>
<p>Rationale:</p>
<p>Sometimes users might need to calculate the inverse power (1 to the power of y divided by x) and this feature saves them from having to manually calculate the inverse power of a number.</p>
<p>Program:</p>
<h1>"problem": Alan needs to compute present values of these future cash flows for 'n' periods and 'r' different rates.However, computing it manually or using traditional Python methods is cumbersome and prone to errors.Help Alan by creating a program that can compute this efficiently for any 'n' and 'r'.</h1>
<p>Scenario:</p>
<p>Alan is a property investor who has recently invested in commercial projects, where the rental income fluctuates.He came across an investment formula (1/(1 + r)^n) that can approximate the present value of future cash flows.Here, 'r' represents the discount rate or interest rate, and 'n' represents the number of cash flow periods.</p>
<p>Solution Signature: def compute_present_value(r: float, n: int) -&gt; float:</p>
<p>Updated API: As discussed in Section 4.1, most of the time, we are able to retrieve full information about a function using the importlib and inspect packages.For our implementation, we decided to also separately extract a function's argument.However, these sometimes might not be possible using importlib and inspect package.Therefore, we devise two fallback options: (1) use regular expression to extract them from documentation; and if that fails, (2) we feed the docstring to GPT-4 and have it write arguments for us.We include the prompt for doing so in Appendix B.5.</p>
<h1></h1>
<p>B.2 UNIT TEST GENERATION</p>
<p>For answer generation (@ANSWER@), we let GPT-4 choose between the following strategies:</p>
<ol>
<li>
<p>directly write out the literal values of the answer (e.g.numpy.array([1,0, 2]);</p>
</li>
<li>
<p>or write a step-by-step code snippet Wei et al. (2022) to accomplish the calculation in which it could call the old API function through old_argsort(array[::-1],...) (e.g.random input in Fig. 4).→ # @ANSWER@ # @ASSERT@</p>
</li>
</ol>
<p>For assertion generation (@ANSWER@), we note that objects in different packages require different ways to check equality, for example, instead of "==", one needs to use numpy.equalfor numpy.array;and df.equals for pandas.DataFrame.To make sure the assertions are appropriately generated, we use package-specific prompts to guide GPT-4 generation.See our package instructions at Prompt B.9.</p>
<p>B.3 DEDUPLICATION</p>
<p>B.4 LITERALIZE ANSWER IN UNIT TEST</p>
<p>We define literalizing a unit test as taking the unit test, which may call an updated API, and turning it into a semantically equivalent version that does not call the updated API.To do this, we obtain answers from unit tests (e.g., pickle.dump(...)), turn the Python object to literal string, e.g.numpy.array2string(),and replace the original answer section with a simple assignment expected_result = ....This can be challenging in a few cases:</p>
<ol>
<li>
<p>when the input of the unit tests is randomly initialized (e.g., torch.randn);</p>
</li>
<li>
<p>when the input is initialized with a large dimension (e.g., image = numpy.full((1000,1000))) and result in very large literal values;</p>
</li>
<li>
<p>when an object like pandas.Dataframe might contain metadata that is hard to generally capture during literalization, or requires changes in the assertions section like re.Match object.</p>
</li>
</ol>
<p>We use pickle serialization and deserialization to literalize tests, and when this process fails as in the cases above, we invoke Claude-3.5-sonnetto edit the unit test to make the appropriate changes while preserving the semantics.After processing, we turn the answers of 4114 unit tests into literal values (out of 4221 unit tests). 4In Table 7, we include the evaluated results without literalization, which shows models like Base model has substantially higher UPass@k.Return the entire response in JSON format as a dictionary.Make sure nested brackets are closed correctly.</p>
<p>Be careful with unterminated string literal.The dictionary should contain the following:</p>
<p>1: "update_description": (as string) a short one-sentence description of the update.2: "rationale": (as string) why any hypothetical designer of the API might want to introduce these changes.</p>
<p>3: "new_function_signature": (as string) the new function signature.</p>
<p>3.1: "new_function_signature" MUST start with the full reference to the function.For example, "numpy.mean"instead of "def mean".4: "update_docstring": (as string) the added documentation that explains the new functionality of the atomic update.It MUST be self-contained, unambiguous, detailed but concise.4.1: You MUST succinctly explain the updated behavior of the new API, and how it differs from the old behavior.4.2: The "update_docstring" MUST fully specify the behavior about the update.For example, how the changes in input would change the output of new API w.r.t. the old version.4.3: A third-person MUST be able to develop a new implementation by just reading the "update_docstring" along with the old docstring.4.4: "update_docstring" could take the form of natural language, numpy-style docstring, pseudo-code examples, etc. Make the most sensible choice.If it's a string with multiple lines, output " n" as line break.4.5: DO NOT include example(s) of using the updated API in "update_docstring".</p>
<p>You will be given a function signature, optionally along with its docstring, and the Python library it belongs to.You will think what realistic update could happen to the function signature.</p>
<p>Give me 1 example of possible update(s) that a new function argument is added.</p>
<p>User Prompt: Package: {{parent_path}}
[DOC] def {{function_signature}} {{summarized_doc_string}} [/DOC]
Note: * "new_function_signature" MUST ONLY contain the function name, instead of the full reference to the function.For example, "mean" instead of "numpy.mean".* Only output the JSON in raw text.</p>
<p>B.4 Update: Unit Test skeleton</p>
<p>System prompt: You are a very experienced programer.You are good at algorithmic reasoning and writing super high quality code.</p>
<p>The API of interest is:
[OLD_SIGN] {{{{old_function_signature}}}} [/OLD_SIGN]
This API recently undergoes an update:
[DESC] {{{{update_description}}}} [/DESC]
The API now has the following new function signature: 6: If a unit test function is NOT testing throwing exception: 6.1:You MUST output a placeholder <code># @ANSWER@</code>for the right answer to be filled in.Writing the right answer is forbidden.6.2: Do not write any assertion.This is forbidden.Instead, put a placeholder <code># @ASSERT@</code>at the end of the test function.6.3: Within the unit function, the placeholders need to start at the left-most indent (i.e. 4 empty spaces -" ").7: Each test MUST be a function without any input arguments.DON'T attempt to test I/O in each unit tests.8: The function name MUST be informative.Avoid it to include generic terms like "case1" or "test1".9: Use "n" as line break.Use 4 empty spaces (" ") as Python code block indent.10: When you have Python string literal, you MUST use escape for quote -<code>"</code>or <code>'</code>; for triple quote -<code>"""</code>or `"' Ùser Prompt: This is the documentation that details the behavior about the update:
[NEW_SIGN] {{{{new_function_signature}}}} [/NEW_SIGN]</p>
<p>B.5 Update: Answer generation</p>
<p>System prompt: You are a very experienced programmer.You are good at algorithmic reasoning and writing super high quality code.</p>
<p>The API of interest is
[OLD_SIGN] {{{{old_function_signature}}}} [/OLD_SIGN]
This API recently undergoes an update:
[DESC] {{{{update_description}}}} [/DESC]
The API now has the following new function signature:
[NEW_SIGN] {{{{new_function_signature}}}} [/NEW_SIGN]
You will be given the detailed documentation about the update, and a unit test skeleton with a <code># @ANSWER@</code>.Your task is to generate a Python code block (<code>python...</code>) to replace <code># @AN-SWER@</code>.The purpose of the code block is to calculate a value for a variable called <code>expected_result</code>or <code>expected_results</code>.</p>
<p>For generating the code block, following the instructions below: 1: You MUST READ the documentation (between "[DOC]" and "[/DOC]") WORD-BY-WORD, take a pause and, understand it PERFECTLY WELL.1.1: Now look at the values of input to the API call, and contemplate on the expected behavior of the <em>new</em> API given those inputs.2: IDENTIFY whether you need to assign value to <code>expected_result</code>or <code>expected_results</code>-<code>ex-pected_result</code>if there's only 1 correct answer; <code>expected_results</code>if there's only multiple correct answers.There is only one right choice.3: Focus on the behavior of the <em>new</em> API.When deriving the expected value of <code>result</code>, work on this problem STEP-BY-STEP.Then, wisely choose one of the strategies from below: a. an assignment of a Python literal value to the variable; b. if the literal is too long or it's best to use arithmetics to get the value, DON'T write literal value.INSTEAD, use step-by-step program code to express how to arrive at the answer.4: In the code block, DO NOT call the <em>new</em> API function.For calculating the answer, you CAN call the <em>old</em> API function.However, you MUST directly call <code>old_quad</code>.ALL other ways to call the old function are FORBIDDEN.5: Within the code block, you MUST generate WITH NO leading indent.Use 4 empty spaces (" ") as indent when writing if-else, for-loop, etc.</p>
<p>User Prompt: This is the documentation that details the behavior about the update: 6.1:You MUST output a placeholder <code># @ANSWER@</code>for the right answer to be filled in.Writing the right answer is forbidden.6.2: Do not write any assertion.This is forbidden.Instead, put a placeholder <code># @ASSERT@</code>at the end of the test function.6.3: Within the unit function, the placeholders need to start at the left-most indent (i.e. 4 empty spaces -" ").7: Each test MUST be a function without any input arguments.DON'T attempt to test I/O in each unit tests.8: The function name MUST be informative.Avoid it to include generic terms like "case1" or "test1".9: Use " n" as line break.Use 4 empty spaces (" ") as Python code block indent.
[DOC] {{{{update_docstring}}}} [/DOC] [TEST] {{{{unit_test_skeleton}}}} [/TEST]</p>
<p>User Prompt:</p>
<p>In a real-world scenario, there exists some trouble to be solved:
[SCENARIO] {{{{scenario}}}} [/SCENARIO]</p>
<p>C.2 HUMAN INSPECTION OF FAILED GPT-4 ATTEMPTS</p>
<p>Taking the predicted solution in Table 3, we manually inspect 330 predicted solutions from 66 PS examples where GPT-4 failed to generate a correct solution. 5We categorize errors into 4 categories.</p>
<p>(1) Incomplete Solution: includes issues like failure to include the edge cases of the problem statement, missing or incorrect library imports, and incorrectly thrown exceptions as per the problem statement.</p>
<p>(2) Wrong Solution: real mistakes due to misinterpretation of the problem statement, using incorrect semantics for mathematical computation, etc. (3) Wrong Test Case: test cases are incorrect or cover cases not expected from the problem statement.(4) Specification Error: the specification was not complete enough for the model to choose the right output.Table 9 shows the breakdown across these categories.We note that an example may have error in multiple places (e.g. unit tests, predicted solution, etc.), and therefore the error categories are not mutually exclusive.</p>
<p>C.3 TEST COVERAGE</p>
<p>We conducted a line coverage analysis with package coverage by running all the unit tests on the reference solution.We heuristically exclude lines and find that our test coverage is high: if we exclude function definition (i.e."def") and imports (i.e."import"), our line coverage is 83.6%.Since we do not test for specific errors being thrown, excluding lines containing "except" and "raise" results in a coverage rate of 97.0%.10: Hyperparameter search over number of gradient updates when FT(U) continues pretraining on the update docstring.We found that our choice of 10 in Table 4 is optimal.In this experiment, other hyperparameters are kept the same, including the constant learning rate schedule and learning rate of 1e-3.UPass (Efficacy) ↑ SPass (Specificity) ↑ Method #gradient update @1 (∆) @5 (∆) @1 (∆) @5 (∆) To evaluate code conforming to a new, non-standard API, we use a setup as shown in Figure 10.We put the implementation of the new API (e.g.argsort) at the top of the program (after imports).</p>
<p>Then, we follow by a simple statement of setattr(numpy, "argsort", argsort) to dynamically rebind the reference of numpy.argsort(old API) to the new API.</p>
<p>Given the total number of trials n, the target value k, and the number of successes c i on example i (pass tests and use the update), we compute UPass@k over D program synthesis examples using the same form as in Chen et al. (2021):
UPass@k = 1 D D i=1 1 − ( n−c k ) ( n k )
.</p>
<p>Finally, note that when performing our editing updates, each example is updated independently; a update u ′ starts again from the base model M.</p>
<p>E EXPERIMENTATION SETUP E.1 HYPERPARAMETERS</p>
<p>In Table 10, for FT(U), we conducted a hyper-parameter search over the number of gradient update when training on the documentation about the API update.More training does not necessarily lead to degradation in specificity.</p>
<h1>training hyper # if hypers are unspecified, the values are set to be the default in <code>transformersò ptimizer: adamw_torch # as defined in TrainingArgument in</code>transformersl r: 1e-3 lr_scheduler_type: constant # in preliminary study, we found using <code>linear</code>leads to worse performances  The training sets of FT (U) and FT (PS) slightly differ and we will describe them separately.</h1>
<p>FT (U)</p>
<p>The training set only consists of a single copy of the API update information following the template in Prompt E.4.r is fixed to be 1.We observed that c = 2 is the optimal hyper, beyond which we observe diminishing gains in efficacy and drops in specificity.c is fixed to be 2.Although different models have different optimal values, we found that larger r will continue to decrease models' performances for efficacy and specificity.</p>
<p>[</p>
<p>Unit tests]: test case 1, test case 2, … [Update] numpy.argsort(a,axis=-1, kind=None, order=None, reverse=False) [Description] Adding a new argument 'reverse' which reverses the sorted indices if set to True.", [Docstring] A new parameter 'reverse' (default False) for optionally reversing the order.If set to True… CodeUpdate: Synthetic API updates Arena: Program synthesis problems using updated API [Original API] numpy.argsort(a,axis=-1, kind=None, order=None) 1. Update LLM parameters to edit their knowledge about the API usage.whether edited LLMs can apply the updated API on program synthesis problems.</p>
<p>Figure 1 :
1
Figure 1: CodeUpdateArena overview.We generate synthetic API updates, and then evaluate whether an edited model can successfully apply the updated API on a targeted program synthesis instance.</p>
<p>Based on this function and [Rationale], generate a problem setting and specification requiring it…Scenario: You are building a software for an online auction site… Problem: Create a function that returns the indices of the bidders in the order of their bids, with the highest bidder first… Step 3: Reference Solution Generation def auction_bid_ordering(bids: List[int]) -&gt; List[int]Generate unit tests… use argsort by old_argsort … Now generate an assert statement… Final test:</p>
<p>Figure 2 :
2
Figure 2: Overview of CodeUpdateArena generation pipeline.We first generate a spec for an update, unit tests for an update, and then the update's implementation.To generate program synthesis examples, we take an update, generate a problem specification, tests, and then a reference solution.</p>
<p>of training data for FT (PS) In our main experiment, the training set consists of c = 2 copies of N u examples from target update and N r = 2 examples from r = 1 random updates. 3In this section, we investigated how the construct of training data affects knowledge injection, by changing the values of c and r while fixing c + r.</p>
<p>Figure 3 :
3
Figure3: Sensitivity test on learning rate.Sensitivity for specificity is model-specific and may have trade-offs with efficacy.A large enough learning rate (e.g.1e-3) is required to outperform the prepend setting.</p>
<p>Figure 4 :
4
Figure 4: Example of unit test skeleton def test_reverse_true(): array = np.random.rand(5),ax = -1 result = np.argsort(array,axis=ax, reverse=True)</p>
<p>Only output the set of unit tests skeletons (<em>a list of strings</em>) in JSON code block (<code>json...</code>).Include <code>global {{{{package_name}}}}</code>as the first line of each unit test function.If you want to call the old function, you MUST directly call <code>old_{{{{function_name}}}}</code>.All other ways to call the old function are FORBIDDEN.</p>
<p>Figure 6 :
6
Figure 6: Distribution of Program Synthesis examples covered by different update types.</p>
<p>Figure 7 :
7
Figure 7: Number of program synthesis instances per API update in CodeUpdateArena.</p>
<p>Figure 8 :
8
Figure 8: Edit distances between canonicalized reference solutions of PS instances; pairing happens among PS of a single update.</p>
<p>Figure 10 :
10
Figure 10: Example of test execution</p>
<p>FT</p>
<h1>the low rank dim (hidden-&gt;r-&gt;hidden ) alpha: 1 dropout: 0.1</h1>
<p>FT ( PS )Figure 11 :
PS11
Figure11: Ablation study of c -the number of times to repeat N u unique program synthesis examples from target update.r is fixed to be 1.We observed that c = 2 is the optimal hyper, beyond which we observe diminishing gains in efficacy and drops in specificity.</p>
<p>Figure 12 :
12
Figure12: Ablation study of r -the number of random updates where a pair of unique program synthesis examples are drawn from each update.c is fixed to be 2.Although different models have different optimal values, we found that larger r will continue to decrease models' performances for efficacy and specificity.</p>
<p>i,j ) = a j (the update passes all test cases); (2) use of f : ci contains a call to the updated function f ; (3) specificity: the update minimally changes the language model.See examples in Figure A.1.Measuring whether samples from a code LLM pass test cases is typically done with pass@k (Chen et al., 2021).Drawing k samples from an LLM, what is the probability that one of those samples passes the test cases?This can be computed analytically without bias by drawing n &gt; k samples, observing what number c of those samples pass the test cases, and using the formula from Chen et al. (2021) (reproduced in Appendix D).In this work, we set n = 5 and k ∈ {1, 2, 5}.</p>
<p>Implementation Implementation For each test: Update Generation Arena Generation
Step 1: Specific Update GenerationDocString: …a new argument <code>reverse</code> is added… IfTrue, sort in descending order…along <code>axis</code>… If False,…numpy.argsort(a, axis=-1, kind=None, order=None, reverse=False)Description: Adding a new argument 'reverse' whichreverses the sorted indices if set to True.",• the rationale behind this updateSee Appendix B.5 for the details of the prompt. Notably, we generate the update providing the modelonly the function path and the function's docstring, obtained from the importlib library. See moredetails in Appendix B.1.</p>
<p>Based on [API function], generate an update for [Update type] …
Step 3: Update Implementation GenerationGiven: [def argsort(a, ..., reverse=False):if reverse:return old_argsort(-a,...)else:return old_argsort(a,...)</p>
<p>DocString, Description, Unit Tests] Now generate an implementation for [Signature] API function: numpy.argsort: Sort an array […] Update type: add-argument: Add a new argument to the function</p>
<h1>Unit Test 1def test_reverse_true():array = np.random.rand(</h1>
<p>Generates: Func Signature Generates: Generates:
Execute against tests,keep if passes 70%+Execute against tests,keep if passes 60%+</p>
<h1>Unit Test 1 ... # Unit Test 10 # Unit Test 1 ... # Unit Test 10 Specific Update Specific Update:</h1>
<p>Table 1 :
1
Dataset size of CodeUpdateArena over seven python packages.
Total # of unique functions Total # updates Total # PS examples Total # unit tests in PS541616706.3</p>
<p>Table 2 :
2
The average number of tokens in generated update specs and program synthesis examples.Arena (PROGRAM SYNTHESIS EXAMPLES) GENERATION Having generated update semantics u and the updated function implementation f ← u, we now generate program synthesis (PS) examples; see bottom half of Figure
Update: lengths in tokensProgram synthesis: lengths in tokensdescription docstring function impl scenario problem specification solution impl20.9129.0164.865.173.6174.14.2</p>
<p>Table 3 :
3
GPT-4's pass@5 score on our benchmark and the number of instances per package Package itertools math numpy pandas re sympy torch Avg.Despite using the same prompt, we see that the sampled solutions to different examples differ substantially.A full example from our dataset can be found in Appendix A.2.
pass@575.689.0 85.887.175.8 91.786.8 85.1count45182141939112106−synthesis examples.</p>
<p>Table 4 :
4
Knowledge editing results on CodeUpdateArena.* : comparing against the base model, the gap is significant according to a paired bootstrap test with p &lt; 0.05.</p>
<p>• Fine-tune on program synthesis examples: FT (PS) In this setting, we conduct supervised finetuning on the program synthesis examples, informing LLM how the new functions should be used.Such program synthesis examples can be collected from the API documentation, cuttingedge repositories, or generated to update code LLMs.To implement this, we select N u examples UPass (Efficacy) ↑ SPass (Specificity) ↑ Pass with updated API ↑ Base</p>
<p>Table 5 :
5
Experiments with different training set construct controlled by (c, r).Our standard setting is (2, 1).We see that the update itself is required to do well; training on unrelated examples is much worse (compare (0, 3)).However, including random update(s) in training data is beneficial when paired with the update (compare (3, 0)).* : comparing against base model, the gap is significant according to a paired bootstrap test with p &lt; 0.05.See additional results in Table</p>
<p>Table 6 :
6
Update Taxonomy componentsWe present a complete example from our dataset below.The unit tests for the update itself are omitted as these are not used by any of our methods and are only used for quality control.
Component ValuesAction Locus Aspect{add, modify, deprecate} {function, argument, output} {NULL, name, data_type, default_value, supported_value}A.2 EXAMPLEA.1 Example DataUpdate Description:</p>
<p>Import the necessary library.</p>
<h1>compute the expected_result using the provided formula 1/(1 +assert result == expected_result, f"Expected {expected_result}, but got {result}" r)\textasciicircum n →# Unit test 3 expected_result = math.pow(1 + r, n, inverse=True)def test_compute_present_value_zero_periods():r = 0.5 # At this point, we are checking the equivalence between <code>result</code>andn = 0 <code>expected_result</code> →# testing with 0 periods should compute to the cash flow amount assert result == expected_result, f'Expected {expected_result}, but got {result}'result = compute_present_value(r, n) # Unit test 9expected_result = math.pow((1 + r), -n, inverse=True) def test_compute_present_value_edge_rate():r = 1.0assert result == expected_result, n = 10f"Error: Expected result {expected_result}, but got {result}." # edge rate of 1.0 should also be handled →# Unit test 4 result = compute_present_value(r, n)def test_compute_present_value_negative_rate(): import mathtry: expected_result = math.pow(1 + r, n, inverse=True)r = -0.1n = 5 assert result == expected_result, f"Expected {expected_result}, but got {result}"import math def compute_present_value(r: float, n: int) -&gt; float: # negative rate should raise an exception compute_present_value(r, n) # Check for invalid inputs. except Exception:if r &lt; 0: assert Trueelse:raise ValueError("Rate cannot be negative.")if n &lt; 0: # Unit test 5 raise ValueError("Number of periods cannot be negative.") assert False B DATA GENERATION DETAILSdef test_compute_present_value_negative_periods(): try: B.1 PREPROCESSING API PATHr = 0.1n = -5# negative number of periods should raise an exceptioncompute_present_value(r, n) except Exception: Unit Tests:assert Trueelse:assert False # Unit test 0 # Unit test d def test_compute_present_value_small_inputs(): def test_compute_present_value_large_rate(): r = 0.1 r = 1.5 n = 3 n = 10 # small inputs for rate and number of periods # large rate should lead to small present value result = compute_present_value(r, n) result = compute_present_value(r, n) import math expected_result = math.pow(1 + r, n, inverse=True) from math import pow# Check equivalence between 'result' and 'expected_result' expected_result = pow(1 + r, n, inverse=True)assert result == expected_result assert abs(result -expected_result) &lt;= 1e-9, # Unit test 1 f"Expected {expected_result}, but got {result}" → def test_compute_present_value_large_inputs(): # Unit test 7 r = 0.9 def test_compute_present_value_one_period(): n = 100 r = 0.2 # large inputs for rate and number of periods n = 1 result = compute_present_value(r, n) # one cash flow period should return a simple discounted value import math result = compute_present_value(r, n)# Since the inverse is required, we set 'inverse' to True in math.pow() expected_result = math.pow(1 + r, n, inverse=True)expected_result = math.pow(1 + r, n, inverse=True) assert result == expected_result, f"Expected {expected_result}, but got {result}"# Unit test 8 assert result == expected_result, f"Expected {expected_result} but got {result}" def test_compute_present_value_many_periods(): # Unit test 2 r = 0.1 def test_compute_present_value_zero_rate(): n = 30 r = 0.0 # more periods should accumulate more discount n = 10 result = compute_present_value(r, n) # testing with 0 rate should compute to the cash flow amount result = compute_present_value(r, n) import mathexpected_result = 1.0</h1>
<h1>Use the updated math.pow()API to calculate the present value.return math.pow(1.0+ r, n, inverse=True)</h1>
<p>Table 7 :
7
Knowledge editing results on CodeUpdateArena without literalizing unit tests.<em> : comparing against the base model, the gap is significant according to a paired bootstrap test with p &lt; 0.05.
UPass (Efficacy) ↑SPass (Specificity) ↑ Pass with updated API ↑Base ModelApproach@1 (∆)@5 (∆)@1 (∆)@5 (∆)@1 (∆)@5 (∆)GPT-4Base Model 22.7 Prepend 42.7  </em>+20.033.3 63.6  <em>+30.3----54.0 64.5  </em>+10.474.6 84.0  <em>+9.4Claude-3.5Base Model 21.8 Prepend 61.5  </em>+39.726.7 73.4  <em>+46.7----52.2 69.5  </em>+17.361.6 78.4  <em>+16.7Base Model 12.217.539.850.028.539.9CODELLAMAPrepend FT (U)14.7  *  11.3 −0.9 17.6 +0.1 28.8  *  +2.5 21.0  *  +3.5 -−10.9-45.9  </em>−4.132.2  *  26.5  <em>+3.7 −1.945.4  *  39.6 −0.3 +5.5FT (PS)24.2  </em>+12.039.4  <em>+21.917.0  </em>−22.837.1  <em>−12.928.2 −0.345.1  </em>+5.2Base Model 12.220.449.379.331.048.2DS-CODER-V1Prepend FT (U)18.1  *  12.7 +0.5 20.9 +0.5 40.0  *  +5.9 29.4  *  +9.0 -−9.2-74.0  <em>−5.235.3  *  32.7  </em>+4.3 +1.853.6  *  50.4 +2.2 +5.4FT (PS)30.6  <em>+18.447.6  </em>+27.252.5  <em>+3.378.4 −0.837.9  </em>+6.958.1  <em>+9.9Base Model 20.029.467.179.346.864.8DS-CODER-V1.5Prepend FT (U)25.8  *  20.1 +0.1 29.9 +0.5 56.4  *  +5.8 38.4  *  +9.0 -−10.7-77.3  </em>−2.051.3  *  47.0 +0.2 +4.571.5  *  66.1 +1.3 +6.7FT (PS)32.7  <em>+12.752.1  </em>+22.737.3  <em>−29.861.2  </em>−18.038.2  <em>−8.660.3  </em>−4.5</p>
<p>You could include some illustrative code in the summary if the summary is ambiguous.3: You MUST keep the most important information, e.g.description, data type, etc. 4: The reader of your summary MUST be able to implement the function with summarized documentation.5: You MUST maintain the original structure, format, and the style of the documentation.6: Output the summarized documentation in text.The update should be as atomic as possible.It only includes one of the three possible editing actions and only happens to one place of the functions.So that the new function signature and old signature only differs at one place.<em> The update should lead to a new function signature whose implementation is non-trivially different from the old ones.An undesirable result is that the new implementation trivially calls the old function.</em> The update should be a sensible change that fits the overall topic of the function and the Python library.<em> The update should NOT contradict existing functionality of the old function.</em> The update needs to be supported by a good reason for library designer to introduce it
See Prompt B.7 for generating function update implementationSee Prompt B.8 for generating missing imports given any code.See Prompt B.9 for different packages when generating assertions and answers.B.1 Update: Docstring summarizationSystem prompt:You are a helpful assistant.You will be given documentation for an API in a popular Python library.You need to do the following:1: You MUST extract descriptions about the functionality, input parameters, and output from the originaldocumentation.2: User Prompt:{{docstring, e.g. numpy.argsort.<strong>doc</strong>}}B.2 Update: Prompt to Infer ArgumentFunction signature takes the form of<code>[full_api_path]([arguments])</code>Òutput the right [arguments].Note:<em> Output raw text.{: User Prompt}Full API path:{{{{full_api_path}}}}B.5 GENERATION PROMPT: UPDATE Documentation:{{{{documentation}}}}See Prompt B.1 for docstring summarization.See Prompt B.2 for inferring the function arguments from the function path, e.g. numpy.argsortSee Prompt B.3 for generating update specifications B.3 Update: Update SpecificationSee Prompt B.4 for generating unit test skeletons
See Prompt B.5 for generating unit test answers; part of the prompt takes corresponding instruction from Prompt B.9 to guide the model to generate for different packages.See Prompt B.6 for generating unit test assertions; part of the the prompt takes corresponding instruction from Prompt B.9 to guide the model to generate for different packages.{:Systemprompt}Infer argument of a Python function signature from documentation (output of <code>[full_api_path].__doc__</code>).</em>DONOTWrap output in a Python code block.<em>DONOTinclude documentation in the output.System prompt:You are a helpful assistant.You think deeply and creatively.Your task is to assist users to think of and instantiate interesting cases of API update.A desirable update should satisfy the following criteria: * The update should make the call site of the old function to be un-executable and one need to follow the new function signature.</em></p>
<p>Your task is to write 10 <em>high-quality</em> and <em>comprehensive</em> unit tests skeletons for testing the validity of the update.A unit test skeleton is a unit test function that only specifies the test inputs.Each unit test skeleton MUST be in raw string, not in Python code block.Return the set of unit tests skeletons in JSON code block as a list of string.For unit test skeletons generation, following the instructions below: 1: You MUST READ the documentation (between "[DOC]" and "[/DOC]") WORD-BY-WORD and understand it PERFECTLY WELL.1.1: Also, IDENTIFY important arguments: the more important arguments are ranked to the front in the new function signature.2: For unit tests, think of a diverse set of API update and the important arguments to test ALL specified behaviors in the documentation -edge-case input, edge-case output, exception raised, etc. 2.1: You need to have different edge-case values for the update and each important arguments (e.g., multi-dimensional input array with different <code>axis</code>values).3: When you generate a new unit test, look CAREFULLY at already generated unit tests, and make sure the inputs are different from previously generated unit tests as much as possible.3.1:You MUST have proper setup code for API inputs: initialize variables for testing the updated -literally, or randomly generated, etc. INCLUDE in-line comments.3.2: PREFERABLY, the input to the updated API SHOULD foreseeably lead to a <em>unique</em> execution result.4: The output of the API call MUST be assigned to a variable <code>result</code>.4.1: You MUST call the updated API, instead of old API.If required, you are allowed to call the <em>old</em> API by directly calling <code>old_quad</code>.ALL other ways to call the old function are FORBIDDEN.</p>
<p>5: If a unit test function is testing throwing exception, you should proceed with <code>try-except</code>and finish the unit test function.5.1: If the test input is meant to testing error catching, check if the API call will raise error.DON'T check error message.</p>
<p>If you want to call the old function, you MUST directly call <code>old_{{{{function_name}}}}</code>.All other ways to call the old function are FORBIDDEN.You are a very experienced programer.You are good at algorithmic reasoning and writing super high quality code.You will be given a unit test function that misses assertion statements to either: 1. check equivalence between <code>result</code>and <code>expected_result2.orcheckequivalence between</code>result<code>and any values in</code>expected_results<code>( i.e. multiple correct answer).Your task is to generate a Python code block (```python...```) to replace</code># @ASSERT@<code>.You will be given the detailed documentation about the update.Your task is to write high quality implementation for the *new* API function in Python code block (```python...```).Return the entire response in JSON format as a dictionary.Make sure nested brackets are closed correctly.Be careful with unterminated string literal.The dictionary should contain the following: 1: "scenario": (as string) a real-world scenario that the problem is situated in.Keep it medium short.1.1:Avoidincludinginformation -e.g.exact term -about API changes, or package needs to be used in "problem".2:"problem":(asstring) problem specification that needs solving by a Python function.Keep it short.2.1:Avoid giving imperative instruction on how to solve the problem.MUST Remain at high-level.Avoid including information -e.g.exact term -about API changes, or package needs to be used in "problem".2.2: Make sure the description of the input is well connected and blend into the description of the scenario.2.3: Design the problem such that each input to the solution is meaningfully used in the code.The problem *MUST* non-trivially benefit from the update (i.e.new API); so that solving the problem with the old API is not possible, or requires more efforts (e.g.need to write longer code).The solution of the problem must accept {{num_param}} parameter(s).Your task is to write 10 *high-quality* and *comprehensive* unit tests skeletons for testing validity of any solution function to a problem specification.A unit test skeleton is a unit test function except the right answer being clearly specified.Each unit test skeleton MUST be in raw string, not in Python code block.Return the set of unit tests skeletons in JSON code block as a list of string.For unit test skeletons generation, following the instructions below: 1: You MUST READ the problem specification (between "[PROBLEM]" and "[/PROBLEM]") WORD-BY-WORD and understand it PERFECTLY WELL.1.1: Also, IDENTIFY important arguments: the more important arguments are ranked to the front in the new You need to have different edge-case values for the update and each important arguments (e.g., multi-dimensional input array with different</code>axis<code>values).3: When you generate a new unit test, look CAREFULLY at already generated unit tests, and make sure the inputs are different from previously generated unit tests as much as possible.3.1:You MUST have proper setup code for solution function inputs: initialize variables for testing the updated -literally, or randomly generated, etc. INCLUDE in-line comments.3.2: PREFERABLY, the input to the solution function call SHOULD foreseeably lead to a *unique* execution result.4: The output of the solution function MUST be assigned to a variable</code>result<code>.4.1: You MUST call the solution function.5: If a unit test function is testing throwing exception, you should proceed with</code>try-except<code _package_name="{{{package_name">and finish the unit test function.5.1: If the test input is meant to testing error catching, check if the API call will raise error.DON'T check error message.6: If a unit test function is NOT testing throwing exception:
Some special notes for</code>}}}<code _package_name="{package_name">package:{{{{package_instruct}}}}{{% endif %}}function signature. B.6 Update: Assertion generation 2: For unit tests, READ the scenario description (between [SCENARIO]...[/SCENARIO]) WORD-BY-WORD and understand it PERFECTLY WELL.2.1: Contemplate, and think of a diverse set of representative inputs to solution function; this set of input System prompt: should capture possible and interesting cases which solution function might encounter after deployment.3: "solution_signature": (as string) the function signature of the solution function. 2.2: BE SURE to test ALL specified behaviors in the problem specification -edge-case input, edge-case3.1: the function name should be derived from "scenario". output, exception raised, etc.2.3:Give me 1 diverse program synthesis example(s).User Prompt:In Python package</code>}<code _api_path="{api_path">, there's an API function</code>}<code _package_name="{{{package_name">as follows: [OLD_SIGN]{{old_func}} [/OLD_SIGN]Maintainer of the package thinks it's best to introduce the following update User Prompt: [DESC] [TEST] {{update_description}} {{{{unit_test_skeleton}}}} [/TEST] [/DESC]This is because {{% if package_instruct %}} [RATIONALE] Remember some special features of</code>}}}}`package: {{update_rationale}} {{{{package_instruct}}}} {{% endif %}} [/RATIONALE]The function docstring now differs with previous version in the following way: B.7 Update: Updated Function Implementation [DOC]{{docstring_diff}}System prompt: [/DOC]You are a very experienced programer. You are good at algorithmic reasoning and writing super high qualitycode. And the function has the following new signature:[NEW_SIGN]{{new_function_signature}} The API of interest is [OLD_SIGN] [/NEW_SIGN]{{{{old_function_signature}}}}[/OLD_SIGN]This API recently undergoes an update:[DESC]{{{{update_description}}}} Note: [/DESC] Only output the JSON in raw text.The API now has the following new function signature:[NEW_SIGN]{{{{new_function_signature}}}}[/NEW_SIGN] B.11 ProgSyn: Unit Test SkeletonAnd the old API is renamed to: System prompt:[OLD_SIGN] You are a very experienced programer. You are good at algorithmic reasoning and writing super high quality{{{{renamed_old_function_signature}}}} code.[/OLD_SIGN]{{% if package_instruct %}</p>
<p>Table 8
8
shows that our benchmark covers a range of different types of API functionalities.
add-argument-semanticsadd-output-data_type modify-function-nameadd-output-semantics add-argument-supported_value(s) add-argument-data_type8% 4% 12%9% 10%10% 7%6% 4% 1% 2% 14% 5% 4% 2% 2%modify-output-semantics modify-argument-name modify-argument-semantics modify-argument-supported_value(s) modify-argument-default_value(s)add-argumentadd-argument-default_value(s) modify-output-data_type add-function modify-argument-data_type</p>
<p>Table 8 :
8
Diversity of packages in CodeUpdateArena.Our benchmark covers a range of different types of API functionalities.Our python version is 3.11.5.
PackageTypeStandard / External lib.reString operationsStandardmathArithmetic operationsStandarditertoolsPython data structure operationsStandardtorch==2.0.1, numpy==1.25.2Vector operationsExternalsympy==1.12Symbolic operationsExternalpandas==2.1.0Table operationsExternal</p>
<p>Table 9 :
9
Manual categorization of 66 failures cases of GPT-4 on program synthesis examples.Categories are not exclusive.As described in Section 3, to evaluate each predicted solution ci , our evaluation procedure executes the set of test cases twice, once with the updated API and once with the old API.
Error CategoryCountIncomplete Solution29Wrong Solution33Wrong Test Case13Specification Error2D IMPLEMENTATION DETAILS OF COMPUTING UPass@k</p>
<p>Table 11 :
11
Experiments on DS-Coder-v1 with different training set construct controlled by (c, r).Despite to a lesser degree, the observations in Table5hold for DS-Coder-v1 as well -training on unrelated examples is worse, but including random updates along with the true updates help.: comparing against base model, the gap is significant according to a paired bootstrap test with p &lt; 0.05.E.3 EVALUATION PROCEDURE FOR FINETUNING EXPERIMENTSRecall that our benchmark CodeUpdateArena is structured by pairing each executable API update with n program synthesis examples.We treat the n program synthesis examples as an ordered list.
UPass (Efficacy) ↑SPass (Specificity) ↑Methodcr@1 (∆)@5 (∆)@1 (∆)@5 (∆)DS-Coder-v1--2.64.349.379.3+ Prepend--10.7  <em>+8.118.6  </em>+14.3--2130.8  <em>+28.249.7  </em>+45.352.5  <em>+3.378.4 −0.8+ FT (PS)1 32 025.7  *  30.4  </em>+23.1 +27.845.3  *  41.6  <em>+41.0 +37.353.3  *  51.6  </em>+4.0 +2.379.8 +0.5 77.0  *  −2.2038.3  <em>+5.718.0+13.751.5  </em>+2.279.1 −0.1# where the lora are inserted:target_modules = ["q_proj", "v_proj"]
*</p>
<p>https://youtu.be/outcGtbnMuQ?t=789
When a small number of unit tests are failed, they are often incorrect unit tests.
We take a pair of unique program synthesis examples from each r random updates
We verify the correctness by executing reference solution on the units and receiving perfect performance.
The inspection was conducted on a preliminary version of the benchmark not including pandas.
To generate the code block, following the instructions below: 1: First of all, you MUST CAREFULLY READ the documentation about the update (between "[DOC]" and "[/DOC]") WORD-BY-WORD and understand it PERFECTLY WELL.2: Before arriving at the new implementation, take a deep breath and work on this problem STEP-BY-STEP.B.8 Generate missing importSystem prompt: You are a very experienced programer.You are good at algorithmic reasoning and writing super high quality code.Your task is to write import statements to include any package dependency before running the code.Return import statements in Python code block (<code>python...</code>).To generate the code block, following the instructions below: 1: First of all, read the code WORD-BY-WORD and understand it PERFECTLY WELL.2: DO NOT miss type hints in function signature, function body, etc. 3: If no import statements is required, output an empty Python code block.Only output the Python code block (<code>python...</code>).B.9 Package Instruction re: Assertion generation: 1: To compare <code>re.Match</code>object, <code>==</code>doesn't work.One should use <code>group()</code>method to obtain the string and then compare, e.g.<code>m1.group() == m2.group()</code>.2: When no match is found, the output will be None.Make sure this situation is dealt with.You are a helpful assistant.You think deeply and creatively.Your task is to think of and write interesting tutorial(s) for an API update.mainly <problem, solution>.You will be given the full information about an update to an existing Python package.You should think of usage (i.e. program synthesis example) of the updated API signature that satisfy the following criteria: * the problem scenario posed by the program synthesis example MUST follow the general functionality of the (old and new) API.<em> the problem scenario MUST be affected and preferably benefited by the API update.By benefit, it means the code complexity of the solution will be reduced.</em> the problem MUST be at least medium hard, so that the solution MUST make <em>non-trivial</em> use of the API's functionality.<em> Be given the number of parameters that the solution accepts.Only output the set of unit tests skeletons (</em>a list of strings*) in JSON code block (<code>json...</code>).B.12 ProgSyn: Answer generationSystem prompt: You are a very experienced programer.You are good at algorithmic reasoning and writing super high quality code.In a real-world scenario, there exists some trouble to be solved:Luckily, someone could solve this trouble by writing a function, as long as the solution function satisfy the following problem specification:An ideal solution function takes the following function signature:You will be a unit test skeleton with a <code># @ANSWER@</code>.Your task is to generate a Python code block (<code>python...</code>) to replace "<code># @ANSWER@".The purpose of the code block is to calculate a value for a variable called</code>expected_result<code>or</code>expected_results<code>.For generating the code block, following the instructions below: 1: You MUST READ the problem specification (between "[PROBLEM]" and "[/PROBLEM]") WORD-BY-WORD, take a pause and, understand it PERFECTLY WELL.1.1: Now look at the values of input to the solution function, and contemplate on the expected behavior of the solution function given those inputs.2: IDENTIFY whether you need to assign value to</code>expected_result<code>or</code>expected_results<code>.There is only one right choice.3: Before arriving at an answer, ALWAYS take a deep breath and work on this problem STEP-BY-STEP.Then, wisely choose one of the strategies from below: a. an assignment of a Python literal value to the variable; b. if the literal is too long or it's best to use arithmetics to get the value, DON'T write literal value.INSTEAD, use step-by-step program code to express how to arrive at the answer.4: Within the code block, you MUST generate WITH NO leading indent.Use 4 empty spaces (" ") as indent when writing if-else, for-loop, etc.User Prompt:To write code to calculate</code>expected_result<code>or</code>expected_results<code>(strategy b), maybe the following two functions are useful:The first function comes from package</code>numpy`.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Deductive closure training of language models for coherence, accuracy, and updatability. Afra Feyza Akyürek, Ekin Akyürek, Leshem Choshen, Derry Wijaya, Jacob Andreas, arXiv:2401.085742024arXiv preprint</p>
<p>Physics of language models: Part 3.1, knowledge storage and extraction. Zeyuan Allen, -Zhu , Yuanzhi Li, Forty-first International Conference on Machine Learning, ICML 2024. Vienna, AustriaJuly 21-27, 2024. 2024OpenReview.net</p>
<p>Claude 3.5 sonnet. Anthropic, 2024</p>
<p>Program synthesis with large language models. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, arXiv:2108.077322021arXiv preprint</p>
<p>Reading Wikipedia to answer open-domain questions. Danqi Chen, Adam Fisch, Jason Weston, Antoine Bordes, 2017Association for Computational Linguistics</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, David W Cummings, Matthias Plappert, Fotios Chantzis, ArXiv, abs/2107.03374Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew M Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Elizabeth Barnes, Ariel Herbert-Voss, William H. Guss, Alex Nichol, Igor Babuschkin, Suchir Balaji, Shantanu Jain, Andrew Carr; Bob McGrew, Dario Amodei, Sam McCandlishJan Leike. 2021235755472</p>
<p>RECKONING: Reasoning through dynamic knowledge encoding. Zeming Chen, Gail Weiss, Eric Mitchell, Asli Celikyilmaz, Antoine Bosselut, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Evaluating the ripple effects of knowledge editing in language models. Roi Cohen, Eden Biran, Ori Yoran, Amir Globerson, Mor Geva, Transactions of the Association for Computational Linguistics. 122024</p>
<p>Editing factual knowledge in language models. Nicola De Cao, Wilker Aziz, Ivan Titov, 10.18653/v1/2021.emnlp-main.522Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Marie-Francine Moens, Xuanjing Huang, Lucia Specia, Scott , Wen-Tau Yih, the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational LinguisticsNovember 2021Online and Punta Cana</p>
<p>. Deepseek-Ai , : , Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y K Li, Wenfeng Liang, Fangyun Lin, A X Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R X Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, 2024Shunfeng Zhou, Qihao Zhu, and Yuheng Zou. DeepSeek LLM: Scaling Open-Source Language Models with Longtermism</p>
<p>CrossCodeEval: A Diverse and Multilingual Benchmark for Cross-File Code Completion. Yangruibo Ding, Zijian Wang, Uddin Wasi, Hantian Ahmad, Ming Ding, Nihal Tan, Jain, Krishna Murali, Ramesh Ramanathan, Parminder Nallapati, Dan Bhatia, Bing Roth, Xiang, ArXiv, abs/2310.112482023</p>
<p>ClassEval: A manually-crafted benchmark for evaluating llms on class-level code generation. Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang, Junwei Liu, Yixuan Chen, Jiayi Feng, Chaofeng Sha, Xin Peng, Yiling Lou, arXiv:2308.018612023arXiv preprint</p>
<p>Model editing by standard fine-tuning. Govind Krishnan, Gangadhar , Karl Stratos, 10.18653/v1/2024.findings-acl.352Findings of the Association for Computational Linguistics ACL 2024. Lun-Wei Ku, Andre Martins, Vivek Srikumar, Bangkok, ThailandAssociation for Computational LinguisticsAugust 2024and virtual meeting</p>
<p>Alex Gu, Baptiste Rozière, Hugh Leather, Armando Solar-Lezama, Gabriel Synnaeve, Sida I Wang, CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution. 2024</p>
<p>DeepSeek-Coder: When the Large Language Model Meets Programming -The Rise of Code Intelligence. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y Wu, Y K Li, Fuli Luo, Yingfei Xiong, Wenfeng Liang, 2024a</p>
<p>Jiawei Guo, Ziming Li, Xueling Liu, Kaijing Ma, Tianyu Zheng, Zhouliang Yu, Ding Pan, Yizhi Li, Ruibo Liu, Yue Wang, arXiv:2404.03543Evaluating Code Editing Capability of Large Language Models. 2024barXiv preprint</p>
<p>Visual programming: Compositional visual reasoning without training. Tanmay Gupta, Aniruddha Kembhavi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)June 2023</p>
<p>Don't stop pretraining: Adapt language models to domains and tasks. Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, Noah A Smith, 10.18653/v1/2020.acl-main.740Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Dan Jurafsky, Joyce Chai, Natalie Schluter, Joel Tetreault, the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJuly 2020</p>
<p>Retrieval augmented language model pre-training. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Mingwei Chang, International conference on machine learning. PMLR2020</p>
<p>Aging with grace: Lifelong model editing with discrete key-value adaptors. Thomas Hartvigsen, Swami Sankaranarayanan, Hamid Palangi, Yoon Kim, Marzyeh Ghassemi, Advances in Neural Information Processing Systems. 2023</p>
<p>Inspecting and editing knowledge representations in language models. Evan Hernandez, Belinda Z Li, Jacob Andreas, Arxiv2023</p>
<p>Tool documentation enables zero-shot tool-usage with large language models. Cheng-Yu Hsieh, Sibei Chen, Chun-Liang Li, Yasuhisa Fujii, Alexander J Ratner, Chen-Yu Lee, Ranjay Krishna, Tomas Pfister, ArXiv, abs/2308.006752023260351459</p>
<p>LoRA: Low-rank adaptation of large language models. J Edward, Phillip Hu, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, International Conference on Learning Representations. 2022</p>
<p>Propagation and Pitfalls: Reasoning-based Assessment of Knowledge Editing through Counterfactual Tasks. Wenyue Hua, Jiang Guo, Mingwen Dong, He Zhu, Patrick Ng, Zhiguo Wang, ArXiv, abs/2401.175852024</p>
<p>Transformerpatcher: One mistake worth one neuron. Zeyu Huang, Yikang Shen, Xiaofeng Zhang, Jie Zhou, Wenge Rong, Zhang Xiong, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Instruction-tuned language models are better knowledge learners. Zhengbao Jiang, Zhiqing Sun, Weijia Shi, Pedro Rodriguez, Chunting Zhou, Graham Neubig, Xi Lin, Wen-Tau Yih, Srini Iyer, 10.18653/v1/2024.acl-long.296Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. Lun-Wei Ku, Andre Martins, Vivek Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational LinguisticsAugust 20241</p>
<p>SWE-bench: Can language models resolve real-world github issues?. Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, ; Karthik R Narasimhan, The Twelfth International Conference on Learning Representations. Ofir Press2024</p>
<p>DS-1000: a natural and reliable benchmark for data science code generation. Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wen-Tau Yih, Daniel Fried, Sida Wang, Tao Yu, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine Learning2023ICML'23. JMLR.org</p>
<p>Ambigdocs: Reasoning across documents on different entities under the same name. Yoonsang Lee, Xi Ye, Eunsol Choi, arXiv:2404.124472024arXiv preprint</p>
<p>Retrieval-augmented generation for knowledge-intensive NLP tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>. Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Sankalp Siva, Dmitry Patel, Marco Abulkhanov, Manan Zocca, Zhihan Dey, Nour Zhang, Urvashi Fahmy, Wenhao Bhattacharyya, Swayam Yu, Sasha Singh, Paulo Luccioni, Maxim Villegas, Fedor Kunakov, Manuel Zhdanov, Tony Romero, Nadav Lee, Jennifer Timor, Claire Ding, Hailey Schlesinger, Schoelkopf, Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva ReddyJanArjun GuhaDaniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes, Thomas WolfLeandro von Werra, and Harm de Vries. Starcoder: may the source be with you!, 2023</p>
<p>Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, Lingming Zhang, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Repobench: Benchmarking repository-level code auto-completion systems. Tianyang Liu, Canwen Xu, Julian Mcauley, 2024</p>
<p>Locating and Editing Factual Associations in GPT. Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, Curran Associates, Inc202235</p>
<p>Mass-editing memory in a transformer. Kevin Meng, Sen Arnab, Alex J Sharma, Yonatan Andonian, David Belinkov, Bau, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Fast model editing at scale. Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, Christopher D Manning, International Conference on Learning Representations. 2022</p>
<p>Instructir: A benchmark for instruction following of information retrieval models. Hanseok Oh, Hyunji Lee, Seonghyeon Ye, Haebin Shin, Hansol Jang, Changwook Jun, Minjoon Seo, arXiv:2402.143342024arXiv preprint</p>
<p>Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge. Yasumasa Onoe, Michael Zhang, Greg Shankar Padmanabhan, Eunsol Durrett, Choi, 10.18653/v1/2023.acl-long.300Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 20231</p>
<p>Propagating knowledge updates to LMs through distillation. Yasumasa Shankar Padmanabhan, Onoe, J Q Michael, Greg Zhang, Eunsol Durrett, Choi, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>RepoHyper: Better Context Retrieval Is All You Need for Repository-Level Code Completion. N Huy, Phan, N Hoang, Tien N Phan, Nguyen, D Q Nghi, Bui, ArXiv, abs/2403.060952024</p>
<p>TAXI: evaluating categorical knowledge editing for language models. Derek Powell, Walter Gerych, Thomas Hartvigsen, 10.48550/arXiv.2404.150042024</p>
<p>Code llama: Open foundation models for code. Jonas Baptiste Rozière, Fabian Gehring, Sten Gloeckle, Itai Sootla, Gat, Ellen Xiaoqing, Yossi Tan, Jingyu Adi, Tal Liu, Jérémy Remez, Artyom Rapin, Ivan Kozhevnikov, Joanna Evtimov, Manish Bitton, Cristian Bhatt, Aaron Canton-Ferrer, Wenhan Grattafiori, Alexandre Xiong, Jade Défossez, Faisal Copet, Hugo Azhar, Louis Touvron, Nicolas Martin, Thomas Usunier, Gabriel Scialom, Synnaeve, 10.48550/arXiv.2308.129502023</p>
<p>Repository-level prompt generation for large language models of code. H Disha Shrivastava, Daniel Larochelle, Tarlow, International Conference on Machine Learning. 2022</p>
<p>Musr: Testing the limits of chain-of-thought with multistep soft reasoning. Zayne Sprague, Xi Ye, Kaj Bostrom, Swarat Chaudhuri, Greg Durrett, Proceedings of ICLR (spotlight). ICLR (spotlight)2024</p>
<p>ARKS: active retrieval in knowledge soup for code generation. Hongjin Su, Shuyang Jiang, Yuhang Lai, Haoyuan Wu, Boao Shi, Che Liu, Qian Liu, Tao Yu, 10.48550/arXiv.2402.123172024</p>
<p>Minicheck: Efficient fact-checking of llms on grounding documents. Liyan Tang, Philippe Laban, Greg Durrett, arXiv2024</p>
<p>Knowledge Editing for Large Language Models: A Survey. Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, Jundong Li, ArXiv, abs/2310.162182023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, Curran Associates, Inc202235</p>
<p>Codebenchgen: Creating scalable execution-based code generation benchmarks. Yiqing Xie, Alex Xie, Divyanshu Sheth, Pengfei Liu, Daniel Fried, Carolyn Rose, arXiv:2404.00566Thirty-seventh Conference on Neural Information Processing Systems. Xi Ye, Qiaochu Chen, Isil Dillig, Greg Durrett, 2024. 2023arXiv preprintSatLM: Satisfiability-aided language models using declarative prompting</p>
<p>Genie: Achieving human parity in content-grounded datasets generation. Asaf Yehudai, Boaz Carmeli, Yosi Mass, Ofir Arviv, Nathaniel Mills, Assaf Toledo, Eyal Shnarch, Leshem Choshen, ArXiv, abs/2401.143672024</p>
<p>RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation. Fengji Zhang, B Chen, Yue Zhang, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, Weizhu Chen, Conference on Empirical Methods in Natural Language Processing. 2023a</p>
<p>ToolCoder: Teach Code Generation Models to use API search tools. Kechi Zhang, Ge Li, Jia Li, Zhuo Li, Zhi Jin, ArXiv, abs/2305.040322023b</p>
<p>SituatedQA: Incorporating extra-linguistic contexts into QA. Michael Zhang, Eunsol Choi, 10.18653/v1/2021.emnlp-main.586Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Marie-Francine Moens, Xuanjing Huang, Lucia Specia, Scott , Wen-Tau Yih, the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational LinguisticsNovember 2021Online and Punta Cana</p>
<p>Set the clock: Temporal alignment of pretrained language models. Bowen Zhao, Zander Brumbaugh, Yizhong Wang, Hannaneh Hajishirzi, Noah A Smith, arXiv:2402.167972024arXiv preprint</p>
<p>MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions. Zexuan Zhong, Zhengxuan Wu, Christopher D Manning, Christopher Potts, Danqi Chen, 10.18653/v1/2023.emnlp-main.971Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational LinguisticsDecember 6-10, 20232023</p>
<p>Prepend in jinja2 [INST] Update note: There's an recent update to a function <code>{{old_function_signature}}</code>-{{update_description}}. The function now has a new function signature -<code>{{new_function_signature}}</code>. Here's a detailed documentation about the update: [DOC] {{update_docstring}} [/DOC] Your task is to write a Python solution to a problem in a real-world scenario. The Python code must be between. Shuyan Zhou, Uri Alon, Frank F Xu, Zhiruo Wang, Zhengbao Jiang, Graham Neubig, Docprompting, arXiv:2207.05987,2022.FT(PS)E.1INST]Generating code by retrieving the docs. arXiv preprint{% if include_update -%} Update note: There's an recent update to a function <code>{{old_function_signature}}</code>-{{update_description}}. The function now has a new function signature -`{{new_function_signature}}. Here's a detailed documentation about the update: [DOC] {{update_docstring}} [/DOC] {% endif %} Your task is to write a Python solution to a problem in a real-world scenario. The Python code must be between [PYTHON] and [/PYTHON] tags. Scenario: {{example_scenario}} Problem: {{example_problem}} Solution signature: {{example_solution_signature}} [TEST] {{example_unit_tests}} [/TEST] [/INST] [PYTHON] {{example_solution}} [/PYTHON] [INST</p>            </div>
        </div>

    </div>
</body>
</html>
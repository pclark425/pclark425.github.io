<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-827 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-827</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-827</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-22.html">extraction-schema-22</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <p><strong>Paper ID:</strong> paper-274656194</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2412.09429v4.pdf" target="_blank">From Intention To Implementation: Automating Biomedical Research via LLMs</a></p>
                <p><strong>Paper Abstract:</strong> Conventional biomedical research is increasingly labor-intensive due to the exponential growth of scientific literature and datasets. Artificial intelligence (AI), particularly Large Language Models (LLMs), has the potential to revolutionize this process by automating various steps. Still, significant challenges remain, including the need for multidisciplinary expertise, logicality of experimental design, and performance measurements. This paper introduces BioResearcher, the first end-to-end automated system designed to streamline the entire biomedical research process involving dry lab experiments. BioResearcher employs a modular multi-agent architecture, integrating specialized agents for search, literature processing, experimental design, and programming. By decomposing complex tasks into logically related sub-tasks and utilizing a hierarchical learning approach, BioResearcher effectively addresses the challenges of multidisciplinary requirements and logical complexity. Furthermore, BioResearcher incorporates an LLM-based reviewer for in-process quality control and introduces novel evaluation metrics to assess the quality and automation of experimental protocols. BioResearcher successfully achieves an average execution success rate of 63.07% across eight previously unmet research objectives. The generated protocols, on average, outperform typical agent systems by 22.0% on five quality metrics. The system demonstrates significant potential to reduce researchers' workloads and accelerate biomedical discoveries, paving the way for future innovations in automated research systems.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e827.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e827.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioResearcher</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioResearcher (modular LLM-based multi-agent research automation system)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end Level-2 automated system for dry-lab biomedical research that composes specialized LLM agents (search, literature processing, experimental design, programming) with hierarchical report/design workflows and LLM-based reviewers to produce executable protocols and code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>BioResearcher (multi-agent system)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A modular multi-agent architecture built on a foundation LLM (GPT-4o) where each agent has a role (query generator, filter, report generator, analyst, experiment designer, code generator, reviewer); uses hierarchical report generation and hierarchical learning for protocol design, Retrieval-Augmented Generation (RAG) with curated analyses, and an execution-feedback loop (Docker) for code refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>End-to-end biomedical research automation: literature search → report generation → experimental protocol design → dry-lab code generation and execution</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>planning; multi-step reasoning; tool use (search, retrieval); sequential decision-making (design → program → execute loop)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Average execution success rate 63.07% across eight previously unmet research objectives (max 87.50%); protocol quality improvements vs best baseline: Completeness +0.254 (62.7% relative), Level of Detail +0.206 (30.0%), Correctness +0.411 (84.9%); generated protocols had 5.9× sentences/step and 4.8× steps/protocol vs baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Modular multi-agent architecture; hierarchical report generation; RAG using curated high-referability sections; LLM-based reviewer agents for iterative quality control; analyst agent; query generator and filter agents; execution-feedback loop with Docker sandboxing.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Primarily prompting / role-based in-context use of a foundation LLM (GPT-4o); no new fine-tuning or RLHF reported for BioResearcher itself.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>architectural change; prompting & workflow engineering (hierarchical processing); hybrid approach (RAG + reviewer + execution loop)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Combines decomposition of complex tasks into role-specific LLM agents, hierarchical (section-wise) report generation to avoid long-context failure, hierarchical learning for protocol headings→outline→details using retrieved analyses as references, an LLM-based reviewer for iterative QC, and a code execution+feedback loop (Docker) to refine generated R code.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Substantial: protocol metrics increased (Completeness +0.254, Level of Detail +0.206, Correctness +0.411) relative to best baseline; program execution success averaged 63.07% vs baselines ≈0%; removing reviewers reduced overall score by 0.098 and Correctness by 0.045; removing the report analyst reduced Completeness from 0.582 to 0.559.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Paper argues general LLMs do well on QA-style tasks (short answers, exam-style, summarization) but struggle on complex, long, domain-specific, multi-step scientific workflows due to: (1) long-context inputs causing truncated/short outputs, (2) need for deep domain knowledge and long logical chains, (3) lack of instruction data to fine-tune for such complex tasks, (4) instability/hallucination when chaining many steps or taking actions iteratively, and (5) iterative-agent summarization/integration degradation in long-context environments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Intention To Implementation: Automating Biomedical Research via LLMs', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e827.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e827.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented generation baseline that retrieves relevant content and generates an answer in a single step using retrieved context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>RAG (baseline system)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Naive retrieval-augmented generation pipeline: retrieve documents (vector store / embeddings) and produce a single-step generated protocol or answer using retrieved context; used here as a baseline for protocol generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Protocol generation and code generation from retrieved literature (single-step RAG pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>multi-step reasoning; document-grounded generation; tool use (retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Protocol quality (Table 2): Completeness 0.405, Detail 0.687, Correctness 0.483, Logical Soundness 0.973, Structure 0.908, Overall 3.456; programming/code execution success for baselines (including RAG) reported as near 0% (almost invariably fail to produce executable code).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Retrieval + generation; single-step integration of retrieved long context.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>RAG pipeline using prompting and retrieval; models used without new fine-tuning in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Authors attribute RAG's relative advantage over iterative agent methods in some long-context situations to collapsing summarization/integration errors in iterative systems; nonetheless, RAG still fails at producing executable code because single-step generation cannot handle multi-step procedural execution/implementation requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Intention To Implementation: Automating Biomedical Research via LLMs', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e827.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e827.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct (reasoning + acting within LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent framework that interleaves chain-of-thought style reasoning with actions (tool use) to solve tasks via LLM-driven planning and acting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>ReAct (baseline system)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Agent paradigm that combines reasoning traces (thoughts) with actions (tool calls) in-model to handle decision-making and tool usage during multi-step tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Iterative reasoning + tool use for protocol generation and retrieval-assisted research tasks</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>planning; tool use; multi-step reasoning; sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Protocol quality (Table 2): Completeness 0.364, Detail 0.577, Correctness 0.484, Logical Soundness 0.963, Structure 0.897, Overall 3.285; when equipped with retrieval tools, ReAct (and other iterative baselines) produced protocols with low executability and code execution success rates near zero.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Interleaved chain-of-thought reasoning and tool actions within a single LLM agent.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Prompting and in-context chain-of-thought style instructions; no new fine-tuning reported in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Paper suggests iterative agent systems (like ReAct) lose important information during repeated summarization/iteration in long-context tasks, leading to degraded final outputs and low procedural/executable performance despite plausible reasoning traces.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Intention To Implementation: Automating Biomedical Research via LLMs', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e827.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e827.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Plan-and-Execute</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Plan-and-Execute (iterative planning-execution agent baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative framework that first plans a sequence of steps and then executes them, with potential re-planning on failures; used as a baseline for multi-step procedural tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Plan-and-Execute (baseline system)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two-phase iterative agent: generate a plan (outline), then execute steps (potentially with tool calls) in sequence, with an optional iterative loop for re-planning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Iterative planning and execution for experimental protocol generation and implementation</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>planning; sequential decision-making; multi-step reasoning; tool use</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Protocol quality (Table 2): Completeness 0.380, Detail 0.587, Correctness 0.483, Logical Soundness 0.965, Structure 0.900, Overall 3.314; code execution success similarly near zero when tested end-to-end.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Separate planning and execution phases; iterative planning loop.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Prompting/in-context with plan-and-execute prompts; no fine-tuning reported in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Authors note that iterative frameworks can suffer from degradation when handling very long contexts and many iterative summarizations, losing critical details needed for executable procedural outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Intention To Implementation: Automating Biomedical Research via LLMs', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e827.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e827.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (foundation LLM used as agent backbone and judge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A foundation LLM (used as the base model for all BioResearcher agents, and used as the main LLM judge/evaluator in experiments) employed in prompting/role-based fashion rather than further fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large foundation language model used as the single backbone for all agents in BioResearcher and as the primary LLM judge; operated with varied temperature settings across agents (query generator 0.7, reviewer/judge 0.1, others 0.5).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>Medical QA / medical exam / clinical summarization (general categories mentioned in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Used as backbone for multi-agent interactive pipeline (search, literature processing, design, programming) and as judge for protocol evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use (APIs for search), multi-step reasoning, planning, code generation</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>As the core of BioResearcher, supported an overall system execution success rate of 63.07% (system-level metric); GPT-4o was also used as LLM judge achieving 80.4% agreement with human experts in protocol evaluation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Foundation LLM used in multiple agent roles via prompting; no model-internal architectural modification reported.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Used off-the-shelf (no further fine-tuning reported); agents rely on prompting, role instructions, and hierarchical workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting & system-level architectural design (multi-agent decomposition, hierarchical processing, reviewer loops)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Rather than training-time interventions, the paper applies system-level interventions (agent decomposition, hierarchical report/design pipelines, RAG with curated references, LLM reviewers, and execution-feedback) to improve procedural performance of the LLM in interactive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>When embedded in the BioResearcher multi-agent system and workflows, GPT-4o produced higher-quality experimental protocols and executable code (system average code execution success 63.07%) compared to using GPT-4o in naïve single-step or baseline agent setups which produced near-zero executable success.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>The paper reiterates that foundation LLMs excel at QA (short, knowledge-centric tasks) but struggle at long, multi-step, domain-specific procedural tasks unless supported by architecture/workflow interventions (decomposition, retrieval with curated analysis, reviewers, and execution-feedback).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Intention To Implementation: Automating Biomedical Research via LLMs', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>React: Synergizing reasoning and acting in language models. <em>(Rating: 2)</em></li>
                <li>Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. <em>(Rating: 1)</em></li>
                <li>Autonomous chemical research with large language models. <em>(Rating: 2)</em></li>
                <li>Voyager: An open-ended embodied agent with large language models. <em>(Rating: 1)</em></li>
                <li>Crispr-gpt: An llm agent for automated design of gene-editing experiments. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-827",
    "paper_id": "paper-274656194",
    "extraction_schema_id": "extraction-schema-22",
    "extracted_data": [
        {
            "name_short": "BioResearcher",
            "name_full": "BioResearcher (modular LLM-based multi-agent research automation system)",
            "brief_description": "An end-to-end Level-2 automated system for dry-lab biomedical research that composes specialized LLM agents (search, literature processing, experimental design, programming) with hierarchical report/design workflows and LLM-based reviewers to produce executable protocols and code.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "BioResearcher (multi-agent system)",
            "model_description": "A modular multi-agent architecture built on a foundation LLM (GPT-4o) where each agent has a role (query generator, filter, report generator, analyst, experiment designer, code generator, reviewer); uses hierarchical report generation and hierarchical learning for protocol design, Retrieval-Augmented Generation (RAG) with curated analyses, and an execution-feedback loop (Docker) for code refinement.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "End-to-end biomedical research automation: literature search → report generation → experimental protocol design → dry-lab code generation and execution",
            "interactive_task_type": "planning; multi-step reasoning; tool use (search, retrieval); sequential decision-making (design → program → execute loop)",
            "interactive_performance": "Average execution success rate 63.07% across eight previously unmet research objectives (max 87.50%); protocol quality improvements vs best baseline: Completeness +0.254 (62.7% relative), Level of Detail +0.206 (30.0%), Correctness +0.411 (84.9%); generated protocols had 5.9× sentences/step and 4.8× steps/protocol vs baselines.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "Modular multi-agent architecture; hierarchical report generation; RAG using curated high-referability sections; LLM-based reviewer agents for iterative quality control; analyst agent; query generator and filter agents; execution-feedback loop with Docker sandboxing.",
            "training_method": "Primarily prompting / role-based in-context use of a foundation LLM (GPT-4o); no new fine-tuning or RLHF reported for BioResearcher itself.",
            "intervention_type": "architectural change; prompting & workflow engineering (hierarchical processing); hybrid approach (RAG + reviewer + execution loop)",
            "intervention_description": "Combines decomposition of complex tasks into role-specific LLM agents, hierarchical (section-wise) report generation to avoid long-context failure, hierarchical learning for protocol headings→outline→details using retrieved analyses as references, an LLM-based reviewer for iterative QC, and a code execution+feedback loop (Docker) to refine generated R code.",
            "intervention_effect": "Substantial: protocol metrics increased (Completeness +0.254, Level of Detail +0.206, Correctness +0.411) relative to best baseline; program execution success averaged 63.07% vs baselines ≈0%; removing reviewers reduced overall score by 0.098 and Correctness by 0.045; removing the report analyst reduced Completeness from 0.582 to 0.559.",
            "hypothesized_cause_of_gap": "Paper argues general LLMs do well on QA-style tasks (short answers, exam-style, summarization) but struggle on complex, long, domain-specific, multi-step scientific workflows due to: (1) long-context inputs causing truncated/short outputs, (2) need for deep domain knowledge and long logical chains, (3) lack of instruction data to fine-tune for such complex tasks, (4) instability/hallucination when chaining many steps or taking actions iteratively, and (5) iterative-agent summarization/integration degradation in long-context environments.",
            "uuid": "e827.0",
            "source_info": {
                "paper_title": "From Intention To Implementation: Automating Biomedical Research via LLMs",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation (RAG)",
            "brief_description": "A retrieval-augmented generation baseline that retrieves relevant content and generates an answer in a single step using retrieved context.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "RAG (baseline system)",
            "model_description": "Naive retrieval-augmented generation pipeline: retrieve documents (vector store / embeddings) and produce a single-step generated protocol or answer using retrieved context; used here as a baseline for protocol generation.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Protocol generation and code generation from retrieved literature (single-step RAG pipeline)",
            "interactive_task_type": "multi-step reasoning; document-grounded generation; tool use (retrieval)",
            "interactive_performance": "Protocol quality (Table 2): Completeness 0.405, Detail 0.687, Correctness 0.483, Logical Soundness 0.973, Structure 0.908, Overall 3.456; programming/code execution success for baselines (including RAG) reported as near 0% (almost invariably fail to produce executable code).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "Retrieval + generation; single-step integration of retrieved long context.",
            "training_method": "RAG pipeline using prompting and retrieval; models used without new fine-tuning in experiments.",
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "Authors attribute RAG's relative advantage over iterative agent methods in some long-context situations to collapsing summarization/integration errors in iterative systems; nonetheless, RAG still fails at producing executable code because single-step generation cannot handle multi-step procedural execution/implementation requirements.",
            "uuid": "e827.1",
            "source_info": {
                "paper_title": "From Intention To Implementation: Automating Biomedical Research via LLMs",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct (reasoning + acting within LLMs)",
            "brief_description": "An agent framework that interleaves chain-of-thought style reasoning with actions (tool use) to solve tasks via LLM-driven planning and acting.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "ReAct (baseline system)",
            "model_description": "Agent paradigm that combines reasoning traces (thoughts) with actions (tool calls) in-model to handle decision-making and tool usage during multi-step tasks.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Iterative reasoning + tool use for protocol generation and retrieval-assisted research tasks",
            "interactive_task_type": "planning; tool use; multi-step reasoning; sequential decision-making",
            "interactive_performance": "Protocol quality (Table 2): Completeness 0.364, Detail 0.577, Correctness 0.484, Logical Soundness 0.963, Structure 0.897, Overall 3.285; when equipped with retrieval tools, ReAct (and other iterative baselines) produced protocols with low executability and code execution success rates near zero.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "Interleaved chain-of-thought reasoning and tool actions within a single LLM agent.",
            "training_method": "Prompting and in-context chain-of-thought style instructions; no new fine-tuning reported in experiments.",
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "Paper suggests iterative agent systems (like ReAct) lose important information during repeated summarization/iteration in long-context tasks, leading to degraded final outputs and low procedural/executable performance despite plausible reasoning traces.",
            "uuid": "e827.2",
            "source_info": {
                "paper_title": "From Intention To Implementation: Automating Biomedical Research via LLMs",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Plan-and-Execute",
            "name_full": "Plan-and-Execute (iterative planning-execution agent baseline)",
            "brief_description": "An iterative framework that first plans a sequence of steps and then executes them, with potential re-planning on failures; used as a baseline for multi-step procedural tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "Plan-and-Execute (baseline system)",
            "model_description": "Two-phase iterative agent: generate a plan (outline), then execute steps (potentially with tool calls) in sequence, with an optional iterative loop for re-planning.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Iterative planning and execution for experimental protocol generation and implementation",
            "interactive_task_type": "planning; sequential decision-making; multi-step reasoning; tool use",
            "interactive_performance": "Protocol quality (Table 2): Completeness 0.380, Detail 0.587, Correctness 0.483, Logical Soundness 0.965, Structure 0.900, Overall 3.314; code execution success similarly near zero when tested end-to-end.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "Separate planning and execution phases; iterative planning loop.",
            "training_method": "Prompting/in-context with plan-and-execute prompts; no fine-tuning reported in experiments.",
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "Authors note that iterative frameworks can suffer from degradation when handling very long contexts and many iterative summarizations, losing critical details needed for executable procedural outputs.",
            "uuid": "e827.3",
            "source_info": {
                "paper_title": "From Intention To Implementation: Automating Biomedical Research via LLMs",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o (foundation LLM used as agent backbone and judge)",
            "brief_description": "A foundation LLM (used as the base model for all BioResearcher agents, and used as the main LLM judge/evaluator in experiments) employed in prompting/role-based fashion rather than further fine-tuning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "GPT-4o",
            "model_description": "Large foundation language model used as the single backbone for all agents in BioResearcher and as the primary LLM judge; operated with varied temperature settings across agents (query generator 0.7, reviewer/judge 0.1, others 0.5).",
            "model_size": null,
            "qa_task_name": "Medical QA / medical exam / clinical summarization (general categories mentioned in related work)",
            "qa_performance": null,
            "interactive_task_name": "Used as backbone for multi-agent interactive pipeline (search, literature processing, design, programming) and as judge for protocol evaluation",
            "interactive_task_type": "tool use (APIs for search), multi-step reasoning, planning, code generation",
            "interactive_performance": "As the core of BioResearcher, supported an overall system execution success rate of 63.07% (system-level metric); GPT-4o was also used as LLM judge achieving 80.4% agreement with human experts in protocol evaluation tasks.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "Foundation LLM used in multiple agent roles via prompting; no model-internal architectural modification reported.",
            "training_method": "Used off-the-shelf (no further fine-tuning reported); agents rely on prompting, role instructions, and hierarchical workflows.",
            "intervention_type": "prompting & system-level architectural design (multi-agent decomposition, hierarchical processing, reviewer loops)",
            "intervention_description": "Rather than training-time interventions, the paper applies system-level interventions (agent decomposition, hierarchical report/design pipelines, RAG with curated references, LLM reviewers, and execution-feedback) to improve procedural performance of the LLM in interactive tasks.",
            "intervention_effect": "When embedded in the BioResearcher multi-agent system and workflows, GPT-4o produced higher-quality experimental protocols and executable code (system average code execution success 63.07%) compared to using GPT-4o in naïve single-step or baseline agent setups which produced near-zero executable success.",
            "hypothesized_cause_of_gap": "The paper reiterates that foundation LLMs excel at QA (short, knowledge-centric tasks) but struggle at long, multi-step, domain-specific procedural tasks unless supported by architecture/workflow interventions (decomposition, retrieval with curated analysis, reviewers, and execution-feedback).",
            "uuid": "e827.4",
            "source_info": {
                "paper_title": "From Intention To Implementation: Automating Biomedical Research via LLMs",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "React: Synergizing reasoning and acting in language models.",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models.",
            "rating": 1,
            "sanitized_title": "planandsolve_prompting_improving_zeroshot_chainofthought_reasoning_by_large_language_models"
        },
        {
            "paper_title": "Autonomous chemical research with large language models.",
            "rating": 2,
            "sanitized_title": "autonomous_chemical_research_with_large_language_models"
        },
        {
            "paper_title": "Voyager: An open-ended embodied agent with large language models.",
            "rating": 1,
            "sanitized_title": "voyager_an_openended_embodied_agent_with_large_language_models"
        },
        {
            "paper_title": "Crispr-gpt: An llm agent for automated design of gene-editing experiments.",
            "rating": 2,
            "sanitized_title": "crisprgpt_an_llm_agent_for_automated_design_of_geneediting_experiments"
        }
    ],
    "cost": 0.014984749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SCIENCE CHINA Information Sciences . RESEARCH PAPER . From Intention to Implementation: Automating Biomedical Research via LLMs
5 Jun 2025</p>
<p>Yi Luo 
School of Informatics
National Institute for Data Science in Health and Medicine
Xiamen University
361101XiamenChina</p>
<p>Linghang Shi 
School of Informatics
National Institute for Data Science in Health and Medicine
Xiamen University
361101XiamenChina</p>
<p>Yihao Li 
Aobo Zhuang 
School of Informatics
National Institute for Data Science in Health and Medicine
Xiamen University
361101XiamenChina</p>
<p>School of Medicine
Xiamen University
361101XiamenChina</p>
<p>Yeyun Gong yegong@microsoft.com 
Ling Liu 
College of Computing
Georgia Institute of Technology
30332AtlantaUSA</p>
<p>Chen Lin chenlin@xmu.edu.cn 
School of Informatics
National Institute for Data Science in Health and Medicine
Xiamen University
361101XiamenChina</p>
<p>Zhongguancun Academy
100094BeijingChina</p>
<p>Microsoft Research Asia
100080BeijingChina</p>
<p>SCIENCE CHINA Information Sciences . RESEARCH PAPER . From Intention to Implementation: Automating Biomedical Research via LLMs
5 Jun 202560B683956F135E03649AE809F4FB080CarXiv:2412.09429v4[cs.MA]Biomedical ResearchAI for ResearchLarge Language ModelsMulti-Agent SystemsAutomation Citation From Intention to Implementation: Automating Biomedical Research via LLMs. Sci China Inf Scifor review
Conventional biomedical research is increasingly labor-intensive due to the exponential growth of scientific literature and datasets.Artificial intelligence (AI), particularly Large Language Models (LLMs), has the potential to revolutionize this process by automating various steps.Still, significant challenges remain, including the need for multidisciplinary expertise, logicality of experimental design, and performance measurements.This paper introduces BioResearcher, the first end-to-end automated system designed to streamline the entire biomedical research process involving dry lab experiments.BioResearcher employs a modular multi-agent architecture, integrating specialized agents for search, literature processing, experimental design, and programming.By decomposing complex tasks into logically related sub-tasks and utilizing a hierarchical learning approach, BioResearcher effectively addresses the challenges of multidisciplinary requirements and logical complexity.Furthermore, BioResearcher incorporates an LLM-based reviewer for in-process quality control and introduces novel evaluation metrics to assess the quality and automation of experimental protocols.BioResearcher successfully achieves an average execution success rate of 63.07%across eight previously unmet research objectives.The generated protocols, on average, outperform typical agent systems by 22.0% on five quality metrics.The system demonstrates significant potential to reduce researchers' workloads and accelerate biomedical discoveries, paving the way for future innovations in automated research systems.</p>
<p>Introduction</p>
<p>Biomedical research is a fundamental driving force behind human development.By uncovering the underlying mechanisms of diseases [1], biomedical research improves global health, extends life expectancy, and enhances life quality.It also fuels economic growth, scientific advancements, and societal well-being.</p>
<p>Traditional biomedical research relies heavily on labor-intensive processes like manual data collection, comprehensive literature reviews, complex experimental designs, and extensive data analysis.Although the conventional approach has facilitated notable breakthroughs in disease prevention [2][3][4], diagnosis [5][6][7], and treatment [8][9][10], it struggles to keep pace with the data explosion.For example, PubMed now hosts over 37 million citations 1) , overwhelming researchers and making it challenging to stay updated with the latest findings.Moreover, traditional research often demands repetitive tasks or interdisciplinary skills, such as coding, hindering research efficiency.</p>
<p>Artificial intelligence (AI) is emerging as a valuable tool in biomedical research, enhancing specific steps in the research pipeline.For instance, novel machine learning models are designed to analyze data and make decisions, including predicting drug-target from compound-protein interactions [11][12][13], detecting the presence of cancer from medical images [14,15], estimating patient outcomes from medical history [16][17][18], and so on.The advancement of Large Language Models (LLMs) further supports AI's role in academic writing [19,20] and literature summarizing [21].However, biomedical research is still</p>
<p>Data Preprocessing 2.1 Quality Control</p>
<p>• For bulk RNA-seq: Use tools like FastQC and Trim Galore to remove low-quality reads and adapters.• For scRNA-seq: Use CellRanger to preprocess raw data.</p>
<p>…</p>
<p>Missing description of specific available datasets.</p>
<p>Missing specific operational guidance and standards.</p>
<p>(a) A protocol generated directly by GPT-4o.</p>
<ol>
<li>Data Preprocessing and Normalization 1.1 Data Collection from RNA sequencing and single-cell sequencing datasets I. Collect RNA sequencing data from the following datasets: -GSE205492: RNA sequencing analysis of NK cells, T cells, and tumor from matched soft tissue sarcoma… -GSE21122: Whole-transcript expression data for soft-tissue sarcoma tumors and control normal fat specimens.… 1.2 Quality Control using FastQC for RNA-seq and Seurat for scRNA-seq I. Perform quality control on RNA sequencing data using FastQC:</li>
</ol>
<p>-Check for read quality, GC content, and presence of adapters or contaminants… II.Perform quality control on single-cell RNA sequencing data using Seurat:</p>
<p>-Exclude cells with mitochondrial gene expression exceeding 15%.</p>
<p>-Exclude cells with an abnormal number of expressed genes (≤200 or ≥9,000).… (b) A protocol generated by BioResearcher.</p>
<p>Specific datasets ID and descriptions.</p>
<p>Specific methodology and standards.Secondly, biomedical research is logically complex.On one hand, it requires a coherent understanding of literature with intricate logical structures, yet existing LLMs perform poorly on critical literature analysis.For example, irrelevant information in lengthy research papers causes catastrophic forgetting of important facts.On the other hand, biomedical research involves breaking down complex problems into logically related subtasks.For example, studying pyroptosis in dedifferentiated liposarcoma (DDL) involves multiple interconnected tasks like analyzing the expression variation and genetic changes of PRGs, performing immune infiltration analyses, identifying PRG-related clusters, characterizing the tumor microenvironment within these clusters, and developing a prognostic gene model based on these clusters [23].Each task is logically interdependent, making planning and execution by LLMs challenging.</p>
<p>Lastly, it is crucial to measure the performance of the research assistant.From the quality control perspective, assessing the results of intermediate steps ensures that the research assistant provides reliable final outputs.From the evaluation perspective, detecting the errors of end-to-end responses helps to identify the weaknesses and strengths of different systems and sheds insight for future improvements.Due to the complexity of biomedical research tasks, manually conducting fine-grained evaluation is infeasible; Artificial intelligence (AI) has improved biomedical applications for processing different textual data [31][32][33].Conventionally, small-scale language models [34] were used, but scaling laws indicate that increasing model parameters brings enhanced performance, leading to superior reasoning capability and higher answer accuracy.Therefore, we have seen large amounts of applications based on large language models (LLMs) [35][36][37][38][39].</p>
<p>Biomedical LLMs.Large language models (LLMs) are generally pre-trained on vast open-domain corpora but lack domain-specific knowledge.To enhance domain-specific performance, several techniques are used: (1) Fine-tuning optimizes the total or a part of parameters to improve the model performance on a small, specific dataset [35,37,38,40].(2) Reinforcement learning with human feedback (RLHF) or AI feedback (RLAIF) updates the model parameters to align LLM's responses with responses from humans or a teacher-model via a reinforcement learning framework [36,39,41].(3) Prompt engineering [42] involves giving instructions or examples to LLM to enforce rules or enhance reasoning [43][44][45].These methods mainly focus on question-answering (QA) tasks, like answering medical consultations [36], taking medical examinations [35] and summarizing clinical reports [33].However, LLMs still struggle with complex tasks that require more profound understanding and reasoning [22].Compared with QA tasks, responses for scientific research tasks are much longer and more professional and involve intricate, long logical chains.Fine-tuning and RLHF are infeasible due to the lack of instruction data, and simple prompt engineering cannot empower LLM with deep logical reasoning for professional topics.</p>
<p>LLM-based Agents for Research.LLM-based agents offer significant advantages over the direct use of LLMs, such as actively acquiring information, interacting with environments, and stronger reasoning and planning [22].LLMs can function as a single agent by being assigned with specific roles through in-domain fine-tuning [29] or role-specific prompts [33,[46][47][48][49][50][51].In contrast, a Multi-Agent System (MAS) comprises multiple LLM-based agents, enabling task completion through various cooperative methods.One approach involves assigning distinct roles to agents, who then reach a consensus through negotiation, like discussing clinical diagnosis via several medical experts [52][53][54].However, the consensus can be unreliable due to potential instability and hallucinations from LLMs.Alternatively, MAS can distribute complex tasks into sub-tasks among agents, such as dividing a scientific discovery process into creating ideas, experimentation, and writing [28].MAS is promising for tasks with complex logical chains and highly detailed requirements.After decomposing tasks and assigning them to agents, we can synthesize the results effectively.Furthermore, employing a professional and rigorous workflow framework helps constrain the agents' generative processes, ensuring highly reliable outcomes.AI for Research.AI for Research (AI4R) [55] has gained significant attention, particularly in automating scientific workflows.Existing studies are categorized into four automation levels [22]: Level 0 automation performs specific predefined tasks [56]; Level 1 automation designs simple experimental protocols with in-silico or lab tools [46,57]; Level 2 automation develops rigorous experimental protocols and employs statistical methods for hypothesis evaluation [28,58].Level 3 agents, which remain undeveloped, are envisioned to discover new methods and employ diverse techniques to measure biological phenomena.</p>
<p>Our work belongs to Level 2 automation and differs from current AI4R systems.(1) The system is not limited to solving a specific biomedical task.For example, CRISPR-GPT [59] customizes the workflow only for gene editing experiments.(2) We automate the entire process without manual maintenance of a template database like Genesis [60].(3) Most level 2 AI4R systems are designed for computer science (CS) [28][29][30]61,62], where public datasets like OpenReview are available, facilitating training and evaluation.Such extensive review data is lacking in the biomedical domain.(4) Existing studies emphasize novelty while we focus on reliability and feasibility, necessitating fine-grained literature analysis.For example, AIScientist [28] improves existing solutions based on a given task and initial experimental code.Conversely, BioResearcher designs a series of executable experiments for a new research subject.3 BioResearcher</p>
<p>Framework Overview</p>
<p>BioResearcher is designed to automate the research process for biomedical studies.Users provide a research objective 4) and the conditions under which the experiments will be conducted.Users can also specify the research requirements, such as desired experimental steps or outcomes.All user input is in natural language.</p>
<p>To emulate the workflow of human researchers, BioResearcher consists of four primary modules: Search, Literature Processing, Experimental Design, and Programming.As shown in Figure 2, the research process begins with the Search module, which comprehends the user input, generates appropriate queries, and searches for relevant research papers and datasets from online repositories.The retrieved literature is then filtered, downloaded, and forwarded to the Literature Processing module.Here, each research paper is standardized into an experimental report, and each report is analyzed in light of the user-specified objective, conditions, and requirements.This module also interacts with the Search module to identify the usability of the datasets mentioned in the reports.The processed reports, analyses, and datasets are then forwarded to the Experimental Design module, which constructs an experimental protocol.Finally, the Programming module extracts a sequence of dry lab experiment tasks from the protocol and generates accurate and executable codes for these tasks.</p>
<p>Search</p>
<p>Scientific research initiates with a thorough review of related literature.Literature searching is especially important in BioResearcher because, unlike previous studies that use literature surveys to identify research gaps and propose novel ideas, BioResearcher ensures the process is built upon existing knowledge and preserves the reliability of the output.The Search module comprehensively explores pertinent 4) Since our goal is to automate the entire research process, BioResearcher currently only supports studies involving only dry lab experiments.Studies that require wet experiments are out of the scope because wet experiments generally require human operation, e.g., hands-on tasks in a laboratory setting, and can not be fully automated.literature and datasets throughout various research phases.Its internal procedure is as follows:</p>
<p>(1) Query Generation: Searching directly with the user input can yield imprecise results for two reasons.Firstly, the user describes the research objective and conditions in natural language, while most databases use Boolean query logic, making it difficult to retrieve results that completely match the lengthy user input.Secondly, the user intention is provided with key high-level concepts, while the literature may use a more detailed description or synonyms, making it challenging to obtain all relevant materials.Thus, query rewriting is crucial [63].However, manually crafting effective queries costs a lot of expertise, labor, and time [64].To address this, the Search module in BioResearcher employs an LLM-based query generator agent to create Boolean queries based on the user input.As displayed in Figure 3, the query generator extracts keywords to improve the retrieval accuracy.It also performs synonym expansion (e.g., expanding "Single-cell sequencing" with "scRNA-seq") to improve the retrieval recall.These structured queries allow the module to interpret the research objective effectively and focus on the most relevant materials.(2) Retrieval: The module interfaces with databases through their APIs, enabling the retrieval of relevant literature and datasets from established repositories such as PubMed Central (PMC), PubMed, and GEO.Additional databases can be integrated as needed to expand the system's capabilities.</p>
<p>(3) Filtration: To retain only the most relevant and useful materials for further stages, our system employs an LLM-based filter agent to filter the returned research articles and datasets.For the research articles, we define a set of criteria detailed in Appendix D (Table D1).The filter examines the titles and abstracts of each article to determine their potential contribution to the research objectives.Each article receives a helpfulness score ranging from one to five.Articles scoring above four are downloaded and forwarded to the Literature Processing module.For the datasets, the filter assesses its usefulness based on the metadata (i.e., the attached online descriptions) of datasets and assigns a binary usability score.The useful datasets' descriptions are forwarded to the Experimental Design module.</p>
<p>The Search module thus ensures that subsequent stages of BioResearcher are provided with highquality, targeted resources, establishing a robust foundation for further processes.</p>
<p>Literature Processing</p>
<p>Literature comprehension can be challenging for researchers and LLMs because the research papers are massive, lengthy, and unstructured with complex logic.To streamline literature comprehension, enhance comprehension, and provide valuable references for experimental design, we introduce the Literature Processing module.This module first standardizes research papers into highly structured experimental reports and analyzes them systematically.It then interacts with the Search module to identify the usability of the datasets mentioned in the reports.Thus, this module operates through two primary phases: report generation and report analysis.Figure 4 illustrates an example of the module in operation.</p>
<p>Report Generation Each biomedical research paper contains experiment-related contents scattered across various sections.We aim to extract and reorganize these contents into a condensed, highly structured experimental report.Doing so brings three advantages.Firstly, the experimental report is shorter than the original paper, enhancing the efficiency of LLMs.Secondly, uniform formatting across reports provides logical structure coherence and format consistency, ensuring the LLMs grasp a big-picture idea of the most commonly acknowledged methods.Finally, the report is modularized with sections focusing on different aspects of the experiments, making it inherently suitable as a unit of analysis and a segment within retrieval-augmented generation (RAG) techniques.</p>
<p>Consequently, we first introduce the hierarchical report generation process in this module, which consists of the following steps, carried out by an LLM-based report generator agent: (1) First-Level Heading To mitigate potential performance degradation caused by excessively long input contexts [65], the report is divided into sections corresponding to the first-level headings, allowing parallel processing and enhancing efficiency.</p>
<p>Report Analysis Drawing inspiration from the Chain of Thought (COT) framework [66], which emphasizes step-by-step reasoning, we recognize that analyzing reports in relation to research objectives is a critical step in designing new experiments.Consequently, we introduce a report analysis process following report generation.Specifically, an LLM-based analyst agent evaluates each section of the experimental report for its referability, considering the research objective, conditions, and requirements.The analyst also provides suggestions for references and modifications, akin to proposing innovations on existing methods.Additionally, this module interfaces with the Search module to identify usable datasets within the report, using regular expressions to extract dataset IDs for retrieval from the database while bypassing the query generation step.As mentioned above, the usefulness of each dataset is determined based on its description, and useful datasets are then integrated with previously collected ones.</p>
<p>Sci China Inf Sci 8</p>
<p>To ensure the quality and accuracy of the generated outputs, an LLM-based reviewer agent is introduced to interact with the experimental report generator and analyst agents at each step.Outputs are refined based on the reviewer's feedback until final approval.</p>
<p>To provide fine-grained retrieval that eliminates irrelevant information, upon a comprehensive review of the relevant literature, we extract and integrate the sections deemed highly referential during the analysis phase, along with their corresponding analysis content from all reports.This consolidated information serves as a reference for the Experimental Design module.</p>
<p>Experimental Design</p>
<p>Scientific findings in biomedical research frequently face reproducibility issues, wasting resources and time while undermining the credibility of scientific outcomes [67].A well-designed experimental protocol is crucial for obtaining reliable results and optimizing resource use [68].Therefore, we develop the Experimental Design module, which uses an LLM-based experiment designer agent to create scientific and reproducible protocols.We propose a hierarchical learning approach to ensure the rationality of the logical structure of the generated experimental protocols.This method employs the Retrieval-Augmented Generation (RAG) technique aided by the analysis (i.e., using relevant reports' sections with high referability as reference materials), enabling the model to subsequently learn first-level headings independently, then outlines, and then experimental details from the reorganized reports.The design process is structured into three essential steps, as demonstrated in Figure 5.  (1) First-Level Heading Design: The designer begins by reviewing the first-level headings and corresponding analyses of relevant experimental reports, paying particular attention to the reference and modification suggestions.Integrating this information with the research objective, conditions, and requirements, the designer crafts first-level headings for the new protocol.Additionally, for each section, the designer provides a rationale detailing the section's purpose, design reason, and reference source.This step is crucial as it lays the foundation for the experimental framework, ensuring alignment with the research objective.</p>
<p>LLM-based Reviewer</p>
<p>(2) Outline Generation: The designer then constructs a brief protocol outline, referencing the outlines of pertinent reports and analyses.The designer also includes the sources of reference following the outlines.The generated outline serves as a framework for organizing the protocol.</p>
<p>(3) Generation of Implementation Details: Finally, the designer generates complete and specific implementation details for each part of the experimental protocol.Relevant sections of experimental reports and corresponding analyses are extracted based on the reference sources provided in the previous step.This information, along with the useful datasets, the protocol outline, and the summaries from earlier sections, is incorporated to produce a detailed protocol.The emphasis on detail and specificity ensures the reproducibility of the experiments, enabling researchers to follow the protocol precisely in subsequent studies.</p>
<p>Furthermore, an LLM-based reviewer agent, like in the Literature Processing module, is involved at each stage, providing continuous feedback to refine the design.This iterative process ensures the quality and accuracy of the final experimental design, thus contributing to the overall robustness of the research.</p>
<p>Programming</p>
<p>Programming in biomedical research presents a unique challenge to researchers, necessitating a combination of programming proficiency and domain-specific knowledge.To address this, we propose the Programming module, which is crucial in enhancing the reproducibility of experimental designs and automating systems.</p>
<p>To reduce the complexity of coding and debugging, the Programming module employs an LLM-based dry lab experiment extractor agent to derive a series of dry experiment tasks from the designed experimental protocol.Each task includes a task ID, a description of the task, and the types and descriptions of the input and output.</p>
<p>Subsequently, this module utilizes an LLM-based code generator agent to create R language code for each task.More programming languages will be supported in future work.To ensure code executability, a code execution within a Docker container is employed, which provides a controlled, isolated environment.Execution results-whether error reports or successful outputs-are fed back to the code generator.Based on these results, the code generator determines the next course of action: either further modification of the original code or termination of the generation process.Through this iterative cycle, the system refines the code until it achieves correctness and operational validity.</p>
<p>By systematically bridging the gap between experimental design and execution, this module significantly contributes to the efficiency and efficacy of the research workflow, making it a critical module of the system's overall functionality.</p>
<p>Evaluation Metrics</p>
<p>Quantitative evaluation metrics are demanded to fully reflect a research assistant's ability to advance the automation of biomedical research.However, no existing study has proposed such metrics.In this context, we propose a comprehensive method for assessing the quality of the resulting experimental protocols and programs.</p>
<p>Protocol Evaluation</p>
<p>We evaluate experimental protocols from five dimensions: completeness, level of detail, correctness, logical soundness, and structural soundness.The definitions and formulas for these five metrics are as follows.</p>
<p>• Completeness: Completeness assesses how thoroughly each section of the protocol is described, considering the necessary steps that should be added to achieve the design purpose of this part.The formula for completeness is given by:
Completeness = m i=1 n i es m i=1 n i ns = n ts n ts + n as ,(1)
where m represents the number of sections in a protocol.Here, n i es refers to the number of existing steps in the i-th section, while n i ns denotes the total necessary steps for that section.Additionally, n ts indicates the total number of steps in a protocol, and n as represents the number of steps that need to be added.</p>
<p>• Level of Detail: The level of detail measures the degree to which a protocol provides sufficient information for each step, ranging from 0 (no detail) to 1 (fully detailed).</p>
<p>• Correctness: Correctness assesses the proportion of protocol steps that are free from factual errors.Our analysis reveals that protocols with shorter steps tend to exhibit higher correctness scores, as the probability of factual errors increases with longer and more detailed steps.Drawing inspiration from the BLEU [25] metric in machine translation, we also introduce a brevity penalty (BP) for shorter steps.The BP is constrained to a minimum of 0.5, and the formula is as follows:
BP = 1, if l steps &gt; L,
max(e
(1− L l steps ) , 0.5), if l steps ⩽ L,(2)
where l step represents the length of steps, quantified by the number of sentences containing more than six words within each step (this criterion is employed to exclude steps' titles from the count).The parameter L denotes the average number of sentences within a step, calculated to be 4.42 from 315 protocols generated by different methods.</p>
<p>Then, the formula for Correctness is defined as:
Correctness = BP • n cs n ts ,(3)
where n cs denotes the number of the correct steps.</p>
<p>• Logical Soundness: Logical soundness evaluates the proportion of steps that are placed in a reasonable order within a protocol.Reasonable steps are those that are logically ordered and appropriately positioned.The formula for Logical Soundness is given by:
Logical Soundness = n rs n ts ,(4)
where n rs denotes the number of reasonable steps.</p>
<p>• Structural Soundness: Structural soundness evaluates the logical coherence and organizational integrity of a protocol's overall framework, with scores from 0 (completely unsound) to 1 (perfectly sound).</p>
<p>To speed up evaluation, we employ an LLM (GPT-4o) as the judge to evaluate the experimental protocols and obtain the above five metrics independently.For completeness, the LLM generates the additional steps required to achieve the design purpose of each section.For correctness and logical soundness, it evaluates the accuracy and coherence of each step.We then calculate these three metrics using their respective formulas.For the remaining two metrics-level of detail and structural soundness-the LLM directly generates the corresponding scores.Ultimately, the overall score for the experimental protocol is determined by summing the scores of all five dimensions.code for each dry lab experiment task.To assess its effectiveness, we propose two scoring systems.The first computes the execution success rate, reflecting the percentage of successfully completed tasks per protocol.The second metric assigns error levels to the tasks that remain incomplete, reflecting the severity of errors encountered during code execution.The detailed grading criteria are outlined in Table 1.</p>
<p>Program Evaluation</p>
<p>Experiment</p>
<p>In this section, we design comprehensive experiments to answer the following research questions:</p>
<ol>
<li>RQ1: How does BioResearcher perform in automating the entire biomedical research process? 2. RQ2: How does each component of BioResearcher perform in their specific sub-tasks, including the search module (RQ2.1),report generation module (RQ2.2), and experimental design module (RQ2.3)?</li>
</ol>
<p>Experiment Setup</p>
<p>In our experiments, we utilize the GPT-4o model as the foundational LLM for all the agents in BioResearcher.The temperature settings for various agents within the system are as follows: the query generator is configured with a temperature of 0.7 to introduce a moderate level of variability in the generated queries.Conversely, the reviewer and the LLM used for evaluation are set to a lower temperature of 0.1 to ensure more deterministic and consistent evaluations.All other agents operate at a temperature of 0.5, balancing between randomness and determinism to maintain overall coherence and reliability in the agents' outputs.Additionally, the query generator generates five queries for each user input, with a maximum retrieval quantity of 10 per query for each database.The maximum number of interaction rounds for the LLM reviewer in a single session is 6.</p>
<p>Baselines We evaluate BioResearcher by comparing it with three well-known agent systems.(1) ReAct [69], which integrates reasoning and action within LLMs to effectively manage complex reasoning and decision-making tasks.(2) Plan-and-Execute [27], which utilizes an iterative framework to accomplish tasks through sequential planning and execution.(3) RAG, which employs a naive Retrieval-Augmented Generation (RAG) module to search for relevant content and generates the answer in a single step.Appendix B provides a detailed description of the implementation of these baseline systems.</p>
<p>Performance of End-To-End Automation</p>
<p>We collect eight ongoing research objectives from a biomedical laboratory to ensure that no published work has addressed these research objectives, as detailed in Appendix C (Table C1).Each objective is processed for three runs to minimize randomness.We evaluate three baseline systems and our system for designing and executing experiments for these objectives.We equip the React and Plan-and-Execute systems with four tools: (1) a search tool utilizing the NCBI API 5) to retrieve descriptions of relevant papers, (2) a download tool for acquiring these papers and storing them in a chunked and vectorized format, (3) a search tool using the NCBI API to obtain descriptions of relevant datasets and storing them in a chunked and vectorized format, and (4) a search tool that extracts pertinent content from the resulting vector database.</p>
<p>Performance by Automatic Evaluation</p>
<p>Table 2 presents the average scores for protocols generated by different systems, including Completeness, Level of Detail, Correctness, Logical Soundness, and Structural Soundness.To minimize single-model evaluation bias, we employed four LLMs (GPT-4o, O3-mini-2025-01-31, Gemini-2.0-Flash,and DeepSeek-V3) for assessment, reporting their average scores across five distinct metrics.The detailed scoring of each model can be found in Appendix F (Table F1).We also calculate the overall performance, the average number of sentences per step (l steps ), and the average number of steps per protocol (n total steps ).(1)</p>
<p>The results highlight BioResearcher's exceptional performance in Completeness, Level of Detail, and Correctness, surpassing the best baseline by 0.254 (62.7%), 0.206 (30.0%), and 0.411 (84.9%), respectively.</p>
<p>(2) Our system is comparable to the best baseline in Logical Soundness and Structural Soundness, exceeding 0.89, consistently maintaining a high performance in these aspects.(3) Furthermore, the protocols generated by BioResearcher have significantly more sentences per step and steps per protocol, 5.9× and 4.8× greater than the average performance of different baselines, respectively, indicating the generation of more detailed and comprehensive protocols.( 4) The comparative analysis of the three baselines indicates that RAG outperforms the other two iterative agent systems.This superiority can be attributed to the fact that, in the long-context environment characterized by iteration and lengthy retrieved information, the summarization and integration capabilities of the latter two systems degrade, resulting in the loss of substantial amounts of valuable information in the final generated protocol.In contrast, we design tailored workflows to effectively integrate results from multiple modules and steps, thereby preserving critical information throughout the integration process.A case comparison of protocols from the four systems is illustrated in Appendix F (Figure F1).Notably, the first three metrics are crucial for the feasibility of protocols.Consequently, in subsequent code generation and execution, our experiments reveal that the baselines almost invariably fail to produce executable code, with success rates near zero.Our approach, however, achieves an average execution success rate of 63.07%across eight topics, with a maximum of 87.50%, as detailed in Figure 6.Furthermore, we conduct an error analysis of tasks that failed during code execution.We assess the corresponding code using the criteria in Table 1 and the error messages.Figure 7 illustrates the distribution of error levels.Across eight objectives, the majority of errors are minor errors (Level 1 errors), with an average portion of 67.19%, while severe errors requiring significant manual correction account for only 5.46%.This indicates that even when some tasks are not executed successfully, they can be corrected with minimal human intervention.</p>
<p>These findings suggest that the programming module has significant potential to enhance research efficiency.While some tasks still experience errors, the majority are successfully completed without the need for human intervention.This greatly reduces the time researchers spend on coding and debugging.By further optimizing the module to reduce error rates, research automation can be improved, thereby substantially increasing overall research productivity.</p>
<p>Quality of Automatic Evaluation</p>
<p>To validate the reliability of evaluations conducted by the LLM judge, we engage three domain experts to independently assess the LLM judge's evaluation outcomes.Given the time-consuming and labor-intensive nature of manual evaluation, we employed only GPT-4o, the best LLM currently, as the foundation LLM in BioResearcher to conduct multiple sampling generations on one research objective, resulting in the generation of 18 protocols used in this experiment.This focused approach also reduces evaluation complexity while enhancing assessment accuracy through deeper contextual understanding by experts.We engage three domain experts to systematically review the LLM judge's evaluation rationales and results for each protocol across five dimensions.This process involves verifying the factual accuracy and logical credibility of the LLM's outputs.Expert judgments are aggregated through majority voting to determine the validity of each step-level evaluation.The final analysis quantifies the LLM's assessment accuracy across five dimensions.</p>
<p>Results are shown in Table 3, demonstrating that the LLM achieves an overall accuracy of 80.4% in protocol evaluations, with exceptional performance in Completeness (91.1%).These metrics confirm the model's capacity to deliver comprehensive and efficient automation assessments.We also calculate the inter-expert consistency using Fleiss' kappa to ensure evaluation reliability.substantial agreement is observed across all dimensions: Detail (κ = 0.92), Structure (κ = 0.90) and Correctness (κ = 0.95) reaches near-perfect consensus, while other dimensions maintain κ &gt; 0.65.The p-values for consistency across the three dimensions are uniformly below 0.05.This agreement underscores the methodological rigor and reproducibility of the evaluation framework.</p>
<p>Performance by Manual Evaluation</p>
<p>To further validate the quality of the protocols generated by BioResearcher, we engage three domain experts to independently assess the 18 original protocols generated on one research objective, same as in Section 5.2.2, using consistent criteria in Section 4.1.Concentrating on one research objective mitigates assessment complexity and enhances accuracy through deeper contextual understanding by the experts.We employ experts to evaluate these protocols across three dimensions: Correctness, Detail, and Structure, following the criteria in Section 4.1.Specifically, the experts systematically evaluate each step of the protocols, determining whether it is correct or not to derive the n cs in equation 3 for calculating the correctness.For the remaining two metrics, the experts assign a direct score to each protocol.We exclude Completeness and Logical Soundness due to their susceptibility to subjective interpretation, as different researchers may design divergent yet valid experimental approaches.We then analyze interrater reliability using appropriate statistical measures.For Correctness, which quantifies the proportion of correct experimental steps in a protocol, we employ Fleiss' kappa.For Detail and Structure (ordinal scoring), we employ Kendall's W. As shown in Table 4, the results in three dimensions are all over 85%, and all consistency metrics exceed 0.6, with Correctness over 0.8.The p-values for consistency across the three dimensions are uniformly below 0.05.These results confirm strong consensus among evaluators across all dimensions.</p>
<p>Performance of Search Module</p>
<p>The Effect of Generated Queries</p>
<p>We conduct a comparative experiment to assess the effectiveness of LLM-generated queries.An LLM and three human participants independently generate five queries for each user input, which specifies a research objective, conditions, and requirements.The evaluation metric is the number of relevant papers or datasets retained after retrieval and filtering.The LLM generates queries three times, with the results averaged, and a similar average is calculated across the three human participants.This experiment, covering ten research objectives listed in Appendix C (Table C2), presents its results in Figure 8. LLM-generated queries generally outperform human-generated ones across most objectives.(1) In Figure 8(a), which compares the number of useful papers retrieved, the LLM-generated queries show significantly stronger performance in objectives 3 and 7, but the increases are modest on objectives 2, 5, 6, and 9. Human-generated queries perform slightly better in three objectives, suggesting that human intuition can offer an edge in certain cases.(2) Figure 8(b) presents a similar pattern in dataset retrieval.The LLM outperforms human participants in retrieving useful datasets for eight objectives, particularly Objective 3, where the LLM retrieved an average of six datasets compared to the human average of one-third.This indicates that only one human participant successfully generated queries that retrieved one useful dataset.Conversely, for objectives such as Objective 2 and Objective 10, human-generated queries show a slight advantage.</p>
<p>However, a key advantage of LLMs lies in their efficiency and scalability.Unlike human participants, who may take longer to generate queries and may experience fatigue, LLMs can quickly generate multiple queries with minimal effort.Furthermore, LLMs can mitigate performance gaps in challenging objectives by repeating query generation to increase the chances of retrieving relevant papers and datasets.This iterative capability allows LLMs to adapt and improve results, making them highly effective for large-scale or repetitive search tasks.</p>
<p>The Effect of LLM-based Filter Agent</p>
<p>To assess the precision of the ratings assigned by the LLM-based filter agent, we engage human reviewers to undertake a parallel assessment using the scoring criteria outlined in Table D1.Both the filter agent and human reviewers evaluate the same set of papers and datasets, with their ratings based solely on each paper's title and abstract or the dataset's description.We use Kendall's W to assess agreement for the ordinal ratings of 150 papers, as it is suitable for measuring concordance in ordinal data.For the binary ratings of 50 datasets, we apply Fleiss' kappa, which evaluates inter-rater agreement for categorical data among multiple raters.These papers and datasets are sampled from the search results of 10 topics listed in Table C2.Higher values in both metrics indicate stronger agreement, enhancing the reliability of the human ratings.A majority voting mechanism establishes the ground truth for human ratings, which serves as the benchmark for calculating the model's accuracy.For papers, those with scores of 4 or higher are classified as useful, while others are deemed not useful, and the accuracy is calculated based on this binary classification.The results of this study are represented in Table 5.</p>
<p>The results indicate strong consistency among human reviewers and notable accuracy of the LLM-based filter.For paper reviews, human consistency, measured by Kendall's W, shows an overall concordance of 0.82, reflecting substantial agreement.Based on the established human ground truth, the filter achieves an average accuracy of 84% in classifying papers as useful or not.The filter's accuracy peaks at 93% for  several objectives, underscoring its effectiveness in aligning with human assessments.In dataset reviews, human consistency, assessed using Fleiss' kappa, exhibits strong agreement with a value of 0.84.The filter maintains an average accuracy of 84%, with perfect accuracy in specific objectives, highlighting its reliability in dataset classification.The statistical significance of these results, confirmed by p-values less than 0.05, reinforces the robustness of the filter in achieving high concordance with human evaluations across both domains.</p>
<p>Performance of Report Generation</p>
<p>To test the impact of our hierarchical report generation method, we compare its reports against those produced by ReAct, Plan-and-Execute, and a naive single-step LLM approach.Specifically, we standardized 20 papers into an experimental report format using each method.None of the methods are equipped with any additional tools.We prompt an LLM to evaluate the reports across four dimensions: logical soundness, level of detail, consistency with the original paper, and readability.The model assigns a score ranging from 1 to 5 for each dimension, with the scoring criteria detailed in Appendix D (Table D2).The final score is calculated as the average across these four dimensions.Figure 9 presents a box plot of the scores for reports generated by the four different methods.</p>
<p>As illustrated in Figure 9, our method consistently outperforms the others, demonstrating significantly higher median values and narrower interquartile ranges, indicative of both superior performance and stability.Conversely, the three baseline models fail to generate high-quality reports.Among them, the Naive LLM shows the greatest score variability, ranging from 2.92 to 4.17, underscoring its instability.In contrast, the results for ReAct and Plan-and-Execute are concentrated around 3.5 points and do not exceed 4 points, reflecting their limited potential.We attribute the poor performance of the baselines to their failure to generate high-quality, long-length outputs when handling the extensive context of an entire paper.Our method, employing a hierarchical approach, enables the model to iteratively generate shorter outputs, which are then integrated into coherent, high-quality reports.These detailed, accurate, and well-structured reports provide excellent references for subsequent experimental design.</p>
<p>Sci China Inf Sci 16</p>
<p>Performance of Experimental Design</p>
<p>To validate the efficacy of the experimental design module, we construct a comparison experiment against the three baselines introduced in Section 5.1.We employ the search module and Literature Processing module of BioResearcher for 15 research objectives listed in Appendix C (Table C3), thereby obtaining reports and analyses that serve as a knowledge base.The three baselines can invoke search tools to retrieve relevant content from this knowledge base.All experiments are repeated three times, and the evaluation results are averaged and presented in Table 6.The results in Table 6 show that our methodology surpasses the three baseline methods in all metrics except Structural Soundness.Specifically, our approach exhibits superior performance in Completeness, Level of Detail, Correctness, and Logical Soundness, exceeding the best baseline scores by 0.131 (29.0%), 0.058 (6.9%), 0.402 (73.9%), and 0.007 (0.7%), respectively.This indicates a more comprehensive, detailed, accurate, and logically sound generation of experimental protocols.Although our method does not lead in the Structural Soundness metric, it still achieves a commendable score of 0.913, and its overall performance is the highest among all methods.These findings confirm that our hierarchical learning approach, which incrementally designs experimental protocols, effectively addresses the challenges posed by lengthy inputs and outputs, resulting in higher-quality experimental protocols.</p>
<p>Referring to Table 2, we observe an improvement in the average performance of all three baseline methods, particularly in Completeness (0.373 to 0.428), Level of Detail (0.666 to 0.783), and Correctness (0.481 to 0.522).Their average l steps and n total steps also increase, with the averages rising from 1.239 to 1.980 and 7.111 to 9.185, respectively.The complexity of the research objectives discussed in this section is consistent with those in Table 2, rendering this comparison meaningful.It also underscores the significant role of our report generation in reducing irrelevant information and providing a valuable reference for experimental design.Additionally, we assess the role of the LLM Reviewers in quality control for the automation of biomedical research.Removing the reviewers from both the literature processing and experimental design modules leads to a decline in performance across most metrics, as shown in Table 7. Specifically, the inclusion of the LLM Reviewers results in an overall score increase of 0.098, with the most notable improvement of 0.045 in Correctness.This indicates that the LLM reviewers effectively identify and correct errors in literature processing and experimental design, thereby enhancing the accuracy of the output.Moreover, our system with reviewers generates more detailed and comprehensive protocols, as evidenced by higher scores in Completeness, Level of Detail, and the average number of sentences per step.</p>
<p>To evaluate the impact of the report analyst agent within the Literature Processing module, we remove this agent from BioResearcher.Instead, we use the same retrieval model employed in the baselines to extract relevant content from reports as references for protocol generation.As shown in Table 7, the Completeness score significantly declines from 0.582 to 0.559.This drop is due to the analyst providing specific references and modification suggestions that enhance protocol completeness.However, in the other four dimensions, removing the report analyst does not negatively affect outcomes and even results in slight improvements.These minor improvements, averaging an increase of only 0.0025, can be considered normal fluctuations.Furthermore, from the perspectives of system interpretability and user-friendliness, the report analyst helps users understand how the system designs new experimental protocols based on relevant materials.</p>
<p>Conclusion</p>
<p>In this study, we introduced BioResearcher, an intelligent research assistant that automates the biomedical research process.Utilizing a modular LLM-based multi-agent architecture, BioResearcher addresses the multidisciplinary demands, logical complexities, and performance evaluation challenges of biomedical research.It automates tasks such as literature review, experimental protocol design, and code implementation, significantly improving research efficiency and reducing manual workload.We developed novel evaluation metrics focusing on protocol quality and experimental automation, providing a robust framework for assessing performance.Our results show that BioResearcher designs executable experimental protocols with a high success rate, outperforming existing typical agent systems.</p>
<p>The practical significance of BioResearcher lies in its ability to automate the research pipeline, allowing researchers to focus on strategic decision-making and innovation.This advancement accelerates biomedical discoveries and future developments in automated research systems.By potentially extending its capabilities to wet lab experiments, BioResearcher promises broader applications.This study lays the groundwork for enhancing automated research technologies, contributing to global health and scientific progress.</p>
<p>Sci China Inf Sci 24</p>
<p>Table D1 Scoring criteria for the helpfulness of papers.</p>
<p>Rating Description 1</p>
<p>Not Helpful at All:</p>
<p>-The title and abstract of the paper are completely unrelated to the research objective, conditions, and requirements.-Provides no valuable information or insights.</p>
<p>2 Slightly Helpful:</p>
<p>-The title and abstract of the paper have some minor relevance to the research objective, conditions, and requirements.-Provides very limited information and insights, making it difficult to contribute significantly to the research objective.</p>
<p>3</p>
<p>Moderately Helpful:</p>
<p>-The title and abstract of the paper are somewhat relevant to the research objective, conditions, and requirements.-Provides some useful information and insights, but additional information or research may be needed to fully utilize it.</p>
<p>4</p>
<p>Very Helpful:</p>
<p>-The title and abstract of the paper are highly relevant to the research objective, conditions, and requirements.-Provides a substantial amount of useful information and insights, making a significant contribution to the research objective.</p>
<p>5</p>
<p>Extremely Helpful:</p>
<p>-The title and abstract of the paper perfectly align with the research objective, conditions, and requirements.-Provides critical information and insights that directly and significantly advance the research objective.</p>
<p>Sci China Inf Sci 25</p>
<p>Table D2 Scoring criteria for experimental reports.</p>
<p>Dimension Criterion</p>
<p>Logical Soundness 5: Completely logical arguments and conclusions, clear reasoning, no gaps.</p>
<p>4: Mostly logical, minor unclear points or leaps.</p>
<p>3: Generally logical, some noticeable gaps or inconsistencies.</p>
<p>2: Many logical flaws, disorganized logic.</p>
<p>1: Lacks coherence, severe flaws undermining arguments.</p>
<p>Detail Level 5: All necessary details, comprehensive descriptions.</p>
<p>4: Most necessary details, some areas brief.</p>
<p>3: Some necessary details, lacks important information.</p>
<p>2: Lacks many details, unclear descriptions.</p>
<p>1: Almost no details, difficult to understand.</p>
<p>Consistency with Original Paper</p>
<p>Acquisition• Download publicly available datasets relevant to liposarcoma or similar sarcomas.</p>
<p>Figure 1
1
Figure 1 Example of experimental protocol generation.For the same user input, (a) illustrates an experimental protocol generated by a single LLM (GPT-4o), while (b) demonstrates a protocol generated by BioResearcher.</p>
<p>Figure 2
2
Figure2The flowchart of BioResearcher.On the left, the system is divided into four main modules: Search, Literature Processing, Experimental Design, and Programming.The right side illustrates the system's workflow: it initiates with the Search module that retrieves pertinent literature and datasets based on the user input.The Literature Processing module then converts the obtained literature into standardized experimental reports, analyzing them against the research objective, conditions, and requirements.It also works with the Search module to evaluate the applicability of the datasets mentioned in the reports.These processed reports, analyses, and suitable datasets are then sent to the Experimental Design module, which designs an experimental protocol aligned with the research objective.Finally, the Programming module extracts the dry lab experimental tasks from the protocol and generates executable code to perform these tasks.Components underlined are agents based on LLMs.</p>
<p>'Figure 3
3
Figure 3 An example of query generation.</p>
<p>Part 2 :Figure 4
24
Figure 4 An example of Literature Processing.(a) illustrates the workflow for Report Generation, while (b) presents an example of Report Analysis.</p>
<p>To gather and preprocess RNA-seq and single-cell RNA-seq … Design Reason: Essential for ensuring high-quality data for downstream … Reference Source: 10318157, Part 1, … Collection A. Gather RNA-seq data from ... B. Gather single-cell RNA-seq data … II.Quality Control … Reference Source: 10318157, Part 1, … Part 2: Differential Gene Expression Analysis I. Identification of DEGs A. Use DESeq2 or edgeR for RNA-… … Reference Source: … Part 3: Consensus Clustering and Subtype Identification I. Consensus Clustering … Reference Source: ... … (More Parts) (1) First-Level Headings Design Relevant experimental reports' first-level headings and analysis.Collection A. Gather RNA-seq data from 80 cases 1. Collect RNA sequencing data from … 2. Store the raw sequencing data in … B. Gather Single-cell RNA-seq data from … 1. Collect single-cell RNA sequencing … Relevant experimental reports and analysis.Datasets description.Designed Outline.Part 2: Differential Gene Expression Analysis I. Identification of DEGs A. Use DESeq2 for RNA-seq data from … 1Please update [content to be modified] based on the modification suggestions you generated … Below threshold Above threshold Judge: Evaluate [content to be evaluated] as a whole on a scale from 1 to 5 … Save</p>
<p>Figure 5
5
Figure 5 Demonstration of experimental design.</p>
<p>Obj. 1 Figure 6 Figure 7
167
Figure 6 Execution success rate.</p>
<p>Figure 8
8
Figure 8 Comparison of the effects of LLM-generated queries and human-generated queries.(a) Comparison of the number of useful papers retrieved by LLM-and human-generated queries.(b) Comparison of the number of useful datasets retrieved by LLMand human-generated queries.</p>
<p>Figure 9
9
Figure 9 Boxplot of scores for experimental reports generated by different methods.</p>
<p>5 : 2 : 5 : 4 : 3 : 2 :
525432
Entirely faithful, no misunderstandings.4: Mostly faithful, minor misunderstandings.3:Generally faithful, noticeable misunderstandings.Many inconsistencies, significant deviations.1: Severely inconsistent, obvious misunderstandings.Clear structure, logical organization, easy to read.Mostly well-structured, minor section issues.Generally clear, some poorly arranged sections.Quite disorganized, poor reading experience.1: Very disorganized, extremely difficult to read.</p>
<p>Table 1
1
Grading criteria and error types for R code execution.
Level DescriptionExample Error Types1Minor ErrorsMissing or incorrect file paths, missing necessary libraries or packages,network timeouts2Moderate Errors Syntax errors, incorrect function or variable names, data type mis-matches3Major ErrorsParameter mismatches, index out of bounds or invalid index, out ofmemory4Severe ErrorsIncorrect algorithms or logic, disorganized code structure, key compo-nents missing or incorrect
As a critical component of research automation, the extent to which the Programming module can augment research efficiency deserves attention.As detailed in Section 3.5, this module generates tailored Sci China Inf Sci 11</p>
<p>Table 2
2
Quality of experimental protocols generated by various systems.
MethodCompletenessDetailCorrectness Logical Soundness StructureOveralll steps n total stepsRAG0.4050.6870.4830.9730.9083.4561.2869.167ReAct0.3640.5770.4840.9630.8973.2851.2375.792Plan and Execute0.3800.5870.4830.9650.9003.3141.1946.375BioResearcher0.659 ↑0.2540.893 ↑0.206 0.895 ↑0.4110.953 ↓0.0200.891 ↓0.017 4.292 ↑0.836 7.32733.958
5) https://www.ncbi.nlm.nih.gov/home/develop/api/Sci China Inf Sci 12</p>
<p>Table 3
3
Performance of the LLM judge, as evaluated by human experts.Human consistency is measured using Fleiss' kappa.
Completeness Detail Structure Logical Soundness Correctness Avg.LLM's accuracy (%)91.188.975.975.071.380.4Human consistency0.660.920.900.880.950.86</p>
<p>Table 4
4
Quality of experimental protocols evaluated by human experts.Consistency for the Correctness metric is measured using Fleiss' kappa, while consistency for the Detail and Structure metrics is assessed using Kendall's W.
Correctness Detail StructureHuman Experts0.850.870.87Human Consistency0.850.680.61</p>
<p>Table 5
5
Evaluation results of the LLM-based filter agent.Human consistency in paper reviews is measured by Kendall's W, and in dataset reviews, Fleiss' kappa is employed.All p-values are less than 0.05, indicating statistical significance.LLM accuracy is calculated using the ground truth from the majority of human ratings.
Objective ID12345678910TotalPapers ReviewHuman's consistency 0.67 0.80 0.84 0.91 0.86 0.80 0.75 0.75 0.80 0.68 0.82LLM's Accuracy80% 73% 80% 93% 93% 93% 80% 80% 87% 80% 84%Datasets ReviewHuman's consistency 0.66 0.70.66 1.00 0.70 0.70 1.00 1.00 1.00 0.73 0.84LLM's Accuracy80% 80% 80% 80% 60% 80% 100% 80% 100% 100% 84%Naive LLMReAct Plan and Execute Ours Methods</p>
<p>Table 6
6
Scores of experimental protocols generated by various methods.
MethodCompleteness Detail Correctness Logical Soundness Structure Overall l steps n total stepsRAG0.4510.8430.5060.9700.9413.7112.00510.178ReAct0.4450.7780.5160.9720.9263.6361.9499.244Plan and Execute0.3880.7270.5440.9650.9203.5441.9878.133Our Method0.5820.9010.9460.9790.9134.3216.47715.289</p>
<p>Table 7
7
Scores of experimental protocols generated by BioResearcher without experimental report analyst or reviewer.
MethodCompleteness Detail Correctness Logical Soundness Structure Overall l steps n total stepsOur Method0.5820.9010.9460.9790.9134.3216.47715.289w/o Reviewers0.5500.8840.9010.9700.9184.2234.58315.333w/o Analyst0.5590.9010.9470.9820.9194.3086.58614.978</p>
<p>Table E1
E1
Cost of BioResearcherTable E2 Time comparison between human and BioResearcher Sci China Inf Sci 27 Table F1 Quality of experimental protocols generated by various systems evaluated by different models.
EvaluatorMethodCompletenessDetailCorrectness Logical Soundness StructureOveralllsteps ntotal stepsRAG0.3920.7650.4700.9910.9523.5701.2869.167GPT-4oReAct Plan and Execute0.361 0.3660.617 0.6170.487 0.4870.972 1.0000.950 0.9353.387 3.4051.237 1.1945.792 6.375BioResearcher0.612 ↑0.2200.902 ↑0.137 0.944 ↑0.4570.987 ↓0.0130.910 ↓0.042 4.355 ↑0.785 7.32733.958RAG0.3760.7310.4880.9600.9673.5221.2869.167o3-mini-3-31ReAct Plan and Execute0.329 0.3750.608 0.6210.479 0.4790.958 0.9430.956 0.9603.330 3.3781.237 1.1945.792 6.375BioResearcher0.509↑0.1330.981↑0.250 0.823↑0.4350.944↓0.0170.952↓0.015 4.309↑0.931 7.32733.958RAG0.3970.5670.4880.9820.8203.2541.2869.167gemini-2.0-flashReAct Plan and Execute0.329 0.3410.455 0.4750.490 0.4880.966 0.9710.795 0.8273.036 3.1021.237 1.1945.792 6.375BioResearcher0.758↑0.3610.754↑0.188 0.872↑0.3820.943↓0.0390.792↓0.035 4.118↑0.864 7.32733.958RAG0.4540.6850.4860.9600.8923.4761.2869.167deepseek-V3ReAct Plan and Execute0.435 0.4370.629 0.6330.478 0.4760.957 0.9450.885 0.8793.385 3.3701.237 1.1945.792 6.375BioResearcher0.758 ↑0.3050.933 ↑0.248 0.843 ↑0.3570.940 ↓0.0170.910 ↓0.019 4.380 ↑0.909 7.32733.958AveragePhase RAG Search ReAct Plan and Execute Input Toktens Output Toktens GPT-4o-mini GPT-4o-2024-08-06 GPT-4o cost 0.405 0.687 0.483 0.973 0.908 3.456 1.286 9.167 0.09M 0.02M 0.03$ 0.45$ 0.364 0.577 0.484 0.963 0.897 3.285 1.237 5.792 0.380 0.587 0.483 0.965 0.900 3.314 1.194 6.375 0.91$ BioResearcher 0.659↑0.254 0.893↑0.206 0.895↑0.411 0.953↓0.020 0.891↓0.017 4.292↑0.836 7.327 33.958Literature Processing49.62M1.74M8.49$141.48$282.96$Design0.77M0.06M0.15$2.48$4.96$Program61.08M2.57M10.70$178.39$344.07$Total111.55M4.39M19.37$322.81$632.90$PhaseHumanBioResearcherSearch1-3 Weeks0.37 hoursLiterature Processing1-2 Weeks5.03 hoursDesign1 Weeks0.16 hoursProgram4-8 Week2.6 hoursTotal7-14 Weeks8.16 hours
Sci China Inf Sci 26FigureF1A comparison of BioResearcher, Plan-and-Execute, React, and RAG in generating protocols.</p>
<p>Acknowledgements Yeyun Gong and Chen Lin are the corresponding authors.Chen Lin is supported by the Natural Science Foundation of China (No. 62432011) and Zhongguancun Academy (No. 20240310).Supporting information Appendix A-F.The supporting information is available online at info.scichina.comand link.springer.com.The supporting materials are published as submitted, without typesetting or editing.The responsibility for scientific accuracy and content remains entirely with the authors.About CitationIf you find our work useful in your research, please cite us using the following BibTeX: @article{BioResearcher, author = "Yi Luo and Linghang Shi and Yihao Li and Aobo Zhuang and Yeyun Gong and Ling Liu and Chen Lin", title = "From Intention To Implementation: Automating Biomedical Research via LLMs", journal = "SCIENCE CHINA Information Sciences", year = "2025" }Appendix A Limitations and Future Works Limitations In this paper, BioResearcher effectively automates the entire biomedical research process, from literature and dataset searches to experimental design and execution, given a research objective and specified conditions.However, several limitations warrant discussion.First, the system does not achieve complete success in executing experiments without manual intervention.This limitation is partly due to the need for further enhancement of the code generator agent's performance.Second, certain anomalies, such as the unavailability of resources specified in the experimental protocols, also hinder fully automated execution.It highlights the necessity of anticipating a broader range of exceptional scenarios and developing corresponding solutions in future work.Third, during our practice, we observed that excessively long inputs often result in LLMs producing shorter, more generalized outputs.To enable LLMs to generate detailed and comprehensive experimental protocols, we employed the multi-step, section-wise processing approach at various stages, including experimental report generation and experimental design.However, this iterative method increases the overall cost.Fourth, the framework currently does not formally model the research process as a constrained optimization problem.This limits its ability to optimally balance competing objectives (e.g., accuracy vs. computational cost) or dynamically allocate resources (e.g., lab equipment, cloud computing budgets), particularly in complex, multi-agent research scenarios.Addressing LLM hallucination continues to be an essential area for future work.Third, incorporating human oversight mechanisms is crucial for ensuring reliability and ethical compliance in complex scientific workflows.We propose three avenues for integrating human intervention: (1) Confidence-Driven Human Intervention, where a confidence threshold mechanism prompts human validation when the reviewer agent's confidence falls below a predefined threshold; (2) Iterative Refinement via Natural Language Feedback, allowing users to provide corrections (e.g., missing controls or outdated references), which the AI agents will process and update accordingly while maintaining audit trails; and (3) Ethical Decision-Making Protocol, introducing an automated ethics checkpoint that flags high-risk proposals, mandates human approval for sensitive experiments, and integrates institutional ethics guidelines into the system's constraints.Future work will quantitatively evaluate the trade-offs between automation and human intervention to optimize system performance and usability.Fourth, incorporating constrained optimization techniques would enable resource-aware scheduling algorithms that balance computational budgets, equipment availability, and temporal constraints through multi-objective optimization -particularly valuable for large-scale collaborative studies.Future WorksAppendix B Details of Baselines ImplementationsIn Section 5.2, due to the NCBI databases' limitations in semantic similarity searches, directly using the user's input objective poses challenges in retrieving relevant papers and datasets.Thus, we provide the RAG system with unfiltered papers and datasets from our Search module, enabling it to extract relevant datasets and content to generate a comprehensive protocol and corresponding code in a single step.In Section 5.4, we directly supply the literature to be processed to the three systems, thereby not equipping them with any tools.In Section 5.5, we equip the three systems with the aforementioned fourth tool.Here, we chunk each report and analysis by parts, rather than using the semantic similarity approach employed for chunking the papers in Section 5.2, and then store them in a vectorized format for the search tool.We implement all baselines using the AutoGen framework[71]and adopt OpenAI's text-embedding-ada-002 as the embedding model in RAG applications.Appendix C Details of Research ObjectivesIn this section, we outline the specific research objectives utilized in our experiments.TableC1presents eight objectives employed for end-to-end automation experiments.These objectives are previously unmet and are gathered from a biomedical research laboratory.TableC2displays ten objectives sourced from publicly available papers for evaluating the search module.TableC3presents fifteen objectives for assessing experimental design; these are also collected from published papers, but their publication dates are later than the cutoff for GPT-4o training data.Appendix D Scoring CriteriaIn this section, we outline the specific scoring criteria used in our experiments.Appendix E Cost AnalysisBelow we illustrate the average number of tokens and the associated costs involved in generating an experimental protocol and its corresponding R code based on specified research targets, conditions, and requirements, using various large language models (LLMs) in TableE1.The process also encompasses the execution and verification of the code.In addition, after investigation, for a biomedical research cycle from the research target to the final dry experimental results, we compared the approximate manual time obtained from our survey with the time consumed by BioResearcher in TableE2.Appendix F Comparison of protocols generated by 4 systems.FigureF1shows a comparison of a part of the experimental protocols generated by four different systems: BioResearcher, Plan-and-execute, React, and RAG.Notably, the protocol produced by BioResearcher is both detailed and accurate.In contrast, the protocols generated by the other systems lack essential details, rendering them impractical for implementation.TableF1presents the evaluation results of the four assessment models on the protocols described in Section 5.2.As shown in the table, our method consistently outperforms the baseline systems across all four evaluation models.Sci China Inf Sci 22TableC210 research objectives used in the evaluation of the Search module.ID Research Objective 1Identify populations that are sensitive to immunotherapy for liposarcoma and explore the mechanisms of therapeutic sensitivity to guide the treatment of those whose immune quality is not sensitive.2To investigate the expression characteristics of the TBC1D4 activating protein molecule and identify key module genes for preventing the progression of thyroid cancer.Through bioinformatics analysis, elucidate the role of TBC1D4 in thyroid cancer and its regulated gene network to provide new molecular targets and research directions for early prevention and treatment of thyroid cancer.3To explore malignant cell characteristics related to microvascular invasion (MVI) in hepatocellular carcinoma (HCC) through multi-omics transcriptomic analysis and develop a machine learning prognostic model based on MVI-related genes.4To develop a machine learning classifier based on host immune response mRNA to accurately distinguish between acute viral respiratory infections (viral ARI) and non-viral respiratory infections (non-viral ARI).Identify and validate gene expression characteristics that can aid in clinical diagnosis using host gene expression data from nasal swab samples through a multi-omics analysis framework.5To identify preventive biomarkers related to DNA replication in ovarian cancer through gene expression analysis and bioinformatics analysis, particularly focusing on the expression of MCM2 protein and its role in ovarian cancer progression.6To analyze coagulation-related genes in hepatocellular carcinoma (HCC) and explore their relationship with the tumor immune microenvironment (TME) and clinical prognosis.7To systematically analyze long non-coding RNAs (lncRNAs) related to immune checkpoints (ICP) and explore their functions in cancer, as well as their potential as biomarkers for predicting immune therapy responses and prognosis.8To reveal the transcriptional patterns of HER2 through a pan-cancer analysis of HER2 indices, facilitating a more precise selection of patients suitable for HER2-targeted therapy.9To construct an immune-related gene prognostic index (IRGPI) to predict the prognosis of head and neck squamous cell carcinoma (HNSCC) patients and clarify the molecular and immune characteristics of different HNSCC subgroups defined by IRGPI, as well as the efficacy of immune checkpoint inhibitor (ICI) therapy.10 To reveal the heterogeneity of T cell exhaustion (TEX) in the tumor microenvironment (TME) of different cancer types, depict the hierarchical functional dysfunction process of T cell exhaustion through pan-cancer analysis and investigate its association with prognosis and immune therapy efficacy.Sci China Inf Sci 23 6Pseudo-time analysis to explore the differentiation trajectory of malignant cells in hepatocellular carcinoma.7Exploring the communication pathways of cells related to microvascular invasion in hepatocellular carcinoma based on intercellular communication analysis.8Using GO and KEGG pathway analysis to reveal the functional enrichment of MCM2-related differentially expressed genes in ovarian cancer.9 Analysis of the relationship between the expression level of MCM2 gene in ovarian cancer and clinical prognosis.10 Analysis of the functional status of CD4+ T cell subsets in rheumatoid arthritis.11 Analysis of key immune cell type composition characteristics in pancreatic ductal adenocarcinoma immune cell death-related gene subtypes.12 Analysis of immune checkpoint expression patterns in high and low subtypes of immune cell deathrelated genes in pancreatic ductal adenocarcinoma.13 Analysis of tumor microenvironment characteristics of pancreatic ductal adenocarcinoma based on high and low expression subtypes of immune cell death-related genes.14 Screening of potential anti-tumor drugs based on the expression pattern of ribosome production genes.
Our effort primarily focuses on dry lab experiments, which rely on computational methods to conduct bioinformatics analysis. Future research endeavors may extend to wet lab experiments. References 1 National Research Council (US) Committee for Monitoring the Nation's Changing Needs for Biomedical, Behavioral, and Clinical Personnel. 2005National Academies PressWashington (DC; US)Advancing the Nation's Health Needs: NIH Research Training Programs</p>
<p>Iron oxide nanozymes stabilize stannous fluoride for targeted biofilm killing and synergistic oral disease prevention. Y Huang, Y Liu, N K Pandey, Nature Communications. 1460872023</p>
<p>Optimal dietary patterns for prevention of chronic disease. P Wang, M Song, A H Eliassen, Nature Medicine. 292023</p>
<p>Fixed-dose combination therapy for the prevention of atherosclerotic cardiovascular disease. A Agarwal, P M Mehta, T Jacobson, Nature Medicine. 302024</p>
<p>Diagnosis of alzheimer's disease using plasma biomarkers adjusted to clinical probability. J Therriault, S Janelidze, A L Benedet, Nature Aging. 42024</p>
<p>Noninvasive, microbiome-based diagnosis of inflammatory bowel disease. J Zheng, Q Sun, M Zhang, Nature Medicine. 302024</p>
<p>Development of a novel colorimetric assay for the rapid diagnosis of coronavirus disease 2019 from nasopharyngeal samples. N Sepahi, S Samsami, Y Mansoori, Scientific Reports. 14121252024</p>
<p>Risk-stratified treatment for drug-susceptible pulmonary tuberculosis. V K Chang, M Z Imperial, P P Phillips, Nature Communications. 1594002024</p>
<p>Semaglutide in patients with overweight or obesity and chronic kidney disease without diabetes: a randomized double-blind placebo-controlled clinical trial. E M Apperloo, J L Gorriz, M J Soler, Nature Medicine. 312025</p>
<p>Ready-to-use ipsc-derived microglia progenitors for the treatment of cns disease in mouse models of neuropathic mucopolysaccharidoses. P Douvaras, D F Buenaventura, B Sun, Nature Communications. 1581322024</p>
<p>Artificial intelligence to deep learning: machine intelligence approach for drug discovery. R Gupta, D Srivastava, M Sahu, Molecular Diversity. 252021</p>
<p>Prediction of drug efficacy from transcriptional profiles with deep learning. J Zhu, J X Wang, X Wang, Nature Biotechnology. 392021</p>
<p>Artificial Intelligence in Drug Discovery and Development. K Mak, Y H Wong, M R Pichika, Drug Discovery and Evaluation: Safety and Pharmacokinetic Assays. F J Hock, M Pugsley, ChamSpringer2024</p>
<p>. B Bizzo, C Almeida R R, T K Alkasab, Artificial intelligence enabling radiology reporting. Radiologic Clinics. 592021</p>
<p>What the radiologist should know about artificial intelligence-an ESR white paper. Insights into Imaging. 10442019European Society of Radiology</p>
<p>Natural language processing applications for computer-aided diagnosis in oncology. Diagnostics, 2023, 13: 286. and shapley additive explanations (shap) approach. C T Li, Y M Zhang, Y Weng, Asian Journal of Psychiatry. 791033162023</p>
<p>An efficient iot-artificial intelligence-based disease prediction using lightweight cnn in healthcare system. A Malibari, Measurement Sensors. 261006952023</p>
<p>Exploring the opportunities and challenges of chatgpt in academic writing: a roundtable discussion. H S H Bom, Nuclear Medicine and Molecular Imaging. 572023</p>
<p>Writing with chatgpt: An illustration of its capacity, limitations &amp; implications for academic writers. L Lingard, Perspectives on Medical Education. 122612023</p>
<p>System for systematic literature review using multiple ai agents: Concept and an empirical evaluation. M Sami, Z Rasheed, K K Kemell, arXiv:2403.083992024arXiv Preprint</p>
<p>Empowering biomedical discovery with ai agents. S H Gao, A Fang, Y P Huang, Cell. 1872024</p>
<p>The pyroptosis-related signature predicts prognosis and influences the tumor immune microenvironment in dedifferentiated liposarcoma. Chen W , J Cheng, J Cai, Y Q , Open Medicine. 19202308862024</p>
<p>Rouge: A package for automatic evaluation of summaries. C Lin, Text Summarization Branches Out. 2004</p>
<p>Bleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational Linguistics2002</p>
<p>Retrieval-augmented generation for large language models: A survey. Y F Gao, Y Xiong, X Y Gao, arXiv:2312.109972023arXiv Preprint</p>
<p>Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. L Wang, W Y Xu, Y H Lan, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. A Rogers, J Boyd-Graber, N Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, Canada20231</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. C Lu, C Lu, Lange R T, arXiv:2408.062922024arXiv Preprint</p>
<p>Improving automated research via automated review. Y Weng, M Zhu, G Bao, The Thirteenth International Conference on Learning Representations. Singapore2025</p>
<p>Researchagent: Iterative research idea generation over scientific literature with large language models. J Baek, S K Jauhar, S Cucerzan, Proceedings of the 2025 Conference of the Nations of the Americas Chapter. L Chiruzzo, A Ritter, L Wang, the 2025 Conference of the Nations of the Americas ChapterAlbuquerque, New Mexico, USAHuman Language Technologies20251</p>
<p>Pubmed and beyond: biomedical literature search in the age of artificial intelligence. Q Jin, R Leaman, Z Lu, 2024EBioMedicine100104988</p>
<p>Huatuo-26M, a Large-scale Chinese Medical QA Dataset. X Wang, J Li, X Wu, Findings of the Association for Computational Linguistics: Annual Conference of the Nations of the Americas Chapter. L Chiruzzo, A Ritter, L Wang, Albuquerque, New Mexico, USAthe Association for Computational Linguistics2025</p>
<p>Agentic llm workflows for generating patient-friendly medical reports. M Sudarshan, S Shih, E Yee, arXiv:2408.011122024arXiv Preprint</p>
<p>Pre-training of deep bidirectional transformers for language understanding. J Devlin, M W Chang, K Lee, Proceedings of the 2019 Conference of the North American Chapter. W Ammar, A Louis, N Mostafazadeh, the 2019 Conference of the North American ChapterMinneapolis, MinnesotaHuman Language Technologies20191</p>
<p>Large language models encode clinical knowledge. K Singhal, S Azizi, T Tu, Nature. 6202023</p>
<p>Towards Taming Language Model to Be a Doctor. H Zhang, J Chen, F Jiang, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. H Bouamor, J Pino, K Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingapore2023</p>
<p>Bianque: Balancing the questioning and suggestion ability of health llms with multi-turn health conversations polished by chatgpt. Y R Chen, Z Y Wang, X F Xing, arXiv:2310.158962023arXiv Preprint</p>
<p>Biomedlm: A 2.7 b parameter language model trained on biomedical text. E Bolton, A Venigalla, M Yasunaga, arXiv:2403.184212024arXiv Preprint</p>
<p>From beginner to expert: Modeling medical knowledge into general llms. Q Li, X Y Yang, H W Wang, arXiv:2312.010402023arXiv Preprint</p>
<p>Low-rank adaptation of large language models. J Hu, Y Shen, P Wallis, The Tenth International Conference on Learning Representations, Virtual Event. 202213</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, 202235</p>
<p>A prompt pattern catalog to enhance prompt engineering with chatgpt. J White, Q Fu, S Hays, Proceedings of the 30th Conference on Pattern Languages of Programs. J Yoder, R Gabriel, V Vranić, K Brown, the 30th Conference on Pattern Languages of ProgramsMonticello IL, USA20235</p>
<p>Openmedlm: prompt engineering can out-perform fine-tuning in medical questionanswering with open-source large language models. J Maharjan, A Garikipati, N P Singh, Scientific Reports. 14141562024</p>
<p>Few shot chain-of-thought driven reasoning to prompt LLMs for open-ended medical question answering. S S Nachane, O Gramopadhye, P Chanda, Findings of the Association for Computational Linguistics: Conference on Empirical Methods in Natural Language Processing. Y Al-Onaizan, M Bansal, Y Chen, Miami, Florida, USA2024</p>
<p>Prompt engineering for healthcare: Methodologies and applications. J Wang, E Shi, S Yu, arXiv:2304.146702023arXiv Preprint</p>
<p>Autonomous chemical research with large language models. D A Boiko, R Macknight, B Kline, Nature. 6242023</p>
<p>Can generalist foundation models outcompete special-purpose tuning? case study in medicine. H Nori, Y T Lee, S Zhang, arXiv:2311.164522023arXiv Preprint</p>
<p>Generative agents: Interactive simulacra of human behavior. J S Park, O 'brien, J Cai, C J , Proceedings of the 36th annual ACM Symposium on User Interface Software and Technology. S Follmer, J Han, J Steimle, N Riche, the 36th annual ACM Symposium on User Interface Software and TechnologySan Francisco, CA, USA2023</p>
<p>Voyager: An open-ended embodied agent with large language models. G Wang, Y Xie, Y Jiang, Transactions on Machine Learning Research. 2024</p>
<p>Promptbreeder: Self-referential self-improvement via prompt evolution. C Fernando, D Banarse, H Michalewski, Proceedings of the 41st International Conference on Machine Learning. the 41st International Conference on Machine LearningVienna, Austria2024235</p>
<p>Large language models as optimizers. C Yang, X Wang, Y Lu, The Twelfth International Conference on Learning Representations. Vienna, Austria2024</p>
<p>Medagents: Large language models as collaborators for zero-shot medical reasoning. R Tang, A N Zou, Z S Zhang, Ku L, Martins A, Srikumar V2024Findings of the Association for Computational LinguisticsBangkok, Thailand</p>
<p>AI Hospital: Benchmarking Large Language Models in a Multi-agent. Z H Fan, L Wei, J L Tang, Medical Interaction Sci China Inf Sci. 19</p>
<p>O Simulator ; Rambow, L Wanner, M Apidianaki, H Al-Khalifa, Proceedings of the 31st International Conference on Computational Linguistics. B D Eugenio, S Schockaert, the 31st International Conference on Computational LinguisticsAbu Dhabi, UAE2025</p>
<p>Agent hospital: A simulacrum of hospital with evolvable medical agents. J K Li, S Y Wang, M Zhang, arXiv:2405.029572024arXiv Preprint</p>
<p>Ai4r: The fifth scientific research paradigm. G Li, Bulletin of Chinese Academy of Sciences. 392024</p>
<p>In silico protein interaction screening uncovers donson's role in replication initiation. Y Lim, L Tamayo-Orrego, E Schmid, Science. 38134482023</p>
<p>Automated bioinformatics analysis via autoba. J Zhou, B Zhang, X Chen, arXiv:2309.032422023arXiv Preprint</p>
<p>Augmenting large language models with chemistry tools. M Bran, A Cox, S Schilter, O , Nature Machine Intelligence. 62024</p>
<p>Crispr-gpt: An llm agent for automated design of gene-editing experiments. K Huang, Y Qu, H Cousins, arXiv:2404.180212024arXiv Preprint</p>
<p>I A Tiukova, D Brunnsaker, E Y Bjurstr¨om, Towards the automation of systems biology research. 2024, abs/2408.10689arXiv Preprint</p>
<p>Two heads are better than one: A multi-agent system has the potential to improve scientific idea generation. H Su, R Chen, S Tang, arXiv:2410.094032024arXiv Preprint</p>
<p>Collaborative evolving strategy for automatic data-centric development. X Yang, H Chen, W Feng, arXiv:2407.186902024arXiv Preprint</p>
<p>Systematic Reviewing. J Clark, Methods of Clinical Epidemiology. S Doi, G Williams, Berlin, HeidelbergSpringer2013</p>
<p>Automatic boolean query formulation for systematic review literature search. H Scells, G Zuccon, B Koopman, Proceedings of the Web Conference. Y Huang, I King, T Liu, M Steen, the Web ConferenceTaipei Taiwan2020</p>
<p>LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding. Y S Bai, X Lv, J J Zhang, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. L Ku, A Martins, V Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, Thailand20241</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, 202235</p>
<p>Reproducibility in science: improving the standard for basic and preclinical research. C Begley, G Ioannidis, J P , Circulation Research. 1162015</p>
<p>Design of experimental studies in biomedical sciences. B Larijani, A Tayanloo-Beik, M Payab, Biomedical Product Development: Bench to Bedside. 2020</p>
<p>React: Synergizing reasoning and acting in language models. S Yao, J Zhao, D Yu, The Eleventh International Conference on Learning Representations. Kigali, Rwanda2023</p>
<p>Cloud labs: where robots do the research. C Arnold, Nature. 6062022</p>
<p>Autogen: Enabling next-gen LLM applications via multi-agent conversation framework. Y Wu, G Bansal, J Y Zhang, abs/2308.081552023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
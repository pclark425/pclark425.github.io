<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5688 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5688</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5688</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-115.html">extraction-schema-115</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <p><strong>Paper ID:</strong> paper-270560315</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.11213v4.pdf" target="_blank">A Survey of AIOps for Failure Management in the Era of Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> As software systems grow increasingly intricate, Artificial Intelligence for IT Operations (AIOps) methods have been widely used in software system failure management to ensure the high availability and reliability of large-scale distributed software systems. However, these methods still face several challenges, such as lack of cross-platform generality and cross-task flexibility. Fortunately, recent advancements in large language models (LLMs) can significantly address these challenges, and many approaches have already been proposed to explore this field. However, there is currently no comprehensive survey that discusses the differences between LLM-based AIOps and traditional AIOps methods. Therefore, this paper presents a comprehensive survey of AIOps technology for failure management in the LLM era. It includes a detailed definition of AIOps tasks for failure management, the data sources for AIOps, and the LLM-based approaches adopted for AIOps. Additionally, this survey explores the AIOps subtasks, the specific LLM-based approaches suitable for different AIOps subtasks, and the challenges and future directions of the domain, aiming to further its development and application.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5688.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5688.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (zero-shot log AD)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (zero-shot log-based anomaly detection / log parsing)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical studies prompting ChatGPT (GPT-3.5-family) to detect anomalies or parse logs in a zero-shot or few-shot setting; reported to show limited but sometimes promising behavior depending on prompts and summarization strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An Assessment of ChatGPT on Log Data.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (GPT-3.5-family / ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Conversational large pre-trained transformer model from OpenAI (GPT-style autoregressive LLM); used via prompting (zero-shot/few-shot) for natural-language style reasoning over log text.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Prompt-based zero-shot / few-shot classification and log parsing (direct prompting, occasionally with summarization or ICL examples).</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Log sequences / semi-structured textual logs (sequence data)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Log-level anomalies (semantic/syntactic irregularities in log segments), detection of abnormal log segments or events</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey reports qualitative evaluations: zero-shot ChatGPT has limited performance and inconsistent responses; few-shot and prompt engineering (with summarization or examples) improve results. No numeric metrics reported in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Survey notes ChatGPT zero-shot performs worse than specialized tuned models for large-scale/log-specific tasks; prompt engineering and summarization narrow the gap but numerical comparisons not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Inconsistency of responses, limited zero-shot accuracy, scalability issues for large log volumes, high inference cost, and context-window limits; requires careful prompt design or few-shot examples to approach acceptable results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of AIOps for Failure Management in the Era of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5688.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5688.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT + summarization (outage understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT prompted with input summarization for log anomaly detection / outage understanding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Works that combine input summarization with ChatGPT prompting to detect anomalies or improve outage understanding from log segments and historical data; combining summaries and history yields better detection accuracy than naive prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assess and summarize: Improve outage understanding with large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (unspecified generation; GPT-3.5 / GPT-4 often used in referenced studies)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained conversational transformer LLM used via prompt-based pipelines where long inputs are compressed by summarization before being fed to the model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Prompt-based with input summarization (LLM-based compression or non-LLM summarization) plus zero/few-shot prompting and retrieval augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Logs and incident reports (long textual sequences); summarization reduces the context to manageable length.</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Anomalous events within long log segments / incident descriptions (semantic anomalies indicating incidents).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey-level qualitative claim: combined summarization + historical context yields better detection/diagnosis than naive prompts; no numeric metrics reported in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Improves over naive zero-shot prompting; no comparison numbers versus traditional anomaly detectors provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Depends on summary quality; LLM context-window limits remain a bottleneck; risk of hallucination in generated summaries or diagnoses; computational overhead for repeated summarization + LLM inference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of AIOps for Failure Management in the Era of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5688.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5688.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAGLog</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RAGLog: Log Anomaly Detection using Retrieval Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Anomaly-detection approach that integrates an LLM with retrieval-augmented generation (RAG) and vector databases to improve log-based anomaly detection via context retrieval and prompt augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RAGLog: Log Anomaly Detection using Retrieval Augmented Generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Unspecified LLM (used within a RAG pipeline; survey does not fix a single model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM used as the generation/decision engine augmented by a retrieval layer (vector DB) that returns relevant past log contexts or known templates to the model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Retrieval-Augmented Generation (RAG) combined with prompt-based LLM classification; retrieved similar log contexts are included as grounding evidence to the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Log sequences / event sequences (semi-structured textual data)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Log anomalies (semantic deviations, unusual event sequences); aims to reduce false positives by grounding decisions in retrieved historical contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey reports that retrieval augmentation improves detection accuracy over naive prompting; no numeric scores included in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Reported to outperform pure prompt-based LLM approaches (and to be more accurate than unguided LLM decisions); direct comparisons to traditional ML anomaly detectors not quantified in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Effectiveness depends on retrieval quality and vector DB coverage; more engineering overhead (indexing, retrieval); possible latency and cost increases; still subject to hallucinations if retrieved context is weak or irrelevant.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of AIOps for Failure Management in the Era of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5688.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5688.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogGPT / LogGPT-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogGPT (exploring ChatGPT for log-based anomaly detection, with Chain-of-Thought prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Uses ChatGPT-style LLMs with chain-of-thought (CoT) prompting to perform more nuanced log-segment anomaly detection, leveraging intermediate reasoning steps to improve classification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Loggpt: Exploring chatgpt for log-based anomaly detection.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT / GPT-family (used with Chain-of-Thought prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer LLM used interactively with prompts that elicit intermediate reasoning steps before a final anomaly decision.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Prompt-based Chain-of-Thought (CoT) reasoning and in-context learning for classifying log segments as anomalous/normal.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Log sequences (textual logs)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Log anomalies detectable via reasoning over sequences, e.g., abnormal event sequences or exceptional error messages.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey states CoT prompting can strengthen LLM performance for anomaly detection qualitatively; no numeric evaluation reported in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>CoT-enhanced LLMs perform better than plain prompting according to survey; no precise comparison to classical ML/DL detectors provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Additional prompt engineering required; CoT increases token usage (cost) and is sensitive to prompt design; still limited by context/window size and hallucination risk.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of AIOps for Failure Management in the Era of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5688.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5688.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TEST (prompt-embedding for metrics AD)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TEST: Text-prototype aligned embedding to activate LLM's ability for time series</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that tokenizes metrics/time-series data, builds contrastive encoders to create embeddings aligned to text prototypes, then uses prompts to activate LLMs for metrics-based anomaly classification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>TEST: Text prototype aligned embedding to activate LLM's ability for time series.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large language models (unspecified) used as downstream classifiers from prompt-activated embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pipeline combines a learned encoder for time-series-to-embedding transformation with LLM prompting to perform classification; the LLM itself is used via prompts rather than fully fine-tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Embedding-based + prompt-based: the numeric time series are embedded (instance-/feature-/text-prototype-aligned contrastive objective) and then provided to an LLM via prompt embeddings for anomaly classification.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Metrics / multivariate time series (structured sequential numeric data)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Time-series anomalies (outliers, distributional shifts, anomalous segments) in metrics-based monitoring</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey states TEST achieves 'excellent results' in failure category classification and metrics-based classification tasks but does not report quantitative metrics in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Reported to be effective at activating LLMs for metrics tasks compared to naive prompting; specific comparisons to classical time-series models not provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires a learned encoder and design of text-prototype alignment; effectiveness depends on quality of embeddings and prompt alignment; computational overhead for encoding + LLM inference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of AIOps for Failure Management in the Era of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5688.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5688.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TabLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TabLLM: Few-shot classification of tabular data with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approach that serializes structured/tabular rows into natural language strings and prompts LLMs in few-shot or zero-shot setups to perform classification on tabular instances.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tabllm: Few-shot classification of tabular data with large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Unspecified large language models (few-shot prompting paradigm)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs used in a prompt-based few-shot classification paradigm after serializing tabular rows into text; no single model size mandated in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Few-shot / zero-shot prompt-based classification on serialized tabular rows (table-to-text conversion and prompting).</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Tabular / structured data (rows serialized to natural-language strings)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Tabular classification anomalies (outliers, rare classes, semantic anomalies in structured rows)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey does not report concrete numeric metrics; TabLLM is mentioned as an example of few-shot tabular classification using LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Survey implies this approach enables few-shot tabular classification but does not provide quantitative comparisons vs. classical tabular ML methods.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Serialization can lose fine-grained numeric structure; prompts may be sensitive to ordering and formatting; latency and cost may be higher than compact tabular models; no numeric evidence provided in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of AIOps for Failure Management in the Era of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5688.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5688.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT-Log / BERTOps</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT-Log / BERTOps: Fine-tuned BERT variants for log anomaly detection / failure classification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods that fine-tune BERT-family models on log data (e.g., Loghub datasets) to perform anomaly detection / failure category classification, often achieving strong task-specific performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bert-log: Anomaly detection for system logs based on pre-trained language model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT / BERT-variants (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained bidirectional transformer encoder (BERT) fine-tuned on log datasets; in some works full fine-tuning is used, in others parameter-efficient approaches (LoRA, adapters) are employed.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Fine-tuning-based supervised classification (BERT fine-tuned to distinguish normal vs anomalous log sequences or to predict log tokens for reconstruction-based detection).</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Log sequences (textual event sequences represented as tokens or templates)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Log anomalies and failure category classification (resource shortages, config errors, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Loghub (mentioned for BERTOps and HilBERT and other log datasets in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey reports that BERTOps achieves state-of-the-art results for failure category classification on cited datasets but does not list numeric scores in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Fine-tuned BERT approaches reported to outperform older semantic-extraction pipelines and non-LLM feature methods on log classification tasks (per the survey), though exact baselines and numbers are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>High training cost for full fine-tuning; potential brittleness when logs change format (need for re-tuning or adapters); may not scale to very large real-time streaming without optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of AIOps for Failure Management in the Era of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5688.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5688.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SigLLM / time-series-to-text prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SigLLM / time-series-to-text prompting for anomaly detection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods that convert time series / metrics into text descriptions and then prompt LLMs directly to indicate which elements are anomalous, enabling end-to-end LLM-based time-series anomaly detection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Unspecified LLMs (prompted via time-series-to-text conversion)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs are used as classifiers by ingesting textual encodings of numeric time-series; models may be off-the-shelf LLMs used with specialized prompt templates.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Prompt-based: convert time-series windows to text and ask LLM to label anomalous time points/segments (time-series-to-text plus LLM prompting).</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Metrics / time series (structured sequential numerical data converted to natural-language)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Time-series point/segment anomalies and outliers</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey references the method but does not report numeric performance in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Not directly compared in the survey to classical forecasting-based detectors; method noted as a strategy used in metrics-based prompt works.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Conversion from numeric series to text may lose precision; context-window limits and tokenization cost; LLM overconfidence and lack of calibrated uncertainty are concerns noted in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of AIOps for Failure Management in the Era of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5688.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e5688.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs as zero-shot time-series forecasters</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language models as zero-shot time series forecasters / anomaly detectors</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Survey cites studies that evaluate LLMs' capability to forecast time series and to act as zero-shot anomaly detectors for time-series data, reporting surprising baseline-level capabilities but concerns about calibration and overconfidence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are zero-shot time series forecasters.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various LLMs (GPT-family, LLaMA variants; exact models vary across studies)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large autoregressive or decoder-only transformer LMs repurposed for numeric forecasting tasks via prompt engineering or reprogramming; some works create time-series foundation models (TimeGPT, Lag-llama) using transformer architectures specialized for time series.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Prompt-based forecasting (zero-shot) and prediction-compare anomaly detection; also foundation/fine-tuned time-series LLMs for probabilistic forecasting.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Univariate and multivariate time series (sequential numeric data)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Forecast-based anomalies (deviations between predicted and observed values), outliers and distributional shifts</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey reports mixed results: LLMs can perform surprisingly well zero-shot on forecasting tasks in some studies, but issues like overconfidence and calibration limit reliability; no unified numeric metrics reported in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Some studies show LLM zero-shot forecasts approach fine-tuned models on some tasks; others highlight that specialized time-series models or fine-tuned LLMs outperform naive LLM prompting. Survey emphasizes heterogenous findings without detailed numeric comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Overconfidence (poor-calibrated uncertainty), dependence on prompt design, and inconsistent results across datasets; computational cost and latency for continuous detection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of AIOps for Failure Management in the Era of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>An Assessment of ChatGPT on Log Data. <em>(Rating: 2)</em></li>
                <li>Assess and summarize: Improve outage understanding with large language models. <em>(Rating: 2)</em></li>
                <li>RAGLog: Log Anomaly Detection using Retrieval Augmented Generation. <em>(Rating: 2)</em></li>
                <li>Loggpt: Exploring chatgpt for log-based anomaly detection. <em>(Rating: 2)</em></li>
                <li>TEST: Text prototype aligned embedding to activate LLM's ability for time series. <em>(Rating: 2)</em></li>
                <li>Tabllm: Few-shot classification of tabular data with large language models. <em>(Rating: 2)</em></li>
                <li>Bert-log: Anomaly detection for system logs based on pre-trained language model. <em>(Rating: 2)</em></li>
                <li>LogPrompt: A Log-based Anomaly Detection Framework Using Prompts. <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot time series forecasters. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5688",
    "paper_id": "paper-270560315",
    "extraction_schema_id": "extraction-schema-115",
    "extracted_data": [
        {
            "name_short": "ChatGPT (zero-shot log AD)",
            "name_full": "ChatGPT (zero-shot log-based anomaly detection / log parsing)",
            "brief_description": "Empirical studies prompting ChatGPT (GPT-3.5-family) to detect anomalies or parse logs in a zero-shot or few-shot setting; reported to show limited but sometimes promising behavior depending on prompts and summarization strategies.",
            "citation_title": "An Assessment of ChatGPT on Log Data.",
            "mention_or_use": "use",
            "model_name": "ChatGPT (GPT-3.5-family / ChatGPT)",
            "model_description": "Conversational large pre-trained transformer model from OpenAI (GPT-style autoregressive LLM); used via prompting (zero-shot/few-shot) for natural-language style reasoning over log text.",
            "model_size": null,
            "anomaly_detection_method": "Prompt-based zero-shot / few-shot classification and log parsing (direct prompting, occasionally with summarization or ICL examples).",
            "data_type": "Log sequences / semi-structured textual logs (sequence data)",
            "anomaly_type": "Log-level anomalies (semantic/syntactic irregularities in log segments), detection of abnormal log segments or events",
            "dataset_name": null,
            "performance_metrics": "Survey reports qualitative evaluations: zero-shot ChatGPT has limited performance and inconsistent responses; few-shot and prompt engineering (with summarization or examples) improve results. No numeric metrics reported in this survey.",
            "baseline_comparison": "Survey notes ChatGPT zero-shot performs worse than specialized tuned models for large-scale/log-specific tasks; prompt engineering and summarization narrow the gap but numerical comparisons not provided.",
            "limitations_or_failure_cases": "Inconsistency of responses, limited zero-shot accuracy, scalability issues for large log volumes, high inference cost, and context-window limits; requires careful prompt design or few-shot examples to approach acceptable results.",
            "uuid": "e5688.0",
            "source_info": {
                "paper_title": "A Survey of AIOps for Failure Management in the Era of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "ChatGPT + summarization (outage understanding)",
            "name_full": "ChatGPT prompted with input summarization for log anomaly detection / outage understanding",
            "brief_description": "Works that combine input summarization with ChatGPT prompting to detect anomalies or improve outage understanding from log segments and historical data; combining summaries and history yields better detection accuracy than naive prompts.",
            "citation_title": "Assess and summarize: Improve outage understanding with large language models.",
            "mention_or_use": "use",
            "model_name": "ChatGPT (unspecified generation; GPT-3.5 / GPT-4 often used in referenced studies)",
            "model_description": "Pretrained conversational transformer LLM used via prompt-based pipelines where long inputs are compressed by summarization before being fed to the model.",
            "model_size": null,
            "anomaly_detection_method": "Prompt-based with input summarization (LLM-based compression or non-LLM summarization) plus zero/few-shot prompting and retrieval augmentation.",
            "data_type": "Logs and incident reports (long textual sequences); summarization reduces the context to manageable length.",
            "anomaly_type": "Anomalous events within long log segments / incident descriptions (semantic anomalies indicating incidents).",
            "dataset_name": null,
            "performance_metrics": "Survey-level qualitative claim: combined summarization + historical context yields better detection/diagnosis than naive prompts; no numeric metrics reported in survey.",
            "baseline_comparison": "Improves over naive zero-shot prompting; no comparison numbers versus traditional anomaly detectors provided in the survey.",
            "limitations_or_failure_cases": "Depends on summary quality; LLM context-window limits remain a bottleneck; risk of hallucination in generated summaries or diagnoses; computational overhead for repeated summarization + LLM inference.",
            "uuid": "e5688.1",
            "source_info": {
                "paper_title": "A Survey of AIOps for Failure Management in the Era of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "RAGLog",
            "name_full": "RAGLog: Log Anomaly Detection using Retrieval Augmented Generation",
            "brief_description": "Anomaly-detection approach that integrates an LLM with retrieval-augmented generation (RAG) and vector databases to improve log-based anomaly detection via context retrieval and prompt augmentation.",
            "citation_title": "RAGLog: Log Anomaly Detection using Retrieval Augmented Generation.",
            "mention_or_use": "use",
            "model_name": "Unspecified LLM (used within a RAG pipeline; survey does not fix a single model)",
            "model_description": "LLM used as the generation/decision engine augmented by a retrieval layer (vector DB) that returns relevant past log contexts or known templates to the model.",
            "model_size": null,
            "anomaly_detection_method": "Retrieval-Augmented Generation (RAG) combined with prompt-based LLM classification; retrieved similar log contexts are included as grounding evidence to the LLM.",
            "data_type": "Log sequences / event sequences (semi-structured textual data)",
            "anomaly_type": "Log anomalies (semantic deviations, unusual event sequences); aims to reduce false positives by grounding decisions in retrieved historical contexts.",
            "dataset_name": null,
            "performance_metrics": "Survey reports that retrieval augmentation improves detection accuracy over naive prompting; no numeric scores included in the survey.",
            "baseline_comparison": "Reported to outperform pure prompt-based LLM approaches (and to be more accurate than unguided LLM decisions); direct comparisons to traditional ML anomaly detectors not quantified in the survey.",
            "limitations_or_failure_cases": "Effectiveness depends on retrieval quality and vector DB coverage; more engineering overhead (indexing, retrieval); possible latency and cost increases; still subject to hallucinations if retrieved context is weak or irrelevant.",
            "uuid": "e5688.2",
            "source_info": {
                "paper_title": "A Survey of AIOps for Failure Management in the Era of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LogGPT / LogGPT-CoT",
            "name_full": "LogGPT (exploring ChatGPT for log-based anomaly detection, with Chain-of-Thought prompting)",
            "brief_description": "Uses ChatGPT-style LLMs with chain-of-thought (CoT) prompting to perform more nuanced log-segment anomaly detection, leveraging intermediate reasoning steps to improve classification.",
            "citation_title": "Loggpt: Exploring chatgpt for log-based anomaly detection.",
            "mention_or_use": "use",
            "model_name": "ChatGPT / GPT-family (used with Chain-of-Thought prompting)",
            "model_description": "Autoregressive transformer LLM used interactively with prompts that elicit intermediate reasoning steps before a final anomaly decision.",
            "model_size": null,
            "anomaly_detection_method": "Prompt-based Chain-of-Thought (CoT) reasoning and in-context learning for classifying log segments as anomalous/normal.",
            "data_type": "Log sequences (textual logs)",
            "anomaly_type": "Log anomalies detectable via reasoning over sequences, e.g., abnormal event sequences or exceptional error messages.",
            "dataset_name": null,
            "performance_metrics": "Survey states CoT prompting can strengthen LLM performance for anomaly detection qualitatively; no numeric evaluation reported in the survey.",
            "baseline_comparison": "CoT-enhanced LLMs perform better than plain prompting according to survey; no precise comparison to classical ML/DL detectors provided.",
            "limitations_or_failure_cases": "Additional prompt engineering required; CoT increases token usage (cost) and is sensitive to prompt design; still limited by context/window size and hallucination risk.",
            "uuid": "e5688.3",
            "source_info": {
                "paper_title": "A Survey of AIOps for Failure Management in the Era of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "TEST (prompt-embedding for metrics AD)",
            "name_full": "TEST: Text-prototype aligned embedding to activate LLM's ability for time series",
            "brief_description": "A method that tokenizes metrics/time-series data, builds contrastive encoders to create embeddings aligned to text prototypes, then uses prompts to activate LLMs for metrics-based anomaly classification.",
            "citation_title": "TEST: Text prototype aligned embedding to activate LLM's ability for time series.",
            "mention_or_use": "use",
            "model_name": "Large language models (unspecified) used as downstream classifiers from prompt-activated embeddings",
            "model_description": "Pipeline combines a learned encoder for time-series-to-embedding transformation with LLM prompting to perform classification; the LLM itself is used via prompts rather than fully fine-tuned.",
            "model_size": null,
            "anomaly_detection_method": "Embedding-based + prompt-based: the numeric time series are embedded (instance-/feature-/text-prototype-aligned contrastive objective) and then provided to an LLM via prompt embeddings for anomaly classification.",
            "data_type": "Metrics / multivariate time series (structured sequential numeric data)",
            "anomaly_type": "Time-series anomalies (outliers, distributional shifts, anomalous segments) in metrics-based monitoring",
            "dataset_name": null,
            "performance_metrics": "Survey states TEST achieves 'excellent results' in failure category classification and metrics-based classification tasks but does not report quantitative metrics in this survey.",
            "baseline_comparison": "Reported to be effective at activating LLMs for metrics tasks compared to naive prompting; specific comparisons to classical time-series models not provided in the survey.",
            "limitations_or_failure_cases": "Requires a learned encoder and design of text-prototype alignment; effectiveness depends on quality of embeddings and prompt alignment; computational overhead for encoding + LLM inference.",
            "uuid": "e5688.4",
            "source_info": {
                "paper_title": "A Survey of AIOps for Failure Management in the Era of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "TabLLM",
            "name_full": "TabLLM: Few-shot classification of tabular data with large language models",
            "brief_description": "Approach that serializes structured/tabular rows into natural language strings and prompts LLMs in few-shot or zero-shot setups to perform classification on tabular instances.",
            "citation_title": "Tabllm: Few-shot classification of tabular data with large language models.",
            "mention_or_use": "use",
            "model_name": "Unspecified large language models (few-shot prompting paradigm)",
            "model_description": "LLMs used in a prompt-based few-shot classification paradigm after serializing tabular rows into text; no single model size mandated in the survey.",
            "model_size": null,
            "anomaly_detection_method": "Few-shot / zero-shot prompt-based classification on serialized tabular rows (table-to-text conversion and prompting).",
            "data_type": "Tabular / structured data (rows serialized to natural-language strings)",
            "anomaly_type": "Tabular classification anomalies (outliers, rare classes, semantic anomalies in structured rows)",
            "dataset_name": null,
            "performance_metrics": "Survey does not report concrete numeric metrics; TabLLM is mentioned as an example of few-shot tabular classification using LLMs.",
            "baseline_comparison": "Survey implies this approach enables few-shot tabular classification but does not provide quantitative comparisons vs. classical tabular ML methods.",
            "limitations_or_failure_cases": "Serialization can lose fine-grained numeric structure; prompts may be sensitive to ordering and formatting; latency and cost may be higher than compact tabular models; no numeric evidence provided in survey.",
            "uuid": "e5688.5",
            "source_info": {
                "paper_title": "A Survey of AIOps for Failure Management in the Era of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "BERT-Log / BERTOps",
            "name_full": "BERT-Log / BERTOps: Fine-tuned BERT variants for log anomaly detection / failure classification",
            "brief_description": "Methods that fine-tune BERT-family models on log data (e.g., Loghub datasets) to perform anomaly detection / failure category classification, often achieving strong task-specific performance.",
            "citation_title": "Bert-log: Anomaly detection for system logs based on pre-trained language model.",
            "mention_or_use": "use",
            "model_name": "BERT / BERT-variants (fine-tuned)",
            "model_description": "Pretrained bidirectional transformer encoder (BERT) fine-tuned on log datasets; in some works full fine-tuning is used, in others parameter-efficient approaches (LoRA, adapters) are employed.",
            "model_size": null,
            "anomaly_detection_method": "Fine-tuning-based supervised classification (BERT fine-tuned to distinguish normal vs anomalous log sequences or to predict log tokens for reconstruction-based detection).",
            "data_type": "Log sequences (textual event sequences represented as tokens or templates)",
            "anomaly_type": "Log anomalies and failure category classification (resource shortages, config errors, etc.)",
            "dataset_name": "Loghub (mentioned for BERTOps and HilBERT and other log datasets in survey)",
            "performance_metrics": "Survey reports that BERTOps achieves state-of-the-art results for failure category classification on cited datasets but does not list numeric scores in the survey text.",
            "baseline_comparison": "Fine-tuned BERT approaches reported to outperform older semantic-extraction pipelines and non-LLM feature methods on log classification tasks (per the survey), though exact baselines and numbers are not provided.",
            "limitations_or_failure_cases": "High training cost for full fine-tuning; potential brittleness when logs change format (need for re-tuning or adapters); may not scale to very large real-time streaming without optimization.",
            "uuid": "e5688.6",
            "source_info": {
                "paper_title": "A Survey of AIOps for Failure Management in the Era of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "SigLLM / time-series-to-text prompting",
            "name_full": "SigLLM / time-series-to-text prompting for anomaly detection",
            "brief_description": "Methods that convert time series / metrics into text descriptions and then prompt LLMs directly to indicate which elements are anomalous, enabling end-to-end LLM-based time-series anomaly detection.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Unspecified LLMs (prompted via time-series-to-text conversion)",
            "model_description": "LLMs are used as classifiers by ingesting textual encodings of numeric time-series; models may be off-the-shelf LLMs used with specialized prompt templates.",
            "model_size": null,
            "anomaly_detection_method": "Prompt-based: convert time-series windows to text and ask LLM to label anomalous time points/segments (time-series-to-text plus LLM prompting).",
            "data_type": "Metrics / time series (structured sequential numerical data converted to natural-language)",
            "anomaly_type": "Time-series point/segment anomalies and outliers",
            "dataset_name": null,
            "performance_metrics": "Survey references the method but does not report numeric performance in this paper.",
            "baseline_comparison": "Not directly compared in the survey to classical forecasting-based detectors; method noted as a strategy used in metrics-based prompt works.",
            "limitations_or_failure_cases": "Conversion from numeric series to text may lose precision; context-window limits and tokenization cost; LLM overconfidence and lack of calibrated uncertainty are concerns noted in the survey.",
            "uuid": "e5688.7",
            "source_info": {
                "paper_title": "A Survey of AIOps for Failure Management in the Era of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LLMs as zero-shot time-series forecasters",
            "name_full": "Large language models as zero-shot time series forecasters / anomaly detectors",
            "brief_description": "Survey cites studies that evaluate LLMs' capability to forecast time series and to act as zero-shot anomaly detectors for time-series data, reporting surprising baseline-level capabilities but concerns about calibration and overconfidence.",
            "citation_title": "Large language models are zero-shot time series forecasters.",
            "mention_or_use": "mention",
            "model_name": "Various LLMs (GPT-family, LLaMA variants; exact models vary across studies)",
            "model_description": "Large autoregressive or decoder-only transformer LMs repurposed for numeric forecasting tasks via prompt engineering or reprogramming; some works create time-series foundation models (TimeGPT, Lag-llama) using transformer architectures specialized for time series.",
            "model_size": null,
            "anomaly_detection_method": "Prompt-based forecasting (zero-shot) and prediction-compare anomaly detection; also foundation/fine-tuned time-series LLMs for probabilistic forecasting.",
            "data_type": "Univariate and multivariate time series (sequential numeric data)",
            "anomaly_type": "Forecast-based anomalies (deviations between predicted and observed values), outliers and distributional shifts",
            "dataset_name": null,
            "performance_metrics": "Survey reports mixed results: LLMs can perform surprisingly well zero-shot on forecasting tasks in some studies, but issues like overconfidence and calibration limit reliability; no unified numeric metrics reported in the survey.",
            "baseline_comparison": "Some studies show LLM zero-shot forecasts approach fine-tuned models on some tasks; others highlight that specialized time-series models or fine-tuned LLMs outperform naive LLM prompting. Survey emphasizes heterogenous findings without detailed numeric comparisons.",
            "limitations_or_failure_cases": "Overconfidence (poor-calibrated uncertainty), dependence on prompt design, and inconsistent results across datasets; computational cost and latency for continuous detection.",
            "uuid": "e5688.8",
            "source_info": {
                "paper_title": "A Survey of AIOps for Failure Management in the Era of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "An Assessment of ChatGPT on Log Data.",
            "rating": 2,
            "sanitized_title": "an_assessment_of_chatgpt_on_log_data"
        },
        {
            "paper_title": "Assess and summarize: Improve outage understanding with large language models.",
            "rating": 2,
            "sanitized_title": "assess_and_summarize_improve_outage_understanding_with_large_language_models"
        },
        {
            "paper_title": "RAGLog: Log Anomaly Detection using Retrieval Augmented Generation.",
            "rating": 2,
            "sanitized_title": "raglog_log_anomaly_detection_using_retrieval_augmented_generation"
        },
        {
            "paper_title": "Loggpt: Exploring chatgpt for log-based anomaly detection.",
            "rating": 2,
            "sanitized_title": "loggpt_exploring_chatgpt_for_logbased_anomaly_detection"
        },
        {
            "paper_title": "TEST: Text prototype aligned embedding to activate LLM's ability for time series.",
            "rating": 2,
            "sanitized_title": "test_text_prototype_aligned_embedding_to_activate_llms_ability_for_time_series"
        },
        {
            "paper_title": "Tabllm: Few-shot classification of tabular data with large language models.",
            "rating": 2,
            "sanitized_title": "tabllm_fewshot_classification_of_tabular_data_with_large_language_models"
        },
        {
            "paper_title": "Bert-log: Anomaly detection for system logs based on pre-trained language model.",
            "rating": 2,
            "sanitized_title": "bertlog_anomaly_detection_for_system_logs_based_on_pretrained_language_model"
        },
        {
            "paper_title": "LogPrompt: A Log-based Anomaly Detection Framework Using Prompts.",
            "rating": 2,
            "sanitized_title": "logprompt_a_logbased_anomaly_detection_framework_using_prompts"
        },
        {
            "paper_title": "Large language models are zero-shot time series forecasters.",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zeroshot_time_series_forecasters"
        }
    ],
    "cost": 0.020645999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Survey of AIOps for Failure Management in the Era of Large Language Models
24 Jun 2024</p>
<p>Lingzhe Zhang zhang.lingzhe@stu.pku.edu.cn 
Xuming Hu xuminghu@hkust-gz.edu.cn 
Philip S Yu psyu@cs.uic.edu 
Tong Jia jia.tong@pku.edu.cn 
Yifan Wu yifanwu@pku.edu.cn 
Ying Li li.ying@pku.edu.cn. 
Mengxi Jia mxjia@pku.edu.cn 
Aiwei Liu 
Yong Yang yang.yong@pku.edu.cn 
Zhonghai Wu </p>
<p>Peking University
China</p>
<p>Peking University
China</p>
<p>The Hong Kong University of Science and Technology (Guangzhou)
China</p>
<p>University of Illinois Chicago
United States</p>
<p>YING LI *
Peking University
China</p>
<p>Lingzhe Zhang
Peking University
BeijingChina</p>
<p>Peking University
BeijingChina</p>
<p>Mengxi Jia
Peking University
BeijingChina</p>
<p>Peking University
BeijingChina</p>
<p>Tsinghua University
Aiwei Liu, BeijingChina</p>
<p>tsinghua.edu.cn; Yong Yang
Peking University
BeijingChina</p>
<p>Peking University
Zhonghai Wu, BeijingChina</p>
<p>The Hong Kong University of Science and Technology (Guangzhou)
Xuming HuGuangzhouChina</p>
<p>University of Illinois Chicago
ChicagoUnited States</p>
<p>Peking University
BeijingChina</p>
<p>A Survey of AIOps for Failure Management in the Era of Large Language Models
24 Jun 20241A6A17F7A0F668E678E9328B805E1992arXiv:2406.11213v4[cs.SE]Large Language ModelAIOpsFailure ManagementMetricsLogsTime SeriesFailure PerceptionAnomaly DetectionRoot Cause AnalysisAuto Remediation
As software systems grow increasingly intricate, Artificial Intelligence for IT Operations (AIOps) methods have been widely used in software system failure management to ensure the high availability and reliability of large-scale distributed software systems.However, these methods still face several challenges, such as lack of cross-platform generality and cross-task flexibility.Fortunately, recent advancements in large language models (LLMs) can significantly address these challenges, and many approaches have already been proposed to explore this field.However, there is currently no comprehensive survey that discusses the differences between LLM-based AIOps and traditional AIOps methods.Therefore, this paper presents a comprehensive survey of AIOps technology for failure management in the LLM era.It includes a detailed definition of AIOps tasks for failure management, the data sources for AIOps, and the LLM-based approaches adopted for AIOps.Additionally, this survey explores the AIOps subtasks, the specific LLM-based approaches suitable for different AIOps subtasks, and the challenges and future directions of the domain, aiming to further its development and application.CCS Concepts:  Software and its engineering  Maintaining software;  Computing methodologies  Artificial intelligence;  General and reference  Surveys and overviews.</p>
<p>INTRODUCTION</p>
<p>Nowadays, software systems are becoming increasingly complex.These systems typically serve massive user bases numbering in the billions, where even minor software glitches can result in significant losses due to service interruptions or degraded service quality [29,87].Therefore, largescale distributed software systems need to ensure uninterrupted 24/7 service, with high availability and reliability requirements.However, due to their vast scale and intricate logic of operation, these software systems frequently experience failures that are challenging to detect, pinpoint, and diagnose.Analyzing and debugging system errors become even more difficult once faults occur.Thus, enhancing fault diagnosis efficiency, swiftly identifying system failures, pinpointing root causes, and promptly remedying them have become critical for ensuring the high availability and reliability of large-scale distributed software systems.</p>
<p>With the advancement of Artificial Intelligence (AI), Artificial Intelligence for IT Operations (AIOps) was first introduced by Gartner in 2016 [103].AIOps leverages machine learning (ML) or deep learning (DL) algorithms to analyze vast amounts of data from various operational tools and devices, automatically detecting and responding to system issues in real-time.This enhances the capabilities and automation levels of Information Technology (IT) operations.Consequently, AIOps for failure management (FM) has become a mainstream approach to ensuring high availability and reliability of software systems.</p>
<p>Why are LLMs Beneficial for AIOps on Failure Management?</p>
<p>While aformethioned ML-based or DL-based AIOps methods for failure management have significantly assisted in software system operations, they still face several challenges as follows:</p>
<p> Need for complex feature extraction engineering.These AIOps methods typically require extensive preprocessing and feature engineering to extract useful information from raw data.They have limited capabilities in understanding and processing data, especially in handling unstructured data such as logs and traces, which appearing relatively weak. Lack of cross-platform generality.Traditional AIOps models are often tuned and trained specifically for a particular software system.Once a different software system is adopted or even minor changes are made to the original system, the performance of the model significantly deteriorates, even when performing the same task. Lack of cross-task flexibility.Due to the singularity of model knowledge and outputs, AIOps models can only perform one task at a time.For example, in Root Cause Analysis (RCA) tasks, some work is aimed at identifying the cause of the problem [79,119,142], while others are focused on identifying the software components involved [50,93,126].In real-world scenarios, multiple models must run simultaneously to complete the entire RCA task. Limited model adaptability.With changes in the software system, deep learning-based AIOps methods typically require frequent model training and updates to adapt to new data and environments.While there are many online learning methods [1,42,72,86] available to address this issue, this process not only consumes time and effort but also can result in delayed responses from the model when handling sudden events. Restricted levels of automation.Current deep learning-based AIOps methods exhibit relatively limited capabilities in terms of automated operations and intelligent decisionmaking.While some degree of automation is achievable, significant manual intervention and configuration are still required.Particularly in the case of Auto Remediation, current efforts are mainly stopped at Incident Triage [114,143] or Solution Recommendation [78,127,159].</p>
<p>Large Language Models (LLMs), pre-trained on natural language understanding tasks, offer a promising avenue for addressing these limitations.(1) Due to their robust natural language processing capabilities, LLMs can efficiently handle and comprehend unstructured data, often eliminating the need for prior feature extraction.(2) Trained on vast amounts of cross-platform data, LLMs possess a strong degree of generality and logical reasoning abilities.(3) Outputting natural language, LLMs offer great flexibility, enabling them to simultaneously perform multiple AIOps tasks, such as identifying the cause of a problem and the involved software components.(4) Leveraging their pre-training, LLMs exhibit powerful adaptive capabilities and can incorporate continuously updated external knowledge using methods like Retrieval-Augmented Generation (RAG), often without requiring retraining.(5) With robust script generation capabilities and the ability to automatically invoke external tools, LLMs can achieve higher levels of automation.</p>
<p>Why a Survey of AIOps for Failure Management in the Era of LLMs?</p>
<p>Numerous literature reviews have summarized research on AIOps.As shown in Table 1, these works are either based on traditional machine learning or deep learning algorithms and do not use LLM-based approaches [19,97,107,131], or they do not provide a systematic summary of all tasks involved in the full process of AIOps for failure management [118].Some reviews may not even specifically focus on failure management within the realm of AIOps [23,30,152].</p>
<p>In summary, comprehensive studies exploring AIOps for failure management in the era of LLMs are lacking.However, as illustrated in Section 1.1, LLM-based approaches offer significant benefits for AIOps tasks.In this study, we present the first comprehensive survey that covers the entire process of AIOps for failure management in the context of large language models.This survey encompasses a detailed definition of AIOps tasks for failure management, the data sources for AIOps, and the LLM-based approaches adopted for AIOps.Moreover, we delve into the AIOps subtasks and the specific LLM-based approaches suitable for different AIOps subtasks.This survey aims to provide researchers with an in-depth understanding of LLM-based approaches for AIOps, facilitating comparisons and contrasts among different methods.It also guides users interested in applying LLM-based AIOps methods by helping them choose suitable algorithms for different application scenarios.</p>
<p>Organization of this survey.This survey is structured as follows: Section 2 introduces the necessary preliminaries, including data sources for AIOps tasks, LLM-based approaches for AIOps, and AIOps tasks for failure management.Sections 3 through 6 detail the new characteristics and methods of four types of AIOps tasks in the era of LLMs according to a specific taxonomy in AIOps tasks: data preprocessing, failure perception, root cause analysis, and auto remediation.Section 7 examines ongoing challenges and potential future research avenues in LLM-based AIOps for failure management.The survey concludes in Section 8.</p>
<p>PRELIMINARY</p>
<p>To facilitate the introduction of various AIOps tasks and their LLM-based methods in subsequent sections, this section presents the data sources adopted for AIOps, the LLM-based approaches widely used in AIOps, and the taxonomy of AIOps tasks for failure management.</p>
<p>Data Source for AIOps</p>
<p>As shown in Table 1, the data sources adopted for AIOps can be divided into two categories based on their origin: system-generated data and human-generated data.System-generated data</p>
<p></p>
<p>is automatically produced by the system, whereas human-generated data is created by people, including developers, operations personnel, and even users.</p>
<p>System-generated Data.This type of data is the most commonly used in traditional ML-based and DL-based AIOps for failure management [71,79,119,142,144,145,147,160,162].The data types include metrics, logs, and traces.</p>
<p> Metrics.Metrics are quantitative measurements collected from various components of the IT infrastructure, such as CPU usage, memory usage, disk I/O, network latency, and throughput.They provide real-time performance data and are crucial for monitoring the health and performance of the system. Logs.Logs are detailed records of events that occur within the system.They can include error messages, transaction records, user activities, and system operations.Logs are essential for diagnosing issues, understanding system behavior, and tracking changes over time. Trace.Traces are records of the sequence of operations or transactions that a request goes through in a distributed system.They provide a high-level view of how different services interact and help identify performance bottlenecks, dependencies, and the root cause of issues in microservices architectures.</p>
<p>Human-generated Data.With the emergence of LLMs, in addition to system-generated data, many approaches also utilize information created by humans to provide auxiliary knowledge for software system failure management [2,9,10,34,37,40,55,59,108,116,137,146,155].The data types include Software Information, Question &amp; Answer (QA), and Incident Reports.</p>
<p> Software Information.Software information is generated during the software development process and includes details such as software architecture, configurations, documentation, and implementation code.This type of information provides valuable knowledge about the development process, helping AIOps approaches to better perform various failure management tasks.By leveraging software information, AIOps can gain insights into the system's design and functionality, which can be crucial for diagnosing issues and implementing effective solutions. Question &amp; Answer (QA).QA data consists of question-and-answer pairs related to operational or development knowledge.In LLMs, utilizing a rich repository of QA data can serve as a knowledge base that functions like a search engine, providing valuable assistance to operations personnel.Additionally, similar to software information, QA data can supply auxiliary knowledge to enhance AIOps solutions. Incident Reports.Incident reports are often written by software users.When an incident is created, the author specifies a title for the incident and describes relevant details such as error messages, anomalous behavior, and other information that could help with resolution.Before the emergence of LLMs, these incident reports were submitted to on-call engineers (OCEs) for diagnosis.However, due to the powerful natural language processing capabilities of LLMs, many approaches now automatically analyze these incident reports, diagnose faults, and even suggest mitigation steps.This automation enhances the efficiency of failure management by quickly identifying and addressing issues based on the detailed information provided in the reports.</p>
<p>LLM-based Approaches for AIOps</p>
<p>Many surveys have proposed various LLM-based approaches [13,36,73,92,109], many of which have been applied to address AIOps for failure management tasks.As shown in Figure 2, in this survey, we categorize these approaches into four groups: foundation model, fine-tuning approach, embedding-based approach, and prompt-based approach.The foundation model refers to pretrained language models that serve as the base for AIOps tasks without further modifications.The fine-tuning approach involves adapting these pre-trained models to specific tasks through additional training on task-specific data.The embedding-based approach uses the representations generated by pre-trained models to capture semantic information and improve task performance.The prompt-based approach leverages natural language prompts to guide the model's responses, enabling it to perform specific tasks based on the given instructions.</p>
<p>Foundational Model.In the foundational models pre-trained for AIOps tasks, most are based on the Transformer framework [22,24,33,39,77,90,106,116], though a few are not [27,28,129,132].</p>
<p> Transformer-based.These models are built on the Transformer architecture, which uses self-attention mechanisms to capture dependencies in data.Examples include BERT, GPT, and T5, which have been widely used for various NLP tasks due to their ability to understand and generate human language effectively. non-Transformer-based.This category includes models such as MLP (Multi-Layer Perceptron), RNN (Recurrent Neural Networks), CNN (Convolutional Neural Networks), diffusionbased models, and others.These models leverage different architectural principles and have unique strengths in handling specific types of data and tasks.For instance, RNNs are particularly suited for sequential data, while CNNs excel in processing grid-like data structures.</p>
<p>Fine-tuning Approach.Directly applying general foundational models to AIOps for failure management tasks often does not yield optimal results.Therefore, many AIOps approaches finetune these models using domain-specific datasets [2,9,12,15,21,37,58,110,157].This fine-tuning can be categorized into two types: full fine-tuning and parameter-efficient fine-tuning.</p>
<p> Full Fine-Tuning.This involves updating all the parameters of the foundational model using domain-specific data.However, due to the large number of parameters in models like GPT-3.5 and GPT-4, most current AIOps work in this area is based on models like BERT and T5, which are more manageable in size and complexity. Parameter-Efficient Fine-Tuning.This includes techniques such as Layer-Freezing, Adapter Tuning, and Task-Conditional Fine-Tuning.These methods require tuning only a small subset of the model's parameters, making them more efficient and practical for AIOps applications.Layer-Freezing involves freezing most of the model's layers and only fine-tuning the top layers, allowing for significant reductions in computational cost and training time.Adapter Tuning introduces small, trainable adapter modules within each layer of the pre-trained model, enabling the model to adapt to new tasks with minimal effort.Task-Conditional Fine-Tuning involves adding task-specific output heads to the model's output and training them accordingly.Parameter-efficient fine-tuning is the predominant approach in the AIOps field.</p>
<p>Embedding-based Approach.Many data sources for AIOps contain rich semantic information, such as logs, documentation, etc., and embeddings play a crucial role in reflecting this semantic information in a data format.Therefore, many AIOps works are based on embeddings.These embedding-based approaches can be categorized into two types: pre-trained embedding [7,47,61,99,115,151] and prompt embedding [11,35,120].Pre-trained embedding works involve using LLMs to embed this information to capture its semantic information, while prompt embedding works design specific embedding methods suitable for LLMs to activate the LLM's processing capability for specific data.</p>
<p> Pre-Trained Embedding.Pre-trained embedding works involve directly leveraging embeddings generated by LLMs, such as BERT, GPT, or T5, to capture the semantic information of various data sources in AIOps.These embeddings are obtained from models pre-trained on large-scale text corpora and capture rich semantic information, making them suitable for a wide range of AIOps tasks without further fine-tuning.Currently, most works related to logs utilize this embedding-based approach. Prompt Embedding.Prompt embedding works focus on designing specific embedding methods tailored for LLMs to effectively capture semantic information from various data sources in AIOps.By providing natural language prompts or instructions, these methods activate the LLM's processing capability to generate task-specific embeddings.This approach enables flexible and task-specific embeddings suitable for different AIOps applications.Currently, many works based on metrics utilize this approach to transform metrics data into a format more suitable for LLM understanding.</p>
<p>Prompt-based Approach.In addition to using the embedding capabilities of large language models (LLMs), many AIOps works directly input prompts into LLMs to complete failure management tasks [3, 10, 75, 82-84, 101, 105, 136, 153].These works can be categorized into In-Context Learning (ICL), Chain of Thoughts (CoT), Task Instruction Prompting, and Knowledge-Based Approach.</p>
<p> In-Context Learning (ICL).In-Context Learning involves providing the model with examples within the input prompt to help it understand how to perform a task.This approach allows the model to generate appropriate responses based on the patterns and information presented in the provided context.Many AIOps works use this method to guide LLMs in producing results that conform to the desired output format. Chain of Thoughts (CoT).Chain of Thoughts involves guiding the model through a logical sequence of intermediate steps or reasoning processes to arrive at the final answer.This approach helps in complex problem-solving tasks where the solution requires multi-step reasoning.Many AIOps works use this method to improve the accuracy of the output. Task Instruction Prompting.Task Instruction Prompting entails giving explicit instructions to the model about the task to be performed.This method leverages the model's ability to follow detailed natural language instructions to complete specific tasks effectively.In this survey, we categorize many simple or zero-shot methods under this approach.Many early AIOps works use this method to accomplish specific tasks. Knowledge-Based Approach.This approach integrates external knowledge into the model's responses.Examples include Tool Augmented Generation (TAG), where the model uses external tools or APIs to enhance its capabilities, and Retrieval Augmented Generation (RAG), which involves retrieving relevant information from external sources to improve the model's responses.This is the most commonly used method in AIOps works and is the most effective in improving LLM performance for specific tasks.</p>
<p>AIOps Tasks for Failure Management</p>
<p>AIOps tasks for failure management comprise a holistic goal consisting of several subtasks.As shown in Figure 3, the entire process can be divided into data preprocessing, failure perception, root cause analysis, and auto remediation.These tasks are sequentially related: failure perception is based on preprocessed data, and once an anomaly is detected in the software system, root cause analysis is conducted to automatically identify where and what the anomaly is.Finally, after determining the cause of the anomaly, appropriate auto remediation methods are implemented to mitigate the software system issue.Data Preprocessing.In the context of AIOps tasks, data preprocessing does not refer to conventional processes like data scraping or cleaning.Instead, it involves tasks specific to the AIOps domain.With the integration of LLMs, these preprocessing tasks can be categorized into three types: Log Parsing [37-39, 56, 69, 88, 94, 121, 135], Metrics Imputation [17,35,129,157], and Input Summarization [18,26,34,59,66,94,155,161].</p>
<p> Log Parsing.Log parsing involves analyzing and structuring log data to extract meaningful information.This task typically includes identifying patterns, categorizing log entries, and converting unstructured log data into structured formats that are easier to analyze.For example, the log template "E0,('instruction', 'cache', 'parity', 'error', 'corrected')" can be extracted from the log message "2005-06-03-15.42.50.363779R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected".There are already many log parsing methods, based on frequent pattern mining [20,112,123,141], clustering [41,96,117,122], and heuristics [43,57,89].However, LLMs can enhance log parsing by using natural language understanding to accurately interpret and classify log data. Metrics Imputation.Metrics imputation deals with estimating missing or incomplete metrics data.This is crucial for maintaining accurate and comprehensive datasets for failure perception and other analysis.LLMs can improve metrics imputation by leveraging contextual information and patterns in the available data to make accurate estimations of missing values. Input Summarization.Input summarization focuses on condensing extensive text into concise summaries that highlight the most important information.This task helps in quickly identifying significant events or issues within the input text.Since many large models have limitations on the number of tokens that can be processed in a single query, and log data can be vast, input summarization often serves as a preliminary step for many log-based tasks.This step is typically performed using LLMs, as they can generate effective summaries by understanding the context and extracting key points from large volumes of text data.</p>
<p>Failure Perception.After completing data preprocessing, failure perception is the next crucial step in identifying deviations from normal behavior in a system.This process is essential for early detection of potential issues, enabling proactive measures to prevent failures.These approaches can be categorized into two types: Failure Prediction [5,6,133,138] and Anomaly Detection [7, 8, 11, 12, 15, 21, 22, 24, 26, 33, 35, 37, 44, 47-49, 58, 61, 62, 62, 68, 70, 75, 81-83, 94, 99, 100, 104, 106, 115, 120, 121, 136, 151, 153, 157].</p>
<p> Failure Prediction.Failure prediction involves forecasting potential system failures before they occur by analyzing historical data and identifying patterns that precede failures.This task enables proactive measures by alerting maintenance personnel to potential issues in advance, allowing for early remediation.However, there has been relatively limited work on failure prediction in the era of LLMs, as this approach often faces challenges related to data quality and the complexity of accurately predicting failures in dynamic environments. Anomaly Detection.Due to the limitations of failure prediction, most work has shifted towards anomaly detection.Anomaly detection aims to identify abnormal behavior or patterns that deviate from the norm, indicating potential issues or failures.This approach is more adaptable to the dynamic nature of software systems and is widely used in LLM-based methods.LLMs can effectively analyze large volumes of data, including logs and metrics, to detect subtle anomalies that traditional methods might miss.This capability is crucial for maintaining the reliability and availability of large-scale distributed systems, as it allows for the timely identification and resolution of issues before they escalate into critical failures.</p>
<p>Root Cause Analysis.After detecting anomalies in a software system, it is crucial to analyze which component is experiencing issues and what specific anomaly is occurring.Accurate root cause analysis can greatly assist operations personnel in the subsequent repair process.These approaches, depending on the task performed, can be categorized into three types: Failure Localization [74,108,110,111,113], Failure Category Classification [18,39,105,120,146,157,161,163], Root Cause Report Generation [2,34,108,130,155].</p>
<p> Failure Localization.This task aims to identify the specific component or machine where the anomaly occurred, often using methods such as Causal Discovery.In microservices environments, it can pinpoint the exact service or machine experiencing the issue.Additionally, it may involve identifying the specific log or metric entry that marked the onset of the anomaly.While widely used in traditional, non-LLM-based works, this type of work is less common in current LLM-based approaches. Failure Category Classification.This task identifies what type of anomaly the system is experiencing, such as CPU resource shortages, memory shortages, or software configuration errors.The advent of LLMs has enhanced the cross-platform generality of these methods and expanded their ability to classify a wider range of anomalies.Currently, this task typically uses incident reports as input. Root Cause Report Generation.This task directly generates a root cause analysis report, leveraging LLMs' powerful natural language generation capabilities.It generally combines the previous two tasks, providing a comprehensive report that details the components and reasons behind the software system's anomalies.</p>
<p>Auto Remediation.After identifying the type and location of the software system anomaly, the next step is to automatically mitigate and repair the issue based on this information.Prior to the advent of LLMs, the automation level of these remediation tasks was relatively low.However, with the introduction of LLMs, the degree of automation has seen a significant improvement.Auto remediation tasks, in increasing level of automation, can be categorized as follows: Assisted Questioning [37,84,101], Mitigation Solution Generation [2,34,40,125], Command Recommendation [116,137], Script Generation [3,55,102,110,128], Automatic Execution [10,64,98,111].</p>
<p> Assisted Questioning.This task involves using LLMs to assist operations personnel by answering system-related questions.By allowing operations staff to directly query the LLM software, detailed responses can be obtained quickly, speeding up the resolution of software system failures.This type of task emerged with the rise of LLMs, particularly GPT-3.5. Mitigation Solution Generation.In this task, LLMs generate potential mitigation solutions for the detected anomalies.These solutions are often derived from large datasets of historical incident reports and resolutions, providing actionable suggestions to operations personnel for addressing issues.Before LLMs, auto remediation for software failures primarily involved incident triage [114,143], but these methods are not intelligent.Mitigation solution generation appears more advanced and tailored. Command Recommendation.This task leverages LLMs to recommend pre-existing scripts that can be used to remediate the detected issue.The system can suggest the next command to enter when operations staff input commands (e.g., shell commands), thus shortening the repair time.This method gaines popularity with the advent of LLMs. Script Generation.This task skips the command recommendation step, directly using LLMs to generate custom scripts tailored to resolve specific detected anomalies.This involves creating new scripts based on the details of the issue and the context provided by system logs and metrics, allowing for more precise and effective remediation actions. Automatic Execution.The highest level of automation involves LLMs not only generating the necessary remediation scripts but also executing them automatically.This end-to-end process ensures that detected anomalies are addressed without manual intervention, significantly speeding up resolution time and reducing the workload on operations personnel.Although this method is highly attractive, related work is limited, and its practical effectiveness remains to be verified.</p>
<p>The survey will follow the structure outlined in Figure 3 to provide a detailed overview of LLM-based AIOps tasks for failure management.Section 3 will delve into the related work on data preprocessing.Section 4 will comprehensively discuss the work on failure perception.Section 5 will cover the efforts related to root cause analysis.Finally, Section 6 will explore the work on auto remediation.</p>
<p>DATA PREPROCESSING</p>
<p>As aforementioned, the data sources for AIOps can be categorized into two groups: system-generated data and human-generated data.Human-generated data, which come in various forms, are typically processed using natural language methods without special handling.In contrast, system-generated data are usually of fixed types and relatively uniform formats, but they tend to be larger in volume and continuously growing.Therefore, most data preprocessing methods are designed to handle system-generated data.In this survey, we categorize these preprocessing methods into three types: log parsing (Section 3.1), input summarization (Section 3.3), and metrics imputation (Section 3.2).</p>
<p>Log Parsing</p>
<p>Log parsing is fundamental to many tasks that use logs as inputs for failure perception and root cause analysis, serving as the basis for subsequent tasks.As shown in Figure 4, raw logs consist of semi-structured text encompassing various fields like timestamps and severity levels.For the benefit of downstream tasks, log parsing is employed to transform each log message into a distinct event template, which includes a constant part paired with variable parameters.For example, the log template "E0,('instruction', 'cache', 'parity', 'error', 'corrected')" can be extracted from the log message "2005-06-03-15.42.50.363779R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected" in Figure 4.After being parsed into event templates, log data can be organized into sequence groups using session, sliding, or fixed windows.Following this, failure perception and root cause analysis are performed on each event group to determine if a failure exists, and if so, to conduct the corresponding root cause analysis.</p>
<p>Next, we provide a formal description of this process.For a given log sequence  = ( 1 ,  2 , ...,   ), where   represents an individual log entry.After log parsing, each log   can be represented as an event   , and the entire collection of unique events can be denoted as  = { 1 ,  2 , ...,   }.After log grouping, the entirety of raw log messages   = ( 1 ,  2 , ...,    ) in a specific time window  can be represented as   = ( ( 1 ) ,  ( 2 ) , ...,  (   ) ).Here,   represents the length of each grouped log sequence in the time window .</p>
<p>A common issue in traditional log parsing methods is their lack of generalizability.These methods often rely on manually designed rules or are trained on limited datasets.As a result, their effectiveness significantly decreases when applied to different software systems or when there are changes in log generation rules.The emergence of powerful LLMs, which possess extensive pretrained knowledge related to code and logging, offers a promising avenue for log parsing.However, the lack of specialized log parsing capabilities currently hinders the accuracy of LLMs in this task.Additionally, the inherent inconsistencies in their responses and the substantial computational overhead prevent the practical adoption of LLM-based log parsing.Consequently, many studies have explored the effectiveness of LLMs for log parsing and have proposed methods to effectively leverage LLMs in this domain.</p>
<p>Empirical Study.Priyanka et al. [94] conducted a study on zero-shot log parsing using ChatGPT.Their findings indicated that the current version of ChatGPT has limited performance in zeroshot log processing, with issues of response inconsistency and scalability.Le et al. [69] evaluated ChatGPT's ability to perform log parsing through two research questions: the effectiveness of ChatGPT in parsing logs and its performance with different prompting methods.Their results showed that ChatGPT can achieve promising outcomes in log parsing with appropriate prompts, especially with a few-shot approach.These empirical studies demonstrate that while LLMs have the potential for log parsing, they require effective methods to guide them.</p>
<p>Prompt-based Approach.Some studies have adopted prompt-based methods to guide LLMs for effective log parsing.LILAC [56] proposes a practical log parsing framework using LLMs with an adaptive parsing cache.LILAC leverages the in-context learning (ICL) capabilities of LLMs by executing a hierarchical candidate sampling algorithm and selecting high-quality demonstrations.It uses an adaptive parsing cache to store and optimize LLM-generated templates, helping mitigate the inefficiency of LLMs by quickly retrieving previously processed log templates.LLMParser [88] employs in-context learning and few-shot tuning methods.This approach is tested on four LLMs: Flan-T5-small, Flan-T5-base, LLaMA-7B, and ChatGLM-6B, across 16 open-source systems, and find that smaller LLMs might be more effective for log parsing tasks than more complex ones.Lemur [38] introduces a novel sampling method inspired by information entropy, effectively clustering typical logs and using the Chain of Thoughts (CoT) method with LLMs to distinguish parameters from invariant tokens.DivLog [135] combines Retrieval-Augmented Generation (RAG) and ICL methods for log parsing, sampling a diverse set of offline logs as candidate logs and selecting five suitable template candidates for each target log during the parsing process.Sun et al. [121] develop a cloud-native log management platform providing log collection, transmission, storage, and system management features, utilizing GPT-3.5 for few-shot log parsing.</p>
<p>Fine-tuning Approach.Other works have employed fine-tuning methods to train pre-trained models specifically for log parsing.OWL [37] utilizes supervised fine-tuning and mixture adapter tuning methods to train a set of large language models for knowledge querying and log parsing based on the LLaMA model and their OWL-Instruct dataset.Pranjal et al. [39] applies full fine-tuning on BERT with 12 Loghub datasets [45] and 5 proprietary data sources, creating a specialized LLM named BERTOps that effectively performs multiple downstream log tasks, including log parsing.</p>
<p>Metrics Imputation</p>
<p>In metrics-based methods, a significant issue arises due to the sheer volume of metrics data-potentially thousands of indicators generating data points every second.During transmission or recording, data loss can easily occur.These losses might not necessarily indicate anomalies in the software system at the time.Therefore, it's crucial to impute these missing metrics data points, which can effectively enhance the performance of subsequent tasks.</p>
<p>The metrics imputation process can be formalized as follows.Given a time series of metrics data  = { 1 ,  2 , ...,   } over time  , where each metric   consists of a set of measurements { 1  ,  Formally, assume   = { 1  ,  2  , ...,    } and let    1, 2, ...,  be the set of indices of observed values at time .Then the imputed metrics at time  can be represented as Equation 1, where m  is the imputed value for the missing metric at index  and time .
   if   O  m  if   O (1)
The imputation process aims to minimize the difference between the true values    and the imputed values m  for all missing entries.This can be formalized as an optimization problem as Equation 2. Thus, the task of metrics imputation involves accurately estimating m  to reconstruct the complete metrics dataset  from the observed dataset   .</p>
<p>min
 *    =1  O      m  2(2)
The metrics imputation approaches can be categorized into two types based on their ability to yield varied imputations that reflect the inherent uncertainty in the imputation process [124]: predictive methods and generative methods.</p>
<p>Predictive Approach.Predictive imputation methods consistently predict deterministic values for the same missing components.Many works have proposed deep learning models for this purpose.For instance, GRU-D [14] employs RNN models, TimesNet [132] utilizes CNN models, and Saits [25] is based on attention models.Additionally, several approaches leverage large language models (LLMs).Zhou et al. [157] fine-tuned pre-trained models like GPT-2 and BERT using layer-freezing methods, while Nate et al. [35] used prompt embedding techniques with GPT-3, LLaMA-2, and GPT-4 to accomplish metrics imputation tasks.GatGPT [17] employs a graph attention network to pre-train a large language model specifically designed for metrics imputation.</p>
<p>Generative Approach.Generative methods are built upon models like VAEs, GANs, and diffusion models.There are fewer works in this category that are based on LLMs.GP-VAE [32], V-RIN [95], and supnotMIWAE [65] use VAE methods for metrics imputation, employing an encoderdecoder structure to approximate the true data distribution by maximizing the Evidence Lower Bound (ELBO) on the marginal likelihood.NAOMI [85] and USGAN [91] utilize GANs for generative metrics imputation.SSSD [4] and CSBI [16] are based on diffusion models, which capture complex data distributions by progressively adding and then reversing noise through a Markov chain of diffusion steps.</p>
<p>Input Summarization</p>
<p>Since the context space of LLMs is always limited, providing extensive data as external knowledge or associative information requires summarizing this data first.The summarized results can then be merged into a single context for submission to the LLM.</p>
<p>Input summarization is essentially an information compression process.It takes long inputs and summarizes them so that the summarized result contains as much effective information as the original input while significantly reducing the input length.</p>
<p>This process can be formalized as follows.For a given original input sequence  = ( 1 ,  2 , ...,   ), where   represents represents a segment of the input.The goal is to produce a summarized sequence , which can be represented as  = ( 1 , 2 , ...,  ), where  &lt;  and  retains the essential information from .The summarization function  can be defined as Equation 3, where  aims to minimize the loss of information, ensuring that  is a compact representation of  with maximum information retention.
 =  ()(3)
This summarized sequence  is then used as the input to the LLM, effectively providing the model with a condensed version of the original data, allowing it to process and utilize the information within the constraints of its context window.We categorize the methods into two types based on whether an LLM is used as the compression tool during the summarization process.</p>
<p>non-LLM-based Approach.This type of approach does not use LLMs as compression tools [164].In AIOps for failure management, these methods generally follow three main strategies.DY-NAICL [158], Selective Context [76], LLMLingua [53], and LongLLMLingua [54] employ prompt pruning to remove unimportant tokens, sentences, or documents from each input prompt online based on predefined or learnable importance indicators.RECOMP [134] and SemanticCompression [31] condense the original prompt into a shorter summary while preserving similar semantic information.Many metric-based methods, such as Nate et al. [35] and TEST [120], use soft prompt-based strategies to design a special prompt tailored to specific scenarios and data, which is significantly shorter than the original prompt for use as input to LLMs.</p>
<p>LLM-based Approach.This type of approach directly uses LLMs as compression tools.Priyanka et al. [94] and Chris et al. [26] employ zero-shot methods by directly inputting prompts to test ChatGPT's compression effectiveness on log data.Zhang et al. [155], Drishti et al. [34], Chen et al. [18], and Oasis [59] attempt to summarize large volumes of incident reports.Zhou et al. [163] and D-Bot [161] summarize external technical documents or code to create auxiliary knowledge bases.</p>
<p>FAILURE PERCEPTION</p>
<p>Failure perception is conducted based on the preprocessed data and serves as the most critical step in AIOps for failure management.It is used to predict or detect whether anomalies have occurred in the runtime software system, forming the foundation for subsequent root cause analysis and auto remediation.Typically, this step involves the continuous online perception using system-generated data.In this survey, we categorize the failure perception into two subtasks: failure prediction (Section 4.1) and anomaly detection (Section 4.2).</p>
<p>Failure Prediction</p>
<p>Failure prediction involves analyzing historical data to detect whether a software system is likely to experience a failure within a future time window.This process helps system operators proactively identify and address issues before they lead to actual failures.</p>
<p>Formally, given a time series of historical system data  = { 1 ,  2 , . . .,   }, the goal is to predict the probability  (  + ) of a failure  occurring within a future interval .This process can be expressed as  (  + | ), which is the predicted probability of a failure occurring at time  +  given the historical data  .</p>
<p>Although failure prediction offers invaluable foresight, empowering operations teams to proactively address impending issues, the realm of LLM-based approaches in this domain remains sparse.This scarcity stems from the inherent challenge that numerous failures evade advanced notice, resulting in methodologies that are either confined to addressing a narrow spectrum of anomalies or suffer from high rates of missed detections (false negatives).</p>
<p>Nonetheless, certain research endeavors harness LLMs to augment the efficacy of failure prediction models, albeit in a supplementary capacity.Clairvoyant [5] leverages log data to train a novel self-supervised model based on BERT to predict node failures in HPC systems.Clairvoyant can predict only failures, while Time Machine [6] builds upon it with a two-stack transformer-decoder architecture to predict not only failures but also their lead times.Yang et al. [138] focus on enhancing data quality through data imputation to improve the performance of the downstream failure prediction task based on a sample-efficient diffusion model.Xiong et al. [133] define a systematic framework based on a prompting approach with three components: prompting strategies for eliciting verbalized confidence, sampling methods for generating multiple responses, and aggregation techniques for computing consistency.They benchmark these methods on the failure prediction task using five widely-used LLMs, including GPT-4 and LLaMA2 Chat, and uncover that LLMs tend to be overconfident, potentially imitating human patterns of expressing confidence.However, employing their proposed strategies, such as human-inspired prompts, consistency among multiple responses, and better aggregation strategies, can help mitigate this overconfidence from various perspectives and achieve results close to those of fine-tuned models.</p>
<p>Anomaly Detection</p>
<p>Unlike failure prediction, anomaly detection involves analyzing historical data to determine whether anomalies exist within a specific time window.Based on the methods used for detecting anomalies, as illustrated in Figure 5, anomaly detection approaches can be categorized into three types: Prediction-Based Methods, Reconstruction-Based Methods, and Classification-Based Methods.Prediction-Based Methods.Prediction-based anomaly detection forecast future system data based on historical data and identify anomalies by comparing the predicted values with the actual observed values.Significant deviations between the prediction and the actual values are flagged as anomalies.This process can be viewed as a time-series forecasting task for metrics data and an event prediction task for logs data.However, regardless of the data type, the general process can be uniformly formalized.</p>
<p>Prediction Model Classification Model
Input
Formally, assume  = { 1 ,  2 , ...,   } represent the historical data, where   is the observed value at time .The goal is to predict the future values X = { x +1 , x +2 , ..., x + }, where  is the prediction horizon.The prediction process of each data point can be represented as Equation 4, where  = 1, 2, ..., , and  represents the prediction model (which could be pre-trained or learned from the data).
x + =  ( 1 ,  2 , ...,   + 1 )(4)
Assume the actual observed values   = {  +1 ,   +2 , ...,   + }.The predicted values X are then compared with   .An anomaly is flagged if the deviation  between X and   exceeds a predefined threshold .</p>
<p>Currently, most works under this classification are based on metrics, with only a few focusing on logs.We categorize these works according to the different LLM-based approaches they use into the following groups: foundational model, fine-tuning approach, prompt-based approach, and embedding-based approach.</p>
<p>Several works have trained foundation models specifically for time-series forecasting tasks.TimeGPT [33] utilizes an encoder-decoder structure with multiple layers to pretrain the first foundation model for time series.Lag-llama [106] employs a decoder-only transformer architecture that uses lags as covariates to pretrain a general-purpose foundation model for univariate probabilistic time series forecasting.TimesFM [22] leverages a patched-decoder style attention model to pretrain a time-series foundation model for forecasting that performs well across different forecasting history lengths, prediction lengths, and temporal granularities.TTM [28] utilizes a lightweight TSMixer architecture that incorporates innovations like adaptive patching, diverse resolution sampling, and resolution prefix tuning to handle pre-training on varied dataset resolutions with minimal model capacity.</p>
<p>Although the aforementioned works have developed foundational models for data forecasting tasks, these models often fall short in specific scenarios.Therefore, fine-tuning for particular tasks and contexts is necessary.Zhou et al. [157] utilizes Frozen Pretrained Transformer (FPT) to finetune language or computer vision models.LLM4TS [12] adopts a two-stage fine-tuning process: first conducting supervised fine-tuning to orient the LLM towards time-series data, followed by task-specific downstream fine-tuning.Khanal et al. [63] proposes a one-step fine-tuning method that incorporates a percentage of source domain data into the target domains, providing the model with diverse time series instances and fine-tuning the pre-trained model using a gradual unfreezing technique.LogFiT [7] fine-tunes a BERT-based pre-trained bidirectional encoder representation language model using log data to recognize the language patterns of normal log data.When new log data appears, the model's top-k token prediction accuracy can serve as a threshold for determining whether the new log data deviates from the normal log data.</p>
<p>Training foundational models or fine-tuning requires significant time and hardware resources.Therefore, several works have been proposed to perform predictions without retraining pretrained models.PromptCast [136] transforms numerical input and output into prompts, framing the forecasting task in a sentence-to-sentence manner.Time-LLM [58] proposes Prompt-as-Prefix (PaP), which enriches the input context and directs the transformation of reprogrammed input patches.The transformed time series patches from the LLM are then projected to obtain the forecasts.TEMPO [11] decouples complex time series into trend, seasonal, and residual components, mapping them to corresponding latent spaces to create inputs recognizable by GPT.To achieve this, TEMPO constructs a prompt pool and assigns different prompts to different decoupled components, enabling the model to adapt to changes in the time series distribution using historical information.LSTPrompt [82] decomposes the time-series forecasting task into short-term and long-term prediction sub-tasks and uses Chain-of-Thought (CoT) techniques to tailor prompts for each sub-task.Some approaches rely on the embedding capabilities of large models to perform prediction tasks, particularly in the context of log data.The embedding capabilities of these large models are effective in extracting semantic information from logs.Egil et al. [62] employs an autoencoding method to compress the embedding representation and conducts anomaly detection through self-supervised learning.Harold et al. [99] utilizes embeddings from models like BERT, GPT-2, and XLNet, chaining the embedding vectors through time in a recurrent neural network (BiLSTM) to learn normal system behavior.LogADSBERT [47] leverages the Sentence-BERT model to extract semantic features from log events, followed by the utilization of BiLSTM for log event prediction.</p>
<p>Reconstruction-Based Methods.Reconstruction-based anomaly detection utilize models to reconstruct the input data and detect anomalies by measuring the reconstruction error.If the reconstruction error exceeds a certain threshold, the data point is considered an anomaly.This approach assumes that anomalies cannot be accurately reconstructed by the model.</p>
<p>Formally, assume  = { 1 ,  2 , ...,   } represent the input data, where   is the observed value at time .The reconstruction-based method uses a model  to generate a reconstructed sequence X = { x1 , x2 , ..., x }, where x = (  ).</p>
<p>Then the reconstruction error  is calculated as the difference between the observed value  and the reconstructed value X , If the reconstruction error  exceeds a predefined threshold , the time window  is flagged as having an anomaly.</p>
<p>Reconstruction-based anomaly detection methods are more commonly used with small-scale models, but they are relatively rare in scenarios involving large language models (LLMs).Currently, existing LLM-related methods are primarily based on masked modeling techniques.LANoBERT [70] uses a BERT model to learn through masked language modeling and performs unsupervised anomaly detection during testing by calculating the masked language modeling loss function for each log key.Prog-BERT-LSTM [115] proposes a progressive masking strategy to aggregate the text semantic vector and sequence feature vector.SimMTM [24] pre-trains deep models based on metrics data by learning to reconstruct the masked content based on the unmasked part, recovering masked time points by the weighted aggregation of multiple neighbors outside the manifold.This approach eases the reconstruction task by assembling ruined but complementary temporal variations from multiple masked series.</p>
<p>Classification-Based Methods.Classification-based anomaly detection classify input data as normal or anomalous based on predefined labels or learned features.Classification-based methods require a labeled dataset for training and typically use supervised learning algorithms to distinguish between normal and anomalous behavior.</p>
<p>Formally, assume that  = { 1 ,  2 , ...,   } represent the input data for a given time window  , and   {0, 1} represents the corresponding label for that time window (0 for normal and 1 for anomalous).The classification process can be represented as Equation 5, where  is the classification model.
 = ( )(5)
During the training phase, the model  is learned using the labeled dataset (, ).In the detection phase, the model  classifies each time window  into normal or anomalous, and if ( ) = 1, the time window  is flagged as an anomaly.Based on the model responsible for the final decision and classification, we categorize classification-based anomaly detection into two types: LLM-assisted classification and LLM-decision classification.</p>
<p>LLM-assisted classification methods utilize large language models to assist smaller-scale models in making final decisions.LLMs provide embeddings or feature extractions, which are then used by smaller, specialized models to classify anomalies.</p>
<p>These works primarily focus on log-based anomaly detection, leveraging LLM embeddings to capture semantic information from log data.RobustLog [156] was one of the earliest to use a pretrained model (FastText algorithm) to extract semantic information from log events and represent them as semantic vectors, eventually using an attention-based Bi-LSTM model for classification.Building on this, PLELog [139] used a similar semantic information extraction algorithm as RobustLog and transformed the classification-based algorithm into a semi-supervised one.NeuralLog [68] employed Word2Vec for semantic information transformation and eliminated log parsing to address errors caused by parsing mistakes.Although RobustLog, PLELog, and NeuralLog considered extracting semantic information from logs, the models and algorithms they used for semantic extraction cannot be classified as LLMs.</p>
<p>Building on these, Egil et al. [61] used BERT for semantic information extraction, demonstrating that using LLMs for semantic extraction yields better detection results.MultiLog [60,[148][149][150] used BERT to extract semantic information from each node in distributed software and performed semantic compression, ultimately using an AutoEncoder at the coordinator node for fusion and classification.LogST [151] used SBERT to extract the semantics of log events and finally employed a GRU model for anomaly detection.Ji et al. [51] combined SBERT for extracting log embedding information with GPT-2 for semantic transformation, ultimately using an alarm strategy layer for anomaly detection.</p>
<p>LLM-decision classification methods utilize large language models directly for decision-making.The LLMs are fine-tuned or prompted to classify input data as normal or anomalous without relying on smaller models for the final classification.These methods can be divided into two categories: training-based methods and prompt-based methods.</p>
<p>Several training-based works utilize BERT and its variants, predominantly focusing on log data.BERT-Log [15] employs a pretrained language model to learn the semantic representation of normal and anomalous logs and fine-tunes the BERT model using a fully connected neural network to detect anomalies.LogPrompt [153] leverages prompts to guide the pretrained language model to better understand the semantic and sequential information of logs, avoiding the need to train a model from scratch.HilBERT [49] uses log template information from 17 log datasets on loghub [45] for full fine-tuning in anomaly detection tasks.To reduce the high cost of full fine-tuning, LogBP-LORA [44] proposes a parameter-efficient log anomaly detection scheme based on BERT and Low-Rank Adaptation (LoRA), which introduces bypass weight matrices and updates only bypass parameters instead of all original parameters.</p>
<p>There are also some works utilize metrics data for fine-tuning.TS-Bert [21] performs full finetuning on BERT using metrics data.Zhou et al. [157] fine-tune pre-trained models like GPT-2 and BERT using layer-freezing methods.Additionally, some works use knowledge distillation to learn anomaly detection knowledge from LLMs.AnomalyLLM [81] trains a student model for metrics-based anomaly detection using GPT-2 as the teacher model.</p>
<p>However, more novel approaches avoid training models and instead leverage prompt engineering to guide LLMs in performing classification tasks.As large language models grow in scale, it is often believed that they inherently possess anomaly detection capabilities, which can be harnessed through effective prompts.Some methods simply input prompts to the LLM to determine the presence of anomalies.</p>
<p>Many works focus on log data.Priyanka et al. [94] conduct a study on zero-shot log-based anomaly detection using ChatGPT, indicating that the current version of ChatGPT has limited performance in this task.Chris et al. [26] not only prompt ChatGPT to detect anomalies in log segments but also employ input summary techniques combined with historical data for a more comprehensive assessment, demonstrating better results.RAGLog [100] enhances LLM-based anomaly detection by integrating retrieval-augmented generation techniques with vector databases, improving detection accuracy.LogGPT [104] employs chain-of-thought (CoT) prompting to strengthen the LLM's performance in anomaly detection, enabling a more nuanced and effective analysis.</p>
<p>Unlike log data-based approaches, metrics-based works often require special prompt embedding for metrics data.TEST [120] first tokenizes metrics data, builds an encoder to embed them by instance-wise, feature-wise, and text-prototype-aligned contrast, and then creates prompts to make the LLM more receptive to embeddings, finally implementing metrics-based classification.Sigllm [8] employs a time-series-to-text conversion module, as well as end-to-end pipelines that prompt language models to perform time series anomaly detection, directly asking the language model to indicate which elements of the input are anomalies.TabLLM [46] prompts large language models with a serialization of the metrics data into a natural-language string for few-shot and zero-shot metrics data classification.</p>
<p>ROOT CAUSE ANALYSIS</p>
<p>Once an anomaly is detected in the software system, it is necessary to further locate and classify the anomaly.Depending on the specific tasks performed, root cause analysis can be categorized into failure localization (Section 5.1), failure category classification (Section 5.2), and root cause report generation (Section 5.3).As illustrated in Figure 6, before the rise of large language models, these tasks typically relied on system-generated data and utilized automated failure perception methods to identify anomalies, which was then used for failure localization and failure category classification.However, with the advent of large language models, the starting point for root cause analysis has shifted from automated failure perception to user-generated data, particularly incident reports which has introduced the ability to analyze natural language.Additionally, root cause analysis now incorporates other human-generated data, such as documentation and code, as supplementary knowledge sources to enhance the analysis.Furthermore, based on the natural language understanding and generation capabilities of LLMs, it is now possible to bypass the processes of failure localization and failure category classification, and directly generate root cause reports.</p>
<p>Failure Localization</p>
<p>Failure localization aims to identify the specific component or machine where the anomaly occurred, often using methods such as causal discovery.In microservices environments, it can pinpoint the exact service or machine experiencing the issue.Additionally, it may involve identifying the specific log or metric entry that marked the onset of the anomaly.While widely used in traditional, non-LLM-based works, this type of approach is less common in current LLM-based methods.</p>
<p>Formally, assume  = { 1 ,  2 , ...,   } represent the set of all components or services in the system, and let  = { 1 ,  2 , ...,   } represent the set of log entries or metric data points.The goal of failure localization is to identify the subset     and     where the anomalies are most likely to have originated.This process can be represented as Equation 6, where  represents the observed anomalies, and  (  ,   | ) is the probability that the components in   and log entries or metrics in   are responsible for the observed anomalies.
(  ,   ) = arg max   ,   (  ,   | )(6)
In the era of large language models (LLMs), this type of work is relatively rare, and the identified failure types to be located are more diverse.Komal et al. [110,111] leverage zero-shot prompt engineering approaches based on metrics data to identify problematic host names, and deployment names.Devjeet et al. [108] utilize incident reports with React [140] for root cause analysis in an out-of-domain setting on a static dataset of real-world production incidents, ultimately identifying the problematic task names.RealTCD [74] leverages domain knowledge to discover temporal causal relationships for root cause analysis without interventional targets, introducing LLM-guided metainitialization to extract meta-knowledge from textual information hidden in systems to enhance discovery quality.LogConfigLocalizer [113] proposes an LLM-based two-stage strategy for endusers to localize root-cause configuration properties based on logs.mABC [154] utilizes alert data (a processed form of metrics data) and employs a multi-agent blockchain-inspired collaboration to identify the specific node where the anomaly occurred.</p>
<p>Failure Category Classification</p>
<p>This approach identifies the type of anomaly the system is experiencing, such as CPU resource shortages, memory shortages, or software configuration errors.The advent of large language models (LLMs) has enhanced the cross-platform generality of these methods and expanded their ability to classify a wider range of anomalies.</p>
<p>Formally, assume  = { 1 ,  2 , ...,   } represents the set of input data items, where each input item   describes an observed anomaly.The goal of anomaly classification is to assign each input item   a label   from a predefined set of anomaly classes  = { 1 ,  2 , ...,   }.This process can be represented as Equation 6, where  is a classification function that maps each input item   to an anomaly class   .
 :    such that  (  ) =  (7)
In the era of large language models, failure category classification methods can be divided into two categories: Fine-Tuning-Based Methods and Prompt-Based Methods.</p>
<p>Fine-Tuning-Based Methods.A considerable number of failure category classification methods are based on fine-tuning and rely on system-generated data.BERTOps [39] applies full fine-tuning on BERT using log data from 12 Loghub datasets [45] and 5 proprietary data sources, achieving state-of-the-art results in failure category classification.Zhou et al. [157] fine-tune pre-trained models like GPT-2 and BERT using layer-freezing methods based on metrics data, effectively performing failure category classification tasks.</p>
<p>Prompt-Based Methods.Due to the enhanced natural language understanding capabilities of larger pre-trained models, prompt-based methods have become increasingly prevalent and have even become the mainstream approach for failure category classification.Some of these methods still primarily rely on system-generated data.Andres [105] uses large language models as a message classifier to perform failure category classification on log data.TEST [120] builds an encoder based on metrics data to embed them by instance-wise, feature-wise, and text-prototype-aligned contrast, and then creates prompts to make the LLM more receptive to embeddings, achieving excellent results in failure category classification.</p>
<p>Other methods incorporate both system-generated data and human-generated data.D-Bot [161] and LLMDB [163] use metrics data as a foundation and employ knowledge augmentation techniques (including retrieval-augmented generation and tool-augmented generation).They also integrate human-generated data such as technical manuals, maintenance documents, and API descriptions as additional knowledge sources to classify database failures.</p>
<p>Additionally, some methods leverage mainly on human-generated data, especially incident reports, for classification.PACE [146] utilizes a two-stage approach: initially, it evaluates its confidence based on historical incident data using vector retrieval methods, considering its assessment of the evidence strength.Subsequently, it uses an LLM to review the root cause generated by the predictor.RCACopilot [18] synthesizes logs, metrics, and trace data to generate corresponding incident reports and employs a combination of retrieval-augmented generation (RAG) and in-context learning to predict the anomaly's root cause category, providing an explanatory narrative.</p>
<p>Root Cause Report Generation</p>
<p>With the enhanced natural language generation and reasoning capabilities of large language models, cutting-edge research is moving beyond isolated tasks of failure location and failure category classification.Instead, there is a growing focus on creating comprehensive and more easily understandable root cause reports.These reports not only include information on failure location and failure category classification but also provide detailed reasoning about the cause of the failure, which significantly aids system maintenance personnel in analyzing and resolving issues.</p>
<p>Toufique et al. [2] conduct the first large-scale empirical study to evaluate the effectiveness of large language models (LLMs) in assisting engineers with root cause analysis and mitigation of production incidents.Their study, performed at Microsoft, involves over 40,000 incident reports and compares several LLMs in zero-shot, fine-tuned, and multi-task settings using semantic and lexical metrics.They utilize various natural language generation evaluation metrics (e.g., BLEU-4, NUBIA) to evaluate the generated root cause reports, demonstrating the efficacy and future potential of using LLMs for resolving cloud incidents.</p>
<p>Building on the study results of Toufique et al. [2], many subsequent works have focused on optimizing LLM-generated root cause reports using various prompt engineering techniques.Zhang et al. [155] propose an in-context learning approach based on GPT-4, which eliminates the need for fine-tuning.Drishti et al. [34] leverage data from different stages of the software development lifecycle (SDLC) and use a method called In-Context Examples Retrieval, a retrievalaugmented generation (RAG) approach, to enhance the LLM's ability to generate root cause reports.RCAgent [130] employs tool-augmented techniques to improve LLM-generated root cause reports, incorporating a unique Self-Consistency for action trajectories, as well as a suite of methods for context management, stabilization, and importing domain knowledge.</p>
<p>AUTO REMEDIATION</p>
<p>Once the root cause of the software failure has been identified, the next step is remediation.In the process of software maintenance, there are four main entities involved: the software system, failure incident, on-call engineer (OCE), and large language model (LLM).During the operation of the software system, failure incidents may occur.OCEs and LLMs collaborate to analyze these failure incidents and ultimately complete the remediation.</p>
<p>As shown in Figure 7, this survey categorizes auto remediation approaches into five types based on the level of automation from low to high: Assisted Questioning(Section 6.1), Mitigation Solution  Generation(Section 6.2), Command Recommendation(Section 6.3), Script Generation(Section 6.4), and Automatic Execution(Section 6.5).</p>
<p>Assisted Questioning</p>
<p>Assisted questioning methods for auto remediation have the lowest level of automation but offer the most flexibility.These methods function as specialized question-answering models in the AIOps domain, allowing OCEs to pose any questions to the model, which then provides specialized responses.The model can even utilizes tools to query the software system for necessary metrics.This approach naturally aligns with the natural language processing and reasoning capabilities of large language models.Currently, these methods can be divided into two categories: Fine-Tuning-Based Methods and Prompt-Based Methods.Fine-Tuning-Based Methods.OWL [37] utilizes supervised fine-tuning and mixture adapter tuning methods to train a set of large language models for knowledge querying based on the LLaMA model and their OWL-Instruct dataset.Gijun et al. [101] propose an interactive AI assistant to aid IT operators in managing deployments.This assistant offers insights into microservice behavior, performance bottlenecks, and preemptive issue resolution, ensuring smooth operations based on a fine-tuned kullm-polyglot-12.8b-v2model.</p>
<p>Prompt-Based Methods.DB-GPT [137] integrates large language models with traditional database systems to enhance user experience and accessibility.It is designed to understand natural language queries and provide context-aware responses.DB-GPT includes a novel retrieval-augmented generation (RAG) knowledge system, an adaptive learning mechanism that continuously improves performance based on user feedback, and a service-oriented multi-model framework (SMMF) with powerful data-driven agents.OpsEval [84] presents a comprehensive task-oriented AIOps benchmark designed for LLMs, demonstrating how various LLM techniques-such as zero-shot, chain-of-thought, and few-shot in-context learning-can affect the performance of AIOps.</p>
<p>Mitigation Solution Generation</p>
<p>Mitigation solution generation has a higher level of automation compared to assisted questioning.These methods use large language models to analyze failure incidents and ultimately produce mitigation solutions for OCEs to reference in remediating the software system.This step is often linked with root cause report generation; that is, while generating the root cause report, the LLM can also directly generate the mitigation solution.</p>
<p>As mentioned in Section 5.3, Toufique et al. [2] conduct the first large-scale empirical study to evaluate the effectiveness of large language models (LLMs) in assisting engineers with root cause analysis and mitigation of production incidents.Drishti et al. [34] leverage data from different stages of the software development lifecycle and utilize an in-context examples retrieval and retrieval-augmented generation approach to enhance the LLM's ability to generate mitigation solutions.Pouya et al. [40] propose a framework analogous to an OCE's natural thought process, which includes three modules as LLM agents: hypothesis former, hypothesis tester, and mitigation planner.Wang et al. [125] propose a control policy generation framework based on a language model for network traffic (NetLM) to refine intent incrementally through different abstraction levels.</p>
<p>Command Recommendation</p>
<p>In mitigation solution generation, the next step is to create corresponding remediation scripts based on the root cause.The first step is to assist OCEs by recommending commands.This commonly involves two approaches: OCEs input the previous command, and the system recommends the next command; or OCEs input the task to be completed, and the system recommends a command that can accomplish that task.</p>
<p>ShellGPT [116] belongs to the first approach.It is based on the GPT series models, trained on a corpus that aligns shell language with natural language.It is fine-tuned for command recommendation tasks related to shell language understanding, involving the recommendation of the most appropriate shell command given a sequence of commands.DB-GPT [137] falls under the second approach.It employs a retrieval-augmented generation method to understand natural language queries, provide context-aware responses, and generate precise and complex SQL queries.</p>
<p>Script Generation</p>
<p>The aforementioned command recommendation methods still require OCEs to write some commands or input natural language instructions, which involving a certain level of human effort.Therefore, many works have begun to directly research the generation of complete remediation scripts for OCEs to execute.</p>
<p>Xpert [55] leverages historical incident data and large language models to generate customized Kusto Query Language (KQL) queries for incident management at Microsoft.The framework efficiently extracts common patterns, such as tables and templates, from historical incidents, facilitating effective automation.To address the limitations of traditional natural language processing (NLP) metrics in evaluating domain-specific queries, Xpert incorporates a novel performance metric called Xcore.This tailored metric allows for more comprehensive evaluation from three different perspectives, enhancing the overall quality assessment of the generated KQL queries.Komal et al. [110,111] is another representative example.They claim to leverage zero-shot prompt engineering approaches based on metrics data to identify root causes.Based on these root causes, they emphasize the value and significance of using code-generation models (including LLaMA, Codex, and GPT-series models) for auto-remediation in self-adaptive microservice architectures (SMA).</p>
<p>A significant body of research has also concentrated on script generation for failure remediation from the perspective of service composition.Pioneering efforts by Marco et al. [3] are among the first to investigate the potential of LLMs in program generation and its implications for service-oriented computing.Complementary to this, Pesl et al. [102] conduct an empirical study identifying six salient scenarios of service compositions from extant literature, experimenting with ChatGPT and GPT-4 as notable implementations of LLM technology.ChatOps4Msa [128], another innovative contribution, harnesses GPT-3.5 to devise a ChatOps mechanism customized for microservices architectures.This mechanism employs ChatOps Query Language (CQL), leveraging a low-code syntax proposed in their research, to compose configuration files.Consequently, it generates complete script code that facilitates service restoration, thereby empowering OCEs in their maintenance tasks.</p>
<p>Automatic Execution</p>
<p>Whether it involves command recommendation or script generation, these methods typically require OCEs to manually execute the generated scripts.The ideal AIOps solution would encompass the entire process: automatic analysis, script generation, and automatic execution, thereby eliminating the need for human intervention in software system maintenance.There have been some pioneering efforts in this direction leveraging large language models to achieve a fully automated maintenance workflow.</p>
<p>Charles et al. [10] delve into the application of LLM-driven AI agents for automating server administration chores in Linux ecosystems.Their empirical investigation showcases a GPT-based AI agent competently performing 150 distinct tasks spanning nine categories, from file manipulation to programming compilations, illustrating the agent's aptitude for autonomous task execution and adaptability to feedback.This study underscores the potential of LLMs in democratizing intricate server management tasks for users with diverse technical proficiencies.Pitikorn et al. [64], meanwhile, put forward solutions integrating generative AI for failure perception, code generation, debugging, and the automatic generation of reports within self-healing systems.Notably, their work also capitalizes on GPT-4 to devise a comprehensive and efficacious Python code completion mechanism.This enhancement not only bolsters the functionality of backend systems but also facilitates the repair of malfunctioning components, further demonstrating the versatility of advanced LLMs in IT infrastructure maintenance and optimization.</p>
<p>CHALLENGES AND FUTURE DIRECTIONS</p>
<p>Although detailed introductions to the various LLM-based methods in AIOps for failure management have been provided in previous sections, numerous challenges remain in this field.These include computational efficiency and cost in the utilization of LLMs, in-depth usage of diverse data sources, generalizability and model adaptability during software evolution, and hallucinations in LLMs.These challenges will be discussed in detail below.</p>
<p>Computational Efficiency and Cost</p>
<p>Large language models require substantial computational resources and energy, leading to high operational costs.The training and deployment of LLMs involve extensive use of GPUs or TPUs, making them less accessible for many organizations.Additionally, the inference costs associated with LLMs can be prohibitive, especially when real-time or near-real-time responses are required.This poses significant hurdles, particularly for small to medium-sized enterprises or in scenarios where computational resources are constrained.</p>
<p>Among the tasks of AIOps for failure management, the computational overhead of LLMs is most critical for data preprocessing and failure perception tasks.These steps theoretically need to run continuously during the operation of a software system and require high real-time performance.For example, if the failure perception time window is set to 10 seconds, meaning an failure perception process is triggered every 10 seconds, the failure perception model must infer results within 1 second to promptly notify OCEs in case of anomalies.Some non-LLM-based approaches have focused on addressing this issue [52,80], but currently, no LLM-based work has adequately tackled this problem.However, this issue is becoming increasingly significant in the LLM era.</p>
<p>On the other hand, while the higher computational efficiency and cost associated with LLMs may be acceptable for root cause analysis and auto remediation (since these steps do not need to run continuously within the software system), this issue still requires careful consideration.Excessive computational costs could potentially make LLM-based approaches less effective than using small-scale models in combination with OCEs.For example, a well-designed smaller model augmented by human expertise might achieve comparable results at a fraction of the cost and resource consumption, making it a more practical and scalable solution in certain contexts.</p>
<p>In summary, balancing computational cost and performance is crucial for the application of LLMs in AIOps.If the cost of running LLMs outweighs their benefits, organizations may find it more advantageous to leverage traditional models or hybrid approaches that combine the strengths of smaller-scale models and human oversight.Therefore, future research should focus on optimizing the computational efficiency of LLMs for AIOps, possibly by exploring methods that organically combine small-scale models, large language models, and OCEs.This integrated approach could provide a cost-effective and scalable solution while maximizing the strengths of each component.</p>
<p>In-depth Usage of More Diverse Data Sources</p>
<p>Several important data sources have yet to be utilized in LLM-based approaches for AIOps.As illustrated in Section 2.1, there are three crucial types of system-generated data in software system runtime: metrics, logs, and traces.While there has been significant progress in utilizing metrics and logs, no current work has effectively incorporated trace data.</p>
<p>Trace data is particularly valuable as it provides insights into the calls between nodes in a software system cluster and the interactions within internal components.The lack of LLM-based approaches using trace data can be attributed to its high complexity and volume, making it challenging to integrate trace data with LLMs.Traces often involve detailed and nested sequences of events that need to be represented in a manner that LLMs can effectively understand and utilize.However, trace data remains a crucial resource for comprehensive system monitoring and failure management.Future research should focus on developing methods to effectively integrate trace data into LLMbased AIOps frameworks.</p>
<p>On the other hand, while many failure perception works have been based on logs, a significant portion of these works utilize pre-trained models like BERT, which are not particularly large in scale [15,44,45,49,153].These approach result in relatively high training costs.Only a small fraction of works have utilized larger models such as GPT-3.5, but these studies are primarily based on simple datasets [26,100,104].For these datasets, failure perception can often be achieved with traditional machine learning models [67], which diminishes the perceived superiority of LLM-based methods.Therefore, future research should explore more effective ways to apply logs to LLM-based approaches.One feasible solution could be prompt embedding, which has been widely used in metrics-based work.</p>
<p>Lastly, in LLM-based root cause analysis, many works have used incident reports as the primary data source and achieved excellent results.However, this approach disrupts the automation flow of AIOps for failure management, as root cause analysis should be triggered by failure perception.Therefore, future work should focus on using system-generated data to trigger failure perception, generate corresponding incident reports, and then apply root cause analysis methods to analyze these generated incident reports.</p>
<p>Generalizability and Model Adaptability in Software Evolution</p>
<p>A prominent challenge confronted in conventional ML-based and DL-based AIOps approaches for failure management revolves around the severe limitation of model performance degradation when applied to different software systems or following modifications to the original system.Numerous efforts have attempted to mitigate this issue by leveraging techniques such as metalearning and online learning, which aim to retrain models effectively with minimal new data, thereby rejuvenating their performance.</p>
<p>In the context of LLM-based AIOps methodologies, which primarily harness the inferential capabilities of large language models, there is an inherent expectation that these approaches should naturally exhibit a high degree of generalizability across platforms and model adaptability amidst software changes.The reasoning behind this expectation lies in the extensive pre-training of LLMs on diverse textual data, theoretically enabling them to understand and adapt to a wide range of software contexts without requiring substantial retraining.</p>
<p>However, despite this theoretical promise, current research in the field reveals a gap in empirical validation.While some studies that have established foundational models have indeed demonstrated cross-platform effectiveness, the majority of works, especially prompt-based work, have not systematically assessed the generalizability and adaptability of their LLM-based AIOps approaches in real-world scenarios of software evolution.This oversight points to a crucial area for future research.</p>
<p>Therefore, future endeavors in LLM-based AIOps must prioritize rigorous testing and validation of these models' ability to generalize across different software systems and their resilience to changes within a system.This could involve designing experiments that systematically introduce variations in software environments and measuring how well the models maintain or regain their efficacy post-change, possibly through incremental fine-tuning strategies or continuous learning paradigms.Moreover, exploring the synergy between advancements in LLM architectures, transfer learning methodologies, and adaptive learning algorithms will be pivotal in realizing the full potential of LLMs in ensuring robust and adaptable AIOps solutions.</p>
<p>Hallucinations in LLMs</p>
<p>A prevalent issue encountered in LLM methodologies, despite their potent inferential prowess, is their propensity to generate inaccurate or deceptive content.This phenomenon, known as hallucination, undermines the reliability of model outputs, since LLMs can generate seemingly plausible yet entirely fabricated data or trends.Such inaccuracies can lead to misinformed decisions and flawed analyses, posing significant risks in operational and strategic planning.</p>
<p>Currently, there are no dedicated efforts within LLM-based AIOps research addressing this critical problem.The implications of hallucination in AIOps are significant, as erroneous insights can exacerbate system issues rather than resolve them.For instance, in failure perception or root cause analysis, hallucinated outputs could mislead OCEs, resulting in incorrect diagnostics and remediation strategies that could potentially disrupt system operations further.</p>
<p>Addressing hallucination requires rigorous measures to identify, quantify, and mitigate these inaccuracies.This may entail developing specialized evaluation metrics tailored to detect inconsistencies and contradictions in LLM-generated outputs.Advanced techniques such as fact-checking integration, adversarial training to challenge model outputs, and reinforcement learning strategies that incentivize factual correctness are potential solutions.Additionally, incorporating human oversight and validation within LLM workflows can further enhance the reliability of outputs and mitigate the risks associated with hallucinations.</p>
<p>CONCLUSION</p>
<p>This survey thoroughly delves into the landscape of AIOps for failure management in the era of large language models (LLMs), encompassing a detailed definition of AIOps tasks for failure management, the data sources for AIOps, and the LLM-based approaches adopted for AIOps.Additionally, this survey explores the AIOps subtasks, the specific LLM-based approaches suitable for different AIOps subtasks, and the challenges and future directions of the domain.</p>
<p>Despite the progress made, several areas require further exploration.These include computational efficiency and cost in the utilization of LLMs, in-depth usage of diverse data sources, generalizability and model adaptability during software evolution, and hallucinations in LLMs.</p>
<p>Future research must concentrate on achieving a optimal balance between computational cost and performance in applying LLMs to AIOps tasks.Expanding the exploration of unique data source attributes and devising sophisticated methods for their integration into LLMs is central to enhancing model performance and adaptability.Additionally, prioritizing rigorous testing and validation to assess models' cross-platform generalizability and their resilience against software changes is of utmost importance.Lastly, the development of specialized metrics to identify and rectify hallucinations within LLM-generated outputs for AIOps applications is imperative.</p>
<p>In conclusion, the realm of AIOps for failure management in the era of LLMs is a dynamically advancing field teeming with both promise and obstacles.Its progression is vital to underpinning the stability and reliability of software systems.By tackling the challenges outlined in the survey, LLMpowered AIOps methodologies for failure management are poised to see broader, more impactful real-world implementation, thereby reinforcing the resilience and efficiency of modern software ecosystems.</p>
<p>Fig. 2 .
2
Fig. 2. LLM-based Approaches for AIOps</p>
<p>Fig. 5 .
5
Fig. 5. Workflow of Various Anomaly Detection Approaches: Prediction-based, Reconstruction-based, and Classification-based</p>
<p>Fig. 6 .
6
Fig. 6.Evolution of Root Cause Analysis with the Rise of LLMs</p>
<p>Fig. 7 .
7
Fig. 7. Various Types of Auto Remediation Approaches</p>
<p>Table 1 .
1
State-of-art survey related to AIOps for failure management (FM)
ReferenceYearScope of AIOps tasksScope of FM tasksLLM-basedFailure PerceptionPaolop et al. [97] 2021Failure ManagementRoot Cause AnalysisRemediationJosu et al. [23]2023Challenges, Architectures, Future FieldsAnomaly Detection Root Cause Analysis Auto RemediationRequirement EngineeringAngela et al. [30] 2023Design &amp; Planning Code &amp; Testing-Maintainance &amp; DeploymentIncident DetectionQian et al. [19]2023Failure ManagementFailure Prediction Root Cause AnalysisAutomated ActionsCode-related tasksRequirements &amp; DesignZhang et al. [152] 2023Software Development-Software TestingSoftware MaintenanceIncident ReportingYoucef et al. [107] 2024Incident ManagementIncident Triage Incident DiagnosisIncident MitigationWei et al. [131]2024Failure ManagementAnomaly DetectionJing et al. [118]2024Failure ManagementTime-series Forecasting Anomaly DetectionData PreprocessingOur work-Failure ManagementFailure Perception Root Cause AnalysisAuto Remediation</p>
<p>Survey of AIOps for Failure Management in the Era of Large Language Models
111:5MetricsLogsDocumentConfigCodeArchitectureSystem-generated DataTraceSoftwareQuestionInformation&amp; AnswerData SourceHuman-Incidentfor AIOpsgenerated DataReportFig. 1. Data Source of AIOps for Failure Management
J. ACM, Vol.37, No. 4, Article 111.Publication date: August 2024.A</p>
<p>Survey of AIOps for Failure Management in the Era of Large Language Models
111:132  , ...,    } for  different indicators, metrics imputation involves filling in the missing valuesin this data. Let   = {
 1 ,   2 , ...,    } be the observed metrics data with missing values.For each time step ,    includes both observed measurements and missing measurements.The goal is to estimate the missing values    such that the imputed data  * = { * 1 ,  * 2 , ...,  *  } approximates the original complete data .J. ACM, Vol.37, No. 4, Article 111.Publication date: August 2024.A</p>
<p>J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.
ACKNOWLEDGMENTSThis work was supported by the National Key R&amp;D Research Fund of China (2021YFF0704202).
Anomaly detection, localization and classification using drifting synchrophasor data streams. Arman Ahmed, Anurag Sadanandan Sajan, Yinghui Srivastava, Wu, IEEE Transactions on Smart Grid. 122021. 2021</p>
<p>Recommending root-cause and mitigation steps for cloud incidents using large language models. Toufique Ahmed, Supriyo Ghosh, Chetan Bansal, Thomas Zimmermann, Xuchao Zhang, Saravan Rajmohan, 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE2023</p>
<p>Service composition in the ChatGPT era. Marco Aiello, Ilche Georgievski, Service Oriented Computing and Applications. 172023. 2023</p>
<p>Diffusion-based time series imputation and forecasting with structured state space models. Juan Miguel, Lopez Alcaraz, Nils Strodthoff, arXiv:2208.093992022. 2022arXiv preprint</p>
<p>Clairvoyant: a log-based transformerdecoder for failure prediction in large-scale systems. Khalid Ayedh Alharthi, Arshad Jhumka, Sheng Di, Franck Cappello, Proceedings of the 36th ACM International Conference on Supercomputing. the 36th ACM International Conference on Supercomputing2022</p>
<p>Time machine: generative real-time model for failure (and lead time) prediction in hpc systems. Arshad Khalid Ayed Alharthi, Sheng Jhumka, Lin Di, Franck Gui, Simon Cappello, Mcintosh-Smith, 2023 53rd Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN). IEEE2023</p>
<p>LogFiT: Log anomaly detection using fine-tuned language models. Crispin Almodovar, Fariza Sabrina, Sarvnaz Karimi, Salahuddin Azad, IEEE Transactions on Network and Service Management. 2024. 2024</p>
<p>Large language models can be zero-shot anomaly detectors for time series?. Sarah Alnegheimish, Linh Nguyen, Laure Berti-Equille, Kalyan Veeramachaneni, arXiv:2405.147552024. 2024arXiv preprint</p>
<p>Exploring Large Language Models for Low-Resource IT Information Extraction. Bhavya Bhavya, Paulina Toro Isaza, Yu Deng, Michael Nidd, Amar Prakash Azad, Larisa Shwartz, Chengxiang Zhai, Conference on Data Mining Workshops (ICDMW). IEEE2023. 2023. August 202437Publication date</p>
<p>. Charles Cao, Feiyi Wang, Lisa Lindley, Zejiang Wang, n.d.</p>
<p>Managing Linux Servers with Llm-Based Ai Agents: An Empirical Evaluation with Gpt4. Available at SSRN. 4741492</p>
<p>Tempo: Prompt-based generative pre-trained transformer for time series forecasting. Defu Cao, Furong Jia, Tomas Sercan O Arik, Yixiang Pfister, Wen Zheng, Yan Ye, Liu, arXiv:2310.049482023. 2023arXiv preprint</p>
<p>Ching Chang, Wen-Chih Peng, Tien-Fu Chen, arXiv:2308.08469Llm4ts: Two-stage fine-tuning for time-series forecasting with pre-trained llms. 2023. 2023arXiv preprint</p>
<p>A survey on evaluation of large language models. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, ACM Transactions on Intelligent Systems and Technology. 152024. 2024</p>
<p>Recurrent neural networks for multivariate time series with missing values. Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, Yan Liu, Scientific reports. 860852018. 2018</p>
<p>Bert-log: Anomaly detection for system logs based on pre-trained language model. Song Chen, Hai Liao, Applied Artificial Intelligence. 3621456422022. 2022</p>
<p>Provably convergent Schrdinger bridge with applications to probabilistic time series imputation. Yu Chen, Wei Deng, Shikai Fang, Fengpei Li, Nicole Tianjiao Yang, Yikai Zhang, Kashif Rasul, Shandian Zhe, Anderson Schneider, Yuriy Nevmyvaka, International Conference on Machine Learning. PMLR2023</p>
<p>Gatgpt: A pre-trained large language model with graph attention network for spatiotemporal imputation. Yakun Chen, Xianzhi Wang, Guandong Xu, arXiv:2311.143322023. 2023arXiv preprint</p>
<p>Automatic root cause analysis via large language models for cloud incidents. Yinfang Chen, Huaibing Xie, Minghua Ma, Yu Kang, Xin Gao, Liu Shi, Yunjie Cao, Xuedong Gao, Ming Hao Fan, Wen, Proceedings of the Nineteenth European Conference on Computer Systems. the Nineteenth European Conference on Computer Systems2024</p>
<p>Qian Cheng, Doyen Sahoo, Amrita Saha, Wenzhuo Yang, Chenghao Liu, Gerald Woo, Manpreet Singh, Silvio Saverese, Steven Ch Hoi, arXiv:2304.04661Ai for it operations (aiops) on cloud platforms: Reviews, opportunities and challenges. 2023. 2023arXiv preprint</p>
<p>Logram: Efficient Log Parsing Using  n-Gram Dictionaries. Hetong Dai, Heng Li, Che-Shao Chen, Weiyi Shang, Tse-Hsun Chen, IEEE Transactions on Software Engineering. 482020. 2020</p>
<p>TS-Bert: Time Series Anomaly Detection via Pre-training Model Bert. Weixia Dang, Biyu Zhou, Lingwei Wei, Weigang Zhang, Ziang Yang, Songlin Hu, Computational Science-ICCS 2021: 21st International Conference. Krakow, PolandSpringer2021. June 16-18, 2021Proceedings, Part II 21</p>
<p>Abhimanyu Das, Weihao Kong, Rajat Sen, Yichen Zhou, arXiv:2310.10688A decoder-only foundation model for time-series forecasting. 2023. 2023arXiv preprint</p>
<p>A joint study of the challenges, opportunities, and roadmap of mlops and aiops: A systematic survey. Josu Diaz-De-Arcaya, Ana I Torre-Bastida, Gorka Zarate, Raul Minon, Aitor Almeida, Comput. Surveys. 562023. 2023</p>
<p>Simmtm: A simple pre-training framework for masked time-series modeling. Jiaxiang Dong, Haixu Wu, Haoran Zhang, Li Zhang, Jianmin Wang, Mingsheng Long, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>Saits: Self-attention-based imputation for time series. Wenjie Du, David Ct, Yan Liu, Expert Systems with Applications. 2191196192023. 2023</p>
<p>Early exploration of using chatgpt for log-based anomaly detection on parallel file systems logs. Chris Egersdoerfer, Di Zhang, Dong Dai, Proceedings of the 32nd International Symposium on High-Performance Parallel and Distributed Computing. the 32nd International Symposium on High-Performance Parallel and Distributed Computing2023</p>
<p>Tsmixer: Lightweight mlp-mixer model for multivariate time series forecasting. Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, Jayant Kalagnanam, Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2023</p>
<p>Vijay Ekambaram, Arindam Jati, Nam H Nguyen, Pankaj Dayama, Chandra Reddy, Wesley M Gifford, Jayant Kalagnanam, arXiv:2401.03955TTMs: Fast Multi-level Tiny Time Mixers for Improved Zero-shot and Few-shot Forecasting of Multivariate Time Series. 2024. 2024arXiv preprint</p>
<p>DevOps and the cost of downtime: Fortune 1000 best practice metrics quantified. Stephen Elliot, 2014. 2014International Data Corporation (IDC</p>
<p>Large language models for software engineering: Survey and open problems. Angela Fan, Beliz Gokkaya, Mark Harman, Mitya Lyubarskiy, Shubho Sengupta, Shin Yoo, Jie M Zhang, arXiv:2310.035332023. 2023arXiv preprint</p>
<p>Extending Context Window of Large Language Models via Semantic Compression. Weizhi Fei, Xueyan Niu, Pingyi Zhou, Lu Hou, Bo Bai, Lei Deng, Wei Han, arXiv:2312.095712023. 2023arXiv preprint</p>
<p>Gp-vae: Deep probabilistic time series imputation. Dmitry Vincent Fortuin, Gunnar Baranchuk, Stephan Rtsch, Mandt, International conference on artificial intelligence and statistics. PMLR2020</p>
<p>. Azul Garza, Max Mergenthaler-Canseco, arXiv:2310.035892023. 2023TimeGPT-1. arXiv preprint</p>
<p>Publication date. J Acm, August 202437</p>
<p>. Drishti Goel, Fiza Husain, Aditya Singh, Supriyo Ghosh, Anjaly Parayil, Chetan Bansal, Xuchao Zhang, Saravan Rajmohan, arXiv:2404.036622024. 2024X-lifecycle Learning for Cloud Incident Management using LLMs. arXiv preprint</p>
<p>Large language models are zero-shot time series forecasters. Nate Gruver, Marc Finzi, Shikai Qiu, Andrew G Wilson, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>A systematic survey of prompt engineering on vision-language foundation models. Jindong Gu, Zhen Han, Shuo Chen, Ahmad Beirami, Bailan He, Gengyuan Zhang, Ruotong Liao, Yao Qin, Philip Volker Tresp, Torr, arXiv:2307.129802023. 2023arXiv preprint</p>
<p>Hongcheng Guo, Jian Yang, Jiaheng Liu, Liqun Yang, Linzheng Chai, Jiaqi Bai, Junran Peng, Xiaorong Hu, Chao Chen, Dongfeng Zhang, arXiv:2309.09298Owl: A large language model for it operations. 2023. 2023arXiv preprint</p>
<p>Hongcheng Guo, Wei Zhang, Anjie Le, Jian Yang, Jiaheng Liu, Zhoujun Li, Tieqiao Zheng, Shi Xu, Runqiang Zang, Liangfan Zheng, arXiv:2402.18205Lemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging. 2024. 2024arXiv preprint</p>
<p>Learning Representations on Logs for AIOps. Pranjal Gupta, Harshit Kumar, Debanjana Kar, Karan Bhukar, Pooja Aggarwal, Prateeti Mohapatra, 2023 IEEE 16th International Conference on Cloud Computing (CLOUD). IEEE2023</p>
<p>A Holistic View of AI-driven Network Incident Management. Pouya Hamadanian, Behnaz Arzani, Sadjad Fouladi, Siva Kesava Reddy, Rodrigo Kakarla, Denizcan Fonseca, Ahmad Billor, Edet Cheema, Ranveer Nkposong, Chandra, Proceedings of the 22nd ACM Workshop on Hot Topics in Networks. the 22nd ACM Workshop on Hot Topics in Networks2023</p>
<p>Logmine: Fast pattern recognition for log analytics. Hossein Hamooni, Biplob Debnath, Jianwu Xu, Hui Zhang, Guofei Jiang, Abdullah Mueen, Proceedings of the 25th ACM International on Conference on Information and Knowledge Management. the 25th ACM International on Conference on Information and Knowledge Management2016</p>
<p>Log-based anomaly detection with robust feature extraction and online learning. Shangbin Han, Qianhong Wu, Han Zhang, Bo Qin, Jiankun Hu, Xingang Shi, Linfeng Liu, Xia Yin, IEEE Transactions on Information Forensics and Security. 162021. 2021</p>
<p>Drain: An online log parsing approach with fixed depth tree. Pinjia He, Jieming Zhu, Zibin Zheng, Michael R Lyu, 2017 IEEE international conference on web services (ICWS). IEEE2017</p>
<p>Parameter-Efficient Log Anomaly Detection based on Pre-training model and LORA. Shiming He, Ying Lei, Ying Zhang, Kun Xie, Pradip Kumar Sharma, 2023 IEEE 34th International Symposium on Software Reliability Engineering (ISSRE). IEEE2023</p>
<p>Loghub: A large collection of system log datasets towards automated log analytics. Shilin He, Jieming Zhu, Pinjia He, Michael R Lyu, 2023. 2023</p>
<p>Tabllm: Few-shot classification of tabular data with large language models. Stefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi Jiang, David Sontag, International Conference on Artificial Intelligence and Statistics. PMLR2023</p>
<p>Research on log anomaly detection based on sentence-BERT. Caiping Hu, Xuekui Sun, Hua Dai, Hangchuan Zhang, Haiqiang Liu, Electronics. 1235802023. 2023</p>
<p>Hitanomaly: Hierarchical transformers for anomaly detection in system log. Shaohan Huang, Yi Liu, Carol Fung, Rong He, Yining Zhao, Hailong Yang, Zhongzhi Luan, 2020. 202017</p>
<p>Improving log-based anomaly detection by pre-training hierarchical transformers. Shaohan Huang, Yi Liu, Carol Fung, He Wang, Hailong Yang, Zhongzhi Luan, IEEE Trans. Comput. 2023. 2023</p>
<p>Root cause analysis of failures in microservices through causal discovery. Azam Ikram, Sarthak Chakraborty, Subrata Mitra, Shiv Saini, Saurabh Bagchi, Murat Kocaoglu, Advances in Neural Information Processing Systems. 352022. 2022</p>
<p>Log Anomaly Detection Through GPT-2 for Large Scale Systems. J I Yuhe, Han Jing, Zhao Yongxin, Zhang Shenglin, Gong Zican, ZTE Communications. 21702023. 2023</p>
<p>Logflash: Real-time streaming anomaly detection and diagnosis from system logs for large-scale software systems. Tong Jia, Yifan Wu, Chuanjia Hou, Ying Li, 2021 IEEE 32nd International Symposium on Software Reliability Engineering (ISSRE). IEEE2021</p>
<p>Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, Lili Qiu, arXiv:2310.05736Llmlingua: Compressing prompts for accelerated inference of large language models. 2023. 2023arXiv preprint</p>
<p>Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, Lili Qiu, arXiv:2310.06839Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression. 2023. 2023arXiv preprint</p>
<p>Xpert: Empowering incident management with query recommendations via. Yuxuan Jiang, Chaoyun Zhang, Shilin He, Zhihao Yang, Minghua Ma, Si Qin, Yu Kang, Yingnong Dang, Saravan Rajmohan, Qingwei Lin, Proceedings of the IEEE/ACM 46th International Conference on Software Engineering. the IEEE/ACM 46th International Conference on Software Engineering202437Publication date: August 2024. large language models</p>
<p>Zhihan Jiang, Jinyang Liu, Zhuangbin Chen, Yichen Li, Junjie Huang, Yintong Huo, Pinjia He, Jiazhen Gu, Michael R Lyu, arXiv:2310.01796Llmparser: A llm-based log parsing framework. 2023. 2023arXiv preprint</p>
<p>Abstracting execution logs to execution events for enterprise applications (short paper). Ming Zhen, Ahmed E Jiang, Parminder Hassan, Gilbert Flora, Hamann, The Eighth International Conference on Quality Software. IEEE2008. 2008</p>
<p>Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, arXiv:2310.01728Time-llm: Time series forecasting by reprogramming large language models. 2023. 2023arXiv preprint</p>
<p>Assess and summarize: Improve outage understanding with large language models. Pengxiang Jin, Shenglin Zhang, Minghua Ma, Haozhe Li, Yu Kang, Liqun Li, Yudong Liu, Bo Qiao, Chaoyun Zhang, Pu Zhao, Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering2023</p>
<p>Separation or not: On handing out-of-order time-series data in leveled lsm-tree. Yuyuan Kang, Xiangdong Huang, Shaoxu Song, Lingzhe Zhang, Jialin Qiao, Chen Wang, Jianmin Wang, Julian Feinauer, 2022 IEEE 38th International Conference on Data Engineering (ICDE). IEEE2022</p>
<p>Exploring semantic vs. Syntactic features for unsupervised learning on application log files. Egil Karlsen, Rafael Copstein, Xiao Luo, Jeff Schwartzentruber, Bradley Niblett, Andrew Johnston, Malcolm I Heywood, Nur Zincir-Heywood, 2023 7th Cyber Security in Networking Conference (CSNet). IEEE2023</p>
<p>Large language models and unsupervised feature learning: implications for log analysis. Egil Karlsen, Xiao Luo, Nur Zincir-Heywood, Malcolm Heywood, Annals of Telecommunications. 2024. 2024</p>
<p>Domain Adaptation for Time series Transformers using One-step fine-tuning. Subina Khanal, Seshu Tirupathi, Giulio Zizzo, Ambrish Rawat, Torben Bach Pedersen, arXiv:2401.065242024. 2024arXiv preprint</p>
<p>Generative AI for self-healing systems. Pitikorn Khlaisamniang, Prachaya Khomduean, Kriangkrai Saetan, Supasin Wonglapsuwan, 2023 18th International Joint Symposium on Artificial Intelligence and Natural Language Processing. IEEE2023iSAI-NLP</p>
<p>Probabilistic imputation for time-series classification with missing data. Seunghyun Kim, Hyunsu Kim, Eunggu Yun, Hwangrae Lee, Jaehun Lee, Juho Lee, International Conference on Machine Learning. PMLR2023</p>
<p>Jinxi Kuang, Jinyang Liu, Junjie Huang, Renyi Zhong, Jiazhen Gu, Lan Yu, Rui Tan, Zengyin Yang, Michael R Lyu, arXiv:2403.06485Knowledge-aware Alert Aggregation in Large-scale Cloud Systems: a Hybrid Approach. 2024. 2024arXiv preprint</p>
<p>A Critical Review of Common Log Data Sets Used for Evaluation of Sequence-based Anomaly Detection Techniques. Max Landauer, Florian Skopik, Markus Wurzenberger, arXiv:2309.028542023. 2023arXiv preprint</p>
<p>Log-based anomaly detection without log parsing. Van-Hoang Le, Hongyu Zhang, 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE2021</p>
<p>Log Parsing: How Far Can ChatGPT Go?. Van-Hoang Le, Hongyu Zhang, 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE2023</p>
<p>Lanobert: System log anomaly detection based on bert masked language model. Yukyung Lee, Jina Kim, Pilsung Kang, Applied Soft Computing. 1461106892023. 2023</p>
<p>opengauss: An autonomous database system. Guoliang Li, Xuanhe Zhou, Ji Sun, Xiang Yu, Yue Han, Lianyuan Jin, Wenbo Li, Tianqing Wang, Shifu Li, Proceedings of the VLDB Endowment. the VLDB Endowment2021. 202114</p>
<p>Few-shot time-series anomaly detection with unsupervised domain adaptation. Hongbo Li, Wenli Zheng, Feilong Tang, Yanmin Zhu, Jielong Huang, Information Sciences. 6491196102023. 2023</p>
<p>Pre-trained language models for text generation: A survey. Junyi Li, Tianyi Tang, Wayne Xin Zhao, Jian-Yun Nie, Ji-Rong Wen, Comput. Surveys. 562024. 2024</p>
<p>Peiwen Li, Xin Wang, Zeyang Zhang, Yuan Meng, Fang Shen, Yue Li, Jialong Wang, Yang Li, Wenweu Zhu, arXiv:2404.14786LLM-Enhanced Causal Discovery in Temporal Domain from Interventional Data. 2024. 2024arXiv preprint</p>
<p>Evaluating BERT on cloud-edge time series forecasting and sentiment analysis via prompt learning. Qizhi Li, Xianyong Li, Yujia Song, Maolin Zhang, Longqi Chen, Gang Wang, Yajun Du, 2022 IEEE 24th Int Conf on High Performance Computing &amp; Communications; 8th Int Conf on Data Science &amp; Systems; 20th Int Conf on Smart City; 8th Int Conf on Dependability in Sensor, Cloud &amp; Big Data Systems &amp; Application. HPCC/DSS/SmartCity/DependSys. IEEE2022</p>
<p>Compressing context to enhance inference efficiency of large language models. Yucheng Li, Bo Dong, Chenghua Lin, Frank Guerin, arXiv:2310.062012023. 2023arXiv preprint</p>
<p>Wenlong Liao, Fernando Porte-Agel, Jiannong Fang, Christian Rehtanz, Shouxiang Wang, Dechang Yang, Zhe Yang, arXiv:2404.04885TimeGPT in Load Forecasting: A Large Time Series Model Perspective. 2024arXiv preprint</p>
<p>Publication date. J Acm, August 2024. 20243731</p>
<p>Hardware remediation at scale. Fan Lin, Matt Beadon, Dattatraya Harish, Gautham Dixit, Amol Vunnam, Sriram Desai, Sankar, 2018 48th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-W). IEEE2018</p>
<p>Log clustering based problem identification for online service systems. Qingwei Lin, Hongyu Zhang, Jian-Guang Lou, Yu Zhang, Xuewei Chen, Proceedings of the 38th International Conference on Software Engineering Companion. the 38th International Conference on Software Engineering Companion2016</p>
<p>FastLogAD: Log Anomaly Detection with Mask-Guided Pseudo Anomaly Generation and Discrimination. Yifei Lin, Hanqiu Deng, Xingyu Li, arXiv:2404.087502024. 2024arXiv preprint</p>
<p>Large Language Model Guided Knowledge Distillation for Time Series Anomaly Detection. Chen Liu, Shibo He, Qihang Zhou, Shizhong Li, Wenchao Meng, arXiv:2401.151232024. 2024arXiv preprint</p>
<p>Haoxin Liu, Zhiyuan Zhao, Jindong Wang, Harshavardhan Kamarthi, Aditya Prakash, arXiv:2402.16132LSTPrompt: Large Language Models as Zero-Shot Time Series Forecasters by Long-Short-Term Prompting. 2024. 2024arXiv preprint</p>
<p>Scalable and adaptive log-based anomaly detection with expert in the loop. Jinyang Liu, Junjie Huang, Yintong Huo, Zhihan Jiang, Jiazhen Gu, Zhuangbin Chen, Cong Feng, Minzhi Yan, Michael R Lyu, arXiv:2306.050322023. 2023arXiv preprint</p>
<p>Yuhe Liu, Changhua Pei, Longlong Xu, Bohan Chen, Mingze Sun, Zhirui Zhang, Yongqian Sun, Shenglin Zhang, Kun Wang, Haiming Zhang, arXiv:2310.07637OpsEval: A Comprehensive Task-Oriented AIOps Benchmark for Large Language Models. 2023. 2023arXiv preprint</p>
<p>Naomi: Non-autoregressive multiresolution sequence imputation. Yukai Liu, Rose Yu, Stephan Zheng, Eric Zhan, Yisong Yue, Advances in neural information processing systems. 322019. 2019</p>
<p>Yingzhe Lyu, Heng Li, Zhen Ming, Ahmed E Hassan, arXiv:2311.03213Assessing the maturity of model maintenance techniques for AIOps solutions. 2023. 2023arXiv preprint</p>
<p>Diagnosing root causes of intermittent slow queries in cloud databases. Minghua Ma, Zheng Yin, Shenglin Zhang, Sheng Wang, Christopher Zheng, Xinhao Jiang, Hanwen Hu, Cheng Luo, Yilin Li, Nengjun Qiu, Proceedings of the VLDB Endowment. the VLDB Endowment2020. 202013</p>
<p>LLMParser: An Exploratory Study on Using Large Language Models for Log Parsing. Zeyang Ma, An Ran Chen, Dong Jae Kim, Tse-Hsun Chen, Shaowei Wang, Proceedings of the IEEE/ACM 46th International Conference on Software Engineering. the IEEE/ACM 46th International Conference on Software Engineering2024</p>
<p>Clustering event logs using iterative partitioning. Adetokunbo Ao Makanju, Evangelos E Nur Zincir-Heywood, Milios, Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining. the 15th ACM SIGKDD international conference on Knowledge discovery and data mining2009</p>
<p>Using deep learning to generate complete log statements. Antonio Mastropaolo, Luca Pascarella, Gabriele Bavota, Proceedings of the 44th International Conference on Software Engineering. the 44th International Conference on Software Engineering2022</p>
<p>Generative semi-supervised learning for multivariate time series imputation. Xiaoye Miao, Yangyang Wu, Jun Wang, Yunjun Gao, Xudong Mao, Jianwei Yin, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202135</p>
<p>Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, Jianfeng Gao, arXiv:2402.06196Large language models: A survey. 2024. 2024arXiv preprint</p>
<p>Learning DAGs from data with few root causes. Panagiotis Misiakos, Chris Wendler, Markus Pschel, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>An Assessment of ChatGPT on Log Data. Priyanka Mudgal, Rita Wouhaybi, International Conference on AI-generated Content. Springer2023</p>
<p>Uncertainty-aware variational-recurrent imputation network for clinical time series. Ahmad Wisnu Mulyadi, Eunji Jun, Heung-Il Suk, IEEE Transactions on Cybernetics. 522021. 2021</p>
<p>Self-supervised log parsing. Sasho Nedelkoski, Jasmin Bogatinovski, Alexander Acker, Jorge Cardoso, Odej Kao, Machine Learning and Knowledge Discovery in Databases: Applied Data Science Track: European Conference, ECML PKDD 2020. Ghent, BelgiumSpringer2021. September 14-18, 2020Proceedings, Part IV</p>
<p>A survey of aiops methods for failure management. Paolo Notaro, Jorge Cardoso, Michael Gerndt, ACM Transactions on Intelligent Systems and Technology (TIST). 122021. 2021</p>
<p>Fostering websites accessibility: A case study on the use of the Large Language Models ChatGPT for automatic remediation. Achraf Othman, Amira Dhouib, Aljazi Nasser, Al Jabor, Proceedings of the 16th International Conference on PErvasive Technologies Related to Assistive Environments. the 16th International Conference on PErvasive Technologies Related to Assistive Environments2023</p>
<p>Robust and transferable anomaly detection in log data using pre-trained language models. Harold Ott, Jasmin Bogatinovski, Alexander Acker, Sasho Nedelkoski, Odej Kao, IEEE/ACM international workshop on cloud intelligence. 2021. 2021IEEE</p>
<p>Publication date. J Acm, August 202437</p>
<p>RAGLog: Log Anomaly Detection using Retrieval Augmented Generation. Jonathan Pan, Swee Liang Wong, Yidi Yuan, arXiv:2311.052612023. 2023arXiv preprint</p>
<p>Formulating an Korean LLM-Based Interactive Assistant for Enhanced IT Collaboration in Microservice Environments. Gijun Park, Dohoon Kim, 2023 IEEE 8th International Conference on Smart Cloud (SmartCloud). IEEE2023</p>
<p>Uncovering LLMs for Service-Composition: Challenges and Opportunities. Robin D Pesl, Miles Sttzner, Ilche Georgievski, Marco Aiello, International Conference on Service-Oriented Computing. Springer2023</p>
<p>Market guide for aiops platforms. Pankaj Prasad, Charley Rich, 2018. March 12, 2020. 2018</p>
<p>Loggpt: Exploring chatgpt for log-based anomaly detection. Jiaxing Qi, Shaohan Huang, Zhongzhi Luan, Shu Yang, Carol Fung, Hailong Yang, Depei Qian, Jing Shang, Zhiwen Xiao, Zhihui Wu, 2023 IEEE International Conference on High Performance Computing &amp; Communications, Data Science &amp; Systems, Smart City &amp; Dependability in Sensor, Cloud &amp; Big Data Systems &amp; Application. HPCC/DSS/SmartCity/DependSys. IEEE2023</p>
<p>Heterogeneous Syslog Analysis: There Is Hope. Andres Quan, Leah Howell, Hugh Greenberg, Proceedings of the SC'23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis. the SC'23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis2023</p>
<p>Lag-llama: Towards foundation models for time series forecasting. Kashif Rasul, Arjun Ashok, Andrew Robert Williams, Arian Khorasani, George Adamopoulos, Rishika Bhagwatkar, Marin Bilo, Hena Ghonia, Nadhir Vincent Hassen, Anderson Schneider, arXiv:2310.082782023. 2023arXiv preprint</p>
<p>Aiops solutions for incident management: Technical guidelines and a comprehensive literature review. Youcef Remil, Anes Bendimerad, Romain Mathonat, Mehdi Kaytoue, arXiv:2404.013632024. 2024arXiv preprint</p>
<p>Exploring LLM-based Agents for Root Cause Analysis. Devjeet Roy, Xuchao Zhang, Rashi Bhave, Chetan Bansal, Pedro Las-Casas, Rodrigo Fonseca, Saravan Rajmohan, arXiv:2403.041232024. 2024arXiv preprint</p>
<p>Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, Aman Chadha, arXiv:2402.07927A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications. 2024. 2024arXiv preprint</p>
<p>Leveraging Large Language Models for Auto-remediation in Microservices Architecture. Komal Sarda, 2023 IEEE International Conference on Autonomic Computing and Self-Organizing Systems Companion (ACSOS-C). IEEE2023</p>
<p>Adarma auto-detection and auto-remediation of microservice anomalies by leveraging large language models. Komal Sarda, Zakeya Namrud, Raphael Rouf, Harit Ahuja, Mohammadreza Rasolroveicy, Marin Litoiu, Larisa Shwartz, Ian Watts, Proceedings of the 33rd Annual International Conference on Computer Science and Software Engineering. the 33rd Annual International Conference on Computer Science and Software Engineering2023</p>
<p>An Effective Approach for Parsing Large Log Files. Issam Sedki, Abdelwahab Hamou-Lhadj, Otmane Ait-Mohamed, Mohammed A Shehab, 2022 IEEE International Conference on Software Maintenance and Evolution (ICSME). IEEE2022</p>
<p>Shiwen Shan, Yintong Huo, Yuxin Su, Yichen Li, Dan Li, Zibin Zheng, arXiv:2404.00640Face It Yourselves: An LLM-Based Two-Stage Strategy to Localize Configuration Errors via Logs. 2024. 2024arXiv preprint</p>
<p>Efficient ticket routing by resolution sequence mining. Qihong Shao, Yi Chen, Shu Tao, Xifeng Yan, Nikos Anerousis, Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining. the 14th ACM SIGKDD international conference on Knowledge discovery and data mining2008</p>
<p>Log anomaly detection method based on bert model optimization. Yangyi Shao, Wenbin Zhang, Peishun Liu, Ren Huyue, Ruichun Tang, Qilin Yin, Qi Li, 2022 7th International Conference on Cloud Computing and Big Data Analytics (ICCCBDA). IEEE2022</p>
<p>ShellGPT: Generative Pre-trained Transformer Model for Shell Language Understanding. Jie Shi, Sihang Jiang, Bo Xu, Jiaqing Liang, Yanghua Xiao, Wei Wang, 2023 IEEE 34th International Symposium on Software Reliability Engineering (ISSRE). IEEE2023</p>
<p>Keiichi Shima, arXiv:1611.03213Length matters: Clustering system log messages using length of words. 2016. 2016arXiv preprint</p>
<p>Large Language Models for Forecasting and Anomaly Detection: A Systematic Literature Review. Jing Su, Chufeng Jiang, Xin Jin, Yuxin Qiao, Tingsong Xiao, Hongda Ma, Rong Wei, Zhi Jing, Jiajun Xu, Junhong Lin, arXiv:2402.103502024. 2024arXiv preprint</p>
<p>LogKG: Log Failure Diagnosis through Knowledge Graph. Yicheng Sui, Yuzhe Zhang, Jianjun Sun, Ting Xu, Shenglin Zhang, Zhengdan Li, Yongqian Sun, Fangrui Guo, Junyu Shen, Yuzhi Zhang, IEEE Transactions on Services Computing. 2023. 2023</p>
<p>TEST: Text prototype aligned embedding to activate LLM's ability for time series. Chenxi Sun, Yaliang Li, Hongyan Li, Shenda Hong, arXiv:2308.082412023. 2023arXiv preprint</p>
<p>Publication date. J Acm, August 20243733</p>
<p>Design and Development of a Log Management System Based on Cloud Native Architecture. Yuchen Sun, Yanpiao Chen, Haotian Zhao, Shan Peng, 2023 9th International Conference on Systems and Informatics (ICSAI). IEEE2023</p>
<p>LogSig: Generating system events from raw textual logs. Liang Tang, Tao Li, Chang-Shing Perng, Proceedings of the 20th ACM international conference on Information and knowledge management. the 20th ACM international conference on Information and knowledge management2011</p>
<p>Logcluster-a data clustering and pattern mining algorithm for event logs. Risto Vaarandi, Mauno Pihelgas, 2015 11th International conference on network and service management (CNSM). IEEE2015</p>
<p>Jun Wang, Wenjie Du, Wei Cao, Keli Zhang, Wenjia Wang, Yuxuan Liang, Qingsong Wen, arXiv:2402.04059Deep Learning for Multivariate Time Series Imputation: A Survey. 2024. 2024arXiv preprint</p>
<p>Network Meets ChatGPT: Intent Autonomous Management, Control and Operation. Jingyu Wang, Lei Zhang, Yiran Yang, Zirui Zhuang, Qi Qi, Haifeng Sun, Lu Lu, Junlan Feng, Jianxin Liao, Journal of Communications and Information Networks. 82023. 2023</p>
<p>Root cause analysis for microservice systems via hierarchical reinforcement learning from human feedback. Lu Wang, Chaoyun Zhang, Ruomeng Ding, Yong Xu, Qihang Chen, Wentao Zou, Qingjun Chen, Meng Zhang, Xuedong Gao, Hao Fan, Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2023</p>
<p>Constructing the knowledge base for cognitive it service management. Qing Wang, Wubai Zhou, Chunqiu Zeng, Tao Li, Larisa Shwartz, Genady Ya, Grabarnik , 2017 IEEE International Conference on Services Computing (SCC). IEEE2017</p>
<p>Low-code ChatOps for Microservices Systems Using Service Composition. Sheng-Kai Wang, Shang-Pin Ma, Chen-Hao Chao, Guan-Hong Lai, 2023 IEEE International Conference on e-Business Engineering (ICEBE). IEEE2023</p>
<p>An observed value consistent diffusion model for imputing missing values in multivariate time series. Xu Wang, Hongbo Zhang, Pengkun Wang, Yudong Zhang, Binwu Wang, Zhengyang Zhou, Yang Wang, Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2023</p>
<p>Zefan Wang, Zichuan Liu, Yingying Zhang, Aoxiao Zhong, Lunting Fan, Lingfei Wu, Qingsong Wen, arXiv:2310.16340RCAgent: Cloud Root Cause Analysis by Autonomous Agents with Tool-Augmented Large Language Models. 2023. 2023arXiv preprint</p>
<p>Log-based anomaly detection for distributed systems: State of the art, industry experience, and open issues. Xinjie Wei, Jie Wang, Chang-Ai Sun, Dave Towey, Shoufeng Zhang, Wanqing Zuo, Yiming Yu, Ruoyi Ruan, Guyang Song, Journal of Software: Evolution and Process. 2024. 2024</p>
<p>Timesnet: Temporal 2d-variation modeling for general time series analysis. Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, Mingsheng Long, The eleventh international conference on learning representations. 2022</p>
<p>Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, Bryan Hooi, arXiv:2306.130632023. 2023arXiv preprint</p>
<p>Recomp: Improving retrieval-augmented lms with compression and selective augmentation. Fangyuan Xu, Weijia Shi, Eunsol Choi, arXiv:2310.044082023. 2023arXiv preprint</p>
<p>Junjielong Xu, Ruichun Yang, Yintong Huo, Chengyu Zhang, Pinjia He, arXiv:2307.09950Prompting for Automatic Log Template Extraction. 2023. 2023arXiv preprint</p>
<p>Promptcast: A new prompt-based learning paradigm for time series forecasting. Hao Xue, Flora D Salim, IEEE Transactions on Knowledge and Data Engineering. 2023. 2023</p>
<p>Siqiao Xue, Caigao Jiang, Wenhui Shi, Fangyin Cheng, Keting Chen, Hongjun Yang, Zhiping Zhang, Jianshan He, Hongyang Zhang, Ganglin Wei, arXiv:2312.17449Db-gpt: Empowering database interactions with private large language models. 2023. 2023arXiv preprint</p>
<p>Diffusion-Based Time Series Data Imputation for Cloud Failure Prediction at Microsoft 365. Fangkai Yang, Wenjie Yin, Lu Wang, Tianci Li, Pu Zhao, Bo Liu, Paul Wang, Bo Qiao, Yudong Liu, Mrten Bjrkman, Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering2023</p>
<p>Semisupervised log-based anomaly detection via probabilistic label estimation. Lin Yang, Junjie Chen, Zan Wang, Weijing Wang, Jiajun Jiang, Xuyuan Dong, Wenbin Zhang, 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEE2021</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022. 2022arXiv preprint</p>
<p>Brain: Log Parsing with Bidirectional Parallel Tree. Siyu Yu, Pinjia He, Ningjiang Chen, Yifan Wu, IEEE Transactions on Services Computing. 2023. 2023</p>
<p>An approach to cloud execution failure diagnosis based on exception logs in openstack. Yue Yuan, Wenchang Shi, Bin Liang, Bo Qin, 2019 IEEE 12th International Conference on Cloud Computing (CLOUD). IEEE2019</p>
<p>Publication date. J Acm, August 202437</p>
<p>Knowledge guided hierarchical multi-label classification over ticket data. Chunqiu Zeng, Wubai Zhou, Tao Li, Larisa Shwartz, Genady Ya, Grabarnik , IEEE Transactions on Network and Service Management. 142017. 2017</p>
<p>Traceark: Towards actionable performance anomaly alerting for online service systems. Zhengran Zeng, Yuqun Zhang, Yong Xu, Minghua Ma, Bo Qiao, Wentao Zou, Qingjun Chen, Meng Zhang, Xu Zhang, Hongyu Zhang, 2023 IEEE/ACM 45th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP). IEEE2023</p>
<p>Deeptralog: Trace-log combined microservice anomaly detection through graph-based deep learning. Chenxi Zhang, Xin Peng, Chaofeng Sha, Ke Zhang, Zhenqing Fu, Xiya Wu, Qingwei Lin, Dongmei Zhang, Proceedings of the 44th International Conference on Software Engineering. the 44th International Conference on Software Engineering2022</p>
<p>Dylan Zhang, Xuchao Zhang, Chetan Bansal, Pedro Las-Casas, Rodrigo Fonseca, Saravan Rajmohan, arXiv:2309.05833PACE: Prompting and Augmentation for Calibrated Confidence Estimation with GPT-4 in Cloud Incident Root Cause Analysis. 2023. 2023arXiv preprint</p>
<p>Putracead: Trace anomaly detection with partial labels based on gnn and pu learning. Ke Zhang, Chenxi Zhang, Xin Peng, Chaofeng Sha, 2022 IEEE 33rd International Symposium on Software Reliability Engineering (ISSRE). IEEE2022</p>
<p>Two-stage file compaction framework by log-structured merge-tree for time series data. Zhang Lingzhe, Huang Xiangdong, Qiao Jialin, Gou Wangminhao, Jianmin Wang, Journal of Computer Applications. 416182021. 2021</p>
<p>Lingzhe Zhang, Tong Jia, Mengxi Jia, Ying Li, Yong Yang, Zhonghai Wu, arXiv:2406.07976Multivariate Log-based Anomaly Detection for Distributed Database. 2024. 2024arXiv preprint</p>
<p>Timetired compaction: An elastic compaction scheme for LSM-tree based time-series database. Ling-Zhe Zhang, Xiang-Dong Huang, Yan-Kai Wang, Jia-Lin Qiao, Shao-Xu Song, Jian-Min Wang, Advanced Engineering Informatics. 591022242024. 2024</p>
<p>LogST: Log semi-supervised anomaly detection based on sentence-BERT. Mingyang Zhang, Jianfei Chen, Jianyi Liu, Jingchu Wang, Rui Shi, Hua Sheng, 2022 7th International Conference on Signal and Image Processing (ICSIP). IEEE2022</p>
<p>Quanjun Zhang, Chunrong Fang, Yang Xie, Yaxin Zhang, Yun Yang, Weisong Sun, Shengcheng Yu, Zhenyu Chen, arXiv:2312.15223A Survey on Large Language Models for Software Engineering. 2023. 2023arXiv preprint</p>
<p>LogPrompt: A Log-based Anomaly Detection Framework Using Prompts. Ting Zhang, Xin Huang, Wen Zhao, Shaohuang Bian, Peng Du, 2023 International Joint Conference on Neural Networks (IJCNN). IEEE2023</p>
<p>Wei Zhang, Hongcheng Guo, Jian Yang, Yi Zhang, Chaoran Yan, Zhoujin Tian, Hangyuan Ji, Zhoujun Li, Tongliang Li, Tieqiao Zheng, arXiv:2404.12135mABC: multi-Agent Blockchain-Inspired Collaboration for root cause analysis in micro-services architecture. 2024. 2024arXiv preprint</p>
<p>Automated Root Causing of Cloud Incidents using In-Context Learning with. Xuchao Zhang, Supriyo Ghosh, Chetan Bansal, Rujia Wang, Minghua Ma, Yu Kang, Saravan Rajmohan, arXiv:2401.138102024. 2024GPT-4. arXiv preprint</p>
<p>Robust log-based anomaly detection on unstable log data. Xu Zhang, Yong Xu, Qingwei Lin, Bo Qiao, Hongyu Zhang, Yingnong Dang, Chunyu Xie, Xinsheng Yang, Qian Cheng, Ze Li, Proceedings of the 2019 27th ACM joint meeting on European software engineering conference and symposium on the foundations of software engineering. the 2019 27th ACM joint meeting on European software engineering conference and symposium on the foundations of software engineering2019</p>
<p>One fits all: Power general time series analysis by pretrained lm. Tian Zhou, Peisong Niu, Liang Sun, Rong Jin, Advances in neural information processing systems. 362023. 2023</p>
<p>Efficient prompting via dynamic in-context learning. Wangchunshu Zhou, Yuchen Eleanor Jiang, Ryan Cotterell, Mrinmaya Sachan, arXiv:2305.111702023. 2023arXiv preprint</p>
<p>Resolution recommendation for event tickets in service management. Wubai Zhou, Liang Tang, Chunqiu Zeng, Tao Li, Larisa Shwartz, Genady Ya, Grabarnik , IEEE Transactions on Network and Service Management. 132016. 2016</p>
<p>Dbmind: A self-driving platform in opengauss. Xuanhe Zhou, Lianyuan Jin, Ji Sun, Xinyang Zhao, Xiang Yu, Jianhua Feng, Shifu Li, Tianqing Wang, Kun Li, Luyang Liu, Proceedings of the VLDB Endowment. the VLDB Endowment2021. 202114</p>
<p>Xuanhe Zhou, Guoliang Li, Zhaoyan Sun, Zhiyuan Liu, Weize Chen, Jianming Wu, Jiesi Liu, arXiv:2312.01454Ruohang Feng, and Guoyang Zeng. 2023. D-bot: Database diagnosis system using large language models. 2023arXiv preprint</p>
<p>Fault analysis and debugging of microservice systems: Industrial survey, benchmark system, and empirical study. Xiang Zhou, Xin Peng, Tao Xie, Jun Sun, Chao Ji, Wenhai Li, Dan Ding, IEEE Transactions on Software Engineering. 472018. 2018</p>
<p>Publication date. J Acm, August 20243735</p>
<p>Xuanhe Zhou, Xinyang Zhao, Guoliang Li, arXiv:2402.02643LLM-Enhanced Data Management. 2024. 2024arXiv preprint</p>
<p>Zixuan Zhou, Xuefei Ning, Ke Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming Lou, Luning Wang, Zhihang Yuan, Xiuhong Li, arXiv:2404.14294A survey on efficient inference for large language models. 2024. 2024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
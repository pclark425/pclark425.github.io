<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1273 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1273</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1273</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-dc8fb699df3504befcdf4037fb625d81f08a2876</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/dc8fb699df3504befcdf4037fb625d81f08a2876" target="_blank">Active Inference, Curiosity and Insight</a></p>
                <p><strong>Paper Venue:</strong> Neural Computation</p>
                <p><strong>Paper TL;DR:</strong> This article uses simulations of abstract rule learning and approximate Bayesian inference to show that minimizing (expected) variational free energy leads to active sampling of novel contingencies and closes explanatory gaps in generative models of the world, thereby reducing uncertainty and satisfying curiosity.</p>
                <p><strong>Paper Abstract:</strong> This article offers a formal account of curiosity and insight in terms of active (Bayesian) inference. It deals with the dual problem of inferring states of the world and learning its statistical structure. In contrast to current trends in machine learning (e.g., deep learning), we focus on how people attain insight and understanding using just a handful of observations, which are solicited through curious behavior. We use simulations of abstract rule learning and approximate Bayesian inference to show that minimizing (expected) variational free energy leads to active sampling of novel contingencies. This epistemic behavior closes explanatory gaps in generative models of the world, thereby reducing uncertainty and satisfying curiosity. We then move from epistemic learning to model selection or structure learning to show how abductive processes emerge when agents test plausible hypotheses about symmetries (i.e., invariances or rules) in their generative models. The ensuing Bayesian model reduction evinces mechanisms associated with sleep and has all the hallmarks of “aha” moments. This formulation moves toward a computational account of consciousness in the pre-Cartesian sense of sharable knowledge (i.e., con: “together”; scire: “to know”).</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1273.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1273.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Active Inference Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Curious active inference agent minimizing expected free energy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated agent that selects actions (saccades and choices) by minimizing expected variational free energy, combining epistemic (information gain about hidden states and likelihood parameters) and extrinsic preferences; it actively samples observations to learn an unknown likelihood mapping and to discover abstract rules from few observations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Active inference (expected free energy) agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Discrete-state active inference agent using a generative model with hidden state factors (rule, correct color, gaze location, choice) and outcome modalities (what, where, feedback). Key components: likelihood parameters A (Dirichlet concentration parameters) to be learned, known transition matrices B and priors D/C, policy evaluation via expected free energy G, softmax policy selection, variational (mean-field) posterior updates, and Dirichlet accumulation for learning A; also uses Bayesian model reduction (structure learning) for model selection/insight.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Active inference / curiosity-driven exploration via expected information gain (mutual information) and novelty (information gain about likelihood parameters); intrinsic value (epistemic) maximization within expected free energy minimization</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>At each decision epoch the agent computes expected free energy G for each candidate policy (sequences of actions) that decomposes into (i) ignorance/novelty term (information gain about likelihood parameters A), (ii) intrinsic/epistemic value (information gain about hidden states), and (iii) extrinsic value (priors/preferences over outcomes). Policies are selected via a softmax over negative G, so when uncertainty about parameters or states is high epistemic/novelty terms dominate and the agent favors actions that maximize expected information gain (e.g., saccading to informative cue locations); as beliefs become precise extrinsic preferences dominate and the agent exploits. Learning of A is achieved by accumulating Dirichlet concentration parameters from co-occurrences of inferred hidden states and observed outcomes; Bayesian model reduction is used offline to test and prune structure.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Abstract rule-learning simulated environment (three-color/three-location cue task)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partially observable (hidden state factors must be inferred), discrete state space, episodic trials, stochastic/deterministic outcome mappings determined by unknown likelihood A (initially uniform for some mappings), discrete actions (saccades and report), context-dependent interactions among hidden factors (context-sensitive contingencies), short-horizon trials with limited observations per trial.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Hidden state factors: rule (3 levels), correct color (3), where/gaze (4), choice (4) — product state-space combinatorics (up to 3×3×4×4 configurations); action space size = 6 (three location-saccades without choosing + return-and-report actions); trials simulated as up to 6 epochs (they report runs of 32 trials with 6 epochs per trial; instruction encouraged decision after up to 3 cue samples). Only likelihood A was learned; state transitions (B) and initial priors (D) assumed known.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Agent performance in the simulated task improved rapidly: posterior policy probabilities sharpen over trials and choices become accurate; authors report that ambiguity is largely resolved by ~trial 10 and performance becomes perfect by trial 14 in their 32-trial simulation (i.e., 100% correct choices after trial 14). Free energy (model evidence proxy) and uncertainty decline as learning proceeds.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>High in this toy task: the agent typically reaches perfect performance after approximately 14 trials (in the reported 32-trial simulation), demonstrating learning of the rule from relatively few trials by actively sampling informative cues. Exact sample counts: ambiguity largely resolved by trial ~10; perfect performance by trial ~14.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Balanced via decomposition of expected free energy: epistemic/novelty (information gain about parameters and states) drives exploration when posterior uncertainty is high; extrinsic value (prior preferences for being correct and responding by a deadline) drives exploitation when uncertainty is low. Policy selection is a softmax over -G, automatically trading exploration and exploitation according to current posterior beliefs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>No explicit baseline comparisons reported in the simulations (e.g., random exploration or UCB) within this paper; method motivated and situated relative to literature on active learning, intrinsic motivation, and Bayesian surprise but no numerical baselines provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>1) Incorporating information gain about likelihood parameters (A) into expected free energy produces curiosity-driven, epistemic policies that actively seek novel state-outcome combinations, enabling rapid learning of context-sensitive abstract rules from few observations. 2) In the simulated rule-learning task the agent actively directed gaze to informative cues and learned the correct mapping such that ambiguity was resolved by ~10 trials and perfect performance achieved by ~14 trials. 3) Bayesian model reduction (structure learning) applied offline to accumulated posterior parameter beliefs can identify simpler, parsimonious models (structure hypotheses) and is associated conceptually with 'insight' or 'aha' moments; this operates without further outcome sampling (fact-free optimization). 4) The formalism naturally integrates epistemic and pragmatic (extrinsic) goals via expected free energy decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>No quantitative comparisons to alternative adaptive-design algorithms (e.g., random sampling, Thompson sampling, UCB) are provided, so relative advantage is not benchmarked. Simulations are small-scale and domain-specific (abstract rule task); transitions B and priors D were assumed known, and only likelihood A was learned, limiting generality. The paper does not report robustness tests under nonstationary environments, adversarial noise, or large/high-dimensional continuous state spaces; computational costs and scalability are not quantified in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Active Inference, Curiosity and Insight', 'publication_date_yy_mm': '2017-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1273",
    "paper_id": "paper-dc8fb699df3504befcdf4037fb625d81f08a2876",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "Active Inference Agent",
            "name_full": "Curious active inference agent minimizing expected free energy",
            "brief_description": "A simulated agent that selects actions (saccades and choices) by minimizing expected variational free energy, combining epistemic (information gain about hidden states and likelihood parameters) and extrinsic preferences; it actively samples observations to learn an unknown likelihood mapping and to discover abstract rules from few observations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Active inference (expected free energy) agent",
            "agent_description": "Discrete-state active inference agent using a generative model with hidden state factors (rule, correct color, gaze location, choice) and outcome modalities (what, where, feedback). Key components: likelihood parameters A (Dirichlet concentration parameters) to be learned, known transition matrices B and priors D/C, policy evaluation via expected free energy G, softmax policy selection, variational (mean-field) posterior updates, and Dirichlet accumulation for learning A; also uses Bayesian model reduction (structure learning) for model selection/insight.",
            "adaptive_design_method": "Active inference / curiosity-driven exploration via expected information gain (mutual information) and novelty (information gain about likelihood parameters); intrinsic value (epistemic) maximization within expected free energy minimization",
            "adaptation_strategy_description": "At each decision epoch the agent computes expected free energy G for each candidate policy (sequences of actions) that decomposes into (i) ignorance/novelty term (information gain about likelihood parameters A), (ii) intrinsic/epistemic value (information gain about hidden states), and (iii) extrinsic value (priors/preferences over outcomes). Policies are selected via a softmax over negative G, so when uncertainty about parameters or states is high epistemic/novelty terms dominate and the agent favors actions that maximize expected information gain (e.g., saccading to informative cue locations); as beliefs become precise extrinsic preferences dominate and the agent exploits. Learning of A is achieved by accumulating Dirichlet concentration parameters from co-occurrences of inferred hidden states and observed outcomes; Bayesian model reduction is used offline to test and prune structure.",
            "environment_name": "Abstract rule-learning simulated environment (three-color/three-location cue task)",
            "environment_characteristics": "Partially observable (hidden state factors must be inferred), discrete state space, episodic trials, stochastic/deterministic outcome mappings determined by unknown likelihood A (initially uniform for some mappings), discrete actions (saccades and report), context-dependent interactions among hidden factors (context-sensitive contingencies), short-horizon trials with limited observations per trial.",
            "environment_complexity": "Hidden state factors: rule (3 levels), correct color (3), where/gaze (4), choice (4) — product state-space combinatorics (up to 3×3×4×4 configurations); action space size = 6 (three location-saccades without choosing + return-and-report actions); trials simulated as up to 6 epochs (they report runs of 32 trials with 6 epochs per trial; instruction encouraged decision after up to 3 cue samples). Only likelihood A was learned; state transitions (B) and initial priors (D) assumed known.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Agent performance in the simulated task improved rapidly: posterior policy probabilities sharpen over trials and choices become accurate; authors report that ambiguity is largely resolved by ~trial 10 and performance becomes perfect by trial 14 in their 32-trial simulation (i.e., 100% correct choices after trial 14). Free energy (model evidence proxy) and uncertainty decline as learning proceeds.",
            "performance_without_adaptation": null,
            "sample_efficiency": "High in this toy task: the agent typically reaches perfect performance after approximately 14 trials (in the reported 32-trial simulation), demonstrating learning of the rule from relatively few trials by actively sampling informative cues. Exact sample counts: ambiguity largely resolved by trial ~10; perfect performance by trial ~14.",
            "exploration_exploitation_tradeoff": "Balanced via decomposition of expected free energy: epistemic/novelty (information gain about parameters and states) drives exploration when posterior uncertainty is high; extrinsic value (prior preferences for being correct and responding by a deadline) drives exploitation when uncertainty is low. Policy selection is a softmax over -G, automatically trading exploration and exploitation according to current posterior beliefs.",
            "comparison_methods": "No explicit baseline comparisons reported in the simulations (e.g., random exploration or UCB) within this paper; method motivated and situated relative to literature on active learning, intrinsic motivation, and Bayesian surprise but no numerical baselines provided.",
            "key_results": "1) Incorporating information gain about likelihood parameters (A) into expected free energy produces curiosity-driven, epistemic policies that actively seek novel state-outcome combinations, enabling rapid learning of context-sensitive abstract rules from few observations. 2) In the simulated rule-learning task the agent actively directed gaze to informative cues and learned the correct mapping such that ambiguity was resolved by ~10 trials and perfect performance achieved by ~14 trials. 3) Bayesian model reduction (structure learning) applied offline to accumulated posterior parameter beliefs can identify simpler, parsimonious models (structure hypotheses) and is associated conceptually with 'insight' or 'aha' moments; this operates without further outcome sampling (fact-free optimization). 4) The formalism naturally integrates epistemic and pragmatic (extrinsic) goals via expected free energy decomposition.",
            "limitations_or_failures": "No quantitative comparisons to alternative adaptive-design algorithms (e.g., random sampling, Thompson sampling, UCB) are provided, so relative advantage is not benchmarked. Simulations are small-scale and domain-specific (abstract rule task); transitions B and priors D were assumed known, and only likelihood A was learned, limiting generality. The paper does not report robustness tests under nonstationary environments, adversarial noise, or large/high-dimensional continuous state spaces; computational costs and scalability are not quantified in this work.",
            "uuid": "e1273.0",
            "source_info": {
                "paper_title": "Active Inference, Curiosity and Insight",
                "publication_date_yy_mm": "2017-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [],
    "cost": 0.00850375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Active Inference, Curiosity and Insight</h1>
<p>Karl J. Friston<br>k.friston@ucl.ac.uk<br>Marco Lin<br>marco.lin91@gmail.com<br>Wellcome Trust Centre for Neuroimaging, Institute of Neurology, University College London WC1N 3BG, U.K.</p>
<p>Christopher D. Frith
c.frith@ucl.ac.uk</p>
<p>Wellcome Trust Centre for Neuroimaging, Institute of Neurology, University
College London WC1N 3BG, and Institute of Philosophy, School of
Advanced Studies, University of London EC1E 7HU, U.K.</p>
<h2>Giovanni Pezzulo</h2>
<p>giovanni.pezzulo@gmail.com
Institute of Cognitive Sciences and Technologies, National Research
Council, 7-00185 Rome, Italy</p>
<h2>J. Allan Hobson</h2>
<p>allan_hobson@hms.harvard.edu
Wellcome Trust Centre for Neuroimaging, Institute of Neurology, University
College London WC1N 3BG, U.K., and Division of Sleep Medicine,
Harvard Medical School, Boston, MA 02215, U.S.A.</p>
<h2>Sasha Ondobaka</h2>
<p>s.ondobaka@ucl.ac.uk</p>
<p>Wellcome Trust Centre for Neuroimaging, Institute of Neurology, University College London WC1N 3BG, U.K.</p>
<p>This article offers a formal account of curiosity and insight in terms of active (Bayesian) inference. It deals with the dual problem of inferring states of the world and learning its statistical structure. In contrast to current trends in machine learning (e.g., deep learning), we focus on how people attain insight and understanding using just a handful of observations, which are solicited through curious behavior. We use simulations of abstract rule learning and approximate Bayesian inference to show that minimizing (expected) variational free energy leads to active sampling of novel contingencies. This epistemic behavior closes explanatory gaps in generative models of the world, thereby reducing uncertainty and</p>
<p>satisfying curiosity. We then move from epistemic learning to model selection or structure learning to show how abductive processes emerge when agents test plausible hypotheses about symmetries (i.e., invariances or rules) in their generative models. The ensuing Bayesian model reduction evinces mechanisms associated with sleep and has all the hallmarks of "aha" moments. This formulation moves toward a computational account of consciousness in the pre-Cartesian sense of sharable knowledge (i.e., con: "together"; scire: "to know").</p>
<h1>1 Introduction</h1>
<p>This article presents a formal (computational) description of epistemic behavior that calls on two themes in theoretical neurobiology. The first is the use of Bayesian principles for understanding the nature of intelligent and purposeful behavior (Koechlin, Ody, \&amp; Kouneiher, 2003; Oaksford \&amp; Chater, 2003; Coltheart, Menzies, \&amp; Sutton, 2010; Nelson, McKenzie, Cottrell, \&amp; Sejnowski, 2010; Collins \&amp; Koechlin, 2012; Solway \&amp; Botvinick, 2012; Donoso, Collins, \&amp; Koechlin, 2014; Seth, 2014; Koechlin, 2015; Lu, Rojas, Beckers, \&amp; Yuille, 2016). The second is the role of self-modeling, reflection, and sleep (Metzinger, 2003; Hobson, 2009). In particular, we formulate curiosity and insight in terms of inference-namely, the updating of beliefs about how our sensations are caused. Our focus is on the transitions from states of ignorance to states of insight-namely, states with (i.e., con) awareness (i.e., scire) of causal contingencies. We associate these epistemic transitions with the process of Bayesian model selection and the emergence of insight. In short, we try to show that resolving uncertainty about the world, through active inference, necessarily entails curious behavior and consequent 'aha' or eureka moments.</p>
<p>The basic theme of this article is that one can cast learning, inference, and decision making as processes that resolve uncertainty about the world. This theme is central to many issues in psychology, cognitive neuroscience, neuroeconomics, and theoretical neurobiology, which we consider in terms of curiosity and insight. The purpose of this article is not to review the large literature in these fields or provide a synthesis of established ideas (e.g., Schmidhuber, 1991; Oaksford \&amp; Chater, 2001; Koechlin et al., 2003; Botvinick \&amp; An, 2008; Nelson et al., 2010; Navarro \&amp; Perfors, 2011; Tenenbaum, Kemp, Griffiths, \&amp; Goodman, 2011; Botvinick \&amp; Toussaint, 2012; Collins \&amp; Koechlin, 2012; Solway \&amp; Botvinick, 2012; Donoso et al., 2014). Our purpose is to show that the issues this diverse literature addresses can be accommodated by a single imperative (minimization of expected free energy, or resolution of uncertainty) that already explains many other phenomena-for example, decision making under uncertainty, stochastic optimal control, evidence accumulation, addiction, dopaminergic responses, habit learning, reversal learning, devaluation, saccadic searches,</p>
<p>scene construction, place cell activity, omission-related responses, mismatch negativity, P300 responses, phase-precession, and theta-gamma coupling (Friston, FitzGerald et al., 2016; Friston, FitzGerald, Rigoli, Schwartenbeck, \&amp; Pezzulo, 2017). In what follows, we ask how the resolution of uncertainty might explain curiosity and insight.
1.1 Curiosity. Curiosity is an important concept in many fields, including psychology (Berlyne, 1950, 1954; Loewenstein, 1994), computational neuroscience, and robotics (Schmidhuber, 1991; Oaksford \&amp; Chater, 2001). Much of neural development can be understood as learning contingencies about the world and how we can act on the world (Saegusa, Metta, Sandini, Sakka, 2009; Nelson et al., 2010; Nelson, Divjak, Gudmundsdottir, Martignon, \&amp; Meder, 2014). This learning rests on intrinsically motivated curious behavior that enables us to predict the consequences of our actions: as nicely summarized by Still and Precup (2012), "A learner should choose a policy that also maximizes the learner's predictive power. This makes the world both interesting and exploitable." This epistemic, worlddisclosing perspective speaks to the notion of optimal data selection and important questions about how rational or optimal we are in querying our world (Oaksford, Chater, Larkin, 2000; Oaksford \&amp; Chater, 2003). Clearly, the epistemic imperatives behind curiosity are especially prescient in developmental psychology and beyond: "In the absence of external reward, babies and scientists and others explore their world. Using some sort of adaptive predictive world model, they improve their ability to answer questions such as what happens if I do this or that?" (Schmidhuber, 2006). In neurorobotics, these imperatives are often addressed in terms of active learning (Markant \&amp; Gureckis, 2014; Markant, Settles, \&amp; Gureckis, 2016), with a focus on intrinsic motivation (Baranes \&amp; Oudeyer, 2009). Active learning and intrinsic motivation are also key concepts in educational psychology, where they play an important role in enabling insight and understanding (Eccles \&amp; Wigfield, 2002).
1.2 Insight and Eureka Moments. The Eureka effect (Auble, Franks, \&amp; Soraci, 1979) was introduced to psychology by comparing the recall for sentences that were initially confusing but subsequently understood. The implicit resolution of confusion appears to be the main determinant of recall and the emotional concomitants of insight (Shen, Yuan, Liu, \&amp; Luo, 2016). Several psychological theories for solving insight problems have been proposed-for example, progress monitoring and representational change theory (Knoblich, Ohlsson, \&amp; Raney, 2001; MacGregor, Ormerod, \&amp; Chronicle, 2001). Both enjoy empirical support, largely from eye movement studies (Jones, 2003). Furthermore, several psychophysical and neuroimaging studies have attempted to clarify the functional anatomy of insight (see Bowden, Jung-Beeman, Fleck, \&amp; Kounios, 2005), for a psychological review and Dresler et al., 2015, for a review of the neural correlates of</p>
<p>insight in dreaming and psychosis). In what follows, we offer a normative framework that complements psychological theories by describing how curiosity engenders insight. Our treatment is framed by two questions posed by Berlyne (1954) in his seminal treatment of curiosity: "The first question is why human beings devote so much time and effort to the acquisition of knowledge. . . . The second question is why, out of the infinite range of knowable items in the universe, certain pieces of knowledge are more ardently sought and more readily retained than others?" (p. 180).</p>
<p>In brief, we will try to show that the acquisition of knowledge and its retention are emergent properties of active inference-specifically, that curiosity manifests as an active sampling of the world to minimize uncertainty about hypotheses-or explanations-for states of the world, while retention of knowledge entails the Bayesian model selection of the most plausible explanation. The first process rests on curious, evidence-accumulating, uncertainty-resolving behavior, while the second operates on knowledge structures (i.e., generative models) after evidence has been accumulated.</p>
<p>Our approach rests on the free energy principle, which asserts that any sentient creature must minimize the entropy of its sensory exchanges with the world. Mathematically, entropy is uncertainty or expected surprise, where surprise can be expressed as a free energy function of sensations and (Bayesian) beliefs about their causes. This suggests that creatures are compelled to minimize uncertainty or expected free energy. In what follows, we will see that resolving different sorts of uncertainty furnishes principled explanations for different sorts of behavior. These levels of uncertainty pertain to plausible states of the world, plausible policies that change those states, and plausible models of those changes.</p>
<p>The first level of uncertainty is about the causes of sensory outcomes under a particular policy (i.e., sequence of actions). Reducing this sort of uncertainty corresponds to perceptual inference (a.k.a. state estimation). In other words, the first thing we need to do is infer the current state of the world and the context in which we are operating. We then have to contend with uncertainty about policies per se that can be cast in terms of uncertainty about future states of the world, outcomes, and the probabilistic contingencies that bind them. We will see that minimizing these three forms of expected surprise-by choosing an uncertainty resolving policycorresponds to information-seeking epistemic behavior, goal-seeking pragmatic behavior, and novelty-seeking curious behavior, respectively. In short, by pursuing the best policy, we accumulate experience and reduce uncertainty about probabilistic contingencies through epistemic learningnamely, inferring (the parameters of our models of) how outcomes are generated.</p>
<p>Finally, curious, novelty-seeking policies enable us to reduce our uncertainty about our generative models per se, leading to structure learning, insight, and understanding. Here, a generative model constitutes a hypothesis about how observable outcomes are generated, where we entertain</p>
<p>Table 1: Sources of Uncertainty Scored by (Expected) Free Energy and the Behaviors Entailed by Its Minimization (Resolution of Uncertainty through Approximate Bayesian Inference).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Source of <br> Uncertainty</th>
<th style="text-align: center;">Free Energy <br> (Surprise)</th>
<th style="text-align: center;">Minimization</th>
<th style="text-align: center;">Active Inference</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Uncertainty about hidden states given a policy</td>
<td style="text-align: center;">$F(\pi)=F\left(\partial, \mathbf{s}_{\mathrm{f}}^{\pi}, \mathbf{a} \mid \pi\right)$</td>
<td style="text-align: center;">With respect to expected states $\mathbf{s}_{\mathrm{f}}^{\pi}$</td>
<td style="text-align: center;">Perceptual inference (state estimation)</td>
</tr>
<tr>
<td style="text-align: center;">Uncertainty about policies in terms of expected: Future states (intrinsic value) Future outcomes (extrinsic value) Model parameters (novelty)</td>
<td style="text-align: center;">$\begin{gathered} G(\pi)=G\left(\mathbf{s}<em _mathrm_f="\mathrm{f">{\mathrm{f}}^{\pi}, \mathbf{a} \mid \pi\right)= \ \mathbf{o}</em>}}^{\pi} \cdot \overline{\mathbf{o}<em _mathrm_f="\mathrm{f">{\mathrm{f}}^{\pi}+\mathbf{H} \cdot \mathbf{s}</em>}}^{\pi}+ \ \mathbf{o<em _mathrm_f="\mathrm{f">{\mathrm{f}}^{\pi} \cdot \mathbf{C}</em>}}+ \ \mathbf{o<em _mathrm_f="\mathrm{f">{\mathrm{f}}^{\pi} \cdot \mathbf{W} \cdot \mathbf{s}</em>$}}^{\pi} \end{gathered</td>
<td style="text-align: center;">With respect to policies $\pi$</td>
<td style="text-align: center;">Epistemic planning Intrinsic motivation Extrinsic motivation Curiosity</td>
</tr>
<tr>
<td style="text-align: center;">Uncertainty about model parameters given a model</td>
<td style="text-align: center;">$F\left(\partial, \mathbf{s}_{\mathrm{f}}^{\pi}, \boldsymbol{\pi}, \mathbf{a} \mid m\right)$</td>
<td style="text-align: center;">With respect to parameters a</td>
<td style="text-align: center;">Epistemic learning (active learning)</td>
</tr>
<tr>
<td style="text-align: center;">Uncertainty about the model</td>
<td style="text-align: center;">$F\left(\partial, \mathbf{s}_{\mathrm{f}}^{\pi}, \boldsymbol{\pi}, \mathbf{a} \mid m\right)$</td>
<td style="text-align: center;">With respect to model $m$</td>
<td style="text-align: center;">Structure learning (insight and understanding)</td>
</tr>
</tbody>
</table>
<p>competing hypotheses that are, a priori, equally plausible. In short, the last level of uncertainty reduction entails the selection of models that render outcomes the least surprising, having suppressed all other forms of uncertainty. All but the last process require experience to resolve uncertainty about either the states (inference) or parameters (learning) of a particular model. However, optimization of the model per se can proceed in a factfree, or outcome-free, fashion, using experience accumulated to date. In other words, no further facts or outcomes are necessary for this last level of optimization: facts and outcomes are constitutive of the experience on which this optimization relies. It is this Bayesian model selection we associate with fact-free learning (Aragones, Gilboa, Postlewaite, \&amp; Schmeidler, 2005) and the emergence of insight (Bowden et al., 2005).</p>
<p>Table 1 provides a summary of these uncertainty-reducing processes, where uncertainty is associated with free energy formulations of surprise such that uncertainty-resolving behavior reduces expected free energy. To motivate and illustrate this formalism, we set ourselves the task of simulating a curious agent that spontaneously learned rules-governing the sensory consequences of her action-from limited and ambiguous sensory evidence (Lu et al., 2016; Tervo, Tenenbaum, \&amp; Gershman, 2016). We chose abstract rule learning to illustrate how conceptual knowledge could be</p>
<p>accumulated through experience (Botvinick \&amp; Toussaint, 2012; Zhang \&amp; Maloney, 2012; Koechlin, 2015) and how implicit Bayesian belief updating can be accelerated by applying Bayesian principles not to sensory samples but to beliefs based on those samples. This structure learning (Tenenbaum et al., 2011; Tervo et al., 2016) is based on recent developments in Bayesian model selection, namely, Bayesian model reduction (Friston, Litvak et al., 2016). Bayesian model reduction refers to the evaluation of reduced forms of a full model to find simpler (reduced) models using only posterior beliefs (Friston \&amp; Penny, 2011). Reduced models furnish parsimonious explanations for sensory contingencies that are inherently more generalizable (Navarro \&amp; Perfors, 2011; Lu et al., 2016) and, as we will see, provide for simpler and more efficient inference. In brief, we use simulations of abstract rule learning to show that context-sensitive contingencies, which are manifest in a high-dimensional space of latent or hidden states, can be learned using straightforward variational principles (i.e., minimization of free energy). This speaks to the notion that people "use their knowledge of real-world environmental statistics to guide their search behavior" (Nelson et al., 2014). We then show that Bayesian model reduction adds an extra level of inference, which rests on testing plausible hypotheses about the structure of internal or generative models. We will see that this process is remarkably similar to physiological processes in sleep, where redundant (synaptic) model parameters are eliminated to minimize model complexity (Hobson \&amp; Friston, 2012). We then show that qualitative changes in model structure emerge when Bayesian model reduction operates online during the assimilation of experience. The ensuing optimization of model evidence provides a plausible (Bayesian) account of abductive reasoning that looks very much like an "aha" moment. To simulate something akin to an aha moment requires a formalism that deals explicitly with probabilistic beliefs about states of the world and its causal structure. This contrasts with the sort of structure or manifold learning that predominates in machine learning (e.g., deep learning; LeCun, Bengio, \&amp; Hinton, 2015), where the objective is to discover structure in large data sets by learning the parameters of neural networks. This article asks whether abstract rules can be identified using active (Bayesian) inference, following a handful of observations and plausible, uncertainty-reducing hypotheses about how sensory outcomes are generated.
1.3 Active Inference and the Resolution of Uncertainty. Active inference is a corollary of the free energy principle that tries to explain action and perception in terms of minimizing variational free energy. Variational free energy is a proxy for surprise or (negative) Bayesian model evidence. This means that minimizing free energy corresponds to avoiding surprises or maximizing model evidence, and minimizing expected free energy corresponds to resolving uncertainty. The active aspect of active inference emphasizes that we are the embodied authors of our sensations. This means</p>
<p>that the consequences of action must themselves be inferred (Baker, Saxe, \&amp; Tenenbaum, 2009). In turn, this implies that we have (prior) beliefs about our behavior. Active inference assumes that the only (self-consistent) prior belief is that we will minimize free energy; in other words, we (believe we) will resolve uncertainty through active sampling of the world (Friston, Mattout, \&amp; Kilner, 2011; Friston et al., 2015). Alternative prior beliefs can be discounted by reductio ad absurdum: if we do not believe that we will resolve uncertainty through active inference, and active inference realizes beliefs by minimizing uncertainty (i.e., fulfilling expectations), then active inference will not minimize uncertainty.</p>
<p>From a technical perspective, this article introduces generalizations of active inference for discrete state-space models (i.e., hidden Markov models and Markov decision processes) along two lines, both concerning the parameters of generative models that encode probabilistic contingencies. First, posterior beliefs about both hidden states and parameters are included in expected free energy, leading to epistemic or exploratory behavior that tries to resolve ignorance, in addition to risk and ambiguity. In other words, policies acquire epistemic value in virtue of resolving uncertainty about states and outcomes (risk and ambiguity) or resolving uncertainty about contingencies (ignorance)-in other words, "what happens if I do this or that?" (Schmidhuber, 2006). Second, we consider minimizing the free energy of the model per se (as opposed to model parameters), in terms of prior beliefs about which parameters are necessary to explain observed outcomes and which parameters are redundant and can be eliminated. As with our previous treatments of active inference, we pay special attention to biological plausibility and try to link optimization to neuronal processes. These developments can be regarded as rolling back the implications of minimizing variational free energy under a generic internal or generative model of the world.
1.4 Overview. This article has three sections. The first briefly reviews active inference and relates the underlying objective function (expected free energy) to established notions like utility, mutual information, and Bayesian surprise. The second describes the paradigm used in this article. In brief, we require agents to learn an abstract rule, in which the correct response is determined by the color of a cue whose location is determined by another cue. By transcribing task instructions into the prior beliefs of a simulated subject, we examine how quickly the rule can be learned-and how this epistemic learning depends on curious, uncertainty-reducing behavior that resolves ignorance (about the meaning of cues), ambiguity (about the context or rule in play), and risk (of making a mistake). In the third section, we turn to Bayesian model reduction or structure learning and consider the improvement in free energy-and performance-when competing hypotheses about the mapping between hidden states and outcomes are tested against the evidence of experience (Nelson et al., 2010). This evidence is accumulated by posterior beliefs over parameters and can be examined</p>
<p>offline to simulate sleep and the emergence of eureka moments. We conclude with a brief illustration of communicating prior beliefs to others (i.e., sharing of knowledge) and discuss the implications for active inference and artificial intelligence.</p>
<h1>2 Active Inference and Free Energy</h1>
<p>Active inference assumes that every characteristic (variable) of an agent minimizes variational free energy (Friston, 2013). This leads to some surprisingly simple update rules for perception, planning, and learning. In principle, the active inference scheme described in this section can be applied to any paradigm or choice behavior. It has been used to model waiting games (Friston et al., 2013), two-step maze tasks (Friston et al., 2015), evidence accumulation in the urn task (FitzGerald, Schwartenbeck, Moutoussis, Dolan, \&amp; Friston, 2015), trust games from behavioral economics (Moutoussis, Trujillo-Barreto, El-Deredy, Dolan, \&amp; Friston, 2014), addictive behavior (Schwartenbeck, FitzGerald, Mathys, Dolan, Wurst, Kronbichler, \&amp; Friston, 2015), saccadic eye movements in scene construction (Mirza, Adams, Mathys, \&amp; Friston, 2016), and engineering benchmarks such as the mountain car problem (Friston, Adams, \&amp; Montague, 2012). It has also been used with computational fMRI (Schwartenbeck, FitzGerald, Mathys, Dolan, \&amp; Friston, 2015). In short, the simulations used to illustrate the emergence of curiosity and insight below follow from a single principle: the minimization of free energy (i.e., surprise) or maximization of model evidence.</p>
<p>Active inference rests on a generative model of observed outcomes. This model is used to infer the most likely causes of outcomes in terms of expected states of the world. These states are called latent, or hidden because they can only be inferred through observations that are usually limited. Crucially, observations depend on action (e.g., where you are looking), which requires the generative model to entertain expectations about outcomes under different sequences of action (i.e., policies). Because the model generates the consequences of action, it must have expectations about future states. These expectations are optimized by minimizing variational free energy, which renders them the most likely states of the world given current observations. Crucially, the prior probability of a policy depends on the free energy expected when pursuing that policy. The (expected) free energy is a proxy for uncertainty and has a number of familiar special cases, including expected utility, epistemic value, Bayesian surprise, and mutual information. After evaluating the expected free energy of each policy; and implicitly their posterior probabilities, the most likely action can be selected. This action generates a new outcome, and the (perception action) cycle starts again.</p>
<p>The resulting behavior represents a principled sampling of sensory cues that has both epistemic and pragmatic aspects. Generally, behavior in an</p>
<p>ambiguous context is dominated by epistemic imperatives until there is no further uncertainty to resolve and pragmatic (prior) preferences predominate. At this point, explorative behavior gives way to exploitative behavior. In this article, we are interested in epistemic behavior, and use prior preferences only to establish a task or instruction set-namely, report a choice when sufficiently confident.</p>
<p>The formal description of active inference that follows introduces many terms and expressions that might appear a bit daunting at first reading. However, most of the technical material represents a standard treatment of Markov decision processes in terms of belief propagation or variational message passing that has been described in a series of previous papers. Furthermore, the simulations reported in this article and previous papers use exactly the same routines (see the software note at the end of the article). We have therefore tried to focus on the essential ideas (and variables) to provide an accessible and basic account of active inference, so that we can focus on curiosity (epistemic novelty-seeking behavior) and insight (Bayesian model reduction). People who want a more detailed account of the basic active inference scheme can refer to Table 2 (for a full glossary of terms described in the appendix) and Friston, FitzGerald et al. (2016) and Friston et al. (2017).
2.1 The Generative Model. Figure 1 provides a schematic specification of the generative model used for the sorts of problems considered in this article. This model is described in more detail in the appendix. In brief, outcomes at any particular time depend on hidden states, while hidden states evolve in a way that depends on action. The generative model is specified by two sets of high-dimensional matrices or arrays. The first, $A^{m}$, maps from hidden states to the $m$ th outcome or modality-for example, exteroceptive (e.g., visual) or proprioceptive (e.g., eye position) modalities. These parameters encode the likelihood of an outcome given their hidden causes. The second set, $\mathbf{B}^{n}$, prescribes transitions among the $n$th factor of hidden states, under an action specified by the current policy. ${ }^{1}$ These hidden factors correspond to different attributes of the world, like the location, color, or category of an object. ${ }^{2}$ The remaining parameters encode prior beliefs about future outcomes $\mathbf{C}^{m}$ and initial states $\mathbf{D}^{n}$. The probabilistic mappings or contingencies are generally parameterized as Dirichlet distributions, whose sufficient statistics are concentration parameters. Concentration parameters can be thought of as counting the number of times a particular combination of</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Generative model and (approximate) posterior. A generative model specifies the joint probability of outcomes or consequences and their (latent or hidden) causes. Usually the model is expressed in terms of a likelihood (the probability of consequences given causes) and priors over causes. When a prior depends on a random variable, it is called an empirical prior. Here, the likelihood is specified by a high-dimensional array $\mathbf{A}$ whose components are the probability of an outcome under every combination of hidden states. The empirical priors in this instance pertain to transitions among hidden states $\mathbf{B}$ that may depend on action, where actions are determined probabilistically in terms of policies (sequences of actions denoted by $\pi$ ). The key aspect of this generative model is that policies are more probable a priori if they minimize the (path integral of) expected free energy G. Bayesian model inversion refers to the inverse mapping from consequences to causes-estimating the hidden states and other variables that cause outcomes. In variational Bayesian inversion, one has to specify the form of an approximate posterior distribution, which is provided in the lower panel. This particular form uses a mean-field approximation in which posterior beliefs are approximated by the product of marginal distributions over hidden states or factors. Here, a mean-field approximation is applied both to posterior beliefs at different points in time and factors. (See the appendix and Table 2 for a detailed explanation of the variables.) The Bayesian network (right panel) provides a graphical representation of the dependencies implied by the equations on the left. Here (and in subsequent figures), $t$ denotes the current time point, and $\tau$ indexes all possible time points.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Schematic overview of belief updating. The left panel lists the belief updates mediating perception (i.e., state estimation), policy selection, and learning; while the right panel assigns the updates to various brain areas. This attribution is purely schematic and serves to illustrate a crude functional anatomy. Here, we have assigned observed outcomes to visual representations in the occipital cortex, with visual (what) modalities entering a ventral stream and proprioceptive (where) modalities originating a dorsal stream. Auditory feedback is associated with the auditory cortex. Hidden states encoding context have been associated with the hippocampal formation and association (parietal) cortex. The evaluation of policies, in terms of their (expected) free energy, has been placed in the caudate. Expectations about policies, assigned to the putamen, are used to create Bayesian model averages of future outcomes (e.g., in the frontal eye fields and supplementary motor area). Finally, expected policies specify the most likely action (e.g., via the deep layers of the superior colliculus). The arrows denote message passing among the sufficient statistics of each factor or marginal. The appendix and Table 2 explain the equations and variables.
states and outcomes has been observed. In this article, we focus on learning the likelihood model and therefore assume that state transitions and initial states are known (or have been learned).</p>
<p>The generative model in Figure 1 means that outcomes are generated in the following way. First, a policy is selected using a softmax function of the expected free energy for each policy. Sequences of hidden states are generated using the probability transitions specified by the selected policy. Finally, these hidden states generate outcomes in one or more modalities. Figure 2 (left panel) provides a graphical summary of the dependencies</p>
<p>implied by the generative model in Figure 1. Perception or inference about hidden states (i.e., state estimation) corresponds to inverting a generative model given a sequence of outcomes, while learning corresponds to updating the parameters of the model. Perception therefore corresponds to optimizing expectations of hidden states and policies with respect to variational free energy, while learning corresponds to accumulating concentration parameters. These constitute the sufficient statistics of posterior beliefs, usually denoted by the probability distribution $Q(x)$, where $x=\delta, \pi, A$ are hidden or unknown quantities.
2.2 Variational Free Energy and Inference. In variational Bayesian inference, model inversion entails minimizing variational free energy with respect to the sufficient statistics of approximate posterior beliefs. These beliefs are approximate because they assume the posterior can be factorized into marginal distributions-here, over hidden states at each point in time, policies, and parameters. This is known as a mean-field assumption (see the factorization of the approximate posterior in the lower right panel of Figure 1). The ensuing minimization of free energy with respect to posterior beliefs can be expressed as follows (see Table 2 for a glossary of expressions):</p>
<p>$$
\begin{aligned}
Q(x) &amp; =\arg \min <em Q="Q">{Q(x)} F \
&amp; \approx P(x \mid \delta) \
F &amp; =E</em>[\ln Q(x)-\ln P(\delta, x)] \
&amp; =\underbrace{D[Q(x) | P(x \mid \delta)]}<em _log="{log" _text="\text" evidence="evidence">{\text {divergence }}-\underbrace{\ln P(\delta)}</em> \
&amp; =\underbrace{D[Q(x) | P(x)]}}<em Q="Q">{\text {complexity }}-\underbrace{E</em>
\end{aligned}
$$}[\ln P(\delta \mid x)]}_{\text {accuracy }</p>
<p>where $\delta=\left(o_{1}, \ldots, o_{t}\right)$ denotes observations up to the current time. Because the (Kullback-Leibler, KL) divergence cannot be less than zero, the penultimate equality means that free energy is minimized when the approximate posterior is the true posterior. At this point, the free energy becomes the negative log evidence for the generative model (Beal, 2003). This means that minimizing free energy is equivalent to maximizing model evidence, which is equivalent to minimizing the complexity of accurate explanations for observed outcomes.</p>
<p>Minimizing free energy ensures expectations encode posterior beliefs, given observed outcomes. However, beliefs about policies rest on future outcomes. This means that policies should, a priori, minimize the free energy expected in the future (Friston et al., 2015). This can be formalized as</p>
<p>follows (see the appendix):</p>
<p>$$
\begin{aligned}
P(\pi)= &amp; \sigma(-G(\pi)), \
G(\pi)= &amp; \sum_{\tau} G(\pi, \tau), \
G(\pi, \tau)= &amp; E_{\tilde{Q}}\left[\ln Q\left(\mathbf{A}, s_{\tau} \mid \pi\right)-\ln P\left(o_{\tau}, \mathbf{A}, s_{\tau} \mid \delta, \pi\right)\right] \
= &amp; \underbrace{E_{\tilde{Q}}\left[\ln Q(\mathbf{A})-\ln Q\left(\mathbf{A} \mid s_{\tau}, o_{\tau}, \pi\right)\right]}<em _tilde_Q="\tilde{Q">{\text {(negative) novelty }} \
&amp; +\underbrace{E</em>}}\left[\ln Q\left(o_{\tau} \mid \pi\right)-\ln Q\left(o_{\tau} \mid s_{\tau}, \pi\right)\right]<em _tilde_Q="\tilde{Q">{\text {(negative) intrinsic or epistemic value }}-\underbrace{E</em>}}\left[\ln P\left(o_{\tau}\right)\right]<em _tilde_Q="\tilde{Q">{\text {extrinsic or expected value }} \
= &amp; \underbrace{E</em>}}\left[\ln Q(\mathbf{A})-\ln Q\left(\mathbf{A} \mid s_{\tau}, o_{\tau}, \pi\right)\right]<em _tau="\tau">{\text {ignorance }}+\underbrace{D\left[Q\left(o</em>} \mid \pi\right) \mid P\left(o_{\tau}\right)\right]<em _tilde_Q="\tilde{Q">{\text {risk }} \
&amp; +\underbrace{E</em>
\end{aligned}
$$}}\left[H\left[P\left(o_{\tau} \mid s_{\tau}\right)\right]\right]}_{\text {ambiguity }</p>
<p>where $\tilde{Q}=Q\left(o_{\tau}, s_{\tau} \mid \pi\right)=P\left(o_{\tau} \mid s_{\tau}\right) Q\left(s_{\tau} \mid \pi\right)$ is the posterior predictive distribution over hidden states and their outcomes under a particular policy. When comparing the penultimate expressions for expected free energy (see equation 2.2) with the free energy per se (see equation 2.1), one sees that the expected divergence becomes mutual information or information gain (see the appendix). Here, we have associated the information gain about the parameters with novelty and information gain about hidden states with intrinsic or epistemic value (i.e., salience). Similarly, expected log evidence becomes expected or extrinsic value provided we associate the prior preference (log probability) over future outcomes with value. The last equality provides a complementary interpretation in which the expected complexity of parameters and hidden states becomes ignorance and risk, while expected inaccuracy becomes ambiguity. We have chosen to label inverse novelty as ignorance in the sense that novelty affords the opportunity to resolve ignorance (i.e., nescience), namely, uncertainty about the contingencies that underwrite outcomes.</p>
<p>There are several special cases of expected free energy that appeal to (and contextualize) established constructs. For example, maximizing mutual information is equivalent to maximizing (expected) Bayesian surprise (Itti \&amp; Baldi, 2009), where Bayesian surprise is the divergence between posterior and prior beliefs. This can also be interpreted in terms of the principle of maximum mutual information or minimum redundancy (Barlow, 1961; Linsker, 1990; Olshausen \&amp; Field, 1996; Laughlin, 2001). Because mutual information cannot be less than zero, it disappears when the (predictive) posterior ceases to be informed by new observations. This means epistemic</p>
<p>behavior will search out observations that resolve uncertainty about the state of the world (e.g., foraging to resolve uncertainty about the hidden location of prey or fixating on an informative part of a face). However, when there is no posterior uncertainty, and the agent is confident about the state of the world, there can be no further information gain, and epistemic value will be the same for all policies, enabling extrinsic value to dominate (if it did not already). This resolution of uncertainty is closely related to satisfying artificial curiosity (Schmidhuber, 1991; Still \&amp; Precup, 2012) and speaks to the value of information (Howard, 1966), particularly in the context of evincing information necessary to realize rewards or payoffs (see Meder \&amp; Nelson, 2012). (See also Nelson et al., 2010, who compare different models of information gain in explaining perceptual decisions.) Expected complexity or risk is exactly the same quantity minimized in risk-sensitive or KL control (Klyubin, Polani, \&amp; Nehaniv, 2005; van den Broek, Wiegerinck, \&amp; Kappen, 2010), and underpins related (free energy) formulations of bounded rationality based on complexity costs (Braun, Ortega, Theodorou, \&amp; Schaal, 2011; Ortega \&amp; Braun, 2013). In other words, minimizing expected complexity renders behavior risk sensitive, while maximizing expected accuracy induces ambiguity-resolving behavior.</p>
<p>The new term introduced by this article is the information gain pertaining to the likelihood mapping between hidden states and outcomes. This term means that policies will be more likely if they resolve uncertaintynot about hidden states - but about how hidden states generate outcomes. Put simply, this means policies that expose the agent to novel combinations of hidden states and outcomes become attractive because they provide evidence for the way that outcomes are generated. In other words, policies that afford a high degree of novelty resolve ignorance about the relationship between causes and consequences. The subsequent resolution of this ignorance or uncertainty lends meaning to outcomes (consequences) in terms of hidden states (causes). This epistemic affordance will be important in what follows.
2.3 Belief Updating. Having defined our objective function, the sufficient statistics encoding posterior beliefs can be updated by minimizing variational free energy. Figure 2 provides these updates. Although the updates look complicated, they are remarkably plausible in terms of neurobiological schemes, as discussed in Friston et al. (2014) and Friston, FitzGerald et al., 2016). The update rules for expected policies (policy selection) and learning are the solutions that minimize free energy, while the updates for expectations over hidden states (for each policy and time) are formulated as a gradient descent. This is important because it provides a dynamical process theory that can be tested against empirical measures of neuronal dynamics. We will see examples of simulated neuronal responses later. Note that the solution for expected policies is a classical softmax function of expected free energy, while learning entails accumulation of concentration</p>
<p>parameters based on the co-occurrence of outcomes and combinations of expected hidden states. Here, the expected hidden states constitute Bayesian model averages over policy-specific expectations (See Friston, FitzGerald et al., 2016) for a more thorough discussion of the neurophysiological implementation of these updates.)</p>
<p>In novel environments, the heavy lifting rests on learning the parameters (and form) of the likelihood mapping. The interesting aspect of these parameters is that they mediate interactions among different hidden states. In other words, they play the role of connections from hidden states to predicted outcomes. From a neurobiological perspective, this means that the connections generating predicted outcomes from expected states (or updating hidden states based on outcomes) are necessarily activity dependent and context sensitive. For example, the first term in the expression for state estimation or perception in Figure 2 is a linear mixture of outcomes formed by a connectivity matrix that itself depends on expectations of over hidden states. In other words, hidden states interact or conspire to generate predictions-or select mixtures of outcomes for Bayesian belief updating. We will return to the importance of these interactions when we consider structure learning.
2.4 Summary. By assuming a generic (Markovian) form for the generative model, it is straightforward to derive Bayesian updates that clarify the interrelationships among perception, planning (i.e., policy selection), and learning. In brief, the agent first infers the hidden states under each policy that it entertains. It then evaluates the evidence for each policy based on observed outcomes and beliefs about future states. Posterior beliefs about each policy are then used to select the next action. The ensuing outcomes are used in conjunction with combinations of expected hidden states to accumulate experience and learn contingencies or model parameters. Figure 2 (right) shows the functional anatomy implied by the belief updating and mean-field approximation in Figure 1. Table 1 lists the sources of uncertainty encoded by (expected) variational free energy and the behaviors entailed by its minimization. As noted in section 1, this formalism provides a nice ontology for perception, planning, and learning where planning or policy selection has distinct novelty, information, and goal-seeking components (driven by novelty, salience, and extrinsic value, respectively). We will use this formalism in the next section to illustrate the behavioral (and electrophysiological) responses that attend rule learning.</p>
<h1>3 Curiosity and Learning</h1>
<p>The first question is why human beings devote so much time and effort to the acquisition of knowledge (Berlyne, 1954).</p>
<p>The (rule-learning) paradigm considered in this section is sufficiently difficult to challenge audiences but simple enough for us to unpack formally. Its agenda is to illustrate curiosity in terms of pursuing policies that afford novelty and the epistemic learning that ensues. The paradigm involves three input modalities (what, where, and feedback) and four sets of hidden states that generate these outcomes-two encoding contextual factors (rule and color) and two hidden states that can be controlled (where and choice; see Figure 3).</p>
<p>In brief, artificial subjects could fixate or attend to a fixation point or one of three cue locations. They were told that the color of the central cue specified a rule that would enable them to report the correct color (red, green, or blue) with a button press (red, green, blue, or undecided). They were told to report the correct color as accurately as possible after looking at three cues or fewer. The rule the subjects had to discover was as follows: the color of the central cue specifies the location of the correct color. For example, if the subject sees red in the center, the correct color is on the left. When demonstrating this task to audiences we usually say something like:</p>
<p>On each trial, we will present three colored dots, arranged around a central fixation point. Your task is to choose the correct color. The dots will be red, blue, or green, and dots of the same color can appear together. All we will tell you is that there is a rule that enables you to identify the correct color on every trial-and that this rule is indicated by the color of the central dot. To make things interesting, you can see only one dot at a time, and we expect a decision after you have looked at three dots. Here is the first trial, which color do you think is correct?</p>
<p>Clearly, we did not instruct our synthetic subject verbally. These instructions were conveyed via prior beliefs about the likelihood of outcomes and prior preferences over a feedback modality. These prior preferences ensured that the subject believed that she was unlikely to be wrong and that she was highly likely to decide after the third epoch (i.e., she was likely to comply with task instructions, even if this entailed choosing the wrong color). These prior beliefs were coded in terms of negative value (i.e., Cost) in a feedback modality; $m=3$, with three levels (undecided, right, and wrong):</p>
<p>$$
\mathbf{C}<em _tau="\tau">{\tau}^{m}=-\ln P\left(o</em>
4 &amp; o_{\tau}^{m}=\text { wrong }: \forall \tau&gt;0, m=3 \
8 &amp; o_{\tau}^{m}=\text { undecided }: \forall \tau&gt;3, m=3 \
0 &amp; \text { otherwise }
\end{array}\right.
$$}^{m}\right)=\left{\begin{array}{ll</p>
<p>In addition to the (visual) color and (auditory) feedback modalities, subjects also received a (proprioceptive) feedback signaling where they were currently looking. Here, $\tau$ indicates the number of saccades or sampled cues in each trial.</p>
<p>The hidden state space induced by the above instructions has four factors: the subject knew that there were three rules; three correct colors (red, green, or blue); where they were looking (left, center, right, or fixation); and their choice (red, green, blue, or undecided). We equipped subjects with six actions: they could look at (or attend to) any of the cue locations without making a choice, or they could return to the fixation point and report their chosen color. In these simulations, policies were very simple and comprised the past sequence of actions plus one of the six actions above. (See Figure 3 for a schematic depiction of the implicit hidden state space.)</p>
<p>The transition matrices were also simple. The first two are identity matrices, because the context (rule and color) states do not change within a trial. In what follows, each trial begins with a new set of cues and comprises a sequence of epochs, where an epoch corresponds to the belief updating following each new observation (e.g., saccadic eye movement). The remaining (where and choice) probability transition matrices depend on action, where the action invariably changes the hidden state to where the subject looks or the choice she makes:</p>
<p>$$
\begin{aligned}
&amp; \mathbf{B}<em i="i" j="j">{i j}^{1,2}(u)= \begin{cases}1 &amp; i=j, \forall u \
0 &amp; i \neq j, \forall u\end{cases} \
&amp; \mathbf{B}</em>1 &amp; i=u, \forall j \
0 &amp; i \neq u, \forall j\end{cases}
\end{aligned}
$$}^{3,4}(u)= \begin{cases</p>
<p>Finally, prior beliefs about the initial states were uniform distributions apart from the sampled location and choice, which was always looking at the fixation point prior to making a choice $\mathbf{D}^{n}=[0, \ldots, 0,1]$.</p>
<p>The only outstanding parameters of the generative model are the concentration parameters of the likelihood $\mathbf{A}^{m}$ that link hidden states and outcomes. The agent effectively knew the mapping to where and feedback outcomes, in the sense that we made the concentration parameters high for the correct contingencies (with a value of 128) and zero elsewhere. In other words, the agents knew that feedback depended on choosing the correct color. Furthermore, we used informative concentration parameters to tell the subject that each of the three rules determined the color of the central cue. However, the agents did not know how the rule determined outcomes. This ignorance corresponds to uniform concentration parameters (of one) in the mapping between the correct color and the color seen at each location, under all three levels of the rule. The important parts of the resulting likelihood array are shown in Figure 3 (right inset panel). Here, we have tiled matrices mapping from the correct color (red, green, blue) to the visual outcome (red, green, blue, gray) for each location sampled (columns) and rule (rows). This arrangement reveals the contingencies generating outcomes. For example, if the agent is looking at the central location, she will see a unique color under each rule (middle column: red, green, and blue for</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" />
each of the three rules). Although the color sampled at the central location signifies the rule for this trial, the subject has no concept of what this rule means (see the uniform priors on either side of the central fixation in Figure 3). This means that the subject believes, a priori, there is no relationship between the correct color and the color observed.</p>
<p>This completes our specification of the subject's generative model. An important aspect of this formulation is that we were able to transcribe task instructions or intentional set into prior beliefs. This suggests that one can regard task instructions as a way of inducing prior beliefs in an experimental setting. After instilling these prior beliefs, the synthetic subject knows quite a lot about the structure of the problem but nothing about its solution. In other words, she knows the number of hidden states and their levels and mappings between some hidden states and others. However, this knowledge is not sufficient to avoid surprising outcomes: making mistakes. Notice that we have been able to specify a fairly complete model of a paradigm, including where subjects look, when they expect to respond, and the sensory modalities entailed. This may appear to be overkill; however, it allows us to make specific predictions about behavior that can be tested empirically. Furthermore, it shows how purposeful, epistemic behavior can emerge under minimal assumptions. In what follows, we will see abstract problem solving and rule learning emerge from the minimization</p>
<p>of expected free energy (i.e., expected surprise or uncertainty) under prior beliefs that make indecisive or erroneous choices surprising.
3.1 The Rule. Hitherto, we have just specified the generative model used by an agent. Clearly, to generate outcomes, we have to specify the true generative process. This is identical to the generative model with one exception: the mapping from states to outcomes contains the causal structure or rule that the subject will learn. As noted, the actual rule used to generate outcomes is as follows: the rule (left, center, right) specifies the location of the correct color. For example, if the subject sees red in the center, the correct color is on the left. However, if she sees green in the center, the correct color is in the center, which is always green. The corresponding part</p>
<p>Figure 3: Graphical representation of the generative model (Left) The Bayesian network shows the conditional dependencies implied by the generative model in Figure 1. The variables in open circles constitute (hyper) priors, while the blue circles contain random variables. This format shows how outcomes are generated from hidden states that evolve according to probabilistic transitions, which depend on policies. The probability of a particular policy being selected depends on its expected free energy. The left panels show the particular hidden states and outcome modalities used to model rule learning. Here, there are three output modalities comprising colored visual cues (what), proprioceptive cues signaling the direction of gaze (where), and (auditory) cues providing feedback (feedback). These three sorts of outcomes are generated by the interactions among four hidden states or factors: an abstract rule indicating the location of an informative color cue (rule); the correct color (colour), where attention or saccadic eye movements are directed (where); and a (manual) response (choice). Hidden states interact to specify outcomes in each modality. In other words, each combination of hidden states has an associated column in the likelihood array that specifies the relative likelihood of outcomes in each modality. For example, if the rule is left, the correct color is red, and the subject is looking at the left cue, the what outcome will be red and the where outcome will be left. (Right) The panel on the upper right shows an example of a trial, where a subject looks from the starting position to the central location, sees a red cue, and subsequently looks to the left. After she has seen a green cue, she knows the correct color and returns to the start position, while indicating her choice (green). The "?" denotes an undecided choice state (and feedback). The matrices (lower left panel) show the likelihood mapping between hidden states and (color) outcomes assumed, a priori, (right) and used to generate actual outcomes (left). These matrices show the likelihood mapping from hidden states to what outcomes-the $\mathbf{A}$ array for the first or what modality. This is a five-dimensional array, of which four dimensions are shown under the undecided level of the choice factor. In other words, these are the contingencies in play until a decision is made. These parameters are shown as a block matrix with $3 \times 3$ blocks (rule times where). Each block shows the $4 \times 3$ matrix mapping the correct color to the outcome. (See Figure 7 for further details.)</p>
<p>of the likelihood mapping (under the undecided state) is shown in Figure 3 (left inset panel). In contrast to the prior beliefs, the true likelihoods mean that if one is looking to the left, the observed color maps to the correct color under the left rule, and similarly when looking to the right. This true likelihood also contains some redundancy. For example, if the subject looks at the central cue when the correct color is red, she will still see a green cue. However, this combination of hidden states never occurs because, a posteriori, a central green cue means the correct color is green (and this is encoded in the likelihood mapping under decided states). It should be noted that the (synthetic) subject does not "know" about these contingencies in an explicit or even subpersonal sense. These contingencies are encoded in model (concentration) parameters that can be associated with synaptic efficacy or connection strengths in the brain.</p>
<p>The rule above may sound simple, but it introduces interesting context sensitivity or interactions among the hidden factors causing outcomes. For example, the outcome depends on a two-way interaction between the correct color and where the subject is looking, but only when the rule is right or left. The subject has to learn these contingencies by accumulating coincidences of inferred states and outcomes. However, this is not a simple problem because agents do not have complete knowledge about hidden states and therefore do not know which states are responsible for generating outcomes. Imagine you had to identify these interactions by designing a multifactorial experiment. You would then manipulate hidden states and record the outcomes. However, there is a problem: you do not know the hidden states (because they are hidden from direct observation). In other words, our synthetic subject has to learn the parameters, while inferring the hidden states. So how does she fare using active inference?</p>
<p>Figure 4 shows the results of simulating 32 trials, where each trial comprises six epochs in which the subject can sample a new cue or make a choice. The upper two panels summarize performance in terms of posterior expectations over policies (top panel) and the final outcomes (second panel). The colored dots indicate the rule for each trial (upper panel) and the final outcomes (second panel). A key point to observe is that the final outcome is usually correct. This is because the subject is allowed to change her mind after making a mistake. These choices are based on posterior expectations about policies that are initially ambiguous and become more precise with learning. By the end of each trial, only the last three policies are entertained (choosing red, green, or blue). In the first trial, two options are entertained with equal probability, but by the 10th trial, any ambiguity appears to be resolved. After the 14th trial, performance becomes perfect. Although there is no definitive phase transition or aha moment, these results suggest that the rule is dawning on the agent.</p>
<p>The lower panels illustrate the implicit transition from ignorance to understanding after trial 14 (highlighted in blue). The second and third panels</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Simulated responses during learning. This figure reports the behavioral responses during 32 successive trials. The first panel shows the first (rule) hidden state as colored circles and subsequent policy selection (in image format) over the policies considered. Darker means more probable. There are six policies corresponding to a saccade to each of the three locations (without making a choice) and three choices (while moving back to the starting location). These policy expectations reflect the fact that at the end of the trial, a choice is always made with greater or lesser confidence, as reflected in the relative probability of the final three policies. The second panel reports the final outcomes (encoded by colored circles) and performance measures in terms of expected cost (see equation 3.1 and Table 1), summed over time (black bars). The red bars indicate mistakes (i.e., the incorrect color is chosen at some point). The lower two panels report the free energy at the end of each trial and fluctuations in confidence as learning proceeds. The cyan region indicates the onset of confident and correct responses.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Parameter matrices in bold denote known parameters. In this article, we consider that all model parameters are known (or have been learned), with the exception of the likelihood mapping; namely, the $A$ parameters.
${ }^{2}$ Implicit in this notation is the factorization of hidden states into factors, whose transitions can be modeled with separate probability transition matrices. This means that the transitions among the levels or states of one factor do not depend on another factor. For example, the way an object moves does not depend on its color.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
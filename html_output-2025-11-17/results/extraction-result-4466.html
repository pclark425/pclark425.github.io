<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4466 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4466</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4466</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-267740701</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.10767v2.pdf" target="_blank">Inference to the Best Explanation in Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> While Large Language Models (LLMs) have found success in real-world applications, their underlying explanatory process is still poorly understood. This paper proposes IBE-Eval, a framework inspired by philosophical accounts on Inference to the Best Explanation (IBE) to advance the interpretation and evaluation of LLMs' explanations. IBE-Eval estimates the plausibility of natural language explanations through a combination of explicit logical and linguistic features including: consistency, parsimony, coherence, and uncertainty. Extensive experiments are conducted on Causal Question Answering (CQA), where \textit{IBE-Eval} is tasked to select the most plausible causal explanation amongst competing ones generated by LLMs (i.e., GPT 3.5 and Llama 2). The experiments reveal that IBE-Eval can successfully identify the best explanation with up to 77\% accuracy ($\approx 27\%$ above random), improving upon a GPT 3.5-as-a-Judge baseline ($\approx+17\%$) while being intrinsically more efficient and interpretable. Additional analyses suggest that, despite model-specific variances, LLM-generated explanations tend to conform to IBE criteria and that IBE-Eval is significantly correlated with human judgment, opening up opportunities for future development of automated explanation verification tools.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4466.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4466.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IBE-Eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inference to the Best Explanation Evaluation Framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An interpretable, automatically computable framework inspired by philosophical IBE for scoring the plausibility of natural-language explanations produced by LLMs via explicit linguistic and logical features (consistency, parsimony, coherence, uncertainty) combined with a learned linear scoring model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>IBE-Eval</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute explicit features for each LLM-generated explanation (logical consistency via autoformalization+Prolog entailment, parsimony via proof depth and concept drift, coherence via stepwise NLI entailment scores, and linguistic uncertainty via sentence-level uncertainty and hedge-detection), then combine these features with a linear regression model θ fitted on training examples to produce a plausibility score for each candidate explanation and select the highest-scoring hypothesis.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Logical consistency (deductive entailment between premise+explanations → conclusion), parsimony (proof depth, concept drift), coherence (average stepwise entailment for each If→Then step), linguistic uncertainty (sentence-level hedging/uncertainty scores and hedge cue distributions).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-Turbo-3.5, LLaMA 2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-3.5 (proprietary, unspecified), LLaMA 2 (13B and 7B)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Commonsense causal reasoning / causal question answering (CQA)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Causal explanations / hypothesis-selection in multiple-choice causal QA</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Composed IBE-Eval (linear combination of features) correctly selected the best explanation with up to 77% accuracy (≈27 percentage points above random 50% and ≈17 pp above a GPT-3.5-as-a-judge baseline). IBE-Eval's plausibility scores have a Spearman correlation of 0.64 (p < 0.01) with human judgments on sampled explanation pairs; GPT-3.5-as-a-judge had a weaker correlation (~0.31, not statistically significant). Individual criteria: linguistic uncertainty and parsimony/coherence were stronger predictors; logical consistency alone provided limited signal.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid: fully automated feature computation and scoring (autoformalization+Prolog, NLI, RoBERTa uncertainty, POS-based noun drift) with validation against human judgments and an LLM-as-judge baseline; ablation studies and regression analysis used to quantify contribution of each feature.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Fitted linear regression on COPA train + sampled E-CARE train; regression analyses and ablation studies; correlation (Spearman) with human judges (Spearman ρ=0.64, p<0.01); inter-annotator agreement for human baseline (Cohen's kappa = 0.68); comparison with GPT-3.5-as-a-judge and random baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Does not evaluate factual groundedness (can accept factually incorrect but logically consistent explanations); consistency metric is weak in isolation because LLMs can over-rationalize incorrect hypotheses; framework applied only to multiple-choice CQA (not single open explanations); requires autoformalization quality; current scores not globally calibrated; some IBE virtues (e.g., unification power, hardness to variation) not implemented and are hard to operationalize.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>COPA (500 examples) and E-CARE (random 500-sample from 2k test); multiple-choice causal QA datasets used for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Inference to the Best Explanation in Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4466.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4466.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Logical Consistency (autoformalization + Prolog)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logical Consistency via Autoformalization and Prolog Entailment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A metric that judges whether an explanation permits a deductive proof from premise to conclusion by translating If-Then steps to Prolog rules (autoformalization with GPT-3.5) and attempting entailment with a Prolog solver (NLProlog) using backward chaining and weak unification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Logical consistency (autofomalization + Prolog entailment)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Translate natural-language If→Then explanation steps and premise/conclusion into Prolog rules, atoms, and a query (GPT-3.5 used for translation); run NLProlog to attempt to satisfy the query via backward chaining; if a proof/path is found, mark explanation as logically consistent and record proof depth and weak unification score.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Binary entailment success (proof exists), plus derived quantities used elsewhere (proof depth = number of rules traversed; weak unification score from NLProlog).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (used for autoformalization); NLProlog (external solver)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-3.5 (proprietary, unspecified); NLProlog (n/a)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Commonsense causal reasoning / general natural-language explanations</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Causal entailment / mechanistic If→Then explanations</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Logical consistency was found to be a weak standalone predictor: consistency scores were similar for explanations tied to correct and incorrect options, because LLMs often over-rationalize and produce logically consistent proofs even for less plausible hypotheses; consistency accuracy ≈51–55% across conditions (near chance).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (neuro-symbolic pipeline combining LLM-driven autoformalization with symbolic solver); validated against human judgments as part of composed IBE-Eval.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Measured contribution in regression/ablation analyses and compared consistency scores between correct vs incorrect explanations; consistency alone provided limited predictive power in correlation/regression tests.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Relies on accurate autoformalization; susceptible to over-rationalization where additional premises introduced produce spurious proofs; does not assess factual truth of premises or assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Inference to the Best Explanation in Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4466.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4466.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Parsimony</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Parsimony (Proof Depth and Concept Drift)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Operationalization of the parsimony IBE virtue using two quantitative metrics: proof depth (number of Prolog rules traversed in a deductive proof) and concept drift (count of novel nouns introduced by the explanation not present in premise/conclusion).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Parsimony (Proof Depth & Concept Drift)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Proof depth is computed from the NLProlog proof as the cardinality of rules traversed to connect premise→conclusion; concept drift is computed by extracting lemmatized nouns with spaCy from premise, conclusion, and explanation and taking the set difference (nouns introduced by the explanation). Smaller depth and lower drift indicate higher parsimony.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Proof depth (Depth(h) = |R|), Concept drift (Drift(h) = |Noun_E − (Noun_p ∪ Noun_c)|).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NLProlog for depth; spaCy used for POS/noun extraction; autoformalization via GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Commonsense causal reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Causal explanations / hypothesis parsimony</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Parsimony correlated negatively with incorrect hypotheses: incorrect options had on average ~6% greater proof depth and ~10% greater concept drift across LLMs; parsimony alone improved predictive power over consistency and was among the top predictors (≈14% improvement over consistency in some comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated metric computation; validated in regression/ablation analysis and correlation with human-preference alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Ablation studies, regression analysis on COPA and E-CARE; compared average depth and drift between explanations tied to correct vs incorrect answers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Counts nouns as proxy for conceptual novelty which may miss paraphrases or multiword concepts; proof depth depends on solver behavior and autoformalization fidelity; more complex but correct explanations may legitimately have greater depth.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Inference to the Best Explanation in Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4466.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4466.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Coherence (Stepwise Entailment)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Coherence via Stepwise Entailment Strength</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A metric measuring the plausibility of each intermediate If→Then step by computing entailment strength between the 'If' clause and the 'Then' clause using a fine-tuned NLI model, then averaging across steps to obtain an overall coherence score.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Coherence (Stepwise Entailment Score)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each explanation step s = (If_s, Then_s), feed the two clauses into a RoBERTa-based NLI model to obtain entailment and contradiction probabilities; compute entailment strength as entailment_prob − contradiction_prob; average these strengths across all steps to get SWE(S). Higher SWE indicates stronger internal coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Step-wise entailment strength for each If→Then; average across steps gives the coherence metric.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-based NLI model (fine-tuned on SNLI, aNLI, multilingual NLI, FEVER-NLI)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Commonsense causal reasoning / natural-language explanations</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Causal and intermediate-step coherence in explanations</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Coherence was a consistent predictor of explanation quality: average coherence scores were higher for correct hypotheses across models and datasets; coherence provided a notable accuracy increase when included in the composed model.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated metric (NLI-based); validated by regression/ablation analysis and correlation with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Regression analysis and ablation studies comparing coherence scores for correct vs incorrect explanations across LLMs (statistically significant for several model/task combinations).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Aggregated average can hide weak internal entailments in longer explanations; NLI model limitations and domain shift can affect scores; may not detect factual errors independent of entailment strength between clauses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Inference to the Best Explanation in Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4466.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4466.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linguistic Uncertainty</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linguistic Uncertainty (Sentence-level Uncertainty + Hedge Detection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A metric that quantifies the degree of hedging and uncertainty in generated explanations using a sentence-level RoBERTa uncertainty regressor and a token-level hedge cue classifier (epistemic/doxatic/conditional), summed across assumptions and summary.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Linguistic Uncertainty (U_C overall)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute uncertainty scores for each assumption sentence and the explanation summary using a RoBERTa-based model (trained by Pei & Jurgens 2021, inverted to give uncertainty); sum assumption scores and summary score to get U_C_overall. Additionally, detect and categorize hedge cues using a BERT token-classifier (CoNLL-2010-style categories) to analyze hedge distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Sentence-level uncertainty scores and frequency/type distribution of hedge cues (epistemic, doxatic, conditional).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-based sentence uncertainty model (Pei & Jurgens 2021); BERT-based hedge cue classifier</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Commonsense causal reasoning; linguistic analysis of explanations</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Assessment of expressed certainty in explanations/hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Linguistic uncertainty was the strongest single predictor of explanation quality across all LLMs: higher uncertainty negatively correlated with QA accuracy. Using uncertainty alone improved accuracy by up to ~13 percentage points on COPA and ~4 pp on E-CARE. Hedge cues were predominantly epistemic across models.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (RoBERTa regressor + token classifier) validated against human judgments via correlation and ablation analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Regression and ablation studies, correlation with human judgment; analysis of hedge cue distributions in incorrect explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Sentence-level uncertainty models trained on scientific/newspaper text may not perfectly transfer to short commonsense explanations; hedging can be used strategically by models (or by humans) for rhetorical reasons, not exclusively to indicate weaker truth.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Inference to the Best Explanation in Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4466.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4466.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linear IBE Scoring (θ)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linear Regression Scoring Model (θ) for Explanation Plausibility</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple linear regression model trained on extracted IBE features to map those features to the probability that an explanation corresponds to the correct answer; used to compose the individual criteria into a single plausibility score.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Linear regression composition (θ)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Fit a linear regression model (scikit-learn) on extracted features from COPA train plus sampled E-CARE train examples, labeling explanations as 1 (correct) or 0 (incorrect); use learned weights to score each candidate explanation at test time and choose argmax as predicted answer.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Weighted linear combination of computed features (consistency binary, depth, drift, coherence score, uncertainty score, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>scikit-learn linear regression (θ)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Commonsense causal reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Scoring/comparison of alternative causal explanations</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>The linear composition of features produced the final IBE-Eval score and led to up to 77% accuracy in selecting the best explanation; ablation showed adding coherence, parsimony and uncertainty improved selection performance over consistency alone.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated scoring model trained and validated against human judgments; hybrid evaluation pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Trained on COPA train + sample of E-CARE train; validated with test sets and correlated with human judgments (Spearman ρ=0.64).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Simple linear model may not capture nonlinear interactions between features; training set limited in size; calibration across diverse domains not addressed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Inference to the Best Explanation in Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4466.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4466.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-a-Judge Baseline (GPT-3.5 as judge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline evaluation method where an LLM (GPT-3.5) is prompted to compare two competing explanations and select which is more plausible, used as a comparison point for automated IBE-Eval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LLM-as-a-Judge (GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Present the LLM judge with both generated explanations and ask it to identify the best and most plausible explanation; treat the judge's choice as an automated human proxy for explanation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>LLM judge's binary selection between competing explanations (prompted preference).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (judge)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>proprietary, unspecified</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Commonsense causal reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Comparative evaluation of explanations</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>GPT-3.5-as-a-judge baseline accuracy ~59% on some settings (see paper table) and exhibited a weak and statistically insignificant Spearman correlation (~0.31) with human judgments. IBE-Eval outperformed this baseline by ≈17 percentage points on average.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (LLM-based) baseline compared against human evaluators and IBE-Eval.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Compared judge selections to human annotations and IBE-Eval; reported correlations and accuracy comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>LLM judges can be inconsistent and may not align with human preferences; can be biased by prompt phrasing; in this work performed worse than IBE-Eval in aligning to human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Inference to the Best Explanation in Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4466.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4466.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human Evaluator Baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human Annotator Evaluation Baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human judges (two annotators, authors) were asked to select the more plausible explanation from pairs, providing a gold-standard reference for alignment and validation of automated metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Human judgement baseline</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Two human evaluators independently judged sampled pairs of explanations (without seeing the original question/answer) and selected which explanation was more plausible; inter-annotator agreement and accuracy relative to the correct answer were computed.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Human preference for plausibility between competing explanations; measured agreement (Cohen's kappa) and identification rate of the explanation associated with the correct answer.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Commonsense causal reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Comparative plausibility judgments of explanations</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Human evaluators identified the explanation associated with the correct answer 96% of the time on COPA and 91% on E-CARE. Inter-evaluator Cohen's kappa = 0.68 (strong agreement). IBE-Eval correlated well with these judgements (Spearman ρ=0.64).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Human-only baseline used to validate automated metrics and correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Inter-annotator agreement (Cohen's kappa) and direct comparison of automated methods' selections to human choices.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Annotators were authors of the paper (not independent/external raters) and small sample size (100 pairs) was used for correlation analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Inference to the Best Explanation in Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4466.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4466.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Autoformalization (GPT-3.5 → Prolog)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Autoformalization using GPT-3.5 to generate Prolog rules/atoms/queries</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline that uses GPT-3.5 to translate natural-language If-Then explanation steps and premise/conclusion into Prolog rules/atoms and a Prolog query which an external symbolic solver (NLProlog) then uses for entailment verification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Autoformalization to Prolog</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Prompt GPT-3.5 to convert each IF-THEN explanation step into an implication rule and premise/conclusion into grounding atoms; produce a Prolog query from the conclusion; post-process GPT output into a Prolog program consumed by NLProlog for backward-chaining entailment search.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Quality of translation enabling solver to find (or not find) a deductive proof; used to compute consistency and proof depth.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (translation step); NLProlog (solver)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-3.5 proprietary, unspecified</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural-language explanation formalization; commonsense causal reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Formal entailment proofs from natural-language explanations</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Autoformalization enabled automated deductive checking; used successfully in pipeline with low incidence (~2%) of tautological self-evident explanations; however, success depends on translation fidelity and NLProlog's weak unification.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated pipeline validated indirectly via downstream metrics (consistency, proof depth) and correlation with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Error analysis on tautological/self-evident explanations; ablation and regression studies to measure impact of consistency/proof depth derived from autoformalization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Autoformalization errors can cause false negatives/positives; translation relies on LLM reliability; Prolog representation may lose nuance; NLProlog uses weak unification which can both help synonymy and introduce spurious matches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Inference to the Best Explanation in Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Inference to the Best Explanation <em>(Rating: 2)</em></li>
                <li>Autoformalization with large language models <em>(Rating: 2)</em></li>
                <li>Nlprolog: Reasoning with weak unification for question answering in natural language <em>(Rating: 2)</em></li>
                <li>Measuring sentence-level and aspect-level (un)certainty in science communications <em>(Rating: 2)</em></li>
                <li>The CoNLL-2010 shared task: Learning to detect hedges and their scope in natural language text <em>(Rating: 2)</em></li>
                <li>Explaining answers with entailment trees <em>(Rating: 2)</em></li>
                <li>Enhancing ethical explanations of large language models through iterative symbolic refinement <em>(Rating: 1)</em></li>
                <li>Selection-inference: Exploiting large language models for interpretable logical reasoning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4466",
    "paper_id": "paper-267740701",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [
        {
            "name_short": "IBE-Eval",
            "name_full": "Inference to the Best Explanation Evaluation Framework",
            "brief_description": "An interpretable, automatically computable framework inspired by philosophical IBE for scoring the plausibility of natural-language explanations produced by LLMs via explicit linguistic and logical features (consistency, parsimony, coherence, uncertainty) combined with a learned linear scoring model.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "IBE-Eval",
            "evaluation_method_description": "Compute explicit features for each LLM-generated explanation (logical consistency via autoformalization+Prolog entailment, parsimony via proof depth and concept drift, coherence via stepwise NLI entailment scores, and linguistic uncertainty via sentence-level uncertainty and hedge-detection), then combine these features with a linear regression model θ fitted on training examples to produce a plausibility score for each candidate explanation and select the highest-scoring hypothesis.",
            "evaluation_criteria": "Logical consistency (deductive entailment between premise+explanations → conclusion), parsimony (proof depth, concept drift), coherence (average stepwise entailment for each If→Then step), linguistic uncertainty (sentence-level hedging/uncertainty scores and hedge cue distributions).",
            "model_name": "GPT-Turbo-3.5, LLaMA 2",
            "model_size": "GPT-3.5 (proprietary, unspecified), LLaMA 2 (13B and 7B)",
            "scientific_domain": "Commonsense causal reasoning / causal question answering (CQA)",
            "theory_type": "Causal explanations / hypothesis-selection in multiple-choice causal QA",
            "human_comparison": true,
            "evaluation_results": "Composed IBE-Eval (linear combination of features) correctly selected the best explanation with up to 77% accuracy (≈27 percentage points above random 50% and ≈17 pp above a GPT-3.5-as-a-judge baseline). IBE-Eval's plausibility scores have a Spearman correlation of 0.64 (p &lt; 0.01) with human judgments on sampled explanation pairs; GPT-3.5-as-a-judge had a weaker correlation (~0.31, not statistically significant). Individual criteria: linguistic uncertainty and parsimony/coherence were stronger predictors; logical consistency alone provided limited signal.",
            "automated_vs_human_evaluation": "Hybrid: fully automated feature computation and scoring (autoformalization+Prolog, NLI, RoBERTa uncertainty, POS-based noun drift) with validation against human judgments and an LLM-as-judge baseline; ablation studies and regression analysis used to quantify contribution of each feature.",
            "validation_method": "Fitted linear regression on COPA train + sampled E-CARE train; regression analyses and ablation studies; correlation (Spearman) with human judges (Spearman ρ=0.64, p&lt;0.01); inter-annotator agreement for human baseline (Cohen's kappa = 0.68); comparison with GPT-3.5-as-a-judge and random baseline.",
            "limitations_challenges": "Does not evaluate factual groundedness (can accept factually incorrect but logically consistent explanations); consistency metric is weak in isolation because LLMs can over-rationalize incorrect hypotheses; framework applied only to multiple-choice CQA (not single open explanations); requires autoformalization quality; current scores not globally calibrated; some IBE virtues (e.g., unification power, hardness to variation) not implemented and are hard to operationalize.",
            "benchmark_dataset": "COPA (500 examples) and E-CARE (random 500-sample from 2k test); multiple-choice causal QA datasets used for evaluation.",
            "uuid": "e4466.0",
            "source_info": {
                "paper_title": "Inference to the Best Explanation in Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Logical Consistency (autoformalization + Prolog)",
            "name_full": "Logical Consistency via Autoformalization and Prolog Entailment",
            "brief_description": "A metric that judges whether an explanation permits a deductive proof from premise to conclusion by translating If-Then steps to Prolog rules (autoformalization with GPT-3.5) and attempting entailment with a Prolog solver (NLProlog) using backward chaining and weak unification.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Logical consistency (autofomalization + Prolog entailment)",
            "evaluation_method_description": "Translate natural-language If→Then explanation steps and premise/conclusion into Prolog rules, atoms, and a query (GPT-3.5 used for translation); run NLProlog to attempt to satisfy the query via backward chaining; if a proof/path is found, mark explanation as logically consistent and record proof depth and weak unification score.",
            "evaluation_criteria": "Binary entailment success (proof exists), plus derived quantities used elsewhere (proof depth = number of rules traversed; weak unification score from NLProlog).",
            "model_name": "GPT-3.5 (used for autoformalization); NLProlog (external solver)",
            "model_size": "GPT-3.5 (proprietary, unspecified); NLProlog (n/a)",
            "scientific_domain": "Commonsense causal reasoning / general natural-language explanations",
            "theory_type": "Causal entailment / mechanistic If→Then explanations",
            "human_comparison": null,
            "evaluation_results": "Logical consistency was found to be a weak standalone predictor: consistency scores were similar for explanations tied to correct and incorrect options, because LLMs often over-rationalize and produce logically consistent proofs even for less plausible hypotheses; consistency accuracy ≈51–55% across conditions (near chance).",
            "automated_vs_human_evaluation": "Automated (neuro-symbolic pipeline combining LLM-driven autoformalization with symbolic solver); validated against human judgments as part of composed IBE-Eval.",
            "validation_method": "Measured contribution in regression/ablation analyses and compared consistency scores between correct vs incorrect explanations; consistency alone provided limited predictive power in correlation/regression tests.",
            "limitations_challenges": "Relies on accurate autoformalization; susceptible to over-rationalization where additional premises introduced produce spurious proofs; does not assess factual truth of premises or assumptions.",
            "uuid": "e4466.1",
            "source_info": {
                "paper_title": "Inference to the Best Explanation in Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Parsimony",
            "name_full": "Parsimony (Proof Depth and Concept Drift)",
            "brief_description": "Operationalization of the parsimony IBE virtue using two quantitative metrics: proof depth (number of Prolog rules traversed in a deductive proof) and concept drift (count of novel nouns introduced by the explanation not present in premise/conclusion).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Parsimony (Proof Depth & Concept Drift)",
            "evaluation_method_description": "Proof depth is computed from the NLProlog proof as the cardinality of rules traversed to connect premise→conclusion; concept drift is computed by extracting lemmatized nouns with spaCy from premise, conclusion, and explanation and taking the set difference (nouns introduced by the explanation). Smaller depth and lower drift indicate higher parsimony.",
            "evaluation_criteria": "Proof depth (Depth(h) = |R|), Concept drift (Drift(h) = |Noun_E − (Noun_p ∪ Noun_c)|).",
            "model_name": "NLProlog for depth; spaCy used for POS/noun extraction; autoformalization via GPT-3.5",
            "model_size": null,
            "scientific_domain": "Commonsense causal reasoning",
            "theory_type": "Causal explanations / hypothesis parsimony",
            "human_comparison": null,
            "evaluation_results": "Parsimony correlated negatively with incorrect hypotheses: incorrect options had on average ~6% greater proof depth and ~10% greater concept drift across LLMs; parsimony alone improved predictive power over consistency and was among the top predictors (≈14% improvement over consistency in some comparisons).",
            "automated_vs_human_evaluation": "Automated metric computation; validated in regression/ablation analysis and correlation with human-preference alignment.",
            "validation_method": "Ablation studies, regression analysis on COPA and E-CARE; compared average depth and drift between explanations tied to correct vs incorrect answers.",
            "limitations_challenges": "Counts nouns as proxy for conceptual novelty which may miss paraphrases or multiword concepts; proof depth depends on solver behavior and autoformalization fidelity; more complex but correct explanations may legitimately have greater depth.",
            "uuid": "e4466.2",
            "source_info": {
                "paper_title": "Inference to the Best Explanation in Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Coherence (Stepwise Entailment)",
            "name_full": "Coherence via Stepwise Entailment Strength",
            "brief_description": "A metric measuring the plausibility of each intermediate If→Then step by computing entailment strength between the 'If' clause and the 'Then' clause using a fine-tuned NLI model, then averaging across steps to obtain an overall coherence score.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Coherence (Stepwise Entailment Score)",
            "evaluation_method_description": "For each explanation step s = (If_s, Then_s), feed the two clauses into a RoBERTa-based NLI model to obtain entailment and contradiction probabilities; compute entailment strength as entailment_prob − contradiction_prob; average these strengths across all steps to get SWE(S). Higher SWE indicates stronger internal coherence.",
            "evaluation_criteria": "Step-wise entailment strength for each If→Then; average across steps gives the coherence metric.",
            "model_name": "RoBERTa-based NLI model (fine-tuned on SNLI, aNLI, multilingual NLI, FEVER-NLI)",
            "model_size": null,
            "scientific_domain": "Commonsense causal reasoning / natural-language explanations",
            "theory_type": "Causal and intermediate-step coherence in explanations",
            "human_comparison": null,
            "evaluation_results": "Coherence was a consistent predictor of explanation quality: average coherence scores were higher for correct hypotheses across models and datasets; coherence provided a notable accuracy increase when included in the composed model.",
            "automated_vs_human_evaluation": "Automated metric (NLI-based); validated by regression/ablation analysis and correlation with human judgments.",
            "validation_method": "Regression analysis and ablation studies comparing coherence scores for correct vs incorrect explanations across LLMs (statistically significant for several model/task combinations).",
            "limitations_challenges": "Aggregated average can hide weak internal entailments in longer explanations; NLI model limitations and domain shift can affect scores; may not detect factual errors independent of entailment strength between clauses.",
            "uuid": "e4466.3",
            "source_info": {
                "paper_title": "Inference to the Best Explanation in Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Linguistic Uncertainty",
            "name_full": "Linguistic Uncertainty (Sentence-level Uncertainty + Hedge Detection)",
            "brief_description": "A metric that quantifies the degree of hedging and uncertainty in generated explanations using a sentence-level RoBERTa uncertainty regressor and a token-level hedge cue classifier (epistemic/doxatic/conditional), summed across assumptions and summary.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Linguistic Uncertainty (U_C overall)",
            "evaluation_method_description": "Compute uncertainty scores for each assumption sentence and the explanation summary using a RoBERTa-based model (trained by Pei & Jurgens 2021, inverted to give uncertainty); sum assumption scores and summary score to get U_C_overall. Additionally, detect and categorize hedge cues using a BERT token-classifier (CoNLL-2010-style categories) to analyze hedge distributions.",
            "evaluation_criteria": "Sentence-level uncertainty scores and frequency/type distribution of hedge cues (epistemic, doxatic, conditional).",
            "model_name": "RoBERTa-based sentence uncertainty model (Pei & Jurgens 2021); BERT-based hedge cue classifier",
            "model_size": null,
            "scientific_domain": "Commonsense causal reasoning; linguistic analysis of explanations",
            "theory_type": "Assessment of expressed certainty in explanations/hypotheses",
            "human_comparison": null,
            "evaluation_results": "Linguistic uncertainty was the strongest single predictor of explanation quality across all LLMs: higher uncertainty negatively correlated with QA accuracy. Using uncertainty alone improved accuracy by up to ~13 percentage points on COPA and ~4 pp on E-CARE. Hedge cues were predominantly epistemic across models.",
            "automated_vs_human_evaluation": "Automated (RoBERTa regressor + token classifier) validated against human judgments via correlation and ablation analyses.",
            "validation_method": "Regression and ablation studies, correlation with human judgment; analysis of hedge cue distributions in incorrect explanations.",
            "limitations_challenges": "Sentence-level uncertainty models trained on scientific/newspaper text may not perfectly transfer to short commonsense explanations; hedging can be used strategically by models (or by humans) for rhetorical reasons, not exclusively to indicate weaker truth.",
            "uuid": "e4466.4",
            "source_info": {
                "paper_title": "Inference to the Best Explanation in Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Linear IBE Scoring (θ)",
            "name_full": "Linear Regression Scoring Model (θ) for Explanation Plausibility",
            "brief_description": "A simple linear regression model trained on extracted IBE features to map those features to the probability that an explanation corresponds to the correct answer; used to compose the individual criteria into a single plausibility score.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Linear regression composition (θ)",
            "evaluation_method_description": "Fit a linear regression model (scikit-learn) on extracted features from COPA train plus sampled E-CARE train examples, labeling explanations as 1 (correct) or 0 (incorrect); use learned weights to score each candidate explanation at test time and choose argmax as predicted answer.",
            "evaluation_criteria": "Weighted linear combination of computed features (consistency binary, depth, drift, coherence score, uncertainty score, etc.).",
            "model_name": "scikit-learn linear regression (θ)",
            "model_size": null,
            "scientific_domain": "Commonsense causal reasoning",
            "theory_type": "Scoring/comparison of alternative causal explanations",
            "human_comparison": null,
            "evaluation_results": "The linear composition of features produced the final IBE-Eval score and led to up to 77% accuracy in selecting the best explanation; ablation showed adding coherence, parsimony and uncertainty improved selection performance over consistency alone.",
            "automated_vs_human_evaluation": "Automated scoring model trained and validated against human judgments; hybrid evaluation pipeline.",
            "validation_method": "Trained on COPA train + sample of E-CARE train; validated with test sets and correlated with human judgments (Spearman ρ=0.64).",
            "limitations_challenges": "Simple linear model may not capture nonlinear interactions between features; training set limited in size; calibration across diverse domains not addressed.",
            "uuid": "e4466.5",
            "source_info": {
                "paper_title": "Inference to the Best Explanation in Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LLM-as-a-Judge",
            "name_full": "LLM-as-a-Judge Baseline (GPT-3.5 as judge)",
            "brief_description": "A baseline evaluation method where an LLM (GPT-3.5) is prompted to compare two competing explanations and select which is more plausible, used as a comparison point for automated IBE-Eval.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "LLM-as-a-Judge (GPT-3.5)",
            "evaluation_method_description": "Present the LLM judge with both generated explanations and ask it to identify the best and most plausible explanation; treat the judge's choice as an automated human proxy for explanation quality.",
            "evaluation_criteria": "LLM judge's binary selection between competing explanations (prompted preference).",
            "model_name": "GPT-3.5 (judge)",
            "model_size": "proprietary, unspecified",
            "scientific_domain": "Commonsense causal reasoning",
            "theory_type": "Comparative evaluation of explanations",
            "human_comparison": true,
            "evaluation_results": "GPT-3.5-as-a-judge baseline accuracy ~59% on some settings (see paper table) and exhibited a weak and statistically insignificant Spearman correlation (~0.31) with human judgments. IBE-Eval outperformed this baseline by ≈17 percentage points on average.",
            "automated_vs_human_evaluation": "Automated (LLM-based) baseline compared against human evaluators and IBE-Eval.",
            "validation_method": "Compared judge selections to human annotations and IBE-Eval; reported correlations and accuracy comparisons.",
            "limitations_challenges": "LLM judges can be inconsistent and may not align with human preferences; can be biased by prompt phrasing; in this work performed worse than IBE-Eval in aligning to human judgments.",
            "uuid": "e4466.6",
            "source_info": {
                "paper_title": "Inference to the Best Explanation in Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Human Evaluator Baseline",
            "name_full": "Human Annotator Evaluation Baseline",
            "brief_description": "Human judges (two annotators, authors) were asked to select the more plausible explanation from pairs, providing a gold-standard reference for alignment and validation of automated metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Human judgement baseline",
            "evaluation_method_description": "Two human evaluators independently judged sampled pairs of explanations (without seeing the original question/answer) and selected which explanation was more plausible; inter-annotator agreement and accuracy relative to the correct answer were computed.",
            "evaluation_criteria": "Human preference for plausibility between competing explanations; measured agreement (Cohen's kappa) and identification rate of the explanation associated with the correct answer.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Commonsense causal reasoning",
            "theory_type": "Comparative plausibility judgments of explanations",
            "human_comparison": true,
            "evaluation_results": "Human evaluators identified the explanation associated with the correct answer 96% of the time on COPA and 91% on E-CARE. Inter-evaluator Cohen's kappa = 0.68 (strong agreement). IBE-Eval correlated well with these judgements (Spearman ρ=0.64).",
            "automated_vs_human_evaluation": "Human-only baseline used to validate automated metrics and correlations.",
            "validation_method": "Inter-annotator agreement (Cohen's kappa) and direct comparison of automated methods' selections to human choices.",
            "limitations_challenges": "Annotators were authors of the paper (not independent/external raters) and small sample size (100 pairs) was used for correlation analysis.",
            "uuid": "e4466.7",
            "source_info": {
                "paper_title": "Inference to the Best Explanation in Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Autoformalization (GPT-3.5 → Prolog)",
            "name_full": "Autoformalization using GPT-3.5 to generate Prolog rules/atoms/queries",
            "brief_description": "A pipeline that uses GPT-3.5 to translate natural-language If-Then explanation steps and premise/conclusion into Prolog rules/atoms and a Prolog query which an external symbolic solver (NLProlog) then uses for entailment verification.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Autoformalization to Prolog",
            "evaluation_method_description": "Prompt GPT-3.5 to convert each IF-THEN explanation step into an implication rule and premise/conclusion into grounding atoms; produce a Prolog query from the conclusion; post-process GPT output into a Prolog program consumed by NLProlog for backward-chaining entailment search.",
            "evaluation_criteria": "Quality of translation enabling solver to find (or not find) a deductive proof; used to compute consistency and proof depth.",
            "model_name": "GPT-3.5 (translation step); NLProlog (solver)",
            "model_size": "GPT-3.5 proprietary, unspecified",
            "scientific_domain": "Natural-language explanation formalization; commonsense causal reasoning",
            "theory_type": "Formal entailment proofs from natural-language explanations",
            "human_comparison": null,
            "evaluation_results": "Autoformalization enabled automated deductive checking; used successfully in pipeline with low incidence (~2%) of tautological self-evident explanations; however, success depends on translation fidelity and NLProlog's weak unification.",
            "automated_vs_human_evaluation": "Automated pipeline validated indirectly via downstream metrics (consistency, proof depth) and correlation with human judgments.",
            "validation_method": "Error analysis on tautological/self-evident explanations; ablation and regression studies to measure impact of consistency/proof depth derived from autoformalization.",
            "limitations_challenges": "Autoformalization errors can cause false negatives/positives; translation relies on LLM reliability; Prolog representation may lose nuance; NLProlog uses weak unification which can both help synonymy and introduce spurious matches.",
            "uuid": "e4466.8",
            "source_info": {
                "paper_title": "Inference to the Best Explanation in Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Inference to the Best Explanation",
            "rating": 2,
            "sanitized_title": "inference_to_the_best_explanation"
        },
        {
            "paper_title": "Autoformalization with large language models",
            "rating": 2,
            "sanitized_title": "autoformalization_with_large_language_models"
        },
        {
            "paper_title": "Nlprolog: Reasoning with weak unification for question answering in natural language",
            "rating": 2,
            "sanitized_title": "nlprolog_reasoning_with_weak_unification_for_question_answering_in_natural_language"
        },
        {
            "paper_title": "Measuring sentence-level and aspect-level (un)certainty in science communications",
            "rating": 2,
            "sanitized_title": "measuring_sentencelevel_and_aspectlevel_uncertainty_in_science_communications"
        },
        {
            "paper_title": "The CoNLL-2010 shared task: Learning to detect hedges and their scope in natural language text",
            "rating": 2,
            "sanitized_title": "the_conll2010_shared_task_learning_to_detect_hedges_and_their_scope_in_natural_language_text"
        },
        {
            "paper_title": "Explaining answers with entailment trees",
            "rating": 2,
            "sanitized_title": "explaining_answers_with_entailment_trees"
        },
        {
            "paper_title": "Enhancing ethical explanations of large language models through iterative symbolic refinement",
            "rating": 1,
            "sanitized_title": "enhancing_ethical_explanations_of_large_language_models_through_iterative_symbolic_refinement"
        },
        {
            "paper_title": "Selection-inference: Exploiting large language models for interpretable logical reasoning",
            "rating": 1,
            "sanitized_title": "selectioninference_exploiting_large_language_models_for_interpretable_logical_reasoning"
        }
    ],
    "cost": 0.01785225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Inference to the Best Explanation in Large Language Models
2 Mar 2025</p>
<p>Dhairya Dalal d.dalal1@universityofgalway.ie 
SFI Centre for Research and Training in Artificial Intelligence
University of Galway
Ireland</p>
<p>Marco Valentino 
Idiap Research Institute
Switzerland</p>
<p>André Freitas 
Idiap Research Institute
Switzerland</p>
<p>Department of Computer Science
University of Manchester
UK</p>
<p>CRUK-MI
National Biomarker Centre
University of Manchester
UK</p>
<p>Paul Buitelaar 
SFI Centre for Research and Training in Artificial Intelligence
University of Galway
Ireland</p>
<p>Insight SFI Research Centre for Data Analytics
University of Galway
Ireland</p>
<p>Inference to the Best Explanation in Large Language Models
2 Mar 2025B7D9D50229BF53DACD03CDD81B1B4B7FarXiv:2402.10767v2[cs.CL]
While Large Language Models (LLMs) have found success in real-world applications, their underlying explanatory process is still poorly understood.This paper proposes IBE-Eval, a framework inspired by philosophical accounts on Inference to the Best Explanation (IBE) to advance the interpretation and evaluation of LLM explanations.IBE-Eval estimates the plausibility of natural language explanations through a combination of explicit logical and linguistic features including: consistency, parsimony, coherence, and uncertainty.Extensive experiments are conducted on Causal Question Answering (CQA), where IBE-Eval is tasked to select the most plausible causal explanation amongst competing ones generated by the LLM (e.g.GPT 3.5 or LLaMA 2).The experiments reveal that IBE-Eval can successfully identify the best explanation with up to 77% accuracy (≈ 27% above random), improving upon a GPT 3.5-as-a-judge baseline (≈ +17%) while being intrinsically more efficient and interpretable.Additional analysis suggests that, despite LLM-specific variances, generated explanations tend to conform to IBE criteria and that IBE-Eval is significantly correlated with human judgment, opening up opportunities for future development of automated explanation verification tools.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) such as OpenAI's GPT (Brown et al., 2020) and LLaMA (Touvron et al., 2023) have been highly effective across a diverse range of language understanding and reasoning tasks (Liang et al., 2023).While LLM performances have been thoroughly investigated across various benchmarks (Wang et al., 2019;Srivastava et al., 2023;Gao et al., 2023;Touvron et al., 2023), the principles and properties behind their step-wise reasoning process are still poorly understood (Valentino et al., 2021).LLMs are no-toriously black-box and can be difficult to interpret (Chakraborty et al., 2017;Danilevsky et al., 2020).Moreover, the commercialization of LLMs has led to strategic secrecy around model architectures and training details (Xiang, 2023;Knight, 2023).Finally, LLMs are susceptible to hallucinations and adversarial perturbations (Geirhos et al., 2020;Camburu et al., 2020), often producing plausible but factually incorrect answers (Ji et al., 2023;Huang et al., 2023).As the size and complexity of LLM architectures increase, the systematic study of generated explanations becomes crucial to better interpret and validate the LLM's internal inference and reasoning processes (Wei et al., 2022b;Lampinen et al., 2022;Huang and Chang, 2022).</p>
<p>The automatic evaluation of natural language explanations presents several challenges (Atanasova et al., 2023;Camburu et al., 2020).Without resource-intensive annotation (Wiegreffe and Marasovic, 2021;Thayaparan et al., 2020;Dalvi et al., 2021;Camburu et al., 2018), explanation quality methods tend to rely on either weak supervision, where the identification of the correct answer is taken as evidence of explanation quality, or require the injection of domain-specific knowledge (Quan et al., 2024).In this paper, we seek to better understand the LLM explanatory process through the investigation of explicit linguistic and logical properties.While explanations are hard to formalize due to their open-ended nature, we hypothesize that they can be analyzed as linguistic objects, with measurable features that can serve to define criteria for assessing their quality.</p>
<p>Specifically, this paper investigates the following overarching research question: "Can the linguistic and logical properties associated with LLMgenerated explanations be used to qualify the models' reasoning process?".To this end, we propose an interpretable framework inspired by philosophical accounts of abductive inference, also known as Inference to the Best Explanation (IBE) -i.e. the</p>
<p>LLM Explanation 1 (E1)</p>
<p>Step 1: IF someone blows into a balloon, THEN it can cause the balloon to inflate.Assumption: Blowing air into a balloon increases the amount of air inside it, leading to inflation.</p>
<p>Step 2: IF the balloon inflates, THEN it can cause the balloon to expand.Assumption: When a balloon inflates, it stretches and expands in size.</p>
<p>Therefore, since I blew into the balloon, it caused the balloon to inflate, which resulted in its expansion.</p>
<p>Explanation 2 (E2)</p>
<p>Step 1: IF a balloon is pricked, THEN the balloon may deflate.Assumption: Pricking a balloon can cause it to lose air and deflate.</p>
<p>Step 2: IF a balloon deflates, THEN there is a decrease in air pressure inside the balloon.Assumption: When a balloon deflates, the air pressure inside the balloon decreases.</p>
<p>Step 3: IF there is a decrease in air pressure inside the balloon, THEN the external air pressure will cause the balloon to expand.Assumption: When the air pressure inside a balloon is lower than the external air pressure, the balloon will expand.</p>
<p>Therefore, since the balloon was pricked, it may have deflated, resulting in a decrease in air pressure inside the balloon, causing the external air pressure to make the balloon expand Causal Question The balloon expanded.What was the cause?</p>
<p>A) I blew into it.B) I pricked it.</p>
<p>Explanation Prompt</p>
<p>For the provided scenario, identify which option is the most plausible cause of the context.Let's think stepby-step and generate an explanation for each option.</p>
<p>Treat each option as the premise and the provided context as the conclusion.Generate a short step-bystep logical proof that explains how the premise can result in the conclusion.For each step provide an IF-THEN rule and the underlying causal or commonsense assumption.Figure 1: IBE-Eval qualifies LLM-generated explanations with a set of logical and linguistic selection criteria to identify the most plausible hypothesis.The corresponding explanation for each hypothesis is evaluated across the IBE criteria of logical consistency, parsimony, internal coherence, and linguistic uncertainty.A final plausibility score is computed across those features and the hypothesis with highest score is identified as the best explanation.</p>
<p>Consistency</p>
<p>process of selecting among competing explanatory theories (Lipton, 2017).In particular, we aim to measure the extent to which LLM-generated explanations conform to IBE expectations when attempting to identify the most plausible explanation.</p>
<p>To this end, we present IBE-Eval, a framework designed to estimate the plausibility of natural language explanations through a set of explicit logical and linguistic features, namely: logical consistency, parsimony, coherence, and linguistic uncertainty.</p>
<p>To evaluate the efficacy of IBE-Eval, we conduct extensive experiments in the multiple-choice Causal Question Answering (CQA) setting.The overall results and contributions of the paper can be summarized as follows:</p>
<ol>
<li>
<p>To the best of our knowledge, we are the first to propose an interpretable framework inspired by philosophical accounts on Inference to the Best Explanation (IBE) to automatically assess the quality of natural language explanations.</p>
</li>
<li>
<p>We propose IBE-Eval, a framework that can be instantiated with external tools for the automatic evaluation of LLM-generated explanations and the identification of the best explanation in a multiple-choice CQA setting.</p>
</li>
<li>
<p>We provide empirical evidence that LLMgenerated explanations tend to conform to IBE expectations with varying levels of statistical significance correlated to the LLM's size.</p>
</li>
<li>
<p>We additionally find that uncertainty, parsimony, and coherence are the best predictors of plausibility and explanation quality across all LLMs.However, we also find that the LLMs tend to be strong rationalizers and can produce logically consistent explanations even for less plausible candidates, making the consistency metric less effective in practice.</p>
</li>
<li>
<p>IBE-Eval can successfully identify the best explanation supporting the correct answers with up to 77% accuracy (+≈ 27% above random and +≈ 17% over GPT 3.5-as-a-Judge baselines)</p>
</li>
<li>
<p>IBE-Eval is significantly correlated with human judgment, outperforming a GPT3.5-asa-Judgebaseline in terms of alignment with human preferences.</p>
</li>
</ol>
<p>For reproducibility, our code is made available on Github1 to encourage future research in the field.</p>
<p>Inference to the Best Explanation (IBE)</p>
<p>Explanatory reasoning is a distinctive feature of human rationality underpinning problem-solving and knowledge creation in both science and everyday scenarios (Lombrozo, 2012;Deutsch, 2011).Accepted epistemological accounts characterize the creation of an explanation as composed of two distinct phases: conjecturing and criticism (Popper, 2014).The explanatory process always involves a conflict between plausible explanations, which is typically resolved through the criticism phase via a selection process, where competing explanations are assessed according to a set of criteria such as parsimony, coherence, unification power, and hardness to variation (Lipton, 2017;Harman, 1965;Mackonis, 2013;Thagard, 1978Thagard, , 1989;;Kitcher, 1989;Valentino and Freitas, 2022).</p>
<p>As LLMs become interfaces for natural language explanations, epistemological frameworks offer an opportunity for developing criticism mechanisms to understand the explanatory process underlying state-of-the-art models.To this end, this paper considers an LLM as a conjecture device producing linguistic objects that can be subject to criticism.In particular, we focus on a subset of criteria that can be computed on explicit linguistic and logical features, namely: consistency, parsimony, coherence, and uncertainty.</p>
<p>To assess the LLM's alignment to such criteria, we focus on the task of selecting among competing explanations in a multiple-choice CQA setting (Figure 1).Specifically, given a set of competing hypotheses (i.e. the multiple-choice options), H = {h 1 , h 2 , . . ., h n }, we prompt the LLM to generate plausible explanations supporting each hypothesis (Section 3).Subsequently, we adopt the proposed IBE selection criteria to assess the quality of the generated explanations (Section 4).IBE-Eval computes an explanation plausibility score derived from the linear combination of the computed selection criteria.The explanation with the highest score is selected as the predicted answer and additionally assessed as to the extent to which the observable IBE features are correlated with QA accuracy.We hypothesize that IBE-Eval will produce higher scores for the explanation associated with the correct answer and that the IBE criteria should meaningfully differentiate between competing explanations.</p>
<p>Explanation Generation</p>
<p>For the first stage, the LLM is prompted to generate competing explanations for the hypotheses using a modified Chain-of-Thought (CoT) prompt (Wei et al., 2022a).Specifically, the COT prompt is modified to instruct the LLM to produce an explanation for each competing hypothesis (see Figure 1).We adopt a methodology similar to Valentino et al. (2021), where the generated explanation is constrained into an entailment form for the downstream IBE evaluation.In particular, we posit that a valid explanation should demonstrate an entailment relationship between the premise and conclusion which are derived from the question-answer pair.</p>
<p>To elicit logical connections between explanation steps and facilitate subsequent analysis, the LLM is constrained to use weak syllogisms expressed as If-Then statements.Additionally, the LLM is instructed to produce the associated causal or commonsense assumption underlying each explanation step.This output is then post-processed to extract the explanation steps and supporting knowledge for evaluation via the IBE selection criteria.Additional details and examples of prompts are reported in Appendix A.2.</p>
<p>Linguistic &amp; Inference Criteria</p>
<p>To perform IBE, we investigate a set of criteria that can be automatically computed on explicit logical and linguistic features, namely: consistency, parsimony, coherence, and uncertainty.</p>
<p>Consistency.Consistency aims to verify whether the explanation is logically valid.Given a hypothesis, comprised of a premise p i , a conclusion c i , and an explanation consisting of a set of If-Then statements E = s 1 , ..., s i , we define E to be logically consistent if p i ∪ E ⊨ c i .Specifically, an explanation is logically consistent if it is possible to build a deductive proof linking premise and conclusion.</p>
<p>To evaluate logical consistency, we leverage external symbolic solvers along with autoformalization -i.e., the translation of natural language into a formal language (Wu et al., 2022).Specifically, the hypotheses and explanations are formalized into a Prolog program which will attempt to generate a deductive proof via backward chaining (Weber et al., 2019).</p>
<p>To perform autoformalization, we leverage the translation capabilities of GPT 3.5.Specifically, we instruct GPT 3.5 to convert each IF-Then explanation step from the generated explanation into an implication rule and the premise statement into grounding atoms.On the other end, the entailment condition and the conclusion are used to create a Prolog query.The query instructs the Prolog solver to attempt to find a path through the implications rules such that the conclusion be directly connected to the premise.Further details about the autoformalization process can be found in Appendix A.3.</p>
<p>After autoformalization, following recent work on neuro-symbolic integration for LLM explanations (Quan et al., 2024), we adopt an external Prolog solver for entailment verification2 .The explanation is considered consistent if the Prolog solver can satisfy the query and successfully build a deductive proof.Technical details can be found in Appendix A.5.</p>
<p>Parsimony.The parsimony principle, also known as Ockham's razor, favors the selection of the simplest explanation consisting of the fewest elements and assumptions (Sober, 1981).Epistemological accounts posit that an explanation with fewer assumptions tends to leave fewer statements unexplained, improving specificity and alleviating the infinite regress (Thagard, 1978).Further, parsimony is an essential feature of causal interpretability, as only parsimonious solutions are guaranteed to reflect causation in comparative analysis (Baumgartner, 2015).In this paper, we adopt two metrics as a proxy of parsimony, namely proof depth, and concept drift.Proof depth, denoted as Depth, is defined as the cardinality of the set of rules, R, required by the Prolog solver to connect the conclusion to the premise via backward chaining.Let h be a hypothesis candidate composed of a premise p and a conclusion c, and let E be a formalized explanation represented as a set of rules R ′ .The proof depth is the number of rules |R|, with R ⊆ R ′ , traversed during backward chaining to connect the conclusion c to the premise p:
Depth(h) = |R|
Concept drift, denoted as Drif t, is defined as the number of additional concepts and entities, outside the ones appearing in the hypothesis (i.e., premise and conclusion), that are introduced by the LLM to support the entailment.For simplicity, we consider nouns as concepts.Let N = {N oun p , N oun c , N oun E } be the unique nouns found in the premise, conclusion, and explanation steps.Concept drift is the cardinality of the set difference between the nouns found in the explanation and the nouns in the hypothesis:
Drif t(h) = |N oun E − (N oun p ∪ N oun c )|
Intuitively, the parsimony principle would predict the most plausible hypothesis as the one supported by an explanation with the smallest observed proof depth and concept drift.Implementation details can be found in Appendix A.6.</p>
<p>Coherence.Coherence attempts to measure the logical validity at the level of the specific explanation steps.An explanation can be formally consistent on the surface while still including implausible or ungrounded intermediate assumptions.Coherence evaluates the quality of each intermediate If-Then implication by measuring the entailment strength between the If and Then clauses.To this end, we employ a fine-tuned natural language inference (NLI) model.Formally, let S be a set of explanation steps, where each step s consists of an If-Then statement, s = (If s , T hen s ).For a given step s i , let ES(s i ) denote the entailment score obtained via the NLI model between If s and T hen s clauses.The step-wise entailment score SW E(S) is then calculated as the averaged sum of the entailment scores across all explanation steps |S|:
SWE(S) = 1 |S| |S| i=1 ES(s i )
We hypothesize that the LLM should generate a higher coherence score for more plausible hypotheses, as such explanations should exhibit stronger step-wise entailment.Additional details can be found in Appendix A.7.</p>
<p>Uncertainty.Finally, we consider the linguistic certainty expressed in the generated explanation as a proxy for plausibility.Hedging words such as probably, might be, could be, etc typically signal ambiguity and are often used when the truth condition of a statement is unknown or improbable.Pei and Jurgens (2021) found that the strength of scientific claims in research papers is strongly correlated with the use of direct language.In contrast, they found that the use of hedging language suggested that the veracity of the claim was weaker or highly contextualized.</p>
<p>To measure the linguistic uncertainty (U C) of an explanation, we consider the explanation's underlying assumptions (A i ) and the overall explanation summary (S).The linguistic uncertainty score is extracted using the fine-tuned sentencelevel RoBERTa model from Pei and Jurgens (2021).The overall linguistic uncertainty score (U C overall ) is the sum of the assumption and explanation summary scores:
U C overall = U C(A) + U C(S)
Where U C(A) is the sum of the linguistic uncertainty scores (U C(A)) across all the assumptions |A| associated with each explanation step i:
U C(A) = |A| i=1 U C(a i )
and linguistic uncertainty of the explanation summary U C(S).We hypothesize that the LLM will use more hedging language when explaining the weaker hypothesis resulting in a higher uncertainty score.Further details can be found in Appendix A.8.</p>
<p>Inference to Best Explanation</p>
<p>After the IBE criteria are computed for each competing hypothesis, they are used to generate the final explanation plausibility score.We define a simple linear regression model θ(•), which was fitted on a small set of training examples consisting of extracted IBE features to predict the probability that an explanation E i corresponds to the correct answer.Specifically, we employ IBE-Eval to score each generated explanation independently and then select the final answer a via argmax:
a = argmax i [θ(E 1 ), . . . , θ(E n )]
Additional details can be found in Appendix A.9.</p>
<p>Experimental Setting</p>
<p>Causal Question-Answering (CQA) requires reasoning about the causes and effects given an event description.We specifically consider the task of cause and effect prediction in a multiple-choice setting, where given a question and two candidate answers, the LLM must decide which is the most plausible cause or effect.Causal reasoning is a challenging task as the model must both possess commonsense knowledge about causal relationships and consider the event context which would make one option more plausible than the other.For our experiments, we use the Choice of Plausible Alternatives (COPA) (Gordon et al., 2012) and E-CARE (Du et al., 2022) datasets.</p>
<p>COPA.COPA is a multiple-choice commonsense causal QA dataset consisting of 500 train and test examples that were manually generated.Each multiple-choice example consists of a question premise and a set of answer candidates which are potential causes or effects of the premise.COPA is a well-established causal reasoning benchmark that is both a part of SuperGlue (Wang et al., 2019) and the CALM-Bench (Dalal et al., 2023).E-CARE.E-CARE is a large-scale multiplechoice causal crowd-sourced QA dataset consisting of 15K train and 2k test examples.Similar to COPA, the task requires the selection of the most likely cause or effect provided an event description.We randomly sample 500 examples from the E-CARE test set for our experiments.</p>
<p>LLMs.We consider GPT-Turbo-3.5,LLaMA 2 13B, and LLaMA 2 7B for all experiments.GPT 3.5 is a proprietary model (Brown et al., 2020) and is highly effective across a wide range of natural language reasoning tasks (Laskar et al., 2023).We additionally evaluate the open-source LLaMA 2 model (Touvron et al., 2023).We consider both the 13B and 7B variants of Llama 2 as both are seen as viable commodity GPT alternatives and have been widely adopted by the research community for LLM benchmarking and evaluation.</p>
<p>Baselines.We employ LLM-as-a-Judge (Zheng et al., 2023) and human evaluators as baseline methods for the selection of the best explanation in the CQA setting.(Zheng et al., 2023) found LLMs can align with human judgment and be utilized for automated evaluation and judgment.We specifically uses GPT 3.5 as the LLM judge.For each CQA example, we present the judges with two competing explanations generated by the target LLM.The judge is asked to identify the best and most plausible explanation.Additional details about the baselines can be found in Appendix A.4.</p>
<p>Preliminary Analysis</p>
<p>We conduct a preliminary analysis as a sanity check to measure the extent to which LLMs generate selfevident or tautological explanations -i.e., explanations that simply restate the premises and conclusions.Tautological explanations present a risk for IBE-Eval as the metrics would be theoretically uninformative if the LLM adopts the tested causal relation as the explanation itself (e.g.A → B) without providing additional supporting statements.</p>
<p>We consider the parsimony metric to compute the percentage of explanations with proof depth equal to 1 (i.e, explanations containing only one inference step) and concept drift equal to 0 (i.e.no Figure 2: A regression analysis measuring the correlation between IBE criteria and question accuracy.All the LLMs tend to conform to IBE expectations with GPT 3.5 exhibiting the most consistent and significant alignment.Linguistic uncertainty is the strongest IBE predictor for explanation quality, where higher uncertainty is negatively correlated with question accuracy.Statistical significance is noted as: '<em><strong>' p &lt; 0.001, '</strong>' p &lt; 0.01 '</em>' p &lt; 0.05.additional concepts other than the ones stated in premises and conclusions appear in the explanation).In such cases, the LLM is effectively generating a self-evident or tautological explanation.</p>
<p>We found that about 2% of the cases consist of self-evident explanations.For GPT 3.5, LLaMA 2 13B, and LLaMA 2 7B, 2% of the generated explanations exhibit a concept drift of 0, and on average 1.5% of the explanations have a proof depth of 1.We then conducted an error analysis to evaluate the cases where IBE-Eval selected a self-evident explanation as the best one.Across all LLMs, less than 0.1% of the errors were caused by the selection of such explanations.Our analysis suggests that the impact of self-evident explanations is not sig-nificant and that the IBE framework can be robustly applied to identify such cases.</p>
<p>Results</p>
<p>To assess the LLM's alignment with the proposed IBE framework and evaluate the efficacy of IBE-Eval, we run a regression analysis and conduct a set of ablation studies to evaluate the relationship between IBE and question accuracy.The main results are presented in Figure 2 and Table 1.</p>
<p>Our regression analysis finds that the IBE criteria are generally consistent across the LLMs as demonstrated by similar correlation patterns found on both the COPA and E-CARE tasks (Figure 2).GPT 3.5 exhibits the strongest alignment with IBE expectations as we observe nearly all the IBE criteria have statistically significant and directionally aligned correlations across both tasks.Thus our proposed IBE criteria can serve as promising build blocks for future work on automated explanation evaluation.</p>
<p>In Table 1 we evaluate the accuracy of the IBE criteria and IBE-Eval in selecting the most plausible explanation in the CQA setting.We find that though independently the IBE criteria are generally limited in their ability to identify the more plausible explanation -they still outperform the GPT-3.5-asa-judgebaseline.IBE-Eval, which considers all IBE criteria, improves the ability to select the best explanation by 17% over both the GPT 3.5-as-a-judge and random baselines.We can achieve up to 77% accuracy utilizing just the extracted IBE criteria demonstrating IBE's potential value for automatic explanation evaluation.</p>
<p>Next, we explore each explanation feature in further detail to better understand the variances across the IBE criteria and LLMs.</p>
<p>Consistency.We find that the LLMs are surprisingly strong conjecture models.The LLMs can generate logically consistent explanations for any hypothesis as observed by similar consistency scores for correct and incorrect (Figure 3) explanations.Moreover, we observe that consistency tends to be a statistically insignificant predictor for the LLaMA models.Therefore, we conclude that evidence of logical consistency provides a limited signal for plausibility and is better understood in the context of other IBE criteria.For the incorrect candidate explanations, we find that LLMs overrationalize and introduce additional premises to demonstrate entailment in their explanations.</p>
<p>Parsimony.The results suggest that parsimony has a more consistent effect and is a better predictor of explanation quality.We observe negative correlations between proof depth, concept drift, and question-answering accuracy, suggesting that LLMs tend to introduce more concepts and explanation steps when explaining less plausible hypotheses.On average, we found the depth and drift to be 6% and 10% greater for the incorrect option across all LLMs (Figure 4).Moreover, the results suggest that as the LLM parameter size increases, the tendency to over-rationalize increases as well.This is attested by the fact that the average difference in depth and drift is the great-Figure 3: An evaluation of explanation consistency.LLMs are strong rationalizers and can generate logically consistent explanations at equal rates for explanations associated with both correct and incorrect answers options.</p>
<p>Figure 4: Explanation parsimony is evaluated using proof depth and concept drift.Both metrics are consistently lower for explanations supporting the correct answers suggesting that LLMs are able to generate efficient explanations for the more plausible hypothesis.est in GPT 3.5, suggesting that the model tends to find the most efficient explanations for stronger hypotheses and articulates explanations for weaker candidates.Finally, we found that the LLaMA models tend to generate more complex explanations overall, with LLaMA 2 13B exhibiting the largest concept drift for less plausible hypotheses.The parsimony criterion supports the IBE predictive power with an average of 14% improvement over consistency.</p>
<p>Coherence.Similarly to parsimony, we found coherence to be a better indicator of explanation quality being statistically significant for both GPT 3.5 and Llama 2 13B on COPA and both Llama 2 models on E-Care.We found that the average coherence score is consistently greater for the  stronger hypothesis across all LLMs and datasets (see Figure 5).Both GPT and Llama 2 13B exhibit a higher relative difference between the correct and incorrect hypotheses in contrast to Llama 2 7B.</p>
<p>Uncertainty.The results reveal that linguistic uncertainty is the strongest predictor of explanation quality and is a statistically significant feature for all LLMs.This suggests that LLMs use more qualifying language when explaining weaker hypotheses (see Figure 6).We found that uncertainty can improve accuracy by 13pp on COPA and 4pp on E-CARE.We also examine the uncertainty cues expressed by LLMs by analyzing both the frequency of hedge words and the types of hedge cues employed in incorrect explanations.We find the distribution of hedge cues across LLMs tends to be similar, with only minor differences be-tween LLMs (Figure 6).Epistemic cues were most frequently used by all three models, with LLaMA 2 7B being more likely to use conditional cues.See Appendix A.8 for further details..</p>
<p>Correlation with Human Judgement.</p>
<p>We first sample 100 generated explanation pairs across both the COPA and E-CARE tasks and evaluated LLMs.Two human evaluators are instructed to evaluate the pair of explanations and to select which explanation is most plausible.No additional information about the original question nor the correct answer is provided to prevent biasing the judge.</p>
<p>The human evaluators on average were able to identify the explanation associated with the correct answer 96% (COPA) and 91% (E-Care) of time.We compute the inter-evaluator agreement score between two human evaluators and find that there is Cohen's Kappa score of .68 suggesting there is a strong agreement between the two evaluators.</p>
<p>To evaluate if IBE-Eval is correlated with human judgment, we compute the Spearman's rank correlation between GPT-3.5-as-a-judge,IBE-Eval and human judgment.We find that GPT-3.5-as-a-judgeexhibits a weak and statistically insignificant correlation with human judgment (0.31).In contrast, we find that the IBE-Eval is significantly aligned with human preferences (with a Spearman's correlation of 0.64 and p &lt; 0.01) further suggesting the IBE's potential for automatic explanation evaluation.</p>
<p>Related Work</p>
<p>Explorations of LLM reasoning capabilities across various domains (e.g. arithmetic, commonsense, planning, symbolic, etc) are an emerging area of interest (Xu et al., 2023;Huang and Chang, 2023).Prompt-based methods (Wei et al., 2022b;Zhou et al., 2023;Wang et al., 2023), such as CoT, investigate strategies to elicit specific types of reasoning behavior through direct LLM interaction.Olausson et al. (2023) investigate automatic proof generation and propose a neurosymbolic framework with an LLM semantic parser and external solver.Creswell et al. (2022) propose an inference framework where the LLM acts as both a selection and inference module to produce explanations consisting of causal reasoning steps in entailment tasks.Research on LLM faithfulness (Atanasova et al., 2023) investigates if LLM explanations are robust to spurious input alterations.Parcalabescu and Frank (2024) propose a self-consistency measure CC-SHAP which measures how specific alterations to a model's input contribute to the generated explanation.This paper primarily draws inspiration from recent work on the evaluation of natural language explanations (Quan et al., 2024;Valentino et al., 2021;Wiegreffe and Marasovic, 2021;Thayaparan et al., 2020;Dalvi et al., 2021;Camburu et al., 2018).However, differently from previous methods that require extensive human annotations or specific domain knowledge, we are the first to propose a set of criteria that can be automatically computed on explicit linguistic and logical features.</p>
<p>Conclusion</p>
<p>This paper proposed IBE-Eval, an interpretable framework for LLM explanation evaluation inspired by philosophical accounts of Inference to the Best Explanation (IBE).IBE-Eval can identify the best explanation supporting the correct answer with up to 77% accuracy in CQA scenarios, improving upon a GPT 3.5 Judge baselines by +17%.Our regression study suggests that LLM explanations tend to conform to IBE expectations and that IBE-Eval is strongly correlated with human judgment.Linguistic uncertainty is the stronger IBE predictor for explanation quality closely followed by parsimony and coherence.However, we also found that LLMs tend to be strong conjecture models able to generate logically consistent explanations for less plausible hypotheses, suggesting limited applicability for the logical consistency criterion in isolation.We believe our findings can open new lines of research on external evaluation methods for LLMs as well as interpretability tools for understanding the LLM's underlying explanatory process.</p>
<p>10 Limitations IBE-Eval offers an interpretable explanation evaluation framework utilizing logical and linguistic features.Our current instantiation of the framework is primarily limited in that it does not consider grounded knowledge for factuality.We observe that LLMs can generate factually incorrect but logically consistent explanations.In some cases, the coherence metric can identify those factual errors when the step-wise entailment score is comparatively lower.However, our reliance on aggregated metrics can hide weaker internal entailment especially when the explanation is longer or the entailment strength of the surrounding explanation steps is stronger.Future work can introduce metrics to evaluate grounded knowledge or perform more granular evaluations of explanations to better weight factual inaccuracies.</p>
<p>Additionally, IBE-Eval currently does not support single natural language explanations and was evaluated in the limited domain of causal commonsense reasoning.Future work will explore globally calibrating IBE-Eval plausibility scores to extend evaluation to more diverse explanation generation and QA settings.Calibration efforts would allow for IBE-Eval to generate comparable scores across unrelated explanations and could be used to produce global thresholds explanation classification.</p>
<p>Finally, the list of criteria considered in this work is not exhaustive and can be extended in future work.However, additional criteria for IBE might not be straightforward to implement (e.g., unification power, hardness to variation) and would probably require further progress in both epistemological accounts and existing NLP technology.</p>
<p>Ethics Statement</p>
<p>The human annotators for computing the human judgment baseline are all authors of the papers and as such were not further compensated for the annotation task.A modified CoT prompt is used to instruct the LLM to generate explanations.The prompt includes a set of instructions for explanation generation and an in-context example.Appended to the end of the prompt are the CQA context, causal question, and answer candidates.The LLM is instructed to first convert the options into the EEV format consisting of a premise and conclusion.The EEV format will differ depending on the directionality of the causal question (see Figure 7).Cause prediction questions will treat the answer candidate as the premise and the context as the conclusion.In contrast, effect prediction reverses the relationship treating the context as the premise and the answer options as the conclusion.After the EEV conversion, the model is instructed to generate a step-by-step explanation consisting of IF-THEN statements and the associated causal or commonsense assumptions.For ease of post-processing, the LLM is instructed to use headers and enumerate steps using the Step # format.A full example of the prompt template is exhibited in Figure 8.</p>
<p>A.3 Autoformalization</p>
<p>Autoformalization is the process of translating natural language descriptions into formal specifications (Wu et al., 2022).We adopt the translational capabilities of GPT-3.5-Turbo to convert the expla-nation into a formal entailment hypothesis.The IF-THEN explanation steps are converted into a set of Prolog rules, the entailment description is used to generate Prolog atoms, and the conclusion statement is translated into a Prolog query.We provide an example of the autoformalization prompt in Figure 9 and an example of the formalized output in Figure 11.After autoformalization, we deploy a post-processing script to extract the formalized rules, atoms, and query and generate a Prolog program for entailment verification.</p>
<p>A.4 LLM-as-a-Judge Baseline GPT 3.5 is used as the LLM for the LLM-as-a-Judge baseline.Similar to the human evaluators, GPT is presented with both generated explanations and asked to identify which explanation is more plausible.An explanation hypothesis is considered logically consistent if the external solver can build a deductive proof connecting the conclusion to the premise.We use NLProlog (Weber et al., 2019), a neuro-symbolic Prolog solver integrating backward chaining with word embedding models via a weak unification mechanism.NLProlog allows for a level of flexibility and robustness that is necessary for NLP use cases (e.g.unification applied to synonyms).We provide the autoformalized query, atoms, and rules to NLProlog.If NLProlog can satisfy the entailment query, it will return the proof consisting of the set of rules traversed, the weak unification score, and the proof depth.For simplicity, we assign a score of one if the entailment query is satisfied and zero if it is not.The proof depth score is evaluated as part of the parsimony analysis.An end-to-end example of consistency evaluation can be found in Figure 11.</p>
<p>A.5 Logical Consistency</p>
<p>A.6 Parsimony</p>
<p>Parsimony measures the complexity of an explanation and is represented by the proof depth and concept drift metrics.Proof depth is automatically calculated by NLProlog and reflects the number of rules traversed by the solver to satisfy the entailment query.If the hypothesis is not logically consistent, depth is set to zero.The concept drift metric measures the entropy of novel concepts introduced to bridge the premise and conclusion.To compute the drift of an explanation, we consider the nouns found in the premise, conclusion, and explanation steps.We use Spacy (Honnibal and Mon-tani, 2017) to tokenize and extract part-of-speech (POS) tags.All tokens with the 'NOUN' POS tag extracted.For normalization purposes, we consider the lemma of the tokens.Concept drift then is calculated as the set difference between the unique nouns found across all explanation steps and those found in the premise and conclusion.Coherence evaluates the plausibility of the intermediate explanation.We propose stepwise entailment as a metric to measure the entailment strength of the If-then implications.We employ a RoBERTabased NLI model (Nie et al., 2020) that has been finetuned on a range of NLI and fact verification datasets consisting of SNLI (Bowman et al., 2015), aNLI (Nie et al., 2020), multilingual NLI (Williams et al., 2018)), and FEVER-NLI (Nie et al., 2019).To compute the stepwise entailment score, we first measure the entailment strength between the If and Then propositions.For example, to calculate the score of the statement "IF a balloon is pricked, THEN the balloon may deflate" we consider "a balloon is pricked" and "the balloon may deflate" as input sentences for the NLI model.The NLI will produce independent scores for the entailment and contradiction labels.We compute the entailment strength by subtracting the contraction label score from the entailment label score.An entailment strength of one indicates the If-then implication is strongly plausible whereas a score of zero suggests that it is likely implausible.The overall stepwise entailment score is the average of entailment strength measures across all explanation steps.</p>
<p>A.8 Linguistic Uncertainty</p>
<p>Linguistic uncertainty measures the confidence of a statement where hedging cues and indirect language suggest ambiguity around the proposition.To measure sentence-level uncertainty, we employ a finetuned RoBERTa model provided by (Pei and Jurgens, 2021).The model was trained on a sentence-level dataset consisting of findings and statements extracted from new articles and scientific publications and human annotated evaluation of sentence certainty.A scale from one to six was used to annotate sentences where one corresponds to the lowest degree of certainty and six is the highest expressed by the sentence.We invert the scale to retrieve the uncertainty scores.To compute the overall linguistic uncertainty of an explanation, we first compute the uncertainty for each assumption and the explanation summary and then average all the scores.</p>
<p>We use a fine-tuned BERT-based token classification model to classify all the words in the generated explanation with uncertainty categories introduced in the 2010 CoNLL shared task on Hedge Detection (Farkas et al., 2010).Farkas et al. (2010) classify hedge cues into three categories: epistemic, doxatic, and conditional.Epistemic cues refer to hedging scenarios where the truth value of a proposition can be determined but is unknown in the present (e.g. the blocks may fall).Doxatic cues refer to beliefs and hypotheses that can be held to be true or false by others (e.g. the child believed the blocks would fall).Finally, conditional cues refer to propositions whose truth value is dependent on another proposition's truth value (e.g. if the balloon is pricked it may deflate).To perform IBE, we first fit a linear regression model over the extracted explanation features from the COPA train set and 500 random sample train examples from the E-CARE train set.We consider all explanations independently and annotate each explanation with a 1 if it corresponds to a correct answer or 0 if corresponds to an incorrect answer.After the linear model is fitted, we evaluate the COPA and E-CARE test sets.For each example, we use the trained linear model to score each answer candidate explanation and then select a candidate with the highest score.We use the linear regression implementation from scikit-learn (Buitinck et al., 2013) for the IBE model.We additionally use the R stats package (R Core Team, 2013) for conducting our regression analysis.A.10.7 E-CARE Hedge Distribution See Figure 18.</p>
<p>A.11 Causal Directionality</p>
<p>When considering the causal directionality (i.e.cause vs effect), we observed that accuracy tended to differ between LLMs on COPA.In particular, we found both GPT and LLaMA 2 7B to be more accurate in predicting the effects in causal scenarios (see Figure 19).We hypothesize that LLMs may suffer the challenge of causal sufficiency as the space of potential causal explanations can be far greater than the range of effects once an event has been observed.This hypothesis is partly supported by the fact that GPT and LLaMA 2 7B express greater linguistic uncertainty and produce more complex explanations when predicting causes rather than effects.</p>
<p>A.12 Dataset Details</p>
<p>COPA is released under a BSD-2 license and made available for broad research usage with copyright notification restrictions4 .We do not modify or use COPA outside of its intended use which is primarily open-domain commonsense causal reasoning.E-CARE is released under the MIT license and can be used for broad purposes with copyright notification restrictions5 .We do not modify or use E-CARE outside of its intended use which is causal reasoning evaluation of language models.</p>
<p>Figure 5 :
5
Figure5: An evaluation of the explanation coherence and question accuracy.The average coherence score is consistently higher for explanations corresponding to the correct hypotheses across the LLMs.</p>
<p>Figure 6 :
6
Figure6: Evaluation of linguistic uncertainty in LLMgenerated explanations.LLMs tend to use more hedging language in explanations supporting less plausible hypotheses.Across the LLMs, the hedging language is found to be predominantly epistemic A.8.</p>
<p>Figure 8 :
8
Figure 8: An example of the modified CoT prompt template for explanation generation.</p>
<p>Figure 9 :
9
Figure 9: An example of the autoformalization prompt.</p>
<p>Figure 10 :
10
Figure 10: An example of prompt used by the LLM-asa-Judge model for evaluating competing explanations.</p>
<p>Figure 11 :
11
Figure 11: An example of the autoformalization prompt.</p>
<p>Figure 12 :
12
Figure 12: Average consistency comparison between correct and incorrect options for the E-CARE dataset.</p>
<p>Figure 13 :
13
Figure 13: Comparison of average proof depth between correct and incorrect options.</p>
<p>Figure 14 :
14
Figure 14: Comparison of average concept drift between correct and incorrect options.</p>
<p>Figure 15 :
15
Figure 15: Comparison of average coherence scores between correct and incorrect options.</p>
<p>Figure 16 :
16
Figure 16: Comparison of average uncertainty scores between correct and incorrect options.</p>
<p>Figure 17 :
17
Figure 17: Comparison of the average ratio of hedge cues between correct and incorrect options.</p>
<p>Figure 18 :
18
Figure 18: Distribution of hedge cues across incorrect explanations.</p>
<p>Figure 19 :
19
Figure 19: Accuracy in predicting the most plausible causes vs effects on COPA.</p>
<p>Table 1 :
1
An ablation study and evaluation of the IBE criteria and the composed IBE-Eval model.IBE-Eval outperforms the GPT 3.5 Judge baseline by an average of +17.5% across all all models and tasks.
COPAE-CAREGPT 3.5 LlaMA 2 13B LlaMA 2 7B GPT 3.5 LlaMA 2 13B LlaMA 2 7BBaselinesGPT3.5 Judge.59.47.63.43.61.52Human.951.0.91.90.91.92IBE FeaturesConsistency.51.52.55.54.54.54Depth (Parsimony).67.53.63.66.56.54Drift (Parsimony).67.63.58.66.57.57Coherence.66.66.56.56.57.59Linguistic Uncertainty.70.65.61.59.56.60Composed ModelRandom.50.50.50.50.50.50+ Consistency.51.52.55.54.54.54+ Depth.67.53.63.66.56.56+ Drift.70.65.65.72.66.65+ Coherence.73.71.69.73.68.69+ Linguistic Uncertainty.77.74.70.74.70.73
https://github.com/dhairyadalal/ IBE-eval
https://github.com/neuro-symbolic-ai/ explanation_based_ethical_reasoning
https://github.com/dhairyadalal/ IBE-eval
N oun p ← spacy(P remise);
N oun c ← spacy(Conclusion);
N oun E ← spacy(Explanation);
N ← {N oun p , N oun c , N oun E };
drif t ← length(set(N oun E ) − set(N oun p ∪ N oun c ));
return drif t;
people.ict.usc.edu/~gordon/copa.html
github.com/Waste-Wood/e-CARE?tab= MIT-1-ov-file#readme
AcknowledgementsThis work was partially funded by the Swiss National Science Foundation (SNSF) project Neu-Math (200021_204617), by the EPSRC grant EP/T026995/1 entitled "EnnCore: End-to-End Conceptual Guarding of Neural Architectures" under Security for all in an AI-enabled society, by the CRUK National Biomarker Centre, and supported by the Manchester Experimental Cancer Medicine Centre, the Science Foundation Ireland under grants SFI/18/CRT/6223 (Centre for Research Training in Artificial Intelligence), SFI/12/RC/2289_P2 (Insight), co-funded by the European Regional Development Fund, and the NIHR Manchester Biomedical Research Centre.A AppendixA.1 Reproducibility All experimental code is available online 3 to encourage future research in the field.We additionally summarize all the model implementations and technical resources used for the computation of the proposed IBE criteria below:• We adopt the Prolog solver for neurosymbolic integration from(Quan et al., 2024).• We use Spacy(Honnibal and Montani, 2017)to tokenize and extract part-of-speech (POS) tags.• To compute coherence, we employ the RoBERTa-based NLI model(Nie et al., 2020)that has been finetuned on a range of NLI and fact verification datasets consisting of SNLI(Bowman et al., 2015), aNLI(Nie et al., 2020), multilingual NLI(Williams et al., 2018)), and FEVER-NLI(Nie et al., 2019).• To measure sentence-level uncertainty, we employ a finetuned RoBERTa model provided by and Jurgens, 2021).• We use a fine-tuned BERT-based token classification model to classify all the words in the generated explanation with uncertainty categories introduced in the 2010 CoNLL shared task on Hedge Detection(Farkas et al., 2010).A.2 Explanation Prompting
Faithfulness tests for natural language explanations. Pepa Atanasova, Oana-Maria Camburu, Christina Lioma, Thomas Lukasiewicz, Jakob Grue Simonsen, Isabelle Augenstein, 10.18653/v1/2023.acl-short.25Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20232Short Papers)</p>
<p>Michael Baumgartner, Parsimony and causality. Quality &amp; Quantity. 201549</p>
<p>A large annotated corpus for learning natural language inference. R Samuel, Gabor Bowman, Christopher Angeli, Christopher D Potts, Manning, 10.18653/v1/D15-1075Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalAssociation for Computational Linguistics2015</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>API design for machine learning software: experiences from the scikit-learn project. Lars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas Mueller, Olivier Grisel, Vlad Niculae, Peter Prettenhofer, Alexandre Gramfort, Jaques Grobler, Robert Layton, Jake Vanderplas, Arnaud Joly, Brian Holt, Gaël Varoquaux, ECML PKDD Workshop: Languages for Data Mining and Machine Learning. 2013</p>
<p>e-snli: Natural language inference with natural language explanations. Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, Phil Blunsom, Advances in Neural Information Processing Systems. 201831</p>
<p>Make up your mind! adversarial generation of inconsistent natural language explanations. Om Camburu, Shillingford, Minervini, Lukasiewicz, Blunsom, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020. 2020ACL Anthology</p>
<p>Interpretability of deep learning models: A survey of results. Supriyo Chakraborty, Richard Tomsett, Ramya Raghavendra, Daniel Harborne, Moustafa Alzantot, Federico Cerutti, Mani Srivastava, Alun Preece, Simon Julier, M Raghuveer, Troy D Rao, Dave Kelley, Murat Braines, Christopher J Sensoy, Prudhvi Willis, Gurram, 10.1109/UIC-ATC.2017.8397411Scalable Computing &amp; Communications, Cloud &amp; Big Data Computing, Internet of People and Smart City Innovation. 2017. 2017</p>
<p>Selection-inference: Exploiting large language models for interpretable logical reasoning. Antonia Creswell, Murray Shanahan, Irina Higgins, 2022</p>
<p>CALM-bench: A multi-task benchmark for evaluating causality-aware language models. Dhairya Dalal, Paul Buitelaar, Mihael Arcan, 10.18653/v1/2023.findings-eacl.23Findings of the Association for Computational Linguistics: EACL 2023. Dubrovnik, CroatiaAssociation for Computational Linguistics2023</p>
<p>Explaining answers with entailment trees. Bhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan Xie, Hannah Smith, Leighanna Pipatanangkura, Peter Clark, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>A survey of the state of explainable AI for natural language processing. Marina Danilevsky, Kun Qian, Ranit Aharonov, Yannis Katsis, Ban Kawas, Prithviraj Sen, Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing. the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language ProcessingSuzhou, ChinaAssociation for Computational Linguistics2020</p>
<p>The beginning of infinity: Explanations that transform the world. David Deutsch, 2011penguin uK</p>
<ol>
<li>e-CARE: a new dataset for exploring explainable causal reasoning. Li Du, Xiao Ding, Kai Xiong, Ting Liu, Bing Qin, 10.18653/v1/2022.acl-long.33Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, Ireland1Association for Computational Linguistics</li>
</ol>
<p>The CoNLL-2010 shared task: Learning to detect hedges and their scope in natural language text. Richárd Farkas, Veronika Vincze, György Móra, János Csirik, György Szarvas, Proceedings of the Fourteenth Conference on Computational Natural Language Learning -Shared Task. the Fourteenth Conference on Computational Natural Language Learning -Shared TaskUppsala, SwedenAssociation for Computational Linguistics2010</p>
<p>Leo Gao, Jonathan Tow, Stella Baber Abbasi, Sid Biderman, Anthony Black, Charles Dipofi, Laurence Foster, Jeffrey Golding, Alain Hsu, Haonan Le Noac'h, Kyle Li, Mcdonell, 10.5281/zenodo.10256836Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite. Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf; Ben Wang, Kevin Wangand Andy Zou. 2023. A framework for few-shot language model evaluation</p>
<p>Shortcut learning in deep neural networks. Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, Felix A Wichmann, Nature Machine Intelligence. 2112020</p>
<p>SemEval-2012 task 7: Choice of plausible alternatives: An evaluation of commonsense causal reasoning. Andrew Gordon, Zornitsa Kozareva, Melissa Roemmele, *SEM 2012: The First Joint Conference on Lexical and Computational Semantics. Montréal, CanadaAssociation for Computational Linguistics2012. SemEval 20121Proceedings of the Sixth International Workshop on Semantic Evaluation</p>
<p>The inference to the best explanation. The philosophical review. Gilbert H Harman, 196574</p>
<p>spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing. Matthew Honnibal, Ines Montani, 2017To appear</p>
<p>Jie Huang, Kevin Chen, -Chuan Chang, arXiv:2212.10403Towards reasoning in large language models: A survey. 2022arXiv preprint</p>
<p>Jie Huang, Kevin Chen, -Chuan Chang, Towards reasoning in large language models: A survey. 2023</p>
<p>Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. 2023. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. </p>
<p>Survey of hallucination in natural language generation. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye , Jin Bang, Andrea Madotto, Pascale Fung, 10.1145/3571730ACM Comput. Surv. 12552023</p>
<p>Explanatory unification and the causal structure of the world. Philip Kitcher, 1989</p>
<p>Ai is becoming more powerful-but also more secretive. Will Knight, 2023</p>
<p>Can language models learn from explanations in context?. Andrew Lampinen, Ishita Dasgupta, Stephanie Chan, Kory Mathewson, Mh Tessler, Antonia Creswell, James Mcclelland, Jane Wang, Felix Hill, Findings of the Association for Computational Linguistics: EMNLP 2022. 2022</p>
<p>A systematic study and comprehensive evaluation of chatgpt on benchmark datasets. Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, Jimmy Xiangji Huang, 2023</p>
<p>Holistic evaluation of language models. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D Manning, Christopher Ré, Diana Acosta-Navas, Drew A Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, Yuta Koreeda, 2023</p>
<p>Inference to the best explanation. A Companion to the Philosophy of Science. Peter Lipton, 2017</p>
<p>Explanation and abductive inference. Oxford handbook of thinking and reasoning. Tania Lombrozo, 2012</p>
<p>Inference to the best explanation, coherence and other explanatory virtues. Synthese. Adolfas Mackonis, 2013190</p>
<p>Combining fact extraction and verification with neural semantic matching networks. Yixin Nie, Haonan Chen, Mohit Bansal, Association for the Advancement of Artificial Intelligence (AAAI). 2019</p>
<p>Adversarial NLI: A new benchmark for natural language understanding. Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, Douwe Kiela, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics2020</p>
<p>Linc: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers. Theo X Olausson, Alex Gu, Benjamin Lipkin, Cedegao E Zhang, Armando Solar-Lezama, Joshua B Tenenbaum, Roger Levy, 2023</p>
<p>On measuring faithfulness or self-consistency of natural language explanations. Letitia Parcalabescu, Anette Frank, 2024</p>
<p>Measuring sentence-level and aspect-level (un)certainty in science communications. Jiaxin Pei, David Jurgens, 10.18653/v1/2021.emnlp-main.784Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Conjectures and refutations: The growth of scientific knowledge. Karl Popper, 2014routledge</p>
<p>Enhancing ethical explanations of large language models through iterative symbolic refinement. Xin Quan, Marco Valentino, Louise A Dennis, André Freitas, arXiv:2402.007452024arXiv preprint</p>
<p>R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing. 2013Vienna, Austria</p>
<p>The principle of parsimony. Elliott Sober, The British Journal for the Philosophy of Science. 3221981</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S Iyer, Anders Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlmüller, Andrew Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakaş, B Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bartłomiej Bojanowski, Batuhan Özyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Howald, Bryan Orinion, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, César Ferri Ramírez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Chris Waites, Christian Voigt, Christopher D Manning, Christopher Potts, Cindy Ramirez, Clara E Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Moseguí González, Danielle Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito, Dar Gilboa, David Dohan, David Drakard, David Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dimitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Dylan Schrader, Ekaterina Shutova, Ekin Dogus Cubuk, Elad Segal, Eleanor Hagerman, Elizabeth Barnes, Elizabeth Donoway, Ellie Pavlick, Emanuele Rodola, Emma Lam, Eric Chu, Eric Tang, Erkut Erdem, Ernie Chang, Ethan A Chi, Ethan Dyer, Ethan Jerzak, Ethan Kim, Eunice Engefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia, Fatemeh Siar, Fernando Martínez-Plumed, Francesca Happé, Francois Chollet, Frieda Rong, Gaurav Mishra, Genta Indra Winata, Gerard De Melo, Germán Kruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria Wang, Gonzalo Jaimovitch-López, Gregor Betz, Guy Gur-Ari, Hana Galijasevic, Hannah Kim, Hannah Rashkin, Hannaneh Hajishirzi, Harsh Mehta, Hayden Bogar, Henry Shevlin, Hinrich Schütze, Hiromu Yakura, Hongming Zhang, Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, Jackson Kernion, Jacob Hilton, Jaehoon Lee, Jaime Fernández Fisac, James B Simon, James Koppel, James Zheng, James Zou, Jan Kocoń, Jana Thompson, Janelle Wingfield, Jared Kaplan, Jarema Radom, Jascha Sohl-Dickstein, Jason Phang, Jason Wei, Jason Yosinski, Jekaterina Novikova, Jelle Bosscher, Jennifer Marsh, Jeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba Alabi, Jiacheng Xu, Jiaming Song, Jillian Tang, Joan Waweru, John Burden, John Miller, John U Balis, Jonathan Batchelder, Jonathan Berant, Jörg Frohberg, Jos Rozen, Jose Hernandez-Orallo, Joseph Boudeman, Joseph Guerr, Joseph Jones, Joshua B Tenenbaum, Joshua S Rule, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, Katja Markert, D Kaustubh, Kevin Dhole, Kevin Gimpel, Kory Omondi, Kristen Mathewson, Ksenia Chiafullo, Kumar Shkaruta, Kyle Shridhar, Kyle Mc-Donell, Laria Richardson, Leo Reynolds, Li Gao, Liam Zhang, Lianhui Dugan, Lidia Qin, Louis-Philippe Contreras-Ochando, Luca Morency, Lucas Moschella, Lucy Lam, Ludwig Noble, Luheng Schmidt, Luis He, Luke Oliveros Colón, Metz ; Maheen, Manaal Farooqi, Mantas Faruqui, Marco Mazeika, Marco Baturan, Marco Marelli, Maria Maru, Jose Ramírez, Marie Quintana, Mario Tolkiehn, Martha Giulianelli, Martin Lewis, Matthew L Potthast, Matthias Leavitt, Hagen ; Peiyuan, Percy Liao, Peter Liang, Peter Chang, ; Eckersley, S Samuel, Sanghyun Schoenholz, Sanjeev Han, Sarah A Kwatra, Sarik Rous, Sayan Ghazarian, Sean Ghosh, Sebastian Casey, Sebastian Bischoff, Sebastian Gehrmann, Sepideh Schuster, Shadi Sadeghi, Sharon Hamdan, Shashank Zhou, Sherry Srivastava, Shikhar Shi, Singh, Lütfi Kerem Şenel, Maarten Bosma, Maarten Sap, Maartje ter Hoeve. Sahib Singh, Saif M. Mohammad. Pinyu Phu Mon Htut, Piotr Hwang, Piyush Miłkowski, Pouya Patil, Priti Pezeshkpour, Qiaozhu Oli, Qing Mei, Qinlang Lyu, Rabin Chen, Rachel Etta Banjade, Raefer Rudolph, Rahel Gabriel, Habacker, Medina Orduna Baitemirova, Melody Arnaud, Melvin McElrath, Michael A. Yee, Michael Cohen; Nicholas Cameron, Nicholas Roberts, Nick Doiron, Nicole Martinez, Nikita Nangia, Niklas Deckers, Niklas Muennighoff; Omer Levy, Owain Evans, Pablo Antonio Moreno Casares, Parth Doshi; Ramon Risco, Raphaël Millière, Rhythm Garg, Richard Barnes, Rif A. Saurous, Riku Arakawa, Robbe Raymaekers, Robert Frank, Rohan Sikand, Roman Novak, Roman Sitelew, Ronan LeBras, Rosanne Liu, Rowan Jacobs, Rui Zhang; Ryan Chi, Ryan Lee, Ryan Stovall, Ryan Teehan; Sajant Anand, Sam Dillavou, Sam Shleifer, Sam Wiseman; Shima Asaadi; Tal Linzen, TalSamuel Gruetter, Samuel R. BowmanMátyás SchubertSwaroop Mishra</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Tao Schuster, Tao Li, Tariq Yu, Tatsu Ali, Te-Lin Hashimoto, Théo Wu, Theodore Desbordes, Thomas Rothschild, Tianle Phan, Tiberius Wang, Timo Nkinyili, Timofei Schick, Titus Kornev, Tobias Tunduny, Trenton Gerstenberg, Trishala Chang, Tushar Neeraj, Tyler Khot, Uri Shultz, Vedant Shaham, Vera Misra, Victoria Demberg, Vikas Nyamai, Vinay Raunak, Ramasesh, Uday Vinay, Vishakh Prabhu, Vivek Padmakumar, William Srikumar, William Fedus, William Saunders, Wout Zhang, Xiang Vossen, Xiaoyu Ren, Xinran Tong, Xinyi Zhao, Xudong Wu, Yadollah Shen, Yair Yaghoobzadeh, Yangqiu Lakretz, Yasaman Song, Yejin Bahri, Yichi Choi, Yiding Yang, Yifu Hao, Yonatan Chen, Yu Belinkov, Yufang Hou, Yuntao Hou, Zachary Bai, Zhuoye Seid, Zijian Zhao, Zijie J Wang, Zirui Wang, Ziyi Wang, Wu, Behavioral and brain sciences. 1232023Paul Thagard. 1989. Explanatory coherence</p>
<p>The best explanation: Criteria for theory choice. Paul R Thagard, The journal of philosophy. 7521978</p>
<p>Mokanarangan Thayaparan, Marco Valentino, André Freitas, arXiv:2010.00389A survey on explainability in machine reading comprehension. 2020arXiv preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, ; Jian, Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Pushkar Mishra, Igor Molybog. Andrew Nie, Jeremy Poulton, Rashi Reizenstein, Kalyan Rungta, Alan Saladi, Ruan Schelten, Eric Michael Silva, Ranjan Smith, Xiaoqing Subramanian, Ellen Tan, Binh Tang, Ross Taylor, Adina Williams,; Angela Fan, Melanie Kambadur, Sharan Narang; Robert Stojnic, Sergey EdunovAurelien Rodriguezand Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models</p>
<p>Scientific explanation and natural language: A unified epistemological-linguistic perspective for explainable ai. Marco Valentino, André Freitas, arXiv:2205.018092022arXiv preprint</p>
<p>Do natural language explanations represent valid logical arguments? verifying entailment in explainable NLI gold standards. Marco Valentino, Ian Pratt-Hartmann, André Freitas, Proceedings of the 14th International Conference on Computational Semantics (IWCS). the 14th International Conference on Computational Semantics (IWCS)Groningen, The NetherlandsAssociation for Computational Linguistics2021</p>
<p>Superglue: A stickier benchmark for general-purpose language understanding systems. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, CoRR, abs/1905.005372019</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, 2023</p>
<p>Nlprolog: Reasoning with weak unification for question answering in natural language. Leon Weber, Pasquale Minervini, Jannes Münchmeyer, Ulf Leser, Tim Rocktäschel, 2019</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H Chi, Quoc Le, Denny Zhou, CoRR, abs/2201.119032022a</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 2022b35</p>
<p>Teach me to explain: A review of datasets for explainable natural language processing. Sarah Wiegreffe, Ana Marasovic, Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2021</p>
<p>A broad-coverage challenge corpus for sentence understanding through inference. Adina Williams, Nikita Nangia, Samuel Bowman, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational Linguistics20181</p>
<p>Yuhuai Wu, Albert Q Jiang, Wenda Li, Markus N Rabe, Charles Staats, Mateja Jamnik, Christian Szegedy, Autoformalization with large language models. 2022</p>
<p>Openai's gpt-4 is closed source and shrouded in secrecy. Chloe Xiang, 2023</p>
<p>Are large language models really good logical reasoners? a comprehensive evaluation and beyond. Fangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, Jun Liu, and Erik Cambria. 2023</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P Xing, Haotong Zhang, Joseph Gonzalez, Ion Stoica, ArXiv, abs/2306.056852023</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi. 2023. Least-to-most prompting enables complex reasoning in large language models. </p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5384 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5384</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5384</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-db96e019410006c3ee0ae0184800ab206f8704dd</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/db96e019410006c3ee0ae0184800ab206f8704dd" target="_blank">Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work introduces a trained component that retrieves schema elements relevant to the input text that improves the LLMs’ extraction performance in a retrieval-augmented generation-like manner and demonstrates on three KGC benchmarks that EDC is able to extract high-quality triplets without any parameter tuning and with significantly larger schemas compared to prior works.</p>
                <p><strong>Paper Abstract:</strong> In this work, we are interested in automated methods for knowledge graph creation (KGC) from input text. Progress on large language models (LLMs) has prompted a series of recent works applying them to KGC, e.g., via zero/few-shot prompting. Despite successes on small domain-specific datasets, these models face difficulties scaling up to text common in many real-world applications. A principal issue is that, in prior methods, the KG schema has to be included in the LLM prompt to generate valid triplets; larger and more complex schemas easily exceed the LLMs’ context window length. Furthermore, there are scenarios where a fixed pre-defined schema is not available and we would like the method to construct a high-quality KG with a succinct self-generated schema. To address these problems, we propose a three-phase framework named Extract-Define-Canonicalize (EDC): open information extraction followed by schema definition and post-hoc canonicalization. EDC is flexible in that it can be applied to settings where a pre-defined target schema is available and when it is not; in the latter case, it constructs a schema automatically and applies self-canonicalization. To further improve performance, we introduce a trained component that retrieves schema elements relevant to the input text; this improves the LLMs’ extraction performance in a retrieval-augmented generation-like manner. We demonstrate on three KGC benchmarks that EDC is able to extract high-quality triplets without any parameter tuning and with significantly larger schemas compared to prior works. Code for EDC is available at https://github.com/clear-nus/edc.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5384.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5384.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EDC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Extract-Define-Canonicalize</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A three-phase LLM-based pipeline for Knowledge Graph Construction that first performs open information extraction, then prompts LLMs to produce natural-language schema (relation/entity) definitions, and finally canonicalizes relation/entity phrases via vector similarity + LLM verification to produce a concise canonical KG.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Linearized open relational triplets + natural-language schema definitions</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Graphs are represented as lists of relational triplets in the form [Subject, Relation, Object] serialized literally (examples shown as nested lists in prompts). In addition, each induced relation/entity type is paired with a natural-language definition produced by an LLM; these definitions are embedded (sentence transformer) and used to (a) retrieve candidate canonical schema elements by vector similarity and (b) present Multiple-Choice alternatives to an LLM which decides whether and how to map an open phrase to a canonical relation. The canonicalized KG thus uses standardized relation tokens (e.g., birthDate, mission) replacing varied surface forms.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graph (relational triplets)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Open/extractive (no schema required at extraction), post-hoc canonicalization enables scalability to large schemas, human-readable relation definitions (interpretability), reduces redundancy by consolidating semantically-equivalent relation phrases, uses LLM verification to avoid over-generalization (faithfulness), relies on embedding similarity (compactness/accessibility via vector search); requires multiple LLM calls (cost/latency) and separate canonicalization stage (pipeline complexity).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Knowledge Graph Construction (relational triplet extraction) on WebNLG (semantic-parsing), REBEL, and Wiki-NRE; manual evaluation for self-canonicalization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Automatic token-based Precision/Recall/F1 (Partial, Strict, Exact) and human-evaluated metrics (precision of canonicalized triplets, number of relation types, redundancy score = average cosine similarity to nearest relation). Representative results: WebNLG Partial F1 — EDC (GPT-4) 0.783, EDC+R (GPT-4) 0.820; REBEL Partial F1 — EDC+R (GPT-4) 0.601 (EDC (GPT-4) 0.546); Wiki-NRE Partial F1 — EDC (GPT-4) 0.683, EDC+R (GPT-4) 0.713. Self-canonicalization (human) precision: WebNLG 0.956 (EDC) vs CESI 0.724; number of relations reduced (EDC WebNLG 200 vs Open KG 529); redundancy score (EDC WebNLG 0.833 vs CESI 0.893).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared to (1) specialized trained seq2seq models (REGEN, GenIE) EDC/EDC+R outperforms or matches them on the larger-schema datasets studied; (2) clustering-based canonicalization (CESI) EDC yields much higher precision and more concise, less-redundant schemas; (3) prior LLM-based designs that include full schema in prompt (ChatIE, CodeKGC) struggle with large schemas and context-window limits, whereas EDC circumvents the need to include large schemas in the extraction prompt via post-hoc canonicalization and an optional Schema Retriever.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>High cost due to many LLM calls (authors report ~$0.009 USD per example with GPT-3.5 for all components), latency, dependence on LLM quality (OIE stage is upstream bottleneck), current pipeline does not perform entity deduplication/coreference (left as future work), performance can be penalized by incomplete reference annotations in benchmarks, potential LLM hallucinations, and context-window constraints for very large schemas (mitigated by Schema Retriever but not eliminated).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5384.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5384.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Schema Retriever</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EDC Schema Retriever (fine-tuned embedding retriever)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A trained relation-retrieval module that projects input text and schema relation definitions into a vector space (fine-tuned E5-mistral-7b-instruct variant) and retrieves candidate relations relevant to the input to enrich extraction (retrieval-augmented refinement).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Text-to-relation retrieval via fine-tuned sentence embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Schema components (relation definitions) and input texts are embedded; a cosine-similarity ranking is used to retrieve top-k candidate relations for a given text. Training uses synthesized text–relation pairs (from TEKGEN) and an InfoNCE loss to discriminate the correct relation from negatives.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Relation vocabulary / schema for knowledge graphs (used as side information to map open triplets to canonical schema)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Scales canonicalization/extraction to large schemas (efficient vector search), retrieves semantically-relevant candidate relations even when schema is too large to fit in prompt, different similarity space than generic sentence embeddings (trained to retrieve presence rather than mere paraphrase), recall-focused (recall@10 reported).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Used during refinement to improve OIE extraction (Target Alignment); retriever evaluation reported as recall@10 on retrieval of relations present in text (WebNLG/REBEL/Wiki-NRE).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Retriever recall@10: WebNLG 0.823, REBEL 0.663, Wiki-NRE 0.818. Ablation: removing Schema Retriever drops Partial F1 (EDC+R w/o S.R.) on WebNLG from 0.794 to 0.752 (GPT-3.5-turbo OIE), on REBEL from 0.559 to 0.517, on Wiki-NRE from 0.693 to 0.653.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared to no-retriever refinement, the Schema Retriever consistently and significantly improves extraction precision/recall/F1. It helps the LLM discover finer-grained schema relations that pure open extraction misses (example: 'campus' vs generic 'location').</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Quality depends on training data variety/quality (authors used TEKGEN synthetic pairs); retriever can return irrelevant relations (mitigated by including relation definitions in prompts for LLM verification); not a perfect substitute for including full schema in-context but a scalable approximation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5384.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5384.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REGEN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReGen (Reinforcement learning for text and knowledge base generation using pretrained language models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequence-to-sequence model based on T5 that is trained with reinforcement learning to perform bidirectional text-to-graph and graph-to-text generation (used as a state-of-the-art baseline for WebNLG).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Regen: Reinforcement learning for text and knowledge base generation using pretrained language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Seq2Seq linearized triplet serialization (bidirectional text<->graph)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encodes graphs as linearized triplets and trains a T5-based seq2seq model to map between text and that serialization; uses RL to refine generation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graph (relational triplets)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>End-to-end learned mapping (graph<->text) with explicit training for generation consistency; constrained by model capacity/training data; requires schema-awareness during training.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Evaluated on WebNLG (semantic-parsing style text-to-graph/graph-to-text tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported (in paper's comparisons) WebNLG baseline (REGEN) Partial F1 ≈ 0.767 (token-based evaluation), Strict F1 ≈ 0.720, Exact F1 ≈ 0.723 (numbers from Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>EDC/EDC+R matched or exceeded REGEN on WebNLG in the experiments; authors note EDC's advantages on large/complex schemas due to post-hoc canonicalization and not needing to include large schema in prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires supervised training for the target schema and can be less flexible when schema sizes grow large; may not handle literals or out-of-schema relations as flexibly as open-extraction + canonicalization schemes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5384.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5384.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GenIE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GenIE (Generative Information Extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pre-trained BART-based sequence-to-sequence generative information extraction model that applies constrained generation to ensure outputs conform to a pre-defined schema.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GenIE: Generative information extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Seq2Seq constrained linearized triplets (schema-constrained generation)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Generates relational triplets as serialized text under constraints to ensure conformance to a supplied schema (pre-defined relation vocabulary).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graph (relational triplets) under a fixed schema</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Constrained generation enforces schema compliance (precision), but constrained models can miss literals and are brittle when schema is large or when relevant relations are not included in prompt/context.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Evaluated and used as baseline on REBEL and Wiki-NRE datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported baseline performance in this paper: REBEL Partial F1 (GenIE) ≈ 0.385 (Precision 0.381, Recall 0.391); Wiki-NRE Partial F1 (GenIE) ≈ 0.484 (Table 4 & 5).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>EDC significantly outperformed GenIE on REBEL and Wiki-NRE (authors attribute this to GenIE's constrained generation failing to extract literals like numbers/dates and issues when schema/context length grows).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Sensitive to schema size and completeness; constrained decoding may reduce recall for literals and out-of-schema but relevant relations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5384.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5384.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeKGC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeKGC (Code language model for generative knowledge graph construction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that frames KG construction as code generation for code-oriented LMs to improve structured output fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Codekgc: Code language model for generative knowledge graph construction.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph serialization as code / code-like structured output</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Serializes triplets into a code-like format and uses code-capable LMs to generate structured KG outputs (leverages strong structural biases in code models).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graph (relational triplets), serialized as code</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Code-style serialization can improve syntactic fidelity (well-formed structured outputs), but still faces schema/context-size limitations and dependence on LLM prompt/context.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Compared (Appendix experiments) on small-schema datasets (CONLL04, SciERC, Wiki-NRE subset) against EDC and ChatIE.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>From Table 10: CONLL Partial F1 (CodeKGC) ≈ 0.545 (Precision 0.542, Recall 0.55); SciERC Partial F1 ≈ 0.392; Wiki-NRE Partial F1 ≈ 0.612. EDC+R often outperforms CodeKGC on these benchmarks in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>On small-schema datasets CodeKGC is competitive, but EDC+R surpassed it in the authors' comparisons, particularly as schema size increases or when richer canonicalization/refinement is needed.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not intended to scale to arbitrarily large schemas in-context; code-style prompts may still generate out-of-schema tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5384.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5384.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatIE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatIE (multi-turn chat-based IE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt-based multi-turn question-answering approach (chat-style) that extracts triplets via conversational querying of an LLM (used previously for zero/few-shot extraction).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Zeroshot information extraction via chatting with chatgpt.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Multi-turn conversational extraction / QA -> triplet serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Uses multi-turn LLM prompts to ask targeted questions and gather relation/entity information, then serializes results into triplets.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graph (relational triplets) extracted via conversational interactions</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Interactive and interpretable extraction process (can clarify ambiguities), but multi-turn context and schema size constraints can limit scalability; may output out-of-schema relations despite schema-inclusion attempts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Compared as an existing LLM-based approach on small-schema datasets (CONLL04, SciERC, subset of Wiki-NRE).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>From Table 10: CONLL Partial F1 (ChatIE) ≈ 0.468; SciERC Partial F1 ≈ 0.357; Wiki-NRE Partial F1 ≈ 0.571. EDC+R outperformed ChatIE in the authors' reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>ChatIE is effective for small schemas or interactive extraction, but EDC/EDC+R outperform it on the evaluated datasets, especially as schema complexity increases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Multi-turn prompts increase cost and latency; context-window and prompt-length constraints limit inclusion of large schemas; can still generate out-of-schema relations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5384.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5384.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TEKGEN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TEKGEN (text-triplets synthetic corpus)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale text–triplet dataset created by aligning Wikidata triplets to Wikipedia text; used here to synthesize training data for the Schema Retriever (text–relation pairs).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Paired text and serialized triplets (graph-to-text pairs)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Provides aligned examples of natural-language sentences and corresponding Wikidata triplets; used to produce positive text–relation pairs (and negatives) for training the retrieval embedding model.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graph triplets (Wikidata) aligned to Wikipedia text</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Large-scale aligned graph<->text data, useful for supervised training of retrieval or generation modules, synthetic/naturally-aligned but may still contain noise from alignment heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Used to synthesize 37,500 text–relation pairs (balanced positives/negatives) for training the Schema Retriever; not an evaluation benchmark in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not a method but dataset; authors report using 37,500 pairs for training the retriever (split evenly pos/neg).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Used as a training source for the Schema Retriever rather than compared as a model; selected because it aligns KG triples to raw text at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Alignment/label noise inherent to distant supervision-style alignment; may bias retriever to patterns present in Wikipedia/Wikidata.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5384.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5384.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CESI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CESI (Canonicalization using Embeddings and Side Information)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A canonicalization method that constructs embeddings for OIE relations using side information (PPDB, WordNet) and clusters relation phrases to canonicalize an open KG.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cesi: Canonicalizing open knowledge bases using embeddings and side information.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Embedding-based clustering of relation phrases (with external side information)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Creates vector representations of relation phrases augmented by external lexical resources and clusters them to produce canonical relation types.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Open knowledge graph (relations from OIE)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Unsupervised clustering can consolidate many surface variants, but is prone to over-generalization (merging semantically-distinct relations); relies on side resources like PPDB/WordNet for signal.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Compared against EDC in the Self Canonicalization setting; applied to open KG outputs to produce canonicalized schemas.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In this paper's human-evaluation (Table 2) CESI Precision: WebNLG 0.724 (EDC 0.956), REBEL 0.504 (EDC 0.867), Wiki-NRE 0.753 (EDC 0.898). CESI produced more relation types (e.g., WebNLG 280) and higher redundancy (e.g., 0.893) vs EDC.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>EDC outperforms CESI in precision and produces a more concise, less redundant schema; CESI tends to over-generalize and cluster heterogeneous relations together (authors observed examples where distinct relations were incorrectly clustered).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Over-generalization due to clustering; depends on external static resources; less contextual/adaptive than LLM-generated definitions + verification.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5384.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e5384.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>E5-Mistral embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>E5-mistral-7b-instruct (sentence embedding model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sentence embedding model variant (E5 family) used as the base model for the Schema Retriever; authors fine-tune this model to retrieve relations given text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving text embeddings with large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Dense sentence embeddings for relation definitions and text</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Produces fixed-length vector embeddings for natural-language relation definitions and input texts; cosine similarity is used to retrieve candidate relations. The paper fine-tunes a variant (E5-mistral-7b-instruct) with InfoNCE on text–relation pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Schema element representations (relation definitions) for knowledge graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Dense vectors enable scalable approximate nearest-neighbor retrieval; when fine-tuned, similarity captures relevance/presence of relation in text rather than mere paraphrase; embedding-based retrieval enables sub-linear scaling to large schemas.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Used to index relation definitions and compute relevance to texts during canonicalization/refinement; retriever recall@10 reported as a performance measure.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>After fine-tuning, Schema Retriever (based on this model) recall@10: WebNLG 0.823, REBEL 0.663, Wiki-NRE 0.818. Authors used top-5 / top-10 retrieval hyperparameters in canonicalization/refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Authors report this fine-tuned embedding approach is more suitable than off-the-shelf sentence embeddings for retrieving schema relations (because it optimizes for relevance/presence), enabling better refinement compared to no-retriever baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Embedding space needs task-specific fine-tuning to retrieve presence rather than pure semantic equivalence; quality depends on training pairs and negative sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Regen: Reinforcement learning for text and knowledge base generation using pretrained language models. <em>(Rating: 2)</em></li>
                <li>GenIE: Generative information extraction. <em>(Rating: 2)</em></li>
                <li>Codekgc: Code language model for generative knowledge graph construction. <em>(Rating: 2)</em></li>
                <li>Zeroshot information extraction via chatting with chatgpt. <em>(Rating: 1)</em></li>
                <li>Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training. <em>(Rating: 2)</em></li>
                <li>Cesi: Canonicalizing open knowledge bases using embeddings and side information. <em>(Rating: 2)</em></li>
                <li>Improving text embeddings with large language models. <em>(Rating: 2)</em></li>
                <li>Is information extraction solved by chatgpt? an analysis of performance, evaluation criteria, robustness and errors. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5384",
    "paper_id": "paper-db96e019410006c3ee0ae0184800ab206f8704dd",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "EDC",
            "name_full": "Extract-Define-Canonicalize",
            "brief_description": "A three-phase LLM-based pipeline for Knowledge Graph Construction that first performs open information extraction, then prompts LLMs to produce natural-language schema (relation/entity) definitions, and finally canonicalizes relation/entity phrases via vector similarity + LLM verification to produce a concise canonical KG.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Linearized open relational triplets + natural-language schema definitions",
            "representation_description": "Graphs are represented as lists of relational triplets in the form [Subject, Relation, Object] serialized literally (examples shown as nested lists in prompts). In addition, each induced relation/entity type is paired with a natural-language definition produced by an LLM; these definitions are embedded (sentence transformer) and used to (a) retrieve candidate canonical schema elements by vector similarity and (b) present Multiple-Choice alternatives to an LLM which decides whether and how to map an open phrase to a canonical relation. The canonicalized KG thus uses standardized relation tokens (e.g., birthDate, mission) replacing varied surface forms.",
            "graph_type": "Knowledge graph (relational triplets)",
            "representation_properties": "Open/extractive (no schema required at extraction), post-hoc canonicalization enables scalability to large schemas, human-readable relation definitions (interpretability), reduces redundancy by consolidating semantically-equivalent relation phrases, uses LLM verification to avoid over-generalization (faithfulness), relies on embedding similarity (compactness/accessibility via vector search); requires multiple LLM calls (cost/latency) and separate canonicalization stage (pipeline complexity).",
            "evaluation_task": "Knowledge Graph Construction (relational triplet extraction) on WebNLG (semantic-parsing), REBEL, and Wiki-NRE; manual evaluation for self-canonicalization.",
            "performance_metrics": "Automatic token-based Precision/Recall/F1 (Partial, Strict, Exact) and human-evaluated metrics (precision of canonicalized triplets, number of relation types, redundancy score = average cosine similarity to nearest relation). Representative results: WebNLG Partial F1 — EDC (GPT-4) 0.783, EDC+R (GPT-4) 0.820; REBEL Partial F1 — EDC+R (GPT-4) 0.601 (EDC (GPT-4) 0.546); Wiki-NRE Partial F1 — EDC (GPT-4) 0.683, EDC+R (GPT-4) 0.713. Self-canonicalization (human) precision: WebNLG 0.956 (EDC) vs CESI 0.724; number of relations reduced (EDC WebNLG 200 vs Open KG 529); redundancy score (EDC WebNLG 0.833 vs CESI 0.893).",
            "comparison_to_other_representations": "Compared to (1) specialized trained seq2seq models (REGEN, GenIE) EDC/EDC+R outperforms or matches them on the larger-schema datasets studied; (2) clustering-based canonicalization (CESI) EDC yields much higher precision and more concise, less-redundant schemas; (3) prior LLM-based designs that include full schema in prompt (ChatIE, CodeKGC) struggle with large schemas and context-window limits, whereas EDC circumvents the need to include large schemas in the extraction prompt via post-hoc canonicalization and an optional Schema Retriever.",
            "limitations_or_challenges": "High cost due to many LLM calls (authors report ~$0.009 USD per example with GPT-3.5 for all components), latency, dependence on LLM quality (OIE stage is upstream bottleneck), current pipeline does not perform entity deduplication/coreference (left as future work), performance can be penalized by incomplete reference annotations in benchmarks, potential LLM hallucinations, and context-window constraints for very large schemas (mitigated by Schema Retriever but not eliminated).",
            "uuid": "e5384.0",
            "source_info": {
                "paper_title": "Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Schema Retriever",
            "name_full": "EDC Schema Retriever (fine-tuned embedding retriever)",
            "brief_description": "A trained relation-retrieval module that projects input text and schema relation definitions into a vector space (fine-tuned E5-mistral-7b-instruct variant) and retrieves candidate relations relevant to the input to enrich extraction (retrieval-augmented refinement).",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Text-to-relation retrieval via fine-tuned sentence embeddings",
            "representation_description": "Schema components (relation definitions) and input texts are embedded; a cosine-similarity ranking is used to retrieve top-k candidate relations for a given text. Training uses synthesized text–relation pairs (from TEKGEN) and an InfoNCE loss to discriminate the correct relation from negatives.",
            "graph_type": "Relation vocabulary / schema for knowledge graphs (used as side information to map open triplets to canonical schema)",
            "representation_properties": "Scales canonicalization/extraction to large schemas (efficient vector search), retrieves semantically-relevant candidate relations even when schema is too large to fit in prompt, different similarity space than generic sentence embeddings (trained to retrieve presence rather than mere paraphrase), recall-focused (recall@10 reported).",
            "evaluation_task": "Used during refinement to improve OIE extraction (Target Alignment); retriever evaluation reported as recall@10 on retrieval of relations present in text (WebNLG/REBEL/Wiki-NRE).",
            "performance_metrics": "Retriever recall@10: WebNLG 0.823, REBEL 0.663, Wiki-NRE 0.818. Ablation: removing Schema Retriever drops Partial F1 (EDC+R w/o S.R.) on WebNLG from 0.794 to 0.752 (GPT-3.5-turbo OIE), on REBEL from 0.559 to 0.517, on Wiki-NRE from 0.693 to 0.653.",
            "comparison_to_other_representations": "Compared to no-retriever refinement, the Schema Retriever consistently and significantly improves extraction precision/recall/F1. It helps the LLM discover finer-grained schema relations that pure open extraction misses (example: 'campus' vs generic 'location').",
            "limitations_or_challenges": "Quality depends on training data variety/quality (authors used TEKGEN synthetic pairs); retriever can return irrelevant relations (mitigated by including relation definitions in prompts for LLM verification); not a perfect substitute for including full schema in-context but a scalable approximation.",
            "uuid": "e5384.1",
            "source_info": {
                "paper_title": "Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "REGEN",
            "name_full": "ReGen (Reinforcement learning for text and knowledge base generation using pretrained language models)",
            "brief_description": "A sequence-to-sequence model based on T5 that is trained with reinforcement learning to perform bidirectional text-to-graph and graph-to-text generation (used as a state-of-the-art baseline for WebNLG).",
            "citation_title": "Regen: Reinforcement learning for text and knowledge base generation using pretrained language models.",
            "mention_or_use": "use",
            "representation_name": "Seq2Seq linearized triplet serialization (bidirectional text&lt;-&gt;graph)",
            "representation_description": "Encodes graphs as linearized triplets and trains a T5-based seq2seq model to map between text and that serialization; uses RL to refine generation quality.",
            "graph_type": "Knowledge graph (relational triplets)",
            "representation_properties": "End-to-end learned mapping (graph&lt;-&gt;text) with explicit training for generation consistency; constrained by model capacity/training data; requires schema-awareness during training.",
            "evaluation_task": "Evaluated on WebNLG (semantic-parsing style text-to-graph/graph-to-text tasks).",
            "performance_metrics": "Reported (in paper's comparisons) WebNLG baseline (REGEN) Partial F1 ≈ 0.767 (token-based evaluation), Strict F1 ≈ 0.720, Exact F1 ≈ 0.723 (numbers from Table 3).",
            "comparison_to_other_representations": "EDC/EDC+R matched or exceeded REGEN on WebNLG in the experiments; authors note EDC's advantages on large/complex schemas due to post-hoc canonicalization and not needing to include large schema in prompt.",
            "limitations_or_challenges": "Requires supervised training for the target schema and can be less flexible when schema sizes grow large; may not handle literals or out-of-schema relations as flexibly as open-extraction + canonicalization schemes.",
            "uuid": "e5384.2",
            "source_info": {
                "paper_title": "Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "GenIE",
            "name_full": "GenIE (Generative Information Extraction)",
            "brief_description": "A pre-trained BART-based sequence-to-sequence generative information extraction model that applies constrained generation to ensure outputs conform to a pre-defined schema.",
            "citation_title": "GenIE: Generative information extraction.",
            "mention_or_use": "use",
            "representation_name": "Seq2Seq constrained linearized triplets (schema-constrained generation)",
            "representation_description": "Generates relational triplets as serialized text under constraints to ensure conformance to a supplied schema (pre-defined relation vocabulary).",
            "graph_type": "Knowledge graph (relational triplets) under a fixed schema",
            "representation_properties": "Constrained generation enforces schema compliance (precision), but constrained models can miss literals and are brittle when schema is large or when relevant relations are not included in prompt/context.",
            "evaluation_task": "Evaluated and used as baseline on REBEL and Wiki-NRE datasets.",
            "performance_metrics": "Reported baseline performance in this paper: REBEL Partial F1 (GenIE) ≈ 0.385 (Precision 0.381, Recall 0.391); Wiki-NRE Partial F1 (GenIE) ≈ 0.484 (Table 4 & 5).",
            "comparison_to_other_representations": "EDC significantly outperformed GenIE on REBEL and Wiki-NRE (authors attribute this to GenIE's constrained generation failing to extract literals like numbers/dates and issues when schema/context length grows).",
            "limitations_or_challenges": "Sensitive to schema size and completeness; constrained decoding may reduce recall for literals and out-of-schema but relevant relations.",
            "uuid": "e5384.3",
            "source_info": {
                "paper_title": "Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "CodeKGC",
            "name_full": "CodeKGC (Code language model for generative knowledge graph construction)",
            "brief_description": "An approach that frames KG construction as code generation for code-oriented LMs to improve structured output fidelity.",
            "citation_title": "Codekgc: Code language model for generative knowledge graph construction.",
            "mention_or_use": "use",
            "representation_name": "Graph serialization as code / code-like structured output",
            "representation_description": "Serializes triplets into a code-like format and uses code-capable LMs to generate structured KG outputs (leverages strong structural biases in code models).",
            "graph_type": "Knowledge graph (relational triplets), serialized as code",
            "representation_properties": "Code-style serialization can improve syntactic fidelity (well-formed structured outputs), but still faces schema/context-size limitations and dependence on LLM prompt/context.",
            "evaluation_task": "Compared (Appendix experiments) on small-schema datasets (CONLL04, SciERC, Wiki-NRE subset) against EDC and ChatIE.",
            "performance_metrics": "From Table 10: CONLL Partial F1 (CodeKGC) ≈ 0.545 (Precision 0.542, Recall 0.55); SciERC Partial F1 ≈ 0.392; Wiki-NRE Partial F1 ≈ 0.612. EDC+R often outperforms CodeKGC on these benchmarks in the paper's experiments.",
            "comparison_to_other_representations": "On small-schema datasets CodeKGC is competitive, but EDC+R surpassed it in the authors' comparisons, particularly as schema size increases or when richer canonicalization/refinement is needed.",
            "limitations_or_challenges": "Not intended to scale to arbitrarily large schemas in-context; code-style prompts may still generate out-of-schema tokens.",
            "uuid": "e5384.4",
            "source_info": {
                "paper_title": "Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "ChatIE",
            "name_full": "ChatIE (multi-turn chat-based IE)",
            "brief_description": "A prompt-based multi-turn question-answering approach (chat-style) that extracts triplets via conversational querying of an LLM (used previously for zero/few-shot extraction).",
            "citation_title": "Zeroshot information extraction via chatting with chatgpt.",
            "mention_or_use": "use",
            "representation_name": "Multi-turn conversational extraction / QA -&gt; triplet serialization",
            "representation_description": "Uses multi-turn LLM prompts to ask targeted questions and gather relation/entity information, then serializes results into triplets.",
            "graph_type": "Knowledge graph (relational triplets) extracted via conversational interactions",
            "representation_properties": "Interactive and interpretable extraction process (can clarify ambiguities), but multi-turn context and schema size constraints can limit scalability; may output out-of-schema relations despite schema-inclusion attempts.",
            "evaluation_task": "Compared as an existing LLM-based approach on small-schema datasets (CONLL04, SciERC, subset of Wiki-NRE).",
            "performance_metrics": "From Table 10: CONLL Partial F1 (ChatIE) ≈ 0.468; SciERC Partial F1 ≈ 0.357; Wiki-NRE Partial F1 ≈ 0.571. EDC+R outperformed ChatIE in the authors' reported experiments.",
            "comparison_to_other_representations": "ChatIE is effective for small schemas or interactive extraction, but EDC/EDC+R outperform it on the evaluated datasets, especially as schema complexity increases.",
            "limitations_or_challenges": "Multi-turn prompts increase cost and latency; context-window and prompt-length constraints limit inclusion of large schemas; can still generate out-of-schema relations.",
            "uuid": "e5384.5",
            "source_info": {
                "paper_title": "Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "TEKGEN",
            "name_full": "TEKGEN (text-triplets synthetic corpus)",
            "brief_description": "A large-scale text–triplet dataset created by aligning Wikidata triplets to Wikipedia text; used here to synthesize training data for the Schema Retriever (text–relation pairs).",
            "citation_title": "Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training.",
            "mention_or_use": "use",
            "representation_name": "Paired text and serialized triplets (graph-to-text pairs)",
            "representation_description": "Provides aligned examples of natural-language sentences and corresponding Wikidata triplets; used to produce positive text–relation pairs (and negatives) for training the retrieval embedding model.",
            "graph_type": "Knowledge graph triplets (Wikidata) aligned to Wikipedia text",
            "representation_properties": "Large-scale aligned graph&lt;-&gt;text data, useful for supervised training of retrieval or generation modules, synthetic/naturally-aligned but may still contain noise from alignment heuristics.",
            "evaluation_task": "Used to synthesize 37,500 text–relation pairs (balanced positives/negatives) for training the Schema Retriever; not an evaluation benchmark in this paper.",
            "performance_metrics": "Not a method but dataset; authors report using 37,500 pairs for training the retriever (split evenly pos/neg).",
            "comparison_to_other_representations": "Used as a training source for the Schema Retriever rather than compared as a model; selected because it aligns KG triples to raw text at scale.",
            "limitations_or_challenges": "Alignment/label noise inherent to distant supervision-style alignment; may bias retriever to patterns present in Wikipedia/Wikidata.",
            "uuid": "e5384.6",
            "source_info": {
                "paper_title": "Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "CESI",
            "name_full": "CESI (Canonicalization using Embeddings and Side Information)",
            "brief_description": "A canonicalization method that constructs embeddings for OIE relations using side information (PPDB, WordNet) and clusters relation phrases to canonicalize an open KG.",
            "citation_title": "Cesi: Canonicalizing open knowledge bases using embeddings and side information.",
            "mention_or_use": "use",
            "representation_name": "Embedding-based clustering of relation phrases (with external side information)",
            "representation_description": "Creates vector representations of relation phrases augmented by external lexical resources and clusters them to produce canonical relation types.",
            "graph_type": "Open knowledge graph (relations from OIE)",
            "representation_properties": "Unsupervised clustering can consolidate many surface variants, but is prone to over-generalization (merging semantically-distinct relations); relies on side resources like PPDB/WordNet for signal.",
            "evaluation_task": "Compared against EDC in the Self Canonicalization setting; applied to open KG outputs to produce canonicalized schemas.",
            "performance_metrics": "In this paper's human-evaluation (Table 2) CESI Precision: WebNLG 0.724 (EDC 0.956), REBEL 0.504 (EDC 0.867), Wiki-NRE 0.753 (EDC 0.898). CESI produced more relation types (e.g., WebNLG 280) and higher redundancy (e.g., 0.893) vs EDC.",
            "comparison_to_other_representations": "EDC outperforms CESI in precision and produces a more concise, less redundant schema; CESI tends to over-generalize and cluster heterogeneous relations together (authors observed examples where distinct relations were incorrectly clustered).",
            "limitations_or_challenges": "Over-generalization due to clustering; depends on external static resources; less contextual/adaptive than LLM-generated definitions + verification.",
            "uuid": "e5384.7",
            "source_info": {
                "paper_title": "Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "E5-Mistral embeddings",
            "name_full": "E5-mistral-7b-instruct (sentence embedding model)",
            "brief_description": "A sentence embedding model variant (E5 family) used as the base model for the Schema Retriever; authors fine-tune this model to retrieve relations given text.",
            "citation_title": "Improving text embeddings with large language models.",
            "mention_or_use": "use",
            "representation_name": "Dense sentence embeddings for relation definitions and text",
            "representation_description": "Produces fixed-length vector embeddings for natural-language relation definitions and input texts; cosine similarity is used to retrieve candidate relations. The paper fine-tunes a variant (E5-mistral-7b-instruct) with InfoNCE on text–relation pairs.",
            "graph_type": "Schema element representations (relation definitions) for knowledge graphs",
            "representation_properties": "Dense vectors enable scalable approximate nearest-neighbor retrieval; when fine-tuned, similarity captures relevance/presence of relation in text rather than mere paraphrase; embedding-based retrieval enables sub-linear scaling to large schemas.",
            "evaluation_task": "Used to index relation definitions and compute relevance to texts during canonicalization/refinement; retriever recall@10 reported as a performance measure.",
            "performance_metrics": "After fine-tuning, Schema Retriever (based on this model) recall@10: WebNLG 0.823, REBEL 0.663, Wiki-NRE 0.818. Authors used top-5 / top-10 retrieval hyperparameters in canonicalization/refinement.",
            "comparison_to_other_representations": "Authors report this fine-tuned embedding approach is more suitable than off-the-shelf sentence embeddings for retrieving schema relations (because it optimizes for relevance/presence), enabling better refinement compared to no-retriever baselines.",
            "limitations_or_challenges": "Embedding space needs task-specific fine-tuning to retrieve presence rather than pure semantic equivalence; quality depends on training pairs and negative sampling.",
            "uuid": "e5384.8",
            "source_info": {
                "paper_title": "Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Regen: Reinforcement learning for text and knowledge base generation using pretrained language models.",
            "rating": 2
        },
        {
            "paper_title": "GenIE: Generative information extraction.",
            "rating": 2
        },
        {
            "paper_title": "Codekgc: Code language model for generative knowledge graph construction.",
            "rating": 2
        },
        {
            "paper_title": "Zeroshot information extraction via chatting with chatgpt.",
            "rating": 1
        },
        {
            "paper_title": "Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training.",
            "rating": 2
        },
        {
            "paper_title": "Cesi: Canonicalizing open knowledge bases using embeddings and side information.",
            "rating": 2
        },
        {
            "paper_title": "Improving text embeddings with large language models.",
            "rating": 2
        },
        {
            "paper_title": "Is information extraction solved by chatgpt? an analysis of performance, evaluation criteria, robustness and errors.",
            "rating": 1
        }
    ],
    "cost": 0.021646,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction</h1>
<p>Bowen Zhang ${ }^{1}$ and Harold Soh ${ }^{1,2}$<br>${ }^{1}$ Dept. of Computer Science, National University of Singapore, ${ }^{2}$ NUS Smart Systems Institute<br>{bowenzhang, harold}@comp.nus.edu.sg</p>
<h4>Abstract</h4>
<p>In this work, we are interested in automated methods for knowledge graph creation (KGC) from input text. Progress on large language models (LLMs) has prompted a series of recent works applying them to KGC, e.g., via zero/few-shot prompting. Despite successes on small domain-specific datasets, these models face difficulties scaling up to text common in many real-world applications. A principal issue is that, in prior methods, the KG schema has to be included in the LLM prompt to generate valid triplets; larger and more complex schemas easily exceed the LLMs' context window length. Furthermore, there are scenarios where a fixed pre-defined schema is not available and we would like the method to construct a high-quality KG with a succinct selfgenerated schema. To address these problems, we propose a three-phase framework named Extract-Define-Canonicalize (EDC): open information extraction followed by schema definition and post-hoc canonicalization. EDC is flexible in that it can be applied to settings where a pre-defined target schema is available and when it is not; in the latter case, it constructs a schema automatically and applies selfcanonicalization. To further improve performance, we introduce a trained component that retrieves schema elements relevant to the input text; this improves the LLMs' extraction performance in a retrieval-augmented generationlike manner. We demonstrate on three KGC benchmarks that EDC is able to extract highquality triplets without any parameter tuning and with significantly larger schemas compared to prior works. Code for EDC is available at https://github.com/clear-nus/edc.</p>
<h2>1 Introduction</h2>
<p>Knowledge graphs (KGs) (Ji et al., 2021) are a structured representation of knowledge that organizes interconnected information through graph structures, where entities and relations are represented as nodes and edges. They are broadly</p>
<p>EDC: Extract-Define-Canonicalize
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: A high-level illustration of Extract-DefineCanonicalize (EDC) for Knowledge Graph Construction.
used in a variety of downstream tasks such as decision-making (Guo et al., 2021; Lan et al., 2020), question-answering (Huang et al., 2019; Yasunaga et al., 2021), and recommendation (Guo et al., 2020; Wang et al., 2019). However, knowledge graph construction (KGC) is inherently challenging: the task requires competence in understanding syntax and semantics to generate a consistent, concise, and meaningful knowledge graph. As such, KGC predominantly relies on intensive human labor (Ye et al., 2022). KGC is a broad problem and in this work, we focus on the task of relational triplet extraction as it is crucial for</p>
<p>KGC. Following previous works (Ye et al., 2022; Melnyk et al., 2022; Bi et al., 2024), we still refer to the task we are addressing as KGC.</p>
<p>Recent attempts to automate KGC (Zhong et al., 2023; Ye et al., 2022) have employed large language models (LLMs) in view of their remarkable natural language understanding and generation capabilities. LLM-based KGC methods employ various innovative prompt-based techniques, such as multi-turn conversation (Wei et al., 2023) and code generation (Bi et al., 2024), to generate entityrelation triplets that represent the knowledge graph. However, these methods are currently limited to small and domain-specific scenarios - to ensure the validity of generated triplets, schema information (e.g., possible entity and relation types) has to be included in the prompt. Complex datasets (e.g., Wikipedia) typically require large schemas that exceed the context window length or can be ignored by the LLMs (Wadhwa et al., 2023). Furthermore, pre-defined schemas are not always available - the users might not have pre-determined or fixed intentions about what information is of interest in advance but still would like to extract intrinsically high-quality KGs. It is unclear how existing methods will work in such situations.</p>
<p>To address these problems, we propose Ex-tract-Define-Canonicalize (EDC), a structured approach for KGC: the key idea is to decompose KGC into three primary phases corresponding to three subtasks (Fig. 1):</p>
<ol>
<li>Open Information Extraction: extract a list of entity-relation triplets from the input text freely.</li>
<li>Schema Definition: generate a definition for each component of the schema, e.g. entity type and relation type, induced by triplets obtained in the extraction phase.</li>
<li>Schema Canonicalization: use the schema definitions to standardize the triplets such that semantically-equivalent entities/relations types have the same noun/relation phrase.</li>
</ol>
<p>Each phase exploits the strengths of LLMs: the Extract subtask leverages recent findings that LLMs are effective open information extractors ( Li et al., 2023; Han et al., 2023) — they can extract semantically correct and meaningful triplets. However, the resulting triplets typically contain redundant and ambiguous information, e.g., multiple
semantically equivalent relation phrases such as 'profession', 'job', and 'occupation' (Kamp et al., 2023; Putri et al., 2019; Vashishth et al., 2018).</p>
<p>Phases 2 and 3 (Define and Canonicalize) standardize the triplets to make them useful for downstream tasks. We designed EDC to be flexible: it can either discover triplets consistent with a preexisting schema of potentially large size (Target Alignment) or self-generate a schema (Self Canonicalization). To achieve this, we use LLMs to define the schema components by exploiting their explanation generation capabilities - LLMs can justify their extractions via explanations that are agreeable to human experts (Li et al., 2023). The definitions are used to find the closest entity/relation type candidates (via a vector similarity search) that the LLM can then reference to canonicalize a component. In the case there is no equivalent counterpart in the existing schema, we can choose to add it to enrich the schema.</p>
<p>To further improve performance, the three steps above can be followed by an additional Refinement phase: we repeat EDC but provide the previously extracted triplets and a relevant part of the schema in the prompt during the initial extraction. We propose a trained Schema Retriever that retrieves schema components relevant to the input text, akin to retrieval-augmented generation (Lewis et al., 2020), which we find improves the generated triplets.</p>
<p>Experiments on three KGC datasets in both Target Alignment and Self Canonicalization settings show that EDC is able to extract higher-quality KGs compared to state-of-the-art methods through both automatic and manual evaluation. Furthermore, the use of the Schema Retriever is shown to significantly and consistently improve EDC's performance.</p>
<p>In summary, the paper makes the following contributions:</p>
<ul>
<li>EDC, a flexible and performant LLM-based framework for knowledge graph construction that is able to extract high-quality KGs with schema of large size or without any predefined schema.</li>
<li>Schema Retriever, a trained model to extract schema components relevant to input text in the same vein as information retrieval.</li>
<li>Empirical evidence that demonstrate the effectiveness of EDC and the Schema Retriever.</li>
</ul>
<h2>2 Background</h2>
<p>In this section, we provide relevant background on knowledge graph construction (KGC), open information extraction (OIE), and canonicalization.</p>
<p>Knowledge Graph Construction. Traditional methods typically addressed KGC using "pipelines", comprising subtasks like entity discovery (Žukov-Gregorič et al., 2018; Martins et al., 2019), entity typing (Choi et al., 2018; Onoe and Durrett, 2020), and relation classification (Zeng et al., 2014, 2015). Thanks to advances in pre-trained generative language models (e.g., T5 (Raffel et al., 2020) and BERT(Lewis et al., 2019)), more recent works instead frame KGC as a sequence-to-sequence problem and generate relational triplets in an end-to-end manner by fine-tuning these moderately-sized language models (Ye et al., 2022). The success of large language models (LLMs) has pushed this paradigm further: current methods directly prompt the LLMs to generate triplets in a zero/few-shot manner. For example, ChatIE (Wei et al., 2023) extracts triplets by framing the task as a multi-turn question-answering problem and CodeKGC (Bi et al., 2024) approaches the task as a code generation problem. As previously mentioned, these models face difficulties scaling up to general text common in many real-world applications as the KG schema has to be included in the LLM prompt. Our EDC framework circumvents this problem by using post-hoc canonicalization (and without requiring fine-tuning of the base LLMs).</p>
<p>Open Information Extraction and Canonicalization. Standard (closed) information extraction requires the output triplets to follow a pre-defined schema, e.g. a list of relation or entity types to be extracted from. In contrast, open information extraction (OIE) does not have such a requirement. OIE has a long history and we refer readers who want comprehensive coverage to the excellent surveys (Liu et al., 2022; Zhou et al., 2022; Kamp et al., 2023). Recent studies have found LLMs to exhibit excellent performance on OIE tasks ( Li et al., 2023). However, the relational triplets extracted from OIE systems are not canonicalized; multiple semantically equivalent relations can coexist without being unified to a canonical form, causing redundancy and ambiguity in the induced open knowledge graph. An extra canonicalization step is required to standardize the triplets to make
the KGs useful for downstream applications.
Canonicalization methods differ depending on whether a target schema is available. In case a target schema is present, the task is sometimes referred to as "alignment" (Putri et al., 2019). For example, Putri et al. (2019) uses WordNet (Miller, 1995) as side information to obtain definitions for the OIE-extracted relation phrases and a Siamese network to compare an OIE relation definition and a pre-defined relation in the target schema. In case no target schema is available, state-of-the-art methods are commonly based on clustering (Vashishth et al., 2018; Dash et al., 2020). CESI (Vashishth et al., 2018) creates embeddings for the OIE relations using side information from external sources like PPDB (Ganitkevitch et al., 2013) and WordNet. However, clustering-based methods are prone to over-generalization (Kamp et al., 2023; Putri et al., 2019), e.g., CESI may put "is brother of", "is son of", "is main villain of", and "was professor of" into the same relation cluster.</p>
<p>Compared to the existing canonicalization methods, EDC is more general; it works whether a target schema is provided or not. Instead of using static external sources like WordNet, EDC utilizes contextual and semantically-rich side information generated by LLMs. Furthermore, by allowing the LLMs to verify if a transformation can be performed (instead of solely relying on the embedding similarity), EDC alleviates the over-generalization issue faced by previous methods.</p>
<h2>3 Method: EDC for KGC</h2>
<p>This section outlines our primary contribution: an approach to constructing knowledge graphs that leverages LLMs in a structured manner. We first detail the EDC framework followed by a description of refinement $(\mathbf{E D C}+\mathbf{R})$. Given input text, our goal is to extract relational triplets in a canonical form such that the resulting KGs will have minimal ambiguity and redundancy. When there is a predefined target schema, all generated triplets should conform to it. In the scenario where there is not one, the system should dynamically create one and canonicalize the triplets with respect to it.</p>
<h3>3.1 EDC: Extract-Define-Canonicalize</h3>
<p>At a high level, EDC decomposes KGC into three connected subtasks. To ground our discussion, we will use a specific input text example: "Alan Shepard was born on Nov 18, 1923 and selected by</p>
<p>NASA in 1959. He was a member of the Apollo 14 crew" and walk through each of the phases:</p>
<p>Phase 1: Open Information Extraction: we first leverage Large Language Models (LLMs) for open information extraction. Through few-shot prompting, LLMs identify and extract relational triplets ([Subject, Relation, Object]) from input texts, independent of any specific schema. Using our example above, the prompt is:</p>
<h2>OIE Prompt</h2>
<p>Given a piece of text, extract relational triplets in the form of [Subject, Relation, Object] from it. Here are some examples:
Example 1:
Text: The 17068.8 millimeter long ALCO RS-3 has a diesel-electric transmission.
Triplets: [['ALCO RS-3', 'powerType', 'Dieselelectric transmission'], ['ALCO RS-3', 'length', '17068.8 (millimetres)']]
Now please extract triplets from the following text: Alan Shepard was born on Nov 18, 1923 and selected by NASA in 1959. He was a member of the Apollo 14 crew.</p>
<p>The resultant triplets (in this case, ['Alan Shepard', 'bornOn', 'Nov 18, 1923'], ['Alan Shepard', 'participatedIn', 'Apollo 14']) form an open $K G$, which is forwarded to subsequent phases.</p>
<p>Phase 2: Schema Definition: Next, we prompt the LLMs to provide a natural language definition for each component of the schema induced by the open KG:</p>
<h2>Schema Definition Prompt</h2>
<p>Given a piece of text and a list of relational triplets extracted from it, write a definition for each relation present.
Example 1:
Text: The 17068.8 millimeter long ALCO RS-3 has a diesel-electric transmission.
Triplets: [['ALCO RS-3', 'powerType', 'Dieselelectric transmission'], ['ALCO RS-3', 'length', '17068.8 (millimetres)']]
Definitions:
powerType: The subject entity uses the type of power or energy source specified by the object entity.</p>
<p>Now write a definition for each relation present in the triplets extracted from the following text: Text: Alan Shepard was an American who was born on Nov 18, 1923 in New Hampshire, was selected by NASA in 1959, was a member of the Apollo 14 crew and died in California
Triplets: [['Alan Shepard', 'bornOn', 'Nov 18, 1923'], ['Alan Shepard', 'participatedIn', 'Apollo</p>
<h2>14']</h2>
<p>This example prompt results in the definitions for (bornOn: The subject entity was born on the date specified by the object entity.) and (participatedIn: The subject entity took part in the event or mission specified by the object entity.), which are then passed to the next stage as side information used for canonicalization.</p>
<p>Phase 3: Schema Canonicalization: The third phase aims to refine the open KG into a canonical form, eliminating redundancies and ambiguities. We start by vectorizing the definitions of each schema component using a sentence transformer to create embeddings. Canonicalization then proceeds in one of two ways, depending on the availability of a target schema:</p>
<ul>
<li>Target Alignment: With an existing target schema, we identify the most closely related components within the target schema for each element, considering them for canonicalization. To prevent issues of over-generalization, LLMs assess the feasibility of each potential transformation. If a transformation is deemed unreasonable, indicating no semantic equivalent in the target schema, the component, and its related triplets are excluded.</li>
<li>Self Canonicalization: Absent a target schema, the goal is to consolidate semantically similar schema components, standardizing them to a singular representation to streamline the KG. Starting with an empty canonical schema, we examine the open KG triplets, searching for potential consolidation candidates through vector similarity and LLM verification. Unlike target alignment, components deemed non-transformable are added to the canonical schema, thereby expanding it.
Using our example, the prompt is:</li>
</ul>
<h2>Schema Canonicalization Prompt</h2>
<p>Given a piece of text, a relational triplet extracted from it, and the definition of the relation in it, choose the most appropriate relation to replace it in this context if there is any.
Text: Alan Shepard was born on Nov 18, 1923 and selected by NASA in 1959. He was a member of the Apollo 14 crew.
Triplets: ['Alan Shepard', 'participatedIn', 'Apollo 14']
Definition of 'participatedIn': The subject entity</p>
<p>took part in the event or mission specified by the object entity.
Choices:
A. 'mission': The subject entity participated in the event or operation specified by the object entity.
B. 'season': The subject entity participated in the season of a series specified by the object entity.
F. None of the above</p>
<p>Note that the choices above are obtained by using vector similarity search. After the LLM makes its choice, the relations are transformed to yield: ['Alan Shepard', 'birthDate', 'Nov 18, 1923'], ['Alan Shepard', 'mission', 'Apollo 14'], which forms our canonicalized KG.</p>
<h3>3.2 EDC+R: iteratively refine EDC with Schema Retriever</h3>
<p>The refinement process leverages the data generated by EDC to enhance the quality of the extracted triplets. Inspired by retrieval-augmented generation and prior work (Bi et al., 2024), we construct a "hint" for the extraction phase (details in Appendix A.4), which comprises two main elements:</p>
<ul>
<li>Candidate Entities: The entities extracted by EDC from the previous iteration, and entities extracted from the text using the LLM;</li>
<li>Candidate Relations: The relations extracted by EDC from the previous cycle and relations retrieved from the pre-defined/canonicalized schema by using a trained Schema Retriever.</li>
</ul>
<p>The inclusion of entities and relations from both the LLM and the schema retriever provides a richer pool of candidates for the LLM, which addresses issues where the absence of entities or relations impairs the LLM's effectiveness. By merging the entities and relations extracted in earlier phases with new findings from entity extraction and schema retrieval, the hint serves to aid the OIE by bootstrapping from the previous round.</p>
<p>To scale EDC to large schemas, we employ a trained Schema Retriever which allows us to efficiently search schemas. The Schema Retriever works in a similar fashion to information retrieval methods based on vector spaces (Ganguly et al., 2015; Lewis et al., 2020); it projects the schema components and the input text to a vector space such that cosine similarity captures the relevance between the two, i.e., how likely a schema component to be present in the input text. Note that in
our setting, the similarity space is different from the standard sentence embedding models where cosine similarity in the vector space captures semantic equivalence. Our Schema Retriever is a fine-tuned variant of the sentence embedding model E5-mistral-7b-instruct (Wang et al., 2023). We follow the original training methodology detailed in the paper, which involves utilizing pairs of text and their corresponding defined relations. For details, please refer to the Appendix A.3. For a given positive text-relation pair $\left(t^{+}, r^{+}\right)$, we employ an instruction template on $t^{+}$to generate a new text $t_{\text {inst }}^{+}=$"Instruct: retrieve relations that are present in the given text $\backslash n$ Query: $\left{t^{+}\right}$".</p>
<p>We then finetune the embedding model to distinguish between the correct relation associated with a given text and other non-relevant relations using the InfoNCE loss.</p>
<p>Back to our example, refinement with the schema retriever adds the following relation to the previous set: ['Alan Shepard', 'selectedByNasa', '1959']. The relation 'selectedByNasa' is rather obscure but was specified in the target schema.</p>
<h2>4 Experiments</h2>
<p>In this section, we describe experiments designed to evaluate the performance of EDC and EDC+R. Briefly, our results demonstrate that EDC significantly outperforms the state-of-the-art methods in both Target Alignment and Self Canonicalization settings. Refinement further improves EDC. Source code for EDC and to replicate our experiments are available in the supplementary materials, with full tables in the Appendix C.</p>
<h3>4.1 Experimental Setup</h3>
<p>Datasets. We evaluate EDC using three KGC datasets:</p>
<ul>
<li>WebNLG (Ferreira et al., 2020): We use the test split from the semantic parsing task of WebNLG+2020 (v3.0). It contains 1165 pairs of text and triplets. The schema derived from these reference triplets encompasses 159 unique relation types.</li>
<li>
<p>REBEL (Cabot and Navigli, 2021): The original test partition of REBEL comprises 105,516 entries. To manage costs, we select a random sample of 1000 text-triplet pairs. This subset induces a schema with 200 distinct relation types.</p>
</li>
<li>
<p>Wiki-NRE (Distiawan et al., 2019): From Wiki-NRE's test split (29,619 entries), we sample 1000 text-triplet pairs, resulting in a schema with 45 unique relation types.</p>
</li>
</ul>
<p>These datasets were chosen due to their richer variety of relation types over alternatives like ADE (Gurulingappa et al., 2012) (1 relation type), SciERC (Luan et al., 2018) (7 relation types), and CoNLL04 (Roth and Yih, 2004) (4 relation types) used to evaluate previous LLM-based methods (Bi et al., 2024; Wadhwa et al., 2023). This diversity better mimics real-world complexity. In our experiments, we focus on extracting relations as the only schema component available across all datasets. Relations, being a foundational element of KGs, are prioritized over other components like entity or event types. However, note that EDC can be readily extended to other schema components.</p>
<p>EDC Models. EDC contains multiple modules that are powered by LLMs. Since the OIE module is the key upstream module that determines the semantic content captured in the KG, we tested different LLMs of different sizes including GPT4 (Achiam et al., 2023), GPT-3.5-turbo (Brown et al., 2020), and Mistral-7b (Jiang et al., 2023). Mistral-7b was deployed on a local workstation, whereas the GPT models were accessed via the OpenAI API. For the framework's remaining components which required prompting, we used GPT-3.5-turbo. In the canonicalization phase, the E5-Mistral-7b model was utilized for vector similarity searches without modifications.</p>
<h3>4.1.1 Evaluation Criteria and Baselines</h3>
<p>We evaluate our methods differently under Target Alignment (when a schema is provided) and Self Canonicalization (no schema) due to the inherently different objectives: the former aims to recover the ground-truth annotated triplets consistent with the target schema while the latter is to extract semantically correct and meaningful triplets that induce a succinct and non-redundant KG without a predefined target to compare against. For the datasets above, the preivous LLM-based KGC methods (ChatIE and CodeKGC) could not be used due to the schema size. Although EDC is not intended for small domain-specific datasets, we include the results on SciERC and CoNLL04 in the Appendix E for the comprehensiveness of the evaluation.</p>
<p>Target Alignment. We compare EDC and EDC+R against the specialized trained models for
each of the datasets:</p>
<ul>
<li>REGEN (Dognin et al., 2021) is the SOTA model for WebNLG. It is a sequence-tosequence model that leverages pre-trained T5 (Raffel et al., 2020) and Reinforcement Learning (RL) for bidirectional text-to-graph and graph-to-text generation.</li>
<li>GenIE (Josifoski et al., 2022), a sequence-to-sequence model that leverages pre-trained BART (Lewis et al., 2019) and a constrained generation strategy to constrain the output triplets to be consistent with the pre-defined schema. GenIE is the state-of-the-art model for REBEL and Wiki-NRE.</li>
</ul>
<p>Following previous work (Dognin et al., 2021; Melnyk et al., 2022), we use the WEBNLG evaluation script (Ferreira et al., 2020) which computes the Precision, Recall, and F1 scores for the output triplets against the ground truth in a token-based manner. Metrics based on Named Entity Evaluation were used to measure the Precision, Recall, and F1 score in three different ways.</p>
<ul>
<li>Exact: Requires a complete match between the candidate and reference triple, disregarding the type (subject, relation, object).</li>
<li>Partial: Allows for at least a partial match between the candidate and reference triple, disregarding the type.</li>
<li>Strict: Demands an exact match between the candidate and reference triplet, including the element types.</li>
</ul>
<p>Self Canonicalization. For evaluating selfcanonicalization performance, comparisons are made with:</p>
<ul>
<li>Baseline Open KG, which is the initial open KG output from the OIE (Open Information Extraction) phase. This serves as a reference point to illustrate the changes in precision and schema conciseness resulting from the canonicalization process.</li>
<li>CESI (Vashishth et al., 2018), recognized as a leading clustering-based approach for open KG canonicalization. By applying CESI to the open KG, we aim to contrast its performance against canonicalization by EDC.</li>
</ul>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Performance of EDC and EDC+R on WebNLG, REBEL, and Wiki-NRE datasets against baselines in the Target Alignment setting (F1 scores with 'Partial' criteria). EDC+R only performs one iteration of refinement due to diminishing marginal improvement.</p>
<p>Given that canonicalized triplets may use relations phrased differently from the reference triplets or entirely out-of-schema relations, a token-based evaluation becomes unsuitable. Thus, we resort to manual evaluation, focusing on three key aspects that reflect the intrinsic quality of an extracted KG:</p>
<ul>
<li>Precision: The canonicalized triplets remain correct and meaningful with respect to the text compared to the OIE triplets.</li>
<li>Conciseness: The schema's brevity is measured by the number of relations types.</li>
<li>Redundancy: We employ a redundancy score - the average cosine similarity among each canonicalized relation and its nearest counterpart - where low scores indicate that the schema's relations are semantically distinct.</li>
</ul>
<h3>4.2 Results and Analysis</h3>
<p>In the following, we focus on conveying our main findings and results. For full results and tables, please refer to the Appendix.</p>
<h3>4.2.1 Target Alignment</h3>
<p>The bar charts in Figure 2 summarize the Partial F1 scores obtained by EDC and EDC+R on all three datasets with different LLMs for OIE compared against the respective baselines. EDC demonstrates performance that is superior to or on par with the state-of-the-art baselines for all evaluated datasets. Comparing the LLMs, GPT-4 emerges as the top performer, with Mistral-7b and GPT-3.5-turbo exhibiting comparable results. The disparity between our methods and the baselines is more pronounced on the REBEL and Wiki-NRE
datasets; this is primarily due to the GenIE's constrained generation approach, which falls short in extracting triplets that include literals, such as numbers and dates.</p>
<p>Refinement (EDC+R) consistently and significantly enhances performance. Post-refinement, the difference in performance between GPT-3.5turbo and Mistral-7b is larger, suggesting Mistral7 b's was not as able to leverage the provided hints. Nevertheless, a single refinement iteration with the hint improved performance for all the tested LLMs.</p>
<p>From the scores, it appears that EDC performance is significantly better on WebNLG compared to REBEL and Wiki-NRE. However, we observed that EDC was penalized despite producing valid triplets on the latter datasets. A reason for this is that the reference triplets in these datasets are non-exhaustive. For example, given the text in the REBEL dataset, 'Romany Love is a 1931 British musical film directed by Fred Paul and starring Esmond Knight, Florence McHugh and Roy Travers.', EDC extracts: ['Romany Love', 'cast member', 'Esmond Knight'], ['Romany Love', 'cast member', 'Florence McHugh'], ['Romany Love', 'cast member', 'Roy Travers'], which are all semantically correct, but only the first triplet is present in the reference set. The datasets also contain reference triplets based on information extraneous to the text, e.g., 'Daniel is an Ethiopian footballer, who currently plays for Hawassa City S.C.' has a corresponding reference triplet ['Hawassa City S.C.', 'country', 'Ethiopia'].</p>
<p>These issues can be attributed to the distinct methodologies employed in the creation of these datasets. For WebNLG, annotators were asked to</p>
<p>Table 1: Ablation study results (F1 scores with all criteria) on schema retriever, the LLM used for OIE is GPT-3.5-turbo. S.R. stands for Schema Retriever.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Partial</th>
<th style="text-align: center;">Strict</th>
<th style="text-align: center;">Exact</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">WebNLG</td>
<td style="text-align: center;">EDC+R</td>
<td style="text-align: center;">0.794</td>
<td style="text-align: center;">0.753</td>
<td style="text-align: center;">0.772</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EDC+R w/o S.R.</td>
<td style="text-align: center;">0.752</td>
<td style="text-align: center;">0.701</td>
<td style="text-align: center;">0.721</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EDC</td>
<td style="text-align: center;">0.746</td>
<td style="text-align: center;">0.688</td>
<td style="text-align: center;">0.713</td>
</tr>
<tr>
<td style="text-align: center;">REBEL</td>
<td style="text-align: center;">EDC+R</td>
<td style="text-align: center;">0.559</td>
<td style="text-align: center;">0.516</td>
<td style="text-align: center;">0.529</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EDC+R w/o S.R.</td>
<td style="text-align: center;">0.517</td>
<td style="text-align: center;">0.466</td>
<td style="text-align: center;">0.482</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EDC</td>
<td style="text-align: center;">0.506</td>
<td style="text-align: center;">0.449</td>
<td style="text-align: center;">0.473</td>
</tr>
<tr>
<td style="text-align: center;">Wiki-NRE</td>
<td style="text-align: center;">EDC+R</td>
<td style="text-align: center;">0.693</td>
<td style="text-align: center;">0.685</td>
<td style="text-align: center;">0.657</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EDC+R w/o S.R.</td>
<td style="text-align: center;">0.653</td>
<td style="text-align: center;">0.645</td>
<td style="text-align: center;">0.641</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EDC</td>
<td style="text-align: center;">0.647</td>
<td style="text-align: center;">0.638</td>
<td style="text-align: center;">0.640</td>
</tr>
</tbody>
</table>
<p>compose text solely from the triplets. Thus, the text and the triplets have a direct correspondence, and the text typically does not include information other than what is apparent from the triplets. In contrast, REBEL and Wiki-NRE are created by aligning text and triplets using distant supervision (Smirnova and Cudré-Mauroux, 2018). This approach can result in less straightforward triplet extraction and incomplete reference sets, leading to overly pessimistic evaluations for methods like EDC, which generate correct triplets not present in the dataset. (Han et al., 2023; Wadhwa et al., 2023). On average, EDC extracts one additional triplet per sentence on REBEL and Wiki-NRE compared to the reference set, while on WebNLG, it extracts a similar number of triplets to the reference.</p>
<p>Ablation study on schema retriever. To evaluate the impact of the relations provided by the schema retriever during refinement, we conducted an ablation study with GPT-3.5-turbo by removing these relations. The results in Table 1 show that ablating the Schema Retriever leads to a decline in performance. Qualitatively, we find that the schema retriever helps to find relevant relations that are challenging for the LLMs to identify during the OIE stage. For example, given the text 'The University of Burgundy in Dijon has 16,800 undergraduate students', the LLMs extract ['University of Burgundy', 'location', 'Dijon'] during OIE. Although semantically correct, this relation overlooks the more specific relation present in the target schema, namely 'campus', for denoting university's location. The schema retriever successfully identifies this finer relation, enabling the LLMs to adjust their extraction to ['University of Burgundy', 'campus', 'Dijon']. This experiment highlights the schema retriever's value in facilitating the extraction of precise and contextually appropriate relations.</p>
<p>Table 2: Performance of EDC in the Self Canonicalization setting (human-evaluated precision and schema metrics). The best result for each dataset and metric is bolded. Prec. stands for precision, No. Rel. stands for the number of relations and Red. stands for redundancy score.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Prec. ( $\uparrow$ )</th>
<th style="text-align: center;">No. Rel. $(\downarrow)$</th>
<th style="text-align: center;">Red. $(\downarrow)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">WebNLG</td>
<td style="text-align: center;">EDC</td>
<td style="text-align: center;">0.956</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">0.833</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CESI</td>
<td style="text-align: center;">0.724</td>
<td style="text-align: center;">280</td>
<td style="text-align: center;">0.893</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Open KG</td>
<td style="text-align: center;">0.982</td>
<td style="text-align: center;">529</td>
<td style="text-align: center;">0.927</td>
</tr>
<tr>
<td style="text-align: center;">REBEL</td>
<td style="text-align: center;">EDC</td>
<td style="text-align: center;">0.867</td>
<td style="text-align: center;">225</td>
<td style="text-align: center;">0.831</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CESI</td>
<td style="text-align: center;">0.504</td>
<td style="text-align: center;">307</td>
<td style="text-align: center;">0.854</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Open KG</td>
<td style="text-align: center;">0.903</td>
<td style="text-align: center;">667</td>
<td style="text-align: center;">0.895</td>
</tr>
<tr>
<td style="text-align: center;">Wiki-NRE</td>
<td style="text-align: center;">EDC</td>
<td style="text-align: center;">0.898</td>
<td style="text-align: center;">106</td>
<td style="text-align: center;">0.833</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CESI</td>
<td style="text-align: center;">0.753</td>
<td style="text-align: center;">114</td>
<td style="text-align: center;">0.849</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Open KG</td>
<td style="text-align: center;">0.909</td>
<td style="text-align: center;">204</td>
<td style="text-align: center;">0.881</td>
</tr>
</tbody>
</table>
<h3>4.2.2 Self Canonicalization</h3>
<p>Here, we focus on evaluating EDC's selfcanonicalization performance (utilizing GPT-3.5turbo for OIE). We omit refinement in Self Canonicalization setting as it has already been studied above and in subsequent iterations, the selfconstructed canonicalized schema becomes the target schema. Following prior work (Wadhwa et al., 2023; Kolluru et al., 2020), we conducted a targeted human evaluation of knowledge graphs. This evaluation involved two independent annotators assessing the reasonableness of triplet extractions from given text without prior knowledge of the system's details. We observed a high inter-annotator agreement score of 0.94 .</p>
<p>The evaluation results and schema metrics are summarized in Table 2.While the open KG generated by the OIE stage contains semantically valid triplets (which affirms the previous findings that LLMs are competent open information extractors (Li et al., 2023)), there is a significant degree of redundancy within the resultant schema. EDC accurately canonicalizes the open KG and yields a schema that is both more concise and less redundant compared to CESI. EDC avoids CESI's tendency toward over-generalization - in line with prior work (Putri et al., 2019), we observed CESI inappropriately clusters diverse relations such as 'place of death', 'place of birth', 'date of death', 'date of birth', and 'cause of death' into a single 'date of death' category.</p>
<h2>5 Conclusion</h2>
<p>In this work, we presented EDC, an LLM-based three-phase framework that addresses the problem of KGC by open information extraction followed by post-hoc canonicalization. Experiments show</p>
<p>that EDC and EDC+R are able to extract better KGs than specialized trained models when a target schema is available and dynamically create a schema when none is provided. The scalability and versatility of EDC opens up many opportunities for applications: it allows us to automatically extract high-quality KGs from general text using large schemas like Wikidata (Vrandečić and Krötzsch, 2014) and even enrich these schemas with newly discovered relations.</p>
<h2>6 Limitations and Future Directions</h2>
<p>There are several limitations that we would like to address in future works.</p>
<ul>
<li>We only considered schema canonicalization within the scope of this paper, it is of great interest to incorporate an entity de-duplication mechanism in the future to reduce the redundancy in the constructed KGs, e.g., via coreference resolution (Sukthanker et al., 2020). We briefly explored this approach and the preliminary results can be found in Appendix F.</li>
<li>EDC's components can be further improved to boost performance. Specifically, the schema retriever may benefit from training on more diverse and higher-quality data.</li>
<li>Due to time and resource constraints, we only tested different LLMs for OIE while all the other modules of EDC rely on GPT-3.5-turbo, it will be beneficial to test the smaller opensource models' performance on the other tasks as well.</li>
<li>EDC is a costly framework, involving a large number of LLM calls. When GPT-3.5-turbo is used for all components, the cost was around 0.009 USD per example in our experiments. It is possible to have certain components replaced by smaller fine-tuned models - previous works have shown smaller language models can be fine-tuned for OIE (Wadhwa et al., 2023) and smaller BERT-based classifiers can be trained for schema canonicalization. We also explored the possibility of combining the two stages of OIE and Schema Definition in Appendix G.</li>
<li>We are looking to apply EDC towards embodied AI and robotics. Specifically, KGs can form memory sources for VLMs, containing
facts about humans (Zhang and Soh, 2023), the task or goal (Xie et al., 2023), and the environment.</li>
</ul>
<h2>7 Ethical Considerations</h2>
<p>Artifact usage. The datasets we used in the paper are only leveraged for research purposes and we strictly follow the corresponding licenses (e.g. WebNLG uses cc-by-nc-sa-4.0). It is to be noted that, due to the nature of the task, the datasets may inherently contain information about individuals (especially celebrities). Software and code for this paper is publicly available at https: //github.com/clear-nus/edc.</p>
<p>Human annotators. The two annotators (1 male and 1 female) are recruited university students. The annotators are compensated fairly and given abundant and flexible time to complete the tasks. The collection protocol is determined exempt by our institution's IRB committee.</p>
<p>Potential Risks. The use of current LLMs may incur risks such as hallucinations (Xu et al., 2024) and privacy issues (Yao et al., 2024).</p>
<h2>Acknowledgements</h2>
<p>This research is supported by the National Research Foundation Singapore and DSO National Laboratories under the AI Singapore Programme (AISG Award No: AISG2-RP-2020-016).</p>
<h2>References</h2>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.</p>
<p>Oshin Agarwal, Heming Ge, Siamak Shakeri, and Rami Al-Rfou. 2020. Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training. arXiv preprint arXiv:2010.12688.</p>
<p>Zhen Bi, Jing Chen, Yinuo Jiang, Feiyu Xiong, Wei Guo, Huajun Chen, and Ningyu Zhang. 2024. Codekgc: Code language model for generative knowledge graph construction. ACM Transactions on Asian and Low-Resource Language Information Processing, 23(3):1-16.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot</p>
<p>learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Pere-Lluís Huguet Cabot and Roberto Navigli. 2021. Rebel: Relation extraction by end-to-end language generation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 23702381.</p>
<p>Eunsol Choi, Omer Levy, Yejin Choi, and Luke Zettlemoyer. 2018. Ultra-fine entity typing. arXiv preprint arXiv:1807.04905.</p>
<p>Sarthak Dash, Gaetano Rossiello, Nandana Mihindukulasooriya, Sugato Bagchi, and Alfio Gliozzo. 2020. Open knowledge graphs canonicalization using variational autoencoders. arXiv preprint arXiv:2012.04780.</p>
<p>Bayu Distiawan, Gerhard Weikum, Jianzhong Qi, and Rui Zhang. 2019. Neural relation extraction for knowledge base enrichment. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 229-240.</p>
<p>Pierre L Dognin, Inkit Padhi, Igor Melnyk, and Payel Das. 2021. Regen: Reinforcement learning for text and knowledge base generation using pretrained language models. arXiv preprint arXiv:2108.12472.</p>
<p>Thiago Castro Ferreira, Claire Gardent, Nikolai Ilinykh, Chris Van Der Lee, Simon Mille, Diego Moussallem, and Anastasia Shimorina. 2020. The 2020 bilingual, bi-directional webnlg+ shared task overview and evaluation results (webnlg+ 2020). In Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+).</p>
<p>Debasis Ganguly, Dwaipayan Roy, Mandar Mitra, and Gareth JF Jones. 2015. Word embedding based generalized language model for information retrieval. In Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval, pages 795-798.</p>
<p>Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch. 2013. Ppdb: The paraphrase database. In Proceedings of the 2013 conference of the north american chapter of the association for computational linguistics: Human language technologies, pages 758-764.</p>
<p>Liang Guo, Fu Yan, Yuqian Lu, Ming Zhou, and Tao Yang. 2021. An automatic machining process decision-making system based on knowledge graph. International journal of computer integrated manufacturing, 34(12):1348-1369.</p>
<p>Qingyu Guo, Fuzhen Zhuang, Chuan Qin, Hengshu Zhu, Xing Xie, Hui Xiong, and Qing He. 2020. A survey on knowledge graph-based recommender systems. IEEE Transactions on Knowledge and Data Engineering, 34(8):3549-3568.</p>
<p>Harsha Gurulingappa, Abdul Mateen Rajput, Angus Roberts, Juliane Fluck, Martin Hofmann-Apitius, and Luca Toldo. 2012. Development of a benchmark
corpus to support the automatic extraction of drugrelated adverse effects from medical case reports. Journal of biomedical informatics, 45(5):885-892.</p>
<p>Ridong Han, Tao Peng, Chaohao Yang, Benyou Wang, Lu Liu, and Xiang Wan. 2023. Is information extraction solved by chatgpt? an analysis of performance, evaluation criteria, robustness and errors. arXiv preprint arXiv:2305.14450.</p>
<p>Xiao Huang, Jingyuan Zhang, Dingcheng Li, and Ping Li. 2019. Knowledge graph embedding based question answering. In Proceedings of the twelfth ACM international conference on web search and data mining, pages 105-113.</p>
<p>Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and S Yu Philip. 2021. A survey on knowledge graphs: Representation, acquisition, and applications. IEEE transactions on neural networks and learning systems, 33(2):494-514.</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825.</p>
<p>Martin Josifoski, Nicola De Cao, Maxime Peyrard, Fabio Petroni, and Robert West. 2022. GenIE: Generative information extraction. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4626-4643, Seattle, United States. Association for Computational Linguistics.</p>
<p>Serafina Kamp, Morteza Fayazi, Zineb Benameur-El, Shuyan Yu, and Ronald Dreslinski. 2023. Open information extraction: A review of baseline techniques, approaches, and applications. arXiv preprint arXiv:2310.11644.</p>
<p>Keshav Kolluru, Vaibhav Adlakha, Samarth Aggarwal, Soumen Chakrabarti, et al. 2020. Openie6: Iterative grid labeling and coordination analysis for open information extraction. arXiv preprint arXiv:2010.03147.</p>
<p>Luong Thi Hong Lan, Tran Manh Tuan, Tran Thi Ngan, Nguyen Long Giang, Vo Truong Nhu Ngoc, Pham Van Hai, et al. 2020. A new complex fuzzy inference system with fuzzy knowledge graph and extensions in decision making. Ieee Access, 8:164899-164921.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459-9474.</p>
<p>Bo Li, Gexiang Fang, Yang Yang, Quansen Wang, Wei Ye, Wen Zhao, and Shikun Zhang. 2023. Evaluating chatgpt's information extraction capabilities: An assessment of performance, explainability, calibration, and faithfulness. arXiv preprint arXiv:2304.11633.</p>
<p>Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157-173.</p>
<p>Pai Liu, Wenyang Gao, Wenjie Dong, Songfang Huang, and Yue Zhang. 2022. Open information extraction from 2007 to 2022-a survey. arXiv preprint arXiv:2208.08690.</p>
<p>Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi. 2018. Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction. arXiv preprint arXiv:1808.09602.</p>
<p>Pedro Henrique Martins, Zita Marinho, and André FT Martins. 2019. Joint learning of named entity recognition and entity linking. arXiv preprint arXiv:1907.08243.</p>
<p>Igor Melnyk, Pierre Dognin, and Payel Das. 2022. Knowledge graph generation from text. arXiv preprint arXiv:2211.10511.</p>
<p>George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39-41.</p>
<p>Yasumasa Onoe and Greg Durrett. 2020. Fine-grained entity typing for domain independent entity linking. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8576-8583.</p>
<p>Shon Otmazgin, Arie Cattan, and Yoav Goldberg. 2023. LingMess: Linguistically informed multi expert scorers for coreference resolution. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 27522760, Dubrovnik, Croatia. Association for Computational Linguistics.</p>
<p>Rifki Afina Putri, Giwon Hong, and Sung-Hyon Myaeng. 2019. Aligning open ie relations and kb relations using a siamese network based on word embedding. In Proceedings of the 13th International Conference on Computational Semantics-Long Papers, pages 142-153.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1-67.</p>
<p>Dan Roth and Wen-tau Yih. 2004. A linear programming formulation for global inference in natural language tasks. In Proceedings of the eighth conference
on computational natural language learning (CoNLL2004) at HLT-NAACL 2004, pages 1-8.</p>
<p>Alisa Smirnova and Philippe Cudré-Mauroux. 2018. Relation extraction using distant supervision: A survey. ACM Computing Surveys (CSUR), 51(5):1-35.</p>
<p>Rhea Sukthanker, Soujanya Poria, Erik Cambria, and Ramkumar Thirunavukarasu. 2020. Anaphora and coreference resolution: A review. Information Fusion, 59:139-162.</p>
<p>Qingyu Tan, Lu Xu, Lidong Bing, Hwee Tou Ng, and Sharifah Mahani Aljunied. 2022. Revisiting docred - addressing the false negative problem in relation extraction. In Proceedings of EMNLP.</p>
<p>Shikhar Vashishth, Prince Jain, and Partha Talukdar. 2018. Cesi: Canonicalizing open knowledge bases using embeddings and side information. In Proceedings of the 2018 World Wide Web Conference, pages $1317-1327$.</p>
<p>Denny Vrandečić and Markus Krötzsch. 2014. Wikidata: a free collaborative knowledgebase. Communications of the ACM, 57(10):78-85.</p>
<p>Somin Wadhwa, Silvio Amir, and Byron C Wallace. 2023. Revisiting relation extraction in the era of large language models. In Proceedings of the conference. Association for Computational Linguistics. Meeting, volume 2023, page 15566. NIH Public Access.</p>
<p>Hongwei Wang, Miao Zhao, Xing Xie, Wenjie Li, and Minyi Guo. 2019. Knowledge graph convolutional networks for recommender systems. In The world wide web conference, pages 3307-3313.</p>
<p>Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2023. Improving text embeddings with large language models. arXiv preprint arXiv:2401.00368.</p>
<p>Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang, Xin Zhang, Shen Huang, Pengjun Xie, Jinan Xu, Yufeng Chen, Meishan Zhang, et al. 2023. Zeroshot information extraction via chatting with chatgpt. arXiv preprint arXiv:2302.10205.</p>
<p>Yaqi Xie, Chen Yu, Tongyao Zhu, Jinbin Bai, Ze Gong, and Harold Soh. 2023. Translating natural language to planning goals with large-language models. arXiv preprint arXiv:2302.05128.</p>
<p>Ziwei Xu, Sanjay Jain, and Mohan Kankanhalli. 2024. Hallucination is inevitable: An innate limitation of large language models. arXiv preprint arXiv:2401.11817.</p>
<p>Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Zhibo Sun, and Yue Zhang. 2024. A survey on large language model (llm) security and privacy: The good, the bad, and the ugly. High-Confidence Computing, page 100211 .</p>
<p>Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, and Jure Leskovec. 2021. Qagnn: Reasoning with language models and knowledge graphs for question answering. arXiv preprint arXiv:2104.06378.</p>
<p>Hongbin Ye, Ningyu Zhang, Hui Chen, and Huajun Chen. 2022. Generative knowledge graph construction: A review. arXiv preprint arXiv:2210.12714.</p>
<p>Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao. 2015. Distant supervision for relation extraction via piecewise convolutional neural networks. In Proceedings of the 2015 conference on empirical methods in natural language processing, pages 1753-1762.</p>
<p>Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, and Jun Zhao. 2014. Relation classification via convolutional deep neural network. In Proceedings of COLING 2014, the 25th international conference on computational linguistics: technical papers, pages $2335-2344$.</p>
<p>Bowen Zhang and Harold Soh. 2023. Large language models as zero-shot human models for human-robot interaction. In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 7961-7968. IEEE.</p>
<p>Lingfeng Zhong, Jia Wu, Qian Li, Hao Peng, and Xindong Wu. 2023. A comprehensive survey on automatic knowledge graph construction. ACM Computing Surveys, 56(4):1-62.</p>
<p>Shaowen Zhou, Bowen Yu, Aixin Sun, Cheng Long, Jingyang Li, Haiyang Yu, Jian Sun, and Yongbin Li. 2022. A survey on neural open information extraction: Current status and future directions. arXiv preprint arXiv:2205.11725.</p>
<p>Andrej Žukov-Gregorič, Yoram Bachrach, and Sam Coope. 2018. Named entity recognition with parallel recurrent neural networks. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 69-74.</p>
<h2>A Implementation Details</h2>
<h2>A. 1 Models and Infrastructures Details</h2>
<p>We use two OpenAI models, GPT-3.5-turbo and GPT-4 (sizes currently unknown), and an opensource model, Mistral-7b ( 7 billion parameters). The training and inference of open-source models are done with a single machine with an AMD EPYC 7543P 32-Core Processor and 252GB of RAM, equipped with 4 NVIDIA RTX A6000 (48GB) GPUs. We accessed GPT-3.5-turbo and GPT-4 via the OpenAI API. Code for EDC is available at https://github.com/clear-nus/edc.</p>
<h2>A. 2 Prompting-related hyperparameters</h2>
<p>We use few-shot prompting for all modules of EDC, we empirically choose 6 -shot examples from the respective datasets. For the MCQ used in the Schema Canonicalization phase, we retrieve top-5 semantically similar relations from the schema as candidates. For refinement, the schema retriever retrieves top-10 most relevant relations from the schema as candidate relations. These hyperparameters are empirically chosen to balance performance and inference costs.</p>
<h2>A. 3 Schema Retriever Training</h2>
<p>We follow the original training methodology detailed in the original paper (Wang et al., 2023), which involves utilizing pairs of text and their corresponding defined relations. For a given positive text-relation pair $\left(t^{+}, r^{+}\right)$, we employ an instruction template on $t^{+}$to generate a new text $t_{\text {inst }}^{+}=$ "Instruct: retrieve relations that are present in the given text $\backslash n$ Query: $\left{t^{+}\right}$".</p>
<p>We then finetune the embedding model to distinguish between the correct relation associated with a given text and other non-relevant relations using the InfoNCE loss,
$\min \mathcal{L}=-\log \frac{\phi\left(t_{\text {inst }}^{+}, r^{+}\right)}{\phi\left(t_{\text {inst }}^{+}, r^{+}\right)+\sum_{n_{i} \in N} \phi\left(t_{\text {inst }}^{+}, n_{i}\right)}$
Here, $N$ denotes the set of negative samples, and $\phi$ represents the cosine similarity function. Please see the appendix for additional training details.</p>
<p>For training, we synthesized a dataset of textrelation pairs using the TEKGEN dataset (Agarwal et al., 2020), a large-scale text-triplets dataset created by aligning Wikidata triplets to Wikipedia text. The training dataset comprised 37,500 pairs, evenly divided between positive and negative samples. We
adopted an online open-source implementation and hyperparameter configurations for training.</p>
<p>The performance of the fine-tuned schema retriever was assessed on the test splits of WebNLG, REBEL, and Wiki-NRE datasets. The recall@10 scores on these datasets were $0.823,0.663$, and 0.818 , respectively, indicating the effectiveness of the retriever across different knowledge graph contexts.</p>
<h2>A. 4 Details of Refinement Hint</h2>
<p>The refinement hint consists of candidate entities and candidate relations. This section details the obtainment of them and how they are used to improve the OIE performance. We will carry on using the example used in Section 3: "Alan Shepard was born on Nov 18, 1923 and selected by NASA in 1959. He was a member of the Apollo 14 crew" and the triplets extracted by EDC in the first iteration are ['Alan Shepard', 'birthDate', 'Nov 18, 1923'], ['Alan Shepard', 'mission', 'Apollo 14'].</p>
<h2>A.4.1 Obtaining Candidate Entities</h2>
<p>The candidate entities come from two sources:</p>
<ul>
<li>Entities extracted by EDC from the previous iteration, i.e. ['Alan Shepard', 'Nov 18, 1923', 'Apollo 14']</li>
<li>Entities extracted from the text by prompting the LLM to perform an entity extraction task, similar to the triplet extraction task.</li>
</ul>
<h2>Entity Extraction Prompt</h2>
<p>Given a piece of text, extract a list of entities from it.
Here are some examples:
Example 1:
Text: The 17068.8 millimeter long ALCO RS-3 has a diesel-electric transmission. Entities: ['ALCO RS-3', 'Diesel-electric transmission', '17068.8 (millimetres)']
Now please extract entities from the following text: Alan Shepard was born on Nov 18, 1923 and selected by NASA in 1959. He was a member of the Apollo 14 crew.
and the resultant entities are ['Alan Shepard', 'Nov 18, 1923', 'NASA', '1959', 'Apollo 14']</p>
<p>The entities are then merged together as the candidate entities.</p>
<h2>A.4.2 Obtaining Candidate Relations</h2>
<p>The candidate relations also come from two sources:</p>
<ul>
<li>Relations extracted by EDC from the previous iteration, i.e. ['birthDate', 'mission']</li>
<li>Relations extracted by the schema retriever, by calculating the relevance score between the input text and the relations in the schema. The top 5 retrieved relations in this case are ['birthDate', , 'selectedByNasa', 'mission', 'draftPick', 'occupation'].</li>
</ul>
<p>The relations and their corresponding definitions are then merged together as the candidate relations. It is to be noted that, similar to other RAG-based methods, there is a chance that the retriever may retrieve irrelevant information. In this case, the relation definitions can come in handy as it provides more information for the LLMs to decide whether the relation is a valid one with respect to the text or not.</p>
<h2>A.4.3 Usage of Hint for Refined OIE</h2>
<p>The refinement hint is then included in the prompt appropriately to instruct the LLMs to consider (but is not limited to) the candidate entities and candidate relations:</p>
<h2>Refined OIE Prompt</h2>
<p>Given a piece of text, extract relational triplets in the form of [Subject, Relation, Object] from it. Here are some examples:
Example 1:
Text: The 17068.8 millimeter long ALCO RS-3 has a diesel-electric transmission.
Entities: ['ALCO RS-3', 'Diesel-electric transmission', '17068.8 (millimetres)']
Triplets: [['ALCO RS-3', 'powerType', 'Dieselelectric transmission'], ['ALCO RS-3', 'length', '17068.8 (millimetres)']]</p>
<p>Now please extract triplets from the following text: Alan Shepard was born on Nov 18, 1923 and selected by NASA in 1959. He was a member of the Apollo 14 crew. Entities: ['Alan Shepard', 'Nov 18, 1923', 'NASA', '1959', 'Apollo 14'] Here are some potential relations and their descriptions you may look out for during extraction: 1. birthDate: The subject entity was born on the date specified by the object entity.
2. mission: The subject entity participated in the event or operation specified by the object entity. 3. selectedByNasa: The subject entity was selected by NASA in the year specified by the object entity.</p>
<h2>Knowledge Graph Evaluation</h2>
<p>Given a piece of text and a list of triplets in the format of [Subject, Relation, Object], please select all the triplets that you think are correct with respect to the text.</p>
<p>For example, given a text, 'John Bull works as a teacher', both [John Bull, occupation, teacher] and [John Bull, profession, teacher] are considered correct, while [John Bull, job, minee] would be incorrect.</p>
<p>Sign in to Google to save your progress. Learn more</p>
<ol>
<li>He is most well known for his lead role in 'Les amités particulières', the film adaptation of the eponymous novel by Roger Peyrefitte, as Alexandre Motier.
$\square$ ['Les amités particulières', 'based on', 'novel by Roger Peyrefitte]
$\square$ ['Les amités particulières', 'title', 'Les amités particulières']
$\square$ ['Les amités particulières', 'genre', 'film']
$\square$ [Alexandre Motier', 'portrayed by', 'lead role']
$\square$ [Alexandre Motier', 'character in', "Les amités particulières"]</li>
</ol>
<p>Figure 3: An example screenshot of the questionnaire including the instructions given to the annotators.</p>
<p>The extracted triplets by the refined OIE are:['Alan Shepard', 'birthDate', 'Nov 18, 1923'], ['Alan Shepard', 'mission', 'Apollo 14'], ['Alan Shepard', 'selectedByNasa', '1959']. It successfully recovers the subtle and fine-grained relation 'selectedByNasa' that would have been missed without using the hint. Also, the semantically rich descriptions help the LLM avoid excessively extracting noisy relations retrieved by the schema retriever.</p>
<p>We found it important to include the entities from both sources, i.e. extractions from the last round and discovered by a separate module (entity extraction or schema retriever). The significance of the schema retriever is already shown in the ablation study in Sec 4.2.1.</p>
<h2>B Annotation Instruction</h2>
<p>An example screenshot is provided in Figure 3 to illustrate the format of questionnaires and instructions the annotators are given. The purpose of collection of the data was communicated to the annotators verbally.</p>
<h2>C Detailed Results of Target Alignment</h2>
<h2>C. 1 Complete Results</h2>
<p>The complete results of EDC and EDC+R on WebNLG, REBEL and Wiki-NRE are summarized in Table 3, Table 4 and Table 5 respectively. EDC performs better than or comparable to state-of-theart baseline models in terms of all metrics (Precision, Recall, and F1) in all criteria (Partial, Strict, and Exact) and EDC+R is able to consistently im-</p>
<p>Table 3: Complete results of EDC and EDC+R on WebNLG dataset against the baseline REGEN (Precision, Recall, F1 with 'Partial', 'Strict' and 'Exact' criteria). EDC+R only performs 1 iteration of refinement. The best results are bolded.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>LLM for OIE</th>
<th>Partial</th>
<th></th>
<th></th>
<th>Strict</th>
<th></th>
<th></th>
<th>Exact</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Precision</td>
<td>Recall</td>
<td>F1</td>
<td>Precision</td>
<td>Recall</td>
<td>F1</td>
<td>Precision</td>
<td>Recall</td>
<td>F1</td>
</tr>
<tr>
<td>EDC</td>
<td>GPT-4</td>
<td>0.776</td>
<td>0.796</td>
<td>0.783</td>
<td>0.729</td>
<td>0.741</td>
<td>0.733</td>
<td>0.751</td>
<td>0.765</td>
<td>0.756</td>
</tr>
<tr>
<td></td>
<td>GPT-3.5</td>
<td>0.739</td>
<td>0.760</td>
<td>0.746</td>
<td>0.684</td>
<td>0.697</td>
<td>0.688</td>
<td>0.708</td>
<td>0.722</td>
<td>0.713</td>
</tr>
<tr>
<td></td>
<td>Mistral-7b</td>
<td>0.723</td>
<td>0.739</td>
<td>0.728</td>
<td>0.668</td>
<td>0.679</td>
<td>0.672</td>
<td>0.692</td>
<td>0.703</td>
<td>0.696</td>
</tr>
<tr>
<td>EDC+R</td>
<td>GPT-4</td>
<td>0.814</td>
<td>0.831</td>
<td>0.820</td>
<td>0.782</td>
<td>0.794</td>
<td>0.786</td>
<td>0.796</td>
<td>0.808</td>
<td>0.800</td>
</tr>
<tr>
<td></td>
<td>GPT-3.5</td>
<td>0.788</td>
<td>0.806</td>
<td>0.794</td>
<td>0.749</td>
<td>0.761</td>
<td>0.753</td>
<td>0.768</td>
<td>0.781</td>
<td>0.772</td>
</tr>
<tr>
<td></td>
<td>Mistral-7b</td>
<td>0.756</td>
<td>0.775</td>
<td>0.762</td>
<td>0.716</td>
<td>0.727</td>
<td>0.720</td>
<td>0.735</td>
<td>0.747</td>
<td>0.739</td>
</tr>
<tr>
<td>Baseline</td>
<td>REGEN</td>
<td>0.755</td>
<td>0.788</td>
<td>0.767</td>
<td>0.713</td>
<td>0.735</td>
<td>0.720</td>
<td>0.714</td>
<td>0.738</td>
<td>0.723</td>
</tr>
</tbody>
</table>
<p>Table 4: Complete results of EDC and EDC+R on REBEL dataset against the baseline REGEN (Precision, Recall, F1 with 'Partial', 'Strict', and 'Exact' criteria). EDC+R only performs 1 iteration of refinement. The best results are bolded.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>LLM for OIE</th>
<th>Partial</th>
<th></th>
<th></th>
<th>Strict</th>
<th></th>
<th></th>
<th>Exact</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Precision</td>
<td>Recall</td>
<td>F1</td>
<td>Precision</td>
<td>Recall</td>
<td>F1</td>
<td>Precision</td>
<td>Recall</td>
<td>F1</td>
</tr>
<tr>
<td>EDC</td>
<td>GPT-4</td>
<td>0.543</td>
<td>0.552</td>
<td>0.546</td>
<td>0.498</td>
<td>0.503</td>
<td>0.500</td>
<td>0.511</td>
<td>0.517</td>
<td>0.514</td>
</tr>
<tr>
<td></td>
<td>GPT-3.5</td>
<td>0.503</td>
<td>0.512</td>
<td>0.506</td>
<td>0.448</td>
<td>0.453</td>
<td>0.449</td>
<td>0.471</td>
<td>0.476</td>
<td>0.473</td>
</tr>
<tr>
<td></td>
<td>Mistral-7b</td>
<td>0.512</td>
<td>0.523</td>
<td>0.516</td>
<td>0.450</td>
<td>0.457</td>
<td>0.453</td>
<td>0.481</td>
<td>0.488</td>
<td>0.483</td>
</tr>
<tr>
<td>EDC+R</td>
<td>GPT-4</td>
<td>0.599</td>
<td>0.606</td>
<td>0.601</td>
<td>0.557</td>
<td>0.561</td>
<td>0.559</td>
<td>0.572</td>
<td>0.576</td>
<td>0.574</td>
</tr>
<tr>
<td></td>
<td>GPT-3.5</td>
<td>0.556</td>
<td>0.565</td>
<td>0.559</td>
<td>0.513</td>
<td>0.519</td>
<td>0.516</td>
<td>0.527</td>
<td>0.533</td>
<td>0.529</td>
</tr>
<tr>
<td></td>
<td>Mistral-7b</td>
<td>0.525</td>
<td>0.550</td>
<td>0.531</td>
<td>0.461</td>
<td>0.462</td>
<td>0.462</td>
<td>0.506</td>
<td>0.511</td>
<td>0.505</td>
</tr>
<tr>
<td>Baseline</td>
<td>GENIE</td>
<td>0.381</td>
<td>0.391</td>
<td>0.385</td>
<td>0.353</td>
<td>0.361</td>
<td>0.356</td>
<td>0.362</td>
<td>0.369</td>
<td>0.364</td>
</tr>
</tbody>
</table>
<p>Table 5: Complete results of EDC and EDC+R on Wiki-NRE dataset against the baseline REGEN (Precision, Recall, F1 with 'Partial', 'Strict', and 'Exact' criteria). EDC+R only performs 1 iteration of refinement. The best results are bolded.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>LLM for OIE</th>
<th>Precision</th>
<th>Partial Recall</th>
<th>F1</th>
<th>Precision</th>
<th>Strict Recall</th>
<th>F1</th>
<th>Precision</th>
<th>Exact Recall</th>
<th>F1</th>
</tr>
</thead>
<tbody>
<tr>
<td>EDC</td>
<td>GPT-4</td>
<td>0.682</td>
<td>0.686</td>
<td>0.683</td>
<td>0.675</td>
<td>0.679</td>
<td>0.677</td>
<td>0.676</td>
<td>0.680</td>
<td>0.678</td>
</tr>
<tr>
<td></td>
<td>GPT-3.5</td>
<td>0.645</td>
<td>0.651</td>
<td>0.647</td>
<td>0.636</td>
<td>0.640</td>
<td>0.638</td>
<td>0.638</td>
<td>0.643</td>
<td>0.640</td>
</tr>
<tr>
<td></td>
<td>Mistral-7b</td>
<td>0.644</td>
<td>0.650</td>
<td>0.647</td>
<td>0.636</td>
<td>0.640</td>
<td>0.637</td>
<td>0.637</td>
<td>0.641</td>
<td>0.639</td>
</tr>
<tr>
<td>EDC+R</td>
<td>GPT-4</td>
<td>0.712</td>
<td>0.715</td>
<td>0.713</td>
<td>0.708</td>
<td>0.710</td>
<td>0.709</td>
<td>0.708</td>
<td>0.711</td>
<td>0.709</td>
</tr>
<tr>
<td></td>
<td>GPT-3.5</td>
<td>0.691</td>
<td>0.696</td>
<td>0.693</td>
<td>0.684</td>
<td>0.688</td>
<td>0.685</td>
<td>0.685</td>
<td>0.689</td>
<td>0.687</td>
</tr>
<tr>
<td></td>
<td>Mistral-7b</td>
<td>0.661</td>
<td>0.667</td>
<td>0.663</td>
<td>0.647</td>
<td>0.652</td>
<td>0.649</td>
<td>0.656</td>
<td>0.661</td>
<td>0.658</td>
</tr>
<tr>
<td>Baseline</td>
<td>GENIE</td>
<td>0.482</td>
<td>0.486</td>
<td>0.484</td>
<td>0.462</td>
<td>0.464</td>
<td>0.463</td>
<td>0.477</td>
<td>0.479</td>
<td>0.478</td>
</tr>
</tbody>
</table>
<p>Table 6: Results (F1 scores with all criteria) of further iterative refinement, the LLM used for OIE is GPT-3.5-turbo. EDC+2xR is EDC with 2 iterations of refinement.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>WebNLG</th>
<th></th>
<th></th>
<th>REBEL</th>
<th></th>
<th></th>
<th>Wiki-NRE</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Partial</td>
<td>Strict</td>
<td>Exact</td>
<td>Partial</td>
<td>Strict</td>
<td>Exact</td>
<td>Partial</td>
<td>Strict</td>
<td>Exact</td>
</tr>
<tr>
<td>EDC+2xR</td>
<td>0.797</td>
<td>0.761</td>
<td>0.775</td>
<td>0.564</td>
<td>0.521</td>
<td>0.535</td>
<td>0.697</td>
<td>0.689</td>
<td>0.660</td>
</tr>
<tr>
<td>EDC+R</td>
<td>0.794</td>
<td>0.753</td>
<td>0.772</td>
<td>0.559</td>
<td>0.516</td>
<td>0.529</td>
<td>0.693</td>
<td>0.685</td>
<td>0.657</td>
</tr>
<tr>
<td>EDC</td>
<td>0.746</td>
<td>0.688</td>
<td>0.713</td>
<td>0.506</td>
<td>0.449</td>
<td>0.473</td>
<td>0.644</td>
<td>0.634</td>
<td>0.637</td>
</tr>
</tbody>
</table>
<p>Table 7: Results (F1 scores with all criteria) of ablating the entities and relations extracted from the last round from the refinement hint, the LLM used for OIE is GPT-3.5-turbo. EDC+R-lastround is EDC with refinement but entities and relations extracted from the last round are removed from the refinement hint.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>WebNLG</th>
<th></th>
<th></th>
<th>REBEL</th>
<th></th>
<th></th>
<th>Wiki-NRE</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Partial</td>
<td>Strict</td>
<td>Exact</td>
<td>Partial</td>
<td>Strict</td>
<td>Exact</td>
<td>Partial</td>
<td>Strict</td>
<td>Exact</td>
</tr>
<tr>
<td>EDC+R</td>
<td>0.794</td>
<td>0.753</td>
<td>0.772</td>
<td>0.559</td>
<td>0.516</td>
<td>0.529</td>
<td>0.693</td>
<td>0.685</td>
<td>0.657</td>
</tr>
<tr>
<td>EDC+R-lastround</td>
<td>0.748</td>
<td>0.698</td>
<td>0.720</td>
<td>0.534</td>
<td>0.485</td>
<td>0.505</td>
<td>0.634</td>
<td>0.622</td>
<td>0.625</td>
</tr>
<tr>
<td>EDC</td>
<td>0.746</td>
<td>0.688</td>
<td>0.713</td>
<td>0.506</td>
<td>0.449</td>
<td>0.473</td>
<td>0.644</td>
<td>0.634</td>
<td>0.637</td>
</tr>
</tbody>
</table>
<p>Table 8: The average number of triplets extracted per sentence on all three datasets. The baseline model for WebNLG is REGEN while the baseline for Rebel and Wiki-NRE is GENIE. Numbers in the brackets are the difference from the reference annotations.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">LLM for OIE</th>
<th style="text-align: center;">WebNLG</th>
<th style="text-align: center;">REBEL</th>
<th style="text-align: center;">Wiki-NRE</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">$3.47(+0.04)$</td>
<td style="text-align: center;">$5.11(+1.11)$</td>
<td style="text-align: center;">$3.49(+0.63)$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5</td>
<td style="text-align: center;">$3.44(+0.01)$</td>
<td style="text-align: center;">$5.01(+1.01)$</td>
<td style="text-align: center;">$3.49(+0.63)$</td>
</tr>
<tr>
<td style="text-align: left;">Mistral7b</td>
<td style="text-align: center;">$3.45+(0.02)$</td>
<td style="text-align: center;">$4.68(+0.68)$</td>
<td style="text-align: center;">$3.75(+0.89)$</td>
</tr>
<tr>
<td style="text-align: left;">Baseline</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$2.20(-1.80)$</td>
<td style="text-align: center;">$3.08(+0.22)$</td>
</tr>
<tr>
<td style="text-align: left;">Reference</td>
<td style="text-align: center;">3.43</td>
<td style="text-align: center;">4.00</td>
<td style="text-align: center;">2.86</td>
</tr>
</tbody>
</table>
<p>prove upon this in all aspects as well. These results more comprehensively demonstrate the performance of EDC and EDC +R .</p>
<h3>C. 2 Effect of More Refinement Iterations</h3>
<p>Table 6 shows the result of an extra iteration of refinement with EDC on all datasets. Although further refinement improves the results in a stable manner, we observe diminishing returns and hence, only report one iteration in the main results.</p>
<h3>C. 3 Ablation Study on Last-Round Extractions</h3>
<p>Table 7 shows the result of ablating the relations and entities from the last round's extractions from the refinement hint. It shows the importance of performing the refinement in an iterative manner. Merging the two sources led to better coverage of the entities and relations in the text, resulting in better KGC.</p>
<h3>C. 4 Discussion on KGC Dataset Annotations</h3>
<p>As stated in Section 4.2, we observe that EDC is penalized by the scorer on Rebel and WikiNRE datasets due to incomplete annotations. This echoes the previous finding in (Wadhwa et al., 2023; Han et al., 2023) that LLMs can often extract correct results that are missing in the annotations, which results in overly pessimistic evaluations. As shown by Table 8, EDC tends to extract significantly more triplets compared to the reference annotations and the baseline GenIE. Furthermore, as shown from the manual evaluation in Table 2, many of these triplets are indeed meaningful and correct with respect to the input text. Regardless, despite the automatic evaluation result on EDC being overly pessimistic, it still exceeds the baseline by a large margin and the actual performance may be even larger considering the difference in the number of triplets extracted.</p>
<h2>D Experiments on a Novel Dataset</h2>
<p>Since the tested datasets were created several years ago and the training set of the LLMs used are unknown, there is a risk the LLMs have already been trained on these datasets. To address this concern, we create a novel small-scale dataset ( 50 entries) of fictional entities and information, e.g. "Evergreen University was where Emily Johnson received her degree in Biology" and annotated them using the schema of Wiki-NRE. Table 9 shows that EDC and EDC +R still obtain performance superior to the baseline GenIE model.</p>
<h2>E Comparison against previous LLM-based approaches</h2>
<p>Although this is not the intended use scenario for EDC, we include these experimental results for a more comprehensive evaluation to compare against existing LLM-based methods. We conduct experiments on three datasets, CoNLL04 (4 relation types) (Roth and Yih, 2004), SciERC (7 relation types) (Luan et al., 2018) and our sub-sampled version of Wiki-NRE ( 45 relation types), which is the only dataset we use in our main experiments that can fit in the context window. To ensure comparison fairness, we use GPT-3.5-turbo for all the compared methods.</p>
<p>As shown in Table 10, when the relation number is small (CONLL and SciERC), EDC alone may not be superior to the baseline methods due to excluding the schema in the prompt. However, through refinement, EDC +R is able to achieve significantly better results. This may be attributed to the usage of the semantically rich relation descriptions in the refinement step. Specifically, it helps correct two types of errors that may occur during extraction: 1. the Definition step helps disambiguate homonyms, e.g., the word "follows" has two different meanings for "John follows Taoism" v.s. "John follows Mary". EDC changes the "follows" in "John follows Taoism" to "adheres to". 2.</p>
<p>Table 9: Complete results of EDC and EDC+R on the novel fictional dataset against the baseline GenIE (Precision, Recall, F1 with 'Partial', 'Strict' and 'Exact' criteria). EDC+R only performs 1 iteration of refinement. The best results are bolded. The LLM used for OIE is GPT-3.5-turbo.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Partial</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Strict</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Exact</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Method</td>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">F1</td>
</tr>
<tr>
<td style="text-align: left;">EDC</td>
<td style="text-align: center;">0.731</td>
<td style="text-align: center;">0.771</td>
<td style="text-align: center;">0.751</td>
<td style="text-align: center;">0.687</td>
<td style="text-align: center;">0.704</td>
<td style="text-align: center;">0.691</td>
<td style="text-align: center;">0.702</td>
<td style="text-align: center;">0.720</td>
<td style="text-align: center;">0.707</td>
</tr>
<tr>
<td style="text-align: left;">EDC+R</td>
<td style="text-align: center;">0.761</td>
<td style="text-align: center;">0.782</td>
<td style="text-align: center;">0.767</td>
<td style="text-align: center;">0.733</td>
<td style="text-align: center;">0.750</td>
<td style="text-align: center;">0.738</td>
<td style="text-align: center;">0.733</td>
<td style="text-align: center;">0.750</td>
<td style="text-align: center;">0.738</td>
</tr>
<tr>
<td style="text-align: left;">GenIE</td>
<td style="text-align: center;">0.521</td>
<td style="text-align: center;">0.547</td>
<td style="text-align: center;">0.530</td>
<td style="text-align: center;">0.426</td>
<td style="text-align: center;">0.443</td>
<td style="text-align: center;">0.432</td>
<td style="text-align: center;">0.467</td>
<td style="text-align: center;">0.483</td>
<td style="text-align: center;">0.472</td>
</tr>
</tbody>
</table>
<p>Table 10: Complete results of EDC, EDC+R on CONLL, SciERC and Wiki-NRE datasets against the previous LLM-based approaches, CodeKGC and ChatIE. The LLMs used here are GPT-3.5-turbo to ensure comparison fairness. The best results are bolded.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Precision</th>
<th style="text-align: center;">Partial Recall</th>
<th style="text-align: center;">F1</th>
<th style="text-align: center;">Precision</th>
<th style="text-align: center;">Strict <br> Recall</th>
<th style="text-align: center;">F1</th>
<th style="text-align: center;">Precision</th>
<th style="text-align: center;">Exact <br> Recall</th>
<th style="text-align: center;">F1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EDC</td>
<td style="text-align: center;">0.536</td>
<td style="text-align: center;">0.552</td>
<td style="text-align: center;">0.543</td>
<td style="text-align: center;">0.481</td>
<td style="text-align: center;">0.491</td>
<td style="text-align: center;">0.485</td>
<td style="text-align: center;">0.503</td>
<td style="text-align: center;">0.515</td>
<td style="text-align: center;">0.509</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EDC+R</td>
<td style="text-align: center;">0.580</td>
<td style="text-align: center;">0.593</td>
<td style="text-align: center;">0.585</td>
<td style="text-align: center;">0.514</td>
<td style="text-align: center;">0.522</td>
<td style="text-align: center;">0.517</td>
<td style="text-align: center;">0.549</td>
<td style="text-align: center;">0.558</td>
<td style="text-align: center;">0.552</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CodeKGC</td>
<td style="text-align: center;">0.542</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.545</td>
<td style="text-align: center;">0.503</td>
<td style="text-align: center;">0.506</td>
<td style="text-align: center;">0.504</td>
<td style="text-align: center;">0.542</td>
<td style="text-align: center;">0.546</td>
<td style="text-align: center;">0.543</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ChatIE</td>
<td style="text-align: center;">0.463</td>
<td style="text-align: center;">0.477</td>
<td style="text-align: center;">0.468</td>
<td style="text-align: center;">0.360</td>
<td style="text-align: center;">0.366</td>
<td style="text-align: center;">0.363</td>
<td style="text-align: center;">0.418</td>
<td style="text-align: center;">0.427</td>
<td style="text-align: center;">0.421</td>
</tr>
<tr>
<td style="text-align: center;">SciERC</td>
<td style="text-align: center;">EDC</td>
<td style="text-align: center;">0.389</td>
<td style="text-align: center;">0.408</td>
<td style="text-align: center;">0.395</td>
<td style="text-align: center;">0.288</td>
<td style="text-align: center;">0.301</td>
<td style="text-align: center;">0.292</td>
<td style="text-align: center;">0.352</td>
<td style="text-align: center;">0.365</td>
<td style="text-align: center;">0.357</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EDC+R</td>
<td style="text-align: center;">0.447</td>
<td style="text-align: center;">0.461</td>
<td style="text-align: center;">0.451</td>
<td style="text-align: center;">0.340</td>
<td style="text-align: center;">0.349</td>
<td style="text-align: center;">0.343</td>
<td style="text-align: center;">0.406</td>
<td style="text-align: center;">0.416</td>
<td style="text-align: center;">0.410</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CodeKGC</td>
<td style="text-align: center;">0.389</td>
<td style="text-align: center;">0.398</td>
<td style="text-align: center;">0.392</td>
<td style="text-align: center;">0.277</td>
<td style="text-align: center;">0.283</td>
<td style="text-align: center;">0.279</td>
<td style="text-align: center;">0.346</td>
<td style="text-align: center;">0.353</td>
<td style="text-align: center;">0.349</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ChatIE</td>
<td style="text-align: center;">0.351</td>
<td style="text-align: center;">0.367</td>
<td style="text-align: center;">0.357</td>
<td style="text-align: center;">0.212</td>
<td style="text-align: center;">0.221</td>
<td style="text-align: center;">0.215</td>
<td style="text-align: center;">0.294</td>
<td style="text-align: center;">0.302</td>
<td style="text-align: center;">0.297</td>
</tr>
<tr>
<td style="text-align: center;">Wiki-NRE</td>
<td style="text-align: center;">EDC</td>
<td style="text-align: center;">0.645</td>
<td style="text-align: center;">0.651</td>
<td style="text-align: center;">0.647</td>
<td style="text-align: center;">0.636</td>
<td style="text-align: center;">0.640</td>
<td style="text-align: center;">0.638</td>
<td style="text-align: center;">0.638</td>
<td style="text-align: center;">0.643</td>
<td style="text-align: center;">0.640</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EDC+R</td>
<td style="text-align: center;">0.691</td>
<td style="text-align: center;">0.696</td>
<td style="text-align: center;">0.693</td>
<td style="text-align: center;">0.684</td>
<td style="text-align: center;">0.688</td>
<td style="text-align: center;">0.685</td>
<td style="text-align: center;">0.685</td>
<td style="text-align: center;">0.689</td>
<td style="text-align: center;">0.687</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CodeKGC</td>
<td style="text-align: center;">0.611</td>
<td style="text-align: center;">0.614</td>
<td style="text-align: center;">0.612</td>
<td style="text-align: center;">0.605</td>
<td style="text-align: center;">0.607</td>
<td style="text-align: center;">0.606</td>
<td style="text-align: center;">0.607</td>
<td style="text-align: center;">0.609</td>
<td style="text-align: center;">0.608</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ChatIE</td>
<td style="text-align: center;">0.569</td>
<td style="text-align: center;">0.574</td>
<td style="text-align: center;">0.571</td>
<td style="text-align: center;">0.541</td>
<td style="text-align: center;">0.545</td>
<td style="text-align: center;">0.543</td>
<td style="text-align: center;">0.553</td>
<td style="text-align: center;">0.557</td>
<td style="text-align: center;">0.555</td>
</tr>
</tbody>
</table>
<p>Using the relation definitions, we find the Refinement step corrects head-tail relation errors, e.g., for a relation "father", it is unclear if the subject or the object is the father, and the definition prevents inconsistent use. This error-correcting effect was not possible in previous methods.</p>
<p>When tested on Wiki-NRE, which has a moderately-sized schema, EDC already significantly outperforms the baseline methods, potentially due to the confusion of the LLMs when dealing with long context (Liu et al., 2024). Furthermore, we observe that ChatIE and CodeKGC may still output out-of-schema relation words although the entire schema is provided in the prompt, echoing the previous findings (Wadhwa et al., 2023).</p>
<h2>F Combine EDC with other IE tools</h2>
<p>EDC can be integrated with other IE tools, including chunking, coreference, and entity deduplication. This is beneficial in scenarios such as processing long documents that exceed the context window length of LLMs. We ran experiments on Re-DOCRED (Tan et al., 2022) by combining EDC with LingMess (Otmazgin et al., 2023), a SOTA coreference resolution method and simple sentence-level chunking. We observed an increase of strict micro F1 score from 0.132 to 0.234 , while
directly prompting the LLMs only achieves 0.060 .
We also explored the effect of entity deduplication in combination with EDC. We used CESI (Vashishth et al., 2018), a SOTA post-hoc canonicalization method to deduplicate the entities in the resulting KGs from EDC. And we observe a slightly improved F1 score from 0.516 to 0.520 on the REBEL dataset under the 'Partial' criteria.</p>
<h2>G Combining OIE and Schema Definition</h2>
<p>As an attempt to reduce the cost of EDC, we explored combining the OIE and Schema Definition steps. We previously separated these two steps because our preliminary experiments showed OIE to be more challenging and separating the two subtasks allowed us to use a more expensive model for OIE and a smaller, cheaper model for schema definition. However, separate LLM calls increases latency of the pipeline (and cost if the same LLM is used). Also, making the LLMs output the definitions along with the extracted triples might improve consistency. In a further experiment combining EDC and Schema Definition on REBEL using GPT-3.5-turbo, we observed slight performance gains ( 0.516 to 0.518 under the 'Partial' criteria) and token cost reduction ( $\approx 3 \mathrm{k}$ to 2 k tokens per example).</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-531 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-531</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-531</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-93cc9a94e42d7272e3ab55d4d6194e66cd28ae5e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/93cc9a94e42d7272e3ab55d4d6194e66cd28ae5e" target="_blank">Towards an Indexical Model of Situated Language Comprehension for Cognitive Agents in Physical Worlds</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A computational model of situated language comprehension based on the Indexical Hypothesis that generates meaning representations by translating amodal linguistic symbols to modal representations of beliefs, knowledge, and experience external to the linguistic system is proposed.</p>
                <p><strong>Paper Abstract:</strong> We propose a computational model of situated language comprehension based on the Indexical Hypothesis that generates meaning representations by translating amodal linguistic symbols to modal representations of beliefs, knowledge, and experience external to the linguistic system. This Indexical Model incorporates multiple information sources, including perceptions, domain knowledge, and short-term and long-term experiences during comprehension. We show that exploiting diverse information sources can alleviate ambiguities that arise from contextual use of underspecific referring expressions and unexpressed argument alternations of verbs. The model is being used to support linguistic interactions in Rosie, an agent implemented in Soar that learns from instruction.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e531.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e531.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Indexical Model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Indexical Model of Situated Language Comprehension</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A computational implementation of the Indexical Hypothesis that grounds amodal linguistic symbols in modality-specific, task-relevant representations (perceptual symbols, spatial predicates, and procedural task networks) to produce executable action instantiations for an embodied agent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Indexical Model (Rosie)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Rule- and memory-based comprehension component implemented in the Soar cognitive architecture that (1) maps lexical items to 'indexical maps' in semantic memory, (2) retrieves perceptual classes (kNN-based perceptual symbols), spatial predicate compositions, and task-concept & operator networks, and (3) composes and filters candidate action groundings under physical and task constraints (meshing).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Tabletop object-manipulation and instruction-following</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>An embodied robotic-arm domain (Rosie) operating over small foam blocks on a tabletop with named locations (pantry, garbage, table, stove). The agent perceives object bounding volumes, positions, color/shape/size features via Kinect, and executes primitive manipulation commands (pointTo, pickUp, putDown) and location operations (open/close, turnOn/turnOff). Natural-language instructions from a human teacher are parsed and grounded to task operators.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object manipulation; instruction following; household tasks</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + procedural + object-relational (spatial relations, object perceptual classes, affordances, hierarchical task policies)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>explicit experiential knowledge base formed by interactive instruction (semantic memory indexical maps, kNN perceptual classifiers), online perceptions from Kinect, and procedural knowledge encoded as Soar operators learned via instruction</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>interactive grounding through semantic-memory lookup and procedural operator instantiation driven by parsed utterances; learning via situated teacher demonstrations and question-answer dialogs</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>hybrid: kNN-based perceptual symbols for color/shape/size, symbolic spatial predicates composed from primitive spatial literals (distance/alignment), explicit indexical map nodes in semantic memory linking lexical strings to perceptual/spatial/procedural nodes, and hierarchical procedural operators (Soar rules/policies) for tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>human-agent interaction cost measured by number of clarification queries (object identification queries) for referring-expression resolution; number of interactions required to comprehend/execute verbs with alternations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Ref. resolution: models that include perceptual+task+attention+dialog contexts (p+t+a+d) ask significantly fewer object-identification queries than perceptual-only baseline (p); CoreNLP coreference baseline failed on 10 references (28.6%). Unexpressed-argument alternation: model that exploits instructional experience (model+e) required ~1 interaction per task for alternation cases vs ~3 interactions per task for model-e (no experience augmentation).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Successfully grounds descriptive nouns/adjectives to perceptual symbol classes (kNN labels), composes spatial prepositions as symbolic spatial predicate compositions, uses task constraints (task-operator applicability) to prune referent sets, and exploits attentional/dialog context to resolve underspecified pronouns and demonstratives; uses learned default argument values from prior instruction to fill unexpressed verb arguments.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>When required perceptual/past-experience knowledge is absent the model must ask for instruction; purely linguistic, corpus-style coreference resolvers (e.g., CoreNLP) fail on many grounded REs (28.6% failure reported). The model does not address quantification or certain category-generalization issues and currently lacks non-situated (purely retrospective/prospective) perceptual simulation capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Baselines: p (perceptual only) — most queries; p+t (perception + task knowledge) — fewer queries; p+t+a / p+t+a+d (add attention and dialog) — fewest queries and robustness across perceptual ambiguity. CoreNLP (text-only coreference) failed 28.6% of references in the corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Ablations correspond to removing context sources: removing task knowledge (p vs p+t) increases clarification queries; removing attention/dialog (p+t vs p+t+a / p+t+a+d) increases queries especially as perceptual ambiguity rises. For verb alternations, removing instructional experience (model-e) increases required interactions from ~1 to ~3 per underspecified instruction.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Language is used as an index to search multi-modal memories: grounding words to perceptual classes, spatial prepositions to composed symbolic predicates, and verbs to task-concept networks enables executable action instantiations; non-linguistic contexts (perceptual state, task affordances, attentional/dialog history, and instructional experience) materially reduce ambiguity and the need for clarifying interaction; learned experiential defaults allow the system to complete underspecified instructions without sensory re-specification.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards an Indexical Model of Situated Language Comprehension for Cognitive Agents in Physical Worlds', 'publication_date_yy_mm': '2016-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e531.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e531.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rosie</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rosie: RObotic Soar Instructable Entity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An embodied instructable agent implemented in Soar that integrates Kinect-based perception, long-term perceptual and semantic memory, procedural operators, and the Indexical Model for grounding language to action and learning from instruction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Rosie (agent)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A robotics agent with a Kinect perception pipeline feeding working memory, long-term perceptual memory (kNN classifiers), semantic memory (indexical maps, task-concept networks), and procedural memory (Soar operators); executes primitive discrete actions converted to closed-loop continuous controllers by the robot controller.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Interactive instruction-driven learning and manipulation on a tabletop</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Perceive and manipulate foam blocks; learn perceptual categories (color/shape/size), spatial prepositions, and verbs through interactive instruction; execute learned tasks (move, pick, put, cook, store) that change object states or spatial relationships in the workspace.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>instruction following; object manipulation; interactive learning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + procedural + object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>direct sensory input (Kinect) for perceptions; interactive, example-driven learning from a human instructor for building perceptual classes, spatial predicates, and task procedures; semantic & procedural memories updated online</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>interactive dialogs with instructor, pointing (clicks), example demonstrations, and question-answer cycles; semantic memory retrieval driven by parsed linguistic constituents</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>object-oriented working memory with perceptual feature assignments; long-term perceptual memory with kNN-based perceptual symbols; semantic memory of indexical maps linking words to perceptual/spatial/procedural nodes; Soar procedural operators encoding policies and availability/termination conditions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>number of clarification interactions required; ability to learn and subsequently execute verb alternations; correctness of referent resolution (eventual resolution measured by success in execution)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>All models eventually resolved REs and executed tasks in experiments; models using attention/dialog contexts required fewer clarification queries; instructional experience allowed immediate execution on underspecified verb alternations (1 interaction) vs no experience (3 interactions).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Learns perceptual classes and spatial prepositions from examples; composes spatial predicates to represent relations like 'right of' and 'on'; uses procedural memory to sequence pick/put primitives into tasks; uses pointing and dialog to disambiguate when perception is insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Fails when semantic/perceptual training is missing; needs teacher interaction to acquire missing perceptual symbols or task definitions; limited capability for quantification, category-general rules, and non-situated (purely imagined) scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared comprehension variants internally (p, p+t, p+t+a, p+t+a+d) rather than external LMs; interactive models outperform perception-only baseline in terms of interaction cost.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Lesioned versions without attentional or dialog context increased interactions needed for RE resolution; lesioned without instructional experience (model-e) required more human clarification when verbs omitted arguments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>An embodied agent that combines perceptual classifiers, symbolic spatial predicates, and procedural task networks can ground natural language effectively; attention and dialog context are critical to minimize clarification, and experience (learned defaults) enables execution of underspecified commands.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards an Indexical Model of Situated Language Comprehension for Cognitive Agents in Physical Worlds', 'publication_date_yy_mm': '2016-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e531.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e531.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Indexical maps</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Indexical maps in semantic memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Semantic-memory structures that explicitly map lexical strings (nouns/adjectives, prepositions, verbs) to modality-specific modal symbols: perceptual symbols (kNN labels), spatial predicate compositions, and task-concept & operator nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Indexical maps</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Nodes in semantic memory that associate lexical forms with learned perceptual classes (e.g., color symbol C22), spatial-symbol nodes (e.g., P4 for 'right'), and task networks (e.g., M2 linking 'move' to goal G2 and operator P2); retrieved during indexing stage to produce referent sets.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Lexical grounding for situated instruction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used during parsing of instructions to find the perceptual/spatial/procedural referents tied to words and phrases so that referent sets can be assembled and instantiated into policies/goals.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>instruction following; lexical grounding</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational + spatial + procedural (linking lexical items to modal concepts)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>explicit semantic memory entries learned via interactive instruction and stored as symbolic nodes</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>semantic-memory lookup during indexing of parsed lexical items; updated via learning episodes with instructor examples</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>symbolic mapping nodes linking lexical strings to perceptual symbols, spatial predicate nodes, or task-concept networks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>successful retrieval rate of correct referent nodes and downstream effect on referent set size and number of clarification queries</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>When indexical maps exist, direct indexing yields correct referent sets; when absent, Rosie prompts for training. Presence of indexical maps reduces clarification interactions by enabling direct grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Enables direct grounding of adjectives/nouns to perceptual classes and prepositions to spatial predicate nodes, facilitating quick instantiation of tasks when entries exist.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Missing or incorrect indexical maps force the agent into learning dialogs; incomplete mappings prevent grounding and require human demonstration.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Implicitly compared against no-mapping condition where system must ask for examples (learning mode); explicit mappings greatly reduce human-agent interaction overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Not isolated numerically but removing access to indexical maps causes failure to index words and immediate transition to learning interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicit lexical-to-modal mappings are a practical mechanism for connecting language to perceptual/spatial/procedural representations in embodied agents, enabling efficient grounding when present and clear learning-pathways when absent.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards an Indexical Model of Situated Language Comprehension for Cognitive Agents in Physical Worlds', 'publication_date_yy_mm': '2016-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e531.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e531.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Spatial-visual system</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spatial-visual system (spatial predicate composition)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A component that encodes primitive spatial literals (distance, axis alignment) and composes them to form symbolic spatial prepositions (e.g., 'on', 'right of') used for representing inter-object relations for reasoning and action.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Spatial-visual system</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encodes spatial prepositions as compositions of primitive spatial predicates; generates symbolic representations of spatial relationships between perceived objects (e.g., predicates like P3(012,032) encoding 'right-of'), used both for reference filtering and goal specification.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Spatial relation representation for manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Compute and represent relations such as 'on', 'near', 'right of' between perceived objects, and support planning operators that require establishing those relations (e.g., put-down to achieve P3(obj, location)).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object manipulation; spatial reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>derived from perceptual geometry (positions/bounding volumes) and learned prepositional compositions from instruction</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>computed from object positions and perceptual features; spatial preposition nodes retrieved via semantic memory lookup when parsing prepositional phrases</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>symbolic spatial predicates composed from primitive numeric spatial literals (distance thresholds, axis alignment), represented as nodes (e.g., P4) in semantic memory</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>ability to filter referent sets by spatial constraints and to instantiate goals requiring particular spatial relations; measured indirectly by reductions in clarification queries and successful task execution</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Using spatial filters reduced candidate referent sets and supported correct execution of spatially specified tasks; models that leveraged task-based and spatial filters asked fewer clarification queries versus perception-only.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Correctly represents and uses spatial relations to prune referent candidates and to define task goals (e.g., placing object 'right of' another), enabling meshing of linguistic constraints with environment geometry.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>If spatial predicate compositions are not learned or if perceptual data are noisy/missing, spatial filtering fails and additional interaction is required; paper does not quantify robustness to sensor noise.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared implicitly to absence of spatial filtering where referent sets remain larger and require more human clarification.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Removing spatial filters increases ambiguity and clarification queries (shown in comparisons p vs p+t variants).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Encoding prepositions as composed symbolic spatial predicates grounded in perceptual geometry provides an interpretable bridge between language and actionable spatial goals and is effective in disambiguating referring expressions and defining task goals.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards an Indexical Model of Situated Language Comprehension for Cognitive Agents in Physical Worlds', 'publication_date_yy_mm': '2016-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e531.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e531.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Task-concept network</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Task-concept network and procedural operator nodes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Declarative goal definitions (task-concept networks) linked to procedural operator nodes in semantic & procedural memory that together specify goal conditions and the hierarchical policies used to achieve task goals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Task-concept network (Soar operators + goal nodes)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Task representation that pairs goal-definition nodes (e.g., G2,G3) with procedural operator nodes (P2,P3) and a task-mapping node (M2) retrieved when indexing a verb; instantiation of these networks under syntactic constraints yields concrete policy operators (e.g., op_1) to execute.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Learning and executing hierarchical tasks (move/pick/put/store/cook)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Represent and execute multi-step tasks requiring sequencing of primitive actions and achieving particular state and spatial predicates; tasks may have alternations that omit arguments, requiring default instantiation from experience.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>procedural; multi-step planning; object manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural + spatial + object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>learned via interactive instruction and stored as symbolic task-concept graphs and Soar procedural rules</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>retrieved by indexing verbs in sentences; instantiated into operators according to syntactic slots filled by referent sets and meshing with environment availability conditions</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>symbolic network of goal nodes, procedural operator nodes, and slot nodes that can be parameterized with object and spatial referents; policies implemented as Soar operators that propose/select/apply actions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>number of interactions needed to instantiate and execute tasks; ability to execute tasks with underspecified arguments using learned defaults</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>When task networks include learned defaults for unexpressed arguments, the system executes underspecified instructions with no extra clarification (~1 interaction); without defaults, additional questions increase interactions (~3 interactions reported).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Successfully uses task-operator availability conditions and affordances to filter referent sets, instantiate appropriate primitive actions (pick-up, put-down), and execute hierarchical policies to achieve defined spatial/state goals.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Ambiguities in verb alternations without prior experience force the system to ask goal-specification questions; system currently cannot generalize some category-dependent defaults without instructor examples.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Model+e (uses instructional experience/defaults) vs model-e (no experience): model+e significantly reduces required clarification interactions for alternation cases.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Lesioning instructional-experience augmentation (model-e) increases clarification queries/interactions for underspecified verb instances.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Representing verbs as links to task-concept networks plus procedural operators allows the agent to both (a) use task affordances to constrain referent resolution and (b) learn and apply default argument values from experience to execute underspecified commands.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards an Indexical Model of Situated Language Comprehension for Cognitive Agents in Physical Worlds', 'publication_date_yy_mm': '2016-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e531.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e531.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reference-resolution pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Indexical Model reference-resolution pipeline (filters + attention + dialog stack)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A staged reference-resolution process that combines candidate-set heuristics based on RE form, visual filtering via perceptual symbols, spatial filtering via spatial predicates, task-based filtering using operator applicability, and attentional/dialog context ordering to select referents for underspecified expressions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Reference-resolution pipeline (Indexical Model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Algorithmic pipeline: (0) track cognitive status sets (in-focus, activated, perceptible); (1) determine resolution type from surface form; (2) select candidate set (stack/attend/percept); (3) apply visual filter using indexical perceptual symbols; (4) apply spatial filter via spatial predicate satisfaction; (5) apply task filter via currently applicable operators; (6) partially order candidates by cognitive status; (7) resolve or query for clarification.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Resolving referring expressions in situated dialogues</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Resolve REs from definite/indefinite noun phrases, demonstratives, and pronouns by integrating perceptual cues, spatial relations, task constraints, and dialog/attention history to identify the intended referent with minimal clarification.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>reference resolution; dialogue-grounded comprehension</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational + spatial + contextual (attentional/dialog history)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>perceptions (Kinect), attentional recency in semantic memory, interaction stack (dialog focus), task operator applicability from procedural memory</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>incremental application of filters and ordering heuristics; if unresolved, initiate subdialog to elicit more information</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>candidate referent sets in working memory, semantic memory activations for attentional history, symbolic spatial predicates and task operator instantiations used as filters</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>number of object-identification clarification queries required during interactive comprehension</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Models using perceptual+task+attention+dialog (p+t+a+d) produced the fewest clarification queries and were robust across increasing perceptual ambiguity; perceptual-only (p) required the most queries. CoreNLP (text-only) failed 28.6% of coreferences.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Efficiently resolves pronouns and demonstratives by exploiting dialog/attention history; uses task and spatial filters to prune candidates in action contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>When candidate filtering cannot disambiguate, the system must ask clarification questions; performance depends on richness of stored perceptual and task memories.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>p (perceptual only) vs p+t vs p+t+a vs p+t+a+d: incremental addition of task, attention, and dialog contexts monotonically reduces clarification interactions and improves robustness to distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Removing attention/dialog contexts (comparing p+t to p+t+a/d) increases clarification queries, especially under high perceptual ambiguity.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Integrating attentional/dialog history with perceptual, spatial, and task-based filters allows an embodied agent to resolve ambiguous referring expressions with fewer interactions; linguistic surface form signals which contextual sets (focus/attend/percept) should be considered.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards an Indexical Model of Situated Language Comprehension for Cognitive Agents in Physical Worlds', 'publication_date_yy_mm': '2016-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Indexical understanding of instructions <em>(Rating: 2)</em></li>
                <li>Acquiring grounded representation of words with situtated interactive instruction <em>(Rating: 2)</em></li>
                <li>Understanding natural language commands for robotic navigation and mobile manipulation <em>(Rating: 2)</em></li>
                <li>Toward understanding natural language directions <em>(Rating: 2)</em></li>
                <li>A real-time robotic model of human reference resolution using visual constraints <em>(Rating: 2)</em></li>
                <li>Learning to interpret natural language navigation instructions from observations <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-531",
    "paper_id": "paper-93cc9a94e42d7272e3ab55d4d6194e66cd28ae5e",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "Indexical Model",
            "name_full": "Indexical Model of Situated Language Comprehension",
            "brief_description": "A computational implementation of the Indexical Hypothesis that grounds amodal linguistic symbols in modality-specific, task-relevant representations (perceptual symbols, spatial predicates, and procedural task networks) to produce executable action instantiations for an embodied agent.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Indexical Model (Rosie)",
            "model_size": null,
            "model_description": "Rule- and memory-based comprehension component implemented in the Soar cognitive architecture that (1) maps lexical items to 'indexical maps' in semantic memory, (2) retrieves perceptual classes (kNN-based perceptual symbols), spatial predicate compositions, and task-concept & operator networks, and (3) composes and filters candidate action groundings under physical and task constraints (meshing).",
            "task_name": "Tabletop object-manipulation and instruction-following",
            "task_description": "An embodied robotic-arm domain (Rosie) operating over small foam blocks on a tabletop with named locations (pantry, garbage, table, stove). The agent perceives object bounding volumes, positions, color/shape/size features via Kinect, and executes primitive manipulation commands (pointTo, pickUp, putDown) and location operations (open/close, turnOn/turnOff). Natural-language instructions from a human teacher are parsed and grounded to task operators.",
            "task_type": "object manipulation; instruction following; household tasks",
            "knowledge_type": "spatial + procedural + object-relational (spatial relations, object perceptual classes, affordances, hierarchical task policies)",
            "knowledge_source": "explicit experiential knowledge base formed by interactive instruction (semantic memory indexical maps, kNN perceptual classifiers), online perceptions from Kinect, and procedural knowledge encoded as Soar operators learned via instruction",
            "has_direct_sensory_input": true,
            "elicitation_method": "interactive grounding through semantic-memory lookup and procedural operator instantiation driven by parsed utterances; learning via situated teacher demonstrations and question-answer dialogs",
            "knowledge_representation": "hybrid: kNN-based perceptual symbols for color/shape/size, symbolic spatial predicates composed from primitive spatial literals (distance/alignment), explicit indexical map nodes in semantic memory linking lexical strings to perceptual/spatial/procedural nodes, and hierarchical procedural operators (Soar rules/policies) for tasks",
            "performance_metric": "human-agent interaction cost measured by number of clarification queries (object identification queries) for referring-expression resolution; number of interactions required to comprehend/execute verbs with alternations",
            "performance_result": "Ref. resolution: models that include perceptual+task+attention+dialog contexts (p+t+a+d) ask significantly fewer object-identification queries than perceptual-only baseline (p); CoreNLP coreference baseline failed on 10 references (28.6%). Unexpressed-argument alternation: model that exploits instructional experience (model+e) required ~1 interaction per task for alternation cases vs ~3 interactions per task for model-e (no experience augmentation).",
            "success_patterns": "Successfully grounds descriptive nouns/adjectives to perceptual symbol classes (kNN labels), composes spatial prepositions as symbolic spatial predicate compositions, uses task constraints (task-operator applicability) to prune referent sets, and exploits attentional/dialog context to resolve underspecified pronouns and demonstratives; uses learned default argument values from prior instruction to fill unexpressed verb arguments.",
            "failure_patterns": "When required perceptual/past-experience knowledge is absent the model must ask for instruction; purely linguistic, corpus-style coreference resolvers (e.g., CoreNLP) fail on many grounded REs (28.6% failure reported). The model does not address quantification or certain category-generalization issues and currently lacks non-situated (purely retrospective/prospective) perceptual simulation capabilities.",
            "baseline_comparison": "Baselines: p (perceptual only) — most queries; p+t (perception + task knowledge) — fewer queries; p+t+a / p+t+a+d (add attention and dialog) — fewest queries and robustness across perceptual ambiguity. CoreNLP (text-only coreference) failed 28.6% of references in the corpus.",
            "ablation_results": "Ablations correspond to removing context sources: removing task knowledge (p vs p+t) increases clarification queries; removing attention/dialog (p+t vs p+t+a / p+t+a+d) increases queries especially as perceptual ambiguity rises. For verb alternations, removing instructional experience (model-e) increases required interactions from ~1 to ~3 per underspecified instruction.",
            "key_findings": "Language is used as an index to search multi-modal memories: grounding words to perceptual classes, spatial prepositions to composed symbolic predicates, and verbs to task-concept networks enables executable action instantiations; non-linguistic contexts (perceptual state, task affordances, attentional/dialog history, and instructional experience) materially reduce ambiguity and the need for clarifying interaction; learned experiential defaults allow the system to complete underspecified instructions without sensory re-specification.",
            "uuid": "e531.0",
            "source_info": {
                "paper_title": "Towards an Indexical Model of Situated Language Comprehension for Cognitive Agents in Physical Worlds",
                "publication_date_yy_mm": "2016-04"
            }
        },
        {
            "name_short": "Rosie",
            "name_full": "Rosie: RObotic Soar Instructable Entity",
            "brief_description": "An embodied instructable agent implemented in Soar that integrates Kinect-based perception, long-term perceptual and semantic memory, procedural operators, and the Indexical Model for grounding language to action and learning from instruction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Rosie (agent)",
            "model_size": null,
            "model_description": "A robotics agent with a Kinect perception pipeline feeding working memory, long-term perceptual memory (kNN classifiers), semantic memory (indexical maps, task-concept networks), and procedural memory (Soar operators); executes primitive discrete actions converted to closed-loop continuous controllers by the robot controller.",
            "task_name": "Interactive instruction-driven learning and manipulation on a tabletop",
            "task_description": "Perceive and manipulate foam blocks; learn perceptual categories (color/shape/size), spatial prepositions, and verbs through interactive instruction; execute learned tasks (move, pick, put, cook, store) that change object states or spatial relationships in the workspace.",
            "task_type": "instruction following; object manipulation; interactive learning",
            "knowledge_type": "spatial + procedural + object-relational",
            "knowledge_source": "direct sensory input (Kinect) for perceptions; interactive, example-driven learning from a human instructor for building perceptual classes, spatial predicates, and task procedures; semantic & procedural memories updated online",
            "has_direct_sensory_input": true,
            "elicitation_method": "interactive dialogs with instructor, pointing (clicks), example demonstrations, and question-answer cycles; semantic memory retrieval driven by parsed linguistic constituents",
            "knowledge_representation": "object-oriented working memory with perceptual feature assignments; long-term perceptual memory with kNN-based perceptual symbols; semantic memory of indexical maps linking words to perceptual/spatial/procedural nodes; Soar procedural operators encoding policies and availability/termination conditions",
            "performance_metric": "number of clarification interactions required; ability to learn and subsequently execute verb alternations; correctness of referent resolution (eventual resolution measured by success in execution)",
            "performance_result": "All models eventually resolved REs and executed tasks in experiments; models using attention/dialog contexts required fewer clarification queries; instructional experience allowed immediate execution on underspecified verb alternations (1 interaction) vs no experience (3 interactions).",
            "success_patterns": "Learns perceptual classes and spatial prepositions from examples; composes spatial predicates to represent relations like 'right of' and 'on'; uses procedural memory to sequence pick/put primitives into tasks; uses pointing and dialog to disambiguate when perception is insufficient.",
            "failure_patterns": "Fails when semantic/perceptual training is missing; needs teacher interaction to acquire missing perceptual symbols or task definitions; limited capability for quantification, category-general rules, and non-situated (purely imagined) scenarios.",
            "baseline_comparison": "Compared comprehension variants internally (p, p+t, p+t+a, p+t+a+d) rather than external LMs; interactive models outperform perception-only baseline in terms of interaction cost.",
            "ablation_results": "Lesioned versions without attentional or dialog context increased interactions needed for RE resolution; lesioned without instructional experience (model-e) required more human clarification when verbs omitted arguments.",
            "key_findings": "An embodied agent that combines perceptual classifiers, symbolic spatial predicates, and procedural task networks can ground natural language effectively; attention and dialog context are critical to minimize clarification, and experience (learned defaults) enables execution of underspecified commands.",
            "uuid": "e531.1",
            "source_info": {
                "paper_title": "Towards an Indexical Model of Situated Language Comprehension for Cognitive Agents in Physical Worlds",
                "publication_date_yy_mm": "2016-04"
            }
        },
        {
            "name_short": "Indexical maps",
            "name_full": "Indexical maps in semantic memory",
            "brief_description": "Semantic-memory structures that explicitly map lexical strings (nouns/adjectives, prepositions, verbs) to modality-specific modal symbols: perceptual symbols (kNN labels), spatial predicate compositions, and task-concept & operator nodes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Indexical maps",
            "model_size": null,
            "model_description": "Nodes in semantic memory that associate lexical forms with learned perceptual classes (e.g., color symbol C22), spatial-symbol nodes (e.g., P4 for 'right'), and task networks (e.g., M2 linking 'move' to goal G2 and operator P2); retrieved during indexing stage to produce referent sets.",
            "task_name": "Lexical grounding for situated instruction",
            "task_description": "Used during parsing of instructions to find the perceptual/spatial/procedural referents tied to words and phrases so that referent sets can be assembled and instantiated into policies/goals.",
            "task_type": "instruction following; lexical grounding",
            "knowledge_type": "object-relational + spatial + procedural (linking lexical items to modal concepts)",
            "knowledge_source": "explicit semantic memory entries learned via interactive instruction and stored as symbolic nodes",
            "has_direct_sensory_input": null,
            "elicitation_method": "semantic-memory lookup during indexing of parsed lexical items; updated via learning episodes with instructor examples",
            "knowledge_representation": "symbolic mapping nodes linking lexical strings to perceptual symbols, spatial predicate nodes, or task-concept networks",
            "performance_metric": "successful retrieval rate of correct referent nodes and downstream effect on referent set size and number of clarification queries",
            "performance_result": "When indexical maps exist, direct indexing yields correct referent sets; when absent, Rosie prompts for training. Presence of indexical maps reduces clarification interactions by enabling direct grounding.",
            "success_patterns": "Enables direct grounding of adjectives/nouns to perceptual classes and prepositions to spatial predicate nodes, facilitating quick instantiation of tasks when entries exist.",
            "failure_patterns": "Missing or incorrect indexical maps force the agent into learning dialogs; incomplete mappings prevent grounding and require human demonstration.",
            "baseline_comparison": "Implicitly compared against no-mapping condition where system must ask for examples (learning mode); explicit mappings greatly reduce human-agent interaction overhead.",
            "ablation_results": "Not isolated numerically but removing access to indexical maps causes failure to index words and immediate transition to learning interactions.",
            "key_findings": "Explicit lexical-to-modal mappings are a practical mechanism for connecting language to perceptual/spatial/procedural representations in embodied agents, enabling efficient grounding when present and clear learning-pathways when absent.",
            "uuid": "e531.2",
            "source_info": {
                "paper_title": "Towards an Indexical Model of Situated Language Comprehension for Cognitive Agents in Physical Worlds",
                "publication_date_yy_mm": "2016-04"
            }
        },
        {
            "name_short": "Spatial-visual system",
            "name_full": "Spatial-visual system (spatial predicate composition)",
            "brief_description": "A component that encodes primitive spatial literals (distance, axis alignment) and composes them to form symbolic spatial prepositions (e.g., 'on', 'right of') used for representing inter-object relations for reasoning and action.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Spatial-visual system",
            "model_size": null,
            "model_description": "Encodes spatial prepositions as compositions of primitive spatial predicates; generates symbolic representations of spatial relationships between perceived objects (e.g., predicates like P3(012,032) encoding 'right-of'), used both for reference filtering and goal specification.",
            "task_name": "Spatial relation representation for manipulation",
            "task_description": "Compute and represent relations such as 'on', 'near', 'right of' between perceived objects, and support planning operators that require establishing those relations (e.g., put-down to achieve P3(obj, location)).",
            "task_type": "object manipulation; spatial reasoning",
            "knowledge_type": "spatial + object-relational",
            "knowledge_source": "derived from perceptual geometry (positions/bounding volumes) and learned prepositional compositions from instruction",
            "has_direct_sensory_input": true,
            "elicitation_method": "computed from object positions and perceptual features; spatial preposition nodes retrieved via semantic memory lookup when parsing prepositional phrases",
            "knowledge_representation": "symbolic spatial predicates composed from primitive numeric spatial literals (distance thresholds, axis alignment), represented as nodes (e.g., P4) in semantic memory",
            "performance_metric": "ability to filter referent sets by spatial constraints and to instantiate goals requiring particular spatial relations; measured indirectly by reductions in clarification queries and successful task execution",
            "performance_result": "Using spatial filters reduced candidate referent sets and supported correct execution of spatially specified tasks; models that leveraged task-based and spatial filters asked fewer clarification queries versus perception-only.",
            "success_patterns": "Correctly represents and uses spatial relations to prune referent candidates and to define task goals (e.g., placing object 'right of' another), enabling meshing of linguistic constraints with environment geometry.",
            "failure_patterns": "If spatial predicate compositions are not learned or if perceptual data are noisy/missing, spatial filtering fails and additional interaction is required; paper does not quantify robustness to sensor noise.",
            "baseline_comparison": "Compared implicitly to absence of spatial filtering where referent sets remain larger and require more human clarification.",
            "ablation_results": "Removing spatial filters increases ambiguity and clarification queries (shown in comparisons p vs p+t variants).",
            "key_findings": "Encoding prepositions as composed symbolic spatial predicates grounded in perceptual geometry provides an interpretable bridge between language and actionable spatial goals and is effective in disambiguating referring expressions and defining task goals.",
            "uuid": "e531.3",
            "source_info": {
                "paper_title": "Towards an Indexical Model of Situated Language Comprehension for Cognitive Agents in Physical Worlds",
                "publication_date_yy_mm": "2016-04"
            }
        },
        {
            "name_short": "Task-concept network",
            "name_full": "Task-concept network and procedural operator nodes",
            "brief_description": "Declarative goal definitions (task-concept networks) linked to procedural operator nodes in semantic & procedural memory that together specify goal conditions and the hierarchical policies used to achieve task goals.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Task-concept network (Soar operators + goal nodes)",
            "model_size": null,
            "model_description": "Task representation that pairs goal-definition nodes (e.g., G2,G3) with procedural operator nodes (P2,P3) and a task-mapping node (M2) retrieved when indexing a verb; instantiation of these networks under syntactic constraints yields concrete policy operators (e.g., op_1) to execute.",
            "task_name": "Learning and executing hierarchical tasks (move/pick/put/store/cook)",
            "task_description": "Represent and execute multi-step tasks requiring sequencing of primitive actions and achieving particular state and spatial predicates; tasks may have alternations that omit arguments, requiring default instantiation from experience.",
            "task_type": "procedural; multi-step planning; object manipulation",
            "knowledge_type": "procedural + spatial + object-relational",
            "knowledge_source": "learned via interactive instruction and stored as symbolic task-concept graphs and Soar procedural rules",
            "has_direct_sensory_input": null,
            "elicitation_method": "retrieved by indexing verbs in sentences; instantiated into operators according to syntactic slots filled by referent sets and meshing with environment availability conditions",
            "knowledge_representation": "symbolic network of goal nodes, procedural operator nodes, and slot nodes that can be parameterized with object and spatial referents; policies implemented as Soar operators that propose/select/apply actions",
            "performance_metric": "number of interactions needed to instantiate and execute tasks; ability to execute tasks with underspecified arguments using learned defaults",
            "performance_result": "When task networks include learned defaults for unexpressed arguments, the system executes underspecified instructions with no extra clarification (~1 interaction); without defaults, additional questions increase interactions (~3 interactions reported).",
            "success_patterns": "Successfully uses task-operator availability conditions and affordances to filter referent sets, instantiate appropriate primitive actions (pick-up, put-down), and execute hierarchical policies to achieve defined spatial/state goals.",
            "failure_patterns": "Ambiguities in verb alternations without prior experience force the system to ask goal-specification questions; system currently cannot generalize some category-dependent defaults without instructor examples.",
            "baseline_comparison": "Model+e (uses instructional experience/defaults) vs model-e (no experience): model+e significantly reduces required clarification interactions for alternation cases.",
            "ablation_results": "Lesioning instructional-experience augmentation (model-e) increases clarification queries/interactions for underspecified verb instances.",
            "key_findings": "Representing verbs as links to task-concept networks plus procedural operators allows the agent to both (a) use task affordances to constrain referent resolution and (b) learn and apply default argument values from experience to execute underspecified commands.",
            "uuid": "e531.4",
            "source_info": {
                "paper_title": "Towards an Indexical Model of Situated Language Comprehension for Cognitive Agents in Physical Worlds",
                "publication_date_yy_mm": "2016-04"
            }
        },
        {
            "name_short": "Reference-resolution pipeline",
            "name_full": "Indexical Model reference-resolution pipeline (filters + attention + dialog stack)",
            "brief_description": "A staged reference-resolution process that combines candidate-set heuristics based on RE form, visual filtering via perceptual symbols, spatial filtering via spatial predicates, task-based filtering using operator applicability, and attentional/dialog context ordering to select referents for underspecified expressions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Reference-resolution pipeline (Indexical Model)",
            "model_size": null,
            "model_description": "Algorithmic pipeline: (0) track cognitive status sets (in-focus, activated, perceptible); (1) determine resolution type from surface form; (2) select candidate set (stack/attend/percept); (3) apply visual filter using indexical perceptual symbols; (4) apply spatial filter via spatial predicate satisfaction; (5) apply task filter via currently applicable operators; (6) partially order candidates by cognitive status; (7) resolve or query for clarification.",
            "task_name": "Resolving referring expressions in situated dialogues",
            "task_description": "Resolve REs from definite/indefinite noun phrases, demonstratives, and pronouns by integrating perceptual cues, spatial relations, task constraints, and dialog/attention history to identify the intended referent with minimal clarification.",
            "task_type": "reference resolution; dialogue-grounded comprehension",
            "knowledge_type": "object-relational + spatial + contextual (attentional/dialog history)",
            "knowledge_source": "perceptions (Kinect), attentional recency in semantic memory, interaction stack (dialog focus), task operator applicability from procedural memory",
            "has_direct_sensory_input": true,
            "elicitation_method": "incremental application of filters and ordering heuristics; if unresolved, initiate subdialog to elicit more information",
            "knowledge_representation": "candidate referent sets in working memory, semantic memory activations for attentional history, symbolic spatial predicates and task operator instantiations used as filters",
            "performance_metric": "number of object-identification clarification queries required during interactive comprehension",
            "performance_result": "Models using perceptual+task+attention+dialog (p+t+a+d) produced the fewest clarification queries and were robust across increasing perceptual ambiguity; perceptual-only (p) required the most queries. CoreNLP (text-only) failed 28.6% of coreferences.",
            "success_patterns": "Efficiently resolves pronouns and demonstratives by exploiting dialog/attention history; uses task and spatial filters to prune candidates in action contexts.",
            "failure_patterns": "When candidate filtering cannot disambiguate, the system must ask clarification questions; performance depends on richness of stored perceptual and task memories.",
            "baseline_comparison": "p (perceptual only) vs p+t vs p+t+a vs p+t+a+d: incremental addition of task, attention, and dialog contexts monotonically reduces clarification interactions and improves robustness to distractors.",
            "ablation_results": "Removing attention/dialog contexts (comparing p+t to p+t+a/d) increases clarification queries, especially under high perceptual ambiguity.",
            "key_findings": "Integrating attentional/dialog history with perceptual, spatial, and task-based filters allows an embodied agent to resolve ambiguous referring expressions with fewer interactions; linguistic surface form signals which contextual sets (focus/attend/percept) should be considered.",
            "uuid": "e531.5",
            "source_info": {
                "paper_title": "Towards an Indexical Model of Situated Language Comprehension for Cognitive Agents in Physical Worlds",
                "publication_date_yy_mm": "2016-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Indexical understanding of instructions",
            "rating": 2
        },
        {
            "paper_title": "Acquiring grounded representation of words with situtated interactive instruction",
            "rating": 2
        },
        {
            "paper_title": "Understanding natural language commands for robotic navigation and mobile manipulation",
            "rating": 2
        },
        {
            "paper_title": "Toward understanding natural language directions",
            "rating": 2
        },
        {
            "paper_title": "A real-time robotic model of human reference resolution using visual constraints",
            "rating": 2
        },
        {
            "paper_title": "Learning to interpret natural language navigation instructions from observations",
            "rating": 2
        }
    ],
    "cost": 0.01600075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Towards an Indexical Model of Situated Language Comprehension for Cognitive Agents in Physical Worlds</h1>
<p>Shiwali Mohan<br>Aaron H. Mininger<br>John E. Laird</p>
<p>SHIWALI@UMICH.EDU<br>MININGER@UMICH.EDU<br>LAIRD@UMICH.EDU</p>
<p>Division of Computer Science and Engineering, University of Michigan, Ann Arbor, MI 48109</p>
<h4>Abstract</h4>
<p>We propose a computational model of situated language comprehension based on the Indexical Hypothesis that generates meaning representations by translating amodal linguistic symbols to modal representations of beliefs, knowledge, and experience external to the linguistic system. This Indexical Model incorporates multiple information sources including perceptions, domain knowledge, and short-term and long-term experiences during comprehension. We show that exploiting diverse information sources can alleviate ambiguities that arise from contextual use of underspecific referring expressions and unexpressed argument alternations of verbs. The model is being used to support linguistic interactions in Rosie, an agent implemented in Soar that learns from instruction.</p>
<h2>1. Introduction</h2>
<p>As computational agents become pervasive in human society as collaborators, the challenge of supporting flexible human-agent interaction is becoming increasingly important. Natural language has emerged as a strong contender for the modality of human-agent interaction, as it is the primary means of communication in human societies. Recent research in the design of interactive, intelligent agents has shown that linguistic interaction is not only useful in collaboration for human-agent tasks (Fong et al., 2003; Kollar et al., 2010), but it also facilitates novel concept acquisition in interactive agents (Cantrell et al., 2011; Mohan et al., 2012; Tellex et al., 2011). This has motivated research on comprehensive models of natural language for collaborative task execution and learning in human-agent teams.</p>
<h3>1.1 Situated Communication for Joint Activity</h3>
<p>In a joint activity, the speaker and the hearer pursue diverse communicative goals in order to make progress on a task. Various types of utterances are employed for expressing the communicative goals. Imperative sentences such as Take out the trash convey that the speaker intends the hearer to complete a task. The joint communicative goal is for the hearer to identify the intended task and relevant objects and to correctly instantiate the task goals. Other utterances, such as assertions (Rice is in the pantry), may be used to establish shared beliefs about the state for joint activity.</p>
<p>Questions (Where is the milk?) may be used to supplement perceptual information by relying on the collaborative partner's knowledge.</p>
<p>Communication between collaborators who are simultaneously embedded in a shared task is situated. The speaker's linguistic utterances refer to objects, spatial configurations, and actions in the shared environment. To respond and react to utterances, the hearer must associate the amodal linguistic symbols (words) and constructions (phrases) with the modal representations of perceptions, state, domain knowledge, goals, and policies that are required for reasoning about and manipulating the environment.</p>
<p>Being situated provides a common ground of shared perceptions, goals, and domain knowledge that can be exploited during linguistic communication. Information that is apparent from the current state of the environment or that is a component of shared beliefs can be left out of the linguistic utterance by the speaker. This results in more efficient (fewer words) but ambiguous utterances. Humans frequently use referring expressions such as it or that cylinder that do not by themselves provide enough discriminative information for unambiguous resolution. The speaker assumes that the hearer can exploit extra-linguistic information, such as the context of the ongoing discourse, for unambiguous comprehension. Certain imperative sentences such as take out the trash incompletely specify the action by omitting information such as the location where the trash should be moved. Such ambiguities make situated comprehension a significant challenge for interactive agents.</p>
<h1>1.2 Contribution and Claims</h1>
<p>In this paper, we study the utility of the Indexical Hypothesis (Glenberg \&amp; Robertson, 1999) in developing comprehension models for collaborative agents. These agents are embedded in physical tasks that require the use of complex representations for probabilistic perceptual processing, continuous spatial reasoning, and goal-oriented task execution. To support situated communication, the comprehension model must not only perform syntactical analyses, but also synthesize meaning representations by associating linguistic information with representations in other cognitive modules.</p>
<p>The Indexical Hypothesis of language comprehension explains how sentences become meaningful through grounding their interpretation in situated action. The hypothesis asserts that comprehending a sentence requires three processes: indexing words and phrases to referents that establishes the contents of the linguistic input, deriving affordances from these referents, and meshing these affordances under the guidance of physical constraints along with the constraints provided by the syntax of the sentence. According to the hypothesis, the linguistic information specifies the situation by identifying which components (objects, relationships, etc.) are relevant, and the semantic and experiential knowledge associated with these components augments the linguistic input with details that are required for reasoning and taking action. In this formulation of language comprehension, linguistic symbols (words) and constructions (grammatical units) are cues to the hearer to search their perceptions, domain knowledge, and long and short-term experiences to identify the referents intended by the speaker and to compose them.</p>
<p>Earlier work on the Indexical Hypothesis of language comprehension identifies the processes that humans use for comprehension (Glenberg \&amp; Robertson, 1999) and provides supporting data from human studies (Kaschak \&amp; Glenberg, 2000). It does not describe the knowledge representations and computational processes required for implementation of such models on intelligent agents.</p>
<p>The contribution of our work is a computational model which we call the Indexical Model of comprehension that precisely defines the representations and processes described in the Indexical Hypothesis. We make two main claims:</p>
<ol>
<li>The Indexical Model for comprehension can be used effectively by agents that act and learn in physical environments. We support this claim through demonstration: we describe an implementation of the model for Rosie (RObotic Soar Instructable Entity), an agent (Mohan et al., 2012) that learns about various aspects of its environment through instruction.</li>
<li>The Indexical Model exploits diverse knowledge and experience of the domain to address ambiguity in semantic interpretation of the linguistic input. We evaluate this claim by demonstrating the utility of incorporating knowledge and experience in language comprehension on two ubiquitous problems - referring expression resolution and unexpressed argument alternation of verbs.</li>
</ol>
<p>The rest of the paper is organized as follows. Section 2 provides a description of our robotic domain and a brief overview of Rosie. Section 3 describes the indexical model. In section 4, we explain how the model addresses complexities that arise from ambiguity in its linguistic input. Section 5 discusses the related work on designing comprehension models for agents. Section 6 summarizes the paper and identifies our plans for future research.</p>
<h1>2. System Overview</h1>
<p>Rosie is a instructable agent implemented in the Soar cognitive architecture (Laird, 2012). It is embodied as a robotic arm that can manipulate small foam blocks on a table top. The workspace also contains four named locations: pantry, garbage, table, and stove. These locations have associated simulated functionalities. For example, a stove can be turned on and off, and the pantry can be opened and closed. These functionalities change the state of the world. For example, when the stove is turned on, it changes the simulated state of an object on it to cooked.</p>
<h3>2.1 Perception, Actuation, and Interaction</h3>
<p>Rosie senses the environment through a Kinect camera sensor. The perception system segments the scene into objects and extracts features for three perceptual properties: color, shape, and size. These properties along with the position and bounding volume of the objects in the world are provided to Rosie, which uses them for perceptual and spatial reasoning. For locations and objects, the simulated state (such as open, on, cooked) is also included in its description.</p>
<p>To act in the world, Rosie sends discrete primitive commands to the controller. These include object manipulation: pointTo(obj), pickUp(obj), and putDown $(x, y)$; and simulated location operation: open(loc), close(loc), turnOn(stove), and turnOff(stove). The robot controller converts these discrete commands to continuous closed loop policies, which change the state of the environment. Human instructors can interact with Rosie through a simple chat interface. Instructor's utterances are preprocessed using the Link-Grammar parser (Sleator \&amp; Temperley, 1991) to extract parts of speech and syntax. Rosie responds using semantic structures that are translated to language using templates. The instructor can point to an object by clicking on it in the camera feed.</p>
<h1>2.2 Learning with Instruction</h1>
<p>Rosie begins with procedural knowledge for parsing language, maintaining interactions, and learning from instruction. It also knows how to perform primitive actions in the world. Through situated interactive instruction, Rosie learns a novel word by acquiring a corresponding concept and building an association between them. For adjectives and nouns (such as red, large, or cylinder), the agent learns new classifications of perceptual features (color, size, and shape) from interactive training. For prepositions (such as right of), the agent learns compositions of primitive spatial predicates. For verbs (such as move), the agent learns novel task knowledge.</p>
<p>Learning in Rosie is interactive. Whenever it encounters a new word that it cannot comprehend by associating it with known concepts, it initiates interactions to learn the concept and the grounding of the word. The interactions are situated in the environment. Through instruction, the mentor provides specific examples of the concepts in the environment. When the instruction is complete, Rosie can comprehend the word and use the associated concept for classification, spatial reasoning, and action. As the human-agent interaction is linguistic, ambiguities may arise during instruction. A common form of ambiguity arises due to the use of underspecific referring expressions such as it or that object. Other ambiguities arise from imprecise description of actions such as move the red cylinder to the table that do not identify what relationship should be established between the red cylinder and the table. Rosie's comprehension model must alleviate such ambiguities by incorporating information from its state perceptions, domain knowledge, and experience.</p>
<h3>2.3 Memories and Their Contents</h3>
<p>We now give a brief description of the representations used in Rosie. Detailed explanations can be found in our earlier work (Mohan et al., 2012). The agent's beliefs about the current state are held in its working memory. These beliefs are object-oriented and are derived from its perceptions of the world, from its experiential knowledge of the world encoded in its long-term memory, and from its interactions with the human collaborator.</p>
<p>Rosie's visual knowledge is encoded in its long-term perceptual memory. The memory accumulates training examples that are used to classify objects in terms of visual attributes: color, size, and shape. Each visual attribute has a k-nearest neighbor ( kNN ) classifier associated with it and each class within the kNN is referred to using a perceptual symbol. For example, the domain of the color attribute may contain perceptual symbols C22, C53, C49, each of which correspond to colors known to Rosie. All perceptible objects are represented in working memory along with the known value assignments to their visual attributes.</p>
<p>Rosie's spatial knowledge is distributed between its semantic memory and its spatial-visual system. It learns and represents spatial prepositions such as on and near as compositions of known primitive spatial literals in the spatial-visual system that encode alignment along axes and distance between objects. It generates symbolic representations of spatial relationships between perceptible objects using this knowledge. This representation is useful for reasoning about existing spatial relationships on the workspace (such as the red cylinder is on the stove) and executing actions that establish specific spatial relationships between arguments (such as put the red cylinder on the stove).</p>
<p>Rosie can learn goal-oriented tasks, such as cook a steak, that require it to achieve a composition of spatial and state predicates by executing a policy defined hierarchically over primitive actions. Its task knowledge is distributed across its semantic and procedural memories. While Rosie's semantic memory stores a task-concept network that includes the goal definition of the task and constraints on how the goal should be instantiated, its procedural memory encodes the task's availability conditions, policy, and termination conditions represented as rules and implemented through operator proposal, selection, and application.</p>
<h1>3. The Indexical Model of Comprehension</h1>
<p>Consider the imperative sentence move the large red cylinder to right of the blue triangle. We assume that the speaker makes this utterance because she intends for the hearer to execute the requested action. The process of indexical comprehension involves identifying the referents of the linguistic input and composing them to generate an action instantiation that is grounded in the modal symbols that support reasoning about and manipulation of the environment. Following the Indexical Hypothesis, comprehension is carried out in three stages described below. Note that in the remainder of this paper, the impelmentation of the Indexical Model is referred to as the model and the complete agent that includes the perception, actuation, and learning components along with the Indexical Model for comprehension is referred to as Rosie.</p>
<h3>3.1 Indexing</h3>
<p>After preliminary lexical processing, it is established that the linguistic input contains two referring expressions (REs: the red cylinder and the blue triangle), a spatial preposition (to the right of), and a verb (move). The goal of the indexing step is to identify the referents for these linguistic units. The model uses a simple referential grammar: nouns and adjectives refer to visual properties; referring expressions refer to objects; prepositions refer to spatial relationships; and verbs refer to tasks. Figure 1 shows the objects $(012,032)$ and semantic networks A, B, and C that form the referent set of REs, prepositions, nouns/adjectives, and verbs. We introduce the term indexical maps for structures in semantic memory that encode how linguistic symbols (nouns/adjectives, spatial prepositions, and action verbs) are associated with perceptual symbols, spatial compositions, and action-concept networks. Now we describe how indexical maps are used during comprehension. In the following text, $R_{\text {sub }}^{\text {super }}$ denotes the referent set. The superscript super denotes the contents of the set ( $o$ for objects, $s$ for spatial relations, and $a$ for actions/tasks) and the subscript $s u b$ denotes the words used to generate the set.</p>
<p>To index REs (the red cylinder), the model must first index the descriptive words (red and cylinder). For each of these words, the model queries the semantic memory for a node that was previously learned to be associated with the lexical string. For the string red, the memory returns node L1 (refer to Figure 1). Node L1 maps the lexical string red to the corresponding perceptual symbol C22 which is a class in the color classifier. Once the model has retrieved perceptual symbols for all words, it searches working memory for objects that have the required perceptual symbols. These objects are assumed to be the intended referents of the RE. In cases where the RE is underspecific (e.g., this block), there may be multiple objects that match, resulting in ambiguity. The</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Environment state and the knowledge encoded in Rosie's semantic memory. The white nodes (P4, L1, M2, L2, L3) represent indexical maps between amodal linguistic symbols (in red: right, red, move) and modal domain knowledge. Yellow nodes (R10, R11, R12, A31) represent spatial symbols and slots (round rectangles: A31), blue nodes (V1, A11, A21) represent visual symbol and slots, and green nodes (P2, P3, G2, G3) represent procedural symbols.
model can use other kinds of information to resolve such ambiguities as we describe in Section 4.1. For the sake of simplicity, in this example we assume that only one object (012) matches the cue. This object is included in the referent set $\left(R_{\text {red, cylinder }}^{a}=012\right)$ for the RE the red cylinder. Similarly, $R_{\text {blue, triangle }}^{o}={032}$ for the RE the blue triangle. If these sets are empty, it indicates that Rosie lacks knowledge to generate groundings of the RE, in which case it prompts the instructor for training examples.</p>
<p>Prepositions are indexed in a similar fashion. For a preposition string (right), the model queries the semantic memory for an indexical node that had previously been learned and is associated with it. On the retrieval of the requested node (P4), the model creates the referent set $R_{\text {right }}^{s}={\mathrm{P} 4}$. If the set is empty, Rosie asks the instructor to provide an example of right-of in the environment.</p>
<p>To index the verb move, the model queries the semantic memory for a node that is connected to the string move. The memory returns the node L2. Then the model retrieves the mapping node M2 that associates the verb to domain knowledge of the task - the goal definition G2 and procedural operator node P2. The referent set for the verb consists of the task-concept network, $R_{\text {move }}^{a}={(\mathrm{M} 2, \mathrm{P} 2, \mathrm{G} 2)}$.</p>
<h1>3.2 Instantiating Domain Knowledge</h1>
<p>Once the referents have been identified, the next step is to retrieve the domain knowledge associated with them and instantiate it under the syntactical constraints of the sentence. The model begins by retrieving the previously learned syntactical nodes associated with the verb move. The sentence move the large red cylinder to the right of the blue triangle has a direct object (RE, the large red cylinder) and a prepositional object (RE, the blue triangle) connected to the verb through the preposition right. Following this syntactical structure, the model retrieves the direct object (direct-obj</p>
<p>in the figure) node A11 and the prepositional phrase object (pp-object in the figure) node L3 which is further expanded to retrieve nodes A21 and A31. The slot nodes A11 and A21 can receive sets of objects in the environment. A11 is filled by $R_{\text {red, cylinder }}^{o}$ as the red cylinder is the direct object of the verb put and A21 is filled by $R_{\text {blue, triangle }}^{o}$, as the blue cylinder is the RE in the prepositional phrase of the verb. A31 is a spatial slot that is filled by $R_{\text {right }}^{s}$, the referent set for right.</p>
<p>Next, the model expands the domain knowledge nodes P2 and G2. The subgraph (P2, P3, A11, A21, A31) constrains how the policy operator op_1 is instantiated. The subgraph (G2, G3, A11, A21, A31) constrains the instantiation of goal of the task. The values of the slot nodes (A11, A21, A31) determine the contents of the goal and the policy operator op_1. Instantiation of domain knowledge results in the interpretation set $I_{s}$, which contains elements that encode: execute policy op_1 defined over objects drawn from sets $R_{\text {red, cylinder }}^{o}, R_{\text {right }}^{s}$, and $R_{\text {blue, triangle }}^{o}$ until the corresponding goal is achieved. For the imperative move the large red cylinder to the right of the blue triangle in the scene in Figure 1, the policy op_1 will be instantiated over arguments 012, 032, and P3 with the goal P3(012,032). The policy op_1 selects pick-up(012) if the arm is empty and put-down(012,P3,032) if the arm is holding 012.</p>
<p>In Glenberg and Robertson (1999) formulation of the Indexical Hypothesis, this step was described as deriving the affordances. However, the term instantiating domain knowledge better describes our formulation of the process.</p>
<h1>3.3 Meshing</h1>
<p>The interpretation set $I_{s}$ is the set of different groundings of the imperative sentence, which can have several elements arising from underconstraining cues in the linguistic input. However, only a subset of these groundings can be executed in the environment given physical constraints and spatial relationship between objects. For example, the open action can only be executed for stove and pantry. When open is used with an underconstraining RE such as it, there will multiple interpretations, but only two of those interpretations can be executed in the environment.</p>
<p>Suppose $A$ is a set of tasks that can to be executed in the current state based on their availability conditions. The intersection set $I_{s} \cap A$ is the set of tasks that the instructor intends Rosie to execute. If this set contains a single element, that task operator is selected and executed. If this set contains multiple elements, further interaction or internal reasoning is necessary for resolution. The cardinality of the referent sets $(R)$ is used to determine the source of the ambiguity. Rosie asks questions to gather information that will reduce the cardinality of the ambiguous set. If the $I_{s} \cap A=\phi$, Rosie does not have enough knowledge to generate the correct groundings for the required task. This is an opportunity to learn the task, so Rosie begins a learning interaction by prompting the human collaborator to present an example execution.</p>
<h2>4. Dealing with Complexities</h2>
<p>Various issues can arise while attempting to generate an unambiguous and complete interpretation of an utterance. Ambiguities occur when the linguistic cues underspecify their referents, resulting in multiple elements in their referent sets and, consequently, multiple interpretations. In this section, we describe how the Indexical Model deals with ambiguities that arise from the contextual use of</p>
<p>referring expressions and with situations in which the information required to instantiate a policy is not completely specified in the linguistic input.</p>
<h1>4.1 Reference Resolution</h1>
<p>Humans use a variety of surface forms to refer to the same entity. A few, such as definite noun phrases (the large red cylinder on the table), may uniquely identify the intended referent from the current shared perceptions. However, a majority of referring expressions (REs) encountered in conversations, such as noun phrases with indefinite determiners (a cylinder), demonstrative/diectic pronouns (this, that), and personal pronouns (it), are ambiguous.</p>
<p>For the generation and comprehension of REs, the communicative goal is the identification of the intended object by the hearer. The form of REs and other linguistic (word order) and phonetic (intonation) aspects are influenced by the cooperative speaker's assumptions about the relative salience of referents to the hearer. An object might be more salient than others because it is useful in performing a task, it is being pointed at, it changes appearance, or it is unexpected. The ongoing discourse can also make objects more salient. Speakers make assumptions about which objects are more salient to the hearers and use these assumptions to choose an appropriate RE. More salient objects can be referred to by less informative REs, as the hearer can exploit saliency for disambiguation.</p>
<p>Gundel et al. (1993) express the notion of the current and historical salience of an object to the hearer as its cognitive status. They propose a Givenness Hierarchy (GH) that relates the cognitive status of objects with different RE surface forms. The GH identifies six cognitive statuses, only four of which are relevant to this paper: in-focus (personal pronouns) $&gt;$ activated (demonstrative pronouns, demonstrative noun phrases) $&gt;$ uniquely-identifiable (definite noun phrase) $&gt;$ type-identifiable (indefinite noun phrase). Each status in the GH is the necessary and sufficient condition for use of the corresponding RE and entails all the lower statuses. The choice of a RE form by the speaker is indicative of what cognitive status is useful for resolution. Given the cognitive status of an object and the hearer's knowledge about the environment, the information in the RE uniquely identifies the intended referent.</p>
<h3>4.1.1 Non-linguistic Contexts</h3>
<p>Knoeferle and Crocker (2006) identify two dimensions of the interaction between the linguistic and situated context: informational and temporal. The first dimension refers to the rapid integration of diverse information for various cognitive modules including perceptual processes and domain knowledge. The second dimension refers to the temporal coordination between attentional processes and utterance comprehension. While REs such as noun phrases (lower in the GH) exploit the informational dimension of language-context interaction, ambiguous REs such as pronouns (higher in the GH) exploit the temporal dimension. To process the complete range of RE forms in the GH, the Indexical Model exploits both the informational (described earlier) and the temporal dimensions (described below).
Interaction: When conversation participants communicate, they focus their attention on only a small portion of what each of them perceives, knows, and believes. Some entities (objects, relationships,</p>
<p>actions) are central to information transfer at a certain point in dialog and, hence, are more salient than others. This is exploited by both the speaker and the hearer. It lets the speaker refer to focused entities with minimal information and lets the hearer heuristically constrain the set of possible referents, reducing cognitive load on both.</p>
<p>Rosie has a model of instructional interaction (Mohan et al., 2012) based on the computational theory of task-oriented discourse by Grosz and Sidner (1986). The interaction model organizes the discourse structure according to the goals of the task. The current state of the human-agent interaction is represented by three elements. Events cause change, either in the environment (actions such as pick-up(032)), the dialog (utterances such as Where is the red cylinder?), or Rosie's knowledge (learning events such as explanation-based learning). A segment is a contiguous sequence of events that serves a specific purpose and organizes the dialog into purpose-oriented, hierarchical blocks in accordance with Rosie's goals. The purpose of a segment is determined based on preencoded heuristics about instructional interactions. Finally, the focus of the interaction is captured in a stack of active segments. When a new segment is created, it is pushed onto the stack. The top segment of the stack influences the agent's processing by suggesting a purpose that Rosie should act to achieve. When the purpose of the top segment is achieved, it is popped from the stack.</p>
<p>The stack maintains a set of all referents (objects, spatial predicates, actions) that are related to the events in its active segments. The set of objects ( $O^{\text {stack }}$ ) is most pertinent to this paper because it identifies all the objects that have been referred to in the current discourse, making them more salient than other perceivable objects.
Attention: Object referents that have been brought to attention, either through linguistic or extralinguistic means, but are not in the focus of the ongoing communication are usually referred to by demonstrative pronouns or demonstrative noun phrases (this, that cylinder) (Gundel et al., 1993). The extra-linguistic means may include pointing by the speaker or unexpected stimulus such as a loud noise. To resolve such REs, Rosie must maintain the history of references to objects in its perceptions.</p>
<p>Rosie uses the architectural recency-based activation in Soar's semantic memory as a form of attention. The recency-based activation biases retrieval from semantic memory towards the more recently accessed elements. An object is accessed only if it was pointed at or was used in an action or learning. Anytime an object is accessed, Rosie stores its representation in the semantic memory which boosts its activation in accordance with recency computation. A completely ordered subset $O^{\text {active }}$ of the highest activated $n$ objects is retrieved from Rosie's semantic memory to its working memory. These are combined with objects in focus to give a set of objects to which Rosie is attending ( $O^{\text {attend }}=O^{\text {stack }} \cup O^{\text {active }}$ ). This formulation of attention combines linguistic and extra-linguistic notions of salience.</p>
<h1>4.1.2 Resolving References in the Indexical Model</h1>
<p>In Section 3.1, we described the indexing of referring expressions in simple cases where the words in the RE and their corresponding perceptual symbols by themselves uniquely identified the referent object. Here, we give details about how an ambiguous RE is indexed by incrementally adding diverse types of information in the Indexical Model.</p>
<ol>
<li>
<p>Maintain cognitive status. Following the Givenness Hierarchy, the model maintains different cognitive statuses for objects:</p>
</li>
<li>
<p>Objects in the interaction stack ( $O^{\text {stack }}$ ) have the in-focus status;</p>
</li>
<li>Objects that are being attended to ( $O^{\text {active }}$ ) have the activated status;</li>
<li>
<p>Objects in perceptions ( $O^{\text {percept }}$ ) have the identifiable status.</p>
</li>
<li>
<p>Assign resolution type. For any RE $r$, the model determines its resolution type based on its surface form. If the RE is:</p>
</li>
<li>
<p>a definite noun phrase (the red cylinder), demonstrative pronoun (this), or personal pronouns (it), the speaker has a specific intended referent and comprehension should unambiguously determine it (unique resolution);</p>
</li>
<li>
<p>an indefinite noun phrase (a red cylinder), this indicates that there is no specific intended referent and any object that fits the noun phrase can be used for resolution (any resolution).</p>
</li>
<li>
<p>Determine the candidate referent set. The model exploits the heuristic that surface forms of REs indicate which set contains the intended referent. The candidate referent set is:</p>
</li>
<li>
<p>$R_{r}^{o}=O^{\text {stack }}$ for personal pronouns (it);</p>
</li>
<li>$R_{r}^{o}=O^{\text {attend }}$ for demonstrative pronouns (this, that) and noun phrases (this cylinder);</li>
<li>
<p>$R_{r}^{o}=O^{\text {percept }}$ for definite (the cylinder) and indefinite (a cylinder) noun phrases.</p>
</li>
<li>
<p>Apply the visual filter. Rosie's knowledge of the perceptual symbols and how they relate to words is useful in identifying the referents of descriptive REs (the red cylinder). The model indexes each descriptive word (red, cylinder) in a noun phrase, and then the model looks up its corresponding perceptual symbols, which are collected into a set as a cue. All the objects in the candidate set $\left(R_{r}^{o}\right)$ whose working memory representations do not contain this cue are deleted from this set.</p>
</li>
<li>Apply the spatial filter. If the RE uses spatial reference (the cylinder on the right of the pantry), referent sets for both noun phrases ( $R_{\text {cylinder }}^{o}, R_{\text {pantry }}^{o}$ ) are obtained. The model indexes the preposition right to retrieve the corresponding spatial relationship predicate P4. Items in $R_{\text {cylinder }}^{o}$ that do not satisfy the relationship P3 with any item in $R_{\text {pantry }}^{o}$ are deleted. This is a meshing step that combines linguistic information with the domain knowledge and the perceptual state.</li>
<li>Apply the task filter. If the REs are used with verbs, as in an action command (put the cylinder in the pantry), the model uses the knowledge of task restrictions to constrain their interpretation. To access this knowledge, the model indexes the verb to retrieve a task-operator and its corresponding goal. During meshing, it looks at all task-operator instantiations that are applicable in the current environmental state under the physical constraints and the knowledge of object affordances. Any object that does not occur in the arguments of currently applicable task instantiations is removed from $R_{r}^{o}$ of the RE.</li>
<li>Obtain partial ordering. The elements of the referent set $\left(r \in R_{r}^{o}\right)$ are partially ordered based on their cognitive status and resolution type. If resolution is unique (from step 1), then $r_{i} \in O^{\text {stack }}&gt;$ $r_{j} \in O^{\text {active }}&gt;r_{k} \in O^{\text {percept }}$. If resolution is any, then all objects have equal preference.</li>
<li>Resolve. After applying all available filters, if $R_{r}^{o}$ contains only a single object, that object is selected as the intended referent. If it contains multiple objects, the model uses the partial ordering obtained earlier to select the object highest in the order as the intended referent. If the partial</li>
</ol>
<p>ordering is not informative enough for resolution, the model initiates a subdialog to obtain more information from the instructor. If the resolution is any, all objects have equal preference and one is chosen at random.</p>
<p>The resolution process described here integrates seamlessly with the incremental learning modules and the interaction model for mixed-initiative conversations.</p>
<h1>4.1.3 Evaluation and Analysis</h1>
<p>Experiments. We generated a corpus of 25 instructor utterances that address different capabilities of Rosie. This corpus contains instruction sequences that teach and query Rosie about objects and their attributes, present and verify grounded examples of spatial prepositions, and teach verbs. This corpus also contains references to three objects in the scene that use varying forms of referring expressions, including 12 instances of personal pronouns (such as it), four instances of demonstrative pronouns (such as this), three instances of demonstrative phrases (such as that cylinder), and 14 varying length noun phrases with different descriptive words (such as the red cylinder). We evaluated alternative models of comprehension that exploit the informational and temporal dimensions of non-linguistic context. The baseline model $p$ uses the context derived from perceptual semantics only. Model $p+t$ exploits the restrictions derived from task knowledge along with perceptual semantics while model $p+t+a$ exploits the temporal dimension by encoding the attentional state. Model $p+t+a+d$ encodes both the attentional and dialog states. Each comprehension model was evaluated using the instruction corpus on different scenarios of increasing perceptual ambiguity in the environment obtained by adding distractor objects as shown in Figure 2.
Evaluation metric. Rosie is an interactive agent that engages the human instructor in a subdialog if it fails at any stage in its processing. On failing to resolve ambiguous referring expressions in sentences, Rosie asks questions to obtain more information that will constrain its resolution. The instructor can, then, incrementally provide more identifying information. An example dialog is shown in Figure 2. The question-answer pairs (object identification queries) are informative of how ambiguous an RE is to the model given the ambiguity in the current scenario and the contexts. Note that the instructor could have provided all the identifying information in a single response (Which object?, the blue cylinder in the pantry). However, letting Rosie take the initiative in resolution ensures that it accumulates the minimum information required for unique identification in the current situation. The number of object identification queries in this set up is positively correlated with the number of objects in $R_{e}^{a}$ after all filters have been applied.
Results. The graph in Figure 2 shows the number of object identification queries asked by Rosie while using different comprehension models in scenarios with varying perceptual ambiguity. The models reliably integrate information provided incrementally over several interactions for resolution. Consequently, all REs were eventually correctly resolved in all models in all scenarios. The model $p+t+a+d$ can exploit the informational and temporal dimensions effectively for resolution. To establish that the non-linguistic context contributes information above and beyond what is encoded in the linguistic features, we ran Stanford CoreNLP (Lee et al., 2012) on our corpus. Coreference resolution in CoreNLP failed to correctly resolve ten ( $28.6 \%$ ) references. These results suggest that the grounded contexts are essential for robust comprehension in an embodied agent.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Sample Dialog</th>
<th style="text-align: center;">Models \&amp; Dimensions</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">I: Pick it up. <br> A: Which object? <br> I: the blue one. <br> A: Which blue object? <br> I: the cylinder. <br> A: Which blue cylinder? <br> I: the one in the pantry.</td>
<td style="text-align: center;">$p$ : perceptual semantics <br> $p+t$ : perceptual semantics, task knowledge <br> $p+t+a$ : perceptual semantics, task knowledge, attention <br> $p+t+a+d$ : perceptual semantics, task knowledge, attention, dialog context</td>
</tr>
<tr>
<td style="text-align: center;">Scenario Ambiguity</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Ambiguity 1 only intended referents <br> Ambiguity 2 perceptually distinct distractors <br> Ambiguity 3 distractors: different shapes, same colors <br> Ambiguity 4 distractors: same colors and shapes</td>
<td style="text-align: center;"><img alt="img-1.jpeg" src="img-1.jpeg" /></td>
</tr>
</tbody>
</table>
<p>Figure 2. (left) A sample dialog and various models and scenarios used for evaluation of RE resolution. (right) Number of object queries asked by Rosie for RE resolution.</p>
<p>The baseline model $p$, which only exploits the contexts derived from perceptual semantics, generates the most queries for all levels of ambiguity. Model $p+t$ is able to use its knowledge about the task to constrain resolution and, therefore, requires fewer queries for achieving the same resolution results. The models that exploit both the temporal and informational dimensions require even fewer queries to achieve similar performance across all scenarios. Conversing with agents that only encode the informational dimension of non-linguistic context usually requires wordy REs, such as the red cylinder in the pantry, that must be repeated in all interactions related to that object. The use of the temporal dimension for comprehension allows the use of shorter referring expressions (it, this cylinder), resulting in efficient communication.</p>
<p>As perceptual ambiguity in the environment increases, models that exploit only the informational dimension $(p, p+t)$ require more perceptual information for resolving REs. Models that exploit the temporal dimension $(p+t+a, p+t+a+d)$ ask the same number of queries across all scenarios, demonstrating that the use of co-reference is an efficient way to communicate about objects in human-agent dialogs. It lets the instructor communicate the intended referent without incorporating large amounts of information in utterances in perceptually ambiguous scenarios.</p>
<h1>4.2 Unexpressed Argument Alternations of Verbs</h1>
<p>In Rosie, the goal of comprehension of an imperative sentence is to correctly instantiate a task that can be executed in the environment. The verb of the sentence identifies the task and the verb's objects indentify the arguments of the task. The syntax is useful in instantiating the task goals and a policy that can be executed in the environment to achieve them. The syntax of English verbs is flexible and often omits objects. Levin (1993) characterizes the variations in verb usage as alternations. Consider, for an example, an imperative take the trash out to the curb that informs the hearer that the direct object trash has to be placed on the location curb. An alternative imperative sentence that conveys the same meaning is take the trash out. The target location of the trash is left unexpressed. This alternation is termed unexpressed object alternation by Levin (1993). This verb alternation pose a significant challenge to an agent that seeks a precise action interpretation that can be executed in the environment.</p>
<p>Humans generate and comprehend such sentences by relying on the shared knowledge about the domain. In the example, both the speaker and the hearer know that the trash is usually put on the curb. This lets the speaker omit the location in the sentence take the trash out for the sake of communicative efficiency. The choice of this syntax by the speaker indicates that they assume that the hearer can fill the missing location from their knowledge of the domain. Upon hearing the utterance, the hearer must exploit this knowledge and generate an appropriate, complete representation of the action.</p>
<h1>4.2.1 Exploiting the Hearer's Instructional Experience</h1>
<p>To deal with imperative sentences with unexpressed information about the action, the model relies on Rosie's prior experiences of interacting with the instructor and acting in the domain. Consider the verb move and the variations of imperatives that can be constructed from it:
(a) Move the green object to the right of the table.
(b) Move the green object to the table.</p>
<p>In (a), the direct-object the green object, the location the table, and their spatial relationship (right of) are completely specified. In (b), the spatial relationship is omitted with an understanding that there is a default configuration (on) between the object and the location. We extend Levin (1993) characterization of unexpressed object alternations to include other types of verb arguments (spatial relationship in this case) and term it unexpressed argument alternation.</p>
<p>The default configuration can be extracted from the experience of learning how to perform the move task. When Rosie is asked to execute a task for the first time, it leads the instructor through a series of interactions to learn the structure of the task. Suppose assume that Rosie does not know how to perform move. On receiving the imperative sentence (a), it asks a question about the goal (what is the goal of the task?) and the human instructor replies, the goal is the green object to the right of the table. By analyzing the sentence and the goal description, Rosie extracts a general schema that relates the linguistic structure of the utterance to the goal of the task. It uses a simple heuristic that information (object, location, and spatial relationship) specified in the imperative sentence can be generalized away in the goal definition. The agent assumes that future instances of the verb move will completely specify the goal. At a later stage, Rosie receives the sentence (b). Using its knowledge of the goal definition, Rosie attempts to generate an instantiation. This fails because no relationship is specified. So, Rosie asks the instructor to describe the goal. The instructor may reply with the goal is the green object is on the table. By comparing the current situation (for sentence (b)) and its experience with sentence (a), Rosie concludes that the verb move may be used in two alternations. The representation of move is augmented to reflect that, if the relationship is not specified, it should attempt to establish the on relationship between the object and the location. Figure 3 shows this augmentation to the task-concept network of the verb move as dotted edges and nodes. Note that this network is an augmented version of the network C in Figure 1.</p>
<p>When comprehending the verb move in the future, the model can use the default values to complete the argumentation of the action if those values are not specified in the linguistic input itself. This lets the model use Rosie's instructional experience to fill in information that is not specified in the linguistic input but is essential for action.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. (left) Declarative knowledge for move after the training episode. (right) Number of interactions required for comprehending verbs in different alternations.</p>
<h1>4.2.2 Evaluation and Analysis</h1>
<p>Experiment: In an environment with four objects, we instructed Rosie to perform eight instances of five tasks using an uniform distribution over alternations of the relevant verb. Here we characterize the verbs used in the experiments.</p>
<ul>
<li>The verb pick takes a direct object and does not have any alternation. Example: pick up the red cylinder.</li>
<li>The verb put takes a direct object and a prepositional object and does not have any alternation. Example put down the red cylinder on the table.</li>
<li>The verb move has two alternations. The first specifies the object, the prepositional object, and the intended spatial relationship (as in move the red cylinder to the right of the table). The second does not specify the spatial relationship between the direct and prepositional object (as in move the red object to the table).</li>
<li>The verb store has two alternations. The first specifies the direct object, the prepositional object, and the intended spatial relationship between them (as in store the red cylinder in the pantry). The second leaves the prepositional object unexpressed (as in store the red cylinder).</li>
<li>The verb cook has two alternations. One specifies the instrument used for cooking along with the object to be cooked (as in cook the steak on the stove). The other leaves the instrument unexpressed (as in cook the steak).</li>
</ul>
<p>The first two verbs are primitives that have been pre-encoded in Rosie; the last three are acquired through human-agent linguistic interaction. For training, Rosie was taught the task with the first alternation of the corresponding verb. After it successfully learns the task, we asked it to perform the task using the second alternation. Any questions asked by Rosie during this training episode were appropriately answered. Two variations of the comprehension model were evaluated. Model+e uses Rosie's instructional experience to augment the linguistic input that is missing information required</p>
<p>for task execution. Model-e is a lesioned version of model+e that does not exploit the instructional experience but relies on asking the instructor a question for the missing information. Both models were given the same instructional experience ( 12 interactions for move and 16 interactions for cook). Results: The graph in Figure 3 shows the number of interactions that occurred during the comprehension of task commands in model+e (in blue) and model-e (in red). The patterned bars correspond to the first alternation and the plain bars correspond to the second alternation (if applicable). For verbs without alternations (pick and put), both models take equal number of interactions to execute the task (one per task instance). For verbs with alternations, the models behave differently for different alternations. For the first alternation in which all information is specified, both models take one interaction per task. However, for the second alternation that leaves some argumentation unexpressed, model+e takes only one interaction per task for performance because it uses the knowledge acquired through learning to fill in the missing information. Model-e must ask questions to gather the information missing from the sentences with unexpressed verb argumentation, resulting in more human-agent interactions (three per task instance). Both models comprehend both alternations of verbs and correctly execute the task.</p>
<h1>5. Related Work and Qualitative Analysis</h1>
<p>This paper addresses the challenge of situated language comprehension for intelligent agents that continually learn from their experience in complex domains. A primary purpose of a linguistic faculty in an intelligent agent is the exchange of information with collaborators, which influences and guides reasoning, behavior, and learning. This motivates the study of language as a communication system that functions with, and is informed by, other cognitive capabilities. In this light, we propose five properties that a situated comprehension model should incorporate:</p>
<p>D1 Referential. It must implement a theory of translating amodal linguistic symbols to modal representations of beliefs, knowledge, and experience that are external to the linguistic system. The Indexical Model addresses this by formulating the problem of language comprehension as search over perceptions, short-term memory, and long-term knowledge.
D2 Integrative. Human language is highly contextual and relies on several non-linguistic sources to convey meaning. To successful comprehend language, a model must exploit multiple information sources, including perceptions, domain knowledge, common-sensical knowledge, and short- and long-term experiences. It should also readily incorporate information from nonverbal communication such as gestures and eye gazes. In the Indexical Model, the background knowledge constrains and guides search. We have shown how diverse kinds of background knowledge can be exploited to generate and evaluate interpretations to handle ambiguities and missing information.
D3 Active. The model should actively use all its knowledge and reasoning capabilities to generate and reject candidate hypotheses. Such active processing not only informs further communication with the collaborator, by the way of requesting for clarification and repetition, but may also inform knowledge acquisition. The Indexical Model retains information that helps to determine the cause of failure and to generate and ask questions in case of ambiguity or missing information.</p>
<p>D4 Adaptive and expandable. As Rosie gathers more experience and knowledge of the environment, its comprehension capabilities must scale elegantly and robustly. This desideratum was not explicitly pursued in this paper but our previous work (Mohan et al., 2012) shows that indexical comprehension scales with acquisition of perceptual, spatial, and action knowledge.
D5 Incremental. A model must build up the meaning representation as each word is processed. Likely continuations may be inferred from these partial structures informing linguistic perception and syntactic ambiguity resolution. Incremental processing was not addressed in this paper and will be studied in future.</p>
<p>In the remainder of this section, we use these desiderata to analyze the approaches to representing and using language semantics in various fields of AI research.</p>
<p>Research on semantics in computational linguistics and natural language processing can be broadly categorized into three distinct groups, formal, distributional, and grounded semantics. While the earlier two approaches have been well studied in the literature, the last approach has recently gained momentum. The formal approaches to semantics represent meaning as amodal first-order logic symbols and statements. Although, this allows for incorporating extra-linguistic knowledge during comprehension through inference, the symbols and predicates are not grounded in the physical world. Distributional semantics usually incorporates linguistic contexts with no explicit groundings to the observations from the environment.</p>
<p>Work on grounded semantics can be characterized as an extension of formal semantics to include state and action information from environments. Example applications include navigation tasks (Chen \&amp; Mooney, 2011) and RoboCup sportscasting (Liang et al., 2009). These projects have focused on acquisition (D4) of grounded lexicon and semantic parsers (D1) from an aligned corpora of agent behavior and the text that describes it. There are several reasons for why such approaches cannot be used to design collaborative agents that engage in situated communication. These methods focus on statistical batch learning paradigms. Although, this results in comprehension models robust to errors in linguistic input, they cannot be extended online. The inability to comprehend an utterance is reported as a failure and which is not sufficient to drive situated communication or learning (D3). Further, the work proposes that the entire complexity of language comprehension can be encoded in a semantic parser and does not address the use of reasoning mechanisms and background conceptual knowledge for the purposes of language comprehension. Finally, these approaches assume a fairly simplistic agent with propositional state and action representations. This simplistic representation of the world state and dynamics poses problems in adapting the comprehension model to agents embedded in physical environments that require complex, relational representations for reasoning and action. These approaches do not provide insights on the role of non-linguistic context on language processing (D5).</p>
<p>In the robotics community, grounded comprehension has been studied in the context of describing a visual scene (Roy, 2002), understanding descriptions of a scene (Gorniak \&amp; Roy, 2004), understanding spatial directions (Kollar et al., 2010), and understanding natural language commands for navigation (Tellex et al., 2011). These comprehension models work with the complex state and action representations required for reasoning about physical worlds (D1). Their primary focus has been on the acquisition of grounding models through batch-learning from human-generated descriptions of robot's perceptions or behavior. However, generating an annotated corpus is expensive. The</p>
<p>agents are prone to failure if their training is insufficient for grounding a novel instruction. An interactive agent on the other hand will switch to learning mode if it is unable to comprehend the instruction. It is unclear if such data extensive, corpus-based, batch-learning mechanisms can be effectively incorporated in online and incremental human-agent interactions, allowing the agent to guide communication. Additionally, these mechanism do not address the challenges arising from ambiguities in natural language.</p>
<p>SHRDLU (Winograd, 1972) is a well-known early attempt to design an intelligent agent that could understand and generate natural language referring to objects and actions in a simple virtual blocks world (D1). It performed semantic interpretation by attaching short procedures to lexical units. It demonstrated simple learning as the user could define compositions of blocks (such as a tower) that the system would remember and could construct and answer questions about (D3). The system was not physically grounded, did not learn new procedures (D4) and, therefore, was constrained to pre-programmed behaviors.</p>
<p>Ongoing work on Direct Memory Access Parsing (DMAP; Livingston and Riesbeck, 2009) studies the utility of incorporating information from ontological and instance-based inference for linguistic processing in the context of learning by reading. DMAP incrementally integrates memory during parsing, which can reduce the number of ambiguous interpretations and reference resolution. DMAP has several desirable properties. It is referential (D1), integrative (D2), and active (D3) but it has not been investigated in human-agent interaction contexts. Our work provides further support to the primary thesis that linguistic features are cues to the hearer/reader to search their knowledge and experience.</p>
<p>Other cognitive system research has addressed the challenge of situated language processing for human-agent interaction. Scheutz et al. (2004) present a visually-grounded, filter-based model for reference resolution that is implemented on a robot with audio and video inputs. Ambiguities are resolved by accounting for attentional context arising from fixations in the work area. In related work, Kruijff et al. (2007) demonstrated incremental parsing at multiple levels that includes nonlinguistic contexts, such as the ongoing dialog and declarative pre-encoded selectional restrictions along with visual semantics. Apart from being referential and integrative, these projects address issues that arise in spoken dialog processing and online, incremental comprehension (D5). Brenner et al. (2007) describe how action commands can be interpreted in a task-oriented fashion to identify and instantiate goals and plans. This model brings in knowledge about initial state and goal descriptions that are relevant to generating and executing a plan. This is similar to our indexical comprehension of verbs. Our work can be viewed as an extension of these efforts to develop a more complete comprehension model for intelligent agents.</p>
<h1>6. Concluding Remarks</h1>
<p>In comparison to standard approaches to semantics and meaning representations prevalent in natural language community, the Indexical approach to language comprehension described in this paper affords several advantages. Previous approaches either encode semantics as amodal symbols that that are not grounded in real-wold experiences or as propositions which do not capture the relational or probabilistic properties of complex environments. In the Indexical approach, semantics can be en-</p>
<p>coded using diverse, modality-specific representations. These include probabilistic representation for perceptions, relational representation for spatial reasoning, and hierarchical policies for task execution, and models for reasoning about the environmental dynamics. Such representations are typical of agents designed to function in complex environments. Additionally, standard learning algorithms ( kNN , explanation-based learning) can be used to expand the agent's knowledge, thereby, extending its situated comprehension capabilities.</p>
<p>In the formulation of comprehension as a search over short-term and long-term experiential knowledge, non-linguistic context has a natural role. It provides constraints over the hypothesis space and guides search. Non-linguistic context can be derived from various sources including the ongoing discourse, the current perceptual state, the knowledge of tasks, and the models of environmental dynamics. Other cognitive mechanisms such as reasoning and attention also contribute to comprehension by providing additional constraints on interpretations. We show that exploiting different contexts in our model reduces ambiguity in referring expression resolution. Experiential knowledge augments the linguistic input by incorporating knowledge from prior experiences with the environment. This is useful in situations where the linguistic input, such as in take the trash out, is underspecific and does not encode enough information for reasoning and action.</p>
<p>The focus of our future work will be on studying other linguistic ambiguities that may arise in instructional interactions and how they can be addressed by incorporating information from various cognitive modules. A concern is that one verb word may indicate different task goals and policies. For example, the sentences store the rice and store the milk indicate different goal locations (pantry for rice and refrigerator for milk). The comprehension model should be able to use the semantic categorization of the arguments to instantiate the goal with appropriate locations. Other ambiguities arise in determining the site of preposition phrase attachment. In the sentence store the red cylinder on the green block in the pantry, it is unclear if the phrase in the pantry attaches to the verb store directly, or to the phrase on the green block. This can be resolved by the incorporating the current state of the environment. Another concern is that the described model does not support interpretation of quantification (store all red objects) or of categories of objects (chess pawns cannot not move backwards). This limits what can be expressed in instructions for tasks and games. A different dimension of future research is incremental comprehension that would lead to robust performance on incomplete and ungrammatical linguistic input.</p>
<p>This paper has focused on mechanisms that are useful in language comprehension where the elements of an utterance can be directly grounded in the shared state between the speaker and the hearer. However, much of human communication is non-situated where people talk about scenarios that cannot be directly perceived and may have occurred in the past (retrospective) or is expected to occur in the future (prospective). Human hearers readily generate perceptual simulations guided by the content of the utterance and can use these models for reasoning about the scenario being described. The perceptual simulations are informed by the hearer's experience of the world. In future, we are interested in expanding the indexical approach described to address non-situated comprehension and its interaction with knowledge acquisition and learning.</p>
<h1>Acknowledgments</h1>
<p>The authors acknowledge the funding support of the Office of Naval Research under Grant Number N00014-08-1-0099. The authors thank Edwin Olson and the APRIL lab at the University of Michigan, Ann Arbor, for the design and implementation of the robot including its perception and actuation algorithms. The authors also appreciate the insightful comments and suggestions made by the reviewers.</p>
<h2>References</h2>
<p>Brenner, M., Hawes, N., Kelleher, J. D., \&amp; Wyatt, J. L. (2007). Mediating between qualitative and quantitative representations for task-orientated human-robot jnteraction. Proceedings of the Twentieth Joint International Conference on Artificial Intelligence, 2072-2077.
Cantrell, R., Schermerhorn, P., \&amp; Scheutz, M. (2011). Learning actions from human-robot dialogues. Proceedings of the Twentieth IEEE Symposium on Robot and Human Interactive Communication, 125-130. http : / / ieeexplore . ieee . org / lpdocs / epic03 / wrapper . htm ? arnumber $=6005199$
Chen, D. L., \&amp; Mooney, R. J. (2011). Learning to interpret natural language navigation instructions from observations. Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence, 859-865.
Fong, T., Nourbakhsh, I., \&amp; Dautenhahn, K. (2003). A survey of socially interactive robots. Robotics and Autonomous Systems, 42(3-4), 143-166. http://linkinghub.elsevier.com/retrieve/pii/ S092188900200372X
Glenberg, A. M., \&amp; Robertson, D. A. (1999). Indexical understanding of instructions. Discourse Processes, 28(1), 1-26. https://doi.org/10.1080/0163853990
Gorniak, P., \&amp; Roy, D. (2004). Grounded semantic composition for visual scenes. Journal of Artificial Intelligence Research, 21, 429-470. http://www.aaai.org/Papers/JAIR/Vol21/JAIR2113.pdf</p>
<p>Grosz, B., \&amp; Sidner, C. (1986). Attention, intentions, and the structure of discourse. Computational Linguistics, 12(3), 175-204.
Gundel, J. K., Hedberg, N., Zacharski, R., \&amp; Fraser, S. (1993). Cognitive status and the form of referring expressions in discourse. Language, 69(2), 274-307.
Kaschak, M. P., \&amp; Glenberg, A. M. (2000). Constructing meaning: The role of affordances and grammatical constructions in sentence comprehension. Journal of Memory and Language, 43(3), 508-529.
Knoeferle, P., \&amp; Crocker, M. W. (2006). The coordinated interplay of scene, utterance, and world knowledge: Evidence from eye tracking. Cognitive Science, 30(3), 481-529. https://doi. org/10.1207/s15516709cog0000\65
Kollar, T., Tellex, S., Roy, D., \&amp; Roy, N. (2010). Toward understanding natural language directions. Proceeding of the Fifth ACM/IEEE International Conference on Human-robot Interaction, $259-267$.</p>
<p>Kruijff, G.-J., Lison, P., Benjamin, T., Jacobsson, H., \&amp; Hawes, N. (2007). Incremental, multi-level processing for comprehending situated dialogue in human-robot interaction. Proceedings from the Symposium on Language and Robots, 11, 55-64.
Laird, J. E. (2012). The Soar Cognitive Architecture. MIT Press.
Lee, H., Chang, A., Peirsman, Y., Chambers, N., Surdeanu, M., \&amp; Jurafsky, D. (2012). Deterministic coreference resolution based on entity-centric, precision-ranked rules. Computational Linguistics, 39(4), 885-916. https://doi.org/10.1162/COLI
Levin, B. (1993). English verb classes and alternations. University of Chicago Press.
Liang, P., Jordan, M. I., \&amp; Klein, D. (2009). Learning semantic correspondences with less supervision. Proceedings of the Forty-Seventh Annual Meeting of the Association of Computational Linguistics, 91-99.
Mohan, S., Mininger, A., Kirk, J., \&amp; Laird, J. (2012). Acquiring grounded representation of words with situtated interactive instruction. Advances in Cognitive Systems, 2, 113-130.
Roy, D. (2002). Learning visually grounded words and syntax for a scene description task. Computer Speech \&amp; Language, 16(2), 353-385.
Scheutz, M., Eberhard, K., \&amp; Andronache, V. (2004). A real-time robotic model of human reference resolution using visual constraints. Connection Science, 16(3), 145-167. https://doi.org/10. 1080/09540090412331314803
Sleator, D., \&amp; Temperley, D. (1991). Parsing English with a link grammar (tech. rep.). Carnegie Mellon University. Pittsburgh, PA. http://arxiv.org/abs/cmp-lg/9508004
Tellex, S., Kollar, T., Dickerson, S., \&amp; Walter, M. (2011). Understanding natural language commands for robotic navigation and mobile manipulation. Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence.
Winograd, T. (1972). Understanding natural language. Cognitive Psychology, 3(1), 1-191.</p>            </div>
        </div>

    </div>
</body>
</html>
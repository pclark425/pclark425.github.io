<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1131 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1131</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1131</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-263831635</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.06306v2.pdf" target="_blank">Ensemble Active Learning by Contextual Bandits for AI Incubation in Manufacturing</a></p>
                <p><strong>Paper Abstract:</strong> It is challenging but important to save annotation efforts in streaming data acquisition to maintain data quality for supervised learning base learners. We propose an ensemble active learning method to actively acquire samples for annotation by contextual bandits, which is will enforce the exploration-exploitation balance and leading to improved AI modeling performance.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1131.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1131.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CbeAL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ensemble Active Learning by Contextual Bandits</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble active learning framework that adaptively combines explicitly exploration- and exploitation-oriented acquisition agents via a contextual bandits solver to select streaming samples for human annotation in manufacturing ICPS.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CbeAL (ensemble)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An ensemble that takes a set of active learning agents (exploration- and exploitation-oriented) as experts and forms a weighted majority acquisition decision; weights (decision powers) are updated online by a contextual adversarial bandits solver using context from the base learner (x_t, predicted label, class probabilities) and a reward signal based on whether the base learner would have predicted incorrectly.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>stream-based active learning + contextual multi-armed bandits (Exp4.P variant)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>At each streaming sample the ensemble computes each agent's acquisition probability; Exp4.P-EWMA combines agent advice into a final acquisition probability using agent weights that are updated by contextual reward r_t (positive if base learner would have been wrong). The solver monitors standardized weights with an EWMA chart and can flip weights briefly to force exploration if one agent dominates early.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Streaming manufacturing data (simulated Gaussian mixture clusters) and FDM process data (case study)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Non-stationary streaming data, multimodal / clusterwise input distributions (Gaussian mixtures assumed in analysis), highly imbalanced classes, sequentially correlated labels (case study), partially observable in the sense that labels are only revealed when queried (human-in-the-loop), noisy/disturbed labels in some scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>High-dimensional (case study: 519 features), large stream lengths (simulations: up to T=2000; case study: 1588 samples), binary classification action (acquire or skip) so action space size = 2; sliding-window context sizes up to L=150 used by agents.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>CbeAL-6 (recommended 6-agent ensemble) outperformed individual agents and benchmarks in most simulation scenarios: e.g., in simulations CbeAL-6 outperformed the best individual agent in 20/24 scenarios; representative result: sparsity=70%, ds=0%, n=1500 -> CbeAL-6 test accuracy 76.7% (±0.02) (Table 3). In the FDM case study CbeAL-6 improved testing accuracy with limited budgets and reportedly exceeded the accuracy of a model trained on all available samples when budget >10%. CbeAL variants also achieved highest cumulative reward in many scenarios (see Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baselines (uncertainty sampling, random sampling, DBALStream, QBC-PYP) achieved lower testing accuracy in most scenarios; example: same scenario above US = 72.4% (±0.04), DBALStream = 59.4% (±0.02), QBC-PYP = 66.1% (±0.04) (Table 3). Exact baseline values vary by scenario.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Designed to use a limited annotation budget B (typical experiments used B = 10% of stream). Empirically, 10% labeled stream often sufficed to reach near full-data performance in favorable scenarios (e.g., large n and high sparsity); in the FDM case study budgets {3,5,10,15,20}% were tested and CbeAL-6 showed continuous gains and exceeded full-data accuracy when budget >10%.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Tradeoff controlled by ensemble weights over explicit exploration agents (LD-Agent, SPF-Agent) and exploitation agents (RAL-Agent). Exp4.P-EWMA updates weights by reward feedback and uses an EWMA-based flipping mechanism to temporarily force under-weighted agents to explore, preventing early domination by exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Random Sampling (RS), Uncertainty Sampling (US), DBALStream, QBC-PYP, individual agents (LD, SPF, RAL) and variants (CbeAL-2, CbeAL-4, CbeAL-6 etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>An ensemble of explicitly designed exploration/exploitation agents combined via a contextual adversarial bandits solver adaptively balances exploration and exploitation across nonstationary streaming scenarios, improving classifier test accuracy under limited budgets (notably CbeAL-6). EWMA flipping prevents early convergence to a dominant agent and increases later-stage exploration. Reward defined as whether the base learner would have been wrong effectively promotes acquisition of useful samples.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>CbeAL may lose advantage when the task does not require strong exploration (i.e., when exploitation alone suffices); hyperparameters currently tuned by grid search; adding many homogeneous agents can hurt bandit learning; performance can degrade with label disturbance (some scenarios showed inferior performance under ds=3%); Exp4.P becomes less efficient when agent pool is too large.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Ensemble Active Learning by Contextual Bandits for AI Incubation in Manufacturing', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1131.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1131.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Exp4.P-EWMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Exp4.P with EWMA-based flipping mechanism</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A contextual adversarial bandits solver (based on Exp4.P) augmented with an EWMA control-chart flipping mechanism to prevent early dominance of a single expert and to force short-term exploration when weights drift.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Exp4.P-EWMA (bandits solver)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An adaptation of the Exp4.P adversarial contextual bandits algorithm where expert weights are updated per standard Exp4.P using importance-weighted rewards; additionally, each expert's standardized weight is tracked with an EWMA chart and when a weight drifts beyond control limits, standardized weights are flipped (2μ - α_s) for a short period to encourage alternative experts.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>contextual adversarial bandits (Exp4.P) with EWMA-triggered forced exploration</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Uses context (incoming sample x_t, predicted label, predicted class probabilities) and observed reward r_t to update expert weights; EWMA monitors weight drift and triggers a temporary flipping of standardized weights to prevent premature convergence and to promote exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same streaming classification environments (simulations and FDM case study)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Adversarial / nonstationary reward environment due to evolving base learner performance and changing stream distributions; feedback only available when a sample is acquired (partial observability of labels).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Contexts are high-dimensional (e.g., 519 features after feature extraction in case study), two-arm action space (acquire/skip); number of experts N varied (2,4,6,8) in experiments, solver must maintain and update N weights.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Empirically improved final test accuracy and avoided early collapse compared to vanilla Exp4.P; example reported: in one scenario Exp4.P-EWMA final testing accuracy = 0.704 vs Exp4.P = 0.64 (illustrative comparison in text/figures). CbeAL variants using Exp4.P-EWMA gained higher cumulative reward and better downstream classifier performance in many experiments (see Tables 2,7).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Vanilla Exp4.P tended to allow exploitation-oriented agents to dominate early, causing insufficient later exploration and lower final accuracy (example above). Exact numeric differences vary by scenario.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Improves effective use of limited labeling budgets by maintaining exploration opportunities; empirical results show better cumulative reward and downstream accuracy given identical budgets (e.g., B = 10% of stream).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Balances through weighted aggregation of expert advice with weights updated by reward; EWMA flipping periodically increases decision power of under-used experts to re-introduce exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Exp4.P (baseline), and other solvers implicitly via CbeAL comparisons to RS, US, DBALStream, QBC-PYP.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>EWMA-based flipping avoids early domination by exploitation experts, preserves exploration capacity and does not change Exp4.P regret bounds theoretically; empirically leads to higher cumulative reward and improved classifier accuracy in nonstationary streaming annotation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires tuning of EWMA parameters (λ, h, γ); flipping may only be beneficial for short periods and hyperparameter misconfiguration can increase variance of cumulative reward; maintaining many expert weights can reduce solver efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Ensemble Active Learning by Contextual Bandits for AI Incubation in Manufacturing', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1131.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1131.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LD-Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Low-density Based Exploration Agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An exploration-oriented acquisition agent that prefers samples in sparse/low-density regions, approximating local density via a sliding window and selecting samples that are farthest from recent observations to discover remote clusters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LD-Agent (low-density exploration)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Computes a local sparsity factor lsf(x_t) by counting how often x_t is the farthest point relative to samples in a sliding window W of length L using pairwise Euclidean distances; acquisition probability p_t = lsf(x_t) / (L * δ_L).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>density-based exploration (active learning / sequential design)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adapts acquisition probability based on local sparsity estimated over a sliding window of recent stream samples; thus favors samples that are remote from recent observations and so discovers new clusters over time.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Simulated Gaussian-mixture streaming datasets and FDM streaming data</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Clustered/multimodal input distributions (Gaussian mixtures), nonstationary streams where novel clusters may appear, partially observable labels available only when queried, possible label noise (disturbance).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Operates with sliding window length L (examples used L=100,150) in high-dimensional input (q up to 15 in theoretical analysis, 519 in case study after feature extraction).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Promotes exploration: theoretical analysis shows expected acquisition probability for remote-cluster samples increases with cluster-center separation; empirically LD-Agent acquires samples from multiple clusters and increases variance of labelled pool but, when used alone, often yields lower classification accuracy than exploitation-focused methods (reported in simulations and toy visualizations).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Tends to query more samples than pure exploitation agents (empirically acquires more queries in many scenarios; see Table 6), which increases coverage of input space but can be less label-efficient for boundary refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Pure exploration strategy by design; when ensembled in CbeAL its weight competes with exploitation agents and is adaptively down/up-weighted by the bandit solver based on reward.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared individually and within ensembles against SPF-Agent, RAL-Agent, RS, US, DBALStream, QBC-PYP.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>The LD-Agent is effective at discovering remote clusters and increasing labeled-sample variance; theoretical results (Theorem 1 / B.1) show acquisition probability for remote clusters grows with inter-cluster distance; in ensembles contributes to improved downstream accuracy when balanced with exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>When used alone it may select many low-uncertainty samples (not helpful for boundary learning) and thus produce lower classifier accuracy; early-stage pure exploration can be penalized by the bandit solver without EWMA flipping.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Ensemble Active Learning by Contextual Bandits for AI Incubation in Manufacturing', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1131.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1131.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SPF-Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Space-filling Based Exploration Agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An exploration-oriented agent that favors samples that increase coverage by maximizing minimum distance to recent sampled points (online space-filling / Kennard-Stone style criterion).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SPF-Agent (space-filling exploration)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Computes M inDist(x_t, W) = min_j d(x_t, x_j) to the sliding-window samples, and sets acquisition probability p_t proportional to this minimum distance normalized by the largest minimum distance among window points, encouraging acquisitions that fill sparse regions uniformly.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>space-filling sequential design (active learning)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adapts to incoming stream by computing distance to recent points in sliding window W and increasing acquisition probability for samples that are far from recent observations, thereby promoting uniform coverage over time.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Simulated Gaussian-mixture streams and FDM case study</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Multimodal/clustered distributions; partially observable labels; high-dimensional feature space in case study; nonstationary when clusters shift or new clusters appear.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Sliding window length L can be relatively small (e.g., L=60 used for one SPF in experiments) but input dimension may be large (p up to 15 in sims, 519 in case study after feature extraction).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>SPF-Agent yields more uniform exploration than LD-Agent (visualized in toy examples), acquires samples across clusters but may select fewer boundary samples; when ensembled (CbeAL-6) contributed to heterogeneous acquisition decisions that improved overall accuracy compared to homogeneous ensembles.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>More exploratory than exploitation agents; tends to use more of the annotation budget than pure exploitation methods but helps in scenarios where discovering clusters is necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Pure exploration objective; relied upon by ensemble to provide input-space discovery while exploitation agents focus on decision boundary refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against LD-Agent, RAL-Agent, and standard benchmarks (RS, US, DBALStream, QBC-PYP) both individually and within ensembles.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Space-filling exploration yields uniform coverage and complements density-based exploration; ensemble heterogeneity (including SPF) improves adaptive tradeoff and downstream classifier accuracy in many scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>When used alone tends to under-sample near decision boundaries; may be less effective for immediate boundary refinement compared to exploitation methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Ensemble Active Learning by Contextual Bandits for AI Incubation in Manufacturing', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1131.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1131.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAL-Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforced Exploitation Agent (RL-based)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An exploitation-oriented agent that adaptively learns a certainty threshold for uncertainty-based acquisition via a simple reinforcement update to focus queries near the decision boundary.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RAL-Agent (reinforced active learning)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Maintains a certainty threshold θ_t and acquires a sample if the base learner's prediction certainty ct(x_t)=max P_f(ŷ|x_t) is below θ_t; updates θ_t by a multiplicative rule θ_{t+1} = min(θ_t(1+η*(1-2 r_t/ρ_-)),1) using reward r_t when acquisition occurs—thus learning to adjust exploitation intensity.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>reinforcement learning for threshold adaptation (reinforced active learning)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adapts the certainty threshold θ based on acquisition rewards: positive reward slightly increases θ (become more selective) and penalties decrease θ (become less selective), so the agent self-tunes exploitation aggressiveness over time.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Simulated streaming classification and FDM case study</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Relies on base learner probabilities; environments may be imbalanced, multimodal, and partially observable (labels only when queried); base learner confidence can be misleading in cluster-limited initial training.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Works with base learners like logistic regression or SVM; operates in high-dimensional feature spaces and streaming contexts with limited budget.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>RAL-Agent is effective at exploitation in many scenarios and often competitive (e.g., opt.Exploit often high in some tables); however RAL-Agent can get stuck acquiring within one cluster if base learner overconfident about other regions (observed in toy and case study visualizations). Within ensembles, RAL contributes to strong boundary refinement when balanced with exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Non-adaptive uncertainty sampling (fixed threshold) is less adaptive to stream changes; modified RAL with ϵ-greedy (ϵ=0.01) improved competitiveness (applied in experiments). Exact numeric deltas vary by scenario.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Tends to be more sample-efficient for acquiring decision-boundary informative samples (acquires fewer samples than purely exploratory agents); empirical numbers show exploitation agents often used fewer queries (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>By design an exploitation specialist; in CbeAL its weight is adjusted by the bandit solver to trade off with exploration agents.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against uncertainty sampling, and used as one of the ensemble experts in CbeAL-2/4/6; compared to LD and SPF exploration agents.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Adaptive thresholding via reinforcement updates provides an exploitation policy that is responsive to historical usefulness (reward) and often achieves strong performance for boundary learning; however it can fail when initial base-learner confidence is misleading or when cluster discovery is required.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Can become trapped in a single cluster and repeatedly acquire locally ambiguous but globally unhelpful samples; requires an exploration mechanism (e.g., pairing with LD or SPF or adding ϵ-greedy) to avoid missing remote clusters.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Ensemble Active Learning by Contextual Bandits for AI Incubation in Manufacturing', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Ralf: A reinforced active learning formulation for object class recognition <em>(Rating: 2)</em></li>
                <li>Contextual bandit algorithms with supervised learning guarantees <em>(Rating: 2)</em></li>
                <li>High density-focused uncertainty sampling for active learning over evolving stream data <em>(Rating: 2)</em></li>
                <li>Stream-based joint exploration-exploitation active learning <em>(Rating: 2)</em></li>
                <li>On-line choice of active learning algorithms <em>(Rating: 1)</em></li>
                <li>The nonstochastic multiarmed bandit problem <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1131",
    "paper_id": "paper-263831635",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "CbeAL",
            "name_full": "Ensemble Active Learning by Contextual Bandits",
            "brief_description": "An ensemble active learning framework that adaptively combines explicitly exploration- and exploitation-oriented acquisition agents via a contextual bandits solver to select streaming samples for human annotation in manufacturing ICPS.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "CbeAL (ensemble)",
            "agent_description": "An ensemble that takes a set of active learning agents (exploration- and exploitation-oriented) as experts and forms a weighted majority acquisition decision; weights (decision powers) are updated online by a contextual adversarial bandits solver using context from the base learner (x_t, predicted label, class probabilities) and a reward signal based on whether the base learner would have predicted incorrectly.",
            "adaptive_design_method": "stream-based active learning + contextual multi-armed bandits (Exp4.P variant)",
            "adaptation_strategy_description": "At each streaming sample the ensemble computes each agent's acquisition probability; Exp4.P-EWMA combines agent advice into a final acquisition probability using agent weights that are updated by contextual reward r_t (positive if base learner would have been wrong). The solver monitors standardized weights with an EWMA chart and can flip weights briefly to force exploration if one agent dominates early.",
            "environment_name": "Streaming manufacturing data (simulated Gaussian mixture clusters) and FDM process data (case study)",
            "environment_characteristics": "Non-stationary streaming data, multimodal / clusterwise input distributions (Gaussian mixtures assumed in analysis), highly imbalanced classes, sequentially correlated labels (case study), partially observable in the sense that labels are only revealed when queried (human-in-the-loop), noisy/disturbed labels in some scenarios.",
            "environment_complexity": "High-dimensional (case study: 519 features), large stream lengths (simulations: up to T=2000; case study: 1588 samples), binary classification action (acquire or skip) so action space size = 2; sliding-window context sizes up to L=150 used by agents.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "CbeAL-6 (recommended 6-agent ensemble) outperformed individual agents and benchmarks in most simulation scenarios: e.g., in simulations CbeAL-6 outperformed the best individual agent in 20/24 scenarios; representative result: sparsity=70%, ds=0%, n=1500 -&gt; CbeAL-6 test accuracy 76.7% (±0.02) (Table 3). In the FDM case study CbeAL-6 improved testing accuracy with limited budgets and reportedly exceeded the accuracy of a model trained on all available samples when budget &gt;10%. CbeAL variants also achieved highest cumulative reward in many scenarios (see Table 7).",
            "performance_without_adaptation": "Baselines (uncertainty sampling, random sampling, DBALStream, QBC-PYP) achieved lower testing accuracy in most scenarios; example: same scenario above US = 72.4% (±0.04), DBALStream = 59.4% (±0.02), QBC-PYP = 66.1% (±0.04) (Table 3). Exact baseline values vary by scenario.",
            "sample_efficiency": "Designed to use a limited annotation budget B (typical experiments used B = 10% of stream). Empirically, 10% labeled stream often sufficed to reach near full-data performance in favorable scenarios (e.g., large n and high sparsity); in the FDM case study budgets {3,5,10,15,20}% were tested and CbeAL-6 showed continuous gains and exceeded full-data accuracy when budget &gt;10%.",
            "exploration_exploitation_tradeoff": "Tradeoff controlled by ensemble weights over explicit exploration agents (LD-Agent, SPF-Agent) and exploitation agents (RAL-Agent). Exp4.P-EWMA updates weights by reward feedback and uses an EWMA-based flipping mechanism to temporarily force under-weighted agents to explore, preventing early domination by exploitation.",
            "comparison_methods": "Random Sampling (RS), Uncertainty Sampling (US), DBALStream, QBC-PYP, individual agents (LD, SPF, RAL) and variants (CbeAL-2, CbeAL-4, CbeAL-6 etc.).",
            "key_results": "An ensemble of explicitly designed exploration/exploitation agents combined via a contextual adversarial bandits solver adaptively balances exploration and exploitation across nonstationary streaming scenarios, improving classifier test accuracy under limited budgets (notably CbeAL-6). EWMA flipping prevents early convergence to a dominant agent and increases later-stage exploration. Reward defined as whether the base learner would have been wrong effectively promotes acquisition of useful samples.",
            "limitations_or_failures": "CbeAL may lose advantage when the task does not require strong exploration (i.e., when exploitation alone suffices); hyperparameters currently tuned by grid search; adding many homogeneous agents can hurt bandit learning; performance can degrade with label disturbance (some scenarios showed inferior performance under ds=3%); Exp4.P becomes less efficient when agent pool is too large.",
            "uuid": "e1131.0",
            "source_info": {
                "paper_title": "Ensemble Active Learning by Contextual Bandits for AI Incubation in Manufacturing",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Exp4.P-EWMA",
            "name_full": "Exp4.P with EWMA-based flipping mechanism",
            "brief_description": "A contextual adversarial bandits solver (based on Exp4.P) augmented with an EWMA control-chart flipping mechanism to prevent early dominance of a single expert and to force short-term exploration when weights drift.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Exp4.P-EWMA (bandits solver)",
            "agent_description": "An adaptation of the Exp4.P adversarial contextual bandits algorithm where expert weights are updated per standard Exp4.P using importance-weighted rewards; additionally, each expert's standardized weight is tracked with an EWMA chart and when a weight drifts beyond control limits, standardized weights are flipped (2μ - α_s) for a short period to encourage alternative experts.",
            "adaptive_design_method": "contextual adversarial bandits (Exp4.P) with EWMA-triggered forced exploration",
            "adaptation_strategy_description": "Uses context (incoming sample x_t, predicted label, predicted class probabilities) and observed reward r_t to update expert weights; EWMA monitors weight drift and triggers a temporary flipping of standardized weights to prevent premature convergence and to promote exploration.",
            "environment_name": "Same streaming classification environments (simulations and FDM case study)",
            "environment_characteristics": "Adversarial / nonstationary reward environment due to evolving base learner performance and changing stream distributions; feedback only available when a sample is acquired (partial observability of labels).",
            "environment_complexity": "Contexts are high-dimensional (e.g., 519 features after feature extraction in case study), two-arm action space (acquire/skip); number of experts N varied (2,4,6,8) in experiments, solver must maintain and update N weights.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Empirically improved final test accuracy and avoided early collapse compared to vanilla Exp4.P; example reported: in one scenario Exp4.P-EWMA final testing accuracy = 0.704 vs Exp4.P = 0.64 (illustrative comparison in text/figures). CbeAL variants using Exp4.P-EWMA gained higher cumulative reward and better downstream classifier performance in many experiments (see Tables 2,7).",
            "performance_without_adaptation": "Vanilla Exp4.P tended to allow exploitation-oriented agents to dominate early, causing insufficient later exploration and lower final accuracy (example above). Exact numeric differences vary by scenario.",
            "sample_efficiency": "Improves effective use of limited labeling budgets by maintaining exploration opportunities; empirical results show better cumulative reward and downstream accuracy given identical budgets (e.g., B = 10% of stream).",
            "exploration_exploitation_tradeoff": "Balances through weighted aggregation of expert advice with weights updated by reward; EWMA flipping periodically increases decision power of under-used experts to re-introduce exploration.",
            "comparison_methods": "Exp4.P (baseline), and other solvers implicitly via CbeAL comparisons to RS, US, DBALStream, QBC-PYP.",
            "key_results": "EWMA-based flipping avoids early domination by exploitation experts, preserves exploration capacity and does not change Exp4.P regret bounds theoretically; empirically leads to higher cumulative reward and improved classifier accuracy in nonstationary streaming annotation tasks.",
            "limitations_or_failures": "Requires tuning of EWMA parameters (λ, h, γ); flipping may only be beneficial for short periods and hyperparameter misconfiguration can increase variance of cumulative reward; maintaining many expert weights can reduce solver efficiency.",
            "uuid": "e1131.1",
            "source_info": {
                "paper_title": "Ensemble Active Learning by Contextual Bandits for AI Incubation in Manufacturing",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LD-Agent",
            "name_full": "Low-density Based Exploration Agent",
            "brief_description": "An exploration-oriented acquisition agent that prefers samples in sparse/low-density regions, approximating local density via a sliding window and selecting samples that are farthest from recent observations to discover remote clusters.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LD-Agent (low-density exploration)",
            "agent_description": "Computes a local sparsity factor lsf(x_t) by counting how often x_t is the farthest point relative to samples in a sliding window W of length L using pairwise Euclidean distances; acquisition probability p_t = lsf(x_t) / (L * δ_L).",
            "adaptive_design_method": "density-based exploration (active learning / sequential design)",
            "adaptation_strategy_description": "Adapts acquisition probability based on local sparsity estimated over a sliding window of recent stream samples; thus favors samples that are remote from recent observations and so discovers new clusters over time.",
            "environment_name": "Simulated Gaussian-mixture streaming datasets and FDM streaming data",
            "environment_characteristics": "Clustered/multimodal input distributions (Gaussian mixtures), nonstationary streams where novel clusters may appear, partially observable labels available only when queried, possible label noise (disturbance).",
            "environment_complexity": "Operates with sliding window length L (examples used L=100,150) in high-dimensional input (q up to 15 in theoretical analysis, 519 in case study after feature extraction).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Promotes exploration: theoretical analysis shows expected acquisition probability for remote-cluster samples increases with cluster-center separation; empirically LD-Agent acquires samples from multiple clusters and increases variance of labelled pool but, when used alone, often yields lower classification accuracy than exploitation-focused methods (reported in simulations and toy visualizations).",
            "performance_without_adaptation": null,
            "sample_efficiency": "Tends to query more samples than pure exploitation agents (empirically acquires more queries in many scenarios; see Table 6), which increases coverage of input space but can be less label-efficient for boundary refinement.",
            "exploration_exploitation_tradeoff": "Pure exploration strategy by design; when ensembled in CbeAL its weight competes with exploitation agents and is adaptively down/up-weighted by the bandit solver based on reward.",
            "comparison_methods": "Compared individually and within ensembles against SPF-Agent, RAL-Agent, RS, US, DBALStream, QBC-PYP.",
            "key_results": "The LD-Agent is effective at discovering remote clusters and increasing labeled-sample variance; theoretical results (Theorem 1 / B.1) show acquisition probability for remote clusters grows with inter-cluster distance; in ensembles contributes to improved downstream accuracy when balanced with exploitation.",
            "limitations_or_failures": "When used alone it may select many low-uncertainty samples (not helpful for boundary learning) and thus produce lower classifier accuracy; early-stage pure exploration can be penalized by the bandit solver without EWMA flipping.",
            "uuid": "e1131.2",
            "source_info": {
                "paper_title": "Ensemble Active Learning by Contextual Bandits for AI Incubation in Manufacturing",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "SPF-Agent",
            "name_full": "Space-filling Based Exploration Agent",
            "brief_description": "An exploration-oriented agent that favors samples that increase coverage by maximizing minimum distance to recent sampled points (online space-filling / Kennard-Stone style criterion).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "SPF-Agent (space-filling exploration)",
            "agent_description": "Computes M inDist(x_t, W) = min_j d(x_t, x_j) to the sliding-window samples, and sets acquisition probability p_t proportional to this minimum distance normalized by the largest minimum distance among window points, encouraging acquisitions that fill sparse regions uniformly.",
            "adaptive_design_method": "space-filling sequential design (active learning)",
            "adaptation_strategy_description": "Adapts to incoming stream by computing distance to recent points in sliding window W and increasing acquisition probability for samples that are far from recent observations, thereby promoting uniform coverage over time.",
            "environment_name": "Simulated Gaussian-mixture streams and FDM case study",
            "environment_characteristics": "Multimodal/clustered distributions; partially observable labels; high-dimensional feature space in case study; nonstationary when clusters shift or new clusters appear.",
            "environment_complexity": "Sliding window length L can be relatively small (e.g., L=60 used for one SPF in experiments) but input dimension may be large (p up to 15 in sims, 519 in case study after feature extraction).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "SPF-Agent yields more uniform exploration than LD-Agent (visualized in toy examples), acquires samples across clusters but may select fewer boundary samples; when ensembled (CbeAL-6) contributed to heterogeneous acquisition decisions that improved overall accuracy compared to homogeneous ensembles.",
            "performance_without_adaptation": null,
            "sample_efficiency": "More exploratory than exploitation agents; tends to use more of the annotation budget than pure exploitation methods but helps in scenarios where discovering clusters is necessary.",
            "exploration_exploitation_tradeoff": "Pure exploration objective; relied upon by ensemble to provide input-space discovery while exploitation agents focus on decision boundary refinement.",
            "comparison_methods": "Compared against LD-Agent, RAL-Agent, and standard benchmarks (RS, US, DBALStream, QBC-PYP) both individually and within ensembles.",
            "key_results": "Space-filling exploration yields uniform coverage and complements density-based exploration; ensemble heterogeneity (including SPF) improves adaptive tradeoff and downstream classifier accuracy in many scenarios.",
            "limitations_or_failures": "When used alone tends to under-sample near decision boundaries; may be less effective for immediate boundary refinement compared to exploitation methods.",
            "uuid": "e1131.3",
            "source_info": {
                "paper_title": "Ensemble Active Learning by Contextual Bandits for AI Incubation in Manufacturing",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "RAL-Agent",
            "name_full": "Reinforced Exploitation Agent (RL-based)",
            "brief_description": "An exploitation-oriented agent that adaptively learns a certainty threshold for uncertainty-based acquisition via a simple reinforcement update to focus queries near the decision boundary.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "RAL-Agent (reinforced active learning)",
            "agent_description": "Maintains a certainty threshold θ_t and acquires a sample if the base learner's prediction certainty ct(x_t)=max P_f(ŷ|x_t) is below θ_t; updates θ_t by a multiplicative rule θ_{t+1} = min(θ_t(1+η*(1-2 r_t/ρ_-)),1) using reward r_t when acquisition occurs—thus learning to adjust exploitation intensity.",
            "adaptive_design_method": "reinforcement learning for threshold adaptation (reinforced active learning)",
            "adaptation_strategy_description": "Adapts the certainty threshold θ based on acquisition rewards: positive reward slightly increases θ (become more selective) and penalties decrease θ (become less selective), so the agent self-tunes exploitation aggressiveness over time.",
            "environment_name": "Simulated streaming classification and FDM case study",
            "environment_characteristics": "Relies on base learner probabilities; environments may be imbalanced, multimodal, and partially observable (labels only when queried); base learner confidence can be misleading in cluster-limited initial training.",
            "environment_complexity": "Works with base learners like logistic regression or SVM; operates in high-dimensional feature spaces and streaming contexts with limited budget.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "RAL-Agent is effective at exploitation in many scenarios and often competitive (e.g., opt.Exploit often high in some tables); however RAL-Agent can get stuck acquiring within one cluster if base learner overconfident about other regions (observed in toy and case study visualizations). Within ensembles, RAL contributes to strong boundary refinement when balanced with exploration.",
            "performance_without_adaptation": "Non-adaptive uncertainty sampling (fixed threshold) is less adaptive to stream changes; modified RAL with ϵ-greedy (ϵ=0.01) improved competitiveness (applied in experiments). Exact numeric deltas vary by scenario.",
            "sample_efficiency": "Tends to be more sample-efficient for acquiring decision-boundary informative samples (acquires fewer samples than purely exploratory agents); empirical numbers show exploitation agents often used fewer queries (Table 6).",
            "exploration_exploitation_tradeoff": "By design an exploitation specialist; in CbeAL its weight is adjusted by the bandit solver to trade off with exploration agents.",
            "comparison_methods": "Compared against uncertainty sampling, and used as one of the ensemble experts in CbeAL-2/4/6; compared to LD and SPF exploration agents.",
            "key_results": "Adaptive thresholding via reinforcement updates provides an exploitation policy that is responsive to historical usefulness (reward) and often achieves strong performance for boundary learning; however it can fail when initial base-learner confidence is misleading or when cluster discovery is required.",
            "limitations_or_failures": "Can become trapped in a single cluster and repeatedly acquire locally ambiguous but globally unhelpful samples; requires an exploration mechanism (e.g., pairing with LD or SPF or adding ϵ-greedy) to avoid missing remote clusters.",
            "uuid": "e1131.4",
            "source_info": {
                "paper_title": "Ensemble Active Learning by Contextual Bandits for AI Incubation in Manufacturing",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Ralf: A reinforced active learning formulation for object class recognition",
            "rating": 2,
            "sanitized_title": "ralf_a_reinforced_active_learning_formulation_for_object_class_recognition"
        },
        {
            "paper_title": "Contextual bandit algorithms with supervised learning guarantees",
            "rating": 2,
            "sanitized_title": "contextual_bandit_algorithms_with_supervised_learning_guarantees"
        },
        {
            "paper_title": "High density-focused uncertainty sampling for active learning over evolving stream data",
            "rating": 2,
            "sanitized_title": "high_densityfocused_uncertainty_sampling_for_active_learning_over_evolving_stream_data"
        },
        {
            "paper_title": "Stream-based joint exploration-exploitation active learning",
            "rating": 2,
            "sanitized_title": "streambased_joint_explorationexploitation_active_learning"
        },
        {
            "paper_title": "On-line choice of active learning algorithms",
            "rating": 1,
            "sanitized_title": "online_choice_of_active_learning_algorithms"
        },
        {
            "paper_title": "The nonstochastic multiarmed bandit problem",
            "rating": 1,
            "sanitized_title": "the_nonstochastic_multiarmed_bandit_problem"
        }
    ],
    "cost": 0.0199195,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Ensemble Active Learning by Contextual Bandits for AI Incubation in Manufacturing
October 12, 2023</p>
<p>Yingyan Zeng 
University of Buffalo</p>
<p>Xiaoyu Chen 
University of Buffalo</p>
<p>Ran Jin 
University of Buffalo</p>
<p>Virginia Tech 
University of Buffalo</p>
<p>Ensemble Active Learning by Contextual Bandits for AI Incubation in Manufacturing
October 12, 2023DBE5590A76D375712ACCF5F339E8F14CarXiv:2310.06306v2[cs.LG]
An Industrial Cyber-physical System (ICPS) provide a digital foundation for data-driven decision-making by artificial intelligence (AI) models.However, the poor data quality (e.g., inconsistent distribution, imbalanced classes) of high-speed, large-volume data streams poses significant challenges to the online deployment of offline-trained AI models.As an alternative, updating AI models online based on streaming data enables continuous improvement and resilient modeling performance.However, for a supervised learning model (i.e., a base learner), it is labor-intensive to annotate all streaming samples to update the model.Hence, a data acquisition method is needed to select the data for annotation to ensure data quality while saving annotation efforts.In the literature, active learning methods have been proposed to acquire informative samples.Different acquisition criteria were developed for exploration of under-represented regions in the input variable space or exploitation of the well-represented regions for optimal estimation of base learners.However, it remains a challenge to balance the exploration-exploitation trade-off under different online annotation scenarios.On the other hand, an acquisition criterion learned by AI adapts itself to a scenario dynamically, but the ambiguous consideration of the tradeoff limits its performance in frequently changing manufacturing contexts.To overcome these limitations, we propose an ensemble active learning method by contextual bandits (CbeAL).CbeAL incorporates a set of active learning agents (i.e., acquisition criteria) explicitly designed for exploration or exploitation by a weighted combination of their acquisition decisions.The weight of each agent will be dynamically adjusted based on the usefulness of its decisions to improve the performance of the base learner.With adaptive and explicit consideration of both objectives, CbeAL efficiently guides the data acquisition process by selecting informative samples to reduce the human annotation efforts.Furthermore, we characterize the exploration and exploitation capability of the proposed agents theoretically.The evaluation results in a numerical simulation study and a real case study demonstrate the effectiveness and efficiency of CbeAL in the manufacturing process modeling of the ICPS.</p>
<p>Introduction</p>
<p>Industrial Cyber-physical Systems (ICPSs) integrate the cyber and physical worlds, which serve as the backbone of the Fourth Industrial Revolution [Colombo et al., 2017].By embracing the Internet of Things (IoT), an ICPS interconnects manufacturing equipment with ubiquitous sensors, actuators, and computing units, forming a low-cost, high-availability, and high-accessibility network [Zhang et al., 2019].The high-speed and large-volume sensing data collected from such a network have advanced many data-driven decision-making methods to support manufacturing efficiency, quality improvement, and cost reduction.For example, artificial intelligence (AI) models such as support vector machine (SVM) and deep neural networks have been employed for quality modeling and process monitoring of fused deposition modeling (FDM) processes [Gobert et al., 2018, Wang et al., 2021a], Aerosol® Jet Printing processes [Sun et al., 2017], etc.However, most AI models are proposed following an offline-training-online-deployment (OTOD) strategy, which is only effective when the quality of the training data set is guaranteed (e.g., training data can provide adequate estimations of the underlying true model of the variable relationships in supervised learning).In practice, various factors can change such an underlying model in manufacturing processes (e.g., the degradation of manufacturing equipment or change of product design), which results in an erratic performance of AI models during their online deployment.</p>
<p>To improve the modeling performance, one can either build a dynamic model [Jin et al., 2019] or investigate an online model training mechanism to adapt existing models to online data streams.However, constructing a dynamic model highly depends on the prior knowledge of the distribution changing patterns and root causes.Instead of focusing on creating a better model, data-centric AI has been proposed as a more general approach to engineer the data needed to successfully build an AI model [Strickland, 2022].From a holistic view, we envision a resilient AI system to identify and mitigate the performance fluctuation of AI models caused by abrupt changes (i.e., data distribution, learning algorithms, and computational resources), which jointly consider managing the data quality as well as adapting the existing models during the online deployment.Therefore, in this paper we focus on online model training by actively acquiring samples to ensure data quality such that the resilient AI performance can be achieved.</p>
<p>Here, we focus on the supervised learning model as the base learner, where the data quality is considered from the aspect of representativeness (i.e., consistent distribution between the training and testing data sets) and class imbalance [Gupta et al., 2021].In the offline-training step, the high-quality data set can be defined when there are sufficient representative samples for training, such as sufficient samples for multimodal distributions [He and Wang, 2007], a distribution of the training samples close to that of the testing samples, and when there are balanced class distributions [Branco et al., 2016].However, OTOD cannot support the AI modeling since in the context of high-speed, large-volume streaming data, the data quality of training data sets needs to be evaluated continuously.As a motivation example, Fig. 1 demonstrates the multimodal distribution and the imbalanced class of samples collected from a highly personalized FDM process, where the samples are projected to the principle component directions of the input variable space by Principle Component Analysis (PCA).The two clusters are generated from two layers in FDM due to different product geometric designs.The objective of the data analysis in Fig. 1 is to use in situ process variables to predict the layer-to-layer binary quality variable, which indicates the surface roughness as a classification problem.Assume that the samples collected until time t will be used to train the model, where the samples from only one cluster have been observed.After time t + 1, the streaming samples are from the other cluster, i.e., the FDM goes to the next layer with another design in that slice.The collected training data set is not representative due to the shift of distributions, thus resulting in a sudden decrease of the prediction performance of the pre-trained model.Motivated by Fig. 1, it is important to actively select the streaming samples for annotation which ensures data quality.In earlier studies, Design of Experiments (DoE) was proposed to improve the supervised learning models by generating samples to identify significant variables [Fisher, 1936, Jin andDeng, 2015].Recent efforts in data filtering, either model-based [Li et al., 2021] or model-free methods [Trost, 1986, Liberty et al., 2016] aim at accurately modeling the underlying system by sampling a subset with good representativeness of the population distribution.However, DoE focus on actively generating the data while data filtering methods require a completely collected data set before the selection.Neither method can be directly applied to acquire the streaming data in the ICPS.</p>
<p>To obtain high-quality data, humans also play an indispensable role in the data annotation with their domain knowledge.In particular, the online data annotation requires real-time experimentation and a human-machine interface for domain experts to interact with [Tu et al., 2020], which is time-consuming and labor-intensive.While automatic annotation methods employ semi-supervised learning methods to annotate the samples by the most confident predictions [Kostopoulos et al., 2018], one potential disadvantage is the deteriorating performance of AI models caused by mislabelled cases [Carlini, 2021].</p>
<p>Recognizing the significance of human-in-the-loop, we proposed an AI incubation framework [Chen et al., 2022], designed to foster interaction between domain experts and AI systems throughout the training and deployment phases.This framework facilitates AI model development through human-AI collaboration, including model structure inspired by human decision-making processes [Chen et al., 2022], and feature generation based on human visual searching patterns [Chen, 2021].</p>
<p>In this work, we further explore the aspect of training data selection and annotation within AI incubation.Here, the human annotator acts as an 'incubator', labeling the acquired samples for training AI models.Our aim is to reduce human efforts while efficiently improving the learning performance of the base learner.</p>
<p>To create an online data acquisition method, an acquisition criterion needs to be designed to determine whether a sample should be selected for human annotation in order to acquire only informative samples and to provide high-quality training data sets for the base learner.This acquisition decision can be viewed as a dilemma between exploration and exploitation of the input variable space [Bondu et al., 2010].Here, exploitation is defined as acquiring a sample around the conceptual boundary for boundary learning, whereas exploration is defined as acquiring a sample located in the under-represented region for the input variable space discovery.Exploitation-oriented criteria work well when the base learner can easily detect the important regions [Osugi et al., 2005].Otherwise, exploration is required for more complex scenarios such as the exclusive XOR problem.For the scenario of motivation example shown in Fig. 1, if we concentrate exclusively on the samples near the decision boundary for exploitation, samples from another cluster may be overlooked throughout the streaming process, leading to an inaccurate estimate of the decision boundary and poor performance in under-represented regions.On the other hand, exclusive exploration will easily lead to a base learner with high uncertainty.Therefore, a well-balanced exploration-exploitation trade-off is essential for guiding the online annotation of AI models with complex distribution in the input variable space [Loy et al., 2012].</p>
<p>In this paper, we propose an ensemble active learning method by contextual bandits (CbeAL) to improve the exploration and exploitation trade-off under various scenarios, thus guiding an efficient and effective human annotation process.In CbeAL, a set of active learning agents with human-designed criteria is incorporated by contextual bandits [Beygelzimer et al., 2011], where a joint acquisition decision is made by the weighted combination of individual decisions.Here, we use "agents" as an umbrella term to refer the acquisition criteria in active learning methods.</p>
<p>The candidate active learning agents incorporated are designed to pursue an explicit objective of exploration or exploitation with theoretical justification, respectively.Thus, during the annotation process, the weight (i.e., decision power) of each agent indicates the current tendency for exploration or exploitation, which will be updated dynamically by the bandits solver subject to the historical reward.To improve the learning performance of the base learner, the reward is defined as the usefulness of the acquisition behaviour of CbeAL where acquiring a sample which would be wrongly predicted by the base learner is considered useful.Therefore, the online data acquisition problem can be effectively addressed by CbeAL, which pursues the exploration and exploitation trade-off through the ensemble of a set of active learning agents.In this sense, CbeAL is a generic active learning framework that reduces manual adjustment of active learning agents under frequently changed manufacturing data distributions.In addition to improving learning performance, CbeAL also increases the interpretability of AI models from the data perspective [Zhao et al., 2019], as the weight of each agent in each acquisition step explains whether the sample is annotated for exploration or exploitation.</p>
<p>The remainder of this paper is organized as follows.Section 2 summarizes the related work.Section 3 introduces the proposed CbeAL method and provides the theoretical justification.Section 4 evaluates the performance of CbeAL by simulation studies.Section 5 validates CbeAL via a real case study of online quality modeling of FDM in the ICPS.We conclude this work with some discussion of future work in Section 6.</p>
<p>2 Related Work</p>
<p>Online Model Updating in Industrial Cyber-physical Systems</p>
<p>In the past decades, the ICPS has integrated physical manufacturing equipment with sensing and actuation networks as well as ubiquitous computational resources, which provides the digital foundation for the online updating of AI models [Rao et al., 2015, Wang et al., 2020].With the streaming observational data and online computational resources, the online updating techniques of AI models have been investigated to enable the close modeling of manufacturing processes and facilitates the efficient decision-making in ICPSs.For example, Bastani et al. [2016] proposed an online classification model for real-time monitoring in additive and semiconducting manufacturing processes.Wang et al. [2016] developed a large-scale online multitask learning model to coordinate machine actions in the ICPS online.Online model updating strategies have also been developed for model calibration and predictive maintenance [Li et al., 2018, Xia et al., 2018].However, the aforementioned studies focus on developing the online updating algorithm of the AI model via Bayesian methods or distributed optimization methods to reduce the computational burden with large-volume streaming data, which are effective for unsupervised learning problems or supervised learning scenarios with easily collected responses.Yet for many supervised learning scenarios in the ICPS, the passively collected data need to be annotated via real-time experimentation by domain experts, e.g., the inspection of a batch of 400 wafers may take more than 8 hours [Jin et al., 2012].The lack of consideration of human annotation efforts renders these online updating methods inefficient for supervised AI models, especially in highly personalized manufacturing environments with rapid product and process changes [Alexopoulos et al., 2020].</p>
<p>Data Quality and Data Acquisition Methods</p>
<p>Compared to the accuracy and efficiency of learning algorithms, validating and monitoring the quality of data fed to AI models is an equally important problem [Caveness et al., 2020].Metrics for assessing the data quality for classification tasks include outlier detection, boundary complexity, label noise, shifting distribution, class imbalance, etc [Gupta et al., 2021].In the context of streaming data, Caveness et al. [2020] developed a data analysis and validation system to monitor significant changes between successive batches of the training data by summary statistics (i.e., mean, variance, etc.) with human investigation for a machine learning pipeline.However, without considering the informativeness of the data related to the AI model, the data collected by such a system cannot effectively improve the modeling performance.</p>
<p>On the other hand, to improve the performance of supervised learning models and reduce human annotation labor efforts, methods have been developed to facilitate effective data acquisition for high-quality informative data.These methods include providing acquisition recommendations for human annotation (e.g., sequential design and active learning) and automatic annotation (e.g., semi-supervised learning) [Diete et al., 2017], where limited approach suits the online streaming data.Sequential design focuses on selecting the samples in a sequential manner to achieve certain optimality criteria such as maximum entropy, maxmin distance [Lam, 2008, Stinstra et al., 2003].</p>
<p>For example, Yan et al. [2020] proposed an adaptive sequential sampling method to balance sampling efforts between the exploration and exploitation of anomalous regions for anomaly detection in the ICPS.However, these methods provide active recommendations that require experiments to be conducted at selected points in the input variable space, which cannot address passively collected data.Semi-supervised learning has been employed to automate the annotating process such that the base learner can learn from both labeled and unlabeled data.[Zhu and Goldberg, 2009].However, adding mislabelled cases by a semi-supervised learner to the training set may hamper the base learner's learning performance [Carlini, 2021].Due to the aforementioned limitations, we focus on active learning methods which provide acquisition recommendations for the passively collected data.Active learning has been leveraged for minimizing the human effort as well as improving the modeling performance in various applications including human activity recognition [Adaimi and Thomaz, 2019], threatening surveillance event detection [Loy et al., 2010], wearable sensing platforms annotation [Solis et al., 2019], etc.</p>
<p>Exploration and Exploitation in Active Learning</p>
<p>Active learning reduces annotation efforts for supervised learning models by evaluating the informativeness of samples and acquiring the most informative ones [Settles, 2012].The decision on whether one sample should be labelled can be viewed as a dilemma between the exploration and exploitation of the input variable space.In the earlier work, most efforts exploit samples with large amount of information about the base learner as an exploitation-oriented strategy.Metrics such as classification uncertainty [Lewis and Gale, 1994], margin [Balcan et al., 2007], and entropy [Fu et al., 2013] of the base learner have been adopted to measure the informativeness and compared with corresponding thresholds to make the acquisition decision.To achieve exploration for online streaming data, Ienco et al. [2014] modeled the local density of an sample to acquire the sample lying in a dense region with a small classification margin.For trivial scenarios where only parts of the input variable space have to be known in order to perform optimally, exploitation-oriented acquisition criteria can be more effective to avoid exploring regions that are irrelevant for the decision boundary estimation [Thrun, 1995].However, in nontrivial scenarios, exploration is crucial to uncover relevant, unknown regions when the base learner's estimation of the decision boundary is imprecise.Thus, the exploration-exploitation trade-off becomes vital under scenarios with exclusive XOR problem, clusterwise structure, imbalanced class distribution, etc [Osugi et al., 2005, Loy et al., 2012].Relying solely on either exploration or exploitation falls short in achieving optimal learning outcomes due to incompatibility with varied online annotation scenarios.To achieve a compromise, a common acquisition strategy is to conduct exploration and exploitation simultaneously.Considering two acquisition criteria which are dedicated to exploration (e.g., random sampling) and exploitation (e.g., uncertainty sampling) respectively, the compromise can be achieved by selecting one criterion with a certain probability for each streaming sample.One typical example is the ϵ-greedy policy which enforces the input variable space exploration with probability ϵ in each round [Thrun, 1995].</p>
<p>Representative sampling methods have also been designed as the combination of exploitation-oriented and exploration-oriented criteria [Wang and Hua, 2011].However, the ambiguity of the weight of each objective requires further fine-tuning for each learning scenario.To avoid ambiguity, Loy et al. [2012] extended the Query-by-Committee (QBC) [Seung et al., 1992] paradigm to a nonparametric Bayesian model to address unknown class discovery and imbalanced class distribution for the online annotation.However, without taking into account the informativeness (i.e., uncertainty) of a sample about the base learner, the proposed QBC-PYP cannot adjust the exploration-exploitation to the learning performance of the base learner.In brief, the simple combination of active learning criterion cannot handle challenging online data acquisition scenarios even with fine-tuning, which is due to the lack of compatibility with the data stream and the lack of adaptiveness to the learning performance of the base learner [Elreedy et al., 2019].Therefore, an ensemble of multiple criteria is desired to guide the dynamic exploration-exploitation trade-off in an adaptive and data-dependent manner.</p>
<p>As a promising approach, active learning has been recently formulated in the framework of reinforcement learning (RL) and multi-armed bandits where the objective is to learn the optimal acquisition criterion as a policy to maximize the cumulative reward [Ebert et al., 2012].However, these methods can also lack systematic and explicit considerations for both exploration and exploitation objectives.Wassermann et al. [2019] proposed Reinforced Active Learning (RAL) which modeled the stream-based active learning as a contextual bandits problem.In RAL, a set of base learners was gathered as the committee to provide acquisition advice based on the certainty degree of the sample to each learner.The acquisition criterion can be viewed as the weighted combination of different uncertainty sampling policies.In spite of its adaptiveness to the data stream, RAL is highly exploitation-oriented since it mainly focuses on decision boundary learning for each learner.Baram et al. [2004] first proposed COMB to blend multiple acquisition criteria as experts and consider samples as arms in multi-armed bandits.Later, Hsu and Lin [2015] refined COMB with ALBL using the bandits analogy, treating acquisition criteria as experts.While the bandits framework allows for dynamic adjustment of the exploration-exploitation trade-off, neither method provided guidance on selecting criteria nor addressed the explicit objectives of exploration and exploitation.</p>
<p>A random selection of general acquisition criteria with a small size may not well address different online annotation scenarios, while a large size of experts may cause problematic performance of the bandits solver.</p>
<p>Methodology</p>
<p>To develop the active learning agents for CbeAL and derive the theoretical characterization of the agents, we make the following assumptions: (i) The sample size of the initial training set D 0 is not large enough to guarantee satisfactory modeling performance and the samples in D 0 are not uniformly distributed in the input variable space.(ii) The streaming data have highly imbalanced class distribution.(iii) There are multiple clusters in the input data distribution.One common example is that the input data follow a Gaussian mixture distribution.This assumption is validated by the simulation setup and validated in the case study.Note that the proposed CbeAL framework is designed for general online annotation scenarios and does not require the assumptions on the input data distribution.</p>
<p>Overview of the Proposed Methodology</p>
<p>Consider the online data annotation scenario with a sample x t collected at time t, t = 1, 2, ..., T , where x t ∈ R p is the input for the base learner (i.e., the classification model) f t .We assume that the classification problem has c classes, and y t ∈ C = {1, 2, . . ., c} is the label of the sample x t .Denote the labelled data pool at time t as D t = {(x 1 , y 1 ), ..., (x nt , y nt )} with |D t | = n t .The base learner f 0 is pretrained by an initial D 0 , which contains a limited number of labelled samples.Under the aforementioned setting, we propose an active learning strategy to make the acquisition decision [Wang and Zhai, 2016].The strategy is applied with (i) a data source from which one unlabelled sample x t streams at each time stamp without a cost, (ii) a labelled data pool D t , (iii) human annotators who can provide the label of a sample x t if an acquisition decision is made to acquire it, (iv) a proposed ensemble acquisition method, and (v) the base learner f t to be updated online with the lableled data set D t .We have a budget of B samples for annotation during the streaming process.</p>
<p>As an overview (Fig. 2), our key idea is to ensemble the acquisition decisions made by the explorationand exploitation-oriented agents and adaptively balance the two aspects based on the context of incoming samples, the learning performance of the base learner, and the historical performance of the agents.During the online annotation process, at each time point t, (i) we receive a sample x t .(ii) Afterwards, one can obtain the predicted label ŷt and the side information, such as the predicted probability P f (ŷ t |D t ) of each class from f t and take x t and other information as the context input for the proposed contextual bandits solver Exp4.P-EWMA.And (iii) we make the acquisition decision as a weighted majority of the decisions obtained from the set of candidate agents {AG1, AG2, ...}.If the decision is to acquire the sample, we acquire y t from human annotation and obtain the reward r t , update CbeAL with r t , and retrain the classifier with (x t , y t ); otherwise, we pass this sample without annotation.The advantage of the proposed framework lies in three aspects: (i) CbeAL explicitly pursues the input variable space discovery and the decision boundary learning via incorporating exploration-and exploitation-oriented agents while it balances the overall exploration-exploitation trade-off adaptive to the data stream and the learning performance of the base learner by contextual bandits.(ii) The systematic ensemble of multiple pairs of agents save the efforts for agents selection under various learning scenarios with different input data distribution, feature dimension, signal-to-noise ratio, etc.Therefore, CbeAL is a developed as a generic active</p>
<p>The Ensemble Active Learning by Contextual Bandits</p>
<p>During the online annotation process, the needs for exploration and exploitation change over time, which depend on the observed samples, the incoming sample, and the updated learning performance of the base learner.Since there does not exist a consistent optimization criterion to adjust the trade-off, the shift between the two aspects is nontrivial.</p>
<p>To address the challenge of achieving a good exploration-exploitation trade-off adaptive to various online annotation scenarios, we formulate the shift between two objectives as a contextual multiarmed bandits problem.In the bandits problem, the bandits solver needs to make the decision of pulling one of the K arms as the action for time point t based on the received contextual information.</p>
<p>With each arm characterized by an unknown reward distribution, the objective of the solver is to gain the highest cumulative reward R = ∞ t=1 r t .Under the framework of CbeAL, we consider the decision to acquire or not acquire a sample as the arms and propose to ensemble explorationand exploitation-oriented agents as candidate policies (i.e., experts).The acquisition decision (i.e., decision of pulling one arm) is jointly made by a weighted combination of the individual decisions from each agent.Thus, the adjustment of exploration-exploitation trade-off is converted to the selection among different types of agent with the goal of gaining a higher reward, which can be solved by some well-developed bandits solvers.</p>
<p>In CbeAL, the arm pulled at time t (i.e., a t ∈ A = {1, 2}, |A| = K = 2) represents the overall acquisition decision, where a t = 1 refers to acquiring the sample x t and otherwise a t = 2.At each time point t, the incoming sample x t , prediction ŷt , and the predicted probability P f (ŷ|x t ) ∈ R c obtained by the base learner f t are considered as the observed contextual information that affects the exploration-exploitation trade-off.Each incorporated active learning agent (AG i , i ∈ {1, ..., N }) makes its own decision ξ i t ∈ R K based on these contextual information {x t , ŷt , P f (ŷ|x t )}.Here ξ i a,t represents the probability of the i-th agent taking the a-th action.Specifically, we have the decision vector
ξ i t = [p i t , 1 − p i t ]
, where p i t is the acquisition probability for sample x t .Simultaneously, each agent is assigned with a decision power α i,t at time t.The overall decision is a majority voting of all agents' decisions weighted by their decision power, which leads to a decision vector P t ∈ R K .If the overall decision asks for the ground-truth label, a reward r t is received after execution.Afterwards, the proposed bandits solver Exp4.P-EWMA updates the decision power of each agent based on its decision ξ i t in this iteration and the reward r t .With the objective of gaining a high cumulative reward, the ensemble of agents is the same as to combine the decisions made by each agent such that the reward gained in each iteration is close to the highest we can get from the best agent in the agent set.The execution of CbeAL is summarized as Algorithm 1.</p>
<p>Algorithm 1: CbeAL
Input: set of agents {AG 1 , . . . , AG N }, D 0 , f 0 , B Initialize: t = 0, budget_used = 0 while budget_used &lt; B do
Receive sample x t and contextual information {x t , ŷt , P f (ŷ|x t )} Obtain the decision vector ξ 1 t , . . ., ξ N t from agents Execute Exp4.P-EWMA solver for one iteration and obtain the action
a t if a t = 1 then Acquire y t , D t+1 = D t ∪ (x t , y t )
Train the base learner
f t+1 ← − D t+1 budget_used = budget_used + 1 else D t+1 = D t , f t+1 = f t Update {AG 1 , . . . , AG N } t = t + 1 Output: D t+1 , f t+1
Notably, the online updating of the base learner is not the focus of this study and we simply retrain the base learner based on all annotated samples from D t+1 .For a more efficient updating, online learning algorithms, such as first-order algorithms [Zinkevich, 2003] and Bayesian-based approaches [Chai et al., 2002], can be adopted depending on the base learner.</p>
<p>To integrate the goals of active learning and multi-armed bandits in order to provide informative acquisition, the design and characterization of the reward are critical.We define the reward r t as suggested in [Wassermann et al., 2019]:
r t = ρ + , if ŷt ̸ = y t ρ − , if ŷt = y t . (3.1)
The reward r t can only be obtained if the overall decision made by CbeAL acquires the groundtruth label.Otherwise, it is zero.Intuitively, the acquisition action will be rewarded if the base learner would have made a wrong prediction, otherwise it will be penalized since this acquisition is considered unnecessary.Therefore, it measures both the informativeness and the usefulness of an acquisition decision.Based on this design, the reward of acquiring a sample is determined by the performance of the current base learner f t , the incoming sample (x t , y t ), and also the performance of the bandits learner CbeAL.Thus, the sequence of reward {r 1 , r 2 , ..., r t } is autocorrelated.This characteristic is another reason that we adopt the setting of adversarial bandits [Auer et al., 2002], where one active learning agent (AG i ) is considered as one expert and decisions from each expert are simultaneously considered to make a joint decision, since no statistical assumption is made on reward generation in this setting.Additionally, instead of considering one agent as one arm, incorporating agents as experts also makes the number of agents scalable.In summary, with the designed setting of contextual information and the reward, the updated decision power of each agent adjusts the exploration-exploitation trade-off to improve the learning performance of the base learner.</p>
<p>To solve the formulated contextual bandits problem in CbeAL, we propose the Exp4.P-EWMA solver, where we embed a control chart-based flipping mechanism to Exp4.P [Beygelzimer et al., 2011].To balance the overall exploration-exploitation behaviour, the exploration-and exploitationoriented agents are incorporated in CbeAL by pairs, which can be easily adjusted for a specific online annotation scenario.However, with the pair ensemble, the direct application of Exp4.P can easily lead to a dominant agent (i.e., an agent consistently has the highest decision power) from an early stage, which makes CbeAL act no difference from a single active learning agent dedicated to exploration or exploitation.This can be expected since the pure exploration strategy in the early stage may cause the acquisition of samples with low uncertainty, so that the decision power of explorationoriented agents keeps decreasing until a level too low to contribute to the overall acquisition decision any longer.To avoid the early convergence in Exp4.P, a control chart-based flipping mechanism is integrated into the solver.Denote the standardized weight (i.e., standardized decision power) of the i-th agent at time t as α s i,t , where
α s i,t = α i,t N i=1 α i,t
. We monitor each standardized weight by an EWMA chart [Hunter, 1986], which detects weight drift over time.The intuition is that if the decision power of one agent keeps decreasing or increasing from the beginning, the decision power of all pairs of agents will be flipped so that the agents with lower power have more chances to lead the decision in the following period.Note this forced-exploration phase will only happen in a short period during the whole process, which is controlled by a hyperparameter γ.Denote the weighting factor for EWMA as λ, the size factor of shift to detect as h, and the estimated variance of α s i,t as s 2 i,t .With the flipping mechanism, the proposed Exp4.P-EWMA solver is detailed as follows:</p>
<p>As listed in Algorithm 2, at each time point t, the solver will first execute Exp4.P to make the acquisition decision a t and update the decision power of each agent α i,t+1 based on its decision vector ξ i t , the final decision probability P t , and the reward r t .In the second step, the flipping will be triggered if the standardized weight of any agent is outside the updated control limits.</p>
<p>The Exploration and Exploitation Agents</p>
<p>To balance the exploration-exploitation trade-off under different online annotation scenarios, we design distinguished active learning agents with exploration or exploitation objectives to be incorporated into CbeAL so that a systematic approach is developed without ambiguous selection.Another advantage of this design is the tendency for exploration or exploitation can be directly implied by the decision power of different types of agent.</p>
<p>Low-density Based Exploration Agent (LD-Agent)</p>
<p>The objective of exploration is to identify the structure of the input data distribution during the learning process.Two types of agents are proposed to encourage the exploration of the input variable space.The first type adopts a density-based criterion, which encourages the labelling efforts around Algorithm 2: Exp=4.P-EWMA solver
Parameters: δ, γ, h, p min ∈ [0, 1/K] Initialization: Set α i,1 = 1, α EW M A i,1 = 1 N for i = 1, . . . , N, µ = 1 N . for t = 1, 2, . . . do Input: decision vector of each agent ξ 1 t , . . . , ξ N t
Step 1: Exp4.P For a = 1, . . ., K get the final decision probability P t :
P t,a = (1 − Kp min ) N i=1 α i,t ξ i a,t N i=1 α i,t + p min
Draw the action a t based on P t and receive the reward r t
for a = 1, . . . , K set qa,t = r t /P t,a , if a = a t 0, otherwise , qt = [q 1,t , ..., qK,t ] ∈ R K for i = 1, . . . , N set ĝi,t = ξ i t • qT t , vi,t = a ξ i a /P t,a
Update the decision power:
α i,t+1 = α i,t • exp ( p min 2 (ĝ i,t + vi,t ln N KT ))
Step 2: EWMA-based flipping mechanism
for i = 1, . . . , N do 13 α s i,t+1 = α i,t+1 N i=1 α i,t+1 14 α ewma i,t+1 = λ • α s i,t+1 + (1 − λ)α ewma i,t LCL = µ − h • λ 2−λ • s 2 i,t , U CL = µ + h • λ 2−λ • s 2 i,t Set α s i,t+1 = 2µ − α s i,t+1 , α ewma i,t+1 &gt; U CL or α ewma i,t+1 &lt; LCL α s i,t+1 , otherwise , α i,t+1 = ( N i=1 α i,t+1 ) • α s i,t+1 h := h • exp γ
Output: a t each cluster boundary to discover new clusters by annotating samples lying in a sparse region with low density.We adopt the idea in [Ienco et al., 2014] to model the density of a sample.</p>
<p>Denote the set W as a sliding window of L previously observed samples, d(•, •) as the distance between two samples.Denote M axDist as a function, where M axDist(x i , W) returns the maximum distance between x i and other samples in the sliding window W. To approximate the local density for a new coming sample, we define local sparsity (i.e., low-density) factor of a sample x i as the number of times x i is the farthest away from other samples in W as follows:
lsf (x i ) = x j ∈W I{M axDist(x j , W) &lt; d(x i , x j )}. (3.2)
Algorithm 3 provides the pseudocode to acquire samples with lower local density.Given a streaming sample x t at time t, low-density based exploration agent first calculates the local sparsity factor lsf (x t ) to determine the acquisition probability p t as the output.Then, the sliding window W and the maximum pairwise distance between each sample in W will be updated.The sliding window mechanism is adopted to adjust the approximated density based on the most recent data stream.Note the window length L, and the sparsity fraction δ L are hyperparameters that affect the acquisition probability, which can be tuned to best suit the scenario.</p>
<p>Algorithm 3: Low-density Based Exploration Agent (LD-Agent)
Input: x t , W, L, D 0 , δ L Calculate lsf (x t ) for j = 1, 2, . . . , L do if d(x i , x j ) &gt; M axDist(x j , W) then M axDist(x j , W) = d(x i , x j ) Output: Acquisition probability p t = lsf (xt) L•δ L if |W| &gt; L then W := W \ x t−L W := W ∪ x t 3.3.2 Space-filling Based Exploration Agent (SPF-Agent)
The second type of exploration-oriented agents is based on a space-filling criterion.In the DoE literature, space-filling designs are applied to fully explore the response surface of computer experiments [Shang and Apley, 2021].Therefore, as an alternative strategy to explore the input variable space, a space-filling based exploration-oriented agent is developed to acquire samples uniformly distributed in the space.We adopt the idea of minimum pairwise distance criterion [Kennard and Stone, 1969] and propose a corresponding criterion to minimize the pairwise distance between acquired samples during the online data acquisition.</p>
<p>Similarly, a sliding window W keeps the most recent L samples.Denote M inDist as a function where M inDist(x i , W) returns the minimum distance between x i and all samples in W.</p>
<p>Algorithm 4: Space-filling Based Exploration Agent (SPF-Agent)
Input: x t , W, L, D 0 for i=1,2,. . . ,L do min(d i ) = min d(x i , x j ), ∀j ∈ W Calculate M inDist(x t , W) Output: Acquisition probability p t = M inDist(xt,W) max i∈W min(d i ) if |W| &gt; L then W := W \ x t−L W := W ∪ x t
In Algorithm 4, with a coming sample x t at time t, its minimum distance from the samples in W is compared with the largest minimum pairwise distance of samples in W to obtain the acquisition probability, leaving a higher probability for samples distant from the observed ones in W.</p>
<p>Intuitively, the density-based criterion will explore the boundary of the input variable space faster at an early stage, whereas the space-filling criterion allows for a more uniform exploration during the process.The combination of two exploration criteria will enhance the compatibility and adaptiveness of CbeAL to various learning scenarios.In practical, an ϵ-greedy policy can also be embedded which forces a sample to be acquired with probability ϵ for further exploration.</p>
<p>Reinforced Exploitation Agent (RAL-Agent)</p>
<p>The goal of exploitation in active learning is to capture the decision boundary, which is generally achieved by acquiring samples with ambiguous class membership.To enable the agent to intelligently identify the acquisition demand, we formulate it as a RL problem that aims at learning an adaptive threshold as the optimal policy to maximize the cumulative reward.As suggested by Wassermann et al. [2019], a RL-based controller is designed to adjust the certainty threshold θ based on the contribution of historical acquisition decisions.In detail, upon receiving a sample x t at time t, the prediction certainty ct(x t ) = max P f (ŷ|x t ) obtained by the base learner f t is compared with the current certainty threshold θ t to make the acquisition decision.The reward r t will be received if x t is acquired, which follows a consistent definition (i.e., r t ∈ {0, ρ + , ρ − }) as defined in (3.1).Afterwards, the certainty threshold will be updated as:
θ t+1 = min θ t (1 + η • (1 − 2 r t ρ − )), 1 . (3.3)
Note that the threshold will increase slightly with a positive reward and vice versa, which enables a policy adaptive to the decision boundary learned by the base learner.The algorithm of the reinforced exploitation agent is detailed in Algorithm 5.</p>
<p>Algorithm 5: Reinforced Exploitation Agent (RAL-Agent)
Input: x t , θ 0 , η, ρ + , ρ − if ct(x t ) &lt; θ t then p t = 1, obtain the reward r t Update the certainty threshold θ t+1 = min θ t (1 + η • (1 − 2 r t ρ − )), 1 else p t = 0 Output: Acquisition probability p t</p>
<p>Characterization of Agents</p>
<p>To characterize the exploration and exploitation capability of the proposed agents, the variance of the acquired samples D t by one agent is selected as an appropriate metric for assessing its exploration and exploitation activity.A higher variance suggests a learner's ability to explore the input variable space via acquiring samples in a larger region, whereas a lower variance implies a high frequency of acquisition in a small region for exploitation.To compare the variance of D t , the probability of a single sample being acquired by the proposed agents is examined.</p>
<p>We assume that the streaming data belong to a mixture of Gaussian distributions.Denote the previously observed samples stored in the sliding window W before time t as the set {x 1 , x 2 , ..., x L }, where x i belongs to the i-th Gaussian distribution (i.e., x i ∼ N q (µ i , Σ (i) )).Given a streaming sample x t at time t which follows another Gaussian distribution (i.e.,
x t ∼ N q (µ k , Σ (k) )), with the Euclidean distance d(x i , x j ) = ∥x i − x j ∥ 2
2 , For probability that LD-Agent acquires x t , we proved:</p>
<p>Theorem 1.If the streaming samples follow an independent multivariate Gaussian distribution (i.e.,
Σ (i) = σ 2 i I), then there exist M 1 , M 2 ∈ R L such that if ∥µ i − µ k ∥ 2 &gt; M 2,i , ∀i ∈ {1, ..., L}, then the expected acquisition probability of a LD-Agent E x i ,x j ,x k [p t ] will exceed 1, where M 1 , M 2 satisfies: M 2,i + erf −1 (1 − 2δ L ) • 2 • (4(σ 2 i + σ 2 k )M 2,i + 2q(σ 2 i + σ 2 k ) 2 ) + (σ 2 i + σ 2 k )q − M 1,i = 0 M 1,i &gt; ∥µ i − µ j ∥ 2 + (σ 2 i + σ 2 j )q + 4(σ 2 i + σ 2 j ) ∥µ i − µ j ∥ 2 + 2q(σ 2 i + σ 2 j ) 2 • Φ −1 (1 − 1 L − 1 ) + γ Φ −1 (1 − 1 L − 1 • e −1 ) − Φ −1 (1 − 1 L − 1 ) , ∀i ∈ {1, ..., L}. (3.4)
This result illustrates that with the increasing of the distance between the center of the distribution of the observed samples and that of the incoming sample, the expected acquisition probability approaches and exceeds 1.This ensures the acquisition of samples from a remote cluster, resulting in an increased variance and, thus, the exploration of the input variable space.</p>
<p>For the reinforced exploitation agent, assume that a logistic regression model is selected as the base learner and at time t the base learner f t is parameterized by β t .Given the labeled data pool D t at time t, for the expectation of the probability that the RAL-Agent acquires x t , we proved:</p>
<p>Theorem 2. Given the labeled data pool D t at time t, assume the center of the labeled samples in D t is µ i ∈ R q and the incoming sample
x t ∼ N q (µ k , σ 2 k I).
With the increase of the distance between two centers ∥µ i − µ k ∥ 2 , there does not exist
M 3 ∈ R such that P {|E xt [p t ] − M 3 | ≥ ϵ} = 0, ∀ϵ ∈ R.
Since p t belongs to [0, 1] for a RAL-Agent, the result implies that the acquisition probability of the incoming x t will not converge with the increase of the distance between the center of the distribution of D t and that of x t .Hence, for one sample from a remote cluster, the acquisition decision made by a RAL-Agent does not necessarily lead to an increasing variance.</p>
<p>In summary, the theoretical analysis justifies the exploration and exploitation capability of the proposed agents.Therefore, with the ensemble of two types of agents, the trade-off can be dynamically adjusted to the human annotation process.We also include the theoretical justification on the EWMA mechanism where we prove it does not affect the regret bound of the Exp4.p solver.The proof and numerical study can be found in the supplemental material due to the page limit.</p>
<p>Numerical Simulation</p>
<p>Simulation Setup</p>
<p>Suppose that we have a binary classifier as the base learner that requires online updating.Recall the third assumption that multiple clusters exist in the input variable space.Therefore, we adopt a cluster-based classification data set generation method [Pedregosa et al., 2011, Guyon, 2003] to generate the input X ∈ R n×p and the corresponding label y ∈ R n , where n is the sample size and p is the dimension of the input variable.We assume that there are two clusters in each class, thus we have 2 × 2 = 4 clusters in total.In brief, the centroids of 4 Gaussian clusters are first generated as the vertices of one polytope.The input variables are then independently drawn from each Gaussian cluster with unit variance and then multiplied by a random matrix to introduce the random covariance.Then, the samples in two of the four clusters will be assigned with the same label as y.</p>
<p>To evaluate CbeAL comprehensively, four settings are varied to generate different online annotation scenarios: (i) training sample size n, which includes both the initial training set and the streaming training set; (ii) the percentage of samples in the positive class pc, which determines the balanceness of the two classes; (iii) the percentage of disturbance ds; and (iv) the percentage of sparsity sp, which is defined as the percentage of insignificant input variables among total p input variables.Note that disturbances are added by flipping the labels of randomly selected samples.Additionally, to control the sparsity level, insignificant variables are randomly generated and concatenated to informative ones.</p>
<p>The data set generated for each online annotation scenario is subdivided into three subsets: the initial training set, the streaming training set, and the testing set.The initial training set has a constant size of 20 and the testing set has a size of 500.The number of samples in each class is balanced to be equal in the testing set to better illustrate the classification performance of the base learner.For all simulation scenarios, the budget is set to be 10% which gives the number of samples available to be labeled as B = 10% • (n − 20) during the streaming process.All scenarios are replicated 10 times with a randomly generated data set in each replication.</p>
<p>Based on the suggestion in [Ienco et al., 2014, Wassermann et al., 2019] and grid search in simulation experiments, we set the following values for the hyperparameters in CbeAL: p min = ln N KT , T = 2000, δ = 0.1, λ = 0.3, h = 5, γ = t/T , reward ρ + = 1, penalty ρ − = 0.5.Meanwhile, three pairs of agents with recommended hyperparameter values are incorporated into CbeAL, forming the set of six agents in Table 1.Note that the hyperparmaters can be further tuned for different learning scenarios.</p>
<p>Table 1: Agent set adopted in CbeAL Pair Index Agent Index Agent Hyperparameters
1 AG 1 LD 1 L = 100, δ L = 0.01 AG 2
RAL 1 θ 0 = 0.95, η = 0.005
2 AG 3 LD 2 L = 150, δ L = 0.005 AG 4
RAL 2 θ 0 = 0.95, η = 0.01
3 AG 5 SP F 1 L = 60 AG 6 RAL 3 θ 0 = 0.90, η = 0.01
To demonstrate the simulation setup, Fig. 3 visualizes the generated imbalanced and clusterwise input data of a toy example with 2 input variables (i.e., p = 2, n = 500, pc = 10%, sp = 0%, ds = 0%, B = 48).The logistic regression model with default hyperparameters is selected as the base learner [Kleinbaum et al., 2002, Pedregosa et al., 2011].The set of blue lines in Fig. 3 traces the base learner's evolving decision boundary with samples acquired by the candidate agents and CbeAL.The line's color depth indicates time, with the darkest being the final boundary tested for accuracy.</p>
<p>The decision boundary estimated with all training data is shown as orange lines.The results of the agents with the best performance in the agent set are presented (i.e., Table 1).CbeAL ensembles one exploration agent and one exploitation agent with the best performance in pairs, marked as CbeAL-2.</p>
<p>It is clearly shown that the LD-Agent actively seeks samples around the boundary of the input variable space, whereas the samples acquired by SPF-Agent are more evenly distributed.Both exploration-oriented agents successfully acquire the samples in both clusters of each class, but very limited samples around the ground-truth decision boundary are selected.In regard to the RAL-Agent, although the uncertainty threshold is supposed to be updated adaptively, Fig. 3 reveals that it is stuck in one cluster of the positive class while the agent keeps acquiring around the wrongly</p>
<p>A Comprehensive Simulation Study</p>
<p>In the comprehensive simulation study, the settings are varied with the following levels: n ∈ {500, 1000, 1500}; pc ∈ {10%, 5%}; ds ∈ {0%, 3%}; sp ∈ {30%, 70%}.The dimension of the input variable is set as p = 15.SVM is selected as the base learner with default parameters [Gunn et al., 1998, Pedregosa et al., 2011], which validates the effectiveness of CbeAL as a generic framework for classification models.Note that the prediction probability P f (ŷ t |x t ) of SVM is estimated and calibrated by Platt scaling [Platt et al., 1999].Denote CbeAL-2 as the ensemble of the first pair of agents in Table 1 (i.e., AG 1 and AG 2 ), CbeAL-4 as the ensemble of the first two pairs, and so on for CbeAL-6.Specifically, CbeAL-6 is proposed as the recommended configuration due to its superior performance enhanced by the ensemble of multiple distinguished agents, which will be detailed in the scalability study.</p>
<p>In this study, CbeAL is firstly compared with the incorparated agents and their variants to study the effectiveness of the ensemble mechanism.Then, we investigate the impact of the number of agents in the ensemble to study the scalability of CbeAL.Finally, as the recommended configuration, CbeAL-6 is compared with four benchmark methods from literature (i.e., uncertainty sampling [Lewis and Gale, 1994] and random sampling, DBALStream [Ienco et al., 2014] and QBC-PYP [Loy et al., 2012]) to test the general performance.</p>
<p>In summary, nine benchmark methods are compared with CbeAL-6 where the first three are the candidate agents (i.e., LD, SPF and RAL-Agent), the middle two are the ensemble models with different number of agents (i.e., CbeAL-2 and CbeAL-4), and the last four are methods from the literature.Among the benchmarks, RAL-Agent employs an acquisition criterion learned by multi-armed bandits as a cutting-edge AI-guided active learning method; CbeAL-2 and CbeAL-4 adopt the proposed ensemble framework; LD-Agent, SPF-Agent and Random Sampling (RS) focus on the exploration of the input variable space while Uncertainty Sampling (US) [Lewis and Gale, 1994] caters to exploitation; DBALStream [Ienco et al., 2014] and QBC-PYP [Loy et al., 2012] are two state-of-the-art composite active learning methods which integrate the objective of exploration and exploitation in their design of acquisition criteria.In detail, in RS, each streaming-in sample is annotated based on a probability derived from the ratio of the budget b to the training sample size n.QBC-PYP and DBALStream are configured with default parameters.US employs an uncertainty threshold of 0.7, determined through cross-validation.</p>
<p>To demonstrate the effectiveness of CbeAL in achieving high learning performance of the base learner with limited budgets, the classification accuracy of the base learner trained by the samples acquired by each method on the testing set is evaluated as a metric.We further investigate the percentage of positive samples acquired during the learning process as another metric to illustrate the exploration-exploitation trade-off in the supplemental material.</p>
<p>Compared with Individual Agents</p>
<p>First, the comparison of the classification accuracy of the base learners between CbeAL, the incorporated agents, and their variants is shown in Table 2.The performance of the agents that achieve the highest accuracy on average among the exploration-and exploitation-oriented agents in the agent set is selected to be reported as "Opt.Explor." and "Opt.Exploit."The results of other individual agents are omitted here for better readability.It is observed that in the toy example, some of the agents do not use up the budget B. To validate that it is a fair comparison with different numbers of acquired samples, we create "Opt.Explor.(Full)" and "Opt.Exploit.(Full)" as two variants where random sampling is used to artificially acquire from the unselected samples after the agents finish their acquisition of the streaming data, until the budget is used up.Besides, the ϵ-greedy policy with ϵ = 0.01 is applied to RAL-Agents to effectively improve their learning performance to be a more competitive benchmark [Wassermann et al., 2019].They are also applied to CbeAL methods for a fair comparison.</p>
<p>Table 2 summarizes the averages of the classification accuracy and standard errors over 10 replications of the base learner trained by D t .It can be observed that the proposed CbeAL-6 outperforms the best individual agent in 20 of the 24 scenarios, which verifies that the ensemble of multiple agents with explicit consideration for both exploration and exploitation can effectively enhance the learning performance under a highly imbalanced class distribution.The advantage on learning performance compared to benchmarks is more significant when there is no disturbance and the class proportion is more balanced (i.e., ds = 0%, pc = 10%).However, with a more severe imbalance (i.e., pc = 5%), "Opt.Exploit." and its variants sometimes achieve slightly higher accuracy.One possible reason is that under such scenarios, it will be more efficient to only focus on decision boundary learning since the number of positive samples is limited.Besides, the inferior performance of CbeAL-6 under the scenario ds = 3%, pc = 10%, n = 500, sp = 70% can be caused by the disturbance.It can be found that, in general, the learning performance of the base learner is improved with a data stream with a large size and a higher sparsity.However, when the sparsity is low, the accuracy sometimes decreases as the training sample size increases, which can be caused by the high imbalance and the lack of degree of freedom.</p>
<p>Another finding is CbeAL-2 achieves comparable performance with the better of the two incorporated agents, which implies that the proposed ensemble framework enables the intelligent selection among candidate agents in an adaptive manner.Under 14 out of 24 scenarios, it outperforms both "Opt.</p>
<p>Explor." and "Opt.Exploit.",where the results are marked with * .</p>
<p>Comparing the "Opt.Explor." with "Opt.Explor.(Full)" and "Opt.Exploit." with "Opt.Exploit.(Full)", we find that consuming the remaining budget by random acquisition will not make a significant improvement on the learning performance under most scenarios.Sometimes it will select less contributive samples, which leads to a lower accuracy because of the highly imbalanced distribution.Therefore, it validates that the agents have acquired the most informative samples based on their criteria.Thus, this variant will not be considered in the following analysis.</p>
<p>We also find that CbeAL-6 obtains a significantly better balanced labeled data set D t with a higher percentage of positive samples compared to the benchmarks under most scenarios.Detailed results can be found in Table A1 in the supplementary material.</p>
<p>Scalability Study</p>
<p>It has been observed in the previous results (i.e., Tables 2) that CbeAL-6 achieves better performance than CbeAL-2 in general.Here, we further investigate the following two questions: What will be the impact of the ensemble of varying numbers of agent pairs and how should the agents be selected.</p>
<p>Rechecking the results in Table 2, We observe that CbeAL-6 demonstrates a dominate superiority in the learning performance, which indicates the advantage brought by multiple agents.However, comparing the result of CbeAL-4 with CbeAL-2, CbeAL-4 achieves better performance in fewer than half of all scenarios.The counterintuitive result indicates that the ensemble of more agents may not improve the performance.To identify the reason, we investigate the acquisition decision made by each agent in the agent set and their standardized weights in CbeAL-6 under one scenario in Fig. 4 where CbeAL-4 shows inferior performance than CbeAL-2 but CbeAL-6 performs better.It can be observed from the bar charts (Fig. 4(a)-(c)) that in CbeAL-6, the first two pairs of agents (LD 1 and LD 2 , RAL 1 and RAL 2 ) make similar acquisition decisions while the third pair behaves differently.As a direct result of homogeneity, the standardized weights of the first two pairs of exploration-and exploration-oriented agents will be close and change in a similar pattern in Fig. 4(d)), which also causes a comparable performance of CbeAL-4 and CbeAL-2.This also explains that the superior performance of CbeAL-6 lies in the heterogeneous decisions brought by the third pair of agents (SP F 1 and RAL 3 ).Besides, the weights of agents in CbeAL-6 indicate that at the beginning, the exploration dominates the active learning process.Later, the proposed CbeAL switches its tendency to exploitation, and the exploration capability still remains adaptive to the data stream, which contributes to the effective and efficient online annotation.</p>
<p>In summary, the ensemble of distinguished agents provides comprehensive criteria to evaluate the informativeness of each streaming sample in terms of exploration and exploitation, thus achieving a well-balanced trade-off.Since the acquisition behaviour of one active learning agent varies under different scenarios and there does not exist one overall winner, CbeAL-6 is recommended as a default configuration to solve these challenging learning tasks.</p>
<p>Comparison Study with Benchmark Methods</p>
<p>Finally, CbeAL is compared with other four benchmark methods (i.e., RS, US, DBALStream and QBC-PYP) and Table 3 summarizes the classification accuracy of the base learner trained by samples acquired by each method.By investigating the results in Table 3, it is concluded that CbeAL-6 achieves significantly better performance compared to the benchmarks under most scenarios.With a limited budget, CbeAL-6 can achieve high classification accuracy close to that of using all the training data with a data stream of larger size and higher sparsity (i.e., n = 1500, sp = 70%).</p>
<p>Considering the benchmark methods, US demonstrates its competitive performance compared to other benchmarks, but with higher standard errors.This indicates the importance of exploitation for the online annotation scenarios, and this also explains the superiority of RAL-Agents in Table 2.However, the lack of adaptiveness to the data stream causes its inferior performance compared to CbeAL-6.The inferior performance of DBALStream might attribute to its concentration on samples with both high local density and large margin, which does not perform effective input variable space exploration.On the contrary, the proposed LD-Agent is able to complete this exploration task.QBC-PYP underperforms in comparison to US, primarily due to its inadequate exploitation capability under a highly imbalanced class distribution.During the streaming-in process, it takes a mixture of Gaussians to quantify the ambiguity of the class membership of one sample.The lack of connection between its acquisition decision and the evolving performance of the base learner results in the lack of effective exploitation.</p>
<p>Additional results of other benchmarks on the toy example are included in the supplementary material, where DBALStream and QBC-PYP outperform US and RS in terms of learning performance.</p>
<p>The result implies that US will achieve poor performance if the base learner is confident about its classification result at the beginning with the initial training data.</p>
<p>Overall, the results validate that the proposed method can acquire samples effectively and efficiently in an adaptive manner under various circumstances, confirming the benefits of the ensemble of designed exploration-oriented and exploitation-oriented agents.</p>
<p>Case Study</p>
<p>The proposed CbeAL method is applied to a FDM process for online quality modeling and inspection [Chen et al., 2016], which is introduced as the motivation example in Section 1.During the printing process, various in situ process variables (i.e., vibration, nozzle temperature, etc.) are collected in the ICPS to monitor the process and predict the quality of the FDM part [Sun et al., 2016].Here, we focus on the layerwise surface roughness as a binary quality indicator, which is judged and annotated as conforming/nonconforming by domain experts.Fig. 5 shows the example normal and rough surfaces of the printed FDM part.To enable real-time quality prediction and online modeling, the in situ measurements are registered and divided into 10-second windows as samples.The online updating of the quality model requires experts to consistently observe and examine the surface roughness during the printing process for the window-wise annotation, which is labor-intensive and time-consuming.Therefore, CbeAL is employed to develop an accurate quality model with less labeling efforts and high-quality training data through wisely selecting the samples for annotation.</p>
<p>We refer Chen et al. [2016] for the details of the data collection.During the process, the in situ extruder vibration, table vibration, nozzle temperature and table temperature are measured and collected in a functional data format.Considering the wavelet analysis applied to the funcational measures, the process setting variables (i.e., feed/flow ratio and layer thickness) and summary statistics for each functional measurement (i.e., mean, standard deviation, skewness, and kurtosis), 519 features are obtained in total as the model input.48 FDM parts are printed successfully in total with 1588 samples (i.e., windows of measurements).Label the nonconforming samples by 1 and the conforming samples by 0. As a common scenario in quality inspection, we find there are 180 nonconforming samples in total, which implies a highly imbalanced class distribution.We also notice that the positive labels appear in succession during the process (i.e., . . .0000011100 . . . ) because the [2020] with authors' permission malfunction of the printer in a period of time will affect the quality of several consecutive layers.Therefore, we maintain the original order of samples in sequence when we separate the training and testing set.A logistic regression model with L1 penalty is adopted as the base learner for the quality online modeling.In brief, the highly imbalanced data stream with patterns in sequence and an underlying multimodal distribution (i.e., Fig. 1) brings a challenging online annotation scenario.</p>
<p>We evaluate the classification accuracy of the base learner of the proposed CbeAL-6 under different level of budgets (i.e., {3%, 5%, 10%, 15%, 20%}) with 10 replications.In each replication, 1/3 samples are extracted with a random starting point from the whole data set in time order as the testing data set, with the remaining for training.The first 10 samples in the training set will be used for the model pretraining as D 0 .The best performed individual agents in Table 1 and the other four benchmarks in Section 4 are employed for the comparison.Here, the result of QBC-PYP is not included since it consistently refuses to acquire samples during the streaming process, which might be caused by the pattern of positive labels in the sequence that requires the method to adapt its exploration-exploitation tendency to the data stream.</p>
<p>By investigating the results, it can be observed that the proposed method consistently outperforms its incorporated agents and the rest of benchmark methods in testing accuracy under different levels of budgets.The testing classification accuracy of most benchmark methods has an increasing trend as the budget increases from 3% to 10% and then the trend goes smoother, which indicates the samples acquired in the 10% budget make the most of the contribution to the quality modeling.However, the testing accuracy of CbeAL-6 keeps increasing with a higher budget, which validates the continuous acquisition of informative samples.Notably, the testing accuracy of CbeAL-6 exceeds the accuracy of the base learner trained by all available samples when the budget is higher than 10%.This implies, despite the high dimension, a training set with a smaller sample size but better balanced samples has better quality, thus improving the modeling accuracy.Therefore, the proposed method not only reduces the labelling efforts but also contributes to the online modeling performance via acquiring high-quality samples.</p>
<p>In conclusion, the case study verifies that CbeAL achieves a well-balanced exploration-exploitation trade-off during the streaming process in an adaptive manner, which enables the highly accurate online quality modeling with limited labelling efforts for the FDM quality inspection.</p>
<p>Conclusions</p>
<p>While the high-speed, high-volume streaming data brought by the ICPS enhance the data-driven decision-making by AI models, the quality of online data may hamper the modeling performance for manufacturing.To provide resilient AI modeling performance, informative samples need to be acquired from the streaming data to provide a high-quality training data set as well as reducing the human annotation efforts required for AI incubation in an online manner.Existing active learning methods cannot balance the exploration-exploitation trade-off in the challenging online annotation scenario.In this work, we propose an ensemble of exploration-and exploitation-oriented active learning agents as CbeAL.With the ensemble of agents considering each objective explicitly and the proposed Exp4.P-EWMA solver, CbeAL adjusts the exploration and exploitation tendency adaptive to different online annotation scenarios.Simulation studies and the case study in FDM processes demonstrate the advantage of CBEAL over benchmarks in the learning accuracy with a limited acquisition budget.</p>
<p>We notice some limitations of the proposed method.First, CbeAL shows its superiority under learning scenarios with a complex input data distribution.Under other generic scenarios without a demanding exploration-exploitation trade-off, CbeAL may lose its advantage due to the encoded explicit exploration objective.In this case, CbeAL can enhance its concentration on exploitation by adding exploitation-oriented agents or removing exploration-oriented agents.Second, the hyperparameters for the agents are optimized with the grid search.We will formulate the hyperparameter tuning as a meta-learning problem for different base learners as future work [Thrun and Pratt, 2012].</p>
<p>The work leaves us with several future research directions.Firstly, the reward function in CbeAL can be adjusted to quantify regression model performance, such as the root mean squared error of importance-weighted training samples [Beygelzimer et al., 2009].Concurrently, the exploitationoriented agent's reward function could be set as either the base learner's bootstrap uncertainty [Endo et al., 2015] or the expected model change [Cai et al., 2013].These adjustments extend the framework to generic supervised models.Second, we will investigate formulating the budget as a hard constraint to maintain strict control over the acquisition cost.To realize this, we propose modifying our Exp4.P-EWMA algorithm and integrating it with the knapsack bandit algorithm [Badanidiyuru et al., 2018] to effectively address this hard constraint.Furthermore, we consider the ensemble of multiple modalities as the second level actions in CbeAL, enabling the learner to decide not only whether to acquire a sample but also its data source, inspired by [Wang et al., 2021b].</p>
<p>Martin Zinkevich.Online convex programming and generalized infinitesimal gradient ascent.In Proceedings of the 20th international conference on machine learning (icml-03), pages 928-936, 2003.</p>
<p>A Proofs B Characterization of Agents</p>
<p>To validate the exploration and exploitation capability of the proposed agents, theoretical justification is provided for the designed acquisition criteria.The variance of the acquired samples D t by one agent serves as an appropriate metric for assessing its exploration and exploitation activity during the online annotation process.A higher variance suggests a learner's ability to explore the input variable space via acquiring samples in a larger region, whereas a lower variance implies a high frequency of acquisition in a small region for exploitation.To compare the variance of D t , we examine the probability of a single sample being acquired by the proposed agents.</p>
<p>B.1 Exploration-oriented Agent</p>
<p>We assume the streaming data belong to a mixture of Gaussian distributions.Denote the previously observed samples stored in the sliding window W before time t as the set {x 1 , x 2 , ..., x L }, where x i belongs to the i-th Gaussian distribution (i.e., x i ∼ N q (µ i , Σ (i) )).Given a streaming sample x t at time t which follows another Gaussian distribution (i.e., x t ∼ N q (µ k , Σ (k) )), the acquisition probability of the proposed low-density based exploration agent is:
p t = lsf (x t )) Lδ L = L i=1 I{max d(x i , x j ), j ̸ = i, j ∈ {1, ..., L} &lt; d(x i , x t )} Lδ L . (B.1)
With the distance d(x i , x j ) = ∥x i − x j ∥ 2 2 , the acquisition probability can be rewritten as:
p t = L i=1 P { max j̸ =i,j∈{1,...,L} ∥x i − x j ∥ 2 2 &lt; ∥x i − x t ∥ 2 2 }/(L • δ L ). (B.2)
To evaluate the acquisition decision, we calculate the expectation of the acquisition probability:
E x i ,x j ,x k [p t ] = E L i=1 P { max j̸ =i,j∈{1,...,L} ∥x i − x j ∥ 2 2 &lt; ∥x i − x t ∥ 2 2 }/(L • δ L ) = L i=1 E[P { max j̸ =i,j∈{1,...,L} ∥x i − x j ∥ 2 2 &lt; ∥x i − x t ∥ 2 2 }]/(L • δ L ). (B.3)
Denote X i,j and Y i,t as the squared Euclidean distance between x i , x j and x i , x t , (i.e., X i,j = ∥x i − x j ∥ 2 2 ∈ R where j ̸ = i, i, j ∈ {1, ..., L}, and
Y i,t = ∥x i − x t ∥ 2 2 ∈ R, i ∈ {1, ..., L}). Let Z i = max j̸ =i,j∈{1,...,L} ∥x i − x j ∥ 2 2 , F Z (z) = P (X i,1 &lt; z, X i,2 &lt; z, . . . , X i,L &lt; z) = Π L j=1 F X i,j (z). E x i ,x j ,x k [p t ] = L i=1 E[E[P {Z i &lt; Y i,t }|Y i,t ]]/(L • δ L ) = L i=1 E[F Z (Y i,t )]/(L • δ L ) = L i=1 ∞ 0 Π L−1 j=1,j̸ =i F X i,j (y)) f Y i,t (y)dy/(L • δ L ), (B.4)
where F X i,j (•) is the cumulative density function for X i,j and f Y i,t is the probability density function for Y i,t .</p>
<p>Since the samples in W and the incoming sample x t can be considered as independent draws from different Gaussian distributions, we have x i − x j ∼ N (x i − x j , Σ (i) + Σ (j) ), and
x i − x t ∼ N (x i − x t , Σ (i) + Σ (k) )
. Therefore, X i,j and Y i,t are quadratic forms of random normal variables and follow a generalized chi-square distribution.The probability density function for X i,j is [Mathai and Provost, 1992]:
f X i,j =y = ∞ k=1 (−1) k c k y q 2 +k−1 Γ( q 2 + k) , 0 ≤ y ≤ ∞. (B.5)
The cumulative density function is:
F (X i,j &lt; y) = ∞ k=0 (−1) k c k y q 2 +k Γ( q 2 + k + 1) , 0 &lt; y &lt; ∞, (B.6)
where c 0 and c k are defined by:
H T (Σ (i) + Σ (j) )H = diag(λ 1 , ..., λ q ) (B.7) H T H = I, (B.8) b = H T (Σ (i) + Σ (j) ) − 1 2 (µ 1 − µ 2 ), (B.9) c 0 = exp (− 1 2 q j=1 b 2 j )Π q j=1 (2λ j ) − 1 2 (B.10) d k = 1 2 q j=1 (1 − kb 2 j )(2λ j ) −k , k ≥ 1 (B.11) c k = 1 k k−1 r=0 d k−r c r , k ≥ 1 (B.12)
For tractability of the computation, we assume the input variables in the Gaussian distributions to be independent (i.e., Σ (i) = σ 2 i I), then the distance can be simplified and approximated by a normal distribution when q is large based on the central limit theorem:
X i,j ∼ N q ∥µ i − µ j ∥ 2 + (σ 2 i + σ 2 j )q, 4(σ 2 i + σ 2 j ) ∥µ i − µ j ∥ 2 + 2q(σ 2 i + σ 2 j ) 2 (B.13) Y i,t ∼ N q ∥µ i − µ k ∥ 2 + (σ 2 i + σ 2 k )q, 4(σ 2 i + σ 2 k ) ∥µ i − µ k ∥ 2 + 2q(σ 2 i + σ 2 k ) 2 . (B.14)
This gives us the expectation of acquisition probability as:
E x i ,x j ,x k [p t ] = L i=1 ∞ 0 Π L−1 j=1,j̸ =i Φ y i − ∥µ i − µ j ∥ 2 − (σ 2 i + σ 2 j )q ( 4(σ 2 i + σ 2 j ) ∥µ i − µ j ∥ 2 2 + 2q(σ 2 i + σ 2 j ) 2 • ϕ y i − ∥µ i − µ k ∥ 2 − (σ 2 i + σ 2 k )q 4(σ 2 i + σ 2 k ) ∥µ i − µ k ∥ 2 2 + 2q(σ 2 i + σ 2 k ) 2 dy i /(L • δ L ), (B.15)
where Φ(•) is the cumulative distribution function (CDF) for the standard normal distribution and ϕ(•) is the probability density function (PDF) for the standard normal distribution.</p>
<p>Theorem 3. If the streaming samples follow an independent multivariate Gaussian distribution (i.e.,
Σ (i) = σ 2 i I), then there exist M 1 , M 2 ∈ R L such that if ∥µ i − µ k ∥ 2 &gt; M 2,i
, ∀i ∈ {1, ..., L}, then the expected acquisition probability of a LD-Agent E x i ,x j ,x k [p t ] will exceed 1, where M 1 , M 2 satisfies:
M 2,i + erf −1 (1 − 2δ L ) • 2 • 4(σ 2 i + σ 2 k )M 2,i + 2q(σ 2 i + σ 2 k ) 2 + (σ 2 i + σ 2 k )q − M 1,i = 0 M 1,i &gt; ∥µ i − µ j ∥ 2 + (σ 2 i + σ 2 j )q + 4(σ 2 i + σ 2 j ) ∥µ i − µ j ∥ 2 + 2q(σ 2 i + σ 2 j ) 2 • Φ −1 (1 − 1 L − 1 ) + γ Φ −1 (1 − 1 L − 1 • e −1 ) − Φ −1 (1 − 1 L − 1 ) , ∀i∥x i − x j ∥ 2 2 )]/δ L ≥ 1 ∀i ∈ {1, ..., L},
then the expectation of the acquisition probability
E x i ,x j ,x k [p t ] = L i=1 [1 − F Y i,t ( max j̸ =i,j∈{1,...,L} ∥x i − x j ∥ 2 2 )]/(δ L • L) ≥ 1.
Thus, to ensure E x i ,x j ,x k [p t ] ≥ 1, we have:
F −1 Y i,t (1 − δ L ) ≥ max j̸ =ij∈{1,...,L} ∥x i − x j ∥ 2 2 = Z i , ∀i ∈ {1, ..., L} (B.17) Z i − ∥µ i − µ k ∥ 2 + (σ 2 i + σ 2 k )q 2 • (4(σ 2 i + σ 2 k ) ∥µ i − µ k ∥ 2 + 2q(σ 2 i + σ 2 k ) 2 ) ≤ erf −1 (1 − 2δ L ) →g(∥µ i − µ k ∥ 2 ) = ∥µ i − µ k ∥ 2 + erf −1 (1 − 2δ L ) • √ 2• 4(σ 2 i + σ 2 k ) ∥µ i − µ k ∥ 2 + 2q(σ 2 i + σ 2 k ) 2 + (σ 2 i + σ 2 k )q − Z i ≥ 0. (B.18)
By Fisher-Tippett-Gnedenko theorem [Fréchet, 1927] and the independent assumption on the pairwise distances, Z i can be approximated by generalized extreme value (GEV) distribution where the expectation of Z i can be estimated with:
E[Z i ] ≈ ∥µ i − µ j ∥ 2 + (σ 2 i + σ 2 j )q + 4(σ 2 i + σ 2 j )) ∥µ i − µ j ∥ 2 + 2q(σ 2 i + σ 2 j ) 2 • Φ −1 (1 − 1 L − 1 ) + γ Φ −1 (1 − 1 L − 1 • e −1 ) − Φ −1 (1 − 1 L − 1 ) , ∀i, j ∈ {1, ..., L}, i ̸ = j, (B.19)
where γ is the Euler-Mascheroni constant.Accordingly, the variance is:
V[Z i ] ≈ 4(σ 2 i + σ 2 j ) ∥µ i − µ j ∥ 2 + 2q(σ 2 i + σ 2 j ) 2 • Φ −1 (1 − 1 L − 1 • e −1 ) − Φ −1 (1 − 1 L − 1
) , ∀i, j ∈ {1, ..., L}, i ̸ = j.(B.20)By Chebyshev's inequality, we have:
P (|Z i,n − E[Z i ]| ≥ ϵ) ≤ V[Z i ] nϵ 2 . (B.21) Therefore, there exist M 1,i &gt; E[Z i ] such that Z i,n &lt; M 1,i , ∀n ∈ Z. Since 0 &lt; δ L &lt; 1, erf −1 (1 − 2δ L ) &gt; 0, g(∥µ i − µ k ∥ 2 ) is monotonic with ∥µ i − µ k ∥ 2 . Hence, there exist M 1 , M 2 ∈ R L such that if ∥µ i − µ k ∥ 2 &gt; M 2,i ∀i ∈ {1, ..., L}, E x i ,x j ,x k [p t ] ≥ 1, where M 1 , M 2 satisfies: M 2,i + erf −1 (1 − 2δ L ) • 2 • 4(σ 2 i + σ 2 k )M 2,i + 2q(σ 2 i + σ 2 k ) 2 + (σ 2 i + σ 2 k )q − M 1,i = 0 M 1,i &gt; E[Z i ], ∀i, j ∈ {1, ..., L}, i ̸ = j. (B.22)
This result illustrates that with the increasing of the distance between the center of the distribution of the observed samples and that of the incoming sample, the expected acquisition probability approaches and exceeds 1.</p>
<p>Numerical analysis is further investigated for the LD-Agent.To be more intuitive, we set the samples in W all belonging to the same Gaussian distribution while x t belongs to another (i.e., µ i = µ j = µ 1 , σ i = σ j = σ 1 , σ k = σ 2 ).Set q = 15 and δ L = 0.05, the expectation of the acquisition probability calculated by (B.15) is shown as follows:  As shown in Fig. 7, with a larger distance between two centers of the clusters, with less samples in the sliding window W and with higher σ 2 σ 1 ratio, the expectation of the acquisition probability will increase and approach to 1.This ensures the acquisition decision of samples from a remote cluster, resulting in an increased variance and thus, exploration of the input space.</p>
<p>B.2 Numerical Comparison</p>
<p>For further comparison between exploration-oriented agents and exploitation oriented agents, we assume W = D t at time t, and the samples in W and D t all belong to the same Gaussian distribution (i.e., x 1 ∼ N q (µ 1 , σ 2 1 I)).Set σ 1 = σ 2 = 1, q = 15, δ L = 0.5, we have the following result:  In summary, the theoretical analysis and numerical results illustrate that the proposed explorationoriented agent is more likely to acquire a sample from a distribution distinct from the observed one, thus increasing the variance of the labelled samples.On the other hand, the acquisition decision of exploitation-oriented agent is determined by the base learner's uncertainty instead of the distance between the observed and incoming distribution, resulting in a relative smaller variance.This justifies the exploration and exploitation capability of the proposed agents.Therefore, with the ensemble of two types of agents, the exploration and exploitation can be dynamically adjusted to balance the trade-off during the human annotation process.To this end, in the proposed CbeAL, one RAL-agent will be paired with one SPF-Agent or LD-Agent to balance the effort spent on exploration and exploitation.</p>
<p>C Theoretical Justification on Exp4.p-EWMA</p>
<p>To justify applying the Exponentially Weighted Moving Average (EWMA) chart to monitor the weight of each agent in CbeAL, we follow the derivation in Exp4.P paper [Beygelzimer et al., 2011] to investigate its impact on the regret bound of the adversarial bandits solver.</p>
<p>As EWMA tracks the moving average of all previous sample means-specifically, the means of the weight α i,t for each active learning agent (i = 1, . . ., N, t = 1, 2, . . .)-it creates fluctuating control limits.Consequently, deriving a specific bound for the monitored weight α i,t is not straightforward.</p>
<p>To resolve this, we equate the effect of applying EWMA to establishing a bound for each weight, i.e., e &lt; α i,t &lt; f, 0 &lt; e &lt; f. (C.1)</p>
<p>Note that the elements in decision vector are bounded by (0, 1), i.e., 0 &lt; ξ i j &lt; 1, ∀i = 1, . . ., N, ∀a = 1, . . ., K. Based on Algorithm 2 (i.e., Exp4.P-EWMA solver), the bounds for vi,t can be derived as:
0 &lt; vi,t &lt; K (1 − Kp min ) • f e + p min , (C.2)
By comparing Fig. 4(d) (i.e., Exp4.p-EWMA) and Fig. 9 (i.e., Exp4.p), it shows that without the flipping mechanism, the decision power of most exploration-oriented agents keeps decreasing since the early stage.Conversely, exploitation-oriented agents experience a continual increase in decision-making power.This pattern is attributed to the less effective acquisitions made by exploration-oriented agents at the start of the learning process.However, this scenario eventually results in exploitation-oriented agents dominating the active learning process, leading to insufficient exploration in later stages, particularly in the presence of shifting distributions.Consequently, the final testing accuracy under this scenario is considerably reduced to 0.64, as opposed to the 0.704 achieved with Exp4.p-EWMA.These results validate the effectiveness of the EWMA monitoring and flipping mechanism in the proposed Exp4.p-EWMA.It prevents early convergence of decision power and better balances the exploration-exploitation trade-off throughout the process.</p>
<p>E Additional Experimental Results</p>
<p>E.1 Benchmark Results in Toy Example</p>
<p>To provide more concrete insights, we applied Uncertainty Sampling (US), Random Sampling (RS), DBALStream, and QBC-PYP to the two-dimensional toy example and visualized the acquired decisions, as well as the evolution of the base learners' decision boundary in Fig. 10.</p>
<p>Firstly, the results demonstrate that both DBALStream and QBC-PYP outperform US and RS in terms of learning performance.The samples procured by DBALStream are located near the classification boundary and exhibit high local density.However, comparing Fig. 10(a) and Fig. 3(a), it's evident that LD-agent is more effective in acquiring samples located near the boundary of the input variable space compared to DBALStream, thus facilitating more effective exploration.</p>
<p>Secondly, by checking 10(b), QBC-PYP does not perform good exploitation since it acquires considerably more normal samples than abnormal samples.Upon investigating the annotation decision process, we observed that QBC-PYP often acquires sample annotations during the initial stage but seldom requests labels post the early stages, a trend that persists across both toy examples and comprehensive simulation studies.This behavior can be attributed to the disconnection between the acquisition decision and the base learner's performance, which inhibits effective learning.</p>
<p>Finally, US achieve inferior performance compared to DBALStream, QBC-PYP in this top example.By investigating the prediction uncertainty, it is found that the small number of acquired samples is caused by the low uncertainty of the base learner.Therefore, if the base learner is confident about its classification result at the beginning with the initial training data, then the US will achieve poor performance.As per the case study depicted in Fig. 5, due to the high initial training accuracy, US exhibits poorer performance relative to other benchmarks.However, in the comprehensive simulation study shown in Table 6, the low initial training accuracy prompts US to acquire more samples with high prediction uncertainty, thereby enhancing performance.</p>
<p>E.2 Scalability Study</p>
<p>The classification accuracy of the base learners of CbeAL-2, CbeAL-4, CbeAL-4<em>, CbeAL-6, CbeAL-8 are compared to study the scalability of CbeAL.Here, CbeAL-4</em> incorporate pair 1 and pair 3 in Table 1.We further added a new pair with the setting in Table 4 to be incorporated into CbeAL-8.</p>
<p>The results of all the variants of CbeAL in the comprehensive simulation study are individually  RAL 4 θ 0 = 0.90, η = 0.005 presented in Table 5 for better readability.</p>
<p>Comparing CbeAL-4 and CbeAL-4<em>, the latter generally outperforms the former.As detailed in Section 3.2.2, this performance difference is anticipated given the distinct acquisition decisions of agents pair 1 and pair 3. Therefore, the incorporation of pair a and pair 3 (i.e., CbeAL-4</em>) achieves better exploration-exploitation trade-off during the learning process.However, we will find CbeAL-6 still achieves better results, which gains more heterogeneous decisions by the incorporation of three pairs of agents.However, this does not imply that simply adding more active learning agents invariably enhances performance.For instance, CbeAL-8 excels only in 2 of the 24 scenarios, even if its average performance marginally surpasses CbeAL-4.This is attributed to the inefficiency of Exp4.P when managing a vast agent pool.In such cases, Exp4.P will become inefficient in learning since it requires keeping explicit weights over the agents [Besbes et al., 2014].Therefore, CbeAL-6 is recommended as a default setting while the authors are encouraged to tune the settings based on the specific application scenario.</p>
<p>E.3 Number of Acquired Samples</p>
<p>We show the result of the average number of the acquired samples by the proposed method and the candidate agents in the simulation study in Table 6.It is shown that the exploitation-oriented agent tends to acquire less samples under most of the scenarios whereas CbeAL (i.e., CbeAL-2 and CbeAL-6) lie in the middle of the two incorporated agents.This is reasonable since the ensemble method encodes the exploration behaviour during the process, which will lead to a larger number of queries compared to pure exploitation.</p>
<p>E.4 Evaluation of the Bandit's Solver</p>
<p>To evaluate the performance of the proposed bandit's solver, we track and summarize the cumulative reward gained by the solvers at the end of the online updating process and summarize them in Table 7.Here, the variants of CbeAL (i.e., CbeAL-2, CbeAL-4, and CbeAL-6) are included along with the top-performing exploitation-oriented agents achieving the highest accuracy on average (the same agent as listed in Table 2).By investigating the results, it shows that under most scenarios, CbeAL-6 will gain the highest cumulative reward.However, it may result in higher standard error in the cumulative reward, which is caused by the ensemble of more agents compared to CbeAL-2 and CbeAL-4.In general, the bandit's solver for CbeAL and its variants are effectively learning to gain higher rewards during the online acquisition process based on the proposed contextual information and reward function.</p>
<p>While the designed reward is not representing the learning performance of the base learner in a straightforward manner (i.e., a higher cumulative reward does not necessarily lead to a better learning performance), we still observe the positive correlation between the cumulative reward and the learning accuracy (Table 2).This also indicates the effectiveness of the designed reward function.</p>
<p>Additionally, the cumulative rewards attained by CbeAL and its variants consistently surpass those by the RAL-agent in most scenarios.This implies the superior performance of the proposed bandit's solver compared to the simple reinforced mechanism in RAL.</p>
<p>We further depict the trend of the cumulative reward of four solvers in Fig 11 .During the early stage, most solvers get more penalties compared to reward while in the later stage, CbeAL-2, CbeAL-4, and CbeAL-6 efficiently gain the higher reward compared to the benchmark.The optimal RAL agent does not make sufficient acquisitions due to the overestimated confidence on the new samples.</p>
<p>Among its variants, CbeAL-6 achieves the best performance in this scenario by making most efficient and informative acquisitions.To validate the third assumption made on the input data distribution, the input X of the first 500 samples in the FDM process is reduced to the two-dimensional space by principal component analysis (PCA) and visualized in Fig. 12.It can be clearly observed that there exist multiple clusters in the class of good samples, which validates the assumption on the online data stream in the case study.</p>
<p>F Assumption on Input Data Distribution</p>
<p>Figure 1 :
1
Figure 1: Distribution of FDM input data with reduced dimensions by Principle Component Analysis (PCA)</p>
<p>Figure 2 :
2
Figure 2: Overview of the proposed CbeAL framework</p>
<p>Figure 3 :
3
Figure 3: Evolution of the base learner's decision boundary in the toy example: The set of blue lines represents the decision boundaries learned by the base learner every 100 time points where the color depth of the line is proportional to time t; The orange line is the ground-truth decision boundary.</p>
<p>Figure 4 :
4
Figure 4: Testing accuracy of CbeAL methods under the learning scenario n = 1000, ds = 0%, sp = 30%, pc = 10%.(a)-(c): Bar charts of the acquisition decisions made by all candidate agents and CbeAL methods; (d) standardized weights of each candidate agent in CbeAL-6.</p>
<p>Figure 5 :
5
Figure 5: Printed part in the case study and the example surfaces, (a) is modified from Huang et al.[2020] with authors' permission</p>
<p>Figure 6 :
6
Figure 6: The average values of training and testing classification accuracy of CbeAL and benchmark methods for the case study reported over 10 replications</p>
<p>(a) Set σ1 = σ2 = 1, change number of points in W and ∥µ1 − µ2∥ 2 .(b) Set σ1 = 1, ∥µ i − µ j ∥ 2 2 = 4, change number of points in W and σ2/σ1 ratio.(c) Set σ1 = 1, change number of points in W and ∥µ1 − µ2∥ 2 .</p>
<p>Figure 7 :
7
Figure 7: Expectation of the probability of acquiring x t by LD-Agent with σ 1 = 1, q = 15 with different ∥µ i − µ j ∥ 2 2 and σ 2 /σ 1 ratio</p>
<p>(a) Set σ1 = σ2 = 1, change number of points in W and ∥µ1 − µ2∥ 2 .(b) Set σ1 = 1, ∥µ i − µ j ∥ 2 2 = 4, change number of points in W and σ2/σ1 ratio.(c) Set σ1 = 1, change number of points in W and ∥µ1 − µ2∥ 2 .</p>
<p>Figure 8 :
8
Figure 8: Expectation of the probability of acquiring x t of the exploration-oriented agent and exploitation-oriented agent with σ 1 = σ 2 = 1, q = 15, vertical axis as ||µ 1 − µ 2 || 2 , and horizontal axis as the window size L</p>
<p>Figure 10: Evolution of the base learner's decision boundary in the toy example: The set of blue lines represents the decision boundaries learned by the base learner every 100 time points where the color depth of the line is proportional to time t; The oragne line is the ground-truth decision boundary</p>
<p>Figure 11 :
11
Figure 11: Cumulative reward gained during the learning scenario n = 400, ds = 0%, sp = 30%, P C = 10%</p>
<p>Figure 12 :
12
Figure 12: Distribution of the first 500 samples with reduced dimensions by PCA in case study</p>
<p>Table 2 :
2
The average values and standard errors (in parenthesis) of classification accuracy in the simulation study reported over 10 replications.Best results (excluding CbeAL-2 and CbeAL-4) are highlighted in bold.
LevelSparsity = 30%Sparsity = 70%PercentageMethodTraining Sample SizeTraining Sample SizeDisturbanceof Positive Samples5001000150050010001500Opt. Explor.60.1% (0.02)61.5% (0.02)63.3% (0.03)69.8% (0.04)72.2% (0.03)69.7% (0.03)Opt. Explor. (Full)60.1% (0.02)61.6% (0.02)63.3% (0.03)69.8% (0.04)72.2% (0.03)69.7% (0.03)Opt. Exploit.58.1% (0.02)70.2% (0.02)70.4% (0.02)67.4% (0.03)72.6% (0.03)73.7% (0.01)10%Opt. Exploit. (Full)58.5% (0.03)70.1% (0.03)70.4% (0.02)68.7% (0.02)72.9% (0.03)74.3% (0.01)CBEAL-258.1% (0.02)70.3% (0.03)  *  71.8% (0.03)  <em>67.5% (0.03)75.9% (0.02)  *  74.3% (0.03)  </em>CBEAL-461.5% (0.03)67.3% (0.03)74.3% (0.02)69.6% (0.03)77.4% (0.02)73.1% (0.03)0%CBEAL-6 Opt. Explor.61.2% (0.02) 73.9% (0.03) 72.5% (0.02) 69.9% (0.03) 73.9% (0.03) 76.7% (0.02) 52.6% (0.01) 60.8% (0.03) 60.5% (0.03) 59.8% (0.02) 64.3% (0.03) 66.3% (0.03)Opt. Explor. (Full)52.6% (0.01)60.8% (0.03)60.5% (0.03)59.8% (0.02)64.3% (0.03)66.2% (0.03)Opt. Exploit.61.0% (0.03)70.2% (0.03)64.6% (0.03)64.5% (0.03)68.4% (0.04)70.5% (0.03)5%Opt. Exploit. (Full)61.1% (0.03)70.2% (0.03)64.6% (0.03)65.2% (0.03)68.8% (0.04)70.6% (0.03)CBEAL-260.6% (0.02)70.4% (0.03)  *  65.2% (0.02)  <em>63.9% (0.03)68.7% (0.04)  </em>67.4% (0.03)CBEAL-460.7% (0.03)67.9% (0.03)64.8% (0.02)63.5% (0.03)67.6% (0.03)66.7% (0.03)CBEAL-665.3% (0.02) 72.0% (0.02) 66.7% (0.02) 68.9% (0.03) 69.4% (0.03)69.0% (0.03)Opt. Explor.60.4% (0.02)60.8% (0.02)62.1% (0.03)62.0% (0.02)62.1% (0.02)71.7% (0.03)Opt. Explor. (Full)60.5% (0.02)60.6% (0.03)62.1% (0.03)62.0% (0.03)62.4% (0.02)71.7% (0.03)Opt. Exploit.68.1% (0.03)69.3% (0.03)72.9% (0.03)68.9% (0.03)69.2% (0.02)75.0% (0.04)10%Opt. Exploit. (Full)60.5% (0.03)69.8% (0.03)73.3% (0.02)68.9% (0.03)69.3% (0.02)75.3% (0.04)CBEAL-267.7% (0.03)72.2% (0.03)  *  73.1% (0.03)  <em>65.1% (0.04)69.3% (0.02)  *  77.5% (0.01)  </em>CBEAL-468.2% (0.03)67.0% (0.03)73.2% (0.03)66.7% (0.03)66.4% (0.02)74.5% (0.03)3%CBEAL-6 Opt. Explor.69.0% (0.04) 71.6% (0.03) 73.8% (0.02) 53.5% (0.01) 57.6% (0.02) 60.8% (0.03)68.0% (0.03) 54.8% (0.02)70.8% (0.02) 79.4% (0.02) 63.9% (0.03) 65.8% (0.02)Opt. Explor. (Full)53.5% (0.01)57.5% (0.02)60.8% (0.03)54.8% (0.02)63.9% (0.03)65.8% (0.02)Opt. Exploit.61.0% (0.03)67.4% (0.03)69.2% (0.02) 62.5% (0.03)70.7% (0.03)77.1% (0.02)5%Opt. Exploit. (Full)62.4% (0.03)67.4% (0.03)69.0% (0.02)62.1% (0.03)71.3% (0.04)78.3% (0.02)CBEAL-261.5% (0.02)  <em>65.2% (0.04)65.6% (0.03)60% (0.03)72.3% (0.03)  *  77.9% (0.02)  </em>CBEAL-457.7% (0.02)63.2% (0.03)65.5% (0.03)58.7% (0.04)72.7% (0.02)73.5% (0.03)CBEAL-661.9% (0.02) 68.3% (0.03)64.5% (0.03)58.8% (0.03)74.6% (0.03) 80.1% (0.03)</p>
<p>Table 3 :
3
The average values and standard errors (in parenthesis) of the classification accuracy in the simulation study reported over 10 replications.Significant best results are highlighted in bold.
LevelSparsity = 30%Sparsity = 70%DisturbancePercentage of Positive SamplesMethod500Training Sample Size 10001500500Training Sample Size 10001500Initial51.4% (0.00)52.3% (0.01)52.2% (0.01)51.0% (0.00)51.1% (0.01)50.9% (0.00)RS52.0% (0.01)57.3% (0.02)55.6% (0.03)57.4% (0.02)60.6% (0.02)62.1% (0.02)US60.6% (0.03)70.3% (0.04)67.9% (0.04)65.6% (0.04)72.8% (0.04)72.4% (0.04)10%DBALStream54.6% (0.01)60.6% (0.02)58.1% (0.02)56.7% (0.01)60.7% (0.02)59.4% (0.02)QBC-PYP51.2% (0.01)67.8% (0.04)68.1% (0.03)63.5% (0.04)70.3% (0.04)66.1% (0.04)CBEAL-661.2% (0.02) 73.9% (0.03) 72.5% (0.02) 69.9% (0.03) 73.9% (0.03) 76.7% (0.02)0%All Training Data Initial76.7% (0.01) 52.5% (0.01)78.9% (0.02) 52.9% (0.01)81.5% (0.01) 51.1% (0.00)77.7% (0.01) 51.7% (0.01)81.5% (0.01) 52.1% (0.01)80.7% (0.01) 51.5% (0.01)RS51.9% (0.01)53.7% (0.01)55.1% (0.02)54.9% (0.02)55.6% (0.03)53.8% (0.01)US62.2% (0.04)70.3% (0.04)61.6% (0.04)69.8% (0.03)66.6% (0.04)68.3% (0.04)5%DBALStream54.7% (0.01)56.3% (0.00)54.2% (0.01)54.4% (0.01)55.2% (0.01)54.6% (0.01)QBC-PYP53.4% (0.03)61.9% (0.03)60.2% (0.03)58.0% (0.03)64.0% (0.06)63.8% (0.05)CBEAL-665.3% (0.02) 72.0% (0.02) 66.7% (0.02)68.9% (0.03)69.4% (0.03) 69.0% (0.03)All Training Data75.1% (0.02)78.7% (0.01)78.4% (0.01)77.7% (0.01)80.2% (0.01)80.7% (0.01)Initial51.9% (0.01)51.3% (0.01)50.7% (0.00)51.1% (0.00)50.9% (0.00)51.2% (0.00)RS53.0% (0.01)56.2% (0.02)57.7% (0.01)54.3% (0.01)53.8% (0.01)60.6% (0.03)US60.7% (0.04)68.0% (0.04)70.5% (0.04)67.6% (0.04)66.2% (0.04)71.0% (0.05)10%DBALStream56.9% (0.02)60.1% (0.03)60.7% (0.02)56.1% (0.01)57.1% (0.02)61.3% (0.02)QBC-PYP61.2% (0.03)56.8% (0.03)68.8% (0.04)64.5% (0.04)63.6% (0.04)66.4% (0.04)CBEAL-669.0% (0.04) 71.6% (0.03) 73.8% (0.02) 68.0% (0.03) 70.8% (0.02) 79.4% (0.02)3%All Training Data Initial79.2% (0.02) 52.2% (0.01)78.8% (0.01) 53.2% (0.01)81.0% (0.01) 54.1% (0.01)80.9% (0.01) 51.5% (0.01)76.4% (0.01) 51.2% (0.01)81.1% (0.01) 51.5% (0.00)RS52.9% (0.01)54.4% (0.01)52.8% (0.02)53.2% (0.02)54.5% (0.02)52.6% (0.01)US59.4% (0.03)67.9% (0.04)68.0% (0.03)59.9% (0.04)62.6% (0.04)73.9% (0.04)5%DBALStream54.3% (0.01)56.0% (0.01)54.7% (0.01)55.6% (0.01)55.1% (0.01)57.6% (0.02)QBC-PYP62.3% (0.04)58.1% (0.04)62.5% (0.04)60.2% (0.04)64.2% (0.04)61.3% (0.04)CBEAL-661.9% (0.02)68.3% (0.03)64.5% (0.03)58.8% (0.03)74.6% (0.03) 80.1% (0.03)All Training Data73.4% (0.02)78.0% (0.01)76.6% (0.01)73.1% (0.02)78.1% (0.01)80.5% (0.01)</p>
<p>Table 4 :
4
Agent Set Adopted in CbeAL
Pair Index Agent Index AgentHyperparameters4AG 7 AG 8LD 3 L = 100, δ L = 0.005</p>
<p>Table 5 :
5
The average values and standard errors (in parenthesis) of the classification accuracy in the simulation study over 10 replications.Significant best results are highlighted in bold.
LevelSparsity = 30%Sparsity = 70%PercentageMethodTraining Sample SizeTraining Sample SizeDisturbanceof Positive Samples5001000150050010001500CBEAL-258.1% (0.02)70.3% (0.03)71.8% (0.03)67.5% (0.03)75.9% (0.02)74.3% (0.03)CBEAL-4 61.5% (0.03)67.3% (0.03)74.3% (0.02)69.6% (0.03)77.4% (0.02)73.1% (0.03)5%CBEAL-661.2% (0.02)73.9% (0.03)72.5% (0.02)69.9% (0.03)73.9% (0.03)76.7% (0.02)CBEAL-4<em>59.6% (0.03)71.2% (0.03)71.9% (0.02)68.4% (0.03)75.3% (0.02)76.5% (0.03)0%CBEAL-8 CBEAL-258.6% (0.02) 60.6% (0.02)65.2% (0.03) 70.4% (0.03)70.2% (0.02) 65.2% (0.03)69.2% (0.03) 63.9% (0.03)76.2% (0.03) 68.7% (0.04)76.7% (0.03) 67.4% (0.03)CBEAL-460.7% (0.03)67.9% (0.03)64.8% (0.02)63.5% (0.03)67.6% (0.03)66.7% (0.03)10%CBEAL-6 65.3% (0.02) 72.0% (0.02) 66.7% (0.02)68.9% (0.03)69.4% (0.03)69.0% (0.03)CBEAL-4</em>60.9% (0.03)70.1% (0.03)66.4% (0.03)60.1% (0.03)69.5% (0.03)65.7% (0.03)CBEAL-863.4% (0.03)70.2% (0.03)64.9% (0.02)69.2% (0.03)68.6% (0.03)75.0% (0.03)CBEAL-267.7% (0.03)72.2% (0.03)73.1% (0.03)65.1% (0.04)69.3% (0.02)77.5% (0.01)CBEAL-468.2% (0.03)67.0% (0.03)73.2% (0.03)66.7% (0.03)66.4% (0.02)74.5% (0.03)5%CBEAL-6 69.0% (0.04)71.6% (0.03)73.8% (0.02) 68.0% (0.03) 70.8% (0.02) 79.4% (0.02)CBEAL-4<em>68.7% (0.03)72.2% (0.03)70.2% (0.03)66.8% (0.03)64.2% (0.02)74.8% (0.03)3%CBEAL-8 CBEAL-266.7% (0.03) 61.5% (0.02)71.4% (0.03) 65.2% (0.04)69.3% (0.03) 65.6% (0.03) 60.0% (0.03) 67.7% (0.03)66.8% (0.03) 72.3% (0.03)75.5% (0.03) 77.9% (0.02)CBEAL-457.7% (0.02)63.2% (0.03)65.5% (0.03)58.7% (0.04)72.7% (0.02)73.5% (0.03)10%CBEAL-6 61.9% (0.02) 68.3% (0.03)64.5% (0.03)58.8% (0.03)74.6% (0.03) 80.1% (0.03)CBEAL-4</em>60.2% (0.02)64.3% (0.03)65.2% (0.03)56.8% (0.03)71.2% (0.02)74.2% (0.03)CBEAL-860.5% (0.03)64.5% (0.03)64.5% (0.03)55.8% (0.03)73.2% (0.02)77.5% (0.03)</p>
<p>Table 6 :
6
The average values and standard errors (in parenthesis) of the number of the acquired samples over 10 replications.The smallest numbers are highlighted in bold.
LevelSparsity = 30%Sparsity = 70%PercentageMethodSize of Data StreamSize of Data StreamDisturbanceof Positive Samples5001000150050010001500Opt. Explor.48.00 (0.00)97.80 (0.19)148.00 (0.00)48.00 (0.00)98.00 (0.00)148.00 (0.00)10%Opt. Exploit. 37.20 (3.30) 81.00 (3.18) 108.00 (4.06) 44.00 (1.05) 79.60 (6.47) 114.70 (2.55) CBEAL-2 45.60 (2.28) 91.90 (2.18) 119.20 (9.57) 47.60 (0.38) 98.00 (0.00) 123.20 (9.53)0%CBEAL-6 Opt. Explor.45.80 (1.42) 48.00 (0.00)89.20 (3.25) 98.00 (0.00)132.30 (2.42) 147.40 (0.57)48.00 (0.00) 48.00 (0.00)88.20 (4.31) 98.00 (0.00)124.00 (5.78) 147.00 (0.95)5%Opt. Exploit. 30.90 (2.68) 58.00 (3.14) 71.80 (5.26) 37.50 (2.38) 59.20 (3.55) 80.10 (3.51) CBEAL-2 40.30 (2.83) 68.70 (2.48) 78.20 (6.11) 43.60 (1.86) 70.60 (3.80) 83.70 (5.56)CBEAL-644.10 (1.78)68.10 (2.73)83.40 (2.78)43.60 (1.02)68.70 (4.35)83.20 (4.19)Opt. Explor.47.40 (0.57)97.70 (0.29)148.00 (0.00)48.00 (0.00)97.80 (0.19)148.00 (0.00)10%Opt. Exploit. 41.50 (3.28) 71.90 (3.86) 109.10 (6.25) 44.80 (0.99) 81.40 (6.74) 102.50 (9.60) CBEAL-2 48.00 (0.00) 90.70 (0.95) 120.60 (9.60) 48.00 (0.00) 97.00 (0.63) 127.60 (5.84)3%CBEAL-6 Opt. Explor.47.20 (0.76) 47.70 (0.20)87.80 (6.37) 97.00 (0.95)136.90 (3.50) 148.00 (0.00)43.70 (1.91) 92.40 (3.30) 47.70 (0.20) 98.00 (0.00)129.20 (6.65) 148.00 (0.00)5%Opt. Exploit. 30.10 (1.78) 51.80 (4.29) CBEAL-2 40.90 (1.57) 64.60 (4.18)84.10 (2.72) 89.80 (6.87)30.10 (1.78) 56.90 (4.48) 86.20 (4.10) 40.90 (1.57) 73.10 (3.97) 101.60 (2.91)CBEAL-638.70 (2.87)61.60 (4.10)83.00 (5.99)38.70 (2.81)65.60 (4.52)87.60 (4.56)</p>
<p>Table 7 :
7
The average values and standard errors (in parenthesis) of cumulative reward over 10 replications.The smallest numbers are highlighted in bold.
LevelSparsity = 30%Sparsity = 70%PercentageMethodSize of Data StreamSize of Data StreamDisturbanceof Positive Samples5001000150050010001500CBEAL-211.15 (0.59) 14.05 (1.39)20.85 (1.54)10.30 (1.07)12.05 (1.64)22.25 (2.00)10%CBEAL-4 CBEAL-611.50 (0.98) 9.70 (0.78)15.00 (0.79) 23.60 (2.06) 11.65 (0.84) 15.30 (1.36) 22.05 (1.40) 13.35 (1.41) 13.65 (2.51) 25.00 (2.66) 10.50 (2.07) 26.75 (1.63)0%Opt. Exploit. CBEAL-27.80 (0.97) 8.55 (0.83)11.35 (1.45) 9.95 (1.80)19.30 (2.03) 18.90 (1.04)10.50 (1.52) 8.10 (1.17)11.25 (2.20) 9.30 (0.99)18.65 (2.14) 19.40 (1.99)5%CBEAL-4 CBEAL-68.80 (0.47) 9.45 (0.53) 10.15 (1.38) 22.95 (1.46) 8.35 (0.92) 10.90 (1.24) 21.60 (1.15) 9.35 (1.41) 20.35 (1.24) 5.80 (0.75) 9.85 (1.00) 20.05 (1.34)Opt. Exploit.7.96 (0.68)9.15 (1.28)19.65 (1.65)8.12 (0.96)9.85 (1.13)20.56 (1.27)CBEAL-213.65 (1.03)15.35 (1.44)22.40 (1.37)10.85 (0.91)15.55 (0.83)26.80 (2.50)10%CBEAL-4 CBEAL-614.10 (0.67) 17.95 (1.56) 20.70 (2.06) 14.40 (0.90) 15.15 (0.94) 24.35 (1.11) 12.25 (0.59) 16.70 (1.29) 29.10 (2.03) 11.70 (0.88) 14.75 (1.22) 26.40 (1.83)3%Opt. Exploit. 10.25 (0.79) CBEAL-2 7.95 (0.77)15.05 (1.36) 11.80 (1.31)22.56 (1.26) 12.80 (0.96) 16.40 (1.37) 12.45 (1.12) 8.15 (0.95) 10.90 (1.10)25.60 (2.25) 17.70 (1.82)5%CBEAL-4 CBEAL-68.25 (0.91) 8.90 (0.81) 14.45 (1.59) 14.85 (1.31) 14.40 (1.82) 11.24 (2.05)8.20 (0.71) 8.80 (0.81)11.45 (1.32) 12.10 (1.12) 19.80 (2.02) 16.50 (2.23)Opt. Exploit.6.85 (1.03)13.65 (1.70)14.65 (1.75)9.25 (0.85) 12.90 (1.27) 16.40 (2.22)
where i = 1, . . ., N, t = 1, 2, . . . .We found that this will only affect the bound of σi in Lemma 3 in Exp4.P[Beygelzimer et al., 2011], where σi .t=1 vi,t .However, this will not affect the results of Lemma 3 or other theorems in the paper[Beygelzimer et al., 2011].Thus, the proposed Exp4.P-EWMA will enjoy the same regret bound as Exp4.P, theoretically.Considering the integrity, we summarize Theorem 2, Lemma 3, and Lemma 4 in[Beygelzimer et al., 2011]in this section.The main result Theorem 2 is proved by Lemma 3, and Lemma 4: Theorem 2. Assume that ln(N/δ) ≤ KT , and that the set of experts includes one which, on each round, selects an action uniformly at random.Then, with probability at least 1 − δ,where G is the cumulative reward of the solver.Lemma 3.Under the conditions of Theorem 2,Pr ∃i :Lemma 4.Under the conditions of Theorem 2,where Û = max i Ĝi + σi • ln(N/δ) .D Empirical Justification on Exp4.p-EWMATo provide empirical justification for adding the EWMA-based flipping mechanism, we visualize the standardized weight of each agent in CbeAL without the flipping mechanism (i.e., solved by Exp4.P) under the learning scenario n = 1000, ds = 0%, sp = 30%, P C = 10% in Fig.9.
Leveraging active learning and conditional mutual information to minimize data annotation in human activity recognition. Rebecca Adaimi, Edison Thomaz, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies. the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies20193</p>
<p>Digital twin-driven supervised machine learning for the development of artificial intelligence applications in manufacturing. Nikolaos Kosmas Alexopoulos, George Nikolakis, Chryssolouris, International Journal of Computer Integrated Manufacturing. 3352020</p>
<p>The nonstochastic multiarmed bandit problem. Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, Robert E Schapire, SIAM journal on computing. 3212002</p>
<p>Bandits with knapsacks. Ashwinkumar Badanidiyuru, Robert Kleinberg, Aleksandrs Slivkins, Journal of the ACM (JACM). 6532018</p>
<p>Margin based active learning. Maria-Florina Balcan, Andrei Broder, Tong Zhang, International Conference on Computational Learning Theory. Springer2007</p>
<p>Online choice of active learning algorithms. Yoram Baram, Ran El Yaniv, Kobi Luz, Journal of Machine Learning Research. 5Mar. 2004</p>
<p>An online sparse estimation-based classification approach for real-time monitoring in advanced manufacturing processes from heterogeneous sensor data. Kaveh Bastani, Prahalad K Rao, Zhenyu Kong, IIE Transactions. 4872016</p>
<p>Stochastic multi-armed-bandit problem with nonstationary rewards. Omar Besbes, Yonatan Gur, Assaf Zeevi, Advances in neural information processing systems. 201427</p>
<p>Importance weighted active learning. Alina Beygelzimer, Sanjoy Dasgupta, John Langford, Proceedings of the 26th annual international conference on machine learning. the 26th annual international conference on machine learning2009</p>
<p>Contextual bandit algorithms with supervised learning guarantees. Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, Robert Schapire, Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics. the Fourteenth International Conference on Artificial Intelligence and Statistics2011JMLR Workshop and Conference Proceedings</p>
<p>Exploration vs. exploitation in active learning: A bayesian approach. Alexis Bondu, Vincent Lemaire, Marc Boullé, The 2010 International Joint Conference on Neural Networks (IJCNN). IEEE2010</p>
<p>A survey of predictive modeling on imbalanced domains. Paula Branco, Luís Torgo, Rita P Ribeiro, ACM Computing Surveys (CSUR). 4922016</p>
<p>Maximizing expected model change for active learning in regression. Wenbin Cai, Ya Zhang, Jun Zhou, arXiv:2105.01622Nicholas Carlini. Poisoning the unlabeled dataset of semi-supervised learning. IEEE2013. 2021arXiv preprint2013 IEEE 13th International Conference on Data Mining</p>
<p>Tensorflow data validation: Data analysis and validation in continuous ml pipelines. Emily Caveness, Paul Suganthan, G C , Zhuo Peng, Neoklis Polyzotis, Sudip Roy, Martin Zinkevich, Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data. the 2020 ACM SIGMOD International Conference on Management of Data2020</p>
<p>Bayesian online classifiers for text classification and filtering. Kian Ming, Adam Chai, Hai Leong Chieu, Hwee Tou Ng, Proceedings of the 25th annual international ACM SIGIR conference on Research and Development in Information Retrieval. the 25th annual international ACM SIGIR conference on Research and Development in Information Retrieval2002</p>
<p>Multiscale Quantitative Analytics of Human Visual Searching Tasks. Xiaoyu Chen, Jul 2021Virginia TechPhD thesis</p>
<p>Variation analysis and visualization of manufacturing processes via augmented reality. Xiaoyu Chen, Hongyue Sun, Ran Jin, IIE Annual Conference. Proceedings. 2016</p>
<p>Inn: An interpretable neural network for ai incubation in manufacturing. Xiaoyu Chen, Yingyan Zeng, Sungku Kang, Ran Jin, ACM Transactions on Intelligent Systems and Technology (TIST). 1352022</p>
<p>Industrial cyberphysical systems: A backbone of the fourth industrial revolution. Armando W Colombo, Stamatis Karnouskos, Okyay Kaynak, Yang Shi, Shen Yin, IEEE Industrial Electronics Magazine. 1112017</p>
<p>Exact distribution for the product of two correlated gaussian random variables. Guolong Cui, Xianxiang Yu, Salvatore Iommelli, Lingjiang Kong, IEEE Signal Processing Letters. 23112016</p>
<p>A smart data annotation tool for multi-sensor activity recognition. Alexander Diete, Timo Sztyler, Heiner Stuckenschmidt, 2017 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops). IEEE2017</p>
<p>Ralf: A reinforced active learning formulation for object class recognition. Sandra Ebert, Mario Fritz, Bernt Schiele, 2012 IEEE Conference on Computer Vision and Pattern Recognition. IEEE2012</p>
<p>A novel active learning regression framework for balancing the exploration-exploitation trade-off. Dina Elreedy, Amir F Atiya, Samir I Shaheen, Entropy. 2176512019</p>
<p>Confidence interval estimation by bootstrap method for uncertainty quantification using random sampling method. Tomohiro Endo, Tomoaki Watanabe, Akio Yamamoto, Journal of Nuclear Science and Technology. 527-82015</p>
<p>Design of experiments. Aylmer Ronald, Fisher, Br Med J. 139231936</p>
<p>Sur la loi de probabilité de l'écart maximum. Maurice Fréchet, Ann. Soc. Math. Polon. 61927</p>
<p>A survey on instance selection for active learning. Yifan Fu, Xingquan Zhu, Bin Li, Knowledge and information systems. 3522013</p>
<p>Application of supervised machine learning for defect detection during metallic powder bed fusion additive manufacturing using high resolution imaging. Christian Gobert, Edward W Reutzel, Jan Petrich, Shashi Abdalla R Nassar, Phoha, 2018Additive Manufacturing21</p>
<p>Support vector machines for classification and regression. Steve R Gunn, ISIS technical report. 1411998</p>
<p>Data quality for machine learning tasks. Nitin Gupta, Shashank Mujumdar, Hima Patel, Satoshi Masuda, Naveen Panwar, Sambaran Bandyopadhyay, Sameep Mehta, Shanmukha Guttula, Shazia Afzal, Ruhi Sharma Mittal, Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining. the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining2021</p>
<p>Design of experiments for the nips 2003 variable selection benchmark. I Guyon, 2003</p>
<p>Fault detection using the k-nearest neighbor rule for semiconductor manufacturing processes. Peter He, Jin Wang ; Hsuan-Tien Lin, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2007. 201520Active learning by learning</p>
<p>Detecting cognitive hacking in visual inspection with physiological measurements. Wenyan Huang, Xiaoyu Chen, Ran Jin, Nathan Lau, Applied ergonomics. 841030222020</p>
<p>The exponentially weighted moving average. Hunter Stuart, Journal of quality technology. 1841986</p>
<p>High density-focused uncertainty sampling for active learning over evolving stream data. Dino Ienco, Indrė Žliobaitė, Bernhard Pfahringer, Proceedings of the 3rd International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications. the 3rd International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and ApplicationsPMLR2014</p>
<p>Ensemble modeling for data fusion in manufacturing process scale-up. Ran Jin, Xinwei Deng, IIE Transactions. 4732015</p>
<p>Sequential measurement strategy for wafer geometric profile estimation. Ran Jin, Chia-Jung Chang, Jianjun Shi, Iie transactions. 4412012</p>
<p>Dynamic quality-process model in consideration of equipment degradation. Ran Jin, Xinwei Deng, Xiaoyu Chen, Liang Zhu, Jun Zhang, Journal of Quality Technology. 5132019</p>
<p>Computer aided design of experiments. W Ronald, Larry A Kennard, Stone, Technometrics. 1111969</p>
<p>Semi-supervised regression: A recent review. David G Kleinbaum, M Dietz, Mitchel Gail, Mitchell Klein, Klein, Georgios Kostopoulos, Stamatis Karlos, Sotiris Kotsiantis, and Omiros Ragos. Springer2002. 201835Logistic regression</p>
<p>Sequential adaptive designs in computer experiments for response surface model fit. Chen Quin, Lam , 2008The Ohio State UniversityPhD thesis</p>
<p>A sequential algorithm for training text classifiers. D David, William A Lewis, Gale, SIGIR'94. Springer1994</p>
<p>Integration of physically-based and data-driven approaches for thermal field prediction in additive manufacturing. Jingran Li, Ran Jin, Hang Yu, Materials &amp; Design. 1392018</p>
<p>Cluster-based data filtering for manufacturing big data systems. Yifu Li, Xinwei Deng, Shan Ba, William A William R Myers, Steve J Brenneman, Ron Lange, Ran Zink, Jin, Journal of Quality Technology. 2021</p>
<p>Stratified sampling meets machine learning. Edo Liberty, Kevin Lang, Konstantin Shmakov, International conference on machine learning. PMLR2016</p>
<p>Stream-based active unusual event detection. Chen Change Loy, Asian Conference on Computer Vision. Springer2010Tao Xiang, and Shaogang Gong</p>
<p>Stream-based joint exploration-exploitation active learning. Chen Change Loy, Timothy M Hospedales, Tao Xiang, Shaogang Gong, 2012 IEEE Conference on Computer Vision and Pattern Recognition. IEEE2012</p>
<p>Quadratic forms in random variables: theory and applications. M Arakaparampil, Serge B Mathai, Provost, 1992Dekker</p>
<p>Balancing exploration and exploitation: A new algorithm for active machine learning. Thomas Osugi, Deng Kim, Stephen Scott, Fifth IEEE International Conference on Data Mining (ICDM'05). IEEE20058</p>
<p>Scikit-learn: Machine learning in Python. F Pedregosa, G Varoquaux, A Gramfort, V Michel, B Thirion, O Grisel, M Blondel, P Prettenhofer, R Weiss, V Dubourg, J Vanderplas, A Passos, D Cournapeau, M Brucher, M Perrot, E Duchesnay, Journal of Machine Learning Research. 122011</p>
<p>Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. John Platt, Advances in large margin classifiers. 1031999</p>
<p>Online real-time quality monitoring in additive manufacturing processes using heterogeneous sensors. Jia Prahalad K Rao, David Peter Liu, Zhenyu James Roberson, Christopher Kong, Williams, Journal of Manufacturing Science and Engineering. 13762015</p>
<p>Active learning. Burr Settles, Synthesis lectures on artificial intelligence and machine learning. 612012</p>
<p>Query by committee. Sebastian Seung, Manfred Opper, Haim Sompolinsky, Proceedings of the fifth annual workshop on Computational learning theory. the fifth annual workshop on Computational learning theory1992</p>
<p>Fully-sequential space-filling design algorithms for computer experiments. Boyang Shang, Daniel W Apley, Journal of Quality Technology. 5322021</p>
<p>A human-centered wearable sensing platform with intelligent automated data annotation capabilities. Roger Solis, Arash Pakbin, Ali Akbari, J Bobak, Roozbeh Mortazavi, Jafari, Proceedings of the International Conference on Internet of Things Design and Implementation. the International Conference on Internet of Things Design and Implementation2019</p>
<p>Constrained maximin designs for computer experiments. Erwin Stinstra, Peter Dick Den Hertog, Arjen Stehouwer, Vestjens, Technometrics. 4542003</p>
<p>. Eliza Andrew Strickland, Ng, Accessed: 2022-02-182022Unbiggen AI</p>
<p>Logistic regression for crystal growth process modeling through hierarchical nonnegative garrote-based variable selection. Hongyue Sun, Xinwei Deng, Kaibo Wang, Ran Jin, IIE Transactions. 4882016</p>
<p>Quality modeling of printed electronics in aerosol jet printing based on microscopic images. Hongyue Sun, Kan Wang, Yifu Li, Chuck Zhang, Ran Jin, Journal of Manufacturing Science and Engineering. 13972017</p>
<p>Exploration in active learning. Handbook of Brain Science and Neural Networks. Sebastian Thrun, 1995</p>
<p>Learning to learn. Sebastian Thrun, Lorien Pratt, 2012Springer Science &amp; Business Media</p>
<p>Statistically nonrepresentative stratified sampling: A sampling technique for qualitative studies. Jan E Trost, Qualitative sociology. 911986</p>
<p>Better data labelling with emblem (and how that impacts defect prediction. Huy Tu, Zhe Yu, Tim Menzies, IEEE Transactions on Software Engineering. 2020</p>
<p>Large-scale online multitask learning and decision making for flexible manufacturing. Junping Wang, Yunchuan Sun, Wensheng Zhang, Ian Thomas, Shihui Duan, Youkang Shi, IEEE Transactions on Industrial Informatics. 1262016</p>
<p>Online computation performance analysis for distributed machine learning pipelines in fog manufacturing. Lening Wang, Yutong Zhang, Xiaoyu Chen, Ran Jin, 2020 IEEE 16th International Conference on Automation Science and Engineering (CASE). IEEE2020</p>
<p>Pyramid ensemble convolutional neural network for virtual computed tomography image prediction in a selective laser melting process. Lening Wang, Xiaoyu Chen, Daniel Henkel, Ran Jin, Journal of Manufacturing Science and Engineering. 143121210032021a</p>
<p>Moss-multi-modal best subset modeling in smart manufacturing. Lening Wang, Pang Du, Ran Jin, Sensors. 2112432021b</p>
<p>Active learning in multimedia annotation and retrieval: A survey. Meng Wang, Xian-Sheng Hua, ACM Transactions on Intelligent Systems and Technology (TIST). 222011</p>
<p>Learning with uncertainty. Xizhao Wang, Junhai Zhai, 2016CRC Press</p>
<p>Ral-improving stream-based active learning by reinforcement learning. Sarah Wassermann, Thibaut Cuvelier, Pedro Casas, European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD) Workshop on Interactive Adaptive Learning (IAL). 2019</p>
<p>Recent advances in prognostics and health management for advanced manufacturing paradigms. Tangbin Xia, Yifan Dong, Lei Xiao, Shichang Du, Ershun Pan, Lifeng Xi, Reliability Engineering &amp; System Safety. 1782018</p>
<p>Akm2d: An adaptive framework for online sensing and anomaly quantification. Kamran Hao Yan, Jianjun Paynabar, Shi, IISE Transactions. 5292020</p>
<p>Fog computing for distributed family learning in cyber-manufacturing modeling. Yutong Zhang, Lening Wang, Xiaoyu Chen, Ran Jin, 2019 IEEE International Conference on Industrial Cyber Physical Systems (ICPS). IEEE2019</p>
<p>Oui! outlier interpretation on multi-dimensional data via visual analytics. Xun Zhao, Weiwei Cui, Yanhong Wu, Haidong Zhang, Huamin Qu, Dongmei Zhang, Computer Graphics Forum. Wiley Online Library201938</p>
<p>Introduction to semi-supervised learning. Xiaojin Zhu, Andrew B Goldberg, Synthesis lectures on artificial intelligence and machine learning. 312009</p>            </div>
        </div>

    </div>
</body>
</html>
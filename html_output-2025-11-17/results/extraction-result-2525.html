<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2525 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2525</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2525</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-68.html">extraction-schema-68</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <p><strong>Paper ID:</strong> paper-261557055</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2309.02726v3.pdf" target="_blank">Large Language Models for Automated Open-domain Scientific Hypotheses Discovery</a></p>
                <p><strong>Paper Abstract:</strong> Hypothetical induction is recognized as the main reasoning type when scientists make observations about the world and try to propose hypotheses to explain those observations. Past research on hypothetical induction is under a constrained setting: (1) the observation annotations in the dataset are carefully manually handpicked sentences (resulting in a close-domain setting); and (2) the ground truth hypotheses are mostly commonsense knowledge, making the task less challenging. In this work, we tackle these problems by proposing the first dataset for social science academic hypotheses discovery, with the final goal to create systems that automatically generate valid, novel, and helpful scientific hypotheses, given only a pile of raw web corpus. Unlike previous settings, the new dataset requires (1) using open-domain data (raw web corpus) as observations; and (2) proposing hypotheses even new to humanity. A multi-module framework is developed for the task, including three different feedback mechanisms to boost performance, which exhibits superior performance in terms of both GPT-4 based and expert-based evaluation. To the best of our knowledge, this is the first work showing that LLMs are able to generate novel (''not existing in literature'') and valid (''reflecting reality'') scientific hypotheses.</p>
                <p><strong>Cost:</strong> 0.03</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2525.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2525.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MOOSE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-mOdule framewOrk with paSt present future feEdback (MOOSE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodule LLM-prompting framework for automated open-domain scientific hypothesis discovery in social sciences that composes retrieval, multi-stage generation modules, and three iterative feedback mechanisms (present-, past-, future-feedback).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MOOSE</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A pipeline of serializable modules (Background Finder, Inspiration Title Finder, Inspiration Finder, Hypothesis Suggestor, Hypothesis Proposer) that ingest raw web corpus and optionally related surveys; modules are implemented with LLM prompting (gpt-3.5-turbo in main experiments; also tested with Claude3-Opus). Three feedback mechanisms are integrated: present-feedback (LLM-based evaluators produce reality/novelty/clarity feedback used to iteratively refine hypotheses), past-feedback (future-module evaluation used to retroactively refine earlier module outputs, implemented for inspiration title selection with a heuristics-based rule in practice), and future-feedback (FF1: propagate justifications; FF2: insert a preparatory module Hypothesis Suggestor to offload reasoning). Retrieval (title-level BM25 for tractability) is used to nominate inspirations, and novelty checking consults related surveys.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based; retrieval-augmented multi-module pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Social science / Business (dataset: recent social science papers)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Generate backgrounds and inspiration candidates by retrieval over raw web corpus; feed selected background+inspirations into Hypothesis Suggestor (FF2) to produce preliminary suggestions; Hypothesis Proposer (LLM prompt) composes final hypotheses; iterative present-feedback loops use outputs of Reality/Novelty/Clarity checkers to refine hypothesis generations.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Novelty Checker module implemented via LLM(s) that consult related survey corpus S (retrieved with BM25) to judge whether the hypothesis or sub-arguments are present in literature; also uses novelty Likert scoring (5-point) by GPT-4 and human experts during evaluation. Ablation: removing access to surveys increases reported novelty and reduces validness.</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Reality Checker module (LLM-based) assesses whether hypothesis 'reflects reality' (philosophical validness); evaluation also performed via GPT-4 automatic scoring and human expert Likert ratings on validness.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Explicit trade-off acknowledged: framework treats novelty as relatively more important for TOMATO task; past-feedback heuristics can favor novelty (encouraging less-related inspirations), whereas Reality Checker/present-feedback moderates plausibility; tuning achieved by which feedback signals are emphasized and survey access to novelty checker.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Three primary metrics: Validness, Novelty, Helpfulness — each scored on a 5-point Likert scale by GPT-4 and by human experts (PhD students). These are the primary pre-experimental quality metrics used.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Automatic evaluation with GPT-4 using the three metrics (validness, novelty, helpfulness) and human expert evaluation by three social-science PhD students on sampled hypotheses; ablation studies and GPT-3.5 vs Claude3-Opus base-model comparisons; no wet-lab or external experimental validation reported (validation limited to expert assessment and LLM-based checks).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Dataset, code, and generated hypotheses are released on GitHub per paper statement; Algorithm 1 and modular design described in appendices. Dataset of 50 recent social science papers with collected raw web passages provided.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Reality Checker and present-feedback loops that use LLM evaluators to detect and provide corrective feedback; future-feedback (justifications) aim to ground later modules; reliance on related surveys and retrieval attempts to ground novelty checks.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Reality Checker (LLM-based) flags conflicts with reality; expert human evaluation and GPT-4 scoring are used as post-generation detectors of implausible or false claims. The paper also references 'check your facts' style approaches in related work but does not integrate external fact-checking tools beyond the described modules.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>TOMATO dataset: 50 recent social science papers (post-Jan 2023) with collected raw web passages and related survey corpus S.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported mean scores (5-point) comparing methods (GPT-3.5 base unless noted): GPT-4 evaluation — Baseline (Validness 3.954, Novelty 2.483, Helpfulness 3.489); MOOSE-base (3.907, 3.081, 3.859); MOOSE-base w/ future-feedback (3.955, 3.226, 3.953); MOOSE-base w/ future+past-feedback (3.916, 3.390, 3.931). Expert evaluation (selected iteration): Baseline (3.579, 2.276, 2.632); MOOSE-base (3.500, 2.855, 3.026); MOOSE w/ future-feedback (3.645, 3.105, 3.303); MOOSE w/ future+past-feedback (3.750, 3.197, 3.368). Present-feedback iterations steadily improved metrics up to a point.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>MOOSE-base substantially outperforms a simple LLM baseline that directly inputs corpus chunks and outputs hypotheses in novelty and helpfulness, though validness is similar or slightly lower in some automatic evaluations; adding future- and past-feedback further improves novelty/helpfulness (detailed numbers above).</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td>Paper reports many generated hypotheses judged by human experts to be novel ('not existing in literature') and plausible; no experimental (empirical) scientific discoveries were externally validated beyond expert/LMM evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Dataset limited to 50 social science papers; domain focused on social science/business so generalization to natural sciences is untested; reliance on LLM base models (gpt-3.5/Claude3) introduces dependence on model capabilities and possible biases/hallucinations; some feedback modules required heuristics because existing LLMs (ChatGPT) produced poor-quality past-feedback; no experimental/empirical confirmation of machine-proposed hypotheses beyond expert judgment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Automated Open-domain Scientific Hypotheses Discovery', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2525.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2525.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Present-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Present-feedback (iterative LLM evaluation and refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based iterative feedback mechanism where a module's output is immediately evaluated along multiple aspects (reality, novelty, clarity) by LLM evaluators and that feedback is fed back into the same module to regenerate improved outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Present-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>After a module (notably the Hypothesis Proposer) produces output o_i, auxiliary LLM-based checkers (Reality Checker, Novelty Checker, Clarity Checker) produce feedback f_i on multiple aspects. The module then consumes o_i and f_i to regenerate o_i in iterative cycles. Implemented as over-generate-then-refine prompting with multiple iterations; applied primarily to hypothesis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based iterative refinement</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Social science / Business (used within TOMATO experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Not a generation model itself but an iterative scheme applied to Hypothesis Proposer to improve generation quality via LLM feedback loops.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Novelty Checker LLM that consults related surveys and returns novelty feedback; used to guide regeneration.</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Reality Checker LLM provides feedback on whether hypotheses reflect reality; used directly in iterative refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Iterative process can increase both novelty and validness but paper notes trade-offs and an optimal number of iterations (helpfulness peaked around 3 iterations in GPT-4 eval).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Measured by changes in Validness, Novelty, Helpfulness (5-point Likert) across iterations; numerical trend provided (see performance).</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Monitored via automatic GPT-4 evaluation and human expert scoring of hypotheses after 0..N iterations; empirical tables show improvement trends.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>By feeding reality-check feedback into regeneration loops, present-feedback aims to reduce hallucinated or implausible claims.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Reality Checker and expert/GPT-4 evaluation used to detect implausible content during iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>TOMATO dataset</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>GPT-4-evaluated iteration trend (averaged over several MOOSE variants): w/o present-feedback (Validness 3.823, Novelty 3.114, Helpfulness 3.809); 1 iter (3.918, 3.199, 3.900); 2 iters (3.951, 3.293, 3.956); 3 iters (3.969, 3.270, 3.962); 4 iters (3.970, 3.329, 3.951). Expert-evaluated trends also show increases up to an optimal iteration count.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Applying present-feedback to MOOSE variants steadily improves hypothesis quality metrics relative to the same framework without such iterations; baseline (simple LLM) lacks this iterative refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires reliable LLM evaluators — quality depends on evaluator capability; too many iterations may plateau or harm certain metrics; can be computationally expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Automated Open-domain Scientific Hypotheses Discovery', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2525.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2525.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Past-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Past-feedback (retroactive feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A feedback mechanism where outputs of earlier modules are refined using evaluative signals that can only be computed after downstream modules produce their outputs (i.e., retroactive feedback from future evaluations).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Past-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>When a module's output o_i cannot be evaluated immediately, the framework runs forward to produce a downstream output o_j that depends on o_i, obtains present-feedback f_j on o_j, and then uses o_i, o_j, and f_j (optionally through an additional LLM module) to produce past-feedback f_i to refine o_i. Implemented for the Inspiration Title Finder in MOOSE; in practice a heuristics-based past-feedback ('less related inspirations → more novelty') was used because ChatGPT-produced inspiration feedback was low quality.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based + heuristics retroactive feedback</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Social science / Business</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Retroactive refinement of earlier retrieval/selection stages (e.g., inspiration titles) informed by downstream hypothesis evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Uses novelty feedback computed on downstream hypotheses to inform earlier selection towards less-related inspirations to increase novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Downstream reality/novelty feedback is used to adjust earlier choices; can prioritize novelty over plausibility based on heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Past-feedback empirically increased novelty (sometimes at small cost to validness/helpfulness) because heuristics encouraged less-related inspirations.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Measured indirectly by downstream Validness/Novelty/Helpfulness after past-feedback refinement; tables show MOOSE with future+past-feedback increases novelty relative to future-feedback alone.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Evaluated via GPT-4 automatic scoring and expert human evaluation comparing versions with/without past-feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Not primarily targeted at hallucination; past-feedback drove novelty improvements sometimes reducing plausibility.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>TOMATO dataset</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>MOOSE-base w/ future-feedback vs w/ future+past-feedback (GPT-4): future-feedback (Validness 3.955, Novelty 3.226, Helpfulness 3.953) vs future+past (3.916, 3.390, 3.931) — novelty increased notably; expert eval also showed future+past outperforming future-feedback in multiple metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Past-feedback is not present in the baseline; adding past-feedback to MOOSE further improves novelty compared to only future-feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Quality of past-feedback depends on downstream evaluator quality; authors resorted to heuristics because ChatGPT produced low-quality inspiration feedback in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Automated Open-domain Scientific Hypotheses Discovery', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2525.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2525.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Future-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Future-feedback (FF1 & FF2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A forward-looking feedback mechanism where modules provide justifications or preparatory suggestions to aid downstream modules' generation quality (implemented as FF1 justifications and FF2 preparatory module Hypothesis Suggestor).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Future-feedback (FF1/FF2)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>FF1: modules emit justifications/reasons alongside outputs to inform downstream modules (implemented for Background Finder and Inspiration Title Finder). FF2: insert an intermediate module (Hypothesis Suggestor) before the main Hypothesis Proposer to provide preliminary suggestions and offload reasoning burden, improving final hypothesis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based prompting augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Social science / Business</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Provides richer contextual inputs (justifications or suggestions) to the Hypothesis Proposer to produce more novel and complex hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Downstream Novelty Checker evaluates hypotheses produced with FF1/FF2 to measure effect; ablations show FF2 particularly important for performance.</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Downstream Reality Checker assesses plausibility of hypotheses generated with future-feedback inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>FF1 tends to improve validness and novelty slightly; FF2 has larger positive effect across metrics; adding justifications everywhere can make outputs more valid but less novel if over-constraining.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Assessed by Validness/Novelty/Helpfulness (5-point Likert) via GPT-4 and experts.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Ablation experiments and GPT-4/expert scoring show that removing FF2 degrades performance significantly; removing FF1 reduces validness/novelty with helpfulness similar.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Providing justifications (FF1) can ground downstream generation and reduce unsupported claims; Hypothesis Suggestor (FF2) structures candidate reasoning and can reduce hallucinated leaps by breaking tasks into subtasks.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>TOMATO dataset</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Ablation: without FF2 performance comprehensively drops; without FF1, validness and novelty drop while helpfulness remains comparable (exact table numbers in paper Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Future-feedback modules are part of MOOSE improvements not present in baseline; adding them raises novelty/helpfulness versus baseline and MOOSE-base.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Over-providing justifications to certain modules can reduce novelty (over-constraining generation); cost in compute and prompt complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Automated Open-domain Scientific Hypotheses Discovery', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2525.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2525.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hypothesis Proposer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hypothesis Proposer module (MOOSE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The core LLM module that composes final research hypotheses from selected background, inspirations, and optional suggestions and feedback; targeted with present-feedback refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Hypothesis Proposer</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An LLM-prompted module (gpt-3.5-turbo in main runs) that ingests background b, inspirations i, optional survey snippets s, prior suggestion h (from Hypothesis Suggestor), and present-feedback (clarity, reality, novelty) to generate a hypothesis; it participates in iterative present-feedback loops to refine outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based prompt engineering module</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Social science / Business</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Prompt-based conditional generation combining retrieved context and internal chain-of-thought-like justifications (when requested) and iteratively refined using feedback modules.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Uses Novelty Checker feedback as input to regeneration; novelty final judged by GPT-4 and experts.</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Uses Reality Checker feedback during iterations to improve plausibility.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Controlled via feedback signals; Hypothesis Proposer can be guided to be more novel by ignoring strong reality feedback or more plausible by emphasizing reality feedback; MOOSE design prefers novelty as more important for TOMATO.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Evaluated by Validness/Novelty/Helpfulness (5-point Likert).</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Iterative LLM-based checkers plus GPT-4 and human experts; present-feedback applied primarily to this module.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Receives Reality Checker feedback that flags conflicts with reality; can regenerate accordingly.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Downstream evaluation via Reality Checker, GPT-4, and expert review.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>TOMATO dataset</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Central to MOOSE performance improvements reported; iterative present-feedback on Hypothesis Proposer increases validness and novelty (see Present-feedback metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Baseline approaches directly prompting an LLM on corpus chunks lack the structured multi-stage inputs and iterative refinements of Hypothesis Proposer and perform worse on novelty/helpfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Quality depends on base LLM; may produce less valid outputs when optimizing too strongly for novelty; constrained by prompt and retrieved context quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Automated Open-domain Scientific Hypotheses Discovery', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2525.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2525.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hypothesis Suggestor</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hypothesis Suggestor (MOOSE, FF2 preparatory module)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An intermediate LLM module that provides preliminary suggestions about how to combine background and inspirations, feeding these structured suggestions into Hypothesis Proposer to ease its reasoning burden.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Hypothesis Suggestor</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Preliminary LLM module (M_j-0.5) placed before Hypothesis Proposer; produces suggestion-level outputs (possible research angles, variable linkages, mediators/moderators) that the proposer uses to generate more detailed, novel, and complex hypotheses. Implemented as FF2 in future-feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based submodule</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Social science / Business</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Generates intermediate structured suggestions (e.g., potential independent/dependent variables, mediators, moderators) from background+inspirations which are consumed by the Hypothesis Proposer.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Downstream novelty checks evaluate hypotheses that incorporate these suggestions; ablation shows FF2 (Hypothesis Suggestor) has large positive effect.</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Indirect — suggestions can include justifications; final plausibility judged by Reality Checker.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Offloads reasoning to produce richer candidate structures, improving novelty while still enabling proposer to incorporate reality checks.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Assessed via downstream Validness/Novelty/Helpfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Ablation studies show removing this module degrades overall system metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>By structuring intermediate reasoning, may reduce large unsupported jumps in proposer output.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>TOMATO dataset</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Ablation indicates FF2 is more significant than FF1; removing FF2 causes comprehensive performance drops (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Not present in baseline; inclusion improved novelty/helpfulness over baseline and MOOSE-base.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Adds complexity and computation; effectiveness depends on quality of generated suggestions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Automated Open-domain Scientific Hypotheses Discovery', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2525.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2525.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Novelty Checker</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Novelty Checker module</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based module that assesses whether a candidate hypothesis is novel relative to existing literature by consulting a related survey corpus and returning novelty feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Novelty Checker</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Uses LLM(s) to compare generated hypotheses against retrieved survey chunks from a related-survey corpus (retrieved with BM25). Provides a novelty score/feedback used in present-feedback iterations and also for past-feedback heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based retrieval-augmented evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Social science / Business</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Retrieves related survey snippets with BM25; LLM evaluates overlap and novelty; used to compute a novelty Likert score during evaluation and to inform heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Access to surveys reduces reported novelty but increases validness; ablation (cutting survey access) increased novelty +0.04 and decreased validness to ~0.26.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Produces novelty feedback part of present_f vector; final novelty measured on 5-point Likert by GPT-4 and experts.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Ablation studies (with/without survey access) and downstream effect measured by GPT-4/expert scores.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>By checking literature, reduces chance of proposing hypotheses already in literature (not directly preventing factual hallucinations).</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Related survey corpus S (collected by authors) and TOMATO dataset</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Ablation: cutting access to survey increased novelty (approx +0.04) and decreased validness (~ -0.65 absolute to ~0.26 in their scale), demonstrating influence on balance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Novelty checking is not part of the simple baseline; inclusion helps MOOSE tune novelty vs validness trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>BM25 retrieval may fail to retrieve adequate survey chunks leading novelty detector to over-estimate novelty; depends on survey corpus coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Automated Open-domain Scientific Hypotheses Discovery', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2525.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2525.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reality Checker</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reality Checker module</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based evaluator that assesses whether a generated hypothesis 'reflects reality' (plausibility) and returns feedback to refine the hypothesis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Reality Checker</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Evaluates whether hypotheses are consistent with observed reality/commonsense/empirical expectations, producing reality feedback used in present-feedback loops; implemented with LLM evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Social science / Business</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Directly provides a Likert-like reality assessment (validness) that is consumed by the Hypothesis Proposer during present-feedback iterations; also used in automatic GPT-4 evaluation comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Reality Checker tends to reduce implausible/overly-novel hypotheses; in GPT-4 evaluation some highly novel hypotheses scored lower on validness possibly due to evaluator familiarity limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Produces reality feedback component of present_f; final validness is reported on 5-point Likert.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Compared against expert human validness ratings and GPT-4 evaluations; used iteratively to refine hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Primary in-framework mechanism to reduce hallucinated/unrealistic claims by flagging conflicts with reality and prompting regeneration.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Same as prevention: flags via LLM-based assessment and comparison with survey/background passages; supplemented by expert review.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>TOMATO dataset</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used as part of present-feedback to raise validness across iterations (see Present-feedback metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Baseline lacked such multi-aspect LLM evaluators for iterative refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>LLM-based reality assessments may be biased by LLM training data frequency; authors note GPT-4 may grade validness based on text-seen frequency rather than true world understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Automated Open-domain Scientific Hypotheses Discovery', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2525.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2525.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Clarity Checker</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Clarity Checker module</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based module that evaluates whether a generated hypothesis is clear, specific, and provides sufficient detail, returning clarity feedback used for iterative refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Clarity Checker</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Assesses clarity and meaningfulness of hypotheses (reframed as whether hypothesis is clear and provides enough details) and supplies structured feedback to the Hypothesis Proposer in present-feedback iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Social science / Business</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Produces clarity feedback component of present_f; final helpfulness metric partly reflects clarity.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Used in present-feedback; overall helpfulness tracked by GPT-4 and experts shows improvement with clarity feedback iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>TOMATO dataset</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In conjunction with other checkers, present-feedback raises helpfulness scores across iterations (see Present-feedback metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Provides an axis of evaluation missing from the baseline pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Clarity improvements may not guarantee novelty or validness; trade-offs exist.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Automated Open-domain Scientific Hypotheses Discovery', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2525.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2525.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Baseline-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM baseline (direct corpus chunk → hypothesis)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple baseline that feeds a chunk of retrieved corpus into an LLM (gpt-3.5-turbo) and directly outputs hypotheses without multi-module decomposition or iterative feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Baseline LLM prompting</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Single-step prompting: select a corpus chunk and prompt a base LLM to produce a hypothesis directly. Instantiated with gpt-3.5-turbo to match MOOSE base model for fairness.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based end-to-end prompting</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Social science / Business</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Direct conditional generation from a single corpus chunk prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Evaluated by same Validness/Novelty/Helpfulness (5-point Likert) with GPT-4 and human experts.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Compared against MOOSE variants using GPT-4 and expert evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>TOMATO dataset (same inputs)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>GPT-4 evaluation: Baseline (Validness 3.954, Novelty 2.483, Helpfulness 3.489). Expert evaluation: Baseline (3.579, 2.276, 2.632).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Tends to rephrase input passages, yielding low novelty; lacks systematic multi-aspect iterative checking and is weaker on helpfulness and novelty compared to MOOSE.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Automated Open-domain Scientific Hypotheses Discovery', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2525.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2525.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (used as automatic evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's GPT-4 used here as an automatic judge to score generated hypotheses on Validness, Novelty, and Helpfulness (5-point Likert), showing high soft consistency with human experts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4 (evaluation role)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Deployed with temperature 0.0 and top_p 0.9 for stable deterministic evaluation; used to automatically score thousands of generated hypotheses on the three metrics and compared to expert ratings to show surprisingly high agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM used for evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Used for evaluating social-science hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>GPT-4 applied provided definitions and rubric to assess novelty against provided survey/context.</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>GPT-4 provided validness scores based on prompts; authors caution GPT-4 may rely on text-frequency rather than true world understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Produces the 5-point Likert scores for Validness, Novelty, Helpfulness used as automatic metrics in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Compared GPT-4 scores against human expert scores for 400 hypotheses; soft consistency scores above 0.75 reported.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>400-hypothesis sample from TOMATO dataset used for GPT-4 vs expert consistency analysis</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Soft consistency with experts: Validness 0.85, Novelty 0.823, Helpfulness 0.773 (per paper's consistency scoring scheme). Average absolute difference between GPT-4 and experts <1 on 5-point scale.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Authors note GPT-4 may grade validness based on frequency of related texts in its training data and may not fully capture 'reflecting reality' understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Automated Open-domain Scientific Hypotheses Discovery', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2525.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e2525.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>gpt-3.5-base</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>gpt-3.5-turbo (base model used for modules)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI gpt-3.5-turbo used to instantiate all MOOSE modules in the majority of experiments (training-data cutoff Sep 2021), serving as the base LLM for generation and many evaluator modules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>gpt-3.5-turbo (as base model)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Used as the base LLM for each module in MOOSE in main experiments with temperature 0.9 and top_p 0.9 for generation; chosen to ensure it had not seen post-Jan 2023 papers in dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Used for social science hypothesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Prompted conditional generation in each module (background/inspiration/hypothesis generation).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>TOMATO dataset</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Primary reported numbers use gpt-3.5 as base; experiments also run with Claude3-Opus showing higher absolute scores but similar trends.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Model cutoff means it had not seen the target papers but LLM limitations required adaptation (heuristics for past-feedback).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Automated Open-domain Scientific Hypotheses Discovery', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2525.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e2525.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude3-Opus</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude3-Opus (Anthropic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Anthropic's Claude3-Opus model used as an alternative base LLM to test robustness of MOOSE modules; yielded higher absolute metric scores while preserving trends.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Claude3-Opus (as base model)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Each MOOSE module was instantiated with Claude3-Opus in a set of experiments (temperature 0.9, top_p 0.9) to evaluate effect of base model choice; results show MOOSE components effective across different base models and that Claude3-Opus increases absolute scores on Validness/Novelty/Helpfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Social science / Business</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Prompt-based module generation similar to gpt-3.5 instantiation.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>TOMATO dataset</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Authors report that absolute scores across Validness/Novelty/Helpfulness improved markedly when using Claude3-Opus as base model while MOOSE component effects persisted.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>No detailed per-metric numbers in main text beyond noting large absolute improvements; availability/cost differences not discussed in depth.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Automated Open-domain Scientific Hypotheses Discovery', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2525.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e2525.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Heuristics-past-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Heuristics-based past-feedback (authors' pragmatic rule)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pragmatic rule used instead of LLM-generated past-feedback: if inspiration titles are less related to the background then downstream hypotheses tend to be more novel; used because ChatGPT past-feedback was low-quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Heuristics-based past-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>When LLM-produced inspiration feedback was poor, authors used a simple heuristic: favor less-related inspiration titles to increase novelty of downstream hypotheses. Implemented in Inspiration Feedback module and shown to improve final hypothesis novelty in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Heuristic rule + LLM pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Social science / Business</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Affects upstream selection of inspiration titles that feed into hypothesis generation; encourages diversity/less-related inspirations.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Empirically motivated: novelty increased in downstream GPT-4/expert scores when heuristic used.</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Heuristic can trade plausibility for novelty; authors note validness may decrease slightly.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Empirically tilts toward novelty; used because LLM-produced past-feedback tended to push selections to be too related, reducing novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Measured via downstream Validness/Novelty/Helpfulness; using heuristics improved novelty in reported tables.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Ablation comparisons of MOOSE with/without heuristics-based past-feedback measured by GPT-4 and experts.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>TOMATO dataset</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>When implemented, MOOSE with heuristics-based past-feedback shows improved novelty and competitive overall metrics compared to alternatives (see Tables 2-4 discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Heuristic is domain- and dataset-specific; not a principled LLM-derived feedback and may not generalize.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Automated Open-domain Scientific Hypotheses Discovery', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2525.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e2525.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-refine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior work that investigates LLM self-feedback for iterative refinement; cited as related work and contrasted with MOOSE's additional past- and future-feedback mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Self-refine (Madaan et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An LLM-centered method that iteratively refines outputs using feedback generated by the same model (present-feedback). The paper is cited as focusing only on present-feedback while MOOSE extends to past- and future-feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based iterative self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General NLP / reasoning tasks (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Iterative self-correction of generated outputs by LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Cited as only addressing present-feedback and not multi-aspect or iterative novelty/reality trade-offs as MOOSE does.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Automated Open-domain Scientific Hypotheses Discovery', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Large language models are zero shot hypothesis proposers <em>(Rating: 2)</em></li>
                <li>Check your facts and try again: Improving large language models with external knowledge and automated feedback <em>(Rating: 2)</em></li>
                <li>Reflexion: an agent with dynamic memory and self-reflection <em>(Rating: 1)</em></li>
                <li>Learning to generate novel scientific directions with contextualized literature-based discovery <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2525",
    "paper_id": "paper-261557055",
    "extraction_schema_id": "extraction-schema-68",
    "extracted_data": [
        {
            "name_short": "MOOSE",
            "name_full": "Multi-mOdule framewOrk with paSt present future feEdback (MOOSE)",
            "brief_description": "A multimodule LLM-prompting framework for automated open-domain scientific hypothesis discovery in social sciences that composes retrieval, multi-stage generation modules, and three iterative feedback mechanisms (present-, past-, future-feedback).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "MOOSE",
            "system_description": "A pipeline of serializable modules (Background Finder, Inspiration Title Finder, Inspiration Finder, Hypothesis Suggestor, Hypothesis Proposer) that ingest raw web corpus and optionally related surveys; modules are implemented with LLM prompting (gpt-3.5-turbo in main experiments; also tested with Claude3-Opus). Three feedback mechanisms are integrated: present-feedback (LLM-based evaluators produce reality/novelty/clarity feedback used to iteratively refine hypotheses), past-feedback (future-module evaluation used to retroactively refine earlier module outputs, implemented for inspiration title selection with a heuristics-based rule in practice), and future-feedback (FF1: propagate justifications; FF2: insert a preparatory module Hypothesis Suggestor to offload reasoning). Retrieval (title-level BM25 for tractability) is used to nominate inspirations, and novelty checking consults related surveys.",
            "system_type": "LLM-based; retrieval-augmented multi-module pipeline",
            "scientific_domain": "Social science / Business (dataset: recent social science papers)",
            "hypothesis_generation_method": "Generate backgrounds and inspiration candidates by retrieval over raw web corpus; feed selected background+inspirations into Hypothesis Suggestor (FF2) to produce preliminary suggestions; Hypothesis Proposer (LLM prompt) composes final hypotheses; iterative present-feedback loops use outputs of Reality/Novelty/Clarity checkers to refine hypothesis generations.",
            "novelty_assessment_method": "Novelty Checker module implemented via LLM(s) that consult related survey corpus S (retrieved with BM25) to judge whether the hypothesis or sub-arguments are present in literature; also uses novelty Likert scoring (5-point) by GPT-4 and human experts during evaluation. Ablation: removing access to surveys increases reported novelty and reduces validness.",
            "plausibility_assessment_method": "Reality Checker module (LLM-based) assesses whether hypothesis 'reflects reality' (philosophical validness); evaluation also performed via GPT-4 automatic scoring and human expert Likert ratings on validness.",
            "novelty_plausibility_balance": "Explicit trade-off acknowledged: framework treats novelty as relatively more important for TOMATO task; past-feedback heuristics can favor novelty (encouraging less-related inspirations), whereas Reality Checker/present-feedback moderates plausibility; tuning achieved by which feedback signals are emphasized and survey access to novelty checker.",
            "hypothesis_quality_metrics": "Three primary metrics: Validness, Novelty, Helpfulness — each scored on a 5-point Likert scale by GPT-4 and by human experts (PhD students). These are the primary pre-experimental quality metrics used.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Automatic evaluation with GPT-4 using the three metrics (validness, novelty, helpfulness) and human expert evaluation by three social-science PhD students on sampled hypotheses; ablation studies and GPT-3.5 vs Claude3-Opus base-model comparisons; no wet-lab or external experimental validation reported (validation limited to expert assessment and LLM-based checks).",
            "reproducibility_measures": "Dataset, code, and generated hypotheses are released on GitHub per paper statement; Algorithm 1 and modular design described in appendices. Dataset of 50 recent social science papers with collected raw web passages provided.",
            "hallucination_prevention_method": "Reality Checker and present-feedback loops that use LLM evaluators to detect and provide corrective feedback; future-feedback (justifications) aim to ground later modules; reliance on related surveys and retrieval attempts to ground novelty checks.",
            "hallucination_detection_method": "Reality Checker (LLM-based) flags conflicts with reality; expert human evaluation and GPT-4 scoring are used as post-generation detectors of implausible or false claims. The paper also references 'check your facts' style approaches in related work but does not integrate external fact-checking tools beyond the described modules.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "TOMATO dataset: 50 recent social science papers (post-Jan 2023) with collected raw web passages and related survey corpus S.",
            "performance_metrics": "Reported mean scores (5-point) comparing methods (GPT-3.5 base unless noted): GPT-4 evaluation — Baseline (Validness 3.954, Novelty 2.483, Helpfulness 3.489); MOOSE-base (3.907, 3.081, 3.859); MOOSE-base w/ future-feedback (3.955, 3.226, 3.953); MOOSE-base w/ future+past-feedback (3.916, 3.390, 3.931). Expert evaluation (selected iteration): Baseline (3.579, 2.276, 2.632); MOOSE-base (3.500, 2.855, 3.026); MOOSE w/ future-feedback (3.645, 3.105, 3.303); MOOSE w/ future+past-feedback (3.750, 3.197, 3.368). Present-feedback iterations steadily improved metrics up to a point.",
            "comparison_with_baseline": "MOOSE-base substantially outperforms a simple LLM baseline that directly inputs corpus chunks and outputs hypotheses in novelty and helpfulness, though validness is similar or slightly lower in some automatic evaluations; adding future- and past-feedback further improves novelty/helpfulness (detailed numbers above).",
            "validated_on_real_science": true,
            "novel_discoveries": "Paper reports many generated hypotheses judged by human experts to be novel ('not existing in literature') and plausible; no experimental (empirical) scientific discoveries were externally validated beyond expert/LMM evaluation.",
            "limitations": "Dataset limited to 50 social science papers; domain focused on social science/business so generalization to natural sciences is untested; reliance on LLM base models (gpt-3.5/Claude3) introduces dependence on model capabilities and possible biases/hallucinations; some feedback modules required heuristics because existing LLMs (ChatGPT) produced poor-quality past-feedback; no experimental/empirical confirmation of machine-proposed hypotheses beyond expert judgment.",
            "uuid": "e2525.0",
            "source_info": {
                "paper_title": "Large Language Models for Automated Open-domain Scientific Hypotheses Discovery",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Present-feedback",
            "name_full": "Present-feedback (iterative LLM evaluation and refinement)",
            "brief_description": "An LLM-based iterative feedback mechanism where a module's output is immediately evaluated along multiple aspects (reality, novelty, clarity) by LLM evaluators and that feedback is fed back into the same module to regenerate improved outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Present-feedback",
            "system_description": "After a module (notably the Hypothesis Proposer) produces output o_i, auxiliary LLM-based checkers (Reality Checker, Novelty Checker, Clarity Checker) produce feedback f_i on multiple aspects. The module then consumes o_i and f_i to regenerate o_i in iterative cycles. Implemented as over-generate-then-refine prompting with multiple iterations; applied primarily to hypothesis generation.",
            "system_type": "LLM-based iterative refinement",
            "scientific_domain": "Social science / Business (used within TOMATO experiments)",
            "hypothesis_generation_method": "Not a generation model itself but an iterative scheme applied to Hypothesis Proposer to improve generation quality via LLM feedback loops.",
            "novelty_assessment_method": "Novelty Checker LLM that consults related surveys and returns novelty feedback; used to guide regeneration.",
            "plausibility_assessment_method": "Reality Checker LLM provides feedback on whether hypotheses reflect reality; used directly in iterative refinement.",
            "novelty_plausibility_balance": "Iterative process can increase both novelty and validness but paper notes trade-offs and an optimal number of iterations (helpfulness peaked around 3 iterations in GPT-4 eval).",
            "hypothesis_quality_metrics": "Measured by changes in Validness, Novelty, Helpfulness (5-point Likert) across iterations; numerical trend provided (see performance).",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Monitored via automatic GPT-4 evaluation and human expert scoring of hypotheses after 0..N iterations; empirical tables show improvement trends.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": "By feeding reality-check feedback into regeneration loops, present-feedback aims to reduce hallucinated or implausible claims.",
            "hallucination_detection_method": "Reality Checker and expert/GPT-4 evaluation used to detect implausible content during iterations.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "TOMATO dataset",
            "performance_metrics": "GPT-4-evaluated iteration trend (averaged over several MOOSE variants): w/o present-feedback (Validness 3.823, Novelty 3.114, Helpfulness 3.809); 1 iter (3.918, 3.199, 3.900); 2 iters (3.951, 3.293, 3.956); 3 iters (3.969, 3.270, 3.962); 4 iters (3.970, 3.329, 3.951). Expert-evaluated trends also show increases up to an optimal iteration count.",
            "comparison_with_baseline": "Applying present-feedback to MOOSE variants steadily improves hypothesis quality metrics relative to the same framework without such iterations; baseline (simple LLM) lacks this iterative refinement.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Requires reliable LLM evaluators — quality depends on evaluator capability; too many iterations may plateau or harm certain metrics; can be computationally expensive.",
            "uuid": "e2525.1",
            "source_info": {
                "paper_title": "Large Language Models for Automated Open-domain Scientific Hypotheses Discovery",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Past-feedback",
            "name_full": "Past-feedback (retroactive feedback)",
            "brief_description": "A feedback mechanism where outputs of earlier modules are refined using evaluative signals that can only be computed after downstream modules produce their outputs (i.e., retroactive feedback from future evaluations).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Past-feedback",
            "system_description": "When a module's output o_i cannot be evaluated immediately, the framework runs forward to produce a downstream output o_j that depends on o_i, obtains present-feedback f_j on o_j, and then uses o_i, o_j, and f_j (optionally through an additional LLM module) to produce past-feedback f_i to refine o_i. Implemented for the Inspiration Title Finder in MOOSE; in practice a heuristics-based past-feedback ('less related inspirations → more novelty') was used because ChatGPT-produced inspiration feedback was low quality.",
            "system_type": "LLM-based + heuristics retroactive feedback",
            "scientific_domain": "Social science / Business",
            "hypothesis_generation_method": "Retroactive refinement of earlier retrieval/selection stages (e.g., inspiration titles) informed by downstream hypothesis evaluations.",
            "novelty_assessment_method": "Uses novelty feedback computed on downstream hypotheses to inform earlier selection towards less-related inspirations to increase novelty.",
            "plausibility_assessment_method": "Downstream reality/novelty feedback is used to adjust earlier choices; can prioritize novelty over plausibility based on heuristics.",
            "novelty_plausibility_balance": "Past-feedback empirically increased novelty (sometimes at small cost to validness/helpfulness) because heuristics encouraged less-related inspirations.",
            "hypothesis_quality_metrics": "Measured indirectly by downstream Validness/Novelty/Helpfulness after past-feedback refinement; tables show MOOSE with future+past-feedback increases novelty relative to future-feedback alone.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Evaluated via GPT-4 automatic scoring and expert human evaluation comparing versions with/without past-feedback.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": "Not primarily targeted at hallucination; past-feedback drove novelty improvements sometimes reducing plausibility.",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "TOMATO dataset",
            "performance_metrics": "MOOSE-base w/ future-feedback vs w/ future+past-feedback (GPT-4): future-feedback (Validness 3.955, Novelty 3.226, Helpfulness 3.953) vs future+past (3.916, 3.390, 3.931) — novelty increased notably; expert eval also showed future+past outperforming future-feedback in multiple metrics.",
            "comparison_with_baseline": "Past-feedback is not present in the baseline; adding past-feedback to MOOSE further improves novelty compared to only future-feedback.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Quality of past-feedback depends on downstream evaluator quality; authors resorted to heuristics because ChatGPT produced low-quality inspiration feedback in practice.",
            "uuid": "e2525.2",
            "source_info": {
                "paper_title": "Large Language Models for Automated Open-domain Scientific Hypotheses Discovery",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Future-feedback",
            "name_full": "Future-feedback (FF1 & FF2)",
            "brief_description": "A forward-looking feedback mechanism where modules provide justifications or preparatory suggestions to aid downstream modules' generation quality (implemented as FF1 justifications and FF2 preparatory module Hypothesis Suggestor).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Future-feedback (FF1/FF2)",
            "system_description": "FF1: modules emit justifications/reasons alongside outputs to inform downstream modules (implemented for Background Finder and Inspiration Title Finder). FF2: insert an intermediate module (Hypothesis Suggestor) before the main Hypothesis Proposer to provide preliminary suggestions and offload reasoning burden, improving final hypothesis generation.",
            "system_type": "LLM-based prompting augmentation",
            "scientific_domain": "Social science / Business",
            "hypothesis_generation_method": "Provides richer contextual inputs (justifications or suggestions) to the Hypothesis Proposer to produce more novel and complex hypotheses.",
            "novelty_assessment_method": "Downstream Novelty Checker evaluates hypotheses produced with FF1/FF2 to measure effect; ablations show FF2 particularly important for performance.",
            "plausibility_assessment_method": "Downstream Reality Checker assesses plausibility of hypotheses generated with future-feedback inputs.",
            "novelty_plausibility_balance": "FF1 tends to improve validness and novelty slightly; FF2 has larger positive effect across metrics; adding justifications everywhere can make outputs more valid but less novel if over-constraining.",
            "hypothesis_quality_metrics": "Assessed by Validness/Novelty/Helpfulness (5-point Likert) via GPT-4 and experts.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Ablation experiments and GPT-4/expert scoring show that removing FF2 degrades performance significantly; removing FF1 reduces validness/novelty with helpfulness similar.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": "Providing justifications (FF1) can ground downstream generation and reduce unsupported claims; Hypothesis Suggestor (FF2) structures candidate reasoning and can reduce hallucinated leaps by breaking tasks into subtasks.",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "TOMATO dataset",
            "performance_metrics": "Ablation: without FF2 performance comprehensively drops; without FF1, validness and novelty drop while helpfulness remains comparable (exact table numbers in paper Table 7).",
            "comparison_with_baseline": "Future-feedback modules are part of MOOSE improvements not present in baseline; adding them raises novelty/helpfulness versus baseline and MOOSE-base.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Over-providing justifications to certain modules can reduce novelty (over-constraining generation); cost in compute and prompt complexity.",
            "uuid": "e2525.3",
            "source_info": {
                "paper_title": "Large Language Models for Automated Open-domain Scientific Hypotheses Discovery",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Hypothesis Proposer",
            "name_full": "Hypothesis Proposer module (MOOSE)",
            "brief_description": "The core LLM module that composes final research hypotheses from selected background, inspirations, and optional suggestions and feedback; targeted with present-feedback refinement.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Hypothesis Proposer",
            "system_description": "An LLM-prompted module (gpt-3.5-turbo in main runs) that ingests background b, inspirations i, optional survey snippets s, prior suggestion h (from Hypothesis Suggestor), and present-feedback (clarity, reality, novelty) to generate a hypothesis; it participates in iterative present-feedback loops to refine outputs.",
            "system_type": "LLM-based prompt engineering module",
            "scientific_domain": "Social science / Business",
            "hypothesis_generation_method": "Prompt-based conditional generation combining retrieved context and internal chain-of-thought-like justifications (when requested) and iteratively refined using feedback modules.",
            "novelty_assessment_method": "Uses Novelty Checker feedback as input to regeneration; novelty final judged by GPT-4 and experts.",
            "plausibility_assessment_method": "Uses Reality Checker feedback during iterations to improve plausibility.",
            "novelty_plausibility_balance": "Controlled via feedback signals; Hypothesis Proposer can be guided to be more novel by ignoring strong reality feedback or more plausible by emphasizing reality feedback; MOOSE design prefers novelty as more important for TOMATO.",
            "hypothesis_quality_metrics": "Evaluated by Validness/Novelty/Helpfulness (5-point Likert).",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Iterative LLM-based checkers plus GPT-4 and human experts; present-feedback applied primarily to this module.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": "Receives Reality Checker feedback that flags conflicts with reality; can regenerate accordingly.",
            "hallucination_detection_method": "Downstream evaluation via Reality Checker, GPT-4, and expert review.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "TOMATO dataset",
            "performance_metrics": "Central to MOOSE performance improvements reported; iterative present-feedback on Hypothesis Proposer increases validness and novelty (see Present-feedback metrics).",
            "comparison_with_baseline": "Baseline approaches directly prompting an LLM on corpus chunks lack the structured multi-stage inputs and iterative refinements of Hypothesis Proposer and perform worse on novelty/helpfulness.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Quality depends on base LLM; may produce less valid outputs when optimizing too strongly for novelty; constrained by prompt and retrieved context quality.",
            "uuid": "e2525.4",
            "source_info": {
                "paper_title": "Large Language Models for Automated Open-domain Scientific Hypotheses Discovery",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Hypothesis Suggestor",
            "name_full": "Hypothesis Suggestor (MOOSE, FF2 preparatory module)",
            "brief_description": "An intermediate LLM module that provides preliminary suggestions about how to combine background and inspirations, feeding these structured suggestions into Hypothesis Proposer to ease its reasoning burden.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Hypothesis Suggestor",
            "system_description": "Preliminary LLM module (M_j-0.5) placed before Hypothesis Proposer; produces suggestion-level outputs (possible research angles, variable linkages, mediators/moderators) that the proposer uses to generate more detailed, novel, and complex hypotheses. Implemented as FF2 in future-feedback.",
            "system_type": "LLM-based submodule",
            "scientific_domain": "Social science / Business",
            "hypothesis_generation_method": "Generates intermediate structured suggestions (e.g., potential independent/dependent variables, mediators, moderators) from background+inspirations which are consumed by the Hypothesis Proposer.",
            "novelty_assessment_method": "Downstream novelty checks evaluate hypotheses that incorporate these suggestions; ablation shows FF2 (Hypothesis Suggestor) has large positive effect.",
            "plausibility_assessment_method": "Indirect — suggestions can include justifications; final plausibility judged by Reality Checker.",
            "novelty_plausibility_balance": "Offloads reasoning to produce richer candidate structures, improving novelty while still enabling proposer to incorporate reality checks.",
            "hypothesis_quality_metrics": "Assessed via downstream Validness/Novelty/Helpfulness.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Ablation studies show removing this module degrades overall system metrics.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": "By structuring intermediate reasoning, may reduce large unsupported jumps in proposer output.",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "TOMATO dataset",
            "performance_metrics": "Ablation indicates FF2 is more significant than FF1; removing FF2 causes comprehensive performance drops (Table 7).",
            "comparison_with_baseline": "Not present in baseline; inclusion improved novelty/helpfulness over baseline and MOOSE-base.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Adds complexity and computation; effectiveness depends on quality of generated suggestions.",
            "uuid": "e2525.5",
            "source_info": {
                "paper_title": "Large Language Models for Automated Open-domain Scientific Hypotheses Discovery",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Novelty Checker",
            "name_full": "Novelty Checker module",
            "brief_description": "An LLM-based module that assesses whether a candidate hypothesis is novel relative to existing literature by consulting a related survey corpus and returning novelty feedback.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Novelty Checker",
            "system_description": "Uses LLM(s) to compare generated hypotheses against retrieved survey chunks from a related-survey corpus (retrieved with BM25). Provides a novelty score/feedback used in present-feedback iterations and also for past-feedback heuristics.",
            "system_type": "LLM-based retrieval-augmented evaluator",
            "scientific_domain": "Social science / Business",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": "Retrieves related survey snippets with BM25; LLM evaluates overlap and novelty; used to compute a novelty Likert score during evaluation and to inform heuristics.",
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": "Access to surveys reduces reported novelty but increases validness; ablation (cutting survey access) increased novelty +0.04 and decreased validness to ~0.26.",
            "hypothesis_quality_metrics": "Produces novelty feedback part of present_f vector; final novelty measured on 5-point Likert by GPT-4 and experts.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Ablation studies (with/without survey access) and downstream effect measured by GPT-4/expert scores.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": "By checking literature, reduces chance of proposing hypotheses already in literature (not directly preventing factual hallucinations).",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Related survey corpus S (collected by authors) and TOMATO dataset",
            "performance_metrics": "Ablation: cutting access to survey increased novelty (approx +0.04) and decreased validness (~ -0.65 absolute to ~0.26 in their scale), demonstrating influence on balance.",
            "comparison_with_baseline": "Novelty checking is not part of the simple baseline; inclusion helps MOOSE tune novelty vs validness trade-offs.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "BM25 retrieval may fail to retrieve adequate survey chunks leading novelty detector to over-estimate novelty; depends on survey corpus coverage.",
            "uuid": "e2525.6",
            "source_info": {
                "paper_title": "Large Language Models for Automated Open-domain Scientific Hypotheses Discovery",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Reality Checker",
            "name_full": "Reality Checker module",
            "brief_description": "An LLM-based evaluator that assesses whether a generated hypothesis 'reflects reality' (plausibility) and returns feedback to refine the hypothesis.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Reality Checker",
            "system_description": "Evaluates whether hypotheses are consistent with observed reality/commonsense/empirical expectations, producing reality feedback used in present-feedback loops; implemented with LLM evaluators.",
            "system_type": "LLM-based evaluator",
            "scientific_domain": "Social science / Business",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Directly provides a Likert-like reality assessment (validness) that is consumed by the Hypothesis Proposer during present-feedback iterations; also used in automatic GPT-4 evaluation comparisons.",
            "novelty_plausibility_balance": "Reality Checker tends to reduce implausible/overly-novel hypotheses; in GPT-4 evaluation some highly novel hypotheses scored lower on validness possibly due to evaluator familiarity limitations.",
            "hypothesis_quality_metrics": "Produces reality feedback component of present_f; final validness is reported on 5-point Likert.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Compared against expert human validness ratings and GPT-4 evaluations; used iteratively to refine hypotheses.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": "Primary in-framework mechanism to reduce hallucinated/unrealistic claims by flagging conflicts with reality and prompting regeneration.",
            "hallucination_detection_method": "Same as prevention: flags via LLM-based assessment and comparison with survey/background passages; supplemented by expert review.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "TOMATO dataset",
            "performance_metrics": "Used as part of present-feedback to raise validness across iterations (see Present-feedback metrics).",
            "comparison_with_baseline": "Baseline lacked such multi-aspect LLM evaluators for iterative refinement.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "LLM-based reality assessments may be biased by LLM training data frequency; authors note GPT-4 may grade validness based on text-seen frequency rather than true world understanding.",
            "uuid": "e2525.7",
            "source_info": {
                "paper_title": "Large Language Models for Automated Open-domain Scientific Hypotheses Discovery",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Clarity Checker",
            "name_full": "Clarity Checker module",
            "brief_description": "An LLM-based module that evaluates whether a generated hypothesis is clear, specific, and provides sufficient detail, returning clarity feedback used for iterative refinement.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Clarity Checker",
            "system_description": "Assesses clarity and meaningfulness of hypotheses (reframed as whether hypothesis is clear and provides enough details) and supplies structured feedback to the Hypothesis Proposer in present-feedback iterations.",
            "system_type": "LLM-based evaluator",
            "scientific_domain": "Social science / Business",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Produces clarity feedback component of present_f; final helpfulness metric partly reflects clarity.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Used in present-feedback; overall helpfulness tracked by GPT-4 and experts shows improvement with clarity feedback iterations.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "TOMATO dataset",
            "performance_metrics": "In conjunction with other checkers, present-feedback raises helpfulness scores across iterations (see Present-feedback metrics).",
            "comparison_with_baseline": "Provides an axis of evaluation missing from the baseline pipeline.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Clarity improvements may not guarantee novelty or validness; trade-offs exist.",
            "uuid": "e2525.8",
            "source_info": {
                "paper_title": "Large Language Models for Automated Open-domain Scientific Hypotheses Discovery",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Baseline-LLM",
            "name_full": "LLM baseline (direct corpus chunk → hypothesis)",
            "brief_description": "A simple baseline that feeds a chunk of retrieved corpus into an LLM (gpt-3.5-turbo) and directly outputs hypotheses without multi-module decomposition or iterative feedback.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Baseline LLM prompting",
            "system_description": "Single-step prompting: select a corpus chunk and prompt a base LLM to produce a hypothesis directly. Instantiated with gpt-3.5-turbo to match MOOSE base model for fairness.",
            "system_type": "LLM-based end-to-end prompting",
            "scientific_domain": "Social science / Business",
            "hypothesis_generation_method": "Direct conditional generation from a single corpus chunk prompt.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Evaluated by same Validness/Novelty/Helpfulness (5-point Likert) with GPT-4 and human experts.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Compared against MOOSE variants using GPT-4 and expert evaluations.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "TOMATO dataset (same inputs)",
            "performance_metrics": "GPT-4 evaluation: Baseline (Validness 3.954, Novelty 2.483, Helpfulness 3.489). Expert evaluation: Baseline (3.579, 2.276, 2.632).",
            "comparison_with_baseline": null,
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Tends to rephrase input passages, yielding low novelty; lacks systematic multi-aspect iterative checking and is weaker on helpfulness and novelty compared to MOOSE.",
            "uuid": "e2525.9",
            "source_info": {
                "paper_title": "Large Language Models for Automated Open-domain Scientific Hypotheses Discovery",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "GPT-4 evaluator",
            "name_full": "GPT-4 (used as automatic evaluator)",
            "brief_description": "OpenAI's GPT-4 used here as an automatic judge to score generated hypotheses on Validness, Novelty, and Helpfulness (5-point Likert), showing high soft consistency with human experts.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "GPT-4 (evaluation role)",
            "system_description": "Deployed with temperature 0.0 and top_p 0.9 for stable deterministic evaluation; used to automatically score thousands of generated hypotheses on the three metrics and compared to expert ratings to show surprisingly high agreement.",
            "system_type": "LLM used for evaluation",
            "scientific_domain": "Used for evaluating social-science hypotheses",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": "GPT-4 applied provided definitions and rubric to assess novelty against provided survey/context.",
            "plausibility_assessment_method": "GPT-4 provided validness scores based on prompts; authors caution GPT-4 may rely on text-frequency rather than true world understanding.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Produces the 5-point Likert scores for Validness, Novelty, Helpfulness used as automatic metrics in the paper.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Compared GPT-4 scores against human expert scores for 400 hypotheses; soft consistency scores above 0.75 reported.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "400-hypothesis sample from TOMATO dataset used for GPT-4 vs expert consistency analysis",
            "performance_metrics": "Soft consistency with experts: Validness 0.85, Novelty 0.823, Helpfulness 0.773 (per paper's consistency scoring scheme). Average absolute difference between GPT-4 and experts &lt;1 on 5-point scale.",
            "comparison_with_baseline": null,
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Authors note GPT-4 may grade validness based on frequency of related texts in its training data and may not fully capture 'reflecting reality' understanding.",
            "uuid": "e2525.10",
            "source_info": {
                "paper_title": "Large Language Models for Automated Open-domain Scientific Hypotheses Discovery",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "gpt-3.5-base",
            "name_full": "gpt-3.5-turbo (base model used for modules)",
            "brief_description": "OpenAI gpt-3.5-turbo used to instantiate all MOOSE modules in the majority of experiments (training-data cutoff Sep 2021), serving as the base LLM for generation and many evaluator modules.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "gpt-3.5-turbo (as base model)",
            "system_description": "Used as the base LLM for each module in MOOSE in main experiments with temperature 0.9 and top_p 0.9 for generation; chosen to ensure it had not seen post-Jan 2023 papers in dataset.",
            "system_type": "LLM",
            "scientific_domain": "Used for social science hypothesis generation",
            "hypothesis_generation_method": "Prompted conditional generation in each module (background/inspiration/hypothesis generation).",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": null,
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "TOMATO dataset",
            "performance_metrics": "Primary reported numbers use gpt-3.5 as base; experiments also run with Claude3-Opus showing higher absolute scores but similar trends.",
            "comparison_with_baseline": null,
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Model cutoff means it had not seen the target papers but LLM limitations required adaptation (heuristics for past-feedback).",
            "uuid": "e2525.11",
            "source_info": {
                "paper_title": "Large Language Models for Automated Open-domain Scientific Hypotheses Discovery",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Claude3-Opus",
            "name_full": "Claude3-Opus (Anthropic)",
            "brief_description": "Anthropic's Claude3-Opus model used as an alternative base LLM to test robustness of MOOSE modules; yielded higher absolute metric scores while preserving trends.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Claude3-Opus (as base model)",
            "system_description": "Each MOOSE module was instantiated with Claude3-Opus in a set of experiments (temperature 0.9, top_p 0.9) to evaluate effect of base model choice; results show MOOSE components effective across different base models and that Claude3-Opus increases absolute scores on Validness/Novelty/Helpfulness.",
            "system_type": "LLM",
            "scientific_domain": "Social science / Business",
            "hypothesis_generation_method": "Prompt-based module generation similar to gpt-3.5 instantiation.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": null,
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "TOMATO dataset",
            "performance_metrics": "Authors report that absolute scores across Validness/Novelty/Helpfulness improved markedly when using Claude3-Opus as base model while MOOSE component effects persisted.",
            "comparison_with_baseline": null,
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "No detailed per-metric numbers in main text beyond noting large absolute improvements; availability/cost differences not discussed in depth.",
            "uuid": "e2525.12",
            "source_info": {
                "paper_title": "Large Language Models for Automated Open-domain Scientific Hypotheses Discovery",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Heuristics-past-feedback",
            "name_full": "Heuristics-based past-feedback (authors' pragmatic rule)",
            "brief_description": "A pragmatic rule used instead of LLM-generated past-feedback: if inspiration titles are less related to the background then downstream hypotheses tend to be more novel; used because ChatGPT past-feedback was low-quality.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Heuristics-based past-feedback",
            "system_description": "When LLM-produced inspiration feedback was poor, authors used a simple heuristic: favor less-related inspiration titles to increase novelty of downstream hypotheses. Implemented in Inspiration Feedback module and shown to improve final hypothesis novelty in experiments.",
            "system_type": "Heuristic rule + LLM pipeline",
            "scientific_domain": "Social science / Business",
            "hypothesis_generation_method": "Affects upstream selection of inspiration titles that feed into hypothesis generation; encourages diversity/less-related inspirations.",
            "novelty_assessment_method": "Empirically motivated: novelty increased in downstream GPT-4/expert scores when heuristic used.",
            "plausibility_assessment_method": "Heuristic can trade plausibility for novelty; authors note validness may decrease slightly.",
            "novelty_plausibility_balance": "Empirically tilts toward novelty; used because LLM-produced past-feedback tended to push selections to be too related, reducing novelty.",
            "hypothesis_quality_metrics": "Measured via downstream Validness/Novelty/Helpfulness; using heuristics improved novelty in reported tables.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Ablation comparisons of MOOSE with/without heuristics-based past-feedback measured by GPT-4 and experts.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "TOMATO dataset",
            "performance_metrics": "When implemented, MOOSE with heuristics-based past-feedback shows improved novelty and competitive overall metrics compared to alternatives (see Tables 2-4 discussion).",
            "comparison_with_baseline": null,
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Heuristic is domain- and dataset-specific; not a principled LLM-derived feedback and may not generalize.",
            "uuid": "e2525.13",
            "source_info": {
                "paper_title": "Large Language Models for Automated Open-domain Scientific Hypotheses Discovery",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Self-refine",
            "name_full": "Self-refine: Iterative refinement with self-feedback",
            "brief_description": "A prior work that investigates LLM self-feedback for iterative refinement; cited as related work and contrasted with MOOSE's additional past- and future-feedback mechanisms.",
            "citation_title": "Self-refine: Iterative refinement with self-feedback",
            "mention_or_use": "mention",
            "system_name": "Self-refine (Madaan et al., 2023)",
            "system_description": "An LLM-centered method that iteratively refines outputs using feedback generated by the same model (present-feedback). The paper is cited as focusing only on present-feedback while MOOSE extends to past- and future-feedback.",
            "system_type": "LLM-based iterative self-feedback",
            "scientific_domain": "General NLP / reasoning tasks (related work)",
            "hypothesis_generation_method": "Iterative self-correction of generated outputs by LLMs",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": null,
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": "Cited as only addressing present-feedback and not multi-aspect or iterative novelty/reality trade-offs as MOOSE does.",
            "uuid": "e2525.14",
            "source_info": {
                "paper_title": "Large Language Models for Automated Open-domain Scientific Hypotheses Discovery",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Large language models are zero shot hypothesis proposers",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zero_shot_hypothesis_proposers"
        },
        {
            "paper_title": "Check your facts and try again: Improving large language models with external knowledge and automated feedback",
            "rating": 2,
            "sanitized_title": "check_your_facts_and_try_again_improving_large_language_models_with_external_knowledge_and_automated_feedback"
        },
        {
            "paper_title": "Reflexion: an agent with dynamic memory and self-reflection",
            "rating": 1,
            "sanitized_title": "reflexion_an_agent_with_dynamic_memory_and_selfreflection"
        },
        {
            "paper_title": "Learning to generate novel scientific directions with contextualized literature-based discovery",
            "rating": 2,
            "sanitized_title": "learning_to_generate_novel_scientific_directions_with_contextualized_literaturebased_discovery"
        }
    ],
    "cost": 0.029597,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large Language Models for Automated Open-domain Scientific Hypotheses Discovery
12 Jun 2024</p>
<p>Zonglin Yang zonglin001@ntu.edu.sg 
Nanyang Technological University</p>
<p>Xinya Du xinya.du@utdallas.edu 
University of Texas at Dallas</p>
<p>Junxian Li junxian001@ntu.edu.sg 
Nanyang Technological University</p>
<p>Jie Zheng jie.jay.zheng@gmail.com 
Huazhong University of Science and Technology</p>
<p>Soujanya Poria sporia@sutd.edu.sg 
Singapore University of Technology</p>
<p>Erik Cambria cambria@ntu.edu.sg 
Nanyang Technological University</p>
<p>Large Language Models for Automated Open-domain Scientific Hypotheses Discovery
12 Jun 20245D7BE1FCAC1C9C9B8A42E4465C96060AarXiv:2309.02726v3[cs.CL]
Hypothetical induction is recognized as the main reasoning type when scientists make observations about the world and try to propose hypotheses to explain those observations.Past research on hypothetical induction is under a constrained setting: (1) the observation annotations in the dataset are carefully manually handpicked sentences (resulting in a close-domain setting); and (2) the ground truth hypotheses are mostly commonsense knowledge, making the task less challenging.In this work, we tackle these problems by proposing the first dataset for social science academic hypotheses discovery, with the final goal to create systems that automatically generate valid, novel, and helpful scientific hypotheses, given only a pile of raw web corpus.Unlike previous settings, the new dataset requires (1) using open-domain data (raw web corpus) as observations; and (2) proposing hypotheses even new to humanity.A multi-module framework is developed for the task, including three different feedback mechanisms to boost performance, which exhibits superior performance in terms of both GPT-4 based and expert-based evaluation.To the best of our knowledge, this is the first work showing that LLMs are able to generate novel ("not existing in literature") and valid ("reflecting reality") scientific hypotheses 1 .</p>
<p>Introduction</p>
<p>Logical reasoning is central to human cognition (Goel et al., 2017).It is widely recognized as consisting of three components, which are deductive, inductive, and abductive reasoning (Yang et al., 2023b).Hypothetical induction is considered to be an important sub-type of inductive reasoning (Norton, 2003).It is recognized as the main reasoning type when scientists make observations about the world and try to propose hypotheses to explain the observations.For example, the proposal of Geocentrism, Heliocentrism, and Newton's law of universal gravitation based on the observations of the motion of (celestial) objects can be seen as a result of hypothetical induction.Hypothetical induction is a process of knowledge exploration from observations to hypotheses: it is challenging because it involves the exploration of knowledge that is even new to humanity.Recent research on this has two main limitations (Yang et al., 2024).Firstly, the observations in their dataset have already been manually selected from the raw web corpus, resulting in a close-domain setting.As a result, a developed system for this dataset relies on already manually selected observations, and cannot utilize the vast raw web corpus to propose hypotheses.Secondly, the ground truth hypotheses are mostly commonsense knowledge (e.g., Newton's law), making the task less challenging since LLMs might have already seen them during pretraining.To this end, we propose a new task setting of hypothetical induction, which is to generate novel and valid research hypotheses targeting being helpful to researchers while only given (vast) raw web corpus (Figure 1).This hypothesis formation process is seen as the first step for scientific discovery (Wang et al., 2023a).We call this task as "auTOmated open-doMAin hypoThetical inductiOn (TOMATO)".It is "automated" since a method for this task should automatically propose hypotheses with few human efforts; It is open-domain since it is not restricted by any manually collected data.</p>
<p>For the TOMATO task, we constructed a dataset consisting of 50 recent social science papers published after January 2023 in top social science journals.For each paper, social science experts collect its main hypothesis, identify its background and inspirations, find semantically similar contents for its background and inspirations from the web corpus, collect the full passage for each matched content, and use all collected web passages as raw web corpus.Although the new dataset involves many manual selection processes, the manually selected contents are used more as benchmarking human performance for comparison.In the TOMATO task, a method is required to only utilize the raw web corpus in the dataset to propose hypotheses.In addition, the raw web corpus is mostly from common news, Wikipedia, and business reviews, which means it can easily expand in scale without much human involvement.</p>
<p>To tackle the TOMATO task, we develop a multimodule framework called MOOSE based on large language model (LLM) prompting (Figure 4).To further improve the quality of the generated hypotheses, we also propose three different feedback mechanisms (present-feedback, past-feedback, and future-feedback) to use LLMs to retrospect and improve the LLM-generated hypotheses for better quality.For present-feedback, the intuition is that, for some modules, their generation can be evaluated by other LLMs and be provided with feedback, which can be utilized by the modules to refine their generation by taking the feedback and previous generation as input and generating again.Some modules can have feedback instantly after their generation to improve themselves.But just like the reward mechanism in reinforcement learning, some rewards (feedback) might be hard to obtain instantly, but need to wait for feedback for a future module.Similarly, we develop past-feedback where a module can benefit from the feedback for a future module.The last one is future-feedback, where a current module can provide justifications for the current module's generation to help a future module's generation, or can provide some initial suggestions which a future module can build upon to further provide more in-depth generation.</p>
<p>Both GPT-4 (OpenAI, 2023) evaluation and expert evaluation indicate that MOOSE performs better than an LLM (Ouyang et al., 2022) based baseline, and each of the three feedback mechanisms can progressively improve the base framework.During expert evaluation, many hypotheses generated by MOOSE are recognized by social science researchers to be both novel ("not existing in the literature") and valid ("reflecting reality").To the best of our knowledge, this is the first work showing that LLMs can be leveraged to generate novel and valid research hypotheses, indicating the potential for LLMs to serve as a "copilot" for scientists.</p>
<p>2 Related Work 2.1 NLP Methods for Scientific Discovery Zhong et al. (2023) propose a dataset where each data consists of a research goal, a corpus pair, and a discovery.However, (1) their task needs a humanprovided research goal and a pairwise corpus for discovery, which is not an automated setting and has a limited application scope; (2) the discovery is not from recent publications.Wang et al. (2023b) is a concurrent work of ours.Compared the first version of two papers, they do not have an iterative feedback for novelty, reality, and clarity.Later they add for novelty, but still lack the other two.These aspects are required by inductive reasoning, and there's an implicit trade-off between reality and novelty.Only stressing on novelty might lead to incorrect and vague generation.Bran et al. (2023) focuses on integrating computational tools in the chemistry domain, but not on providing novel chemistry findings or hypotheses.Boiko et al. (2023) focuses on using LLMs to design, plan, and execution of scientific experiments, but not on finding novel hypotheses.</p>
<p>LLM-based Self Feedback</p>
<p>Self-refine (Madaan et al., 2023) investigates feedback but it only focuses on present-feedback (our framework also proposes past-feedback and futurefeedback), and it is not specially designed for inductive reasoning tasks.Other similar works to self-refine (Press et al., 2022;Peng et al., 2023;Yang et al., 2022;Shinn et al., 2023) also only focus on present-feedback, and their feedback is not multi-aspect nor iterative compared to ours.</p>
<p>, suggesting that cusy, particularly when rs step up to a selfdings in which stranpacted by this social o scan a QR code on t need to perform in ld need to do when thereby experiencing social presence when rs may also be contheir actions of taking dditional body moverded and shared by effect of social presiety and privacy conumber of customers stomer and watching.ers would experience ivacy concerns caused ent technology than hen more people are e, we develop the folikely to use FR payment ology for their transacine behind them.</p>
<p>The Moderating Effect of Experience on the Social Presence Effect</p>
<p>The social presence effect, as we hypothesized in Hypothesis 1, may be moderated by an individual's experience.When an individual's experience in using the technology grows, that individual has more confidence and less anxiety when using the technology.The evidence for such moderation has been documented in the performing arts (e.g., Steptoe and Fidler 1987) and education (e.g., Meijer and Oostdam 2007).In the retail setting, Dahl et al. (2001) find that familiarity with purchasing embarrassing products reduces the embarrassment caused by the social presence in both the selection and commitment stages of the purchase process.The literature suggests that experience in using a technology can also increase self-efficacy (Crossler and Bélanger 2019) and thus increase perceived control over the technology (Hui and Bateson 1991), which then reduces users' privacy concerns over using the technology.In our case, by accumulating more experience in using FR in front of other customers, the focal customer's technology anxiety and privacy concern caused by the social presence effect is expected to decrease, which is similar to the findings of studies of music performers, test takers, retail shoppers, mobile app users in the literature, whose performance anxiety/privacy concern reduced as their experience increased.This means that the experience of customers in using FR payment technology negatively moderates their social presence effects.Hence, we have the following hypothesis: Hypothesis 3. Customers with more experience in using FR payment technology have less social presence effect when using this technology than those with less experience in using FR payment technology.s is that they must go through dife payment process.ustomers do not need to use their , they stand in front of a built-in heckout POS machine that verifies e consumer chooses the FR option OS machine, a frame located at the f the self-checkout machine will be mer needs to position his or her e built-in camera to scan.This prong a selfie that requires the custoselves so that their faces are within ntification process to proceed.In ay need to perform certain livemetimes, such as slightly shaking , 3 so that the FR payment technolhat it is a live person and not a and liveness test gestures required n be a performance-like action.A tification process runs in the backto match the person in front of the to on the ID card in the database.orized once there is a match.quire sensitive private information ed to be executed in public spaces ing to the literature, privacy conss management," including access ational privacy) or access to interacy) (Laufer and Wolfe 1977).In concerns may be related to both and access to interaction.The entiayment platform, the retail stores, ustomers.cy literature and the specific conwe summarize the three sources concerns for the focal customer ut POS machines may emanate. 4y be concerned about his or her that is captured and stored in the shows that users oftentimes do ew technology because of the perllected to set up the technology 2009, Tsai et al. 2011)).Second, a ncerned about his or her digital tured when the customer uses FR concerns may rise from their perate information is collected, con- (Malhotra et al. 2004) since how ted will be used (or not used) is s.Third, a customer may be conomers behind him or her watching hile he or she is using the FR paynsumers may consider the actions doing additional body movement rivate behaviors and view the disto other stranger shoppers around as a violation of physical privacy (Laufer and Wolfe 1977), and they may consider the videos recorded and shared by those strangers around them as a violation of informational privacy (Choi et al. 2015).The first privacy concern is common to both QR and FR, whereas the last two apply to FR only.We discuss how we deal with these privacy concerns in Online Appendix A3.</p>
<p>Hypothesis Development</p>
<p>In this section, we develop hypotheses associated with FR payment usage.A literature review is included in Online Appendix A1.</p>
<p>Social Presence Effect</p>
<p>Based on the social impact theory, an individual's behavior is impacted by real, implied, or even imagined social presence, and the magnitude of the impact is determined by social size, immediacy, and social source strength.The amount of social presence is a multiplicative function of the strength, immediacy, and number of people who evaluate an individual's performance as a member of an audience (Latané 1981).This theory is tested and supported by empirical evidence in the field of psychology (e.g., Jackson and Latané 1981) and is expanded in other fields.Along a similar line, when social impact exists, the social cognitive theory argues that an individual's self-efficacy and emotions, such as technology affect and technology anxiety, play an important role in the individual's decision on technology use.</p>
<p>In retail, a public setting, social presence exists and can create a feeling of embarrassment for customers (Dahl et al. 2001).Using several laboratory experiments, Dahl et al. (2001) find that awareness of social presence during purchase selection and commitment increases a subject's self-reported sense of embarrassment in both laboratory settings and in field studies.Argo et al. (2005) investigate the impact of two social forces-social size and proximity-on customer emotions and self-presentation behaviors in two field experiments, finding that customer emotion and behavior (e.g., brand choice) can be negatively affected by the presence of others in the surroundings during a shopping session.Using a laboratory experiment, Dabholkar and Bagozzi (2002) document that social anxiety resulting from imagined social presence negatively affects customer attitudes toward and intention to use self-service technology.</p>
<p>Customers may have privacy concerns because of the social presence too.Some studies in the literature have found that social presence is a factor that could prevent individuals from using a certain technology.They show that social presence negatively affects customers' perceived control (Hui and Bateson 1991), which is a factor for privacy concerns (Schmidt and Keating 1979).The customer's perceived control is positively associated with the customer's perceived ion Payment Technology in Retail Articles in Advance, pp.1-12, © 2023 INFORMS 3 front of a webcam as they would need to do when using FR payment technology, thereby experiencing less technology anxiety caused by social presence when strangers are around.Consumers may also be concerned about their privacy when their actions of taking a photo and, sometimes, doing additional body movement for a liveness test, are recorded and shared by those strangers around them.The effect of social presence on the user's technology anxiety and privacy concerns should increase with the number of customers waiting in line behind the focal customer and watching.Therefore, we expect that customers would experience greater technology anxiety and privacy concerns caused by social presence using FR payment technology than using QR payment technology when more people are waiting in line behind them.Hence, we develop the following hypothesis.</p>
<p>Hypothesis 1. Customers are less likely to use FR payment technology than QR payment technology for their transactions when they have more people in line behind them.</p>
<p>Herding Effect</p>
<p>A customer's decision on payment technology use may be affected by the preceding customers before him or her as well.Unlike the customers behind him or her, the focal customer can observe the payment choice of the preceding customers.Such observation may influence the choice of payment technology for the focal customer.</p>
<p>Herding theory argues that people tend to herd because they believe the choice of the majority is the best, thus reducing the uncertainty of their own choice (Banerjee 1992, Sunder et al. 2019).In the process of herding, an individual's own belief regarding the quality of different choices is discounted or even ignored (Banerjee 1992)</p>
<p>The Moderating Effect of Experience on the Social Presence Effect</p>
<p>The social presence effect, as we hypothesized in Hypothesis 1, may be moderated by an individual's experience.When an individual's experience in using the technology grows, that individual has more confidence and less anxiety when using the technology.The evidence for such moderation has been documented in the performing arts (e.g., Steptoe and Fidler 1987) and education (e.g., Meijer and Oostdam 2007).In the retail setting, Dahl et al. (2001) find that familiarity with purchasing embarrassing products reduces the embarrassment caused by the social presence in both the selection and commitment stages of the purchase process.The literature suggests that experience in using a technology can also increase self-efficacy (Crossler and Bélanger 2019) and thus increase perceived control over the technology (Hui and Bateson 1991), which then reduces users' privacy concerns over using the technology.In our case, by accumulating more experience in using FR in front of other customers, the focal customer's technology anxiety and privacy concern caused by the social presence effect is expected to decrease, which is similar to the findings of studies of music performers, test takers, retail shoppers, mobile app users in the literature, whose performance anxiety/privacy concern reduced as their experience increased.This means that the experience of customers in using FR payment technology negatively moderates their social presence effects.Hence, we have the following hypothesis: Hypothesis 3. Customers with more experience in using FR payment technology have less social presence effect when using this technology than those with less experience in using FR payment technology.Downloaded from informs.org by [3.0.220.147] on 12 August 2023, at 02:11 .For personal Our present-feedback is developed upon a multiaspect over-generate-then-filter mechanism (Yang et al., 2024).However, they only utilize LLMs to "filter" but not to provide feedback.</p>
<p>Dataset Collection</p>
<p>In this section, we take one publication (Gao et al., 2023) in our dataset as an example to illustrate the dataset collection process.In total, there are 50 papers published after January 2023.Table 1 shows the statistics of the subject distribution.</p>
<p>Most social science publications highlight their hypotheses.Figure 2 shows our selected main hypothesis in the example publication.The research backgrounds are given in the introduction section.In this example paper, the background is about facial recognition payment technology's usage in society.Most social science publications also have a "Hypothesis Development" section (some may call it by other names, e.g., "Theoretical Development").For example, the left part ("Hypothesis Development") in Figure 3 shows the title of this section in the example paper.In this section, several theories used to develop the main hypothesis are separately introduced.Usually, each theory takes one subsection.For example, the right part ("Herding Effect") in Figure 3 shows the title of a subsection, which is a particular theory being used as an inspiration, which with the background can develop the hypothesis in Figure 2.</p>
<p>For each publication in our dataset, we identify its main hypothesis, research background, and in- spirations, where the background and inspirations together provide enough information to be possible to develop the hypothesis.We also abstract the reasoning process from background and inspirations to hypothesis and note it down for each publication in our dataset.In this selected example, the reasoning process is easy, but it has medium difficulty for researchers to associate the inspiration (herding effect) to the background.For each publication, we include an expert-evaluated complexity for both the reasoning process and the association of the inspiration to the background (details in §A.3).Instead of directly copying the background and inspirations from the paper to construct the dataset, we try to find semantically similar text contents from the web corpus as a substitution to avoid data contamination and fit the requirement of TOMATO task that a system should propose novel and valid research hypotheses only given raw web corpus.</p>
<p>In the example paper, we find news sentences reporting the usage of facial recognition payment as ground truth background and a Wikipedia description of the herding effect as ground truth inspiration.We also collect the web link and the full text of the manually selected web passages for backgrounds and inspirations to be used as raw web corpus.</p>
<p>In addition, we collect the link and the publication date for all fifty papers.We also collected fourteen survey papers in related fields that might help check the novelty of the hypotheses.The dataset is fully constructed by a social science PhD student.We illustrate why the dataset shouldn't be collected by automatic methods in §A.4.</p>
<p>Methodology</p>
<p>In general, our method consists of a base multimodule framework and three feedback mechanisms (past-feedback, present-feedback, and futurefeedback).We call the full framework as Multi-mOdule framewOrk with paSt present future feEdback (MOOSE).The base framework without any feedback is called MOOSE-base.MOOSE is described in Figure 4 and Algorithm 1.</p>
<p>Base Framework</p>
<p>The base framework is developed based on the intuitive understanding of how social science researchers propose an initial research hypothesis.</p>
<p>Firstly, a researcher needs to find a suitable research background, e.g., facial recognition payment system's impact.This background should be  proposed with a deep understanding of the societal world.Accordingly, we develop a background finder module, which reads through raw web corpus to find reasonable research backgrounds.</p>
<p>Secondly, since the proposed hypothesis should be novel, directly copying from raw web corpus usually is not enough.A good social science hypothesis should contain an independent variable and a dependent variable, and describe how the independent variable can influence the dependent variable.Therefore, building connections between two variables that have not been known for established connections contributes to a novel hypothesis.We hypothesize that proper inspiration can help this connection-building process, since it might serve as one of the variables itself, or might help to find such variables.However, it could consume lots of computing resources and even be practically impossible if the framework searches over the full web corpus for every found background.Nevertheless, it could be much more viable if only searching over the titles of the corpus, and then only finding inspirational sentences in the passages which match the selected titles.Accordingly, we develop an inspiration title finder module and an inspiration finder module, together to find proper inspirations given a background.</p>
<p>Lastly, a hypothesis proposer module can utilize backgrounds and inspirations for hypotheses.</p>
<p>In general, MOOSE-base consists of a list of serializable generation modules M 0 , M 1 , ..., M n that function sequentially.The input of a module M i is from the output of previous modules M j,j&lt;i and a raw web corpus C (and optionally a related survey corpus).M i 's output is represented as o i .Feedback to o i is represented as f i .</p>
<p>Present-Feedback</p>
<p>LLMs are not perfect and can lead to flaws in the generation, especially for those modules that undertake a difficult task.Previous work on hypothetical induction (Yang et al., 2024) tackles this problem by leveraging LLMs to identify flaws in the generation and filters those with huge flaws.Here we take a step further that instead of filtering, LLMs are leveraged to provide feedback, so that a generation can be improved rather than just filtered.</p>
<p>Accordingly, we define present-feedback as when an output o i can be directly evaluated and provided feedback f i (by LLMs or experts, here we use LLMs) in terms of some aspects, o i and f i are used as additional inputs to M i , so that M i can regenerate o i to refine the previous one with f i .</p>
<p>We implement present-feedback on the Hypotheses Proposer module, since it is a key module that undertakes a very difficult task.In terms of what aspects should the feedback focus on, Yang et al. (2024) propose four aspects according to the philosophical definition and requirement for hypothetical induction (Norton, 2003).The aspects are whether the hypothesis (1) is consistent with observations; (2) reflects reality; (3) generalizes over the observations; (4) is clear and meaningful.</p>
<p>In MOOSE, we basically adopt the four aspects but reframe them to better fit the current task.Specifically, aspect (2) contains aspect (1) most of the time (unless the observations are wrongly described).To save computing power, we adopt aspect (2) but not aspect (1).In addition, we re-frame aspect (3) as whether the hypothesis is novel, and reframe aspect (4) as whether the hypothesis is clear and provides enough details.Accordingly, we develop a reality checker module, a novelty checker module, and a clarity checker module in Figure 4.</p>
<p>Past-Feedback</p>
<p>Just like the reward mechanism in reinforcement learning, some modules' generation can only be evaluated at a future time point.For instance, it is hard to give feedback on the selected inspirations unless we know what hypotheses these inspirations could lead to.Accordingly, we develop past-feedback as when it is hard to directly evaluate o i , the framework continues to run until generating o j,j&gt;i , where o j is highly influenced by o i and can be directly evaluated to obtain present-feedback f j .Then o i , o j , and f j are utilized, possibly by an additional module implemented with an LLM, to provide past-feedback f i to M i , so that M i can regenerate o i with f i to refine the previous o i .</p>
<p>We implement past-feedback on the Inspiration Title Finder module.The intuition is that improper inspirations can lead to low-quality hypotheses, and it is hard to directly evaluate inspirations.</p>
<p>Future-Feedback</p>
<p>We also develop future-feedback, targeting at providing additional useful information for a future module M j to generate o j in better quality.Specifically, we develop future-feedback-1 (FF1) and future-feedback-2 (FF2).FF1 is that in addition to o i , justifications (reasons) of o i are also provided to M j,j&gt;i so that M j can better leverage o i ; FF2 is that for a key module M j that handles a very complex task, an additional module M j−0.5 is being placed before M j , so that M j−0.5 can undertake some of the reasoning burdens of M j to improve the quality of o j .For example, in MOOSE, M j−0.5 is to provide preliminary suggestions for M j .</p>
<p>Specifically in the MOOSE framework, for FF1, no additional modules are needed.Instead, we modify the prompt to require M i to not only generate o i but also provide the justification of o i .We implement it on the Background Finder and the Inspiration Title Finder modules.The intuition is that it could be helpful if the Inspiration Title Finder module knows not only the background but also what possible research topics could be conducted for this background so as to select suitable titles; it could be also helpful for the Inspiration Finder module to know why this background was selected and what potentially helpful inspirations could be found from the passage with the corresponding selected titles.For FF2, we implement it on the Hypothesis Proposer module, since proposing hypotheses is a very important and complex task.Accordingly, we develop a Hypothesis Suggestor module (as M j−0.5 ) to provide some initial suggestions on how to utilize the inspirations and background first, and then Hypothesis Proposer (as M j ) can build upon the suggestions to generate more novel and more complicated hypotheses.</p>
<p>Experiments</p>
<p>Evaluation Metrics &amp; Details</p>
<p>We conduct both automatic evaluation and human evaluation for the experiments.</p>
<p>For automatic evaluation, we adopt validness, novelty, and helpfulness as three aspects for GPT-4 to evaluate.We choose validness and novelty because they are the two basic requirements for hypothetical induction illustrated in philosophical literature (Norton, 2003;Yang et al., 2024).In addition, these two scores also highly resemble the current ACL review form, which requires reviewers to score submitted papers on soundness and excitement aspects.We choose helpfulness because the final goal of the TOMATO task is to provide help and assistance for human scientists.</p>
<p>In §A.5 we illustrate why we don't adopt evaluation metrics such as (1) relevance and significance, and (2) BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), or METEOR (Banerjee and Lavie, 2005).</p>
<p>For human (expert) evaluation, evaluation metrics are the same.Three experts (social science PhD students) take charge of the expert evaluation.They evaluate on 400 randomly selected hypotheses from the baseline and variants of the MOOSE framework.To avoid any bias, they are not told which methods we are comparing; the order of generated hypotheses to compare is also randomized.We introduce how the 400 hypotheses are selected in §A.6, and the high expert agreement in §A.7.</p>
<p>Each metric is on a 5-point Likert scale.Both experts and GPT-4 are given the same description of the scale and evaluation standard of the three aspects (listed in §A.9).</p>
<p>Out of the metrics, we consider the novelty metric to be relatively more important than the validness metric.Because the goal of the TOMATO task is to assist human researchers, but not to directly add the machine-proposed hypotheses to the  literature.If the hypotheses are fully valid but not novel, then they are not helpful at all; but if the hypotheses are novel but not valid, then they can still be possible to inspire human researchers to develop novel and valid hypotheses.Helpfulness is also an important metric since it could be seen as an overall evaluation of a hypothesis.In §A.8, we introduce the surprisingly high consistency between expert evaluation and GPT4 evaluation, indicating that GPT-4 might be able to provide a relatively reliable evaluation for machinegenerated social science hypotheses.</p>
<p>Baselines &amp; Base Model Selection</p>
<p>Since the TOMATO task is to propose hypotheses given only corpus, a natural baseline is to use a corpus chunk as input, and directly output hypotheses.</p>
<p>Except for §6.3, we use gpt-3.5-turbofor each module in MOOSE.To be fair, the baseline is also instantiated with gpt-3.5-turbo.The training data of the model checkpoint is up to September 2021, while all papers in our dataset are published after January 2023, so the model has not seen any of the collected papers in the dataset.In §6.3, we investigate the effect of base model selection by using Claude3-Opus (Anthropic, 2024) for each module in MOOSE.</p>
<p>Main Results</p>
<p>In this subsection, we compare MOOSE-base with the baseline and examine the effect of each of the three feedback mechanisms to MOOSE-base.</p>
<p>We first introduce the number of generated hypotheses being evaluated in §5.3 and §6.For ex-  periments evaluated with GPT-4, fifty backgrounds are selected for each method.For MOOSE-related methods, for each background, on average around 6 inspirations are extracted, resulting in 4 different hypotheses.Each hypothesis leads to another 4 more refined ones with present-feedback.Therefore on average for each MOOSE-related method in GPT-4 evaluation tables, around 50<em>4</em>5=1000 hypotheses are evaluated.For experiments evaluated with expert evaluation, in general, we randomly select one hypothesis for each background, resulting in 50 hypotheses evaluated for each line of the method in expert evaluation tables.</p>
<p>Table 2 shows GPT-4's evaluation targeting at comparing MOOSE-base and the baseline and shows the effect of future-feedback and pastfeedback.In this table, MOOSE-related results are averaged over iterations of present-feedback to not be influenced by present-feedback.MOOSEbase largely outperforms the baseline in terms of both novelty and helpfulness, but slightly lower in terms of validness.As illustrated in §5.1, since the purpose of the TOMATO task is to inspire and help human researchers, novelty and helpfulness metrics should be more important.In practice, we find many hypotheses from baseline almost only rephrasing some sentences in the input corpus, adding little novelty content.MOOSEbase with future-feedback comprehensively outperforms MOOSE-base in terms of all three metrics.MOOSE-base with both future and pastfeedback largely outperforms MOOSE-base with future-feedback in novelty and performs slightly lower in validness and helpfulness metrics.One of the reasons is that the past-feedback may focus more on the novelty aspect because the novelty checker module provides more negative presentfeedback than the reality checker module.</p>
<p>Table 3 shows the effect of present-feedback with GPT-4 evaluation.In this table, the results are averaged over three experiments: MOOSE-base, MOOSE-base with future-feedback, and MOOSEbase with both future and past-feedback to focus on present-feedback.It shows that as more iterations of present-feedback are conducted, validness and novelty steadily go up; helpfulness also steadily goes up but reaches the best performance with 3 iterations of present-feedback.</p>
<p>Table 4 shows expert evaluation results on the comparison between MOOSE-base and the baseline, and the effect of future-feedback and pastfeedback.MOOSE-related results are selected from the 5 th iteration of present-feedback.Similar to GPT-4 evaluation, MOOSE-base largely outperforms the baseline in terms of Novelty and Helpfulness; MOOSE-base with future-feedback comprehensively outperforms MOOSE-base.Different from GPT-4 evaluation, MOOSE-base with future and past-feedback also comprehensively outperforms MOOSE-base with future-feedback.We think one of the reasons could be that GPT-4 might grade validness based on how frequently it has seen relevant texts, but not true understanding of the world.Therefore a more novel hypothesis might tend to have a relatively lower score in validness and helpfulness under GPT-4 evaluation.</p>
<p>Table 5 shows the expert evaluation of presentfeedback.MOOSE-base and MOOSE are both evaluated.Overall performance generally goes up with more iterations of present-feedback, but there might be an optimal number of iterations.</p>
<p>Analysis</p>
<p>Background and Inspirations</p>
<p>Here we try to answer "Is ChatGPT necessary for background and inspiration selection?".</p>
<p>Table 6 shows various methods for background and inspiration selection.In general, there might be a validness-novelty trade-off that if a method reaches a high novelty score, then it is usually hard for it to reach a high validness score.It is surprising that a randomly selected background and randomly selected inspirations can lead to hypotheses with relatively comparable validness and novelty to ChatGPT-picked background and inspirations.Empirically we hypothesize the reason is that randomly picked inspirations are mostly not related to the background, resulting in a high novelty (but less validness and helpfulness).In addition, BM25 (Robertson et al., 2009) picked background and inspirations reach a much higher novelty score compared to ChatGPT-picked ones.Empirically we do not find BM25 retrieved inspirations to be similar to the background, but they are usually with more concrete contents compared with random inspirations.Not surprisingly, Chat-GPT picked background and inspirations reach the highest helpfulness score among those without any ground-truth annotations.Lastly, ground-truth hypotheses reach the highest novelty and helpfulness.</p>
<p>More Ablation Studies</p>
<p>Table 7 shows ablation studies on future-feedback, access to surveys, and the selection of corpus.Firstly, for future-feedback, we separately test the effect of FF1 and FF2.Without FF2, performance comprehensively drops; without FF1, performance drops on validness and novelty, with helpfulness remaining comparable.It seems that FF2 is more significant than FF1.However, the fact that FF1 works on inspiration title finder and inspiration finer modules does not mean that it works on all modules.Empirically we find that adding the reasons (or prospects) for background and inspirations to the hypothesis proposer module will cause a more valid but much less novel Secondly, we cut the access of novelty detector to related surveys to check the effect of related surveys.As a result, novelty largely goes up (0.04), and validness goes down to around 0.26.Empirically one of the main reasons is that BM25 hardly retrieves enough similar survey chunks, so that access to the survey leads novelty detector to tend to reply the hypotheses are novel since it is not mentioned in the related survey.Without presentfeedback, MOOSE and MOOSE w/o access to survey perform quite comparably.</p>
<p>Lastly, the raw corpus in the dataset is from two sources: passages that contain the ground truth backgrounds and passages that contain the ground truth inspirations.In all of the previous experiments, backgrounds are extracted from the background passages, and inspirations are extracted from the inspirations passages.To see whether the passages are only restricted to their designed role, in MOOSE w/ randomized corpus experiment, we use inspiration corpus for background extraction and use both inspiration and background corpus for inspiration extraction.As a result, validness goes up by about 0.025, while novelty goes down by about 0.16.We think one of the reasons is that, in this setting, after selecting a background from an inspiration passage, MOOSE tends to retrieve the same inspiration passage to find inspirations, which leads to less novel results.</p>
<p>Effect of Base Model Selection</p>
<p>In all previous experiments, we adopt GPT-3.5 as the base model.In this section, we investigate the effect of base model selection by using Claude3-Opus as the base model for each module in MOOSE.With Claude3-Opus as the base model, we again analyze the effect of MOOSE-base, past-feedback, and future-feedback in Table 8; and analyze the effect of present-feedback in Table 9.The experiment settings of Table 8 and Table 9 are exactly the same as in Table 2 and Table 3 correspondingly, but only differ in the base model selection.</p>
<p>In general, there are two conclusions.Firstly, MOOSE's components stay effective regardless of different base model selection.It shows the robustness of the MOOSE framework in terms of different base model.Secondly, the absolute evaluation scores on all three metrics largely improved with Claude3-Opus compared to GPT-3.5, indicating the even larger potential of the MOOSE framework when more powerful LLMs are available.</p>
<p>Qualitative Analysis</p>
<p>The following box shows one generated counterintuitive hypothesis (expert evaluation appended).</p>
<p>In collectivist cultures, individuals engage in more conspicuous consumption behaviors compared to individualistic cultures.(Validness: 3.3; Novelty: 4.0; Helpfulness: 4.0)</p>
<p>Here is the assessment from one of the experts:</p>
<p>The main reason I give a high mark for both three dimensions of this hypothesis is because:</p>
<p>(1) For validness, this hypothesis is based on existing cultural theories and empirical evidence that suggests cultural values significantly impact consumer behavior.It aligns with established concepts like collectivism and individualism that have been widely studied in cross-cultural psychology.</p>
<p>(2) For novelty, this hypothesis is counterintuitive to some extent.Prior research has shown that collectivist cultures often prioritize group harmony, cooperation, and social cohesion over individual desires.This emphasis on collective wellbeing might suggest a reduced inclination toward overt displays of personal wealth or status through conspicuous consumption.However, this hypothesis suggests the opposite that collectivist culture's members engage in more conspicuous consumption, which is more commonly linked to individualistic societies in popular perceptions.This challenges the notion that members of collectivist cultures avoid conspicuous consumption behaviors.</p>
<p>(3) For helpfulness, if this hypothesis is confirmed, it could have significant practical implications.Understanding the impact of cultural values on conspicuous consumption can assist businesses and marketers in crafting more effective cross-cultural marketing strategies.It could also aid policymakers in addressing societal issues related to consumerism.</p>
<p>In addition to the analysis of this counterintuitive example, we also provide qualitative analysis on the difference between hypotheses generated from the baseline, MOOSE-base, MOOSEbase w/ future-feedback, and MOOSE-base w/ future and past-feedback in §A.11.More qualitative analysis on highly scored generated hypotheses can be found in §A.12.Additionally, §A.13 illustrates factors for good hypotheses in social science (particularly in Business).§A.14 shows how MOOSE formulates a hypothesis by giving the generation of each of the modules in MOOSE.</p>
<p>Conclusion</p>
<p>In this paper, we propose a new task, automated open-domain hypothetical induction (TOMATO), which is the first task in NLP to focus on social science research hypotheses discovery.Along with the task, we construct a dataset consisting of 50 recent social science papers published in top academic journals.We also developed a multi-module framework MOOSE for the TOMATO task, which contains a base framework and three novel feedback mechanisms.Experiments indicate that MOOSEbase outperforms an LLM-based baseline, and the three feedback mechanisms can progressively fur-ther improve over MOOSE-base.Surprisingly, evaluated by PhD students, MOOSE is able to produce many novel ("not existing in the literature") and valid ("reflecting reality") research hypotheses.To the best of our knowledge, this is the first work showing that LLMs can be leveraged to generate novel and valid scientific hypotheses, indicating the potential of LLMs to serve as a "copilot" for scientists.</p>
<p>Limitations</p>
<p>From the first look, it might seem that the proposed dataset consists of only 50 recent papers.However, they are all manually collected by experts (PhD students), and are annotated with lots of details (e.g., identifying background and inspirations, finding relevant raw web passages for background and inspirations, reasoning process, complexity level).In addition, each paper has been published in a top social science journal, representing the pinnacle of human intelligence.This means it would be incredibly exciting if LLMs could propose a hypothesis from even a single one of these recent papers.</p>
<p>It might also seem that it is not clear whether the design of the framework can apply to other disciplines.However, to the best of our knowledge, this is the first paper using LLMs that can propose novel scientific hypotheses that are new to humanity.We choose social science as the breakthrough point since the main data format of social science is language.Table 1 shows that the dataset covers 7 different disciplines (e.g., Psychology, Management, Marketing).It would be nearly impossible for the first few works to develop a general method to propose novel hypotheses for all disciplines.</p>
<p>This paper concentrates on an automated task setting in which a system is designed to formulate scientific hypotheses independently, without requiring human intervention.In some scenarios, scientists may prefer to use their own background and inspirations as input for controllable hypotheses generation.It might seem that the automated setting and the controllable setting are in conflict.However, we contend that the automated setting make a step further than the controllable setting, since a system developed for an automated setting would inherently support controllable generation by simply substituting the automatically searched inputs (e.g., background and inspirations) with those that are manually crafted.</p>
<p>Societal Impact: Expert evaluation shows that MOOSE, an LLM-based system, might already be able to serve as a copilot for researchers across various social science disciplines.Particularly, as depicted in Figure 1, it can assist researchers in the hypothesis formation process, which is the first step for scientific discovery (Wang et al., 2023a).This capability signifies a step towards enhancing the efficiency of scientific exploration by accelerating the formation and development of innovative and credible research hypotheses, thereby boosting researchers' productivity.To maximize its impact and ensure equitable access, it is imperative to advocate for the open-sourcing of such systems, thereby democratizing access for the global scientific community.</p>
<p>A Appendix</p>
<p>A.1 Hyper-parameters Experiments in §6.3 adopts Claude3-Opus, all other experiments are conducted with gpt-3.5-turbo.Both Claude3-Opus and gpt-3.5-turbouse 0.9 temperature and 0.9 top_p.The hyperparameters for GPT-4 evaluation are 0.0 temperature to ensure the evaluation scores are stable, and 0.9 top_p.</p>
<p>A.2 More Related Works on Reasoning and Scientific Discovery</p>
<p>This paper is a successive work in inductive reasoning and is different from commonsense reasoning (Bosselut et al., 2019;Yang et al., 2020) in that the novel social science hypotheses do not belong to commonsense.</p>
<p>Case-based reasoning (Das et al., 2021;Yang et al., 2023a) also falls in the domain of inductive reasoning, but case-based reasoning is more about high-level guidance on methodology design (case retrieve, reuse, revise, and retain), which is not involved in this paper.Qi et al. (2023) work on zero-shot hypothesis proposing, which is a concurrent work to our paper.They don't focus on social science and business disciplines, and mostly adopt a single LLM as method (prompting, finetuning).</p>
<p>A.3 Dataset Complexity Distribution</p>
<p>Table 10 illustrates the complexity distribution of the proposed dataset from both reasoning and association perspectives."Easy" in the table means it is relatively easy compared to other publications in the dataset, but does not mean it is actually easy to induce the hypotheses.</p>
<p>A.4 Why the Tomato Dataset Shouldn't Be Collected by Automatic Methods</p>
<p>Firstly, there are many hypotheses in a social science publication, which might need an expert to identify which hypothesis is suitable for this task (e.g., whether it is a main hypothesis, whether the background and inspirations are properly introduced).</p>
<p>Secondly, the background and inspirations scatter in a publication.It needs a deep domain understanding of the hypothesis, related background, and inspirations to select the background and inspirations out to form a complete reasoning chain to conclude the hypothesis.</p>
<p>Thirdly, it needs enough domain knowledge to find semantically similar texts (similar to the groundtruth selected background and inspirations) from the web, where the texts should contain enough details to help elicit the hypothesis.</p>
<p>A.5 Why Not Using Other Evaluation Metrics</p>
<p>Other relevant aspects from related literature include relevance (Wang et al., 2023b) and significance (Zhong et al., 2023).</p>
<p>We do not adopt relevance because our task setting is the automated and open domain, without a manually given background; neither for significance because social science is different from engineering subjects -(1) every hypothesis is to reflect the reality of the world, and as long as it reflects the world, it is significant.Therefore it is hard to tell which one is more significant even by experts; (2) the evaluation standard of significance varies from time to time.For example, in the 60s, conducting research on how to improve the assembly line's efficiency as much as possible was seen as very significant.However, in  recent decades, how to alleviate the psychological depression of assembly line workers is seen as more significant.</p>
<p>We do not adopt BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), or METEOR (Banerjee and Lavie, 2005) as evaluation metric to compare the proposed hypothesis and the ground truth hypothesis since (1) proposing novel research hypotheses is an open problem, and (2) TOMATO has an automated open domain setting, which means the automatically selected background and inspirations are hardly the same as a few given ground truth ones (if background and inspirations are not the same, then it is meaningless to compare the hypothesis).Liu et al. (2016) have conducted a comprehensive analysis that they also reached a similar conclusion that BLEU, METEOR, or ROUGE is not suitable for an open-ended task (such as a dialogue system).</p>
<p>A.6 Hypotheses Selection for Expert Evaluation</p>
<p>In total, we randomly selected 400 hypotheses to be evaluated by experts.Specifically, for each background passage in the dataset (out of 50), we use 4 methods (which are to be compared) to collect in total 8 hypotheses.</p>
<p>The 8 hypotheses are from (1) the baseline;</p>
<p>(2) the MOOSE-base framework;</p>
<p>(3) MOOSE-base + future-feedback; (4) MOOSE-base + future-feedback + past-feedback.For ( 2) and (4), we collect three hypotheses, which are (a) without present-feedback; (b) after 2 iterations of present-feedback; and (c) after 4 iterations of present-feedback.For (1) and (3), we only collect one hypothesis, which is without present-feedback.</p>
<p>With these collections, we can evaluate the effect of both the MOOSE-base framework and the three feedback methods, leading to results in Table 4 and Table 5.</p>
<p>Out of the three experts, one expert evaluates the full 400 hypotheses, and the other two each evaluate 104 hypotheses (the first and second 104 hypotheses out of 400).The reason we choose the number "104" is that (1) social science PhD students are quite busy and two of them can only have time to evaluate around 100 hypotheses; (2) the number should be dividable by 8 (since every 8 hypotheses form a group for comparison).</p>
<p>The results of the expert evaluation are averaged over the three experts.Specifically, expert evaluation essentially compares the 8 hypotheses within a group.The 400, 104, and 104 hypotheses evaluation scores can be written as arrays of [50,8], [13,8], and [13,8].We concatenate them to [76,8], and average them across the first dimension.</p>
<p>The payment for expert evaluation is $1 per hypothesis.</p>
<p>A.7 Expert Qualification and Expert Agreement</p>
<p>The constructed dataset covers many subjects, but every collected publication is somewhat related to Marketing, which is a big topic in Business research.It is common in social science to conduct research that connects with other social science domains.The experts for expert evaluation are three PhD students majoring in Marketing.Therefore the experts are qualified enough to provide assessment for machinegenerated hypotheses in the domain.</p>
<p>The consistency scores between experts are shown in Table 11.The soft consistency and hard consistency are defined in §A.8.All soft consistency scores are above 0.75 means, and the average difference between experts in terms of each metric is less than 1 (out of a 5-point scale), exhibiting high expert evaluation agreement.Aspect 1: Validness</p>
<p>points</p>
<p>The hypothesis completely reflects the reality.</p>
<p>points</p>
<p>The hypothesis almost completely reflects the reality, but has only one or two minor conflictions that can be easily modified.</p>
<p>points</p>
<p>The hypothesis has at least one moderate conflict or several minor conflicts.</p>
<p>points</p>
<p>The hypothesis has at least one major confliction with the reality or only establishes in very rare circumstances that are not mentioned in this hypothesis.</p>
<p>point</p>
<p>The hypothesis completely violates the reality.</p>
<p>A.8 Consistency Between Expert Evaluation and GPT-4 Evaluation</p>
<p>To check the consistency between expert evaluation and GPT-4 evaluation, we use the expert evaluation results and find the corresponding GPT-4 evaluation results.In total, there are 400 hypotheses evaluated by experts, so the sample we use to calculate the consistency score is 400.Specifically, similar to Pan et al. (2011), for soft consistency, if the absolute difference between expert evaluation and GPT-4 evaluation (both are on a 5-point scale) is 0/1/2/3/4, then we assign a consistency score of 1.00/0.75/0.50/0.25/0.00;for hard consistency, if only the difference is 0, can the consistency score be 1.00, otherwise consistency score is 0.00.The hard and soft consistency scores shown in Table 12 are averaged for each metric.</p>
<p>The consistency scores are surprisingly high.All soft consistency scores are above 0.75 means, and the average difference between expert and GPT-4 evaluation in terms of each metric is less than 1 (out of a 5-point scale).The results indicate that GPT-4 might be able to provide a relatively reliable evaluation for machine-generated hypotheses.</p>
<p>Aspect 1: Novelty</p>
<p>points</p>
<p>The hypothesis is completely novel and has not been proposed by any existing literature.</p>
<p>points</p>
<p>The main argument or several sub-arguments of the hypothesis are novel.</p>
<p>points</p>
<p>The main argument is not novel, only one or two sub-arguments appear to be novel.</p>
<p>points</p>
<p>The full hypothesis is not novel, but the way it combines the topics can be inspiring for human researchers.</p>
<p>point</p>
<p>The hypothesis is not novel at all and not inspiring for human researchers.Aspect 1: Helpfulness</p>
<p>points</p>
<p>The hypothesis is novel, valid, clear, and specific enough that it is itself a mature research hypothesis, and human researchers can directly adopt it for publication with no modifications needed.</p>
<p>points</p>
<p>The hypothesis is novel enough and can be directly adopted by human researchers for publication after minor modifications.</p>
<p>points</p>
<p>The hypothesis should be largely modified or reconstructed by human researchers to adopt it.</p>
<p>points</p>
<p>Modifying this hypothesis might not deserve the efforts, but a small part of this hypothesis is inspiring for human researchers to develop a new hypothesis.</p>
<p>1 point The hypothesis is not helpful and not inspiring at all.</p>
<p>A.9 Evaluation Aspects Description</p>
<p>The evaluation standard for Validness, Novelty, and Helpfulness is correspondingly displayed in Table 13, Table 14, and Table 15.A.10 More Details About Past-Feedback Design</p>
<p>In practice, we find that ChatGPT is not capable enough to generate past-feedback with enough good quality for the Inspiration Feedback module.Instead, it tends to provide feedback as "the previous inspiration titles are not very relevant to the hypotheses or the background".As a result, the ChatGPT Inspiration Title Finder module tends to select inspiration titles that are very related to the background, resulting in a less novel hypotheses generation.</p>
<p>Therefore instead of instantiating with ChatGPT for the Inspiration Feedback module, we experiment with leveraging human heuristics.The heuristics are "if the inspiration titles are less related to the background, then more novel hypotheses are likely to be proposed.".With this heuristics-based pastfeedback, MOOSE does perform better (as shown in the tables in §5 and §6).</p>
<p>This heuristics-based feedback is possible to be obtained by a language model since it has access to the novelty feedback of each hypothesis as well as the inspiration titles the hypothesis leveraged.Here our contribution is to propose a useful framework for the TOMATO task, which is not limited by any LLMs for any module in the framework.In the future, it is possible for more powerful LLMs to find better inspiration feedback than human heuristics.</p>
<p>A.11 Qualitative Analysis on Hypotheses Generated From Different Methods</p>
<p>We analyze four hypotheses from the baseline, MOOSE-base, MOOSE-base w/ future-feedback, and MOOSE-base w/ future and past-feedback (MOOSE), where the four methods use the same passage to extract background.</p>
<p>• Hypothesis from the baseline: Companies that prioritize customer understanding will have higher profitability than companies that do not prioritize customer understanding.(Evaluated by the expert, Validness: 4; Novelty: 1.5; Helpfulness: 2)</p>
<p>• Hypothesis from MOOSE-base: The level of empathy displayed by leaders in a startup environment influences employees' job satisfaction and organizational success through the mediation of employees' perceived likelihood of negative outcomes and expectation of enjoyment, as well as their propensity toward risky choices.(Evaluated by the expert, Validness: 3.5; Novelty: 3; Helpfulness: 3)</p>
<p>• Hypothesis from MOOSE-base w/ future-feedback: Female CMOs in startups, leveraging their higher levels of empathy, are more likely to prioritize customer satisfaction by actively listening to customer feedback, incorporating customer insights into decision-making processes, and providing personalized customer experiences.This employee prioritization of customer satisfaction is positively associated with higher levels of customer engagement, increased customer loyalty, and improved brand recall, as measured by objective metrics such as sales figures, customer retention rates, and brand recognition in the market.The influence of female CMOs on employee behavior is mediated by their ability to foster a caring relationship with as supported by empirical data and statistical analysis.(Evaluated by the expert, Validness: 3.5; Novelty: 3.5; Helpfulness: 3.5)</p>
<p>• Hypothesis from MOOSE-base w/ future and past-feedback (MOOSE): Female CMOs' empathy advantage influences their consideration of negative consequences of CSR initiatives, specifically in terms of employee well-being and job security.This relationship is moderated by individual differences in emotional intelligence.Additionally, the organizational culture and industry context will further influence the relationship between empathy advantage and consideration of negative consequences.The hypothesis will investigate whether female CMOs with higher levels of empathy are more likely to prioritize employee well-being and job security in the implementation of CSR initiatives, and whether this relationship is stronger in industries with a stronger emphasis on employee well-being and job security.It will also explore the mediating role of organizational culture and the moderating role of emotional intelligence in shaping the relationship between empathy advantage and consideration of negative consequences.(Evaluated by the expert, Validness: 4.5; Novelty: 4; Helpfulness: 4)</p>
<p>Analysis from the expert:</p>
<p>• H1 falls short of challenging established assumptions or introducing a novel perspective beyond the widely accepted link between customer understanding and profitability.</p>
<p>• Both H2 &amp; H3 center around a specific scenario involving female CMOs in startups and delve into their influence on customer satisfaction, employee behavior, and overall business results.From a research standpoint, this more focused approach points to a potential gap in the existing body of knowledge.Moreover, these two hypotheses surpass conventional understanding by considering how the empathy of female CMOs impacts employee behavior and business outcomes.They put forth a fresh viewpoint, suggesting that cultivating a compassionate rapport with customers, fostered by female CMOs, could positively affect customer engagement, loyalty, and brand recognition.These two hypotheses zoom in on a more specific context, introduce an innovative perspective, and probe a potential void in current research.They are anchored in the dynamic world of innovative business settings and propose a more nuanced and all-encompassing connection between variables.</p>
<p>• H4 retains its relevance within a modern business landscape by scrutinizing the intersection of empathy, CSR initiatives, and the dynamics of organizations.This syncs seamlessly with the criterion of being rooted in an innovative business environment.Moreover, it shakes up established assumptions by considering the potential adverse outcomes of CSR initiatives and the role empathy plays in shaping decision-making within this context.This hypothesis delves into a more intricate and thorough exploration, examining a broader spectrum of factors and interactions within a specific context.Additionally, it imparts a deeper comprehension of the interplay between empathy, business choices, and organizational results.It grapples with a more complex and distinctive scenario, unearths possible gaps in the existing literature, and introduces a new angle on the role of empathy in the realm of business decisions.</p>
<p>A.12 Qualitative Analysis on Two MOOSE-Generated Hypotheses With High Expert Evaluation Scores</p>
<p>In the following two grey boxes are two generated hypotheses from MOOSE with high expert evaluation scores (appended to each hypothesis).The expert's assessment of the two hypotheses is:</p>
<p>Hypothesis 1: The level of personalization in crowdfunding campaign storytelling, the influence of social media influencers who align with the campaign, the presence of trust indicators, and the emotional appeal of the campaign will positively impact potential donors' likelihood of making a donation.Additionally, the timing of donation requests and the type of social media influencers (e.g., celebrities vs. micro-influencers) will moderate this relationship.The perceived risk associated with the crowdfunding campaign will negatively moderate the relationship between the emotional appeal and donation likelihood.(Validness: 4.5; Novelty: 4.5; Helpfulness: 4.5)</p>
<p>Hypothesis 2: Limited financial resources and limited access to networks and markets of women entrepreneurs in the manufacturing sector in developing countries may negatively impact their investment in corporate social responsibility (CSR) initiatives that promote gender equality in host countries.This relationship is further influenced by the intersectionality of gender and race, with women of color facing additional challenges.Additionally, the hypothesis considers the role of institutional factors, such as legal frameworks and policies, and the influence of patriarchal structures on women entrepreneurs' ability to invest in CSR initiatives.(Validness: 3.5; Novelty: 4.0; Helpfulness: 4.0)</p>
<p>These two hypotheses both present a comprehensive view of the research narrative.It encompasses multiple hypotheses, including the primary one, as well as the mediation effect, which serves to elucidate the causal connection between the independent and dependent variables.Concurrently, both hypotheses outline the range of the effect -namely, the circumstances in which this effect is applicable, under which scenarios where it might be weakened, and under which situation it could potentially be inverted.</p>
<p>In terms of novelty: 1. Limited prior research or a gap in the existing literature.This means that there is a dearth of studies or information available on the subject, making it an unexplored area.2. Based on a new business setting.It is grounded in an innovative business environment, characterized by novel technologies, contemporary themes, and evolving business requirements.3. The topic offers a fresh and unique perspective that goes beyond conventional understanding.It might challenge existing assumptions, propose new theories, or present an unconventional approach.</p>
<p>A.13 Essential Factors for Good Social Science (and Business) Hypotheses</p>
<p>According to business PhD students, counter-intuitive and novel hypotheses are the mostly favoured (by top business journals).Intuitive and novel hypotheses are also good but not as good as the counter-intuitive ones.Here "novel" refers to "not pointed out by existing literatures".</p>
<p>Empirically they think of all the hypotheses on top business journals, around 20% are counter-intuitive, leaving the remaining 80% intuitive.</p>
<p>Counter-intuitive hypotheses tend to receive a lower validness evaluation compared to intuitive ones.For this reason, we highlight the counter-intuitive hypothesis in §6.4,even if it receives a lower score in validness than hypotheses in §A.12.</p>
<p>A.14 An Example of Hypothesis Formulation via MOOSE</p>
<p>Here we show a complete flow of hypothesis discovery, by giving the output of relevant modules.Specifically, the found background, found inspirations, generated suggestions, generated hypothesis, reality feedback, novelty feedback, and clarity feedback are copied in this section.</p>
<p>• Found background:</p>
<p>Blind boxes, in terms of how they reach the consumers, are an innovation.Digging deeper into the issue, you will learn the very marketing of blind boxes is called probabilistic selling.That is to say, even after making the payment for a blind box, the buyer still has no idea what will be the contents inside.Suspense.A sense of thrill amid an air of expectation ensue.The sheer anticipation of a surprise can be intoxicating even as it can keep the consumer on tenterhooks, in a nice sort of way.</p>
<p>• Found inspirations:</p>
<p>Inspiration 0: "Surprise is like crack for your brain.Scientists at Emory and Baylor used MRIs to measure changes in human brain activity in response to a sequence of pleasurable stimuli, using fruit juice and water.The patterns of juice and water squirts were either predictable or completely unpredictable.Contrary to the researchers' expectations, the reward pathways in the brain responded most strongly to the unpredictable sequence of squirts."Inspiration 1: "According to Tmall, China's largest B2C online marketplace, about 200,000 post-95 consumers bought blind boxes from its platform in 2019, with each spending CNY 20,000 per year on average; some fanatics even shelled out CNY 1 million on this "game of chances" to get the coveted limited-edition toys."Inspiration 2: "Product differentiation is fundamentally a marketing strategy to encourage the consumer to choose one brand or product over another in a crowded field of competitors."Inspiration 3: "The pandemic has shown how a lack of solid statistics can be dangerous.But even with the firmest of evidence, we often end up ignoring the facts we don't like" Inspiration 4: "User-generated content (also known as UGC or consumer-generated content) is original, brand-specific content created by customers and published on social media or other channels."Inspiration 5: "Higher Star Ratings Lead to More Orders.More stars really do lead to more sales.Thedata shows that products with higher star reviews have a much higher volume of purchases."</p>
<p>• Suggestions from Hypothesis Suggestor Module: Suggestion 1: Combining the background information on blind boxes and probabilistic selling with Inspiration 1 (surprise as a powerful reward stimulus) and Inspiration 2 (high spending on blind boxes by young consumers), a novel business research hypothesis could be: The element of surprise and unpredictability in probabilistic selling, such as blind boxes, triggers stronger reward responses in the brain, leading to higher consumer spending and loyalty, particularly among younger demographics.This hypothesis suggests that the psychological impact of surprise can be a significant driver of consumer behavior and purchasing decisions, especially for novel product categories.Suggestion 2: Integrating the background context with Inspiration 3 (product differentiation strategy) and Inspiration 5 (user-generated content), another innovative hypothesis could be: Encouraging user-generated content around the surprise and reveal aspects of blind box products can serve as a powerful product differentiation strategy, setting brands apart in a competitive market.By leveraging the excitement and social sharing around the unboxing experience, businesses can create a unique brand identity and foster a sense of community among their customers.Suggestion 3: Drawing from the background information, Inspiration 4 (the importance and challenges of evidence-based decision making), and Inspiration 6 (the impact of star ratings on sales), a third hypothesis could be: Despite the proven effectiveness of probabilistic selling in driving consumer engagement and sales, businesses may be reluctant to adopt this strategy due to a lack of traditional, evidence-based metrics and the perceived risks associated with unpredictable outcomes.This hypothesis highlights the potential tension between data-driven decision making and the benefits of innovative, surprise-based selling strategies.</p>
<p>• Hypothesis from Hypothesis Proposer Module:</p>
<p>User-generated content, such as unboxing videos and reviews, can significantly influence the purchasing decisions of potential blind box buyers, particularly among younger demographics.The excitement and social sharing around the unboxing experience, as depicted in user-generated content, can amplify the psychological reward of surprise and drive higher sales, making it an effective tool for businesses to differentiate their products and increase consumer engagement in the blind box market.</p>
<p>• Reality Feedback:</p>
<p>The given hypothesis about the influence of user-generated content on blind box purchasing decisions among younger demographics seems to reflect reality to a certain extent.However, there are a few points to consider that might suggest the hypothesis does not entirely capture the reality:</p>
<ol>
<li>
<p>Generalization: The hypothesis assumes that user-generated content significantly influences all younger demographics.In reality, the impact may vary depending on factors such as personal preferences, cultural background, and socio-economic status.</p>
</li>
<li>
<p>Other influential factors: While user-generated content can be an important factor in purchasing decisions, it may not be the sole or most significant driver.Other aspects like product quality, price, brand reputation, and peer influence might play equally important or even more crucial roles.</p>
</li>
<li>
<p>Limited scope: The hypothesis focuses on the excitement and social sharing aspect of unboxing experiences.However, it does not consider the potential negative impact of user-generated content, such as disappointing unboxing experiences or negative reviews that could discourage purchases.</p>
</li>
<li>
<p>Assumption of psychological reward: The hypothesis assumes that the surprise element in blind box purchases always leads to a positive psychological reward.In reality, some consumers might experience disappointment or frustration if they do not receive the desired item, which could deter future purchases.5. Lack of empirical evidence: The hypothesis presents a plausible explanation but does not provide concrete empirical evidence</p>
</li>
</ol>
<p>• Novelty Feedback:</p>
<p>Based on the provided inspirations and the possibly related business literature paragraph, the research hypothesis appears to be novel and not a direct semantic copy of any of the given information.The hypothesis combines insights from various sources, such as the psychological impact of surprise, the popularity of blind boxes among younger demographics, the role of product differentiation, and the influence of user-generated content on purchasing decisions.While the hypothesis draws from these inspirations, it synthesizes them into a unique and specific context related to the blind box market and the impact of user-generated content on consumer engagement and sales.The provided literature paragraph discusses language modeling and large language models, which is not directly related to the hypothesis.To further enhance the novelty of the hypothesis, you could consider the following suggestions:</p>
<ol>
<li>
<p>Explore the potential long-term effects of user-generated content on brand loyalty and customer retention in the blind box market.</p>
</li>
<li>
<p>Investigate the role of social media platforms and influencers in amplifying the impact of usergenerated content on blind box sales.</p>
</li>
<li>
<p>Examine the potential differences in the effectiveness of user-generated content across various product categories within the blind box market (e.g., collectible toys, beauty products, or snacks).</p>
</li>
<li>
<p>Analyze the impact of user-generated content on the perceived value and collectibility of blind box items among consumers.</p>
</li>
</ol>
<p>• Clarity Feedback:</p>
<p>The given hypothesis is relatively clear and specific, as it identifies the relationship between usergenerated content and purchasing decisions of potential blind box buyers.It also specifies the target demographic (younger buyers), the psychological factors at play (excitement, surprise, and social sharing), and the potential benefits for businesses (product differentiation and increased consumer engagement).However, there is still room for improvement in terms of specificity.To make the hypothesis more specific, consider the following:</p>
<ol>
<li>
<p>Quantify the extent of the influence of user-generated content on purchasing decisions (e.g., "can increase the likelihood of purchase by X").</p>
</li>
<li>
<p>Specify the age range of the "younger demographics" (e.g., "particularly among buyers aged 18-34").</p>
</li>
<li>
<p>Clarify which types of user-generated content have the most significant impact (e.g., "especially unboxing videos that showcase the excitement and surprise of the experience").</p>
</li>
<li>
<p>Indicate the platforms or channels where user-generated content is most effective (e.g., "on social media platforms such as Instagram and TikTok").</p>
</li>
<li>
<p>Provide a more detailed explanation of how user-generated content amplifies the psychological reward (e.g., "by creating a sense of anticipation and vicarious experience for potential buyers").6.Quantify the potential</p>
</li>
</ol>
<p>A.15 Future Directions</p>
<p>This work discovered the possibility of LLMs to propose novel research hypotheses.But it mainly focuses on the social science and business disciplines.It would be very interesting to investigate how LLMs can induce novel hypotheses for other disciplines (especially nature science domains).</p>
<p>In addition, the MOOSE framework could be further improved to induce more valid and novel hypotheses for social science and business domains.</p>
<p>From the aspect of human-AI interaction, it would be also interesting to see how MOOSE can act as an AI Copilot to assist scientists in hypothesis discovery.</p>
<p>A.16 Full Algorithm of the MOOSE Framework</p>
<p>Algorithm 1 shows the full algorithm of the proposed framework.end for 24: end for 25: return H</p>
<p>Figure 1 :
1
Figure 1: Comparison of the two paradigms for scientific hypotheses formulation.The new paradigm shows the role of the MOOSE framework (scientist's copilot) and the new task setting of hypothetical induction.</p>
<p>Figure 2 :
2
Figure 2: A selected hypothesis in a social science publication collected in our dataset.</p>
<p>Figure 3 :
3
Figure 3: Hypothetical development section and a particular theory subsection for developing hypotheses.</p>
<p>Figure 4 :
4
Figure4: MOOSE: Our multi-module framework for TOMATO task.The black part is the base framework; orange part represents past-feedback.; green part represents present-feedback; blue part represents future-feedback.Each capitalized letter represents the generation of one of the modules.The same capitalized letter represents the same regardless of its color.If a module has an input arrow pointing in with a capitalized letter, it represents that this module utilizes one of its previous modules' generation (which has the same letter pointing out) as input.</p>
<p>Algorithm 1
1
Algorithm for MOOSE Input: Raw web corpus C, related surveys S Parameter: Total iterations for past-feedback M , total iterations for present-feedback N Output: A list of hypotheses H 1: for c in C do iteration t ∈ 0...N do 18:cf , rf , nf = Clarity_Checker(h), Reality_Checker(h), Novelty_Checker(h, S)19: present_f = [cf , rf , nf ] 20: h = Hypothesis_Proposer(b, i, s, h, present_f )</p>
<p>In our setting, when a customer watches his or her preceding customers use FR payment technology to check out, his or her own belief regarding the quality of FR and QR payment technologies may be discounted.When a customer steps up to the self-checkout POS machine next, he or she is more likely to follow the preceding customers and choose FR payment technology.Hence, we develop the following hypothesis.Customers whose preceding customers use FR payment technology are more likely to use FR payment technology than those whose preceding customers do not use FR payment technology.
Hypothesis 2.t technology use maytomers before him ors behind him or her,e payment choice ofbservation may influ-nology for the focalople tend to herd be-e majority is the best,eir own choice (Bane-e process of herding,ing the quality of dif-en ignored (Banerjeen widely documentedature on informationdfunding (Zhang andt al. 2020), and online</p>
<p>Table 1 :
1
Statistics of subject distribution of the dataset.
Communication5Psychology7Human Resource Management 8Social ScienceInformation System8International Business5Management6Marketing11</p>
<p>Table 2 :
2
Effect of MOOSE-base, future-feedback and past-feedback (evaluated by GPT-4).MOOSE-related results are averaged over iterations of present-feedback.Base model is GPT-3.5.
Validness Novelty HelpfulnessBaseline3.9542.4833.489MOOSE-base3.9073.0813.859w/ future-feedback3.9553.2263.953w/ future-and past-feedback3.9163.3903.931Validness Novelty HelpfulnessMOOSE (w/o present-feedback)3.8233.1143.809w/ 1 iteration of present-feedback3.9183.1993.900w/ 2 iterations of present-feedback3.9513.2933.956w/ 3 iterations of present-feedback3.9693.2703.962w/ 4 iterations of present-feedback3.9703.3293.951</p>
<p>Table 3 :
3
Effect of present-feedback (evaluated by GPT-4).Base model is GPT-3.5.</p>
<p>Table 4 :
4
Effect of MOOSE-base, future-feedback and past-feedback (evaluated by experts).MOOSE results are selected from the 5 th iteration of present-feedback.Base model is GPT-3.5.
Validness Novelty HelpfulnessBaseline3.5792.2762.632MOOSE-base3.5002.8553.026w/ future-feedback3.6453.1053.303w/ future-and past-feedback3.7503.1973.368Validness Novelty HelpfulnessMOOSE-base (w/o present-feedback)3.3422.3822.500w/ 2 iterations of present-feedback3.5392.8032.934w/ 4 iterations of present-feedback3.5002.8553.026MOOSE (w/o present-feedback)3.2242.7372.855w/ 2 iterations of present-feedback3.5793.2503.342w/ 4 iterations of present-feedback3.7503.1973.368</p>
<p>Table 5 :
5
Effect of present-feedback (evaluated by experts).Base model is GPT-3.5.</p>
<p>Table 6 :
6
Analysis of retrieval's effect on generated hypotheses (evaluated by GPT-4).No methods here utilize any feedback mechanisms.Base model is GPT-3.5.
Validness Novelty Helpfulness</p>
<p>Table 7 :
7
More ablation study (evaluated by GPT-4).Results are averaged over iterations of present-feedback.Base model is GPT-3.5.</p>
<p>Table 10 :
10
Statistics of the complexity of the dataset.
Validness Novelty HelpfulnessHard Consistency0.2980.3370.361Soft Consistency0.7550.7930.791</p>
<p>Table 11 :
11
Hard and soft consistency scores between evaluation from different experts in terms of Validness, Novelty, and Helpfulness metrics.</p>
<p>Table 12 :
12
Hard and soft consistency scores between expert evaluation and GPT-4 evaluation in terms of Validness, Novelty, and Helpfulness metrics.
Validness Novelty HelpfulnessHard Consistency0.4850.3920.321Soft Consistency0.8500.8230.773</p>
<p>Table 13 :
13
Evaluation standard for Validness.</p>
<p>Table 14 :
14
Evaluation standard for Novelty.</p>
<p>Table 15 :
15
Evaluation standard for Helpfulness.</p>
<p>Dataset, code, and generated  hypotheses are available at https://github.com/ZonglinY/MOOSE.git.
AcknowledgementThis research/project is supported by the Ministry of Education, Singapore under its MOE Academic Research Fund Tier 2 (STEM RIE2025 Award MOE-T2EP20123-0005).We thank Qingyun Wang, Jinjie Ni, Xulang Zhang, and Qika Lin for their insightful comments on the first finished draft of this work.
Ai Anthropic, The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card. 2024</p>
<p>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. Satanjeev Banerjee, Alon Lavie, Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or SummarizationAnn Arbor, Michigan2005Association for Computational Linguistics</p>
<p>Emergent autonomous scientific research capabilities of large language models. A Daniil, Robert Boiko, Gabe Macknight, Gomes, 10.48550/arXiv.2304.05332CoRR, abs/2304.053322023</p>
<p>COMET: Commonsense transformers for automatic knowledge graph construction. Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, Yejin Choi, 10.18653/v1/P19-1470Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, Italy2019Association for Computational Linguistics</p>
<p>Sam Andres M Bran, Andrew D Cox, Philippe White, Schwaller, arXiv:2304.05376Chemcrow: Augmenting large-language models with chemistry tools. 2023arXiv preprint</p>
<p>Casebased reasoning for natural language queries over knowledge bases. Rajarshi Das, Manzil Zaheer, Dung Thai, Ameya Godbole, Ethan Perez, Jay Yoon Lee, Lizhen Tan, Lazaros Polymenakos, Andrew Mccallum, 10.18653/v1/2021.emnlp-main.755Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Improving convenience or saving face? an empirical analysis of the use of facial recognition payment technology in retail. Jia Gao, Ying Rong, Xin Tian, Yuliang Yao, 2023Information Systems Research</p>
<p>The reasoning brain: The interplay between cognitive neuroscience and theories of reasoning. Vinod Goel, Gorka Navarrete, Ira A Noveck, Jérôme Prado, 2017</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. 2004</p>
<p>How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation. Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Noseworthy, Laurent Charlin, Joelle Pineau, 10.18653/v1/D16-1230Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasAssociation for Computational Linguistics2016</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Prasad Bodhisattwa, Shashank Majumder, Amir Gupta, Peter Yazdanbakhsh, Clark, 10.48550/arXiv.2303.17651CoRR, abs/2303.176512023</p>
<p>A little survey of induction. John D Norton, 10.48550/arXiv.2303.08774CoRR, abs/2303.08774OpenAI. 2023. GPT-4 technical report. 2003</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022In NeurIPS</p>
<p>Annotating and learning event durations in text. Feng Pan, Rutu Mulkar-Mehta, Jerry R Hobbs, 10.1162/COLI_a_00075Comput. Linguistics. 3742011</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>Check your facts and try again: Improving large language models with external knowledge and automated feedback. Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, Jianfeng Gao, 10.48550/arXiv.2302.12813CoRR, abs/2302.128132023</p>
<p>Measuring and narrowing the compositionality gap in language models. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, Mike Lewis, 10.48550/arXiv.2210.03350CoRR, abs/2210.033502022</p>
<p>Large language models are zero shot hypothesis proposers. Biqing Qi, Kaiyan Zhang, Haoxiang Li, Kai Tian, Sihang Zeng, Bowen Zhang-Ren Chen, Zhou, 10.48550/ARXIV.2311.05965CoRR, abs/2311.059652023</p>
<p>The probabilistic relevance framework: Bm25 and beyond. Stephen Robertson, Hugo Zaragoza, Foundations and Trends® in Information Retrieval. 342009</p>
<p>Reflexion: an agent with dynamic memory and self-reflection. Noah Shinn, Beck Labash, Ashwin Gopinath, 10.48550/arXiv.2303.11366CoRR, abs/2303.113662023</p>
<p>Scientific discovery in the age of artificial intelligence. Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, Nature. 62079722023a</p>
<p>Learning to generate novel scientific directions with contextualized literature-based discovery. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, 10.48550/arXiv.2305.14259CoRR, abs/2305.142592023b</p>
<p>Re3: Generating longer stories with recursive reprompting and revision. Kevin Yang, Yuandong Tian, Nanyun Peng, Dan Klein, 10.18653/v1/2022.emnlp-main.296Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022. December 7-11, 20222022</p>
<p>Language models as inductive reasoners. Zonglin Yang, Li Dong, Xinya Du, Hao Cheng, Erik Cambria, Xiaodong Liu, Jianfeng Gao, Furu Wei, Proceedings of the 18th Conference of the European Chapter. the Association for Computational Linguistics. the 18th Conference of the European ChapterSt. Julian's, MaltaAssociation for Computational Linguistics20241Long Papers</p>
<p>End-to-end case-based reasoning for commonsense knowledge base completion. Zonglin Yang, Xinya Du, Erik Cambria, Claire Cardie, Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics. the 17th Conference of the European Chapter of the Association for Computational LinguisticsDubrovnik, CroatiaAssociation for Computational Linguistics2023a</p>
<p>Jinjie Ni, and Erik Cambria. 2023b. Logical reasoning over natural language as knowledge representation: A survey. Zonglin Yang, Xinya Du, Rui Mao, 10.48550/arXiv.2303.12023CoRR, abs/2303.12023</p>
<p>Improving event duration prediction via time-aware pre-training. Zonglin Yang, Xinya Du, Alexander Rush, Claire Cardie, 10.18653/v1/2020.findings-emnlp.302Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational Linguistics2020</p>
<p>Goal driven discovery of distributional differences via language descriptions. Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, Jacob Steinhardt, 10.48550/arXiv.2302.14233CoRR, abs/2302.142332023</p>            </div>
        </div>

    </div>
</body>
</html>
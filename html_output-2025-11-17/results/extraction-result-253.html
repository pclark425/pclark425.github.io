<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-253 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-253</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-253</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-274234191</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2411.16260v3.pdf" target="_blank">Unraveling Arithmetic in Large Language Models: The Role of Algebraic Structures</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have demonstrated remarkable mathematical capabilities, largely driven by chain-of-thought (CoT) prompting, which decomposes complex reasoning into step-by-step solutions. This approach has enabled significant advancements, as evidenced by performance on benchmarks like GSM8K and MATH. However, the mechanisms underlying LLMs' ability to perform arithmetic in a single step of CoT remain poorly understood. Existing studies debate whether LLMs encode numerical values or rely on symbolic reasoning, while others explore attention and multi-layered processing in arithmetic tasks. In this work, we propose that LLMs learn arithmetic by capturing algebraic structures, such as commutativity and identity properties. Since these structures are observable through input-output relationships, they can generalize to unseen data. We empirically demonstrate that LLMs can learn algebraic structures using a custom dataset of arithmetic problems, as well as providing theoretical evidence showing that, under specific configurations of weights and biases, the transformer-based LLMs can generate embeddings that remain invariant to both permutations of input tokens and the presence of identity elements. Our findings indicate that leveraging algebraic structures can enhance the LLMs' arithmetic capabilities, offering insights into improving their arithmetic performance.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e253.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e253.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reinitialized GPT-2 experiments</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Empirical evaluation of a reinitialized GPT-2 on algebraic-structure arithmetic tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper trains a GPT-2 model with randomly initialized weights from scratch on synthetic datasets of modular-group expressions (Z_n) to test whether LLMs learn algebraic properties (commutativity and identity) rather than numeric computation. It measures generalization to unseen permutations and to insertion of identity tokens and compares operators that do and do not possess those properties.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (reinitialized)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>modular addition in finite Abelian groups Z_n (operators: + and a randomized operator ⊕ that is permutation- and identity-invariant), plus tests with non-commutative/counting operators (⊖) and left/right projection operators (⊳, ⊲)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>elements of Z_n for n in {7, 11, 13}; M = 6 operands per expression (i.e., 6-term modular sums); tokens are single-token representations z0..z_{n-1}</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>fine-tuning (training) from scratch on custom synthetic datasets; no chain-of-thought; single-step prediction (model must directly predict label); custom tokenizer mapping each z_i to one token; dataset splits deliberately hold out permutations/identity insertions; varied training set scale K (100..30,000) with fixed test K=1000</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>All tested tasks ultimately reach >99% test accuracy. Specifics: ⊳ and ⊲ reach ~99% test accuracy with few training samples; ⊖ and identity properties of + and ⊕ converge to ~100% by K≈1,000; commutativity of + and ⊕ requires larger training (K≈10,000–20,000) to exceed 99% but do reach >99% with sufficient K. Results reported across Z_7, Z_11, Z_13.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Empirical evidence that the model learns operator-driven algebraic structure (commutativity and identity) rather than numeric index-summing: performance on randomized operator ⊕ (whose outputs are unrelated numerically to inputs) matches +, showing no reliance on computing numeric indices; internal hidden states become invariant/clustered across permutations for commutative operators and invariant under identity insertion for identity-bearing operators (measured via S_std and S_dist metrics). Additionally, authors show a constructive attention-based mechanism (see separate theoretical result) that can produce permutation- and identity-invariant hidden states.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Performance scales with amount of task-specific training data (K). Learning commutativity required much larger K (10k–20k) than learning identity or projection operators (~1k or less). No model-size scaling experiments reported (authors reinitialized a single GPT-2 variant and hypothesize larger models could capture structures too).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Requires large numbers of training examples to learn commutativity (particularly for + and ⊕); with insufficient K the model memorizes training permutations but fails to generalize to held-out permutations/identity placements. The authors also note the possibility of trivial embedding solutions (operator-agnostic invariance) if datasets/architectures allow it, which they guard against by including non-commutative operators.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Comparison between operator '+' (numeric modular addition) and a randomized operator '⊕' that is permutation- and identity-invariant but not numerically related to inputs; also contrasted operators with and without commutativity/identity (⊖, ⊳, ⊲) to rule out trivial embedding solutions. Training dataset scale K varied to study data dependence.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>A reinitialized GPT-2 can learn and generalize algebraic properties (commutativity and identity) from examples of modular-group expressions without relying on explicit numeric computation, but learning commutativity requires substantially more task-specific training data than identity or trivial projection tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unraveling Arithmetic in Large Language Models: The Role of Algebraic Structures', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e253.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e253.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer invariance construction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Constructive theoretical proof that transformer attention layers can produce permutation- and identity-invariant hidden states</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper gives a concrete construction of attention weights, biases, and token embeddings (including special embeddings using very negative components) demonstrating that a transformer can produce hidden states invariant under permutations of operands (commutativity) and invariant to insertion of identity tokens (identity property) for operators that possess those algebraic properties.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer (attention-layer construction)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>transformer attention layers (analysis conducted in an ℓ-layer attention setup; example proof uses ℓ=1 attention layer as constructive illustration)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>formal modular-group addition (abstract operator with commutativity and identity properties); construction applies to inputs interleaving group elements and operator tokens</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>abstract finite Abelian group elements (Z_n), construction independent of numeric magnitude; examples use n arbitrary</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>mathematical constructive proof (set W_v = identity, set W_q and W_k to zero, biases set to ones, design embeddings with orthogonal word and position components and operator embeddings containing −∞ components), padding to large context L to force uniform attention; shows existence of weight/embedding assignments that yield required invariances</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Mechanism: uniform attention weights (achieved by zeroing query/key transforms and using constant positive biases) cause the '=' token's hidden state to be an (order-independent) average/sum of value vectors, producing permutation invariance; identity invariance is obtained by arranging embedding components so that identity-token and operator-token components nullify additional contributions (using mutually orthogonal components and −∞ entries), making hidden state unchanged when identity+operator inserted. The construction proves feasibility of operator-driven invariances and explains how attention can implement algebraic structure recognition.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Construction depends on special (non-generic) weight/embedding configurations (authors note non-uniqueness); trivial constructions could make invariances independent of operator, so model must tie invariances to operator embeddings to avoid failing on non-commutative operators.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Transformers can implement algebraic invariances (permutation and identity insertion) via specific attention/embedding configurations, providing a concrete mechanistic pathway by which LLMs could internalize commutativity and identity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unraveling Arithmetic in Large Language Models: The Role of Algebraic Structures', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e253.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e253.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>⊕ randomized-operator dataset</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Randomized permutation- and identity-invariant operator dataset (⊕) to exclude numeric computation solutions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset design in which a new operator ⊕ maps each multiset of operands to a random symbol r_i (outside Z_n) but is invariant to permutation and insertion of identity tokens; used to force models to learn algebraic invariance rather than numeric index-sum computation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (reinitialized) (used with ⊕ dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>operator ⊕ that is defined to be permutation- and identity-invariant (synthetic mapping), contrasted with '+' modular addition</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>same token vocabulary as '+' experiments (Z_n elements), M=6 operands, n in {7,11,13}; outputs are randomly assigned labels r_0..r_{n-1} not numerically related to inputs</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>dataset-level intervention: outputs for ⊕ are randomized per multiset of inputs to remove any numeric-pattern solution; training/testing splits arranged so that at least one permutation appears in training but other permutations are held out to test generalization of invariance</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Model learned commutativity and identity for ⊕ similarly to +; commutativity and identity learning for ⊕ required large K for full generalization (commutativity K≈10k–20k), and identity/other tasks reached high accuracy earlier (≈1k). '+' did not outperform '⊕', indicating models did not rely on numeric calculation when solving '+'.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>By making outputs numerically unrelated to input indices, equivalent performance on ⊕ vs + demonstrates that the model's successful generalization is due to learning operator-specific algebraic invariances rather than computing numeric index sums; inclusion of non-commutative operators rules out trivial operator-agnostic permutation solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Same data-scaling behavior as '+' experiments: commutativity requires much more data to converge; identity easier to learn.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>If dataset design did not enforce operator distinctions and include non-commutative operators, models could exploit trivial embedding solutions; the randomized mapping eliminates numeric shortcuts but still requires enough training instances to learn invariant mapping for many multisets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Direct comparison of '+' (numeric modular addition) vs '⊕' (random mapping) under identical training/test splits to test whether numeric computation accounts for performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>The randomized ⊕ dataset shows that LLMs can acquire permutation and identity invariances as abstract algebraic structures distinct from numeric computation: identical learning curves for + and ⊕ imply learning of algebraic structure rather than index-summing arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unraveling Arithmetic in Large Language Models: The Role of Algebraic Structures', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e253.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e253.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hidden-state invariance metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>S_std and S_dist hidden-state metrics for detecting commutativity/identity invariance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Metrics defined to quantify how similar transformer hidden states are across permutations (S_std) and how close hidden states are between base expressions and identity-inserted variants (S_dist); used to visualize and verify internal invariance emergence across layers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (reinitialized) (used to compute hidden-state metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer (layer-wise hidden state analysis across 13 layers of GPT-2)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>applied to commutative operators (+, ⊕) and non-commutative operators (⊖, ⊳, ⊲) over Z_n expressions</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>applied on same experimental settings: n in {7,11,13}, M=6</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>compute per-layer metrics S_std (sum of elementwise std across hidden states for all permutations of a multiset) and S_dist (sum of absolute differences between base hidden state and identity-inserted variants); analyze differences between '+' and non-commutative operators across training scales</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>As training scale K increases and model accuracy improves on commutative/identity tasks, S_std for commutative operator becomes substantially smaller than for non-commutative operators (S_com values more negative), and S_dist for identity decreases relative to non-identity operators (S_ide more negative). Visualization across layers shows emergence of invariances at internal layers.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Hidden-state clustering/invariance is a mechanistic signature: commutative operators produce low variance across permutation hidden states; identity insertion produces hidden states close to base expression. These layerwise signatures support the claim that the model represents algebraic properties internally (not merely memorizing specific token orders).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Layerwise invariance metrics strengthen with larger training set K and parallel the behavioral accuracy improvements; earlier layers show changes as K increases, and deeper layers reflect learned invariances when test accuracy is high.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>If model has not seen sufficient examples (small K), S_std and S_dist do not separate commutative vs non-commutative operators and test generalization fails.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared S_std and S_dist between '+' and each of {⊖, ⊳, ⊲} across layers and training scales.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Layerwise hidden-state similarity metrics provide mechanistic evidence that transformers internalize algebraic invariances: commutative and identity operators yield lower permutation-induced variance and smaller identity-induced hidden-state shifts than non-algebraic operators.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unraveling Arithmetic in Large Language Models: The Role of Algebraic Structures', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are symbolic learners in arithmetic. <em>(Rating: 2)</em></li>
                <li>Language models know the value of numbers. <em>(Rating: 2)</em></li>
                <li>How does gpt-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. <em>(Rating: 2)</em></li>
                <li>A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis. <em>(Rating: 2)</em></li>
                <li>Neural discovery of permutation subgroups. <em>(Rating: 2)</em></li>
                <li>A unified framework for discovering discrete symmetries. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-253",
    "paper_id": "paper-274234191",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "Reinitialized GPT-2 experiments",
            "name_full": "Empirical evaluation of a reinitialized GPT-2 on algebraic-structure arithmetic tasks",
            "brief_description": "This paper trains a GPT-2 model with randomly initialized weights from scratch on synthetic datasets of modular-group expressions (Z_n) to test whether LLMs learn algebraic properties (commutativity and identity) rather than numeric computation. It measures generalization to unseen permutations and to insertion of identity tokens and compares operators that do and do not possess those properties.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 (reinitialized)",
            "model_size": null,
            "model_architecture": "decoder-only transformer",
            "arithmetic_operation_type": "modular addition in finite Abelian groups Z_n (operators: + and a randomized operator ⊕ that is permutation- and identity-invariant), plus tests with non-commutative/counting operators (⊖) and left/right projection operators (⊳, ⊲)",
            "number_range_or_complexity": "elements of Z_n for n in {7, 11, 13}; M = 6 operands per expression (i.e., 6-term modular sums); tokens are single-token representations z0..z_{n-1}",
            "method_or_intervention": "fine-tuning (training) from scratch on custom synthetic datasets; no chain-of-thought; single-step prediction (model must directly predict label); custom tokenizer mapping each z_i to one token; dataset splits deliberately hold out permutations/identity insertions; varied training set scale K (100..30,000) with fixed test K=1000",
            "performance_result": "All tested tasks ultimately reach &gt;99% test accuracy. Specifics: ⊳ and ⊲ reach ~99% test accuracy with few training samples; ⊖ and identity properties of + and ⊕ converge to ~100% by K≈1,000; commutativity of + and ⊕ requires larger training (K≈10,000–20,000) to exceed 99% but do reach &gt;99% with sufficient K. Results reported across Z_7, Z_11, Z_13.",
            "mechanistic_insight": "Empirical evidence that the model learns operator-driven algebraic structure (commutativity and identity) rather than numeric index-summing: performance on randomized operator ⊕ (whose outputs are unrelated numerically to inputs) matches +, showing no reliance on computing numeric indices; internal hidden states become invariant/clustered across permutations for commutative operators and invariant under identity insertion for identity-bearing operators (measured via S_std and S_dist metrics). Additionally, authors show a constructive attention-based mechanism (see separate theoretical result) that can produce permutation- and identity-invariant hidden states.",
            "performance_scaling": "Performance scales with amount of task-specific training data (K). Learning commutativity required much larger K (10k–20k) than learning identity or projection operators (~1k or less). No model-size scaling experiments reported (authors reinitialized a single GPT-2 variant and hypothesize larger models could capture structures too).",
            "failure_modes": "Requires large numbers of training examples to learn commutativity (particularly for + and ⊕); with insufficient K the model memorizes training permutations but fails to generalize to held-out permutations/identity placements. The authors also note the possibility of trivial embedding solutions (operator-agnostic invariance) if datasets/architectures allow it, which they guard against by including non-commutative operators.",
            "comparison_baseline": "Comparison between operator '+' (numeric modular addition) and a randomized operator '⊕' that is permutation- and identity-invariant but not numerically related to inputs; also contrasted operators with and without commutativity/identity (⊖, ⊳, ⊲) to rule out trivial embedding solutions. Training dataset scale K varied to study data dependence.",
            "key_finding": "A reinitialized GPT-2 can learn and generalize algebraic properties (commutativity and identity) from examples of modular-group expressions without relying on explicit numeric computation, but learning commutativity requires substantially more task-specific training data than identity or trivial projection tasks.",
            "uuid": "e253.0",
            "source_info": {
                "paper_title": "Unraveling Arithmetic in Large Language Models: The Role of Algebraic Structures",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Transformer invariance construction",
            "name_full": "Constructive theoretical proof that transformer attention layers can produce permutation- and identity-invariant hidden states",
            "brief_description": "The paper gives a concrete construction of attention weights, biases, and token embeddings (including special embeddings using very negative components) demonstrating that a transformer can produce hidden states invariant under permutations of operands (commutativity) and invariant to insertion of identity tokens (identity property) for operators that possess those algebraic properties.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Transformer (attention-layer construction)",
            "model_size": null,
            "model_architecture": "transformer attention layers (analysis conducted in an ℓ-layer attention setup; example proof uses ℓ=1 attention layer as constructive illustration)",
            "arithmetic_operation_type": "formal modular-group addition (abstract operator with commutativity and identity properties); construction applies to inputs interleaving group elements and operator tokens",
            "number_range_or_complexity": "abstract finite Abelian group elements (Z_n), construction independent of numeric magnitude; examples use n arbitrary",
            "method_or_intervention": "mathematical constructive proof (set W_v = identity, set W_q and W_k to zero, biases set to ones, design embeddings with orthogonal word and position components and operator embeddings containing −∞ components), padding to large context L to force uniform attention; shows existence of weight/embedding assignments that yield required invariances",
            "performance_result": null,
            "mechanistic_insight": "Mechanism: uniform attention weights (achieved by zeroing query/key transforms and using constant positive biases) cause the '=' token's hidden state to be an (order-independent) average/sum of value vectors, producing permutation invariance; identity invariance is obtained by arranging embedding components so that identity-token and operator-token components nullify additional contributions (using mutually orthogonal components and −∞ entries), making hidden state unchanged when identity+operator inserted. The construction proves feasibility of operator-driven invariances and explains how attention can implement algebraic structure recognition.",
            "performance_scaling": null,
            "failure_modes": "Construction depends on special (non-generic) weight/embedding configurations (authors note non-uniqueness); trivial constructions could make invariances independent of operator, so model must tie invariances to operator embeddings to avoid failing on non-commutative operators.",
            "comparison_baseline": null,
            "key_finding": "Transformers can implement algebraic invariances (permutation and identity insertion) via specific attention/embedding configurations, providing a concrete mechanistic pathway by which LLMs could internalize commutativity and identity.",
            "uuid": "e253.1",
            "source_info": {
                "paper_title": "Unraveling Arithmetic in Large Language Models: The Role of Algebraic Structures",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "⊕ randomized-operator dataset",
            "name_full": "Randomized permutation- and identity-invariant operator dataset (⊕) to exclude numeric computation solutions",
            "brief_description": "A dataset design in which a new operator ⊕ maps each multiset of operands to a random symbol r_i (outside Z_n) but is invariant to permutation and insertion of identity tokens; used to force models to learn algebraic invariance rather than numeric index-sum computation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 (reinitialized) (used with ⊕ dataset)",
            "model_size": null,
            "model_architecture": "decoder-only transformer",
            "arithmetic_operation_type": "operator ⊕ that is defined to be permutation- and identity-invariant (synthetic mapping), contrasted with '+' modular addition",
            "number_range_or_complexity": "same token vocabulary as '+' experiments (Z_n elements), M=6 operands, n in {7,11,13}; outputs are randomly assigned labels r_0..r_{n-1} not numerically related to inputs",
            "method_or_intervention": "dataset-level intervention: outputs for ⊕ are randomized per multiset of inputs to remove any numeric-pattern solution; training/testing splits arranged so that at least one permutation appears in training but other permutations are held out to test generalization of invariance",
            "performance_result": "Model learned commutativity and identity for ⊕ similarly to +; commutativity and identity learning for ⊕ required large K for full generalization (commutativity K≈10k–20k), and identity/other tasks reached high accuracy earlier (≈1k). '+' did not outperform '⊕', indicating models did not rely on numeric calculation when solving '+'.",
            "mechanistic_insight": "By making outputs numerically unrelated to input indices, equivalent performance on ⊕ vs + demonstrates that the model's successful generalization is due to learning operator-specific algebraic invariances rather than computing numeric index sums; inclusion of non-commutative operators rules out trivial operator-agnostic permutation solutions.",
            "performance_scaling": "Same data-scaling behavior as '+' experiments: commutativity requires much more data to converge; identity easier to learn.",
            "failure_modes": "If dataset design did not enforce operator distinctions and include non-commutative operators, models could exploit trivial embedding solutions; the randomized mapping eliminates numeric shortcuts but still requires enough training instances to learn invariant mapping for many multisets.",
            "comparison_baseline": "Direct comparison of '+' (numeric modular addition) vs '⊕' (random mapping) under identical training/test splits to test whether numeric computation accounts for performance.",
            "key_finding": "The randomized ⊕ dataset shows that LLMs can acquire permutation and identity invariances as abstract algebraic structures distinct from numeric computation: identical learning curves for + and ⊕ imply learning of algebraic structure rather than index-summing arithmetic.",
            "uuid": "e253.2",
            "source_info": {
                "paper_title": "Unraveling Arithmetic in Large Language Models: The Role of Algebraic Structures",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Hidden-state invariance metrics",
            "name_full": "S_std and S_dist hidden-state metrics for detecting commutativity/identity invariance",
            "brief_description": "Metrics defined to quantify how similar transformer hidden states are across permutations (S_std) and how close hidden states are between base expressions and identity-inserted variants (S_dist); used to visualize and verify internal invariance emergence across layers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 (reinitialized) (used to compute hidden-state metrics)",
            "model_size": null,
            "model_architecture": "decoder-only transformer (layer-wise hidden state analysis across 13 layers of GPT-2)",
            "arithmetic_operation_type": "applied to commutative operators (+, ⊕) and non-commutative operators (⊖, ⊳, ⊲) over Z_n expressions",
            "number_range_or_complexity": "applied on same experimental settings: n in {7,11,13}, M=6",
            "method_or_intervention": "compute per-layer metrics S_std (sum of elementwise std across hidden states for all permutations of a multiset) and S_dist (sum of absolute differences between base hidden state and identity-inserted variants); analyze differences between '+' and non-commutative operators across training scales",
            "performance_result": "As training scale K increases and model accuracy improves on commutative/identity tasks, S_std for commutative operator becomes substantially smaller than for non-commutative operators (S_com values more negative), and S_dist for identity decreases relative to non-identity operators (S_ide more negative). Visualization across layers shows emergence of invariances at internal layers.",
            "mechanistic_insight": "Hidden-state clustering/invariance is a mechanistic signature: commutative operators produce low variance across permutation hidden states; identity insertion produces hidden states close to base expression. These layerwise signatures support the claim that the model represents algebraic properties internally (not merely memorizing specific token orders).",
            "performance_scaling": "Layerwise invariance metrics strengthen with larger training set K and parallel the behavioral accuracy improvements; earlier layers show changes as K increases, and deeper layers reflect learned invariances when test accuracy is high.",
            "failure_modes": "If model has not seen sufficient examples (small K), S_std and S_dist do not separate commutative vs non-commutative operators and test generalization fails.",
            "comparison_baseline": "Compared S_std and S_dist between '+' and each of {⊖, ⊳, ⊲} across layers and training scales.",
            "key_finding": "Layerwise hidden-state similarity metrics provide mechanistic evidence that transformers internalize algebraic invariances: commutative and identity operators yield lower permutation-induced variance and smaller identity-induced hidden-state shifts than non-algebraic operators.",
            "uuid": "e253.3",
            "source_info": {
                "paper_title": "Unraveling Arithmetic in Large Language Models: The Role of Algebraic Structures",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are symbolic learners in arithmetic.",
            "rating": 2,
            "sanitized_title": "language_models_are_symbolic_learners_in_arithmetic"
        },
        {
            "paper_title": "Language models know the value of numbers.",
            "rating": 2,
            "sanitized_title": "language_models_know_the_value_of_numbers"
        },
        {
            "paper_title": "How does gpt-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model.",
            "rating": 2,
            "sanitized_title": "how_does_gpt2_compute_greaterthan_interpreting_mathematical_abilities_in_a_pretrained_language_model"
        },
        {
            "paper_title": "A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis.",
            "rating": 2,
            "sanitized_title": "a_mechanistic_interpretation_of_arithmetic_reasoning_in_language_models_using_causal_mediation_analysis"
        },
        {
            "paper_title": "Neural discovery of permutation subgroups.",
            "rating": 2,
            "sanitized_title": "neural_discovery_of_permutation_subgroups"
        },
        {
            "paper_title": "A unified framework for discovering discrete symmetries.",
            "rating": 1,
            "sanitized_title": "a_unified_framework_for_discovering_discrete_symmetries"
        }
    ],
    "cost": 0.019818749999999996,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Published at ICLR 2025 Workshop on Reasoning and Planning for LLMs UNRAVELING ARITHMETIC IN LARGE LANGUAGE MODELS: THE ROLE OF ALGEBRAIC STRUCTURES
19 Apr 2025</p>
<p>Fu-Chieh Chang 
You-Chen Lin 
Pei-Yuan Wu peiyuanwu@ntu.edu.tw </p>
<p>MediaTek Research
TaipeiTaiwan</p>
<p>Graduate Institute of Communication Engineering
National Taiwan University
TaipeiTaiwan</p>
<p>Graduate Institute of Communication Engineering
National Taiwan University
TaipeiTaiwan</p>
<p>Graduate Institute of Communication Engineering
National Taiwan University
TaipeiTaiwan</p>
<p>Published at ICLR 2025 Workshop on Reasoning and Planning for LLMs UNRAVELING ARITHMETIC IN LARGE LANGUAGE MODELS: THE ROLE OF ALGEBRAIC STRUCTURES
19 Apr 2025E2F112D648EDB5860C7DC8A37098C71FarXiv:2411.16260v3[cs.LG]
Large language models (LLMs) have demonstrated remarkable mathematical capabilities, largely driven by chain-of-thought (CoT) prompting, which decomposes complex reasoning into step-by-step solutions.This approach has enabled significant advancements, as evidenced by performance on benchmarks like GSM8K and MATH.However, the mechanisms underlying LLMs' ability to perform arithmetic in a single step of CoT remain poorly understood.Existing studies debate whether LLMs encode numerical values or rely on symbolic reasoning, while others explore attention and multi-layered processing in arithmetic tasks.In this work, we propose that LLMs learn arithmetic by capturing algebraic structures, such as commutativity and identity properties.Since these structures are observable through input-output relationships, they can generalize to unseen data.We empirically demonstrate that LLMs can learn algebraic structures using a custom dataset of arithmetic problems, as well as providing theoretical evidence showing that, under specific configurations of weights and biases, the transformerbased LLMs can generate embeddings that remain invariant to both permutations of input tokens and the presence of identity elements.Our findings indicate that leveraging algebraic structures can enhance the LLMs' arithmetic capabilities, offering insights into improving their arithmetic performance.</p>
<p>INTRODUCTION</p>
<p>With the advancement of large-language models (LLMs), their mathematical capabilities have become a crucial factor in their success.This progress is largely attributed to chain-of-thought (CoT) prompting Wei et al. (2022), which enables LLMs to move beyond pattern matching and tackle complex reasoning problems through step-by-step guidance.The mathematical prowess of LLMs is demonstrated by several commercial models OpenAI (2024); Anthropic (2024) that have achieved notable success on benchmarks such as GSM8K Cobbe et al. (2021) and MATH Hendrycks et al. (2021).</p>
<p>Despite their achievements in solving arithmetic problems, the underlying mechanisms through which LLMs learn arithmetic operations from training data remain unclear.Although CoT explains how breaking down tasks into smaller steps facilitates mathematical problem solving, how LLMs process arithmetic tokens and execute arithmetic operations within individual steps of CoT are not yet well understood.Some studies Fangwei Zhu (2024); Levy &amp; Geva (2024) suggest that LLMs encode numerical values, while others Deng et al. (2024) propose that LLMs learn symbolic relationships between numbers rather than directly encoding their values.Furthermore, studies such as Gorceix et al. (2024) suggest that LLMs can learn mathematical rules.However, no consensus has been reached on how or why LLMs are capable of arithmetic reasoning.A detailed literature review is shown in Sec.A.1.</p>
<p>In this work, we provide a novel point of view on how LLMs acquire arithmetic abilities, suggesting that such skills stem from the learning of algebraic structures such as commutativity and identity.Since LLMs only observe input and output tokens rather than explicit numerical values, direct tokento-number mapping is challenging.Instead, LLMs infer algebraic structures by examining the relationship between inputs and outputs.Previous work Karjol et al. (2023;2024); Yang et al. (2023) has demonstrated that machine learning methods can learn symmetric structures from training data; however, these studies have not yet connected such findings to arithmetic learning in LLMs.In this work, we present both empirical and theoretical support showing that LLMs can learn algebraic structures from training data, generalizing these structures to unseen inputs.Our main contributions are as follows.</p>
<p>• Empirical Evidence: By constructing a dataset of arithmetic problems and splitting it into training and testing sets, we demonstrate that LLMs can learn and generalize the commutativity and identity properties to unseen inputs.</p>
<p>• Theoretical Construction: We provide a constructive proof illustrating how transformerbased models, with specific weight and bias configurations, can preserve hidden-state invariance under token permutations and the insertion of identity elements.</p>
<p>Overall, these findings suggest that LLMs can internalize algebraic structures, providing a foundation for designing strategies to further enhance their arithmetic capabilities.</p>
<p>METHODOLOGY 2.1 PROBLEM SETTINGS</p>
<p>We demonstrate that LLMs can learn algebraic structures from training samples and generalize to previously unseen instances.To keep our focus clear, we concentrate on arithmetic problems represented by numeric and operator symbols, rather than natural language or real-world contexts.Moreover, our study is set within a finite Abelian group (see Sec.A.1).A well-known example of a finite Abelian group is Z n , the set of integers modulo n under addition modulo n.For example, in Z 5 , the elements are {0, 1, 2, 3, 4}, and the group operation is defined as a + b mod 5.The identity element of this group is 0 , and every element a ∈ Z n has an inverse b ∈ Z n such that a + b ≡ 0 mod 5.In this work, we analyze the group operation properties of commutativity and identity in Z n .</p>
<p>DATASET FOR COMMUTATIVITY AND IDENTITY</p>
<p>In this work, we show that LLMs can acquire the concepts of commutativity and identity purely from the dataset we provide, rather than relying on any preexisting, pre-trained knowledge.To validate this, we construct a dataset of addition problems in Z n .Each element in Z n is denoted as z i , where z 0 = 0, z 1 = 1, . .., z n−1 = n − 1.The dataset consists of addition problems with M terms, formally expressed as:
z i1 + z i2 + • • • + z iM = z i1+i2+•••+iM mod n ,(1)
where 0 ≤ i 1 , i 2 , . . ., i M &lt; n.Here, mod denotes the modulo operator.For each problem, "z i1 + z i2 +• • •+z iM = " serves as input tokens, while the label is "z
i1+i2+•••+iM mod n
". Thus, the LLM must predict the correct element "z
i1+i2+•••+iM mod n
" given the inputs.In addition, we focus on the scenario that the model must directly predict the label from the input without performing any intermediate CoT reasoning.Fig. 1 and Appendix A.4 provide examples of both the training and testing sets for Z 7 .In the subsequent sections, we explain how the datasets are constructed so as to test whether LLMs genuinely learn and generalize the underlying algebraic properties of commutativity and identity.</p>
<p>Training, for Operator "+" Testing, for Operator "+" Commutativity Identity Commutativity Identity z3 + z4 + z5 + z5 + z5 + z6 = z0 z0 + z4 + z3 + z5 + z3 + z1 = z2 z2 + z3 + z3 + z5 + z5 + z6 = z3 z0 + z5 + z2 + z5 + z2 + z3 = z3 z4 + z5 + z3 + z5 + z5 + z6 = z0 z4 + z0 + z3 + z5 + z3 + z1 = z2 z3 + z3 + z5 + z5 + z6 + z2 = z3 z5 + z0 + z2 + z5 + z2 + z3 = z3 z3 + z5 + z5 + z6 + z4 + z5 = z0 z4 + z3 + z0 + z5 + z3 + z1 = z2 z5 + z6 + z5 + z3 + z2 + z3 = z3 z5 + z2 + z0 + z5 + z2 + z3 = z3 z4 + z5 + z5 + z6 + z3 + z5 = z0 z4 + z3 + z5 + z0 + z3 + z1 = z2 z3 + z2 + z5 + z3 + z6 + z5 = z3 z5 + z2 + z5 + z0 + z2 + z3 = z3 z6 + z5 + z5
+ z3 + z5 + z4 = z0 z4 + z3 + z5 + z3 + z0 + z1 = z2 z5 + z3 + z5 + z3 + z2 + z6 = z3 z5 + z2 + z5 + z2 + z0 + z3 = z3 z3 + z4 + z5 + z5 + z6 + z5 = z0 z4 + z3 + z5 + z3 + z1 + z0 = z2 z6 + z3 + z5 + z3 + z2 + z5 = z3 z5 + z2 + z5 + z2 + z3 + z0 = z3 z3 + z6 + z4 + z5 + z5 + z5 = z0 z4 + z3 + z5 + z3 + z1 = z2 z6 + z3 + z5 + z3 + z5 + z2 = z3 z5 + z2 + z5 + z2 + z3 = z3
Training, for Operator ⊕ Testing, for Operator ⊕</p>
<p>Commutativity</p>
<p>Identity</p>
<p>Commutativity Identity
z3 ⊕ z4 ⊕ z5 ⊕ z5 ⊕ z5 ⊕ z6 = r2 z0 ⊕ z4 ⊕ z3 ⊕ z5 ⊕ z3 ⊕ z1 = r1 z2 ⊕ z3 ⊕ z3 ⊕ z5 ⊕ z5 ⊕ z6 = r4 z0 ⊕ z5 ⊕ z2 ⊕ z5 ⊕ z2 ⊕ z3 = r5 z4 ⊕ z5 ⊕ z3 ⊕ z5 ⊕ z5 ⊕ z6 = r2 z4 ⊕ z0 ⊕ z3 ⊕ z5 ⊕ z3 ⊕ z1 = r1 z3 ⊕ z3 ⊕ z5 ⊕ z5 ⊕ z6 ⊕ z2 = r4 z5 ⊕ z0 ⊕ z2 ⊕ z5 ⊕ z2 ⊕ z3 = r5 z3 ⊕ z5 ⊕ z5 ⊕ z6 ⊕ z4 ⊕ z5 = r2 z4 ⊕ z3 ⊕ z0 ⊕ z5 ⊕ z3 ⊕ z1 = r1 z5 ⊕ z6 ⊕ z5 ⊕ z3 ⊕ z2 ⊕ z3 = r4 z5 ⊕ z2 ⊕ z0 ⊕ z5 ⊕ z2 ⊕ z3 = r5 z4 ⊕ z5 ⊕ z5 ⊕ z6 ⊕ z3 ⊕ z5 = r2 z4 ⊕ z3 ⊕ z5 ⊕ z0 ⊕ z3 ⊕ z1 = r1 z3 ⊕ z2 ⊕ z5 ⊕ z3 ⊕ z6 ⊕ z5 = r4 z5 ⊕ z2 ⊕ z5 ⊕ z0 ⊕ z2 ⊕ z3 = r5 z6 ⊕ z5 ⊕ z5 ⊕ z3 ⊕ z5 ⊕ z4 = r2 z4 ⊕ z3 ⊕ z5 ⊕ z3 ⊕ z0 ⊕ z1 = r1 z5 ⊕ z3 ⊕ z5 ⊕ z3 ⊕ z2 ⊕ z6 = r4 z5 ⊕ z2 ⊕ z5 ⊕ z2 ⊕ z0 ⊕ z3 = r5 z3 ⊕ z4 ⊕ z5 ⊕ z5 ⊕ z6 ⊕ z5 = r2 z4 ⊕ z3 ⊕ z5 ⊕ z3 ⊕ z1 ⊕ z0 = r1 z6 ⊕ z3 ⊕ z5 ⊕ z3 ⊕ z2 ⊕ z5 = r4 z5 ⊕ z2 ⊕ z5 ⊕ z2 ⊕ z3 ⊕ z0 = r5 z3 ⊕ z6 ⊕ z4 ⊕ z5 ⊕ z5 ⊕ z5 = r2 z4 ⊕ z3 ⊕ z5 ⊕ z3 ⊕ z1 = r1 z6 ⊕ z3 ⊕ z5 ⊕ z3 ⊕ z5 ⊕ z2 = r4 z5 ⊕ z2 ⊕ z5 ⊕ z2 ⊕ z3 = r5
Training, for Operator ⊖ Testing, for Operator ⊖
z3 ⊖ z4 ⊖ z5 ⊖ z5 ⊖ z5 ⊖ z6 = 2 z0 ⊖ z4 ⊖ z3 ⊖ z5 ⊖ z3 ⊖ z1 = 3 z2 ⊖ z3 ⊖ z3 ⊖ z5 ⊖ z5 ⊖ z6 = 2 z0 ⊖ z5 ⊖ z2 ⊖ z5 ⊖ z2 ⊖ z3 = 2 z4 ⊖ z5 ⊖ z3 ⊖ z5 ⊖ z5 ⊖ z6 = 2 z4 ⊖ z0 ⊖ z3 ⊖ z5 ⊖ z3 ⊖ z1 = 3 z3 ⊖ z3 ⊖ z5 ⊖ z5 ⊖ z6 ⊖ z2 = 3 z5 ⊖ z0 ⊖ z2 ⊖ z5 ⊖ z2 ⊖ z3 = 2 z3 ⊖ z5 ⊖ z5 ⊖ z6 ⊖ z4 ⊖ z5 = 2 z4 ⊖ z3 ⊖ z0 ⊖ z5 ⊖ z3 ⊖ z1 = 4 z5 ⊖ z6 ⊖ z5 ⊖ z3 ⊖ z2 ⊖ z3 = 3 z5 ⊖ z2 ⊖ z0 ⊖ z5 ⊖ z2 ⊖ z3 = 3 z4 ⊖ z5 ⊖ z5 ⊖ z6 ⊖ z3 ⊖ z5 = 2 z4 ⊖ z3 ⊖ z5 ⊖ z0 ⊖ z3 ⊖ z1 = 3 z3 ⊖ z2 ⊖ z5 ⊖ z3 ⊖ z6 ⊖ z5 = 3 z5 ⊖ z2 ⊖ z5 ⊖ z0 ⊖ z2 ⊖ z3 = 2 z6 ⊖ z5 ⊖ z5 ⊖ z3 ⊖ z5 ⊖ z4 = 4 z4 ⊖ z3 ⊖ z5 ⊖ z3 ⊖ z0 ⊖ z1 = 3 z5 ⊖ z3 ⊖ z5 ⊖ z3 ⊖ z2 ⊖ z6 = 3 z5 ⊖ z2 ⊖ z5 ⊖ z2 ⊖ z0 ⊖ z3 = 3 z3 ⊖ z4 ⊖ z5 ⊖ z5 ⊖ z6 ⊖ z5 = 2 z4 ⊖ z3 ⊖ z5 ⊖ z3 ⊖ z1 ⊖ z0 = 4 z6 ⊖ z3 ⊖ z5 ⊖ z3 ⊖ z2 ⊖ z5 = 3 z5 ⊖ z2 ⊖ z5 ⊖ z2 ⊖ z3 ⊖ z0 = 3 z3 ⊖ z6 ⊖ z4 ⊖ z5 ⊖ z5 ⊖ z5 = 3 z4 ⊖ z3 ⊖ z5 ⊖ z3 ⊖ z1 = 3 z6 ⊖ z3 ⊖ z5 ⊖ z3 ⊖ z5 ⊖ z2 = 3 z5 ⊖ z2 ⊖ z5 ⊖ z2 ⊖ z3 = 2
Figure 1: Illustration of dataset for operator "+", ⊕ and ⊖.Notice that the same set of tokens is maintained across all operators to ensure that certain token combinations appear exclusively either in the training set or the testing set, as required.</p>
<p>Commutativity: To demonstrate that LLMs can learn the commutative property, we begin by selecting a sequence z i1 , z i2 , . . ., z iM from Z n , where z i1 ≤ z i2 ≤ • • • ≤ z iM and z i1 &gt; 0, and generate sequences in the form of Eq. equation 1. Next, we sample several permutations of z i1 , z i2 , . . ., z iM , denoting each permutation as z j1 , z j2 , . . ., z jM , and generate sequences as in Eq. equation 1 accordingly.For any given sequence z i1 , z i2 , . . ., z iM , to allow the model to recognize the element z i1+i2+•••+iM mod n in the first place, at least one permutation of each sequence in testsing set appears in the training set.However, if more than one permutations are included in the training set, all other permutations of this same sequence are excluded from the training set and placed in the testing set.This ensures that the model must generalize the commutativity property, i.e. it must infer correct outputs for unseen permutations in the testing set, based on the only one permutation learned during training.As shown in the upper row of Fig. 1, every permutation of (z 3 , z 4 , z 5 , z 5 , z 5 , z 6 ) is included in the training set (highlighted in red).Meanwhile, the testing set contains a different permutation sequences of Eq. equation 1, (z 2 , z 3 , z 3 , z 5 , z 5 , z 6 ), highlighted in orange.However, one permutation of that sequence-(z 6 , z 3 , z 5 , z 3 , z 5 , z 2 )-does appear in the training set, ensuring that the model is exposed to at least one variant of the same element sum.</p>
<p>Identity: In Z n , the identity element for addition is z 0 .To verify whether LLMs learn this identity property, we first pick M − 1 variables, z i1 , . . ., z iM−1 from Z n , and insert z 0 among them in all possible positions.Substituting each arrangement into Eq.equation 1 yields M distinct equations:
z 0 + z i1 + z i2 + • • • + z iM−1 = z i1+i2+•••+iM−1 mod n , z i1 + z 0 + z i2 + • • • + z iM−1 = z i1+i2+•••+iM−1 mod n , . . . z i1 + z i2 + • • • + z iM−1 + z 0 = z i1+i2+•••+iM−1 mod n .
To ensure that the model does not merely exploit permutation invariance, we assign all possible insertions of z 0 in the sequence z i1 , . . ., z iM−1 exclusively to either the training set or the testing set.Moreover, because the value of z i1+i2+•••+iM−1 mod n should be established using the equation without the identity element, the following base equation without z 0 must be included in the training set:
z i1 + z i2 + • • • + z iM−1 = z i1+i2+•••+iM−1 mod n .
In the first row of Fig. 1, we illustrate how identity elements appear in both the training and testing sets.The training set includes the base equation, z 4 + z 3 + z 5 + z 3 + z 1 = z 2 , along with variants where z 0 is inserted into every possible position (highlighted in blue).In contrast, the equation z 0 + z 5 + z 2 + z 5 + z 2 + z 3 = z 3 and its variants with z 0 inserted in different positions are placed in the testing set (highlighted in cyan).However, the base equation z 5 + z 2 + z 5 + z 2 + z 3 = z 3 appears in the training set, ensuring that the model can infer z 0 + z 5 + z 2 + z 5 + z 2 + z 3 = z 3 when recognizing z 0 as the identity element.</p>
<p>DATASET TO EXCLUDE NUMERICAL CALCULATION</p>
<p>Training on the dataset introduced in Sec.2.2 leaves the possibility that LLMs might exploit numerical relationships inherent in the indices.Specifically, an LLM could potentially identify the input token indices and rely on computing the modulo sum of these indices to derive results, bypassing the application of commutativity and identity principles.For example, in Fig. 1, the dataset for commutativity contains z 6 + z 3 + z 5 + z 3 + z 5 + z 2 = z 3 in training set and z 2 + z 3 + z 3 + z 5 + z 5 + z 6 = z 3 in testing set.When answering the test equation, we anticipate that LLMs are able to leverage commutativity to infer z 3 from the training equation.Nonetheless, we cannot entirely rule out the possibility that an LLM could deduce the numerical value of the index (2, 3, 3, 5, 5, 6) and use the sum (2 + 3 + 3 + 5 + 5 + 6) mod 7 = 3.To prevent this, we introduce a new operator ⊕ which takes the same inputs as "+" but produces outputs unrelated to the inputs in a numerical sense.Specifically, given any sequence z i1 , z i2 , . . ., z iM in Z n where z i1 ≤ z i2 ≤ • • • ≤ z iM and z i1 &gt; 0, the result of applying ⊕ to this sequence (or any permutation of it) is a randomly selected element r i from the set {r 0 , r 1 , . . ., r n−1 }, whose elements lie outside Z n .Hence, ⊕ is invariant under input permutations.Namely,
z i1 ⊕ z i2 ⊕ • • • ⊕ z iM = r i , z i2 ⊕ z i1 ⊕ • • • ⊕ z iM = r i , • • • , z iM ⊕ z iM−1 ⊕ • • • ⊕ z i1 = r i .
Besides, ⊕ is invariant under insertion of the identity element z 0 at any position such that
z 0 ⊕ z i1 ⊕ z i2 ⊕ • • • ⊕ z iM = r i , • • • , z i1 ⊕ z i2 ⊕ • • • ⊕ z iM ⊕ z 0 = r i .
Hence, for LLMs to perform well on this dataset, they cannot rely on the numerical relationship of the index; rather, they must learn the commutative property and identity property of ⊕.We include a corresponding dataset involving ⊕ in both training and testing.As shown in the second row of Fig. 1, we replace "+" with ⊕ and adjust the resulting values accordingly.By comparing how "+" and ⊕ perform on commutativity and identity tasks under the same training and test inputs in Z n , we can discern whether the performance of "+" benefits from numerical computation or purely from algebraic structures.</p>
<p>DATASET TO AVOID TRIVIAL SOLUTIONS</p>
<p>In this work, our goal is for LLMs to learn the commutative and identity properties specifically for the "+" and ⊕ operator.If the model were trained only on the dataset described in the previous sections, it could adopt a trivial solution in which commutativity and identity trivially apply to all tokens z i regardless of the existence of the operator "+" or ⊕.For instance, by setting all position embeddings to zero (see Sec. 2.5).To prevent this, we include an additional dataset that features operators that lack commutativity and identity properties.Specifically, we introduce three new operators, ⊖, ⊳, and ⊲.Details of these operators are provided as follows.</p>
<p>Operator ⊖ : Counts of Encountering z 0 along the Cyclic Group Z n .It is straightforward to show that Z n is a cyclic group, as illustrated below:
z 5 z 4 z 3 z 2 • • • z n−1 z 0 z 1
We define ⊖ : Z n × Z n → N as an operator that counts the number of times z 0 is encountered.Concretely, for any z i , z j ∈ Z n , z i ⊖ z j equals the number of occurrences of z 0 when traveling from z i to z j around the cyclic group.
z i → z (i+1) mod n → z (i+2) mod n → • • • → z (j−1) mod n → z j .
For example, in Z 5 , z 3 ⊖ z 1 = 1 because traveling from z 3 → z 4 → z 0 → z 1 encounters z 0 once.Similarly, z 2 ⊖ z 5 = 0 because z 2 → z 3 → z 4 → z 5 does not pass through z 0 .We set z i ⊖ z i = 1 because one must traverse all elements of Z n to return to the same element.We also define z i ⊖z j ⊖z k as the total number of z 0 encounters when traveling from z i to z j , then continuing to z k , such that
z i ⊖ z j ⊖ z k = (z i ⊖ z j ) + (z j ⊖ z k ).
For instance, in Z 5 , z 4 ⊖ z 2 ⊖ z 1 = 2. Besides, it is straightforward to verify that ⊖ does not satisfy commutativity, nor is z 0 an identity element, because z i ⊖ z j = z j ⊖ z i (for all i = j with i, j &gt; 0), and z i ⊖ z j = z i ⊖ z 0 ⊖ z j (for all i &lt; j with i, j &gt; 0).We add a dataset involving the ⊖ operator to the training set and testing set.As shown in Fig. 1, we replace "+" with ⊖ and update the resulting values accordingly.Note that the output of ⊖ is a natural number in N rather than an element of Z n .</p>
<p>Operators ⊳ and ⊲ : Left-Hand Side and Right-Hand Side Elements.Alongside the ⊖ operator, we introduce two additional operators, ⊳ and ⊲, which also lack commutativity and identity elements.These operators simply return the left-hand-side or right-hand-side argument, respectively:
z i ⊲ z j = z j and z i ⊳ z j = z i .
It is straightforward to see that these operators do not satisfy commutativity and that z 0 is not an identity element for either.However, they satisfy associative, so expressions such as z i ⊲ z j ⊲ z k produce a unique result.We add a dataset that includes these operators in both the training and testing sets.An example of the whole dataset, which includes all operators "+", ⊕, ⊖, ⊳ and ⊲ is shown in Sec.A.4.In the following section, we discuss how LLMs can learn the underlying algebraic structures from these datasets.Wq,bq,Wk,bk,Wv ,bv Wq,bq,Wk,bk,Wv ,bv Wq,bq,Wk,bk,Wv ,bv Wq,bq,Wk,bk,Wv ,bv Wq,bq,Wk,bk,Wv ,bv Figure 2: Illustration of the symbols defined for the hidden states of tokens and the variables for the attnetion layers
z i1 + • • • + z iM = e i1,1 e +,2 e +,2M−2 e iM ,2M−1 e =,2M s (ℓ−1) 1 s (ℓ−1) 2 s (ℓ−1) 2M−2 s (ℓ−1) 2M−1 s (ℓ−1) 2M q 1 , k 1 , v 1 q 2 , k 2 , v 2 q 2M−2 , k 2M−2 , v 2M−2 q 2M−1 , k 2M−1 , v 2M−1 q 2M , k 2M , v 2M s (ℓ) 2M = 2M i=1 σ( q ⊤ 2M ki 2M j=1 q ⊤ 2M kj )v i z (i1+i2+•••+iM ) mod n</p>
<p>HOW DO LLMS LEARN ALGEBRAIC STRUCTURE?</p>
<p>In this section, we provide theoretical evidence that LLMs composed of attention layers can be constructed to compute addition under commutativity and identity.We present an example using Z n with M input elements as an illustrative example (see Fig. 2).The input sequence intersperses elements z i ∈ Z n with the symbols "+" and "=", resulting in a total length of 2M .We denote e i , where i ∈ {0, ..., n − 1} as the embedding for z i ∈ Z n , and let e + and e = represent the embeddings for the "+" and "=" tokens, respectively.If z i appears in position m where 1 ≤ m ≤ 2M , its embedding is e i,m .Analogously, if a "+" or "=" symbol appears at position m, its embedding is e +,m or e =,m .These embeddings consist of word embedding w i and position embedding p m , as
e i,m = [w i , p m ] ⊤ for i ∈ {0, 1, . . . , n − 1} ∪ {+, =}.
We assume that all the vectors w i and p m are mutually orthogonal.In the following section, we illustrate how a language model can enforce commutativity for the "+" operator by providing a proof through construction.Specifically, we assign explicit values to the model's weights and biases, assuming that these parameters can be learned from training.In the ℓ-th attention layer, we denote s
(ℓ−1) m
as the hidden state at position m from the previous attention layer.Let W q , W k , W v and b q , b k , b v denote the attention weights and biases that produce the query, key, and value, respectively, and let q m , k m , and v m denote the query, key, and value vectors at position m in the ℓ-th attention layer, respectively.Furthermore, we define s (ℓ) 2M</p>
<p>as the hidden state in position 2M after ℓ attention layers.Namely, s
(ℓ) 2M = 2M i=1 σ( q ⊤ 2M ki 2M j=1 q ⊤ 2M kj )v i .
The following theorem demonstrates that LLM can learn hidden states to achieve commutativity.</p>
<p>Theorem 2.1 (Commutativity-Invariant to the Input Permutations).Given the LLMs' settings mentioned in Sec.2.5, there exists a special assignment of the weights and biases
W q , W k , W v and b q , b k , b v and specific assignment of embeddings e i,m , for i ∈ {0, 1, . . . , n − 1} ∪ {+, =}, such that s (ℓ) 2M could be invariant to the permutation of input elements z i1 , z i2 , • • • , z iM ∈ Z n .</p>
<p>However, this invariance holds only when the input contains commutative operators.</p>
<p>Proof.The proof can be found in Sec.A.3.1.</p>
<p>With the invariance of the hidden states s (ℓ) 2M , the subsequent layers of the transformer could serve as a classifier, mapping s (ℓ) 2M to the token z (i1+i2+•••+iM ) mod n , and hence endow the addition operation with commutativity.In addition to commutativity, we present a theorem demonstrating how an LLM can produce hidden states that remain essentially unchanged under the insertion of identity elements.</p>
<p>Theorem 2.2 (Identity-Invariant to the Insertion of Identity Tokens).Under the LLM settings in Sec.2.5, let s
(ℓ)
2M ′ where M ′ = M + 1 denote the hidden state after inserting an identity token z 0 and an operator's token into the input sequence.There exists a specific assignment of weights and biases
W q , W k , W v and b q , b k , b v , together with particular embeddings e i,m for i ∈ {0, 1, . . . , n − 1} ∪ {+, =}, such that s (ℓ) 2M ′ is equal to s (ℓ)
2M .However, this property is valid only when the input includes operators for which z 0 serves as the identity element.</p>
<p>Proof.The proof can be found in Sec.A.3.2.</p>
<p>With this theorem, a classifier can interpret s
(ℓ) 2M ′ as s (ℓ)
2M , which is already learned from base equation in the training set.This ensures that the appending z 0 does not alter the outcome, thus reflecting the identity property.Remark 2.3 (Non-uniqueness of Weights and Bias Assignments).Note that the weights, biases, and embeddings described in the proof of these theorems represent only one possible configuration to achieve commutativity and identity; many others could also be valid.However, it is critical that these properties be triggered specifically by operators with the properties of commutativity and ideneity, rather than by the operand tokens themselves.</p>
<p>Here we discuss a solution in which commutativity and identity arise without the existence of operators' embeddings.Remark 2.4 (Trivial Solution of Embeddings).Language models may converge to a trivial embedding solution to achieve commutativity and identity.For instance, one might assign all nonidentity tokens z 1 , z 2 , . . ., z n−1 , +, = zero-valued position embeddings: e i,m = [w i , 0] ⊤ for i ∈ {1, 2, . . ., n−1}∪{+, =}, and give the identity token z 0 zero-valued word and position embeddings: e 0,m = [ 0, 0 ] ⊤ .While this setup indeed satisfies both commutativity and identity, these properties are no longer tied to the operator itself.Consequently, the model fails to produce correct outputs for operators lacking commutativity and identity-such as ⊖, ⊳, and ⊲-when using these same trivial embeddings.</p>
<p>EXPERIMENTS</p>
<p>Settings: We conduct our experiments using the datasets described in Sec.2.2, Sec.2.3 and 2.4, which encompass addition problems that test for commutativity and identity of operator "+" and ⊕, as well as operations involving ⊖, ⊳, and ⊲.We set n = 7, 11 and 13 for Z n and the number of input elements M = 6.For the language model, we choose GPT-2 Radford et al. ( 2019) but reinitialize its weights before training to strip away any pre-existing knowledge, ensuring that the model acquires its understanding of algebraic structures solely from our data.In addition, we customize the tokenizer so that each element is represented as a single token (e.g., z 10 becomes the token [z10] rather than several character-based tokens).For reproducibility, we have made our experimental code publicly available1 .</p>
<p>Dataset Construction: We construct both training and testing sets by first choosing a scale K, which determines the number of examples.Each training set or testing set with scale K contains 10K instances, including</p>
<p>• 4K instances: Operators with commutativity and identity, including +'s commutativity, +'s identity, ⊕'s commutativity, and ⊕'s identity.Each of them contains K instances.</p>
<p>• 6K instances: Operator without commutativity and identity, including ⊖, ⊳, and ⊲.Each of them contains 2K instances.</p>
<p>An example illustrating both training and testing with K = 50 appears in Sec.A.4.Throughout subsequent experiments, we fix the K = 1000 for the testing set, while K ranges from 100 to 30,000 for training set.</p>
<p>RESULTS</p>
<p>TRAINING DYNAMICS</p>
<p>We investigate how the training progresses for the case Z 7 when K = 3000 for the training set.The upper row of Fig. 3  We vary the size of the training set from K = 100 to K = 30,000 and measure testing accuracy once both the training and testing accuracy have plateaued.The results, depicted in the second row of Fig. 3, yield the following observations.</p>
<p>All Tasks Achieve Over 99% Testing Accuracy: We find that ⊳ and ⊲ are the most easily learned, each achieving 99% testing accuracy with relatively few training samples.Next are ⊖ and the identity properties of "+" and ⊕, which converge to 100% at around K = 1000.The most challenging part is learning the commutative properties of "+" and ⊕, which requires K between 10,000 and 20,000 to reach over 99% testing acuracy.Despite these differences, all tasks ultimately achieve over 99% accuracy, suggesting that commutativity and identity learning is indeed operator-driven rather than a trivial result of embeddings.</p>
<p>Generalization of Commutative Operations.Despite the number of training instances required to achieve high accuracy appears large, it is still much smaller than the full combinatorial space of expressions like z i1 + z i2 + • • • + z i6 , which, for instance, includes (7 − 1) 6 = 46,656 possibilities in Z 7 and (13 − 1) 6 = 2,985,984 in Z 13 .Thus, LLMs can actually learn and generalize commutativity for both "+" and ⊕ without enumerating all possible permutations.</p>
<p>No Reliance on Numerical Computation We observe that "+" does not exceed ⊕ in performance, indicating that LLMs learn commutativity and identity rather than relying on a direct numerical calculation.It also provides evidence that LLMs could not acquire computation skills for numerical values if the numerical values are not explicitly specified in the input.</p>
<p>VISUALIZATION OF HIDDEN STATES</p>
<p>Commutative: As pointed out in Sec.2.5, when the operator preserves commutativity, the hidden states remain invariant under permuting the inputs.In practice, however, these states need not be identical; it suffices that hidden states from different permutations be recognized as the same category.Consequently, given a set of input tokens, we expect slight variation (that is, a small standard deviation) in the hidden states across different permutations.For example, assume that the inputs are z i1 , z i2 , • • • , z im and the output is y.Let ⊙ denote an operator.Then, considering all permutations, we have
y 1 = z i1 ⊙ z i2 ⊙ • • • ⊙ z im , y 2 = z i2 ⊙ z i1 ⊙ • • • ⊙ z i3 , • • • , y m = z im ⊙ • • • ⊙ z i2 ⊙ z i1 .
If we define s i ∈ R D as the hidden state of each y i where D is the size of hidden states, then for a commutative operator ⊙, we would have y 1 = y 2 = • • • = y m .Consequently, s 1 , s 2 , . . ., s m should also be similar.The sum of their element-wise standard-deviation is defined by S std (s 1 , . . ., s m ; ⊙) = D k=1 std {s 1 } k , . . ., {s m } k , where std(x 1 , • • • , x m ) denotes the standard deviation among x 1 , • • • , x m , {s 1 } k denotes the k-th element of s 1 , and ⊙ indicates that the hidden states are produced by the operator ⊙.For a non-commutative operator ⊙ ′ , these hidden states would differ more substantially, resulting in a higher value of S std (s 1 , . . ., s m ; ⊙ ′ ).The left column of Fig. 4 shows the differences in S std between the "+" operator and various non-commutative operators, denoted as:
S (ℓ) com (+, ⊙ ′ ) = S std (s (ℓ) 1 , . . . , s (ℓ) m ; +) − S std (s (ℓ)
1 , . . ., s (ℓ) m ; ⊙ ′ ), where ⊙ ′ ∈ {⊖, ⊳, ⊲}.Here, ℓ denotes the layer index (GPT-2 has 13 layers), and each column of the heat map in Fig. 4 corresponds to one of these layers.As the scale of the training set increases and the accuracy of the model in commutative operations improves, S std for the commutative operator "+" becomes noticeably smaller compared to that of the non-commutative operators and consequently S com (+, ⊙ ′ ) become more negative.</p>
<p>Identity: We consider non-identity tokens z 1 , z 2 , • • • , z m and an identity token z 0 to show the hidden states when the operator ⊙ remains invariant in the presence of an identity element.Concretely, we compare the outputs
ȳ = z 1 ⊙ z 2 ⊙ • • • ⊙ z m , y 1 = z 0 ⊙ z 1 ⊙ z 2 ⊙ • • • ⊙ z m , • • • , y m = z 1 ⊙ z 2 ⊙ • • • ⊙ z m ⊙ z 0 .
where ȳ is the result without z 0 .We denote s and s 1 , s 2 , . . ., s m as the hidden states of ȳ and y 1 , y 2 , . . ., y m , respectively.If y i for i ∈ {1, ..., m} and ȳ the same under the insertion of identity elements.Then, the distance between s and any of s i for i ∈ {1, ..., m} should be small.The sum of their distances is defined by
S dist (s, s 1 , . . . , s m ; ⊙) = D k=1 m i=1 |{s i } k − {s} k |
, where {s i } k denotes the k-th element of s i , and ⊙ indicates that the hidden states are produced by the operator ⊙.For an operator without an identity element, denoted as ⊙ ′ , the distance between these two hidden states would be substantially larger, resulting in a larger value of S dist (s, s 1 , . . ., s m ; ⊙ ′ ).The right column of Fig. 4 shows the differences in S dist between the "+" operator and various operators without identity elements, denoted as:
S (ℓ) ide (+, ⊙ ′ ) = S dist (s (ℓ) , s (ℓ) 1 , . . . , s (ℓ) m ; +) − S dist (s (ℓ) , s(ℓ)
1 , . . ., s (ℓ) m ; ⊙ ′ ), where ⊙ ′ ∈ {⊖, ⊳, ⊲}.As more training data is used and the model becomes better at identity-invariant operations, the value of S dist for the "+" operator decreases significantly compared to operators without identity elements, leading to more negative values of S ide (+, ⊙ ′ ).</p>
<p>LIMITATIONS</p>
<p>In this work, we assume our problem scope is limited to a finite Abelian group Z 5 , focusing exclusively on commutativity and identity.Other properties such as inverse and associativity, remain to be verified.Furthermore, real-world mathematical problems often involve real numbers and diverse forms of descriptions including natural language.Despite these limitations, we believe that our research takes the first step toward unraveling the mystery of the mathematical capabilities of LLMs.On the other hand, we tested only a relatively small LLM, GPT-2.Nevertheless, we hypothesize that larger models, with greater expressive power, are also capable of capturing algebraic structures within training data.</p>
<p>CONCLUSION</p>
<p>We have demonstrated that LLMs can learn and internalize fundamental algebraic properties, especially commutativity and identity, purely from training data.Our strategy involved constructing a dataset of finite Abelian group expressions, ensuring that both commutative and identity instances appear in training and are held out for testing.Using a reinitialized GPT-2, we observed successful generalization to unseen tasks.Furthermore, We also provided a constructive proof showing how transformer-based models preserve invariance under permutations and identity insertion.Hiddenstate visualizations revealed that operators preserving commutativity and identity produced more uniform internal representations compared to those that did not.Although our experiments centered on finite Abelian groups and basic algebraic properties, these results indicate the potential for LLMs to acquire and generalize more intricate algebraic structures directly from data.Extensions to larger systems, real numbers, advanced group properties, and more natural language settings remain promising directions for future research.</p>
<p>A APPENDIX</p>
<p>A.1 RELATED WORKS Theory of Chain-of-thought Reasoning in LLMs: Chain-of-Thought (CoT) techniques Wei et al. (2022) empower large language models (LLMs) to tackle complex mathematical reasoning tasks by breaking solutions into sequential steps, making them essential for solving mathematical problems.Recent studies shed light on CoT's theoretical underpinnings.For example, Prystawski et al. (2024) models CoT with Bayesian networks, where questions, answers, and reasoning steps form interconnected nodes, demonstrating that structured reasoning improves LLM performance.Xiao &amp; Liu (2024) introduces the concept of length generalization, showing that LLMs can extrapolate from simple examples to address more complex problems.Expanding the PAC learning framework, Malach (2023) shows that auto-regressive learners can effectively learn linear threshold circuits when CoT steps are provided.Additionally, Feng et al. (2024) proves that CoT enables transformers to handle dynamic programming problems, even with polynomially increasing complexity.Although these studies establish a theoretical basis for CoT, which decomposes complex mathematical problems into manageable steps, they rarely address how LLMs solve mathematical problems within a single step of CoT reasoning.</p>
<p>Enhancing mathematical reasoning in LLMs: Several recent works have developed different fine-tuning strategies to improve LLMs' mathematical reasoning.First, Guo et al. (2024) mainly focuses on improving the "reversal curse" by introducing a reverse training task, thereby enhancing logical consistency.Similarly, Zhou &amp; Zhao (2024) enhances the CoT ability by introducing two auxiliary tasks, including Intermediate Reasoning State Prediction and Instruction Reconstruction task, which model mathematical reasoning from both forward and reverse direction.Moreover, Liu et al. (2023) provides three fine-tuning methods to improve the LLMs' performance on mathematical problems.By utilizing the supervision signal of the evaluation tasks, these methods effectively improve the model performance in generating solutions for math problems.Meanwhile, Yin &amp; Yin (2024) proposes Scaffolding Learning, which first allows the model to master arithmetic operations and then fine-tunes it efficiently on the more general task of solving word math problems.Futhermore, Lyu et al. (2024) provides a two-component fine-tuning method, consisting of World Knowledge Distillation (WKD) and Tool Usage Adaptation (TUA).By leveraging these two components, the model surpasses state-of-the-art models such as GPT-4o in mathematical problemsolving.Finally, Tang et al. (2024) proposes a method called MathScale, which is used to construct the fine-tuning dataset MathScaleQA to enhance mathematical reasoning capabilities.Additionally, MWPBENCH is introduced as a benchmark to systematically evaluate performance.Although mathematical reasoning in these works has been enhanced, the underlying principles behind the reasoning process remain unknown.This gap suggests the need for further exploration in how LLMs solve such problems.To better understand these mechanism, we should start with the fundamental aspects, such as arithmetic, to uncover their underlying principles.</p>
<p>Improving Arithmetic ability in LLMs:</p>
<p>The LLMs have demonstrated their power in natural language process tasks.However, they still exhibit limitations when it comes to performing arithmetic calculations.Recent studies have explored the application based on fine-tuning techniques to enhance the arithmetic capabilities of LLMs.For example, Liu et al. (2025) propose ArithemticGPT, which enhances advanced arithmetic calculation, such as exponentiation, logarithms, and trigonometric functions.Similarly, Liu &amp; Low (2023) propose supervised fine-tuning, mainly focuses on large-number arithmetic problem, particularly improving addition and developing decomposition strategies for multiplication and division.Nye et al. (2021) applied scratchpads fine-tuning, enabling the model to generalize to unseen 9-digit addition.In a different approach, Zhang et al. (2024) examines the inner component responsible for arithmetic calculations and uses the precise fine-tuning to enhance the attention head values and MLPs within the associated components.While Lai et al. (2024) fine-tunes LLMs to imitate Turing machine behavior, enabling step-by-step arithmetic calculations and enhancing their computational capability.Beyond fine-tuning, alternative methods have been proposed, Shen et al. (2024) apply RevOrder, a technique that reverses the arithmetic output order, to fine-tune LLMs, leading to a significant reduction in calculation errors.Schwartz et al. (2024) incorporates digit length information as a prefix, enabling the model to better understand numerical magnitude, thereby improving its arithmetic performance.Despite these improvements, these studies primarily aim to enhance arithmetic capabilities, rather than understanding the fundamental principle of how LLMs could acquire arithematic ability.Consequently, a deeper investigation into how LLMs internalize and generalize arithmetic concepts is still needed.</p>
<p>How Arithmetic Abilities Arise in LLMs:</p>
<p>The mechanisms behind LLMs' arithmetic abilities remain debated.Some studies suggest that LLMs encode numerical values internally.Fangwei Zhu (2024) demonstrates this by using linear probes on addition problems, showing that number values are encoded across layers and can be extracted.On the other hand, Levy &amp; Geva (2024) finds that LLM errors are distributed across digits rather than numeric values, revealing that numbers are represented with per-digit circular structures in base 10.In addition, other works argue that LLMs rely on symbolic reasoning.Deng et al. (2024) shows LLMs learn simple patterns at the edges of a sequence of numbers faster than in the middle of the numbers of a sequence, indicating an easy-to-hard learning approach and symbolic arithmetic processing.Hanna et al. (2024) explores GPT-2 small's mechanism for predicting valid end years in date-related tasks, identifying a circuit responsible for "greater-than" comparisons that generalize across contexts.Additionally, Stolfo et al. (2023) demonstrates that LLMs transmit query-relevant information through attention mechanisms and process results with MLP modules, integrating them into the residual stream.Despite these insights, there is no consensus on whether LLMs primarily encode numerical values or rely on symbolic reasoning, highlighting the need for further research to clarify their mathematical processing mechanisms.</p>
<p>Machine Learning for Symmetric Discovery: The ability to discover symmetries enables machine learning models to uncover algebraic structures from training data.Karjol et al. (2023) demonstrate that sub-groups can be identified through a neural network with a specially designed architecture, supported by a general theorem.They validate their approach with numerical experiments on tasks such as image-digit sum and symmetric polynomial regression.Similarly, Karjol et al. (2024) present a unified framework for discovering symmetries across various subgroups, including locally symmetric, dihedral, and cyclic subgroups.Their architecture combines linear, matrix-valued, and non-linear functions to systematically capture invariance.Yang et al. (2023) introduce Latent Lie-GAN (LaLiGAN), a generative model that maps data to a latent space where nonlinear symmetries become linear.LaLiGAN simultaneously learns the mapping and the latent space symmetries, theoretically proving its ability to express nonlinear symmetries under specific group action conditions.However, these works do not explore the connection between symmetry learning and large language models' (LLMs) arithmetic capabilities.While Imani &amp; Palangi (2024) reveals that LLMs struggle with fundamental group properties and exhibit vulnerabilities in arithmetic reasoning, it does not investigate whether LLMs are possible to learn algebraic structures from training data.In contrast, our work demonstrates that LLMs can learn algebraic structures from training instances and generalize to solve unseen arithmetic problems.</p>
<p>LLMs can Learn Mathematical Rules:</p>
<p>The work most closely related to ours is Gorceix et al. (2024), where the authors propose that LLMs can learn mathematical rules, such as distributivity or equation simplification.Although distributivity is also a type of algebraic structure, our work still has significant difference from them.First, our research demonstrates that LLMs can learn algebraic structures by training from scratch, showing that these rules are learned solely from the arithmetic equations we provide.We also rule out the possibility that LLMs learn these roles without any numerical computation.This differs from their approach, which relies on pre-trained models and cannot rule out the possibility that these rules were acquired from external materials or numerical computations.Additionally, our work further provides theoretical evidences of how transformers learn algebraic structures and providing an analysis of the hidden states of transformers, aspects that are not addressed in their study.</p>
<p>A.2 BACKGROUND KNOWLEDGE</p>
<p>Definition A.1.A finite Abelian group is a set G with a binary operation • that satisfies the following properties:</p>
<p>• Closure: For any a, b ∈ G, the results of a • b are in G.
• Associativity: For any a, b, c ∈ G, (a • b) • c = a • (b • c). • Commutativity: For any a, b ∈ G, a • b = b • a.
• Identity: There exists an element e ∈ G such that for any a ∈ G, a • e = e • a = a.</p>
<p>• Inverse: For each a ∈ G, there exists an element a −1 ∈ G such that a • a −1 = a −1 • a = e.</p>
<p>A.3 PROOF OF THEOREMS</p>
<p>A.3.1 PROOF OF THEOREM 2.1</p>
<p>Proof.Here, we prove this theorem by constructing a concrete example.In this example, the layer index ℓ = 1, and hence the hidden state of the previous layer s (ℓ−1) m at position m is the input embedding e i,m .We choose W v to be the identity matrix, so the value vector at position m is simply:
v m = W v e i,m = e i,m .
To enforce commutativity for "+", we assign the embedding of "+" at position m as
e +,m = w + , −∞ ⊤ ,
where −∞ represents the smallest floating-point value.Additionally, we set both W q and W k to zero matrices and make the biases b q and b k all ones.We assume that the context window size of attention is L, where L ≫ 2M and the remaining positions 2M &lt; m ≤ L are padded with zero embeddings e m = [0, 0] ⊤ for all 2M &lt; m ≤ L. Under these settings, the attention weights in the first layer become uniform across positions, i.e.,
σ q ⊤ m k i L j=1 q ⊤ m k j = 1 L for all 1 ≤ i, m ≤ L.
Thus, the hidden state of the "=" token at position 2M after the first attention layer, denoted s</p>
<p>(1) 2M , is
s (1) 2M = 2M i=1 σ q ⊤ 2M k i 2M j=1 q ⊤ 2M k j v i = 2M m=1 1 L e i,m = 1 L 2M m=1 w im , −∞ ⊤ . Since 1 L 2M m=1 w im , −∞
⊤ is invariant to the position embeddings of e i1 , . . ., e iM relative to e + , this hidden state does not depend on the order of the input tokens z i1 , . . ., z iM .</p>
<p>A.3.2 PROOF OF THEOREM 2.2</p>
<p>Proof.Building on the setup from the previous section, let each non-identity token z i (i = 0) at position m have the embedding
e i,m = [w i , 0, p m ] ⊤ for i ∈ {1, 2, . . . , n − 1},
where w i and p m are mutually orthogonal vectors.We then define the embedding of the identity token z 0 at position m as e 0,m = [0, w 0 , p m ] ⊤ , and the embedding of the addition operator "+" at position m as
e +,m = [0, −∞, −∞] ⊤ .
When we append z 0 with an extra "+" operator to the sequence of input tokens, the position of "=" become 2M ′ = 2M + 2, and the hidden state after the attention layer at position 2M ′ is
s (1) 2M ′ = 2M m=1 e i,m + e +,2M+1 + e 0,2M+2 L = 1 L 2M m=1 w im , −∞, −∞ ⊤ ,
and the hidden states before inserting z 0 is
s (1) 2M = 2M m=1 e i,m L = 1 L 2M m=1 w im , −∞, −∞ ⊤ = s (1) 2M . A.4 EXAMPLE OF DATASET FOR Z 7 WITH K = 50
Training, for Operator +'s Commutativity
z2 + z2 + z4 + z3 + z6 + z4 = z0 z4 + z3 + z6 + z4 + z2 + z2 = z0 z3 + z6 + z4 + z4 + z2 + z2 = z0 z6 + z2 + z3 + z2 + z4 + z4 = z0 z2 + z3 + z4 + z2 + z6 + z4 = z0 z2 + z6 + z4 + z4 + z3 + z2 = z0 z6 + z2 + z4 + z2 + z4 + z3 = z0 z4 + z2 + z6 + z2 + z3 + z4 = z0 z6 + z2 + z2 + z3 + z4 + z4 = z0 z6 + z4 + z2 + z2 + z3 + z4 = z0 z6 + z6 + z4 + z4 + z2 + z3 = z4 z6 + z4 + z2 + z6 + z4 + z3 = z4 z4 + z6 + z2 + z3 + z6 + z4 = z4 z4 + z2 + z6 + z4 + z6 + z3 = z4 z6 + z4 + z6 + z3 + z2 + z4 = z4 z4 + z4 + z6 + z2 + z3 + z6 = z4 z6 + z3 + z4 + z6 + z4 + z2 = z4 z3 + z6 + z2 + z4 + z4 + z6 = z4 z6 + z3 + z4 + z4 + z6 + z2 = z4 z6 + z2 + z4 + z4 + z3 + z6 = z4 z5 + z5 + z6 + z2 + z3 + z3 = z3 z5 + z2 + z3 + z5 + z6 + z3 = z3 z6 + z2 + z3 + z5 + z3 + z5 = z3 z5 + z2 + z3 + z3 + z5 + z6 = z3 z3 + z5 + z5 + z2 + z3 + z6 = z3 z5 + z6 + z3 + z2 + z3 + z5 = z3 z2 + z5 + z3 + z3 + z6 + z5 = z3 z6 + z2 + z3 + z3 + z5 + z5 = z3 z3 + z5 + z3 + z6 + z5 + z2 = z3 z6 + z3 + z2 + z3 + z5 + z5 = z3 z5 + z5 + z5 + z3 + z5 + z2 = z4 z5 + z5 + z2 + z5 + z3 + z5 = z4 z3 + z5 + z5 + z2 + z5 + z5 = z4 z5 + z2 + z5 + z5 + z3 + z5 = z4 z5 + z3 + z2 + z5 + z5 + z5 = z4 z3 + z5 + z5 + z5 + z2 + z5 = z4 z5 + z5 + z3 + z5 + z5 + z2 = z4 z5 + z5 + z5 + z3 + z2 + z5 = z4 z5 + z2 + z3 + z5 + z5 + z5 = z4 z2 + z5 + z3 + z5 + z5 + z5 = z4 z6 + z6 + z2 + z5 + z5 + z3 = z6 z6 + z6 + z5 + z2 + z5 + z3 = z6 z5 + z6 + z3 + z2 + z5 + z6 = z6 z5 + z3 + z2 + z6 + z6 + z5 = z6 z6 + z5 + z4 + z1 + z2 + z4 = z1 z6 + z2 + z3 + z5 + z6 + z1 = z2 z4 + z6 + z2 + z2 + z4 + z5 = z2 z2 + z3 + z6 + z1 + z5 + z5 = z1 z5 + z4 + z5 + z5 + z4 + z2 = z4 z4 + z1 + z1 + z4 + z5 + z1 = z2
Testing, for Operator +'s Commutativity
z0 + z4 + z2 + z4 + z6 + z3 = z5 z4 + z0 + z2 + z4 + z6 + z3 = z5 z4 + z2 + z0 + z4 + z6 + z3 = z5 z4 + z2 + z4 + z0 + z6 + z3 = z5 z4 + z2 + z4 + z6 + z0 + z3 = z5 z4 + z2 + z4 + z6 + z3 + z0 = z5 z0 + z5 + z1 + z1 + z5 + z3 = z1 z5 + z0 + z1 + z1 + z5 + z3 = z1 z5 + z1 + z0 + z1 + z5 + z3 = z1 z5 + z1 + z1 + z0 + z5 + z3 = z1 z5 + z1 + z1 + z5 + z0 + z3 = z1 z5 + z1 + z1 + z5 + z3 + z0 = z1 z0 + z2 + z1 + z2 + z6 + z2 = z6 z2 + z0 + z1 + z2 + z6 + z2 = z6 z2 + z1 + z0 + z2 + z6 + z2 = z6 z2 + z1 + z2 + z0 + z6 + z2 = z6 z2 + z1 + z2 + z6 + z0 + z2 = z6 z2 + z1 + z2 + z6 + z2 + z0 = z6 z0 + z6 + z1 + z2 + z5 + z4 = z4 z6 + z0 + z1 + z2 + z5 + z4 = z4 z6 + z1 + z0 + z2 + z5 + z4 = z4 z6 + z1 + z2 + z0 + z5 + z4 = z4 z6 + z1 + z2 + z5 + z0 + z4 = z4 z6 + z1 + z2 + z5 + z4 + z0 = z4 z0 + z1 + z4 + z6 + z1 + z3 = z1 z1 + z0 + z4 + z6 + z1 + z3 = z1 z1 + z4 + z0 + z6 + z1 + z3 = z1 z1 + z4 + z6 + z0 + z1 + z3 = z1 z1 + z4 + z6 + z1 + z0 + z3 = z1 z1 + z4 + z6 + z1 + z3 + z0 = z1 z0 + z4 + z1 + z2 + z4 + z6 = z3 z4 + z0 + z1 + z2 + z4 + z6 = z3 z4 + z1 + z0 + z2 + z4 + z6 = z3 z4 + z1 + z2 + z0 + z4 + z6 = z3 z4 + z1 + z2 + z4 + z0 + z6 = z3 z4 + z1 + z2 + z4 + z6 + z0 = z3 z0 + z4 + z4 + z1 + z2 + z2 = z6 z4 + z0 + z4 + z1 + z2 + z2 = z6 z4 + z4 + z0 + z1 + z2 + z2 = z6 z4 + z4 + z1 + z0 + z2 + z2 = z6 z4 + z4 + z1 + z2 + z0 + z2 = z6 z4 + z4 + z1 + z2 + z2 + z0 = z6 z0 + z2 + z1 + z3 + z5 + z4 = z1 z2 + z0 + z1 + z3 + z5 + z4 = z1
Training, for Operator ⊕'s Commutativity
z2 ⊕ z2 ⊕ z4 ⊕ z3 ⊕ z6 ⊕ z4 = r4 z4 ⊕ z3 ⊕ z6 ⊕ z4 ⊕ z2 ⊕ z2 = r4 z3 ⊕ z6 ⊕ z4 ⊕ z4 ⊕ z2 ⊕ z2 = r4 z6 ⊕ z2 ⊕ z3 ⊕ z2 ⊕ z4 ⊕ z4 = r4 z2 ⊕ z3 ⊕ z4 ⊕ z2 ⊕ z6 ⊕ z4 = r4 z2 ⊕ z6 ⊕ z4 ⊕ z4 ⊕ z3 ⊕ z2 = r4 z6 ⊕ z2 ⊕ z4 ⊕ z2 ⊕ z4 ⊕ z3 = r4 z4 ⊕ z2 ⊕ z6 ⊕ z2 ⊕ z3 ⊕ z4 = r4 z6 ⊕ z2 ⊕ z2 ⊕ z3 ⊕ z4 ⊕ z4 = r4 z6 ⊕ z4 ⊕ z2 ⊕ z2 ⊕ z3 ⊕ z4 = r4 z6 ⊕ z6 ⊕ z4 ⊕ z4 ⊕ z2 ⊕ z3 = r5 z6 ⊕ z4 ⊕ z2 ⊕ z6 ⊕ z4 ⊕ z3 = r5 z4 ⊕ z6 ⊕ z2 ⊕ z3 ⊕ z6 ⊕ z4 = r5 z4 ⊕ z2 ⊕ z6 ⊕ z4 ⊕ z6 ⊕ z3 = r5 z6 ⊕ z4 ⊕ z6 ⊕ z3 ⊕ z2 ⊕ z4 = r5 z4 ⊕ z4 ⊕ z6 ⊕ z2 ⊕ z3 ⊕ z6 = r5 z6 ⊕ z3 ⊕ z4 ⊕ z6 ⊕ z4 ⊕ z2 = r5 z3 ⊕ z6 ⊕ z2 ⊕ z4 ⊕ z4 ⊕ z6 = r5 z6 ⊕ z3 ⊕ z4 ⊕ z4 ⊕ z6 ⊕ z2 = r5 z6 ⊕ z2 ⊕ z4 ⊕ z4 ⊕ z3 ⊕ z6 = r5 z5 ⊕ z5 ⊕ z6 ⊕ z2 ⊕ z3 ⊕ z3 = r0 z5 ⊕ z2 ⊕ z3 ⊕ z5 ⊕ z6 ⊕ z3 = r0 z6 ⊕ z2 ⊕ z3 ⊕ z5 ⊕ z3 ⊕ z5 = r0 z5 ⊕ z2 ⊕ z3 ⊕ z3 ⊕ z5 ⊕ z6 = r0 z3 ⊕ z5 ⊕ z5 ⊕ z2 ⊕ z3 ⊕ z6 = r0 z5 ⊕ z6 ⊕ z3 ⊕ z2 ⊕ z3 ⊕ z5 = r0 z2 ⊕ z5 ⊕ z3 ⊕ z3 ⊕ z6 ⊕ z5 = r0 z6 ⊕ z2 ⊕ z3 ⊕ z3 ⊕ z5 ⊕ z5 = r0 z3 ⊕ z5 ⊕ z3 ⊕ z6 ⊕ z5 ⊕ z2 = r0 z6 ⊕ z3 ⊕ z2 ⊕ z3 ⊕ z5 ⊕ z5 = r0 z5 ⊕ z5 ⊕ z5 ⊕ z3 ⊕ z5 ⊕ z2 = r3 z5 ⊕ z5 ⊕ z2 ⊕ z5 ⊕ z3 ⊕ z5 = r3 z3 ⊕ z5 ⊕ z5 ⊕ z2 ⊕ z5 ⊕ z5 = r3 z5 ⊕ z2 ⊕ z5 ⊕ z5 ⊕ z3 ⊕ z5 = r3 z5 ⊕ z3 ⊕ z2 ⊕ z5 ⊕ z5 ⊕ z5 = r3 z3 ⊕ z5 ⊕ z5 ⊕ z5 ⊕ z2 ⊕ z5 = r3 z5 ⊕ z5 ⊕ z3 ⊕ z5 ⊕ z5 ⊕ z2 = r3 z5 ⊕ z5 ⊕ z5 ⊕ z3 ⊕ z2 ⊕ z5 = r3 z5 ⊕ z2 ⊕ z3 ⊕ z5 ⊕ z5 ⊕ z5 = r3 z2 ⊕ z5 ⊕ z3 ⊕ z5 ⊕ z5 ⊕ z5 = r3 z6 ⊕ z6 ⊕ z2 ⊕ z5 ⊕ z5 ⊕ z3 = r3 z6 ⊕ z6 ⊕ z5 ⊕ z2 ⊕ z5 ⊕ z3 = r3 z5 ⊕ z6 ⊕ z3 ⊕ z2 ⊕ z5 ⊕ z6 = r3 z5 ⊕ z3 ⊕ z2 ⊕ z6 ⊕ z6 ⊕ z5 = r3 z6 ⊕ z5 ⊕ z4 ⊕ z1 ⊕ z2 ⊕ z4 = r3 z6 ⊕ z2 ⊕ z3 ⊕ z5 ⊕ z6 ⊕ z1 = r1 z4 ⊕ z6 ⊕ z2 ⊕ z2 ⊕ z4 ⊕ z5 = r3 z2 ⊕ z3 ⊕ z6 ⊕ z1 ⊕ z5 ⊕ z5 = r5 z5 ⊕ z4 ⊕ z5 ⊕ z5 ⊕ z4 ⊕ z2 = r2 z4 ⊕ z1 ⊕ z1 ⊕ z4 ⊕ z5 ⊕ z1 = r4
Testing, for Operator ⊕'s Commutativity
z4 ⊕ z4 ⊕ z5 ⊕ z6 ⊕ z2 ⊕ z1 = r3 z4 ⊕ z2 ⊕ z6 ⊕ z5 ⊕ z4 ⊕ z1 = r3 z4 ⊕ z4 ⊕ z2 ⊕ z1 ⊕ z6 ⊕ z5 = r3 z6 ⊕ z2 ⊕ z1 ⊕ z5 ⊕ z4 ⊕ z4 = r3 z6 ⊕ z4 ⊕ z1 ⊕ z4 ⊕ z2 ⊕ z5 = r3 z5 ⊕ z2 ⊕ z4 ⊕ z4 ⊕ z6 ⊕ z1 = r3 z4 ⊕ z5 ⊕ z4 ⊕ z6 ⊕ z1 ⊕ z2 = r3 z6 ⊕ z5 ⊕ z1 ⊕ z4 ⊕ z2 ⊕ z4 = r3 z1 ⊕ z2 ⊕ z4 ⊕ z6 ⊕ z5 ⊕ z4 = r3 z6 ⊕ z1 ⊕ z6 ⊕ z2 ⊕ z3 ⊕ z5 = r1 z2 ⊕ z5 ⊕ z3 ⊕ z1 ⊕ z6 ⊕ z6 = r1 z3 ⊕ z2 ⊕ z6 ⊕ z1 ⊕ z5 ⊕ z6 = r1 z6 ⊕ z2 ⊕ z5 ⊕ z1 ⊕ z3 ⊕ z6 = r1 z6 ⊕ z6 ⊕ z3 ⊕ z5 ⊕ z2 ⊕ z1 = r1 z1 ⊕ z6 ⊕ z5 ⊕ z6 ⊕ z3 ⊕ z2 = r1 z5 ⊕ z1 ⊕ z6 ⊕ z3 ⊕ z6 ⊕ z2 = r1 z5 ⊕ z6 ⊕ z6 ⊕ z3 ⊕ z1 ⊕ z2 = r1 z5 ⊕ z3 ⊕ z6 ⊕ z2 ⊕ z6 ⊕ z1 = r1 z2 ⊕ z4 ⊕ z5 ⊕ z2 ⊕ z4 ⊕ z6 = r3 z4 ⊕ z5 ⊕ z4 ⊕ z2 ⊕ z2 ⊕ z6 = r3 z2 ⊕ z5 ⊕ z2 ⊕ z4 ⊕ z4 ⊕ z6 = r3 z5 ⊕ z2 ⊕ z6 ⊕ z4 ⊕ z2 ⊕ z4 = r3 z5 ⊕ z4 ⊕ z6 ⊕ z2 ⊕ z4 ⊕ z2 = r3 z2 ⊕ z4 ⊕ z2 ⊕ z6 ⊕ z4 ⊕ z5 = r3 z4 ⊕ z2 ⊕ z4 ⊕ z5 ⊕ z6 ⊕ z2 = r3 z2 ⊕ z4 ⊕ z5 ⊕ z6 ⊕ z4 ⊕ z2 = r3 z4 ⊕ z5 ⊕ z2 ⊕ z2 ⊕ z4 ⊕ z6 = r3 z2 ⊕ z1 ⊕ z6 ⊕ z5 ⊕ z5 ⊕ z3 = r5 z6 ⊕ z5 ⊕ z3 ⊕ z1 ⊕ z5 ⊕ z2 = r5 z6 ⊕ z2 ⊕ z5 ⊕ z5 ⊕ z1 ⊕ z3 = r5 z3 ⊕ z6 ⊕ z1 ⊕ z5 ⊕ z2 ⊕ z5 = r5 z5 ⊕ z2 ⊕ z5 ⊕ z1 ⊕ z6 ⊕ z3 = r5 z3 ⊕ z1 ⊕ z6 ⊕ z2 ⊕ z5 ⊕ z5 = r5 z3 ⊕ z6 ⊕ z5 ⊕ z2 ⊕ z5 ⊕ z1 = r5 z6 ⊕ z5 ⊕ z1 ⊕ z3 ⊕ z2 ⊕ z5 = r5 z1 ⊕ z5 ⊕ z5 ⊕ z2 ⊕ z6 ⊕ z3 = r5 z2 ⊕ z5 ⊕ z5 ⊕ z5 ⊕ z4 ⊕ z4 = r2 z4 ⊕ z5 ⊕ z5 ⊕ z2 ⊕ z5 ⊕ z4 = r2 z2 ⊕ z5 ⊕ z5 ⊕ z4 ⊕ z5 ⊕ z4 = r2 z2 ⊕ z5 ⊕ z4 ⊕ z5 ⊕ z5 ⊕ z4 = r2 z5 ⊕ z4 ⊕ z5 ⊕ z5 ⊕ z2 ⊕ z4 = r2 z2 ⊕ z4 ⊕ z5 ⊕ z4 ⊕ z5 ⊕ z5 = r2 z5 ⊕ z4 ⊕ z2 ⊕ z5 ⊕ z5 ⊕ z4 = r2 z5 ⊕ z4 ⊕ z5 ⊕ z2 ⊕ z5 ⊕ z4 = r2 z5 ⊕ z2 ⊕ z4 ⊕ z5 ⊕ z5 ⊕ z4 = r2 z4 ⊕ z1 ⊕ z4 ⊕ z5 ⊕ z1 ⊕ z1 = r4 z1 ⊕ z5 ⊕ z4 ⊕ z1 ⊕ z1 ⊕ z4 = r4 z1 ⊕ z1 ⊕ z4 ⊕ z1 ⊕ z5 ⊕ z4 = r4 z4 ⊕ z1 ⊕ z1 ⊕ z5 ⊕ z4 ⊕ z1 = r4 z1 ⊕ z5 ⊕ z1 ⊕ z4 ⊕ z1 ⊕ z4 = r4
Training, for Operator ⊕'s Identity
z0 ⊕ z4 ⊕ z3 ⊕ z5 ⊕ z3 ⊕ z1 = r6 z4 ⊕ z0 ⊕ z3 ⊕ z5 ⊕ z3 ⊕ z1 = r6 z4 ⊕ z3 ⊕ z0 ⊕ z5 ⊕ z3 ⊕ z1 = r6 z4 ⊕ z3 ⊕ z5 ⊕ z0 ⊕ z3 ⊕ z1 = r6 z4 ⊕ z3 ⊕ z5 ⊕ z3 ⊕ z0 ⊕ z1 = r6 z4 ⊕ z3 ⊕ z5 ⊕ z3 ⊕ z1 ⊕ z0 = r6 z4 ⊕ z3 ⊕ z5 ⊕ z3 ⊕ z1 = r6 z0 ⊕ z1 ⊕ z5 ⊕ z6 ⊕ z6 ⊕ z1 = r0 z1 ⊕ z0 ⊕ z5 ⊕ z6 ⊕ z6 ⊕ z1 = r0 z1 ⊕ z5 ⊕ z0 ⊕ z6 ⊕ z6 ⊕ z1 = r0 z1 ⊕ z5 ⊕ z6 ⊕ z0 ⊕ z6 ⊕ z1 = r0 z1 ⊕ z5 ⊕ z6 ⊕ z6 ⊕ z0 ⊕ z1 = r0 z1 ⊕ z5 ⊕ z6 ⊕ z6 ⊕ z1 ⊕ z0 = r0 z1 ⊕ z5 ⊕ z6 ⊕ z6 ⊕ z1 = r0 z0 ⊕ z5 ⊕ z2 ⊕ z5 ⊕ z2 ⊕ z3 = r0 z5 ⊕ z0 ⊕ z2 ⊕ z5 ⊕ z2 ⊕ z3 = r0 z5 ⊕ z2 ⊕ z0 ⊕ z5 ⊕ z2 ⊕ z3 = r0 z5 ⊕ z2 ⊕ z5 ⊕ z0 ⊕ z2 ⊕ z3 = r0 z5 ⊕ z2 ⊕ z5 ⊕ z2 ⊕ z0 ⊕ z3 = r0 z5 ⊕ z2 ⊕ z5 ⊕ z2 ⊕ z3 ⊕ z0 = r0 z5 ⊕ z2 ⊕ z5 ⊕ z2 ⊕ z3 = r0 z0 ⊕ z3 ⊕ z1 ⊕ z2 ⊕ z2 ⊕ z2 = r4 z3 ⊕ z0 ⊕ z1 ⊕ z2 ⊕ z2 ⊕ z2 = r4 z3 ⊕ z1 ⊕ z0 ⊕ z2 ⊕ z2 ⊕ z2 = r4 z3 ⊕ z1 ⊕ z2 ⊕ z0 ⊕ z2 ⊕ z2 = r4 z3 ⊕ z1 ⊕ z2 ⊕ z2 ⊕ z0 ⊕ z2 = r4 z3 ⊕ z1 ⊕ z2 ⊕ z2 ⊕ z2 ⊕ z0 = r4 z3 ⊕ z1 ⊕ z2 ⊕ z2 ⊕ z2 = r4 z0 ⊕ z2 ⊕ z4 ⊕ z4 ⊕ z3 ⊕ z4 = r2 z2 ⊕ z0 ⊕ z4 ⊕ z4 ⊕ z3 ⊕ z4 = r2 z2 ⊕ z4 ⊕ z0 ⊕ z4 ⊕ z3 ⊕ z4 = r2 z2 ⊕ z4 ⊕ z4 ⊕ z0 ⊕ z3 ⊕ z4 = r2 z2 ⊕ z4 ⊕ z4 ⊕ z3 ⊕ z0 ⊕ z4 = r2 z2 ⊕ z4 ⊕ z4 ⊕ z3 ⊕ z4 ⊕ z0 = r2 z2 ⊕ z4 ⊕ z4 ⊕ z3 ⊕ z4 = r2 z0 ⊕ z1 ⊕ z4 ⊕ z6 ⊕ z5 ⊕ z2 = r1 z1 ⊕ z0 ⊕ z4 ⊕ z6 ⊕ z5 ⊕ z2 = r1 z1 ⊕ z4 ⊕ z0 ⊕ z6 ⊕ z5 ⊕ z2 = r1 z1 ⊕ z4 ⊕ z6 ⊕ z0 ⊕ z5 ⊕ z2 = r1 z1 ⊕ z4 ⊕ z6 ⊕ z5 ⊕ z0 ⊕ z2 = r1 z1 ⊕ z4 ⊕ z6 ⊕ z5 ⊕ z2 = r1 z5 ⊕ z6 ⊕ z1 ⊕ z4 ⊕ z4 = r6 z4 ⊕ z2 ⊕ z4 ⊕ z6 ⊕ z3 = r6 z5 ⊕ z1 ⊕ z1 ⊕ z5 ⊕ z3 = r0 z2 ⊕ z1 ⊕ z2 ⊕ z6 ⊕ z2 = r1 z6 ⊕ z1 ⊕ z2 ⊕ z5 ⊕ z4 = r1 z1 ⊕ z4 ⊕ z6 ⊕ z1 ⊕ z3 = r5 z4 ⊕ z1 ⊕ z2 ⊕ z4 ⊕ z6 = r1 z4 ⊕ z4 ⊕ z1 ⊕ z2 ⊕ z2 = r5 z2 ⊕ z1 ⊕ z3 ⊕ z5 ⊕ z4 = r0
Testing, for Operator ⊕'s Identity
z0 ⊕ z5 ⊕ z6 ⊕ z1 ⊕ z4 ⊕ z4 = r6 z5 ⊕ z0 ⊕ z6 ⊕ z1 ⊕ z4 ⊕ z4 = r6 z5 ⊕ z6 ⊕ z0 ⊕ z1 ⊕ z4 ⊕ z4 = r6 z5 ⊕ z6 ⊕ z1 ⊕ z0 ⊕ z4 ⊕ z4 = r6 z5 ⊕ z6 ⊕ z1 ⊕ z4 ⊕ z0 ⊕ z4 = r6 z5 ⊕ z6 ⊕ z1 ⊕ z4 ⊕ z4 ⊕ z0 = r6 z0 ⊕ z4 ⊕ z2 ⊕ z4 ⊕ z6 ⊕ z3 = r6 z4 ⊕ z0 ⊕ z2 ⊕ z4 ⊕ z6 ⊕ z3 = r6 z4 ⊕ z2 ⊕ z0 ⊕ z4 ⊕ z6 ⊕ z3 = r6 z4 ⊕ z2 ⊕ z4 ⊕ z0 ⊕ z6 ⊕ z3 = r6 z4 ⊕ z2 ⊕ z4 ⊕ z6 ⊕ z0 ⊕ z3 = r6 z4 ⊕ z2 ⊕ z4 ⊕ z6 ⊕ z3 ⊕ z0 = r6 z0 ⊕ z5 ⊕ z1 ⊕ z1 ⊕ z5 ⊕ z3 = r0 z5 ⊕ z0 ⊕ z1 ⊕ z1 ⊕ z5 ⊕ z3 = r0 z5 ⊕ z1 ⊕ z0 ⊕ z1 ⊕ z5 ⊕ z3 = r0 z5 ⊕ z1 ⊕ z1 ⊕ z0 ⊕ z5 ⊕ z3 = r0 z5 ⊕ z1 ⊕ z1 ⊕ z5 ⊕ z0 ⊕ z3 = r0 z5 ⊕ z1 ⊕ z1 ⊕ z5 ⊕ z3 ⊕ z0 = r0 z0 ⊕ z2 ⊕ z1 ⊕ z2 ⊕ z6 ⊕ z2 = r1 z2 ⊕ z0 ⊕ z1 ⊕ z2 ⊕ z6 ⊕ z2 = r1 z2 ⊕ z1 ⊕ z0 ⊕ z2 ⊕ z6 ⊕ z2 = r1 z2 ⊕ z1 ⊕ z2 ⊕ z0 ⊕ z6 ⊕ z2 = r1 z2 ⊕ z1 ⊕ z2 ⊕ z6 ⊕ z0 ⊕ z2 = r1 z2 ⊕ z1 ⊕ z2 ⊕ z6 ⊕ z2 ⊕ z0 = r1 z0 ⊕ z6 ⊕ z1 ⊕ z2 ⊕ z5 ⊕ z4 = r1 z6 ⊕ z0 ⊕ z1 ⊕ z2 ⊕ z5 ⊕ z4 = r1 z6 ⊕ z1 ⊕ z0 ⊕ z2 ⊕ z5 ⊕ z4 = r1 z6 ⊕ z1 ⊕ z2 ⊕ z0 ⊕ z5 ⊕ z4 = r1 z6 ⊕ z1 ⊕ z2 ⊕ z5 ⊕ z0 ⊕ z4 = r1 z6 ⊕ z1 ⊕ z2 ⊕ z5 ⊕ z4 ⊕ z0 = r1 z0 ⊕ z1 ⊕ z4 ⊕ z6 ⊕ z1 ⊕ z3 = r5 z1 ⊕ z0 ⊕ z4 ⊕ z6 ⊕ z1 ⊕ z3 = r5 z1 ⊕ z4 ⊕ z0 ⊕ z6 ⊕ z1 ⊕ z3 = r5 z1 ⊕ z4 ⊕ z6 ⊕ z0 ⊕ z1 ⊕ z3 = r5 z1 ⊕ z4 ⊕ z6 ⊕ z1 ⊕ z0 ⊕ z3 = r5 z1 ⊕ z4 ⊕ z6 ⊕ z1 ⊕ z3 ⊕ z0 = r5 z0 ⊕ z4 ⊕ z1 ⊕ z2 ⊕ z4 ⊕ z6 = r1 z4 ⊕ z0 ⊕ z1 ⊕ z2 ⊕ z4 ⊕ z6 = r1 z4 ⊕ z1 ⊕ z0 ⊕ z2 ⊕ z4 ⊕ z6 = r1 z4 ⊕ z1 ⊕ z2 ⊕ z0 ⊕ z4 ⊕ z6 = r1 z4 ⊕ z1 ⊕ z2 ⊕ z4 ⊕ z0 ⊕ z6 = r1 z4 ⊕ z1 ⊕ z2 ⊕ z4 ⊕ z6 ⊕ z0 = r1 z0 ⊕ z4 ⊕ z4 ⊕ z1 ⊕ z2 ⊕ z2 = r5 z4 ⊕ z0 ⊕ z4 ⊕ z1 ⊕ z2 ⊕ z2 = r5 z4 ⊕ z4 ⊕ z0 ⊕ z1 ⊕ z2 ⊕ z2 = r5 z4 ⊕ z4 ⊕ z1 ⊕ z0 ⊕ z2 ⊕ z2 = r5 z4 ⊕ z4 ⊕ z1 ⊕ z2 ⊕ z0 ⊕ z2 = r5 z4 ⊕ z4 ⊕ z1 ⊕ z2 ⊕ z2 ⊕ z0 = r5 z0 ⊕ z2 ⊕ z1 ⊕ z3 ⊕ z5 ⊕ z4 = r0 z2 ⊕ z0 ⊕ z1 ⊕ z3 ⊕ z5 ⊕ z4 = r0
Training, for Operator ⊖
z2 ⊖ z2 ⊖ z4 ⊖ z3 ⊖ z6 ⊖ z4 = 3 z4 ⊖ z3 ⊖ z6 ⊖ z4 ⊖ z2 ⊖ z2 = 4 z3 ⊖ z6 ⊖ z4 ⊖ z4 ⊖ z2 ⊖ z2 = 4 z6 ⊖ z2 ⊖ z3 ⊖ z2 ⊖ z4 ⊖ z4 = 3 z2 ⊖ z3 ⊖ z4 ⊖ z2 ⊖ z6 ⊖ z4 = 2 z2 ⊖ z6 ⊖ z4 ⊖ z4 ⊖ z3 ⊖ z2 = 4 z6 ⊖ z2 ⊖ z4 ⊖ z2 ⊖ z4 ⊖ z3 = 3 z4 ⊖ z2 ⊖ z6 ⊖ z2 ⊖ z3 ⊖ z4 = 2 z6 ⊖ z2 ⊖ z2 ⊖ z3 ⊖ z4 ⊖ z4 = 3 z6 ⊖ z4 ⊖ z2 ⊖ z2 ⊖ z3 ⊖ z4 = 3 z6 ⊖ z6 ⊖ z4 ⊖ z4 ⊖ z2 ⊖ z3 = 4 z6 ⊖ z4 ⊖ z2 ⊖ z6 ⊖ z4 ⊖ z3 = 4 z4 ⊖ z6 ⊖ z2 ⊖ z3 ⊖ z6 ⊖ z4 = 2 z4 ⊖ z2 ⊖ z6 ⊖ z4 ⊖ z6 ⊖ z3 = 3 z6 ⊖ z4 ⊖ z6 ⊖ z3 ⊖ z2 ⊖ z4 = 3 z4 ⊖ z4 ⊖ z6 ⊖ z2 ⊖ z3 ⊖ z6 = 2 z6 ⊖ z3 ⊖ z4 ⊖ z6 ⊖ z4 ⊖ z2 = 3 z3 ⊖ z6 ⊖ z2 ⊖ z4 ⊖ z4 ⊖ z6 = 2 z6 ⊖ z3 ⊖ z4 ⊖ z4 ⊖ z6 ⊖ z2 = 3 z6 ⊖ z2 ⊖ z4 ⊖ z4 ⊖ z3 ⊖ z6 = 3 z5 ⊖ z5 ⊖ z6 ⊖ z2 ⊖ z3 ⊖ z3 = 3 z5 ⊖ z2 ⊖ z3 ⊖ z5 ⊖ z6 ⊖ z3 = 2 z6 ⊖ z2 ⊖ z3 ⊖ z5 ⊖ z3 ⊖ z5 = 2 z5 ⊖ z2 ⊖ z3 ⊖ z3 ⊖ z5 ⊖ z6 = 2 z3 ⊖ z5 ⊖ z5 ⊖ z2 ⊖ z3 ⊖ z6 = 2 z5 ⊖ z6 ⊖ z3 ⊖ z2 ⊖ z3 ⊖ z5 = 2 z2 ⊖ z5 ⊖ z3 ⊖ z3 ⊖ z6 ⊖ z5 = 3 z6 ⊖ z2 ⊖ z3 ⊖ z3 ⊖ z5 ⊖ z5 = 3 z3 ⊖ z5 ⊖ z3 ⊖ z6 ⊖ z5 ⊖ z2 = 3 z6 ⊖ z3 ⊖ z2 ⊖ z3 ⊖ z5 ⊖ z5 = 3 z5 ⊖ z5 ⊖ z5 ⊖ z3 ⊖ z5 ⊖ z2 = 4 z5 ⊖ z5 ⊖ z2 ⊖ z5 ⊖ z3 ⊖ z5 = 3 z3 ⊖ z5 ⊖ z5 ⊖ z2 ⊖ z5 ⊖ z5 = 3 z5 ⊖ z2 ⊖ z5 ⊖ z5 ⊖ z3 ⊖ z5 = 3 z5 ⊖ z3 ⊖ z2 ⊖ z5 ⊖ z5 ⊖ z5 = 4 z3 ⊖ z5 ⊖ z5 ⊖ z5 ⊖ z2 ⊖ z5 = 3 z5 ⊖ z5 ⊖ z3 ⊖ z5 ⊖ z5 ⊖ z2 = 4 z5 ⊖ z5 ⊖ z5 ⊖ z3 ⊖ z2 ⊖ z5 = 4 z5 ⊖ z2 ⊖ z3 ⊖ z5 ⊖ z5 ⊖ z5 = 3 z2 ⊖ z5 ⊖ z3 ⊖ z5 ⊖ z5 ⊖ z5 = 3 z6 ⊖ z6 ⊖ z2 ⊖ z5 ⊖ z5 ⊖ z3 = 4 z6 ⊖ z6 ⊖ z5 ⊖ z2 ⊖ z5 ⊖ z3 = 4 z5 ⊖ z6 ⊖ z3 ⊖ z2 ⊖ z5 ⊖ z6 = 2 z5 ⊖ z3 ⊖ z2 ⊖ z6 ⊖ z6 ⊖ z5 = 4 z6 ⊖ z5 ⊖ z4 ⊖ z1 ⊖ z2 ⊖ z4 = 3 z6 ⊖ z2 ⊖ z3 ⊖ z5 ⊖ z6 ⊖ z1 = 2 z4 ⊖ z6 ⊖ z2 ⊖ z2 ⊖ z4 ⊖ z5 = 2 z2 ⊖ z3 ⊖ z6 ⊖ z1 ⊖ z5 ⊖ z5 = 2 z5 ⊖ z4 ⊖ z5 ⊖ z5 ⊖ z4 ⊖ z2 = 4 z4 ⊖ z1 ⊖ z1 ⊖ z4 ⊖ z5 ⊖ z1 = 3 z0 ⊖ z4 ⊖ z3 ⊖ z5 ⊖ z3 ⊖ z1 = 3 z4 ⊖ z0 ⊖ z3 ⊖ z5 ⊖ z3 ⊖ z1 = 3 z4 ⊖ z3 ⊖ z0 ⊖ z5 ⊖ z3 ⊖ z1 = 4 z4 ⊖ z3 ⊖ z5 ⊖ z0 ⊖ z3 ⊖ z1 = 3 z4 ⊖ z3 ⊖ z5 ⊖ z3 ⊖ z0 ⊖ z1 = 3 z4 ⊖ z3 ⊖ z5 ⊖ z3 ⊖ z1 ⊖ z0 = 4 z4 ⊖ z3 ⊖ z5 ⊖ z3 ⊖ z1 = 3 z0 ⊖ z1 ⊖ z5 ⊖ z6 ⊖ z6 ⊖ z1 = 2 z1 ⊖ z0 ⊖ z5 ⊖ z6 ⊖ z6 ⊖ z1 = 3 z1 ⊖ z5 ⊖ z0 ⊖ z6 ⊖ z6 ⊖ z1 = 3 z1 ⊖ z5 ⊖ z6 ⊖ z0 ⊖ z6 ⊖ z1 = 2 z1 ⊖ z5 ⊖ z6 ⊖ z6 ⊖ z0 ⊖ z1 = 2 z1 ⊖ z5 ⊖ z6 ⊖ z6 ⊖ z1 ⊖ z0 = 3 z1 ⊖ z5 ⊖ z6 ⊖ z6 ⊖ z1 = 2 z0 ⊖ z5 ⊖ z2 ⊖ z5 ⊖ z2 ⊖ z3 = 2 z5 ⊖ z0 ⊖ z2 ⊖ z5 ⊖ z2 ⊖ z3 = 2 z5 ⊖ z2 ⊖ z0 ⊖ z5 ⊖ z2 ⊖ z3 = 3 z5 ⊖ z2 ⊖ z5 ⊖ z0 ⊖ z2 ⊖ z3 = 2 z5 ⊖ z2 ⊖ z5 ⊖ z2 ⊖ z0 ⊖ z3 = 3 z5 ⊖ z2 ⊖ z5 ⊖ z2 ⊖ z3 ⊖ z0 = 3 z5 ⊖ z2 ⊖ z5 ⊖ z2 ⊖ z3 = 2 z0 ⊖ z3 ⊖ z1 ⊖ z2 ⊖ z2 ⊖ z2 = 3 z3 ⊖ z0 ⊖ z1 ⊖ z2 ⊖ z2 ⊖ z2 = 3 z3 ⊖ z1 ⊖ z0 ⊖ z2 ⊖ z2 ⊖ z2 = 4 z3 ⊖ z1 ⊖ z2 ⊖ z0 ⊖ z2 ⊖ z2 = 3 z3 ⊖ z1 ⊖ z2 ⊖ z2 ⊖ z0 ⊖ z2 = 3 z3 ⊖ z1 ⊖ z2 ⊖ z2 ⊖ z2 ⊖ z0 = 4 z3 ⊖ z1 ⊖ z2 ⊖ z2 ⊖ z2 = 3 z0 ⊖ z2 ⊖ z4 ⊖ z4 ⊖ z3 ⊖ z4 = 2 z2 ⊖ z0 ⊖ z4 ⊖ z4 ⊖ z3 ⊖ z4 = 3 z2 ⊖ z4 ⊖ z0 ⊖ z4 ⊖ z3 ⊖ z4 = 2 z2 ⊖ z4 ⊖ z4 ⊖ z0 ⊖ z3 ⊖ z4 = 2 z2 ⊖ z4 ⊖ z4 ⊖ z3 ⊖ z0 ⊖ z4 = 3 z2 ⊖ z4 ⊖ z4 ⊖ z3 ⊖ z4 ⊖ z0 = 3 z2 ⊖ z4 ⊖ z4 ⊖ z3 ⊖ z4 = 2 z0 ⊖ z1 ⊖ z4 ⊖ z6 ⊖ z5 ⊖ z2 = 2 z1 ⊖ z0 ⊖ z4 ⊖ z6 ⊖ z5 ⊖ z2 = 3 z1 ⊖ z4 ⊖ z0 ⊖ z6 ⊖ z5 ⊖ z2 = 3 z1 ⊖ z4 ⊖ z6 ⊖ z0 ⊖ z5 ⊖ z2 = 2 z1 ⊖ z4 ⊖ z6 ⊖ z5 ⊖ z0 ⊖ z2 = 2 z1⊖z4⊖z6⊖z5⊖z2 = 2 z5⊖z6⊖z1⊖z4⊖z4 = 2 z4⊖z2⊖z4⊖z6⊖z3 = 2 z5⊖z1⊖z1⊖z5⊖z3 = 3 z2⊖z1⊖z2⊖z6⊖z2 = 2 z6⊖z1⊖z2⊖z5⊖z4 = 2 z1⊖z4⊖z6⊖z1⊖z3 = 1 z4⊖z1⊖z2⊖z4⊖z6 = 1 z4 ⊖ z4 ⊖ z1 ⊖ z2 ⊖ z2 = 3 z2 ⊖ z1 ⊖ z3 ⊖ z5 ⊖ z4 = 2 Testing, for Operator ⊖ z4 ⊖ z4 ⊖ z5 ⊖ z6 ⊖ z2 ⊖ z1 = 3 z4 ⊖ z2 ⊖ z6 ⊖ z5 ⊖ z4 ⊖ z1 = 4 z4 ⊖ z4 ⊖ z2 ⊖ z1 ⊖ z6 ⊖ z5 = 4 z6 ⊖ z2 ⊖ z1 ⊖ z5 ⊖ z4 ⊖ z4 = 4 z6 ⊖ z4 ⊖ z1 ⊖ z4 ⊖ z2 ⊖ z5 = 3 z5 ⊖ z2 ⊖ z4 ⊖ z4 ⊖ z6 ⊖ z1 = 3 z4 ⊖ z5 ⊖ z4 ⊖ z6 ⊖ z1 ⊖ z2 = 2 z6 ⊖ z5 ⊖ z1 ⊖ z4 ⊖ z2 ⊖ z4 = 3 z1 ⊖ z2 ⊖ z4 ⊖ z6 ⊖ z5 ⊖ z4 = 2 z6 ⊖ z1 ⊖ z6 ⊖ z2 ⊖ z3 ⊖ z5 = 2 z2 ⊖ z5 ⊖ z3 ⊖ z1 ⊖ z6 ⊖ z6 = 3 z3 ⊖ z2 ⊖ z6 ⊖ z1 ⊖ z5 ⊖ z6 = 2 z6 ⊖ z2 ⊖ z5 ⊖ z1 ⊖ z3 ⊖ z6 = 2 z6 ⊖ z6 ⊖ z3 ⊖ z5 ⊖ z2 ⊖ z1 = 4 z1 ⊖ z6 ⊖ z5 ⊖ z6 ⊖ z3 ⊖ z2 = 3 z5 ⊖ z1 ⊖ z6 ⊖ z3 ⊖ z6 ⊖ z2 = 3 z5 ⊖ z6 ⊖ z6 ⊖ z3 ⊖ z1 ⊖ z2 = 3 z5 ⊖ z3 ⊖ z6 ⊖ z2 ⊖ z6 ⊖ z1 = 3 z2 ⊖ z4 ⊖ z5 ⊖ z2 ⊖ z4 ⊖ z6 = 1 z4 ⊖ z5 ⊖ z4 ⊖ z2 ⊖ z2 ⊖ z6 = 3 z2 ⊖ z5 ⊖ z2 ⊖ z4 ⊖ z4 ⊖ z6 = 2 z5 ⊖ z2 ⊖ z6 ⊖ z4 ⊖ z2 ⊖ z4 = 3 z5 ⊖ z4 ⊖ z6 ⊖ z2 ⊖ z4 ⊖ z2 = 3 z2 ⊖ z4 ⊖ z2 ⊖ z6 ⊖ z4 ⊖ z5 = 2 z4 ⊖ z2 ⊖ z4 ⊖ z5 ⊖ z6 ⊖ z2 = 2 z2 ⊖ z4 ⊖ z5 ⊖ z6 ⊖ z4 ⊖ z2 = 2 z4 ⊖ z5 ⊖ z2 ⊖ z2 ⊖ z4 ⊖ z6 = 2 z2 ⊖ z1 ⊖ z6 ⊖ z5 ⊖ z5 ⊖ z3 = 4 z6 ⊖ z5 ⊖ z3 ⊖ z1 ⊖ z5 ⊖ z2 = 4 z6 ⊖ z2 ⊖ z5 ⊖ z5 ⊖ z1 ⊖ z3 = 3 z3 ⊖ z6 ⊖ z1 ⊖ z5 ⊖ z2 ⊖ z5 = 2 z5 ⊖ z2 ⊖ z5 ⊖ z1 ⊖ z6 ⊖ z3 = 3 z3 ⊖ z1 ⊖ z6 ⊖ z2 ⊖ z5 ⊖ z5 = 3 z3 ⊖ z6 ⊖ z5 ⊖ z2 ⊖ z5 ⊖ z1 = 3 z6 ⊖ z5 ⊖ z1 ⊖ z3 ⊖ z2 ⊖ z5 = 3 z1 ⊖ z5 ⊖ z5 ⊖ z2 ⊖ z6 ⊖ z3 = 3 z2 ⊖ z5 ⊖ z5 ⊖ z5 ⊖ z4 ⊖ z4 = 4 z4 ⊖ z5 ⊖ z5 ⊖ z2 ⊖ z5 ⊖ z4 = 3 z2 ⊖ z5 ⊖ z5 ⊖ z4 ⊖ z5 ⊖ z4 = 3 z2 ⊖ z5 ⊖ z4 ⊖ z5 ⊖ z5 ⊖ z4 = 3 z5 ⊖ z4 ⊖ z5 ⊖ z5 ⊖ z2 ⊖ z4 = 3 z2 ⊖ z4 ⊖ z5 ⊖ z4 ⊖ z5 ⊖ z5 = 2 z5 ⊖ z4 ⊖ z2 ⊖ z5 ⊖ z5 ⊖ z4 = 4 z5 ⊖ z4 ⊖ z5 ⊖ z2 ⊖ z5 ⊖ z4 = 3 z5 ⊖ z2 ⊖ z4 ⊖ z5 ⊖ z5 ⊖ z4 = 3 z4 ⊖ z1 ⊖ z4 ⊖ z5 ⊖ z1 ⊖ z1 = 3 z1 ⊖ z5 ⊖ z4 ⊖ z1 ⊖ z1 ⊖ z4 = 3 z1 ⊖ z1 ⊖ z4 ⊖ z1 ⊖ z5 ⊖ z4 = 3 z4 ⊖ z1 ⊖ z1 ⊖ z5 ⊖ z4 ⊖ z1 = 4 z1 ⊖ z5 ⊖ z1 ⊖ z4 ⊖ z1 ⊖ z4 = 2 z0 ⊖ z5 ⊖ z6 ⊖ z1 ⊖ z4 ⊖ z4 = 2 z5 ⊖ z0 ⊖ z6 ⊖ z1 ⊖ z4 ⊖ z4 = 3 z5 ⊖ z6 ⊖ z0 ⊖ z1 ⊖ z4 ⊖ z4 = 2 z5 ⊖ z6 ⊖ z1 ⊖ z0 ⊖ z4 ⊖ z4 = 3 z5 ⊖ z6 ⊖ z1 ⊖ z4 ⊖ z0 ⊖ z4 = 2 z5 ⊖ z6 ⊖ z1 ⊖ z4 ⊖ z4 ⊖ z0 = 3 z0 ⊖ z4 ⊖ z2 ⊖ z4 ⊖ z6 ⊖ z3 = 2 z4 ⊖ z0 ⊖ z2 ⊖ z4 ⊖ z6 ⊖ z3 = 2 z4 ⊖ z2 ⊖ z0 ⊖ z4 ⊖ z6 ⊖ z3 = 3 z4 ⊖ z2 ⊖ z4 ⊖ z0 ⊖ z6 ⊖ z3 = 3 z4 ⊖ z2 ⊖ z4 ⊖ z6 ⊖ z0 ⊖ z3 = 2 z4 ⊖ z2 ⊖ z4 ⊖ z6 ⊖ z3 ⊖ z0 = 3 z0 ⊖ z5 ⊖ z1 ⊖ z1 ⊖ z5 ⊖ z3 = 3 z5 ⊖ z0 ⊖ z1 ⊖ z1 ⊖ z5 ⊖ z3 = 3 z5 ⊖ z1 ⊖ z0 ⊖ z1 ⊖ z5 ⊖ z3 = 3 z5 ⊖ z1 ⊖ z1 ⊖ z0 ⊖ z5 ⊖ z3 = 4 z5 ⊖ z1 ⊖ z1 ⊖ z5 ⊖ z0 ⊖ z3 = 3 z5 ⊖ z1 ⊖ z1 ⊖ z5 ⊖ z3 ⊖ z0 = 4 z0 ⊖ z2 ⊖ z1 ⊖ z2 ⊖ z6 ⊖ z2 = 2 z2 ⊖ z0 ⊖ z1 ⊖ z2 ⊖ z6 ⊖ z2 = 2 z2 ⊖ z1 ⊖ z0 ⊖ z2 ⊖ z6 ⊖ z2 = 3 z2 ⊖ z1 ⊖ z2 ⊖ z0 ⊖ z6 ⊖ z2 = 3 z2 ⊖ z1 ⊖ z2 ⊖ z6 ⊖ z0 ⊖ z2 = 2 z2 ⊖ z1 ⊖ z2 ⊖ z6 ⊖ z2 ⊖ z0 = 3 z0 ⊖ z6 ⊖ z1 ⊖ z2 ⊖ z5 ⊖ z4 = 2 z6 ⊖ z0 ⊖ z1 ⊖ z2 ⊖ z5 ⊖ z4 = 2 z6 ⊖ z1 ⊖ z0 ⊖ z2 ⊖ z5 ⊖ z4 = 3 z6 ⊖ z1 ⊖ z2 ⊖ z0 ⊖ z5 ⊖ z4 = 3 z6 ⊖ z1 ⊖ z2 ⊖ z5 ⊖ z0 ⊖ z4 = 2 z6 ⊖ z1 ⊖ z2 ⊖ z5 ⊖ z4 ⊖ z0 = 3 z0 ⊖ z1 ⊖ z4 ⊖ z6 ⊖ z1 ⊖ z3 = 1 z1 ⊖ z0 ⊖ z4 ⊖ z6 ⊖ z1 ⊖ z3 = 2 z1 ⊖ z4 ⊖ z0 ⊖ z6 ⊖ z1 ⊖ z3 = 2 z1 ⊖ z4 ⊖ z6 ⊖ z0 ⊖ z1 ⊖ z3 = 1 z1 ⊖ z4 ⊖ z6 ⊖ z1 ⊖ z0 ⊖ z3 = 2 z1 ⊖ z4 ⊖ z6 ⊖ z1 ⊖ z3 ⊖ z0 = 2 z0 ⊖ z4 ⊖ z1 ⊖ z2 ⊖ z4 ⊖ z6 = 1 z4 ⊖ z0 ⊖ z1 ⊖ z2 ⊖ z4 ⊖ z6 = 1 z4 ⊖ z1 ⊖ z0 ⊖ z2 ⊖ z4 ⊖ z6 = 2 z4 ⊖ z1 ⊖ z2 ⊖ z0 ⊖ z4 ⊖ z6 = 2 z4 ⊖ z1 ⊖ z2 ⊖ z4 ⊖ z0 ⊖ z6 = 2 z4 ⊖ z1 ⊖ z2 ⊖ z4 ⊖ z6 ⊖ z0 = 2 z0 ⊖ z4 ⊖ z4 ⊖ z1 ⊖ z2 ⊖ z2 = 3 z4 ⊖ z0 ⊖ z4 ⊖ z1 ⊖ z2 ⊖ z2 = 3 z4 ⊖ z4 ⊖ z0 ⊖ z1 ⊖ z2 ⊖ z2 = 3 z4 ⊖ z4 ⊖ z1 ⊖ z0 ⊖ z2 ⊖ z2 = 4 z4 ⊖ z4 ⊖ z1 ⊖ z2 ⊖ z0 ⊖ z2 = 3 z4 ⊖ z4 ⊖ z1 ⊖ z2 ⊖ z2 ⊖ z0 = 4 z0 ⊖ z2 ⊖ z1 ⊖ z3 ⊖ z5 ⊖ z4 = 2 z2 ⊖ z0 ⊖ z1 ⊖ z3 ⊖ z5 ⊖ z4 = 2
Training, for Operator z0 ⊳ z3 ⊳ z1 ⊳ z2 ⊳ z2 ⊳ z2 = z0 z3 ⊳ z0 ⊳ z1 ⊳ z2 ⊳ z2 ⊳ z2 = z3 z3 ⊳ z1 ⊳ z0 ⊳ z2 ⊳ z2 ⊳ z2 = z3 z3 ⊳ z1 ⊳ z2 ⊳ z0 ⊳ z2 ⊳ z2 = z3 z3 ⊳ z1 ⊳ z2 ⊳ z2 ⊳ z0 ⊳ z2 = z3 z3 ⊳ z1 ⊳ z2 ⊳ z2 ⊳ z2 ⊳ z0 = z3 z3 ⊳ z1 ⊳ z2 ⊳ z2 ⊳ z2 = z3 z0 ⊳ z2 ⊳ z4 ⊳ z4 ⊳ z3 ⊳ z4 = z0 z2 ⊳ z0 ⊳ z4 ⊳ z4 ⊳ z3 ⊳ z4 = z2 z2 ⊳ z4 ⊳ z0 ⊳ z4 ⊳ z3 ⊳ z4 = z2 z1 ⊲ z5 ⊲ z6 ⊲ z0 ⊲ z6 ⊲ z1 = z1 z1 ⊲ z5 ⊲ z6 ⊲ z6 ⊲ z0 ⊲ z1 = z1 z1 ⊲ z5 ⊲ z6 ⊲ z6 ⊲ z1 ⊲ z0 = z0 z1 ⊲ z5 ⊲ z6 ⊲ z6 ⊲ z1 = z1 z0 ⊲ z5 ⊲ z2 ⊲ z5 ⊲ z2 ⊲ z3 = z3 z5 ⊲ z0 ⊲ z2 ⊲ z5 ⊲ z2 ⊲ z3 = z3 z5 ⊲ z2 ⊲ z0 ⊲ z5 ⊲ z2 ⊲ z3 = z3 z5 ⊲ z2 ⊲ z5 ⊲ z0 ⊲ z2 ⊲ z3 = z3 z5 ⊲ z2 ⊲ z5 ⊲ z2 ⊲ z0 ⊲ z3 = z3 z5 ⊲ z2 ⊲ z5 ⊲ z2 ⊲ z3 ⊲ z0 = z0 z5 ⊲ z2 ⊲ z5 ⊲ z2 ⊲ z3 = z3 z0 ⊲ z3 ⊲ z1 ⊲ z2 ⊲ z2 ⊲ z2 = z2 z3 ⊲ z0 ⊲ z1 ⊲ z2 ⊲ z2 ⊲ z2 = z2 z3 ⊲ z1 ⊲ z0 ⊲ z2 ⊲ z2 ⊲ z2 = z2 z3 ⊲ z1 ⊲ z2 ⊲ z0 ⊲ z2 ⊲ z2 = z2 z3 ⊲ z1 ⊲ z2 ⊲ z2 ⊲ z0 ⊲ z2 = z2 z3 ⊲ z1 ⊲ z2 ⊲ z2 ⊲ z2 ⊲ z0 = z0 z3 ⊲ z1 ⊲ z2 ⊲ z2 ⊲ z2 = z2 z0 ⊲ z2 ⊲ z4 ⊲ z4 ⊲ z3 ⊲ z4 = z4 z2 ⊲ z0 ⊲ z4 ⊲ z4 ⊲ z3 ⊲ z4 = z4 z2 ⊲ z4 ⊲ z0 ⊲ z4 ⊲ z3 ⊲ z4 = z4 z2 ⊲ z4 ⊲ z4 ⊲ z0 ⊲ z3 ⊲ z4 = z4 z2 ⊲ z4 ⊲ z4 ⊲ z3 ⊲ z0 ⊲ z4 = z4 z2 ⊲ z4 ⊲ z4 ⊲ z3 ⊲ z4 ⊲ z0 = z0 z2 ⊲ z4 ⊲ z4 ⊲ z3 ⊲ z4 = z4 z0 ⊲ z1 ⊲ z4 ⊲ z6 ⊲ z5 ⊲ z2 = z2 z1 ⊲ z0 ⊲ z4 ⊲ z6 ⊲ z5 ⊲ z2 = z2 z1 ⊲ z4 ⊲ z0 ⊲ z6 ⊲ z5 ⊲ z2 = z2 z1 ⊲ z4 ⊲ z6 ⊲ z0 ⊲ z5 ⊲ z2 = z2 z1 ⊲ z4 ⊲ z6 ⊲ z5 ⊲ z0 ⊲ z2 = z2 z1 ⊲ z4 ⊲ z6 ⊲ z5 ⊲ z2 = z2 z5 ⊲ z6 ⊲ z1 ⊲ z4 ⊲ z4 = z4 z4 ⊲ z2 ⊲ z4 ⊲ z6 ⊲ z3 = z3 z5 ⊲ z1 ⊲ z1 ⊲ z5 ⊲ z3 = z3 z2 ⊲ z1 ⊲ z2 ⊲ z6 ⊲ z2 = z2 z6 ⊲ z1 ⊲ z2 ⊲ z5 ⊲ z4 = z4 z1 ⊲ z4 ⊲ z6 ⊲ z1 ⊲ z3 = z3 z4 ⊲ z1 ⊲ z2 ⊲ z4 ⊲ z6 = z6 z4 ⊲ z4 ⊲ z1 ⊲ z2 ⊲ z2 = z2 z2 ⊲ z1 ⊲ z3 ⊲ z5 ⊲ z4 = z4
⊳ z2 ⊳ z2 ⊳ z4 ⊳ z3 ⊳ z6 ⊳ z4 = z2 z4 ⊳ z3 ⊳ z6 ⊳ z4 ⊳ z2 ⊳ z2 = z4 z3 ⊳ z6 ⊳ z4 ⊳ z4 ⊳ z2 ⊳ z2 = z3 z6 ⊳ z2 ⊳ z3 ⊳ z2 ⊳ z4 ⊳ z4 = z6 z2 ⊳ z3 ⊳ z4 ⊳ z2 ⊳ z6 ⊳ z4 = z2 z2 ⊳ z6 ⊳ z4 ⊳ z4 ⊳ z3 ⊳ z2 = z2 z6 ⊳ z2 ⊳ z4 ⊳ z2 ⊳ z4 ⊳ z3 = z6 z4 ⊳ z2 ⊳ z6 ⊳ z2 ⊳ z3 ⊳ z4 = z4 z6 ⊳ z2 ⊳ z2 ⊳ z3 ⊳ z4 ⊳ z4 = z6 z6 ⊳ z4 ⊳ z2 ⊳ z2 ⊳ z3 ⊳ z4 = z6 z6 ⊳ z6 ⊳ z4 ⊳ z4 ⊳ z2 ⊳ z3 = z6 z6 ⊳ z4 ⊳ z2 ⊳ z6 ⊳ z4 ⊳ z3 = z6 z4 ⊳ z6 ⊳ z2 ⊳ z3 ⊳ z6 ⊳ z4 = z4 z4 ⊳ z2 ⊳ z6 ⊳ z4 ⊳ z6 ⊳ z3 = z4 z6 ⊳ z4 ⊳ z6 ⊳ z3 ⊳ z2 ⊳ z4 = z6 z4 ⊳ z4 ⊳ z6 ⊳ z2 ⊳ z3 ⊳ z6 = z4 z6 ⊳ z3 ⊳ z4 ⊳ z6 ⊳ z4 ⊳ z2 = z6 z3 ⊳ z6 ⊳ z2 ⊳ z4 ⊳ z4 ⊳ z6 = z3 z6 ⊳ z3 ⊳ z4 ⊳ z4 ⊳ z6 ⊳ z2 = z6 z6 ⊳ z2 ⊳ z4 ⊳ z4 ⊳ z3 ⊳ z6 = z6 z5 ⊳ z5 ⊳ z6 ⊳ z2 ⊳ z3 ⊳ z3 = z5 z5 ⊳ z2 ⊳ z3 ⊳ z5 ⊳ z6 ⊳ z3 = z5 z6 ⊳ z2 ⊳ z3 ⊳ z5 ⊳ z3 ⊳ z5 = z6 z5 ⊳ z2 ⊳ z3 ⊳ z3 ⊳ z5 ⊳ z6 = z5 z3 ⊳ z5 ⊳ z5 ⊳ z2 ⊳ z3 ⊳ z6 = z3 z5 ⊳ z6 ⊳ z3 ⊳ z2 ⊳ z3 ⊳ z5 = z5 z2 ⊳ z5 ⊳ z3 ⊳ z3 ⊳ z6 ⊳ z5 = z2 z6 ⊳ z2 ⊳ z3 ⊳ z3 ⊳ z5 ⊳ z5 = z6 z3 ⊳ z5 ⊳ z3 ⊳ z6 ⊳ z5 ⊳ z2 = z3 z6 ⊳ z3 ⊳ z2 ⊳ z3 ⊳ z5 ⊳ z5 = z6 z5 ⊳ z5 ⊳ z5 ⊳ z3 ⊳ z5 ⊳ z2 = z5 z5 ⊳ z5 ⊳ z2 ⊳ z5 ⊳ z3 ⊳ z5 = z5 z3 ⊳ z5 ⊳ z5 ⊳ z2 ⊳ z5 ⊳ z5 = z3 z5 ⊳ z2 ⊳ z5 ⊳ z5 ⊳ z3 ⊳ z5 = z5 z5 ⊳ z3 ⊳ z2 ⊳ z5 ⊳ z5 ⊳ z5 = z5 z3 ⊳ z5 ⊳ z5 ⊳ z5 ⊳ z2 ⊳ z5 = z3 z5 ⊳ z5 ⊳ z3 ⊳ z5 ⊳ z5 ⊳ z2 = z5 z5 ⊳ z5 ⊳ z5 ⊳ z3 ⊳ z2 ⊳ z5 = z5 z5 ⊳ z2 ⊳ z3 ⊳ z5 ⊳ z5 ⊳ z5 = z5 z2 ⊳ z5 ⊳ z3 ⊳ z5 ⊳ z5 ⊳ z5 = z2 z6 ⊳ z6 ⊳ z2 ⊳ z5 ⊳ z5 ⊳ z3 = z6 z6 ⊳ z6 ⊳ z5 ⊳ z2 ⊳ z5 ⊳ z3 = z6 z5 ⊳ z6 ⊳ z3 ⊳ z2 ⊳ z5 ⊳ z6 = z5
Testing, for Operator ⊲</p>
<p>Figure 3 :
3
Figure 3: Plots of training and testing accuracy.The first row is the training dynamics for Z 7 given the scale of training set K = 3000.The second row are the accuracies for Z 7 (left), Z 11 (middle), Z 13 (right) with varying K of training set.</p>
<p>Figure 4 :
4
Figure 4: Visualization of S ℓ com and S ℓ ide where 1 ≤ ℓ ≤ 13.The upper row displays the values of S ℓ com (+, ⊖), S ℓ com (+, ⊳), and S ℓ com (+, ⊲) and the lower row displays the values of S ℓ ide (+, ⊖), S ℓ ide (+, ⊳) and S ℓ ide (+, ⊲).The numbers in the left axis represent K ∈ {100, 300, • • • , 10000}.For clarity, non-negative values are highlighted in green and yellow.</p>
<p>z5 ⊳ z3 ⊳ z2 ⊳ z6 ⊳ z6 ⊳ z5 = z5 z6 ⊳ z5 ⊳ z4 ⊳ z1 ⊳ z2 ⊳ z4 = z6 z6 ⊳ z2 ⊳ z3 ⊳ z5 ⊳ z6 ⊳ z1 = z6 z4 ⊳ z6 ⊳ z2 ⊳ z2 ⊳ z4 ⊳ z5 = z4 z2 ⊳ z3 ⊳ z6 ⊳ z1 ⊳ z5 ⊳ z5 = z2 z5 ⊳ z4 ⊳ z5 ⊳ z5 ⊳ z4 ⊳ z2 = z5 z4 ⊳ z1 ⊳ z1 ⊳ z4 ⊳ z5 ⊳ z1 = z4 z0 ⊳ z4 ⊳ z3 ⊳ z5 ⊳ z3 ⊳ z1 = z0 z4 ⊳ z0 ⊳ z3 ⊳ z5 ⊳ z3 ⊳ z1 = z4 z4 ⊳ z3 ⊳ z0 ⊳ z5 ⊳ z3 ⊳ z1 = z4 z4 ⊳ z3 ⊳ z5 ⊳ z0 ⊳ z3 ⊳ z1 = z4 z4 ⊳ z3 ⊳ z5 ⊳ z3 ⊳ z0 ⊳ z1 = z4 z4 ⊳ z3 ⊳ z5 ⊳ z3 ⊳ z1 ⊳ z0 = z4 z4 ⊳ z3 ⊳ z5 ⊳ z3 ⊳ z1 = z4 z0 ⊳ z1 ⊳ z5 ⊳ z6 ⊳ z6 ⊳ z1 = z0 z1 ⊳ z0 ⊳ z5 ⊳ z6 ⊳ z6 ⊳ z1 = z1 z1 ⊳ z5 ⊳ z0 ⊳ z6 ⊳ z6 ⊳ z1 = z1 z1 ⊳ z5 ⊳ z6 ⊳ z0 ⊳ z6 ⊳ z1 = z1 z1 ⊳ z5 ⊳ z6 ⊳ z6 ⊳ z0 ⊳ z1 = z1 z1 ⊳ z5 ⊳ z6 ⊳ z6 ⊳ z1 ⊳ z0 = z1 z1 ⊳ z5 ⊳ z6 ⊳ z6 ⊳ z1 = z1 z0 ⊳ z5 ⊳ z2 ⊳ z5 ⊳ z2 ⊳ z3 = z0 z5 ⊳ z0 ⊳ z2 ⊳ z5 ⊳ z2 ⊳ z3 = z5 z5 ⊳ z2 ⊳ z0 ⊳ z5 ⊳ z2 ⊳ z3 = z5 z5 ⊳ z2 ⊳ z5 ⊳ z0 ⊳ z2 ⊳ z3 = z5 z5 ⊳ z2 ⊳ z5 ⊳ z2 ⊳ z0 ⊳ z3 = z5 z5 ⊳ z2 ⊳ z5 ⊳ z2 ⊳ z3 ⊳ z0 = z5 z5 ⊳ z2 ⊳ z5 ⊳ z2 ⊳ z3 = z5</p>
<p>z4 ⊲ z4 ⊲ z5 ⊲ z6 ⊲ z2 ⊲ z1 = z1 z4 ⊲ z2 ⊲ z6 ⊲ z5 ⊲ z4 ⊲ z1 = z1 z4 ⊲ z4 ⊲ z2 ⊲ z1 ⊲ z6 ⊲ z5 = z5 z6 ⊲ z2 ⊲ z1 ⊲ z5 ⊲ z4 ⊲ z4 = z4 z6 ⊲ z4 ⊲ z1 ⊲ z4 ⊲ z2 ⊲ z5 = z5 z5 ⊲ z2 ⊲ z4 ⊲ z4 ⊲ z6 ⊲ z1 = z1 z4 ⊲ z5 ⊲ z4 ⊲ z6 ⊲ z1 ⊲ z2 = z2 z6 ⊲ z5 ⊲ z1 ⊲ z4 ⊲ z2 ⊲ z4 = z4 z1 ⊲ z2 ⊲ z4 ⊲ z6 ⊲ z5 ⊲ z4 = z4 z6 ⊲ z1 ⊲ z6 ⊲ z2 ⊲ z3 ⊲ z5 = z5 z2 ⊲ z5 ⊲ z3 ⊲ z1 ⊲ z6 ⊲ z6 = z6 z3 ⊲ z2 ⊲ z6 ⊲ z1 ⊲ z5 ⊲ z6 = z6 z6 ⊲ z2 ⊲ z5 ⊲ z1 ⊲ z3 ⊲ z6 = z6 z6 ⊲ z6 ⊲ z3 ⊲ z5 ⊲ z2 ⊲ z1 = z1 z1 ⊲ z6 ⊲ z5 ⊲ z6 ⊲ z3 ⊲ z2 = z2 z5 ⊲ z1 ⊲ z6 ⊲ z3 ⊲ z6 ⊲ z2 = z2 z5 ⊲ z6 ⊲ z6 ⊲ z3 ⊲ z1 ⊲ z2 = z2 z5 ⊲ z3 ⊲ z6 ⊲ z2 ⊲ z6 ⊲ z1 = z1 z2 ⊲ z4 ⊲ z5 ⊲ z2 ⊲ z4 ⊲ z6 = z6 z4 ⊲ z5 ⊲ z4 ⊲ z2 ⊲ z2 ⊲ z6 = z6 z2 ⊲ z5 ⊲ z2 ⊲ z4 ⊲ z4 ⊲ z6 = z6 z5 ⊲ z2 ⊲ z6 ⊲ z4 ⊲ z2 ⊲ z4 = z4 z5 ⊲ z4 ⊲ z6 ⊲ z2 ⊲ z4 ⊲ z2 = z2 z2 ⊲ z4 ⊲ z2 ⊲ z6 ⊲ z4 ⊲ z5 = z5 z4 ⊲ z2 ⊲ z4 ⊲ z5 ⊲ z6 ⊲ z2 = z2 z2 ⊲ z4 ⊲ z5 ⊲ z6 ⊲ z4 ⊲ z2 = z2 z4 ⊲ z5 ⊲ z2 ⊲ z2 ⊲ z4 ⊲ z6 = z6 z2 ⊲ z1 ⊲ z6 ⊲ z5 ⊲ z5 ⊲ z3 = z3 z6 ⊲ z5 ⊲ z3 ⊲ z1 ⊲ z5 ⊲ z2 = z2 z6 ⊲ z2 ⊲ z5 ⊲ z5 ⊲ z1 ⊲ z3 = z3 z3 ⊲ z6 ⊲ z1 ⊲ z5 ⊲ z2 ⊲ z5 = z5 z5 ⊲ z2 ⊲ z5 ⊲ z1 ⊲ z6 ⊲ z3 = z3 z3 ⊲ z1 ⊲ z6 ⊲ z2 ⊲ z5 ⊲ z5 = z5 z3 ⊲ z6 ⊲ z5 ⊲ z2 ⊲ z5 ⊲ z1 = z1 z6 ⊲ z5 ⊲ z1 ⊲ z3 ⊲ z2 ⊲ z5 = z5 z1 ⊲ z5 ⊲ z5 ⊲ z2 ⊲ z6 ⊲ z3 = z3 z2 ⊲ z5 ⊲ z5 ⊲ z5 ⊲ z4 ⊲ z4 = z4 z4 ⊲ z5 ⊲ z5 ⊲ z2 ⊲ z5 ⊲ z4 = z4 z2 ⊲ z5 ⊲ z5 ⊲ z4 ⊲ z5 ⊲ z4 = z4 z2 ⊲ z5 ⊲ z4 ⊲ z5 ⊲ z5 ⊲ z4 = z4 z5 ⊲ z4 ⊲ z5 ⊲ z5 ⊲ z2 ⊲ z4 = z4 z2 ⊲ z4 ⊲ z5 ⊲ z4 ⊲ z5 ⊲ z5 = z5 z5 ⊲ z4 ⊲ z2 ⊲ z5 ⊲ z5 ⊲ z4 = z4 z5 ⊲ z4 ⊲ z5 ⊲ z2 ⊲ z5 ⊲ z4 = z4 z5 ⊲ z2 ⊲ z4 ⊲ z5 ⊲ z5 ⊲ z4 = z4 z4 ⊲ z1 ⊲ z4 ⊲ z5 ⊲ z1 ⊲ z1 = z1 z1 ⊲ z5 ⊲ z4 ⊲ z1 ⊲ z1 ⊲ z4 = z4 z1 ⊲ z1 ⊲ z4 ⊲ z1 ⊲ z5 ⊲ z4 = z4 z4 ⊲ z1 ⊲ z1 ⊲ z5 ⊲ z4 ⊲ z1 = z1 z1 ⊲ z5 ⊲ z1 ⊲ z4 ⊲ z1 ⊲ z4 = z4 z0 ⊲ z5 ⊲ z6 ⊲ z1 ⊲ z4 ⊲ z4 = z4 z5 ⊲ z0 ⊲ z6 ⊲ z1 ⊲ z4 ⊲ z4 = z4 z5 ⊲ z6 ⊲ z0 ⊲ z1 ⊲ z4 ⊲ z4 = z4 z5 ⊲ z6 ⊲ z1 ⊲ z0 ⊲ z4 ⊲ z4 = z4 z5 ⊲ z6 ⊲ z1 ⊲ z4 ⊲ z0 ⊲ z4 = z4 z5 ⊲ z6 ⊲ z1 ⊲ z4 ⊲ z4 ⊲ z0 = z0 z0 ⊲ z4 ⊲ z2 ⊲ z4 ⊲ z6 ⊲ z3 = z3 z4 ⊲ z0 ⊲ z2 ⊲ z4 ⊲ z6 ⊲ z3 = z3 z4 ⊲ z2 ⊲ z0 ⊲ z4 ⊲ z6 ⊲ z3 = z3 z4 ⊲ z2 ⊲ z4 ⊲ z0 ⊲ z6 ⊲ z3 = z3 z4 ⊲ z2 ⊲ z4 ⊲ z6 ⊲ z0 ⊲ z3 = z3 z4 ⊲ z2 ⊲ z4 ⊲ z6 ⊲ z3 ⊲ z0 = z0 z0 ⊲ z5 ⊲ z1 ⊲ z1 ⊲ z5 ⊲ z3 = z3 z5 ⊲ z0 ⊲ z1 ⊲ z1 ⊲ z5 ⊲ z3 = z3 z5 ⊲ z1 ⊲ z0 ⊲ z1 ⊲ z5 ⊲ z3 = z3 z5 ⊲ z1 ⊲ z1 ⊲ z0 ⊲ z5 ⊲ z3 = z3 z5 ⊲ z1 ⊲ z1 ⊲ z5 ⊲ z0 ⊲ z3 = z3 z5 ⊲ z1 ⊲ z1 ⊲ z5 ⊲ z3 ⊲ z0 = z0 z0 ⊲ z2 ⊲ z1 ⊲ z2 ⊲ z6 ⊲ z2 = z2 z2 ⊲ z0 ⊲ z1 ⊲ z2 ⊲ z6 ⊲ z2 = z2 z2 ⊲ z1 ⊲ z0 ⊲ z2 ⊲ z6 ⊲ z2 = z2 z2 ⊲ z1 ⊲ z2 ⊲ z0 ⊲ z6 ⊲ z2 = z2 z2 ⊲ z1 ⊲ z2 ⊲ z6 ⊲ z0 ⊲ z2 = z2 z2 ⊲ z1 ⊲ z2 ⊲ z6 ⊲ z2 ⊲ z0 = z0 z0 ⊲ z6 ⊲ z1 ⊲ z2 ⊲ z5 ⊲ z4 = z4 z6 ⊲ z0 ⊲ z1 ⊲ z2 ⊲ z5 ⊲ z4 = z4 z6 ⊲ z1 ⊲ z0 ⊲ z2 ⊲ z5 ⊲ z4 = z4 z6 ⊲ z1 ⊲ z2 ⊲ z0 ⊲ z5 ⊲ z4 = z4 z6 ⊲ z1 ⊲ z2 ⊲ z5 ⊲ z0 ⊲ z4 = z4 z6 ⊲ z1 ⊲ z2 ⊲ z5 ⊲ z4 ⊲ z0 = z0 z0 ⊲ z1 ⊲ z4 ⊲ z6 ⊲ z1 ⊲ z3 = z3 z1 ⊲ z0 ⊲ z4 ⊲ z6 ⊲ z1 ⊲ z3 = z3 z1 ⊲ z4 ⊲ z0 ⊲ z6 ⊲ z1 ⊲ z3 = z3 z1 ⊲ z4 ⊲ z6 ⊲ z0 ⊲ z1 ⊲ z3 = z3 z1 ⊲ z4 ⊲ z6 ⊲ z1 ⊲ z0 ⊲ z3 = z3 z1 ⊲ z4 ⊲ z6 ⊲ z1 ⊲ z3 ⊲ z0 = z0 z0 ⊲ z4 ⊲ z1 ⊲ z2 ⊲ z4 ⊲ z6 = z6 z4 ⊲ z0 ⊲ z1 ⊲ z2 ⊲ z4 ⊲ z6 = z6 z4 ⊲ z1 ⊲ z0 ⊲ z2 ⊲ z4 ⊲ z6 = z6 z4 ⊲ z1 ⊲ z2 ⊲ z0 ⊲ z4 ⊲ z6 = z6 z4 ⊲ z1 ⊲ z2 ⊲ z4 ⊲ z0 ⊲ z6 = z6 z4 ⊲ z1 ⊲ z2 ⊲ z4 ⊲ z6 ⊲ z0 = z0 z0 ⊲ z4 ⊲ z4 ⊲ z1 ⊲ z2 ⊲ z2 = z2 z4 ⊲ z0 ⊲ z4 ⊲ z1 ⊲ z2 ⊲ z2 = z2 z4 ⊲ z4 ⊲ z0 ⊲ z1 ⊲ z2 ⊲ z2 = z2 z4 ⊲ z4 ⊲ z1 ⊲ z0 ⊲ z2 ⊲ z2 = z2 z4 ⊲ z4 ⊲ z1 ⊲ z2 ⊲ z0 ⊲ z2 = z2 z4 ⊲ z4 ⊲ z1 ⊲ z2 ⊲ z2 ⊲ z0 = z0 z0 ⊲ z2 ⊲ z1 ⊲ z3 ⊲ z5 ⊲ z4 = z4 z2 ⊲ z0 ⊲ z1 ⊲ z3 ⊲ z5 ⊲ z4 = z4</p>
<p>tracks the evolution of training and test accuracy over the course of training.We observe that the model ultimately achieves 100% accuracy in the training set, indicating that it has
Published at ICLR 2025 Workshop on Reasoning and Planning for LLMs3.1.2 VARYING THE TRAINING SET'S SCALE.
memorized all training instances.However, for the commutativity property of "+" or "⊕" operators, it does not achieve high accuracy in testing set.A plausible explanation is that the scale of the training set is still insufficient.In the next experiment, we examine the testing accuracy for multiple training scales to investigate this further.</p>
<p>https://github.com/d09942015ntu/unraveling_llm_algebra
z4 + z4 + z5 + z6 + z2 + z1 = z1 z4 + z2 + z6 + z5 + z4 + z1 = z1 z4 + z4 + z2 + z1 + z6 + z5 = z1 z6 + z2 + z1 + z5 + z4 + z4 = z1 z6 + z4 + z1 + z4 + z2 + z5 = z1 z5 + z2 + z4 + z4 + z6 + z1 = z1 z4 + z5 + z4 + z6 + z1 + z2 = z1 z6 + z5 + z1 + z4 + z2 + z4 = z1 z1+ z2 + z4 + z6 + z5 + z4 = z1 z6 + z1 + z6 + z2 + z3 + z5 = z2 z2 + z5 + z3 + z1 + z6 + z6 = z2 z3 + z2 + z6 + z1 + z5 + z6 = z2 z6 + z2 + z5 + z1 + z3 + z6 = z2 z6 + z6 + z3 + z5 + z2 + z1 = z2 z1 + z6 + z5 + z6 + z3 + z2 = z2 z5 + z1 + z6 + z3 + z6 + z2 = z2 z5 + z6 + z6 + z3 + z1 + z2 = z2 z5 + z3 + z6 + z2 + z6 + z1 = z2 z2 + z4 + z5 + z2 + z4 + z6 = z2 z4 + z5 + z4 + z2 + z2 + z6 = z2 z2 + z5 + z2 + z4 + z4 + z6 = z2 z5 + z2 + z6 + z4 + z2 + z4 = z2 z5 + z4 + z6 + z2 + z4 + z2 = z2 z2 + z4 + z2 + z6 + z4 + z5 = z2 z4 + z2 + z4 + z5 + z6 + z2 = z2 z2 + z4 + z5 + z6 + z4 + z2 = z2 z4 + z5 + z2 + z2 + z4 + z6 = z2 z2 + z1 + z6 + z5 + z5 + z3 = z1 z6 + z5 + z3 + z1 + z5 + z2 = z1 z6 + z2 + z5 + z5 + z1 + z3 = z1 z3 + z6 + z1 + z5 + z2 + z5 = z1 z5 + z2 + z5 + z1 + z6 + z3 = z1 z3 + z1 + z6 + z2 + z5 + z5 = z1 z3 + z6 + z5 + z2 + z5 + z1 = z1 z6 + z5 + z1 + z3 + z2 + z5 = z1 z1 + z5 + z5 + z2 + z6 + z3 = z1 z2 + z5 + z5 + z5 + z4 + z4 = z4 z4 + z5 + z5 + z2 + z5 + z4 = z4 z2 + z5 + z5 + z4 + z5 + z4 = z4 z2 + z5 + z4 + z5 + z5 + z4 = z4 z5 + z4 + z5 + z5 + z2 + z4 = z4 z2 + z4 + z5 + z4 + z5 + z5 = z4 z5 + z4 + z2 + z5 + z5 + z4 = z4 z5 + z4 + z5 + z2 + z5 + z4 = z4 z5 + z2 + z4 + z5 + z5 + z4 = z4 z4 + z1 + z4 + z5 + z1 + z1 = z2 z1 + z5 + z4 + z1 + z1 + z4 = z2 z1 + z1 + z4 + z1 + z5 + z4 = z2 z4 + z1 + z1 + z5 + z4 + z1 = z2 z1 + z5 + z1 + z4 + z1 + z4 = z2Training, for Operator +'s Identityz0 + z4 + z3 + z5 + z3 + z1 = z2 z4 + z0 + z3 + z5 + z3 + z1 = z2 z4 + z3 + z0 + z5 + z3 + z1 = z2 z4 + z3 + z5 + z0 + z3 + z1 = z2 z4 + z3 + z5 + z3 + z0 + z1 = z2 z4 + z3 + z5 + z3 + z1 + z0 = z2 z4 + z3 + z5 + z3 + z1 = z2 z0 + z1 + z5 + z6 + z6 + z1 = z5 z1 + z0 + z5 + z6 + z6 + z1 = z5 z1 + z5 + z0 + z6 + z6 + z1 = z5 z1 + z5 + z6 + z0 + z6 + z1 = z5 z1 + z5 + z6 + z6 + z0 + z1 = z5 z1 + z5 + z6 + z6 + z1 + z0 = z5 z1 + z5 + z6 + z6 + z1 = z5 z0 + z5 + z2 + z5 + z2 + z3 = z3 z5 + z0 + z2 + z5 + z2 + z3 = z3 z5 + z2 + z0 + z5 + z2 + z3 = z3 z5 + z2 + z5 + z0 + z2 + z3 = z3 z5 + z2 + z5 + z2 + z0 + z3 = z3 z5 + z2 + z5 + z2 + z3 + z0 = z3 z5 + z2 + z5 + z2 + z3 = z3 z0 + z3 + z1 + z2 + z2 + z2 = z3 z3 + z0 + z1 + z2 + z2 + z2 = z3 z3 + z1 + z0 + z2 + z2 + z2 = z3 z3 + z1 + z2 + z0 + z2 + z2 = z3 z3 + z1 + z2 + z2 + z0 + z2 = z3 z3 + z1 + z2 + z2 + z2 + z0 = z3 z3 + z1 + z2 + z2 + z2 = z3 z0 + z2 + z4 + z4 + z3 + z4 = z3 z2 + z0 + z4 + z4 + z3 + z4 = z3 z2 + z4 + z0 + z4 + z3 + z4 = z3 z2 + z4 + z4 + z0 + z3 + z4 = z3 z2 + z4 + z4 + z3 + z0 + z4 = z3 z2 + z4 + z4 + z3 + z4 + z0 = z3 z2 + z4 + z4 + z3 + z4 = z3 z0 + z1 + z4 + z6 + z5 + z2 = z4 z1 + z0 + z4 + z6 + z5 + z2 = z4 z1 + z4 + z0 + z6 + z5 + z2 = z4 z1 + z4 + z6 + z0 + z5 + z2 = z4 z1 + z4 + z6 + z5 + z0 + z2 = z4 z1 + z4 + z6 + z5 + z2 = z4 z5 + z6 + z1 + z4 + z4 = z6 z4 + z2 + z4 + z6 + z3 = z5 z5 + z1 + z1 + z5 + z3 = z1 z2 + z1 + z2 + z6 + z2 = z6 z6 + z1 + z2 + z5 + z4 = z4 z1 + z4 + z6 + z1 + z3 = z1 z4 + z1 + z2 + z4 + z6 = z3 z4 + z4 + z1 + z2 + z2 = z6 z2 + z1 + z3 + z5 + z4 = z1Testing, for Operator +'s Identity z0 + z5 + z6 + z1 + z4 + z4 = z6 z5 + z0 + z6 + z1 + z4 + z4 = z6 z5 + z6 + z0 + z1 + z4 + z4 = z6 z5 + z6 + z1 + z0 + z4 + z4 = z6 z5 + z6 + z1 + z4 + z0 + z4 = z6 z5 + z6 + z1 + z4 + z4 + z0 = z6
ACKNOWLEDGMENTThis work was supported in part by the Asian Office of Aerospace Research &amp; Development (AOARD) under Grant NTU-112HT911020, National Science and Technology Council of Taiwan under Grant NSTC-112-2221-E-002-204-and NSTC-113-2221-E-002-208-, Ministry of Education (MOE) of Taiwan under Grant NTU-113L891406, and Ministry of Environment under Grant NTU-113BT911001.Published at ICLR 2025 Workshop on Reasoning and Planning for LLMsTesting, for OperatorTraining, for Operatorz4 z6 ⊲ z2 ⊲ z3 ⊲ z5 ⊲ z6 ⊲ z1 = z1 z4 ⊲ z6 ⊲ z2 ⊲ z2 ⊲ z4 ⊲ z5 = z5 z2 ⊲ z3 ⊲ z6 ⊲ z1 ⊲ z5 ⊲ z5 = z5 z5 ⊲ z4 ⊲ z5 ⊲ z5 ⊲ z4 ⊲ z2 = z2 z4 ⊲ z1 ⊲ z1 ⊲ z4 ⊲ z5 ⊲ z1 = z1 z0 ⊲ z4 ⊲ z3 ⊲ z5 ⊲ z3 ⊲ z1 = z1 z4 ⊲ z0 ⊲ z3 ⊲ z5 ⊲ z3 ⊲ z1 = z1 z4 ⊲ z3 ⊲ z0 ⊲ z5 ⊲ z3 ⊲ z1 = z1 z4 ⊲ z3 ⊲ z5 ⊲ z0 ⊲ z3 ⊲ z1 = z1 z4 ⊲ z3 ⊲ z5 ⊲ z3 ⊲ z0 ⊲ z1 = z1 z4 ⊲ z3 ⊲ z5 ⊲ z3 ⊲ z1 ⊲ z0 = z0 z4 ⊲ z3 ⊲ z5 ⊲ z3 ⊲ z1 = z1 z0 ⊲ z1 ⊲ z5 ⊲ z6 ⊲ z6 ⊲ z1 = z1 z1 ⊲ z0 ⊲ z5 ⊲ z6 ⊲ z6 ⊲ z1 = z1 z1 ⊲ z5 ⊲ z0 ⊲ z6 ⊲ z6 ⊲ z1 = z1
Claude 3 haiku: our fastest model yet. Anthropic, 2024</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.141682021arXiv preprint</p>
<p>Language models are symbolic learners in arithmetic. Chunyuan Deng, Zhiqi Li, Roy Xie, Ruidi Chang, Hanjie Chen, arXiv:2410.155802024arXiv preprint</p>
<p>Language models know the value of numbers. Zhifang Sui, Fangwei Zhu, Damai Dai, 2024</p>
<p>Towards revealing the mystery behind chain of thought: a theoretical perspective. Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, Liwei Wang, Advances in Neural Information Processing Systems. 362024</p>
<p>Learning mathematical rules with large language models. Antoine Gorceix, Le Bastien, Ahmad Chenadec, Nelson Rammal, Manuela Vadori, Veloso, 2024</p>
<p>Exploring reversal mathematical reasoning ability for large language models. Pei Guo, Wangjie You, Juntao Li, Yan Bowen, Min Zhang, Findings of the Association for Computational Linguistics: ACL 2024. Lun-Wei Ku, Andre Martins, Vivek Srikumar, Bangkok, ThailandAssociation for Computational LinguisticsAugust 2024</p>
<p>How does gpt-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. Michael Hanna, Ollie Liu, Alexandre Variengien, Advances in Neural Information Processing Systems. 202436</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.038742021arXiv preprint</p>
<p>Exploring group and symmetry principles in large language models. Shima Imani, Hamid Palangi, arXiv:2402.061202024arXiv preprint</p>
<p>Neural discovery of permutation subgroups. Pavan Karjol, Rohan Kashyap, Prathosh, International Conference on Artificial Intelligence and Statistics. PMLR2023</p>
<p>A unified framework for discovering discrete symmetries. Pavan Karjol, Rohan Kashyap, Aditya Gopalan, Prathosh, International Conference on Artificial Intelligence and Statistics. PMLR2024</p>
<p>Junyu Lai, Jiahe Xu, Yao Yang, Yunpeng Huang, Chun Cao, Jingwei Xu, arXiv:2410.20124Executing arithmetic: Fine-tuning large language models as turing machines. 2024arXiv preprint</p>
<p>Amit Arnold, Levy , Mor Geva, arXiv:2410.11781Language models encode numbers using digit representations in base 10. 2024arXiv preprint</p>
<p>Tiedong Liu, Bryan Kian, Hsiang Low, arXiv:2305.14201Goat: Fine-tuned llama outperforms gpt-4 on arithmetic tasks. 2023arXiv preprint</p>
<p>Improving large language model fine-tuning for solving math problems. Yixin Liu, Avi Singh, C Daniel Freeman, John D Co-Reyes, Peter J Liu, arXiv:2310.100472023arXiv preprint</p>
<p>Arithmeticgpt: Empowering small-size large language models with advanced arithmetic skills. Zitao Liu, Ying Zheng, Zhibo Yin, Jiahao Chen, Tianqiao Liu, Mi Tian, Weiqi Luo, 2025Machine Learning11424</p>
<p>Adapting while learning: Grounding llms for scientific problems with intelligent tool usage adaptation. Bohan Lyu, Yadi Cao, Duncan Watson-Parris, Leon Bergen, Taylor Berg-Kirkpatrick, Rose Yu, arXiv:2411.004122024arXiv preprint</p>
<p>Auto-regressive next-token predictors are universal learners. Eran Malach, arXiv:2309.069792023arXiv preprint</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, Augustus Odena, arXiv:2112.00114Show your work: Scratchpads for intermediate computation with language models. 2021arXiv preprint</p>
<p>. Openai, Chatgpt, 2024</p>
<p>Why think step by step? reasoning emerges from the locality of experience. Ben Prystawski, Michael Li, Noah Goodman, Advances in Neural Information Processing Systems. 362024</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, 2019OpenAITechnical report</p>
<p>Eli Schwartz, Leshem Choshen, Joseph Shtok, Sivan Doveh, Leonid Karlinsky, Assaf Arbelle, Numerologic, arXiv:2404.00459Number encoding for enhanced llms' numerical reasoning. 2024arXiv preprint</p>
<p>Revorder: A novel method for enhanced arithmetic in language models. Si Shen, Peijun Shen, Danhao Zhu, arXiv:2402.038222024arXiv preprint</p>
<p>A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis. Alessandro Stolfo, Yonatan Belinkov, Mrinmaya Sachan, 2023</p>
<p>Mathscale: Scaling instruction tuning for mathematical reasoning. Zhengyang Tang, Xingxing Zhang, Benyou Wang, Furu Wei, arXiv:2403.028842024arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>A theory for length generalization in learning to reason. Changnan Xiao, Bing Liu, arXiv:2404.005602024arXiv preprint</p>
<p>. Jianke Yang, Nima Dehmamy, Robin Walters, Rose Yu, arXiv:2310.001052023Latent space symmetry discovery. arXiv preprint</p>
<p>Scaffolding learning: From specific to generic with large language models. David S Yin, Xiaoxin Yin, 10.1371/journal.pone.0310409e0310409PLOS ONE. 1992024</p>
<p>Interpreting and improving large language models in arithmetic calculation. Wei Zhang, Wan Chaoqun, Yonggang Zhang, Ming Yiu, Xinmei Cheung, Xu Tian, Jieping Shen, Ye, arXiv:2403.18295Dual instruction tuning with large language models for mathematical reasoning. 2024. 2024235arXiv preprintProceedings of the 41st International Conference on Machine Learning</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2567 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2567</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2567</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-65.html">extraction-schema-65</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-2b292ff89d808fba10579871591a22f1649cd039</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2b292ff89d808fba10579871591a22f1649cd039" target="_blank">Counterfactual Multi-Agent Policy Gradients</a></p>
                <p><strong>Paper Venue:</strong> AAAI Conference on Artificial Intelligence</p>
                <p><strong>Paper TL;DR:</strong> A new multi-agent actor-critic method called counterfactual multi- agent (COMA) policy gradients that uses a centralised critic to estimate the Q-function and decentralised actors to optimise the agents' policies.</p>
                <p><strong>Paper Abstract:</strong> 
 
 Many real-world problems, such as network packet routing and the coordination of autonomous vehicles, are naturally modelled as cooperative multi-agent systems. There is a great need for new reinforcement learning methods that can efficiently learn decentralised policies for such systems. To this end, we propose a new multi-agent actor-critic method called counterfactual multi-agent (COMA) policy gradients. COMA uses a centralised critic to estimate the Q-function and decentralised actors to optimise the agents' policies. In addition, to address the challenges of multi-agent credit assignment, it uses a counterfactual baseline that marginalises out a single agent's action, while keeping the other agents' actions fixed. COMA also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass. We evaluate COMA in the testbed of StarCraft unit micromanagement, using a decentralised variant with significant partial observability. COMA significantly improves average performance over other multi-agent actor-critic methods in this setting, and the best performing agents are competitive with state-of-the-art centralised controllers that get access to the full state.
 
</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2567.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2567.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>COMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Counterfactual Multi-Agent Policy Gradients</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent actor-critic method that uses centralized training with a centralized critic and decentralized recurrent actors; it computes an agent-specific counterfactual advantage by marginalizing out one agent's action (keeping others fixed) to address multi-agent credit assignment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>COMA (Counterfactual Multi-Agent Policy Gradients)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Decentralised actors (parameter-shared GRU policies) that act from local action-observation histories, trained with a single centralized critic that conditions on the global state or joint histories and joint actions. The critic is represented so it can output Q-values for all actions of a target agent conditioned on the other agents' actions; the per-agent advantage is computed as A^a(s,u) = Q(s,u) - sum_{u'^a} pi^a(u'^a|tau^a) Q(s,(u^{-a},u'^a)). Training uses TD(lambda) on the centralized critic and policy gradients for actors. Execution is fully decentralized (actors do not require the critic or inter-agent communication).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (experiments used teams of size 3 and 5; scenarios: 3 marines, 5 marines, 5 wraiths, 2 dragoons + 3 zealots => 3 or 5)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Homogeneous controller agents (parameter-shared); specialization arises only from the controlled unit's type and local observations (unit types: marine, wraith, dragoon, zealot) and an agent-specific ID passed as input. No explicit role-based specialization such as 'idea generation' or 'evaluation' agents—each agent controls a single game unit and learns a policy conditioned on its local history.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Training/learning (centralized training with simulator), execution/operation (decentralised execution), and evaluation (win-rate metrics). Not used for literature review or human-in-the-loop phases.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Centralized training with a centralized critic guiding decentralized actors. Coordination at training time is via the critic which conditions on joint actions and global state; at execution agents act independently (decentralised execution) with no runtime coordination mechanism beyond implicit coordination learned in policies.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>No explicit inter-agent message passing during execution. During training the centralized critic has access to global state/joint histories (effectively shared memory / centralized information) and actors share parameters. There is no structured message format between agents at runtime.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Centralized critic provides per-agent scalar advantages computed via a counterfactual baseline (marginalizing the agent's action). Critic trained by on-policy TD(lambda) (lambda=0.8), uses n-step returns and target networks; actor gradients use the counterfactual advantage to update policies. The baseline's expected contribution to gradient is zero, reducing variance without biasing learning.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Centralized critic evaluates and provides feedback on every training update / trajectory (i.e., per sampled experience during learning). During execution agents do not communicate.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Game-playing: StarCraft II unit micromanagement (multi-agent control benchmark with partial observability and delayed rewards).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Mean win percentage over final evaluation episodes (mean ± 95% CI) per map: 3m: 87% (±3) mean, best 98%; 5m: 81% (±5) mean, best 95%; 5w: 82% (±3) mean, best 98%; 2d_3z: 47% (±5) mean, best 65%. Training curves (win rate vs episodes) and training speed/stability comparisons were also reported.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against Independent Actor-Critic variants (IAC-V, IAC-Q), centralized-V (central critic estimating V), central-QV (central Q and V but using V as baseline), hand-coded heuristic, and published centralized controllers (centralised DQN and GMEZO). COMA outperformed all baseline multi-agent actor-critic variants in final win rate and training speed; achieved comparable performance to state-of-the-art centralized controllers in many maps despite decentralized execution and restricted field-of-view.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Improved credit assignment leading to faster learning, higher final win rates, and more stable training compared to baselines. Quantitatively: e.g., on 5m COMA mean 81% vs IAC-V 63% (an +18 percentage point improvement); on 5w COMA 82% vs IAC-Q 57% (approx +25 points). COMA also outperformed central-QV and central-V, indicating the counterfactual baseline provides shaped, lower-variance training signals.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Scalability concerns for scenarios with large numbers of agents (centralized critic input grows linearly and training becomes harder). Exploration in multi-agent settings remains difficult and is the dominant bottleneck rather than critic centralization. No runtime communication means coordination must be learned implicitly, which can be hard under severe partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Yes. Ablations included: IAC-V and IAC-Q (decentralised critics), central-V (centralized V critic with TD error used for advantage), central-QV (central Q and V where advantage computed as Q - V instead of counterfactual). Results: COMA strictly dominated central-QV in training speed and final performance, outperformed central-V in final performance and speed, and outperformed IAC variants (which learned more slowly or to lower final performance).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Empirical hyperparameters found effective: TD(lambda) = 0.8; actor GRU hidden size = 128; epsilon-greedy softmax lower bound annealed linearly from 0.5 to 0.02 across 750 episodes; critic architecture: feedforward ReLU layers; parameter sharing among actors. Architecturally, the combination of a centralized Q-critic with the counterfactual baseline and a critic representation that outputs per-agent action-Qs is recommended by the paper as the best-performing configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Counterfactual Multi-Agent Policy Gradients', 'publication_date_yy_mm': '2017-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2567.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2567.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IAC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Independent Actor-Critic (parameter-shared implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline multi-agent approach where each agent learns its own actor and critic from its local action-observation history; in this paper a parameter-shared implementation is used so one actor and one critic network serve all agents but critics are local (condition only on agent-local history).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Independent Actor-Critic (IAC)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Each agent has an actor conditioned on its local history; critic(s) estimate local value or Q-functions based only on each agent's own action-observation history (no centralized state input). In the paper, parameters are shared among agents (single network used for all agents) and agents receive an agent ID so behavior can diverge. Two variants: IAC-V (critic estimates V(tau^a) and uses TD error) and IAC-Q (critic estimates Q(tau^a,u^a) and computes advantage Q-V).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (same scenarios as COMA: 3 or 5 agents depending on map)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Homogeneous controllers (parameter-shared); observed specialization only via per-agent observations and agent IDs. No explicit role differentiation.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Training/learning and execution/evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>No explicit centralized coordination; each agent learns independently (coordination can only emerge implicitly through shared environment reward and shared network parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>No inter-agent communication or centralized information during training or execution (other than shared parameters via training); purely local observations used.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Local critics provide TD or advantage signals to each agent's policy; feedback is local and does not condition on joint actions or global state.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>N/A (no inter-agent communication). Critics update per training batch/trajectory.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>StarCraft unit micromanagement (same maps as COMA experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Mean win percentage (examples from Table 1): 3m: IAC-Q 56% (±6), IAC-V 47%; 5m: IAC-V 63% (±2), IAC-Q 58% (±3); 5w: IAC-Q 57% (±5), IAC-V 18% (±5); 2d_3z: IAC-V 27% (±9), IAC-Q 19% (±21).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Used as a baseline for COMA; generally underperformed compared to COMA and centralised critics, learning more slowly and/or converging to lower win rates.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Implicit coordination can emerge but is weaker; parameter sharing can speed some aspects of learning but lacks access to global state to improve policy evaluation and credit assignment.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Poor multi-agent credit assignment due to critics conditioning only on local histories; noisy gradients when other agents explore; struggles under partial observability and when coordination between agents is required.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>IAC variants served as ablations; they showed slower learning and worse final performance relative to COMA and centralised critic baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Parameter-sharing among agents (single actor/critic network) with agent ID as input used in the paper; IAC performed better with Q-based critics (IAC-Q) than V-only in some maps, but overall inferior to centralized critic approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Counterfactual Multi-Agent Policy Gradients', 'publication_date_yy_mm': '2017-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2567.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2567.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>central-V</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Centralized V critic baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A centralized critic variant that estimates the state-value V(s) using centralised state information; advantage estimates are obtained via TD error, and actors remain decentralized.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>central-V (centralized value critic)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Actors are decentralized (as in COMA) but the critic is centralized and estimates V(s) conditioned on global state; training uses TD-error for advantage estimation rather than counterfactual Q-based baselines. The critic receives global state and agents' observations but does not condition on joint actions.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (3 or 5 in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Homogeneous agents controlling individual units; no explicit role specialization beyond unit type and local observations.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Training/learning and execution/evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Centralized critic provides a common value baseline during training but does not reason about individual agents' action-level contributions; coordination is therefore weaker than COMA's counterfactual approach.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>No runtime inter-agent communication; centralized critic uses global state during training only.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>TD-error from centralized V critic provides feedback to actor updates (used in policy gradient).</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Critic feedback provided each training update / trajectory sample.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>StarCraft micromanagement benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Mean win percentage from Table 1: 3m: central-V 83% (±3); 5m: 67% (±5); 5w: 65% (±3); 2d_3z: 36% (±6).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared directly to COMA and other baselines. COMA outperformed central-V in final performance and typically trained faster and more stably.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Centralized state access improves policy evaluation compared to local critics; yields better performance than purely decentralized IAC in many maps.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Does not solve per-agent credit assignment as effectively as COMA since it does not condition on joint actions or compute counterfactuals; training stability issues observed compared to COMA.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>central-V used as an ablation; results show COMA (central Q + counterfactual baseline) outperforms central-V, indicating the value of action-conditional Q information and counterfactual baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Centralized V critic with TD(lambda) (paper used lambda=0.8), but empirically COMA's counterfactual Q-based approach is preferable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Counterfactual Multi-Agent Policy Gradients', 'publication_date_yy_mm': '2017-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2567.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2567.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>central-QV</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Centralized Q and V critic baseline (Q-V advantage)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A centralized critic variant that learns both Q(s,u) and V(s) and computes the per-agent advantage as Q - V (instead of COMA's counterfactual marginalization).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>central-QV (centralized Q and V critics)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Centralized critic learns both a joint Q(s, u) and a state-value V(s). Advantage used for policy updates is Q(s,u) - V(s) rather than the counterfactual per-agent baseline used by COMA. Actors remain decentralized with parameter sharing.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (3 or 5 in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Homogeneous agents differentiated only by observations and unit types.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Training/learning and execution/evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Centralized Q and V are used to estimate advantages but without marginalizing out single-agent actions; coordination information is present in Q but credit assignment is less precisely targeted to individual agents.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>No explicit agent-to-agent communication; centralized critic used during training.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Centralized Q and V produce an advantage signal (Q - V) used as the policy gradient target.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Per training update / trajectory.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>StarCraft micromanagement.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Mean win percentage from Table 1: 3m: central-QV 83% (±5); 5m: 71% (±9); 5w: 76% (±1); 2d_3z: 39% (±5).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Serves as an ablation testing the importance of counterfactual baseline; COMA strictly dominated central-QV in both training speed and final performance across maps.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Using centralized Q and V provides richer information than decentralized critics, but without counterfactual marginalization it provides a less targeted credit assignment for each agent.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Less effective per-agent credit assignment relative to COMA; inferior final performance and slower/fewer stable improvements in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>central-QV is an explicit ablation demonstrating the value of computing a counterfactual baseline rather than using V as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Learning both Q and V centrally can help but requires using the counterfactual baseline (as in COMA) to achieve the best empirical results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Counterfactual Multi-Agent Policy Gradients', 'publication_date_yy_mm': '2017-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2567.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2567.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GMEZO (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GMEZO (greedy-MDP with episodic zero-order optimisation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art centralized StarCraft controller from prior work (Usunier et al. 2016) that uses a greedy MDP formulation combined with episodic zero-order optimisation and full-field-of-view centralized control; cited as an upper-bound comparator.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Episodic exploration for deep deterministic policies: An application to starcraft micromanagement tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GMEZO (centralized greedy-MDP + zero-order optimisation)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Centralized controller that sequentially chooses actions for agents using a greedy MDP formulation and episodic zero-order optimization; has access to full global state and StarCraft macro-actions (attack-move), unlike the decentralized restricted-FoV setup used for COMA.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (comparable maps; evaluated for same maps under full-field-of-view centralized control)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Centralized planner controls all units; per-unit specialization depends on planner outputs and unit types.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Training/optimisation and execution/evaluation (centralized).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Centralized greedy MDP planner controlling all agents centrally at every step.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Not applicable (single centralized controller).</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Optimization-based policy updates (episodic zero-order optimization).</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Centralized decisions at each timestep.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>StarCraft micromanagement (full-field-of-view centralized control).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Published centralized results (from Usunier et al. 2016) reported in paper for comparison: e.g., 5m GMEZO 100% win in full-FoV centralized setting (table shows GMEZO 100 for 5m).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Used as an upper-bound reference: COMA (decentralized, limited FoV) achieved comparable performance to GMEZO in several maps despite information and action restrictions.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Full centralized state and macro-actions simplify coordination and yield high win rates in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Centralized controllers require full state and are less realistic for decentralized execution settings; not directly comparable when FoV and macro-actions differ.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not part of this paper's ablations; cited as prior centralized baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Counterfactual Multi-Agent Policy Gradients', 'publication_date_yy_mm': '2017-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Episodic exploration for deep deterministic policies: An application to starcraft micromanagement tasks <em>(Rating: 2)</em></li>
                <li>Learning to communicate with deep multi-agent reinforcement learning <em>(Rating: 2)</em></li>
                <li>Multi-agent actor-critic for mixed cooperative-competitive environments <em>(Rating: 2)</em></li>
                <li>Stabilising experience replay for deep multi-agent reinforcement learning <em>(Rating: 2)</em></li>
                <li>Learning cooperative visual dialog agents with deep reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2567",
    "paper_id": "paper-2b292ff89d808fba10579871591a22f1649cd039",
    "extraction_schema_id": "extraction-schema-65",
    "extracted_data": [
        {
            "name_short": "COMA",
            "name_full": "Counterfactual Multi-Agent Policy Gradients",
            "brief_description": "A multi-agent actor-critic method that uses centralized training with a centralized critic and decentralized recurrent actors; it computes an agent-specific counterfactual advantage by marginalizing out one agent's action (keeping others fixed) to address multi-agent credit assignment.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "COMA (Counterfactual Multi-Agent Policy Gradients)",
            "system_description": "Decentralised actors (parameter-shared GRU policies) that act from local action-observation histories, trained with a single centralized critic that conditions on the global state or joint histories and joint actions. The critic is represented so it can output Q-values for all actions of a target agent conditioned on the other agents' actions; the per-agent advantage is computed as A^a(s,u) = Q(s,u) - sum_{u'^a} pi^a(u'^a|tau^a) Q(s,(u^{-a},u'^a)). Training uses TD(lambda) on the centralized critic and policy gradients for actors. Execution is fully decentralized (actors do not require the critic or inter-agent communication).",
            "number_of_agents": "variable (experiments used teams of size 3 and 5; scenarios: 3 marines, 5 marines, 5 wraiths, 2 dragoons + 3 zealots =&gt; 3 or 5)",
            "agent_specializations": "Homogeneous controller agents (parameter-shared); specialization arises only from the controlled unit's type and local observations (unit types: marine, wraith, dragoon, zealot) and an agent-specific ID passed as input. No explicit role-based specialization such as 'idea generation' or 'evaluation' agents—each agent controls a single game unit and learns a policy conditioned on its local history.",
            "research_phases_covered": "Training/learning (centralized training with simulator), execution/operation (decentralised execution), and evaluation (win-rate metrics). Not used for literature review or human-in-the-loop phases.",
            "coordination_mechanism": "Centralized training with a centralized critic guiding decentralized actors. Coordination at training time is via the critic which conditions on joint actions and global state; at execution agents act independently (decentralised execution) with no runtime coordination mechanism beyond implicit coordination learned in policies.",
            "communication_protocol": "No explicit inter-agent message passing during execution. During training the centralized critic has access to global state/joint histories (effectively shared memory / centralized information) and actors share parameters. There is no structured message format between agents at runtime.",
            "feedback_mechanism": "Centralized critic provides per-agent scalar advantages computed via a counterfactual baseline (marginalizing the agent's action). Critic trained by on-policy TD(lambda) (lambda=0.8), uses n-step returns and target networks; actor gradients use the counterfactual advantage to update policies. The baseline's expected contribution to gradient is zero, reducing variance without biasing learning.",
            "communication_frequency": "Centralized critic evaluates and provides feedback on every training update / trajectory (i.e., per sampled experience during learning). During execution agents do not communicate.",
            "task_domain": "Game-playing: StarCraft II unit micromanagement (multi-agent control benchmark with partial observability and delayed rewards).",
            "performance_metrics": "Mean win percentage over final evaluation episodes (mean ± 95% CI) per map: 3m: 87% (±3) mean, best 98%; 5m: 81% (±5) mean, best 95%; 5w: 82% (±3) mean, best 98%; 2d_3z: 47% (±5) mean, best 65%. Training curves (win rate vs episodes) and training speed/stability comparisons were also reported.",
            "baseline_comparison": "Compared against Independent Actor-Critic variants (IAC-V, IAC-Q), centralized-V (central critic estimating V), central-QV (central Q and V but using V as baseline), hand-coded heuristic, and published centralized controllers (centralised DQN and GMEZO). COMA outperformed all baseline multi-agent actor-critic variants in final win rate and training speed; achieved comparable performance to state-of-the-art centralized controllers in many maps despite decentralized execution and restricted field-of-view.",
            "coordination_benefits": "Improved credit assignment leading to faster learning, higher final win rates, and more stable training compared to baselines. Quantitatively: e.g., on 5m COMA mean 81% vs IAC-V 63% (an +18 percentage point improvement); on 5w COMA 82% vs IAC-Q 57% (approx +25 points). COMA also outperformed central-QV and central-V, indicating the counterfactual baseline provides shaped, lower-variance training signals.",
            "coordination_challenges": "Scalability concerns for scenarios with large numbers of agents (centralized critic input grows linearly and training becomes harder). Exploration in multi-agent settings remains difficult and is the dominant bottleneck rather than critic centralization. No runtime communication means coordination must be learned implicitly, which can be hard under severe partial observability.",
            "ablation_studies": "Yes. Ablations included: IAC-V and IAC-Q (decentralised critics), central-V (centralized V critic with TD error used for advantage), central-QV (central Q and V where advantage computed as Q - V instead of counterfactual). Results: COMA strictly dominated central-QV in training speed and final performance, outperformed central-V in final performance and speed, and outperformed IAC variants (which learned more slowly or to lower final performance).",
            "optimal_configurations": "Empirical hyperparameters found effective: TD(lambda) = 0.8; actor GRU hidden size = 128; epsilon-greedy softmax lower bound annealed linearly from 0.5 to 0.02 across 750 episodes; critic architecture: feedforward ReLU layers; parameter sharing among actors. Architecturally, the combination of a centralized Q-critic with the counterfactual baseline and a critic representation that outputs per-agent action-Qs is recommended by the paper as the best-performing configuration.",
            "uuid": "e2567.0",
            "source_info": {
                "paper_title": "Counterfactual Multi-Agent Policy Gradients",
                "publication_date_yy_mm": "2017-05"
            }
        },
        {
            "name_short": "IAC",
            "name_full": "Independent Actor-Critic (parameter-shared implementation)",
            "brief_description": "Baseline multi-agent approach where each agent learns its own actor and critic from its local action-observation history; in this paper a parameter-shared implementation is used so one actor and one critic network serve all agents but critics are local (condition only on agent-local history).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Independent Actor-Critic (IAC)",
            "system_description": "Each agent has an actor conditioned on its local history; critic(s) estimate local value or Q-functions based only on each agent's own action-observation history (no centralized state input). In the paper, parameters are shared among agents (single network used for all agents) and agents receive an agent ID so behavior can diverge. Two variants: IAC-V (critic estimates V(tau^a) and uses TD error) and IAC-Q (critic estimates Q(tau^a,u^a) and computes advantage Q-V).",
            "number_of_agents": "variable (same scenarios as COMA: 3 or 5 agents depending on map)",
            "agent_specializations": "Homogeneous controllers (parameter-shared); observed specialization only via per-agent observations and agent IDs. No explicit role differentiation.",
            "research_phases_covered": "Training/learning and execution/evaluation.",
            "coordination_mechanism": "No explicit centralized coordination; each agent learns independently (coordination can only emerge implicitly through shared environment reward and shared network parameters).",
            "communication_protocol": "No inter-agent communication or centralized information during training or execution (other than shared parameters via training); purely local observations used.",
            "feedback_mechanism": "Local critics provide TD or advantage signals to each agent's policy; feedback is local and does not condition on joint actions or global state.",
            "communication_frequency": "N/A (no inter-agent communication). Critics update per training batch/trajectory.",
            "task_domain": "StarCraft unit micromanagement (same maps as COMA experiments).",
            "performance_metrics": "Mean win percentage (examples from Table 1): 3m: IAC-Q 56% (±6), IAC-V 47%; 5m: IAC-V 63% (±2), IAC-Q 58% (±3); 5w: IAC-Q 57% (±5), IAC-V 18% (±5); 2d_3z: IAC-V 27% (±9), IAC-Q 19% (±21).",
            "baseline_comparison": "Used as a baseline for COMA; generally underperformed compared to COMA and centralised critics, learning more slowly and/or converging to lower win rates.",
            "coordination_benefits": "Implicit coordination can emerge but is weaker; parameter sharing can speed some aspects of learning but lacks access to global state to improve policy evaluation and credit assignment.",
            "coordination_challenges": "Poor multi-agent credit assignment due to critics conditioning only on local histories; noisy gradients when other agents explore; struggles under partial observability and when coordination between agents is required.",
            "ablation_studies": "IAC variants served as ablations; they showed slower learning and worse final performance relative to COMA and centralised critic baselines.",
            "optimal_configurations": "Parameter-sharing among agents (single actor/critic network) with agent ID as input used in the paper; IAC performed better with Q-based critics (IAC-Q) than V-only in some maps, but overall inferior to centralized critic approaches.",
            "uuid": "e2567.1",
            "source_info": {
                "paper_title": "Counterfactual Multi-Agent Policy Gradients",
                "publication_date_yy_mm": "2017-05"
            }
        },
        {
            "name_short": "central-V",
            "name_full": "Centralized V critic baseline",
            "brief_description": "A centralized critic variant that estimates the state-value V(s) using centralised state information; advantage estimates are obtained via TD error, and actors remain decentralized.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "central-V (centralized value critic)",
            "system_description": "Actors are decentralized (as in COMA) but the critic is centralized and estimates V(s) conditioned on global state; training uses TD-error for advantage estimation rather than counterfactual Q-based baselines. The critic receives global state and agents' observations but does not condition on joint actions.",
            "number_of_agents": "variable (3 or 5 in experiments)",
            "agent_specializations": "Homogeneous agents controlling individual units; no explicit role specialization beyond unit type and local observations.",
            "research_phases_covered": "Training/learning and execution/evaluation.",
            "coordination_mechanism": "Centralized critic provides a common value baseline during training but does not reason about individual agents' action-level contributions; coordination is therefore weaker than COMA's counterfactual approach.",
            "communication_protocol": "No runtime inter-agent communication; centralized critic uses global state during training only.",
            "feedback_mechanism": "TD-error from centralized V critic provides feedback to actor updates (used in policy gradient).",
            "communication_frequency": "Critic feedback provided each training update / trajectory sample.",
            "task_domain": "StarCraft micromanagement benchmark.",
            "performance_metrics": "Mean win percentage from Table 1: 3m: central-V 83% (±3); 5m: 67% (±5); 5w: 65% (±3); 2d_3z: 36% (±6).",
            "baseline_comparison": "Compared directly to COMA and other baselines. COMA outperformed central-V in final performance and typically trained faster and more stably.",
            "coordination_benefits": "Centralized state access improves policy evaluation compared to local critics; yields better performance than purely decentralized IAC in many maps.",
            "coordination_challenges": "Does not solve per-agent credit assignment as effectively as COMA since it does not condition on joint actions or compute counterfactuals; training stability issues observed compared to COMA.",
            "ablation_studies": "central-V used as an ablation; results show COMA (central Q + counterfactual baseline) outperforms central-V, indicating the value of action-conditional Q information and counterfactual baselines.",
            "optimal_configurations": "Centralized V critic with TD(lambda) (paper used lambda=0.8), but empirically COMA's counterfactual Q-based approach is preferable.",
            "uuid": "e2567.2",
            "source_info": {
                "paper_title": "Counterfactual Multi-Agent Policy Gradients",
                "publication_date_yy_mm": "2017-05"
            }
        },
        {
            "name_short": "central-QV",
            "name_full": "Centralized Q and V critic baseline (Q-V advantage)",
            "brief_description": "A centralized critic variant that learns both Q(s,u) and V(s) and computes the per-agent advantage as Q - V (instead of COMA's counterfactual marginalization).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "central-QV (centralized Q and V critics)",
            "system_description": "Centralized critic learns both a joint Q(s, u) and a state-value V(s). Advantage used for policy updates is Q(s,u) - V(s) rather than the counterfactual per-agent baseline used by COMA. Actors remain decentralized with parameter sharing.",
            "number_of_agents": "variable (3 or 5 in experiments)",
            "agent_specializations": "Homogeneous agents differentiated only by observations and unit types.",
            "research_phases_covered": "Training/learning and execution/evaluation.",
            "coordination_mechanism": "Centralized Q and V are used to estimate advantages but without marginalizing out single-agent actions; coordination information is present in Q but credit assignment is less precisely targeted to individual agents.",
            "communication_protocol": "No explicit agent-to-agent communication; centralized critic used during training.",
            "feedback_mechanism": "Centralized Q and V produce an advantage signal (Q - V) used as the policy gradient target.",
            "communication_frequency": "Per training update / trajectory.",
            "task_domain": "StarCraft micromanagement.",
            "performance_metrics": "Mean win percentage from Table 1: 3m: central-QV 83% (±5); 5m: 71% (±9); 5w: 76% (±1); 2d_3z: 39% (±5).",
            "baseline_comparison": "Serves as an ablation testing the importance of counterfactual baseline; COMA strictly dominated central-QV in both training speed and final performance across maps.",
            "coordination_benefits": "Using centralized Q and V provides richer information than decentralized critics, but without counterfactual marginalization it provides a less targeted credit assignment for each agent.",
            "coordination_challenges": "Less effective per-agent credit assignment relative to COMA; inferior final performance and slower/fewer stable improvements in experiments.",
            "ablation_studies": "central-QV is an explicit ablation demonstrating the value of computing a counterfactual baseline rather than using V as a baseline.",
            "optimal_configurations": "Learning both Q and V centrally can help but requires using the counterfactual baseline (as in COMA) to achieve the best empirical results.",
            "uuid": "e2567.3",
            "source_info": {
                "paper_title": "Counterfactual Multi-Agent Policy Gradients",
                "publication_date_yy_mm": "2017-05"
            }
        },
        {
            "name_short": "GMEZO (mentioned)",
            "name_full": "GMEZO (greedy-MDP with episodic zero-order optimisation)",
            "brief_description": "A state-of-the-art centralized StarCraft controller from prior work (Usunier et al. 2016) that uses a greedy MDP formulation combined with episodic zero-order optimisation and full-field-of-view centralized control; cited as an upper-bound comparator.",
            "citation_title": "Episodic exploration for deep deterministic policies: An application to starcraft micromanagement tasks",
            "mention_or_use": "mention",
            "system_name": "GMEZO (centralized greedy-MDP + zero-order optimisation)",
            "system_description": "Centralized controller that sequentially chooses actions for agents using a greedy MDP formulation and episodic zero-order optimization; has access to full global state and StarCraft macro-actions (attack-move), unlike the decentralized restricted-FoV setup used for COMA.",
            "number_of_agents": "variable (comparable maps; evaluated for same maps under full-field-of-view centralized control)",
            "agent_specializations": "Centralized planner controls all units; per-unit specialization depends on planner outputs and unit types.",
            "research_phases_covered": "Training/optimisation and execution/evaluation (centralized).",
            "coordination_mechanism": "Centralized greedy MDP planner controlling all agents centrally at every step.",
            "communication_protocol": "Not applicable (single centralized controller).",
            "feedback_mechanism": "Optimization-based policy updates (episodic zero-order optimization).",
            "communication_frequency": "Centralized decisions at each timestep.",
            "task_domain": "StarCraft micromanagement (full-field-of-view centralized control).",
            "performance_metrics": "Published centralized results (from Usunier et al. 2016) reported in paper for comparison: e.g., 5m GMEZO 100% win in full-FoV centralized setting (table shows GMEZO 100 for 5m).",
            "baseline_comparison": "Used as an upper-bound reference: COMA (decentralized, limited FoV) achieved comparable performance to GMEZO in several maps despite information and action restrictions.",
            "coordination_benefits": "Full centralized state and macro-actions simplify coordination and yield high win rates in prior work.",
            "coordination_challenges": "Centralized controllers require full state and are less realistic for decentralized execution settings; not directly comparable when FoV and macro-actions differ.",
            "ablation_studies": "Not part of this paper's ablations; cited as prior centralized baseline.",
            "uuid": "e2567.4",
            "source_info": {
                "paper_title": "Counterfactual Multi-Agent Policy Gradients",
                "publication_date_yy_mm": "2017-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Episodic exploration for deep deterministic policies: An application to starcraft micromanagement tasks",
            "rating": 2
        },
        {
            "paper_title": "Learning to communicate with deep multi-agent reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Multi-agent actor-critic for mixed cooperative-competitive environments",
            "rating": 2
        },
        {
            "paper_title": "Stabilising experience replay for deep multi-agent reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Learning cooperative visual dialog agents with deep reinforcement learning",
            "rating": 1
        }
    ],
    "cost": 0.01678475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Counterfactual Multi-Agent Policy Gradients</h1>
<p>Jakob N. Foerster*<br>University of Oxford, United Kingdom<br>jakob.foerster@cs.ox.ac.uk</p>
<p>Triantafyllos Afouras<br>University of Oxford, UK<br>afourast@robots.ox.ac.uk</p>
<h2>Gregory Farquhar ${ }^{\dagger}$</h2>
<p>University of Oxford, United Kingdom
gregory.farquhar@cs.ox.ac.uk</p>
<h2>Triantafyllos Afouras</h2>
<p>University of Oxford, UK
afourast@robots.ox.ac.uk</p>
<p>Nantas Nardelli<br>University of Oxford, UK<br>nantas@robots.ox.ac.uk</p>
<h2>Shimon Whiteson</h2>
<p>University of Oxford, UK
shimon.whiteson@cs.ox.ac.uk</p>
<h4>Abstract</h4>
<p>Many real-world problems, such as network packet routing and the coordination of autonomous vehicles, are naturally modelled as cooperative multi-agent systems. There is a great need for new reinforcement learning methods that can efficiently learn decentralised policies for such systems. To this end, we propose a new multi-agent actor-critic method called counterfactual multi-agent (COMA) policy gradients. COMA uses a centralised critic to estimate the $Q$-function and decentralised actors to optimise the agents' policies. In addition, to address the challenges of multi-agent credit assignment, it uses a counterfactual baseline that marginalises out a single agent's action, while keeping the other agents' actions fixed. COMA also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass. We evaluate COMA in the testbed of StarCraft unit micromanagement, using a decentralised variant with significant partial observability. COMA significantly improves average performance over other multi-agent actorcritic methods in this setting, and the best performing agents are competitive with state-of-the-art centralised controllers that get access to the full state.</p>
<h2>1 Introduction</h2>
<p>Many complex reinforcement learning (RL) problems such as the coordination of autonomous vehicles (Cao et al. 2013), network packet delivery (Ye, Zhang, and Yang 2015), and distributed logistics (Ying and Dayong 2005) are naturally modelled as cooperative multi-agent systems. However, RL methods designed for single agents typically fare poorly on such tasks, since the joint action space of the agents grows exponentially with the number of agents.</p>
<p>To cope with such complexity, it is often necessary to resort to decentralised policies, in which each agent selects its own action conditioned only on its local action-observation history. Furthermore, partial observability and communication constraints during execution may necessitate the use of decentralised policies even when the joint action space is not prohibitively large.</p>
<p>Hence, there is a great need for new RL methods that can efficiently learn decentralised policies. In some settings,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>the learning itself may also need to be decentralised. However, in many cases, learning can take place in a simulator or a laboratory in which extra state information is available and agents can communicate freely. This centralised training of decentralised policies is a standard paradigm for multi-agent planning (Oliehoek, Spaan, and Vlassis 2008; Kraemer and Banerjee 2016) and has recently been picked up by the deep RL community (Foerster et al. 2016; Jorge, Kägebäck, and Gustavsson 2016). However, the question of how best to exploit the opportunity for centralised learning remains open.</p>
<p>Another crucial challenge is multi-agent credit assignment (Chang, Ho, and Kaelbling 2003): in cooperative settings, joint actions typically generate only global rewards, making it difficult for each agent to deduce its own contribution to the team's success. Sometimes it is possible to design individual reward functions for each agent. However, these rewards are not generally available in cooperative settings and often fail to encourage individual agents to sacrifice for the greater good. This often substantially impedes multi-agent learning in challenging tasks, even with relatively small numbers of agents.</p>
<p>In this paper, we propose a new multi-agent RL method called counterfactual multi-agent (COMA) policy gradients, in order to address these issues. COMA takes an actor-critic (Konda and Tsitsiklis 2000) approach, in which the actor, i.e., the policy, is trained by following a gradient estimated by a critic. COMA is based on three main ideas.</p>
<p>First, COMA uses a centralised critic. The critic is only used during learning, while only the actor is needed during execution. Since learning is centralised, we can therefore use a centralised critic that conditions on the joint action and all available state information, while each agent's policy conditions only on its own action-observation history.</p>
<p>Second, COMA uses a counterfactual baseline. The idea is inspired by difference rewards (Wolpert and Tumer 2002; Tumer and Agogino 2007), in which each agent learns from a shaped reward that compares the global reward to the reward received when that agent's action is replaced with a default action. While difference rewards are a powerful way to perform multi-agent credit assignment, they require access to a simulator or estimated reward function, and in general it is unclear how to choose the default action. COMA addresses this by using the centralised critic to compute an</p>
<p>agent-specific advantage function that compares the estimated return for the current joint action to a counterfactual baseline that marginalises out a single agent's action, while keeping the other agents' actions fixed. This is similar to calculating an aristocrat utility (Wolpert and Tumer 2002), but avoids the problem of a recursive interdependence between the policy and utility function because the expected contribution of the counterfactual baseline to the policy gradient is zero. Hence, instead of relying on extra simulations, approximations, or assumptions regarding appropriate default actions, COMA computes a separate baseline for each agent that relies on the centralised critic to reason about counterfactuals in which only that agent's action changes.</p>
<p>Third, COMA uses a critic representation that allows the counterfactual baseline to be computed efficiently. In a single forward pass, it computes the $Q$-values for all the different actions of a given agent, conditioned on the actions of all the other agents. Because a single centralised critic is used for all agents, all $Q$-values for all agents can be computed in a single batched forward pass.</p>
<p>We evaluate COMA in the testbed of StarCraft unit micromanagement ${ }^{1}$, which has recently emerged as a challenging RL benchmark task with high stochasticity, a large stateaction space, and delayed rewards. Previous works (Usunier et al. 2016; Peng et al. 2017) have made use of a centralised control policy that conditions on the entire state and can use powerful macro-actions, using StarCraft's built-in planner, that combine movement and attack actions. To produce a meaningfully decentralised benchmark that proves challenging for scenarios with even relatively few agents, we propose a variant that massively reduces each agent's field-of-view and removes access to these macro-actions.</p>
<p>Our empirical results on this new benchmark show that COMA can significantly improve performance over other multi-agent actor-critic methods, as well as ablated versions of COMA itself. In addition, COMA's best agents are competitive with state-of-the-art centralised controllers that are given access to full state information and macro-actions.</p>
<h2>2 Related Work</h2>
<p>Although multi-agent RL has been applied in a variety of settings (Busoniu, Babuska, and De Schutter 2008; Yang and Gu 2004), it has often been restricted to tabular methods and simple environments. One exception is recent work in deep multi-agent RL, which can scale to high dimensional input and action spaces. Tampuu et al. (2015) use a combination of DQN with independent $Q$-learning (Tan 1993; Shoham and Leyton-Brown 2009) to learn how to play twoplayer pong. More recently the same method has been used by Leibo et al. (2017) to study the emergence of collaboration and defection in sequential social dilemmas.</p>
<p>Also related is work on the emergence of communication between agents, learned by gradient descent (Das et al. 2017; Mordatch and Abbeel 2017; Lazaridou, Peysakhovich, and Baroni 2016; Foerster et al. 2016; Sukhbaatar, Fergus, and others 2016). In this line of work, passing gradients between</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>agents during training and sharing parameters are two common ways to take advantage of centralised training. However, these methods do not allow for extra state information to be used during learning and do not address the multi-agent credit assignment problem.</p>
<p>Gupta, Egorov, and Kochenderfer (2017) investigate actor-critic methods for decentralised execution with centralised training. However, in their methods both the actors and the critic condition on local, per-agent, observations and actions, and multi-agent credit assignment is addressed only with hand-crafted local rewards.</p>
<p>Most previous applications of RL to StarCraft micromanagement use a centralised controller, with access to the full state, and control of all units, although the architecture of the controllers exploits the multi-agent nature of the problem. Usunier et al. (2016) use a greedy MDP, which at each timestep sequentially chooses actions for agents given all previous actions, in combination with zero-order optimisation, while Peng et al. (2017) use an actor-critic method that relies on RNNs to exchange information between the agents.</p>
<p>The closest to our problem setting is that of Foerster et al. (2017), who also use a multi-agent representation and decentralised policies. However, they focus on stabilising experience replay while using DQN and do not make full use of the centralised training regime. As they do not report on absolute win-rates we do not compare performance directly. However, Usunier et al. (2016) address similar scenarios to our experiments and implement a DQN baseline in a fully observable setting. In Section 6 we therefore report our competitive performance against these state-of-the-art baselines, while maintaining decentralised control. Omidshafiei et al. (2017) also address the stability of experience replay in multi-agent settings, but assume a fully decentralised training regime.
(Lowe et al. 2017) concurrently propose a multi-agent policy-gradient algorithm using centralised critics. Their approach does not address multi-agent credit assignment. Unlike our work, it learns a separate centralised critic for each agent and is applied to competitive environments with continuous action spaces.</p>
<p>Our work builds directly off of the idea of difference rewards (Wolpert and Tumer 2002). The relationship of COMA to this line of work is discussed in Section 4.</p>
<h2>3 Background</h2>
<p>We consider a fully cooperative multi-agent task that can be described as a stochastic game $G$, defined by a tuple $G=\langle S, U, P, r, Z, O, n, \gamma\rangle$, in which $n$ agents identified by $a \in A \equiv{1, \ldots, n}$ choose sequential actions. The environment has a true state $s \in S$. At each time step, each agent simultaneously chooses an action $u^{a} \in U$, forming a joint action $\mathbf{u} \in \mathbf{U} \equiv U^{n}$ which induces a transition in the environment according to the state transition function $P\left(s^{\prime} \mid s, \mathbf{u}\right): S \times \mathbf{U} \times S \rightarrow[0,1]$. The agents all share the same reward function $r(s, \mathbf{u}): S \times \mathbf{U} \rightarrow \mathbb{R}$ and $\gamma \in[0,1)$ is a discount factor.</p>
<p>We consider a partially observable setting, in which agents draw observations $z \in Z$ according to the observation function $O(s, a): S \times A \rightarrow Z$. Each agent has an</p>
<p>action-observation history $\tau^{a} \in T \equiv(Z \times U)^{*}$, on which it conditions a stochastic policy $\pi^{a}\left(u^{a} \mid \tau^{a}\right): T \times U \rightarrow[0,1]$. We denote joint quantities over agents in bold, and joint quantities over agents other than a given agent $a$ with the superscript $-a$.</p>
<p>The discounted return is $R_{t}=\sum_{l=0}^{\infty} \gamma^{l} r_{t+l}$. The agents' joint policy induces a value function, i.e., an expectation over $R_{t}, V^{\boldsymbol{\pi}}\left(s_{t}\right)=\mathbb{E}<em _infty="\infty" t_1:="t+1:">{s</em>}, \mathbf{u<em t="t">{t: \infty}}\left[R</em>} \mid s_{t}\right]$, and an actionvalue function $Q^{\boldsymbol{\pi}}\left(s_{t}, \mathbf{u<em _infty="\infty" s__t_1:="s_{t+1:">{t}\right)=\mathbb{E}</em>}, \mathbf{u<em t="t">{t+1: \infty}}\left[R</em>} \mid s_{t}, \mathbf{u<em t="t">{t}\right]$. The advantage function is given by $A^{\boldsymbol{\pi}}\left(s</em>}, \mathbf{u<em t="t">{t}\right)=$ $Q^{\boldsymbol{\pi}}\left(s</em>}, \mathbf{u<em t="t">{t}\right)-V^{\boldsymbol{\pi}}\left(s</em>\right)$.</p>
<p>Following previous work (Oliehoek, Spaan, and Vlassis 2008; Kraemer and Banerjee 2016; Foerster et al. 2016; Jorge, Kågebäck, and Gustavsson 2016), our problem setting allows centralised training but requires decentralised execution. This is a natural paradigm for a large set of multi-agent problems where training is carried out using a simulator with additional state information, but the agents must rely on local action-observation histories during execution. To condition on this full history, a deep RL agent may make use of a recurrent neural network (Hausknecht and Stone 2015), typically with a gated model such as LSTM (Hochreiter and Schmidhuber 1997) or GRU (Cho et al. 2014).</p>
<p>In Section 4, we develop a new multi-agent policy gradient method for tackling this setting. In the remainder of this section, we provide some background on single-agent policy gradient methods (Sutton et al. 1999). Such methods optimise a single agent's policy, parameterised by $\theta^{x}$, by performing gradient ascent on an estimate of the expected discounted total reward $J=\mathbb{E}<em 0="0">{\pi}\left[R</em>\right]$. Perhaps the simplest form of policy gradient is REINFORCE (Williams 1992), in which the gradient is:</p>
<p>$$
g=\mathbb{E}<em 0:="0:" _infty="\infty">{s</em>\right)\right]
$$}, u_{0: \infty}}\left[\sum_{t=0}^{T} R_{t} \nabla_{\theta^{u}} \log \pi\left(u_{t} \mid s_{t</p>
<p>In actor-critic approaches (Sutton et al. 1999; Konda and Tsitsiklis 2000; Schulman et al. 2015), the actor, i.e., the policy, is trained by following a gradient that depends on a critic, which usually estimates a value function. In particular, $R_{t}$ is replaced by any expression equivalent to $Q\left(s_{t}, u_{t}\right)-b\left(s_{t}\right)$, where $b\left(s_{t}\right)$ is a baseline designed to reduce variance (Weaver and Tao 2001). A common choice is $b\left(s_{t}\right)=V\left(s_{t}\right)$, in which case $R_{t}$ is replaced by $A\left(s_{t}, u_{t}\right)$. Another option is to replace $R_{t}$ with the temporal difference (TD) error $r_{t}+\gamma V\left(s_{t+1}\right)-V(s)$, which is an unbiased estimate of $A\left(s_{t}, u_{t}\right)$. In practice, the gradient must be estimated from trajectories sampled from the environment, and the (action-)value functions must be estimated with function approximators. Consequently, the bias and variance of the gradient estimate depends strongly on the exact choice of estimator (Konda and Tsitsiklis 2000).</p>
<p>In this paper, we train critics $f^{c}\left(\cdot, \theta^{c}\right)$ on-policy to estimate either $Q$ or $V$, using a variant of TD( $\lambda$ ) (Sutton 1988) adapted for use with deep neural networks. TD( $\lambda$ ) uses a mixture of $n$-step returns $G_{t}^{(n)}=\sum_{l=1}^{n} \gamma^{l-1} r_{t+l}+$ $\gamma^{n} f^{c}\left(\cdot_{t+n}, \theta^{c}\right)$. In particular, the critic parameters $\theta^{c}$ are updated by minibatch gradient descent to minimise the follow-
ing loss:</p>
<p>$$
\mathcal{L}_{t}\left(\theta^{c}\right)=\left(y^{(\lambda)}-f^{c}\left(\cdot t, \theta^{c}\right)\right)^{2}
$$</p>
<p>where $y^{(\lambda)}=(1-\lambda) \sum_{n=1}^{\infty} \lambda^{n-1} G_{t}^{(n)}$, and the $n$-step returns $G_{t}^{(n)}$ are calculated with bootstrapped values estimated by a target network (Mnih et al. 2015) with parameters copied periodically from $\theta^{c}$.</p>
<h2>4 Methods</h2>
<p>In this section, we describe approaches for extending policy gradients to our multi-agent setting.</p>
<h2>Independent Actor-Critic</h2>
<p>The simplest way to apply policy gradients to multiple agents is to have each agent learn independently, with its own actor and critic, from its own action-observation history. This is essentially the idea behind independent $Q$ learning (Tan 1993), which is perhaps the most popular multi-agent learning algorithm, but with actor-critic in place of $Q$-learning. Hence, we call this approach independent actor-critic (IAC).</p>
<p>In our implementation of IAC, we speed learning by sharing parameters among the agents, i.e., we learn only one actor and one critic, which are used by all agents. The agents can still behave differently because they receive different observations, including an agent-specific ID, and thus evolve different hidden states. Learning remains independent in the sense that each agent's critic estimates only a local value function, i.e., one that conditions on $u^{a}$, not $\mathbf{u}$. Though we are not aware of previous applications of this specific algorithm, we do not consider it a significant contribution but instead merely a baseline algorithm.</p>
<p>We consider two variants of IAC. In the first, each agent's critic estimates $V\left(\tau^{a}\right)$ and follows a gradient based on the TD error, as described in Section 3. In the second, each agent's critic estimates $Q\left(\tau^{a}, u^{a}\right)$ and follows a gradient based on the advantage: $A\left(\tau^{a}, u^{a}\right)=Q\left(\tau^{a}, u^{a}\right)-$ $V\left(\tau^{a}\right)$, where $V\left(\tau^{a}\right)=\sum_{u^{a}} \pi\left(u^{a} \mid \tau^{a}\right) Q\left(\tau^{a}, u^{a}\right)$. Independent learning is straightforward, but the lack of information sharing at training time makes it difficult to learn coordinated strategies that depend on interactions between multiple agents, or for an individual agent to estimate the contribution of its actions to the team's reward.</p>
<h2>Counterfactual Multi-Agent Policy Gradients</h2>
<p>The difficulties discussed above arise because, beyond parameter sharing, IAC fails to exploit the fact that learning is centralised in our setting. In this section, we propose counterfactual multi-agent (COMA) policy gradients, which overcome this limitation. Three main ideas underly COMA: 1) centralisation of the critic, 2) use of a counterfactual baseline, and 3) use of a critic representation that allows efficient evaluation of the baseline. The remainder of this section describes these ideas.</p>
<p>First, COMA uses a centralised critic. Note that in IAC, each actor $\pi\left(u^{a} \mid \tau^{a}\right)$ and each critic $Q\left(\tau^{a}, u^{a}\right)$ or $V\left(\tau^{a}\right)$ conditions only on the agent's own action-observation history $\tau^{a}$. However, the critic is used only during learning and</p>
<p>only the actor is needed during execution. Since learning is centralised, we can therefore use a centralised critic that conditions on the true global state $s$, if it is available, or the joint action-observation histories $\tau$ otherwise. Each actor conditions on its own action-observation histories $\tau^{a}$, with parameter sharing, as in IAC. Figure 1a illustrates this setup.</p>
<p>A naive way to use this centralised critic would be for each actor to follow a gradient based on the TD error estimated from this critic:</p>
<p>$$
g=\nabla_{\theta^{a}} \log \pi\left(u \mid \tau_{t}^{a}\right)\left(r+\gamma V\left(s_{t+1}\right)-V\left(s_{t}\right)\right)
$$</p>
<p>However, such an approach fails to address a key credit assignment problem. Because the TD error considers only global rewards, the gradient computed for each actor does not explicitly reason about how that particular agent's actions contribute to that global reward. Since the other agents may be exploring, the gradient for that agent becomes very noisy, particularly when there are many agents.</p>
<p>Therefore, COMA uses a counterfactual baseline. The idea is inspired by difference rewards (Wolpert and Tumer 2002), in which each agent learns from a shaped reward $D^{a}=r(s, \mathbf{u})-r\left(s,\left(\mathbf{u}^{-a}, c^{a}\right)\right)$ that compares the global reward to the reward received when the action of agent $a$ is replaced with a default action $c^{a}$. Any action by agent $a$ that improves $D^{a}$ also improves the true global reward $r(s, \mathbf{u})$, because $r\left(s,\left(\mathbf{u}^{-a}, c^{a}\right)\right)$ does not depend on agent $a$ 's actions.</p>
<p>Difference rewards are a powerful way to perform multiagent credit assignment. However, they typically require access to a simulator in order to estimate $r\left(s,\left(\mathbf{u}^{-a}, c^{a}\right)\right)$. When a simulator is already being used for learning, difference rewards increase the number of simulations that must be conducted, since each agent's difference reward requires a separate counterfactual simulation. Proper and Tumer (2012) and Colby, Curran, and Tumer (2015) propose estimating difference rewards using function approximation rather than a simulator. However, this still requires a user-specified default action $c^{a}$ that can be difficult to choose in many applications. In an actor-critic architecture, this approach would also introduce an additional source of approximation error.</p>
<p>A key insight underlying COMA is that a centralised critic can be used to implement difference rewards in a way that avoids these problems. COMA learns a centralised critic, $Q(s, \mathbf{u})$ that estimates $Q$-values for the joint action $\mathbf{u}$ conditioned on the central state $s$. For each agent $a$ we can then compute an advantage function that compares the $Q$-value for the current action $u^{a}$ to a counterfactual baseline that marginalises out $u^{a}$, while keeping the other agents' actions $\mathbf{u}^{-a}$ fixed:</p>
<p>$$
A^{a}(s, \mathbf{u})=Q(s, \mathbf{u})-\sum_{u^{\prime a}} \pi^{a}\left(u^{\prime a} \mid \tau^{a}\right) Q\left(s,\left(\mathbf{u}^{-a}, u^{\prime a}\right)\right)
$$</p>
<p>Hence, $A^{a}\left(s, u^{a}\right)$ computes a separate baseline for each agent that uses the centralised critic to reason about counterfactuals in which only $a$ 's action changes, learned directly from agents' experiences instead of relying on extra simulations, a reward model, or a user-designed default action.</p>
<p>This advantage has the same form as the aristocrat utility (Wolpert and Tumer 2002). However, optimising for an aristocrat utility using value-based methods creates a selfconsistency problem because the policy and utility function depend recursively on each other. As a result, prior work focused on difference evaluations using default states and actions. COMA is different because the counterfactual baseline's expected contribution to the gradient, as with other policy gradient baselines, is zero. Thus, while the baseline does depend on the policy, its expectation does not. Consequently, COMA can use this form of the advantage without creating a self-consistency problem.</p>
<p>While COMA's advantage function replaces potential extra simulations with evaluations of the critic, those evaluations may themselves be expensive if the critic is a deep neural network. Furthermore, in a typical representation, the number of output nodes of such a network would equal $|U|^{n}$, the size of the joint action space, making it impractical to train. To address both these issues, COMA uses a critic representation that allows for efficient evaluation of the baseline. In particular, the actions of the other agents, $\mathbf{u}_{t}^{-a}$, are part of the input to the network, which outputs a $Q$-value for each of agent $a$ 's actions, as shown in Figure 1c. Consequently, the counterfactual advantage can be calculated efficiently by a single forward pass of the actor and critic, for each agent. Furthermore, the number of outputs is only $|U|$ instead of $\left(|U|^{n}\right)$. While the network has a large input space that scales linearly in the number of agents and actions, deep neural networks can generalise well across such spaces.</p>
<p>In this paper, we focus on settings with discrete actions. However, COMA can be easily extended to continuous actions spaces by estimating the expectation in (4) with Monte Carlo samples or using functional forms that render it analytical, e.g., Gaussian policies and critic.</p>
<p>The following lemma establishes the convergence of COMA to a locally optimal policy. The proof follows directly from the convergence of single-agent actor-critic algorithms (Sutton et al. 1999; Konda and Tsitsiklis 2000), and is subject to the same assumptions.
Lemma 1. For an actor-critic algorithm with a compatible TD(1) critic following a COMA policy gradient</p>
<p>$$
g_{k}=\mathbb{E}<em a="a">{\boldsymbol{\pi}}\left[\sum</em>)\right]
$$} \nabla_{\theta_{k}} \log \pi^{a}\left(u^{a} \mid \tau^{a}\right) A^{a}(s, \mathbf{u</p>
<p>at each iteration $k$,</p>
<p>$$
\liminf _{k}||\nabla J||=0 \quad w \cdot p .1
$$</p>
<p>Proof. The COMA gradient is given by</p>
<p>$$
\begin{aligned}
g &amp; =\mathbb{E}<em a="a">{\boldsymbol{\pi}}\left[\sum</em>)\right] \
A^{a}(s, \mathbf{u}) &amp; =Q(s, \mathbf{u})-b\left(s, \mathbf{u}^{-a}\right)
\end{aligned}
$$} \nabla_{\theta} \log \pi^{a}\left(u^{a} \mid \tau^{a}\right) A^{a}(s, \mathbf{u</p>
<p>where $\theta$ are the parameters of all actor policies, e.g. $\theta=$ $\left{\theta^{1}, \ldots, \theta^{|A|}\right}$, and $b\left(s, \mathbf{u}^{-a}\right)$ is the counterfactual baseline defined in equation 4 .</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: In (a), information flow between the decentralised actors, the environment and the centralised critic in COMA; red arrows and components are only required during centralised learning. In (b) and (c), architectures of the actor and critic.</p>
<p>First consider the expected contribution of the this baseline $b\left(s, \mathbf{u}^{-a}\right)$ :</p>
<p>$$
g_{b}=-\mathbb{E}<em a="a">{\boldsymbol{\pi}}\left[\sum</em>\right)\right]
$$} \nabla_{\theta} \log \pi^{a}\left(u^{a} \mid \tau^{a}\right) b\left(s, \mathbf{u}^{-a</p>
<p>where the expectation $\mathbb{E}_{\boldsymbol{\pi}}$ is with respect to the state-action distribution induced by the joint policy $\boldsymbol{\pi}$. Now let $d^{\boldsymbol{\pi}}(s)$ be the discounted ergodic state distribution as defined by Sutton et al. (1999):</p>
<p>$$
\begin{aligned}
g_{b}= &amp; -\sum_{s} d^{\boldsymbol{\pi}}(s) \sum_{a} \sum_{\mathbf{u}^{-a}} \boldsymbol{\pi}\left(\mathbf{u}^{-a} \mid \boldsymbol{\tau}-a\right) \
&amp; \sum_{u^{a}} \pi^{a}\left(u^{a} \mid \tau^{a}\right) \nabla_{\theta} \log \pi^{a}\left(u^{a} \mid \tau^{a}\right) b\left(s, \mathbf{u}^{-a}\right) \
= &amp; -\sum_{s} d^{\boldsymbol{\pi}}(s) \sum_{a} \sum_{\mathbf{u}^{-a}} \boldsymbol{\pi}\left(\mathbf{u}^{-a} \mid \boldsymbol{\tau}-a\right) \
&amp; \sum_{u^{a}} \nabla_{\theta} \pi^{a}\left(u^{a} \mid \tau^{a}\right) b\left(s, \mathbf{u}^{-a}\right) \
= &amp; -\sum_{s} d^{\boldsymbol{\pi}}(s) \sum_{a} \sum_{\mathbf{u}^{-a}} \boldsymbol{\pi}\left(\mathbf{u}^{-a} \mid \boldsymbol{\tau}-a\right) b\left(s, \mathbf{u}^{-a}\right) \nabla_{\theta} 1 \
= &amp; 0
\end{aligned}
$$</p>
<p>Clearly, the per-agent baseline, although it reduces variance, does not change the expected gradient, and therefore does not affect the convergence of COMA.</p>
<p>The remainder of the expected policy gradient is given by:</p>
<p>$$
\begin{aligned}
g &amp; =\mathbb{E}<em a="a">{\boldsymbol{\pi}}\left[\sum</em>)\right] \
&amp; =\mathbb{E}} \nabla_{\theta} \log \pi^{a}\left(u^{a} \mid \tau^{a}\right) Q(s, \mathbf{u<em _theta="\theta">{\boldsymbol{\pi}}\left[\nabla</em>)\right]
\end{aligned}
$$} \log \prod_{a} \pi^{a}\left(u^{a} \mid \tau^{a}\right) Q(s, \mathbf{u</p>
<p>Writing the joint policy as a product of the independent actors:</p>
<p>$$
\boldsymbol{\pi}(\mathbf{u} \mid s)=\prod_{a} \pi^{a}\left(u^{a} \mid \tau^{a}\right)
$$</p>
<p>yields the standard single-agent actor-critic policy gradient:</p>
<p>$$
g=\mathbb{E}<em _theta="\theta">{\boldsymbol{\pi}}\left[\nabla</em>)\right]
$$} \log \boldsymbol{\pi}(\mathbf{u} \mid s) Q(s, \mathbf{u</p>
<p>Konda and Tsitsiklis (2000) prove that an actor-critic following this gradient converges to a local maximum of the expected return $J^{\pi}$, given that:</p>
<ol>
<li>the policy $\pi$ is differentiable,</li>
<li>the update timescales for $Q$ and $\pi$ are sufficiently slow, and that $\pi$ is updated sufficiently slower than $Q$, and</li>
<li>$Q$ uses a representation compatible with $\pi$,
amongst several further assumptions. The parameterisation of the policy (i.e., the single-agent joint-action learner is decomposed into independent actors) is immaterial to convergence, as long as it remains differentiable. Note however that COMA's centralised critic is essential for this proof to hold.</li>
</ol>
<h2>5 Experimental Setup</h2>
<p>In this section, we describe the StarCraft problem to which we apply COMA, as well as details of the state features, network architectures, training regimes, and ablations.</p>
<p>Decentralised StarCraft Micromanagement. StarCraft is a rich environment with stochastic dynamics that cannot be easily emulated. Many simpler multi-agent settings, such as Predator-Prey (Tan 1993) or Packet World (Weyns, Helleboogh, and Holvoet 2005), by contrast, have full simulators with controlled randomness that can be freely set to any state in order to perfectly replay experiences. This makes it possible, though computationally expensive, to compute difference rewards via extra simulations. In StarCraft, as in the real world, this is not possible.</p>
<p>In this paper, we focus on the problem of micromanagement in StarCraft, which refers to the low-level control of individual units' positioning and attack commands as they fight enemies. This task is naturally represented as a multi-agent system, where each StarCraft unit is replaced by a decentralised controller. We consider several scenarios with symmetric teams formed of: 3 marines ( 3 m ), 5 marines ( 5 m ), 5 wraiths ( 5 w ), or 2 dragoons with 3 zealots (2d.3z). The enemy team is controlled by the StarCraft AI, which uses reasonable but suboptimal hand-crafted heuristics.</p>
<p>We allow the agents to choose from a set of discrete actions: move [direction], attack[enemy_id],</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Starting position with example local field of view for the $2 \mathrm{~d} .3 \mathrm{z}$ map.
stop, and noop. In the StarCraft game, when a unit selects an attack action, it first moves into attack range before firing, using the game's built-in pathfinding to choose a route. These powerful attack-move macro-actions make the control problem considerably easier.</p>
<p>To create a more challenging benchmark that is meaningfully decentralised, we impose a restricted field of view on the agents, equal to the firing range of ranged units' weapons, shown in Figure 2. This departure from the standard setup for centralised StarCraft control has three effects.</p>
<p>First, it introduces significant partial observability. Second, it means units can only attack when they are in range of enemies, removing access to the StarCraft macro-actions. Third, agents cannot distinguish between enemies who are dead and those who are out of range and so can issue invalid attack commands at such enemies, which results in no action being taken. This substantially increases the average size of the action space, which in turn increases the difficulty of both exploration and control.</p>
<p>Under these difficult conditions, scenarios with even relatively small numbers of units become much harder to solve. As seen in Table 1, we compare against a simple hand-coded heuristic that instructs the agents to run forwards into range and then focus their fire, attacking each enemy in turn until it dies. This heuristic achieves a $98 \%$ win rate on 5 m with a full field of view, but only $66 \%$ in our setting. To perform well in this task, the agents must learn to cooperate by positioning properly and focussing their fire, while remembering which enemy and ally units are alive or out of view.</p>
<p>All agents receive the same global reward at each time step, equal to the sum of damage inflicted on the opponent units minus half the damage taken. Killing an opponent generates a reward of 10 points, and winning the game generates a reward equal to the team's remaining total health plus 200. This damage-based reward signal is comparable to that used by Usunier et al. (2016). Unlike (Peng et al. 2017), our approach does not require estimating local rewards.</p>
<p>State Features. The actor and critic receive different input features, corresponding to local observations and global state, respectively. Both include features for allies and enemies. Units can be either allies or enemies, while agents are the decentralised controllers that command ally units.</p>
<p>The local observations for every agent are drawn only from a circular subset of the map centred on the unit it
controls and include for each unit within this field of view: distance, relative $x$, relative $y$, unit type and shield. ${ }^{2}$ All features are normalised by their maximum values. We do not include any information about the units' current target.</p>
<p>The global state representation consists of similar features, but for all units on the map regardless of fields of view. Absolute distance is not included, and $x-y$ locations are given relative to the centre of the map rather than to a particular agent. The global state also includes health points and cooldown for all agents. The representation fed to the centralised $Q$-function critic is the concatenation of the global state representation with the local observation of the agent whose actions are being evaluated. Our centralised critic that estimates $V(s)$, and is therefore agent-agnostic, receives the global state concatenated with all agents' observations. The observations contain no new information but include the egocentric distances relative to that agent.</p>
<p>Architecture \&amp; Training. The actor consists of 128-bit gated recurrent units (GRUs) (Cho et al. 2014) that use fully connected layers both to process the input and to produce the output values from the hidden state, $h_{t}^{a}$. The IAC critics use extra output heads appended to the last layer of the actor network. Action probabilities are produced from the final layer, $\mathbf{z}$, via a bounded softmax distribution that lower-bounds the probability of any given action by $\epsilon /|U|$ : $P(u)=(1-\epsilon) \operatorname{softmax}(\mathbf{z})_{u}+\epsilon /|U|)$. We anneal $\epsilon$ linearly from 0.5 to 0.02 across 750 training episodes. The centralised critic is a feedforward network with multiple ReLU layers combined with fully connected layers. Hyperparameters were coarsely tuned on the 5 m scenario and then used for all other maps. We found that the most sensitive parameter was $\operatorname{TD}(\lambda)$, but settled on $\lambda=0.8$, which worked best for both COMA and our baselines. Our implementation uses TorchCraft (Synnaeve et al. 2016) and Torch 7 (Collobert, Kavukcuoglu, and Farabet 2011). Pseudocode and further details on the training procedure are in the supplementary material.</p>
<p>We experimented with critic architectures that are factored at the agent level and further exploit internal parameter sharing. However, we found that the bottleneck for scalability was not the centralisation of the critic, but rather the difficulty of multi-agent exploration. Hence, we defer further investigation of factored COMA critics to future work.</p>
<p>Ablations. We perform ablation experiments to validate three key elements of COMA. First, we test the importance of centralising the critic by comparing against two IAC variants, IAC- $Q$ and IAC- $V$. These critics take the same decentralised input as the actor, and share parameters with the actor network up to the final layer. IAC- $Q$ then outputs $|U|$ $Q$-values, one for each action, while IAC- $V$ outputs a single state-value. Note that we still share parameters between agents, using the egocentric observations and ID's as part of</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Win rates for COMA and competing algorithms on four different scenarios. COMA outperforms all baseline methods. Centralised critics also clearly outperform their decentralised counterparts. The legend at the top applies across all plots.
the input to allow different behaviours to emerge. The cooperative reward function is still shared by all agents.</p>
<p>Second, we test the significance of learning $Q$ instead of $V$. The method central- $V$ still uses a central state for the critic, but learns $V(s)$, and uses the TD error to estimate the advantage for policy gradient updates.</p>
<p>Third, we test the utility of our counterfactual baseline. The method central- $Q V$ learns both $Q$ and $V$ simultaneously and estimates the advantage as $Q-V$, replacing COMA's counterfactual baseline with $V$. All methods use the same architecture and training scheme for the actors, and all critics are trained with TD $(\lambda)$.</p>
<h2>6 Results</h2>
<p>Figure 3 shows average win rates as a function of episode for each method and each StarCraft scenario. For each method, we conducted 35 independent trials and froze learning every 100 training episodes to evaluate the learned policies across 200 episodes per method, plotting the average across episodes and trials. Also shown is one standard deviation in performance.</p>
<p>The results show that COMA is superior to the IAC baselines in all scenarios. Interestingly, the IAC methods also eventually learn reasonable policies in 5 m , although they need substantially more episodes to do so. This may seem counterintuitive since in the IAC methods, the actor and critic networks share parameters in their early layers (see Section 5), which could be expected to speed learning. However, these results suggest that the improved accuracy of pol-
icy evaluation made possible by conditioning on the global state outweighs the overhead of training a separate network.</p>
<p>Furthermore, COMA strictly dominates central- $Q V$, both in training speed and in final performance across all settings. This is a strong indicator that our counterfactual baseline is crucial when using a central $Q$-critic to train decentralised policies.</p>
<p>Learning a state-value function has the obvious advantage of not conditioning on the joint action. Still, we find that COMA outperforms the central- $V$ baseline in final performance. Furthermore, COMA typically achieves good policies faster, which is expected as COMA provides a shaped training signal. Training is also more stable than central- $V$, which is a consequence of the COMA gradient tending to zero as the policy becomes greedy. Overall, COMA is the best performing and most consistent method.</p>
<p>Usunier et al. (2016) report the performance of their best agents trained with their state-of-the-art centralised controller labelled GMEZO (greedy-MDP with episodic zeroorder optimisation), and for a centralised DQN controller, both given a full field of view and access to attack-move macro-actions. These results are compared in Table 1 against the best agents trained with COMA for each map. Clearly, in most settings these agents achieve performance comparable to the best published win rates despite being restricted to decentralised policies and local fields of view.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Local Field of View (FoV)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Full FoV, Central Control</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">map</td>
<td style="text-align: center;">heur.</td>
<td style="text-align: center;">IAC- $V$</td>
<td style="text-align: center;">IAC- $Q$</td>
<td style="text-align: center;">cnt- $V$</td>
<td style="text-align: center;">cnt- $Q V$</td>
<td style="text-align: center;">COMA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">heur.</td>
<td style="text-align: center;">DQN</td>
<td style="text-align: center;">GMEZO</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">mean</td>
<td style="text-align: center;">best</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">3m</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">47 (3)</td>
<td style="text-align: center;">56 (6)</td>
<td style="text-align: center;">83 (3)</td>
<td style="text-align: center;">83 (5)</td>
<td style="text-align: center;">$\mathbf{8 7 ( 3 )}$</td>
<td style="text-align: center;">98</td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">5 m</td>
<td style="text-align: center;">66</td>
<td style="text-align: center;">63 (2)</td>
<td style="text-align: center;">58 (3)</td>
<td style="text-align: center;">67 (5)</td>
<td style="text-align: center;">71 (9)</td>
<td style="text-align: center;">$\mathbf{8 1 ( 5 )}$</td>
<td style="text-align: center;">95</td>
<td style="text-align: center;">98</td>
<td style="text-align: center;">99</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: left;">5 w</td>
<td style="text-align: center;">70</td>
<td style="text-align: center;">18 (5)</td>
<td style="text-align: center;">57 (5)</td>
<td style="text-align: center;">65 (3)</td>
<td style="text-align: center;">76 (1)</td>
<td style="text-align: center;">$\mathbf{8 2 ( 3 )}$</td>
<td style="text-align: center;">98</td>
<td style="text-align: center;">82</td>
<td style="text-align: center;">70</td>
<td style="text-align: center;">$74^{3}$</td>
</tr>
<tr>
<td style="text-align: left;">2d_3z</td>
<td style="text-align: center;">$\mathbf{6 3}$</td>
<td style="text-align: center;">27 (9)</td>
<td style="text-align: center;">19 (21)</td>
<td style="text-align: center;">36 (6)</td>
<td style="text-align: center;">39 (5)</td>
<td style="text-align: center;">47 (5)</td>
<td style="text-align: center;">65</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">61</td>
<td style="text-align: center;">90</td>
</tr>
</tbody>
</table>
<p>Table 1: Mean win percentage averaged across final 1000 evaluation episodes for the different maps, for all methods and the hand-coded heuristic in the decentralised setting with a limited field of view. The highest mean performances are in bold, while the values in parentheses denote the $95 \%$ confidence interval, for example $87(3)=87 \pm 3$. Also shown, maximum win percentages for COMA (decentralised), in comparison to the heuristic and published results (evaluated in the centralised setting).</p>
<h2>7 Conclusions \&amp; Future Work</h2>
<p>This paper presented COMA policy gradients, a method that uses a centralised critic in order to estimate a counterfactual advantage for decentralised policies in mutliagent RL. COMA addresses the challenges of multi-agent credit assignment by using a counterfactual baseline that marginalises out a single agent's action, while keeping the other agents' actions fixed. Our results in a decentralised StarCraft unit micromanagement benchmark show that COMA significantly improves final performance and training speed over other multi-agent actor-critic methods and remains competitive with state-of-the-art centralised controllers under best-performance reporting. Future work will extend COMA to tackle scenarios with large numbers of agents, where centralised critics are more difficult to train and exploration is harder to coordinate. We also aim to develop more sample-efficient variants that are practical for real-world applications such as self-driving cars.</p>
<h2>Acknowledgements</h2>
<p>This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement number 637713). It was also supported by the OxfordGoogle DeepMind Graduate Scholarship, the UK EPSRC CDT in Autonomous Intelligent Machines and Systems, and a generous grant from Microsoft for their Azure cloud computing services. We would like to thank Nando de Freitas, Yannis Assael, and Brendan Shillingford for helpful comments and discussion. We also thank Gabriel Synnaeve, Zeming Lin, and the rest of the TorchCraft team at FAIR for their work on the interface.</p>
<h2>References</h2>
<p>Busoniu, L.; Babuska, R.; and De Schutter, B. 2008. A comprehensive survey of multiagent reinforcement learning. IEEE Transactions on Systems Man and Cybernetics Part C Applications and Reviews 38(2):156.
Cao, Y.; Yu, W.; Ren, W.; and Chen, G. 2013. An overview of recent progress in the study of distributed multi-agent coordination. IEEE Transactions on Industrial informatics $9(1): 427-438$.</p>
<p>Chang, Y.-H.; Ho, T.; and Kaelbling, L. P. 2003. All learning is local: Multi-agent learning in global reward games. In NIPS, 807-814.
Cho, K.; van Merriënboer, B.; Bahdanau, D.; and Bengio, Y. 2014. On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259.
Colby, M. K.; Curran, W.; and Tumer, K. 2015. Approximating difference evaluations with local information. In Proceedings of the 2015 International Conference on $A u$ tonomous Agents and Multiagent Systems, 1659-1660. International Foundation for Autonomous Agents and Multiagent Systems.
Collobert, R.; Kavukcuoglu, K.; and Farabet, C. 2011. Torch7: A matlab-like environment for machine learning. In BigLearn, NIPS Workshop.
Das, A.; Kottur, S.; Moura, J. M.; Lee, S.; and Batra, D. 2017. Learning cooperative visual dialog agents with deep reinforcement learning. arXiv preprint arXiv:1703.06585.
Foerster, J.; Assael, Y. M.; de Freitas, N.; and Whiteson, S. 2016. Learning to communicate with deep multi-agent reinforcement learning. In Advances in Neural Information Processing Systems, 2137-2145.
Foerster, J.; Nardelli, N.; Farquhar, G.; Torr, P.; Kohli, P.; Whiteson, S.; et al. 2017. Stabilising experience replay for deep multi-agent reinforcement learning. In Proceedings of The 34th International Conference on Machine Learning.
Gupta, J. K.; Egorov, M.; and Kochenderfer, M. 2017. Cooperative multi-agent control using deep reinforcement learning.
Hausknecht, M., and Stone, P. 2015. Deep recurrent q-learning for partially observable mdps. arXiv preprint arXiv:1507.06527.
Hochreiter, S., and Schmidhuber, J. 1997. Long short-term memory. Neural computation 9(8):1735-1780.
Jorge, E.; Kågebäck, M.; and Gustavsson, E. 2016. Learning to play guess who? and inventing a grounded language as a consequence. arXiv preprint arXiv:1611.03218.
Konda, V. R., and Tsitsiklis, J. N. 2000. Actor-critic algorithms. In Advances in neural information processing systems, 1008-1014.</p>
<p>Kraemer, L., and Banerjee, B. 2016. Multi-agent reinforcement learning as a rehearsal for decentralized planning. Neurocomputing 190:82-94.
Lazaridou, A.; Peysakhovich, A.; and Baroni, M. 2016. Multi-agent cooperation and the emergence of (natural) language. arXiv preprint arXiv:1612.07182.
Leibo, J. Z.; Zambaldi, V.; Lanctot, M.; Marecki, J.; and Graepel, T. 2017. Multi-agent reinforcement learning in sequential social dilemmas. arXiv preprint arXiv:1702.03037.
Lowe, R.; Wu, Y.; Tamar, A.; Harb, J.; Abbeel, P.; and Mordatch, I. 2017. Multi-agent actor-critic for mixed cooperative-competitive environments. arXiv preprint arXiv:1706.02275.
Mnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Veness, J.; Bellemare, M. G.; Graves, A.; Riedmiller, M.; Fidjeland, A. K.; Ostrovski, G.; et al. 2015. Humanlevel control through deep reinforcement learning. Nature 518(7540):529-533.
Mordatch, I., and Abbeel, P. 2017. Emergence of grounded compositional language in multi-agent populations. arXiv preprint arXiv:1703.04908.
Oliehoek, F. A.; Spaan, M. T. J.; and Vlassis, N. 2008. Optimal and approximate Q-value functions for decentralized POMDPs. 32:289-353.
Omidshafiei, S.; Pazis, J.; Amato, C.; How, J. P.; and Vian, J. 2017. Deep decentralized multi-task multi-agent rl under partial observability. arXiv preprint arXiv:1703.06182.
Peng, P.; Yuan, Q.; Wen, Y.; Yang, Y.; Tang, Z.; Long, H.; and Wang, J. 2017. Multiagent bidirectionally-coordinated nets for learning to play starcraft combat games. arXiv preprint arXiv:1703.10069.
Proper, S., and Tumer, K. 2012. Modeling difference rewards for multiagent learning. In Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems-Volume 3, 1397-1398. International Foundation for Autonomous Agents and Multiagent Systems.
Schulman, J.; Moritz, P.; Levine, S.; Jordan, M. I.; and Abbeel, P. 2015. High-dimensional continuous control using generalized advantage estimation. CoRR abs/1506.02438.
Shoham, Y., and Leyton-Brown, K. 2009. Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations. New York: Cambridge University Press.
Sukhbaatar, S.; Fergus, R.; et al. 2016. Learning multiagent communication with backpropagation. In Advances in Neural Information Processing Systems, 2244-2252.
Sutton, R. S.; McAllester, D. A.; Singh, S. P.; Mansour, Y.; et al. 1999. Policy gradient methods for reinforcement learning with function approximation. In NIPS, volume 99, $1057-1063$.
Sutton, R. S. 1988. Learning to predict by the methods of temporal differences. Machine learning 3(1):9-44.
Synnaeve, G.; Nardelli, N.; Auvolat, A.; Chintala, S.; Lacroix, T.; Lin, Z.; Richoux, F.; and Usunier, N. 2016. Torchcraft: a library for machine learning research on realtime strategy games. arXiv preprint arXiv:1611.00625.</p>
<p>Tampuu, A.; Matiisen, T.; Kodelja, D.; Kuzovkin, I.; Korjus, K.; Aru, J.; Aru, J.; and Vicente, R. 2015. Multiagent cooperation and competition with deep reinforcement learning. arXiv preprint arXiv:1511.08779.
Tan, M. 1993. Multi-agent reinforcement learning: Independent vs. cooperative agents. In Proceedings of the tenth international conference on machine learning, 330-337.
Tumer, K., and Agogino, A. 2007. Distributed agent-based air traffic flow management. In Proceedings of the 6th international joint conference on Autonomous agents and multiagent systems, 255. ACM.
Usunier, N.; Synnaeve, G.; Lin, Z.; and Chintala, S. 2016. Episodic exploration for deep deterministic policies: An application to starcraft micromanagement tasks. arXiv preprint arXiv:1609.02993.
Weaver, L., and Tao, N. 2001. The optimal reward baseline for gradient-based reinforcement learning. In Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence, 538-545. Morgan Kaufmann Publishers Inc.
Weyns, D.; Helleboogh, A.; and Holvoet, T. 2005. The packet-world: A test bed for investigating situated multiagent systems. In Software Agent-Based Applications, Platforms and Development Kits. Springer. 383-408.
Williams, R. J. 1992. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning 8(3-4):229-256.
Wolpert, D. H., and Tumer, K. 2002. Optimal payoff functions for members of collectives. In Modeling complexity in economic and social systems. World Scientific. 355-369.
Yang, E., and Gu, D. 2004. Multiagent reinforcement learning for multi-robot systems: A survey. Technical report, tech. rep.
Ye, D.; Zhang, M.; and Yang, Y. 2015. A multi-agent framework for packet routing in wireless sensor networks. sensors 15(5):10026-10047.
Ying, W., and Dayong, S. 2005. Multi-agent framework for third party logistics in e-commerce. Expert Systems with Applications 29(2):431-436.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5} 5 \mathrm{w}$ DQN and GMEZO benchmark performances are of a policy trained on a larger map and tested on 5 w&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
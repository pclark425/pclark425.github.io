<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3624 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3624</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3624</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-88.html">extraction-schema-88</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-270045904</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.15185v1.pdf" target="_blank">An Evaluation of Estimative Uncertainty in Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Words of estimative probability (WEPs), such as ''maybe'' or ''probably not'' are ubiquitous in natural language for communicating estimative uncertainty, compared with direct statements involving numerical probability. Human estimative uncertainty, and its calibration with numerical estimates, has long been an area of study -- including by intelligence agencies like the CIA. This study compares estimative uncertainty in commonly used large language models (LLMs) like GPT-4 and ERNIE-4 to that of humans, and to each other. Here we show that LLMs like GPT-3.5 and GPT-4 align with human estimates for some, but not all, WEPs presented in English. Divergence is also observed when the LLM is presented with gendered roles and Chinese contexts. Further study shows that an advanced LLM like GPT-4 can consistently map between statistical and estimative uncertainty, but a significant performance gap remains. The results contribute to a growing body of research on human-LLM alignment.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3624.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3624.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary large autoregressive transformer language model from OpenAI used in the paper to map verbal probability expressions (WEPs) to numerical probabilities and to compare estimative uncertainty to human survey distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An Evaluation of Estimative Uncertainty in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary OpenAI transformer-based LLM (GPT family). Paper treats it as a high-capability conversational model pre-trained on large English-dominant corpora; exact parameter count not specified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Two tasks: (1) map 17 Words of Estimative Probability (WEPs) in short/extended/gendered narrative contexts to a numerical probability (0–1 float) and compare distributions to human survey results; (2) (experiment primarily on GPT-4) not central for GPT-3.5 in RQ2, but GPT-3.5 was evaluated for WEP-to-number mapping across English and Chinese prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Direct prompting with an instruction to output a float between 0 and 1 (temperature = 0); prompts included instantiated context templates containing the target WEP. Also tested in English and Chinese; no fine-tuning or calibration techniques applied.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Dataset constructed for RQ1: 17 WEPs × multiple context templates (Concise Narrative, Extended Narrative, Male-Centric, Female-Centric) resulting in 776 English prompts; Chinese translations of Concise Narrative Contexts also used to test cross-lingual effects. Human reference distribution from Fagen-Ulmschneider survey (n=123 undergraduates).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Compared distributions using Kullback-Leibler (KL) divergence and Mann-Whitney U test (median differences, AMD). Report: GPT-3.5 diverged from human distributions for 11 of 17 WEPs (Mann-Whitney U significant). Example: for 'probable' AMD = 5% (U(N1=123,N2=15)=506.5, p<0.01), RBC=0.45 (95% CI 0.3–0.58). KL divergence reported per-WEP in heatmap.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Baseline = human survey distributions. GPT-3.5 generally aligned with humans on extreme/high-positive WEPs (e.g., 'almost certain', 'highly likely' had AMD=0), and exhibited lower overall divergence (lower KL) than GPT-4 in many analyses despite GPT-4 being a stronger NLU model. Compared also qualitatively to ERNIE-4 and Llama-series which showed larger or different divergences.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Observed divergence across many mid/low-probability WEPs; sensitivity to language (English vs Chinese) exists but was smaller for GPT-3.5 than for some other models; experiments used a student human sample as reference and templated prompts rather than real-world dialogues; no calibration/fine-tuning applied.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>GPT-3.5's numerical mappings for many WEPs were closer to the human survey medians than GPT-4's mappings; GPT-3.5 aligned well with humans on high-certainty WEPs and showed smaller KL divergence overall in many contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Evaluation of Estimative Uncertainty in Large Language Models', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3624.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3624.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art proprietary OpenAI transformer LLM used extensively in the paper to (a) map WEPs to numerical probabilities across contexts and languages, and (b) evaluate consistency when converting statistical (sample-based) uncertainty into estimative WEP choices under controlled scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An Evaluation of Estimative Uncertainty in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (gpt-4-0613 used via API)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary advanced OpenAI transformer LLM (GPT-4 family). Described in the paper as an advanced LLM with strong natural language understanding; exact architecture and parameter count are not specified in the paper but treated as the top-performing model used for RQ2.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>(1) Map 17 WEPs in different narrative contexts (concise/extended/gendered) and languages (English/Chinese) to numerical probabilities and compare to human survey distributions; (2) Given samples from a distribution and an interval (e.g., 'below 99'), choose the best WEP from a set that corresponds to the empirical probability that a new draw falls in that interval (360 prompts across Height/Score/Sound scenarios).</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Direct prompting to output a float between 0 and 1 for RQ1 (temperature = 0). For RQ2, two prompting modes were used: standard prompting ('choose the best WEP') and zero-shot Chain-of-Thought (CoT) prompting where the model was asked to 'compute the associated probability' then 'complete the sentence' and give 'I choose:' final answer. No fine-tuning or calibration beyond prompt design; responses recorded via API.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>RQ1 dataset: same 776 prompts derived from 17 WEPs × context templates (English and Chinese versions for CNC) compared to human survey (Fagen-Ulmschneider). RQ2 dataset: 360 fully instantiated prompts across 3 scenarios (Height, Score, Sound) × 2 CHOICES sets (5-choice and 3-choice) × 2 NUMBERS distributions (narrow/wide) × 5 confidence points × 6 interval types; ground truth derived from empirical proportion ranges mapped to WEP bins.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>RQ1: KL divergence and Mann-Whitney U for distribution comparisons; reported GPT-4 diverged from humans for 12 of 17 WEPs. Example: for 'probable' AMD = 10% (U(N1=123,N2=15)=417.5, p<0.01), RBC=0.49 (95% CI 0.34–0.61). RQ2: four custom consistency metrics — pair-wise consistency, monotonicity consistency, empirical consistency, empirical monotonicity consistency (scores 0–100; random baselines computed by simulation). GPT-4 scored significantly above random on all four metrics; monotonicity consistency was near-perfect (but noted as an artefact), empirical consistency improved significantly with CoT prompting (statistical tests: e.g., Height scenario t(59) = −4.15, p<0.01, Cohen's d = −0.358). Exact aggregate metric scores are shown in paper figures but not fully enumerated in text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Baselines: human survey distributions for RQ1; random-choice baseline (averaged over 10 random reps) for RQ2. GPT-4 outperformed random on all RQ2 consistency metrics; nonetheless it did not reach perfect alignment and showed systematic gaps relative to human distributions (diverged on many WEPs). GPT-3.5 was often closer to human WEP distributions than GPT-4. ERNIE-4 and Llama-series also compared and showed language- and model-specific differences.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>For RQ1, GPT-4 diverged from humans on most mid/low WEPs and showed sensitivity to gendered contexts (distributions collapsed in some gender-specific prompts). For RQ2, monotonicity metric could be gamed by consistent but uncalibrated choices (illusion of high performance); CoT prompting did not uniformly improve performance except for empirical consistency; datasets were synthetic and derived from controlled templates rather than real-world scientific forecasting tasks; human reference set limited to undergraduates.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>GPT-4 can map between statistical uncertainty and WEPs better-than-random across multiple consistency metrics, and CoT prompting produces significant improvements in empirical consistency for some scenarios. However, a substantial performance gap remains: GPT-4 diverges from human WEP distributions for many expressions (12/17 WEPs), shows limited nuanced calibration (monotonicity artefact), and CoT yields limited additional gains compared to other NLP tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Evaluation of Estimative Uncertainty in Large Language Models', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3624.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3624.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ERNIE-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Baidu ERNIE-4.0</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based Chinese-pretrained LLM (ERNIE-4.0) evaluated in the paper for mapping Chinese WEP prompts to numerical probabilities and compared to human survey responses and to GPT-family models prompted in Chinese.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An Evaluation of Estimative Uncertainty in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ERNIE-4.0</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Baidu's ERNIE-4.0, described in the paper as an LLM pre-trained primarily on Chinese corpora. Specific architecture or parameter counts are not detailed in the paper; treated as a Chinese-dominant model for cross-lingual comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Map the same 17 WEPs (translated into Chinese) in concise narrative contexts to numerical probabilities; compare resulting distributions to human survey and to GPT-3.5/GPT-4 responses to Chinese prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Direct prompting in Chinese with instruction to output a float between 0 and 1 (two-decimal float) and to avoid chain-of-thought; temperature = 0; no calibration or fine-tuning reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Chinese translations of Concise Narrative Context templates instantiated with 17 WEPs (subset of RQ1 dataset), compared to the same human survey baseline (English human survey used as reference) and to GPT-3.5/GPT-4 prompted in Chinese.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>KL divergence per-WEP and Mann-Whitney U test for median differences. Observed low KL divergence for most WEPs (16/17 low), with a large exception: 'we doubt' showed AMD = 40 and Mann-Whitney U significant (U(N1=15, N2=123) = 1778, p<0.01). However, 12 WEPs exhibited statistically significant Mann-Whitney U differences despite low KL for many WEPs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Compared to human survey distributions and to GPT models prompted in Chinese. ERNIE-4's overall information-content divergence (KL) was low for many WEPs but median differences existed for many terms; ERNIE-4 differed significantly from GPT-3.5 on several WEPs when both prompted in Chinese, indicating model-training-corpus effects beyond simple language ability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Differences may reflect pre-training corpus differences and cultural/linguistic calibrations; human reference distribution is English-survey based and may not perfectly align cross-linguistically; only Chinese prompts evaluated for ERNIE-4 in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>ERNIE-4 mostly matched humans in KL terms for many WEPs but had large median differences for specific expressions (e.g., 'we doubt'). Overall, cross-model and cross-language comparisons show that pre-training language and data sources affect estimative uncertainty mappings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Evaluation of Estimative Uncertainty in Large Language Models', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3624.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3624.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-2 7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source Llama-series model with ~7 billion parameters evaluated in the paper for mapping WEPs to numerical probabilities; used for comparison to proprietary GPT models and to examine model-size effects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An Evaluation of Estimative Uncertainty in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source Llama-series transformer LLM with approximately 7B parameters (paper cites Llama-2-7B), pre-trained on large corpora (English-dominant); used without additional fine-tuning in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Map 17 WEPs in English concise/extended/gendered contexts to numerical probabilities and compare distributions to human survey and to other LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Direct prompting in English requesting a float between 0 and 1 (temperature = 0). No CoT or calibration reported for Llama models in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Same RQ1 English dataset of 776 prompts across context templates; human survey baseline from Fagen-Ulmschneider.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>KL divergence and Mann-Whitney U comparisons to human distributions. The Llama-2-7B estimates were broadly consistent with GPT-family estimates but typically exhibited larger divergence from humans than GPT-3.5/GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Compared to human distributions and to GPT models; Llama estimates were often farther from human medians and had larger divergence than the GPT models, consistent with smaller model capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Smaller model size likely reduces alignment with human WEP mappings; experiments used zero system messages and temperature 0 which may limit output diversity; no model-specific calibration applied.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>Llama-2-7B can perform the mapping task but shows larger divergence from human estimations than the GPT models, supporting a relationship between model capacity and closeness to human-like estimative uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Evaluation of Estimative Uncertainty in Large Language Models', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3624.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3624.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-2 13B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source Llama-series model with ~13 billion parameters used in the paper for comparison in WEP-to-probability mapping experiments, showing behavior intermediate between smaller Llama and larger proprietary GPT models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An Evaluation of Estimative Uncertainty in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source Llama transformer LLM with approximately 13B parameters (cited in paper), pre-trained on large corpora (English-dominant); used without fine-tuning in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Map 17 WEPs across narrative contexts in English to numerical probability outputs and compare distributions to humans and other LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Direct prompting to supply a float between 0 and 1 (temperature = 0); no Chain-of-Thought or fine-tuning used for Llama models.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Same RQ1 English dataset of templated prompts (776 prompts) and human survey baseline from Fagen-Ulmschneider.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>KL divergence and Mann-Whitney U test comparisons; Llama-2-13B estimates were largely consistent with GPT-model trends but often exhibited larger divergence from human distributions than GPT-3.5 and sometimes GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Compared to human distributions and GPT-family models. Llama-2-13B performed better than the 7B variant in some respects but still showed greater divergence than the GPT models, consistent with capacity differences.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No in-paper calibration; relative underperformance compared to GPT models may reflect pre-training data differences and absence of deployed instruction tuning or alignment steps present in commercial models.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>Llama-2-13B demonstrates the capability to produce numerical probability mappings for WEPs but with larger deviations from human judgments than GPT-3.5, suggesting model scale and training regimen impact estimative uncertainty alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Evaluation of Estimative Uncertainty in Large Language Models', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Words of Estimative Probability <em>(Rating: 2)</em></li>
                <li>Perception of probability words <em>(Rating: 2)</em></li>
                <li>Probing neural language models for understanding of words of estimative probability <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
                <li>Using cognitive psychology to understand gpt-3 <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3624",
    "paper_id": "paper-270045904",
    "extraction_schema_id": "extraction-schema-88",
    "extracted_data": [
        {
            "name_short": "GPT-3.5",
            "name_full": "OpenAI GPT-3.5-turbo",
            "brief_description": "A proprietary large autoregressive transformer language model from OpenAI used in the paper to map verbal probability expressions (WEPs) to numerical probabilities and to compare estimative uncertainty to human survey distributions.",
            "citation_title": "An Evaluation of Estimative Uncertainty in Large Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "Proprietary OpenAI transformer-based LLM (GPT family). Paper treats it as a high-capability conversational model pre-trained on large English-dominant corpora; exact parameter count not specified in the paper.",
            "prediction_task": "Two tasks: (1) map 17 Words of Estimative Probability (WEPs) in short/extended/gendered narrative contexts to a numerical probability (0–1 float) and compare distributions to human survey results; (2) (experiment primarily on GPT-4) not central for GPT-3.5 in RQ2, but GPT-3.5 was evaluated for WEP-to-number mapping across English and Chinese prompts.",
            "method_of_probability_estimation": "Direct prompting with an instruction to output a float between 0 and 1 (temperature = 0); prompts included instantiated context templates containing the target WEP. Also tested in English and Chinese; no fine-tuning or calibration techniques applied.",
            "dataset_or_benchmark": "Dataset constructed for RQ1: 17 WEPs × multiple context templates (Concise Narrative, Extended Narrative, Male-Centric, Female-Centric) resulting in 776 English prompts; Chinese translations of Concise Narrative Contexts also used to test cross-lingual effects. Human reference distribution from Fagen-Ulmschneider survey (n=123 undergraduates).",
            "performance_metrics": "Compared distributions using Kullback-Leibler (KL) divergence and Mann-Whitney U test (median differences, AMD). Report: GPT-3.5 diverged from human distributions for 11 of 17 WEPs (Mann-Whitney U significant). Example: for 'probable' AMD = 5% (U(N1=123,N2=15)=506.5, p&lt;0.01), RBC=0.45 (95% CI 0.3–0.58). KL divergence reported per-WEP in heatmap.",
            "comparison_to_baselines": "Baseline = human survey distributions. GPT-3.5 generally aligned with humans on extreme/high-positive WEPs (e.g., 'almost certain', 'highly likely' had AMD=0), and exhibited lower overall divergence (lower KL) than GPT-4 in many analyses despite GPT-4 being a stronger NLU model. Compared also qualitatively to ERNIE-4 and Llama-series which showed larger or different divergences.",
            "limitations_or_challenges": "Observed divergence across many mid/low-probability WEPs; sensitivity to language (English vs Chinese) exists but was smaller for GPT-3.5 than for some other models; experiments used a student human sample as reference and templated prompts rather than real-world dialogues; no calibration/fine-tuning applied.",
            "notable_findings": "GPT-3.5's numerical mappings for many WEPs were closer to the human survey medians than GPT-4's mappings; GPT-3.5 aligned well with humans on high-certainty WEPs and showed smaller KL divergence overall in many contexts.",
            "uuid": "e3624.0",
            "source_info": {
                "paper_title": "An Evaluation of Estimative Uncertainty in Large Language Models",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "OpenAI GPT-4",
            "brief_description": "A state-of-the-art proprietary OpenAI transformer LLM used extensively in the paper to (a) map WEPs to numerical probabilities across contexts and languages, and (b) evaluate consistency when converting statistical (sample-based) uncertainty into estimative WEP choices under controlled scenarios.",
            "citation_title": "An Evaluation of Estimative Uncertainty in Large Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-4 (gpt-4-0613 used via API)",
            "model_description": "Proprietary advanced OpenAI transformer LLM (GPT-4 family). Described in the paper as an advanced LLM with strong natural language understanding; exact architecture and parameter count are not specified in the paper but treated as the top-performing model used for RQ2.",
            "prediction_task": "(1) Map 17 WEPs in different narrative contexts (concise/extended/gendered) and languages (English/Chinese) to numerical probabilities and compare to human survey distributions; (2) Given samples from a distribution and an interval (e.g., 'below 99'), choose the best WEP from a set that corresponds to the empirical probability that a new draw falls in that interval (360 prompts across Height/Score/Sound scenarios).",
            "method_of_probability_estimation": "Direct prompting to output a float between 0 and 1 for RQ1 (temperature = 0). For RQ2, two prompting modes were used: standard prompting ('choose the best WEP') and zero-shot Chain-of-Thought (CoT) prompting where the model was asked to 'compute the associated probability' then 'complete the sentence' and give 'I choose:' final answer. No fine-tuning or calibration beyond prompt design; responses recorded via API.",
            "dataset_or_benchmark": "RQ1 dataset: same 776 prompts derived from 17 WEPs × context templates (English and Chinese versions for CNC) compared to human survey (Fagen-Ulmschneider). RQ2 dataset: 360 fully instantiated prompts across 3 scenarios (Height, Score, Sound) × 2 CHOICES sets (5-choice and 3-choice) × 2 NUMBERS distributions (narrow/wide) × 5 confidence points × 6 interval types; ground truth derived from empirical proportion ranges mapped to WEP bins.",
            "performance_metrics": "RQ1: KL divergence and Mann-Whitney U for distribution comparisons; reported GPT-4 diverged from humans for 12 of 17 WEPs. Example: for 'probable' AMD = 10% (U(N1=123,N2=15)=417.5, p&lt;0.01), RBC=0.49 (95% CI 0.34–0.61). RQ2: four custom consistency metrics — pair-wise consistency, monotonicity consistency, empirical consistency, empirical monotonicity consistency (scores 0–100; random baselines computed by simulation). GPT-4 scored significantly above random on all four metrics; monotonicity consistency was near-perfect (but noted as an artefact), empirical consistency improved significantly with CoT prompting (statistical tests: e.g., Height scenario t(59) = −4.15, p&lt;0.01, Cohen's d = −0.358). Exact aggregate metric scores are shown in paper figures but not fully enumerated in text.",
            "comparison_to_baselines": "Baselines: human survey distributions for RQ1; random-choice baseline (averaged over 10 random reps) for RQ2. GPT-4 outperformed random on all RQ2 consistency metrics; nonetheless it did not reach perfect alignment and showed systematic gaps relative to human distributions (diverged on many WEPs). GPT-3.5 was often closer to human WEP distributions than GPT-4. ERNIE-4 and Llama-series also compared and showed language- and model-specific differences.",
            "limitations_or_challenges": "For RQ1, GPT-4 diverged from humans on most mid/low WEPs and showed sensitivity to gendered contexts (distributions collapsed in some gender-specific prompts). For RQ2, monotonicity metric could be gamed by consistent but uncalibrated choices (illusion of high performance); CoT prompting did not uniformly improve performance except for empirical consistency; datasets were synthetic and derived from controlled templates rather than real-world scientific forecasting tasks; human reference set limited to undergraduates.",
            "notable_findings": "GPT-4 can map between statistical uncertainty and WEPs better-than-random across multiple consistency metrics, and CoT prompting produces significant improvements in empirical consistency for some scenarios. However, a substantial performance gap remains: GPT-4 diverges from human WEP distributions for many expressions (12/17 WEPs), shows limited nuanced calibration (monotonicity artefact), and CoT yields limited additional gains compared to other NLP tasks.",
            "uuid": "e3624.1",
            "source_info": {
                "paper_title": "An Evaluation of Estimative Uncertainty in Large Language Models",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "ERNIE-4",
            "name_full": "Baidu ERNIE-4.0",
            "brief_description": "A transformer-based Chinese-pretrained LLM (ERNIE-4.0) evaluated in the paper for mapping Chinese WEP prompts to numerical probabilities and compared to human survey responses and to GPT-family models prompted in Chinese.",
            "citation_title": "An Evaluation of Estimative Uncertainty in Large Language Models",
            "mention_or_use": "use",
            "model_name": "ERNIE-4.0",
            "model_description": "Baidu's ERNIE-4.0, described in the paper as an LLM pre-trained primarily on Chinese corpora. Specific architecture or parameter counts are not detailed in the paper; treated as a Chinese-dominant model for cross-lingual comparisons.",
            "prediction_task": "Map the same 17 WEPs (translated into Chinese) in concise narrative contexts to numerical probabilities; compare resulting distributions to human survey and to GPT-3.5/GPT-4 responses to Chinese prompts.",
            "method_of_probability_estimation": "Direct prompting in Chinese with instruction to output a float between 0 and 1 (two-decimal float) and to avoid chain-of-thought; temperature = 0; no calibration or fine-tuning reported.",
            "dataset_or_benchmark": "Chinese translations of Concise Narrative Context templates instantiated with 17 WEPs (subset of RQ1 dataset), compared to the same human survey baseline (English human survey used as reference) and to GPT-3.5/GPT-4 prompted in Chinese.",
            "performance_metrics": "KL divergence per-WEP and Mann-Whitney U test for median differences. Observed low KL divergence for most WEPs (16/17 low), with a large exception: 'we doubt' showed AMD = 40 and Mann-Whitney U significant (U(N1=15, N2=123) = 1778, p&lt;0.01). However, 12 WEPs exhibited statistically significant Mann-Whitney U differences despite low KL for many WEPs.",
            "comparison_to_baselines": "Compared to human survey distributions and to GPT models prompted in Chinese. ERNIE-4's overall information-content divergence (KL) was low for many WEPs but median differences existed for many terms; ERNIE-4 differed significantly from GPT-3.5 on several WEPs when both prompted in Chinese, indicating model-training-corpus effects beyond simple language ability.",
            "limitations_or_challenges": "Differences may reflect pre-training corpus differences and cultural/linguistic calibrations; human reference distribution is English-survey based and may not perfectly align cross-linguistically; only Chinese prompts evaluated for ERNIE-4 in this study.",
            "notable_findings": "ERNIE-4 mostly matched humans in KL terms for many WEPs but had large median differences for specific expressions (e.g., 'we doubt'). Overall, cross-model and cross-language comparisons show that pre-training language and data sources affect estimative uncertainty mappings.",
            "uuid": "e3624.2",
            "source_info": {
                "paper_title": "An Evaluation of Estimative Uncertainty in Large Language Models",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Llama-2-7B",
            "name_full": "LLaMA-2 7B",
            "brief_description": "An open-source Llama-series model with ~7 billion parameters evaluated in the paper for mapping WEPs to numerical probabilities; used for comparison to proprietary GPT models and to examine model-size effects.",
            "citation_title": "An Evaluation of Estimative Uncertainty in Large Language Models",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-7B",
            "model_description": "Open-source Llama-series transformer LLM with approximately 7B parameters (paper cites Llama-2-7B), pre-trained on large corpora (English-dominant); used without additional fine-tuning in the experiments.",
            "prediction_task": "Map 17 WEPs in English concise/extended/gendered contexts to numerical probabilities and compare distributions to human survey and to other LLMs.",
            "method_of_probability_estimation": "Direct prompting in English requesting a float between 0 and 1 (temperature = 0). No CoT or calibration reported for Llama models in the experiments.",
            "dataset_or_benchmark": "Same RQ1 English dataset of 776 prompts across context templates; human survey baseline from Fagen-Ulmschneider.",
            "performance_metrics": "KL divergence and Mann-Whitney U comparisons to human distributions. The Llama-2-7B estimates were broadly consistent with GPT-family estimates but typically exhibited larger divergence from humans than GPT-3.5/GPT-4.",
            "comparison_to_baselines": "Compared to human distributions and to GPT models; Llama estimates were often farther from human medians and had larger divergence than the GPT models, consistent with smaller model capacity.",
            "limitations_or_challenges": "Smaller model size likely reduces alignment with human WEP mappings; experiments used zero system messages and temperature 0 which may limit output diversity; no model-specific calibration applied.",
            "notable_findings": "Llama-2-7B can perform the mapping task but shows larger divergence from human estimations than the GPT models, supporting a relationship between model capacity and closeness to human-like estimative uncertainty.",
            "uuid": "e3624.3",
            "source_info": {
                "paper_title": "An Evaluation of Estimative Uncertainty in Large Language Models",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Llama-2-13B",
            "name_full": "LLaMA-2 13B",
            "brief_description": "An open-source Llama-series model with ~13 billion parameters used in the paper for comparison in WEP-to-probability mapping experiments, showing behavior intermediate between smaller Llama and larger proprietary GPT models.",
            "citation_title": "An Evaluation of Estimative Uncertainty in Large Language Models",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-13B",
            "model_description": "Open-source Llama transformer LLM with approximately 13B parameters (cited in paper), pre-trained on large corpora (English-dominant); used without fine-tuning in experiments.",
            "prediction_task": "Map 17 WEPs across narrative contexts in English to numerical probability outputs and compare distributions to humans and other LLMs.",
            "method_of_probability_estimation": "Direct prompting to supply a float between 0 and 1 (temperature = 0); no Chain-of-Thought or fine-tuning used for Llama models.",
            "dataset_or_benchmark": "Same RQ1 English dataset of templated prompts (776 prompts) and human survey baseline from Fagen-Ulmschneider.",
            "performance_metrics": "KL divergence and Mann-Whitney U test comparisons; Llama-2-13B estimates were largely consistent with GPT-model trends but often exhibited larger divergence from human distributions than GPT-3.5 and sometimes GPT-4.",
            "comparison_to_baselines": "Compared to human distributions and GPT-family models. Llama-2-13B performed better than the 7B variant in some respects but still showed greater divergence than the GPT models, consistent with capacity differences.",
            "limitations_or_challenges": "No in-paper calibration; relative underperformance compared to GPT models may reflect pre-training data differences and absence of deployed instruction tuning or alignment steps present in commercial models.",
            "notable_findings": "Llama-2-13B demonstrates the capability to produce numerical probability mappings for WEPs but with larger deviations from human judgments than GPT-3.5, suggesting model scale and training regimen impact estimative uncertainty alignment.",
            "uuid": "e3624.4",
            "source_info": {
                "paper_title": "An Evaluation of Estimative Uncertainty in Large Language Models",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Words of Estimative Probability",
            "rating": 2,
            "sanitized_title": "words_of_estimative_probability"
        },
        {
            "paper_title": "Perception of probability words",
            "rating": 2,
            "sanitized_title": "perception_of_probability_words"
        },
        {
            "paper_title": "Probing neural language models for understanding of words of estimative probability",
            "rating": 2,
            "sanitized_title": "probing_neural_language_models_for_understanding_of_words_of_estimative_probability"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Using cognitive psychology to understand gpt-3",
            "rating": 1,
            "sanitized_title": "using_cognitive_psychology_to_understand_gpt3"
        }
    ],
    "cost": 0.013040999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>An Evaluation of Estimative Uncertainty in Large Language Models
24 May 2024</p>
<p>Zhisheng Tang 
Information Sciences Institute
University of Southern California
Marina del Rey
90292United States of America</p>
<p>Ke Shen 
Information Sciences Institute
University of Southern California
Marina del Rey
90292United States of America</p>
<p>Mayank Kejriwal kejriwal@isi.edu 
Information Sciences Institute
University of Southern California
Marina del Rey
90292United States of America</p>
<p>An Evaluation of Estimative Uncertainty in Large Language Models
24 May 2024CDB29C1F525E1A3009B506CA94A9083FarXiv:2405.15185v1[cs.CL]
Words of estimative probability (WEPs), such as "maybe" or "probably not" are ubiquitous in natural language for communicating estimative uncertainty, compared with direct statements involving numerical probability.Human estimative uncertainty, and its calibration with numerical estimates, has long been an area of study -including by intelligence agencies like the CIA.This study compares estimative uncertainty in commonly used large language models (LLMs) like GPT-4 and ERNIE-4 to that of humans, and to each other.Here we show that LLMs like GPT-3.5 and GPT-4 align with human estimates for some, but not all, WEPs presented in English.Divergence is also observed when the LLM is presented with gendered roles and Chinese contexts.Further study shows that an advanced LLM like GPT-4 can consistently map between statistical and estimative uncertainty, but a significant performance gap remains.The results contribute to a growing body of research on human-LLM alignment.MainIn natural language, commonsense expressions of uncertainty play an important role in human communication, 1, 2 allowing people to account for the uncertain nature of the world without always having to rely on numbers and statistics.For example, given that it is been rainy for the past three days, a statement like "it is highly likely that tomorrow will also be rainy" is more common and natural than "I estimate that the probability that it will rain tomorrow is 95 percent."Such words or phrases ("highly likely") are known as Words of Estimative Probability (WEPs) and intended to express estimative uncertainty in natural language.In human language, these words play a critical role in nuanced discussions.They allow us to express beliefs, make predictions, and communicate uncertainties in a way that can be understood and acted upon by others.][5]First investigated systematically by Sherman Kent 6 during his time at the Central Intelligence Agency (CIA), WEPs have also been studied by intelligence units and geopolitical experts owing to their ubiquity in reports, and their importance as linguistic elements in conveying varying degrees of uncertainty and ambiguity.Kent specifically sought to quantify the numerical probability (which can be a distribution) that people implicitly refer to when using different WEPs.6Using carefully constructed surveys, he was able to map key WEPs into probability distributions that then came to be used by the CIA.][9][10][11]In his handbook, Barclay 12 references a survey among the NATO officers on the associated numerical probabilities for different WEPs.More recently, Fagen-Ulmschneider 13 surveyed 123 people on their perception of probabilistic words via social media, and found that current perceptions of these WEPs have remained largely consistent with those found in Kent's earlier study.Recent advent, and continued application, of Large Language Models 14 (LLMs) introduces a new dimension to this research.LLMs are pre-trained on a vast amount of text data, which includes books, articles, and websites, enabling them to grasp a wide range of linguistic patterns, cultural nuances, and factual information.As a result, they demonstrate humanlike performance on many natural language processing tasks.15,16ecause of their generative and conversational capabilities, they are already being widely adopted in commercial applications that people interact with on a daily basis, including conversational assistants,17,18   automated summarization, 19 and customer service.20][23][24]This study investigates whether LLMs can express uncertainties in a way that closely mirrors human practices.This examination is motivated by the observation that how LLMs interpret WEPs may impact the trust and reliability people place in them during use.We begin by baselining estimative uncertainty in several important LLMs to distributional data constructed from (externally conducted) human surveys as a reference.To ensure robustness, we evaluate LLMs' estimative uncertainty under different controls, including whether gender and the choice of language (English or Chinese) make a difference to the</p>
<p>baseline findings.For example, we consider whether adding a gendered role to the prompt that is presented to an LLM affects any of the conclusions.In a similar vein, we also investigate changes in effect strength and direction both when an LLM like GPT-4 (that understands both English and Chinese) is prompted using Chinese, as well as when the LLM (such as ERNIE-4 25 ) is trained primarily using Chinese text.The latter experiment is motivated by the fact that LLMs are increasingly being used for multi-lingual tasks like machine translation. 26,27 o investigate these effects systematically, we construct datasets involving different scenarios, controls, and WEPs.A suite of LLMs (GPT-3.5-Turbo 17, GPT-4 18 , Llama-series models e.g., Llama-2-7B 16 , Llama-2-13B 16 and ERNIE-4 25 ) are examined.</p>
<p>Our second objective considers an issue that is especially important for communicating scientific information in everyday language.In scientific discourse, statistical uncertainty tends to dominate when presenting findings, especially in formal publications.In contrast, estimative uncertainty is the norm for communicating uncertainty in everyday natural language.LLMs are starting to be used increasingly often in science, including for summarizing scientific findings. 28,29 herefore, for a specific high-performing LLM (GPT-4), we consider whether different levels of statistical uncertainty in the prompt, appropriately controlled, lead to consistent changes (or lack thereof) in the LLM's elicitation of estimative uncertainty.Because formal evaluation of such consistency in AI systems has not been explored thus far in the literature, we propose and formalize four novel consistency metrics (see Methods for details) for evaluating the extent to which an LLM like GPT-4 is able to change its level of estimative uncertainty when prompted with changing levels of statistical uncertainty.</p>
<p>Experimentally, we find that most LLMs, especially the GPT models, closely align with human uncertainty estimates in contexts of extreme or balanced certainty, and reflect a high degree of sophistication in interpreting WEPs that express positive certainty.However, we also found that of the 17 WEPs we consider, the probability estimates from GPT-3.5 and GPT-4 diverge from those of humans for 11 and 12 WEPs, respectively.Hence, alignment between humans and LLMs on a significant portion of WEPs is still wanting.</p>
<p>When narratives involve gender-specific contexts, all LLMs displayed a higher divergence from human estimations, suggesting a gap in processing gender-centered context.GPT-3.5 stood out for its lower divergence from human judgments across various contexts, even when compared with GPT-4, an LLM with more advanced natural language understanding capabilities.Intriguingly, LLMs' uncertainty estimates showed only minor differences when prompted using either English or Chinese.However, ERNIE-4, an LLM pre-trained using mostly Chinese text, shows significantly different uncertainty estimates for several WEPs compared to those of GPT-3.5, using the same Chinese prompts.Because GPT-3.5 can also be prompted in Chinese, the divergence cannot be explained away simply due to the latter's lack of Chinese understanding.</p>
<p>Finally, our investigation into LLMs' performance on statistically uncertain data underscored the effectiveness of GPT-4, which yielded better-than-random performance across all four consistency metrics.However, using more advanced prompting methods like Chain-of-Thought (CoT) prompting 30 failed to realize the improvements in performance that have been observed for more traditional natural language understanding problems, like question answering and commonsense reasoning. 30,31 he findings suggest that further bridging the gap between statistical uncertainty and estimative uncertainty in LLMs may prove to be more challenging using currently popular prompt engineering methods.</p>
<p>Results</p>
<p>Comparing estimative uncertainty in LLMs to humans under different experimental conditions</p>
<p>Figure 1 shows the distribution of probability estimates for 17 Words of Estimative Probability (WEPs) provided by GPT-3.5 and GPT-4, aggregated across independent concise contexts presented in English and Chinese.It also includes results from ERNIE-4.0, an LLM pre-trained primarily on Chinese text, which is prompted using only Chinese.The results show that the distributions for GPT-3.5 and GPT-4 diverge from those of humans for 11 and 12 WEPs, respectively.Using the Mann-Whitney U test, the differences are found to be statistically significant.For example, there is an absolute median difference (AMD) of 5% between the human and GPT-3.5 for the WEP 'probable' (U(N 1 = 123, N 2 = 15) = 506.5,p &lt; 0.01), with rank-biserial correlation (RBC) of 0.45, (95%CI 0.3 to 0.58).There is an even larger AMD of 10% between humans and GPT-4 (U(N 1 = 123, N 2 = 15) = 417.5,p &lt; 0.01), with RBC of 0.49, (95%CI 0.34 to 0.61).Median differences between humans and GPT-4 are also observed for WEPs such as 'likely' (AMD Interestingly, we find that humans and GPT models have statistically indistinguishable distributions for WEPs with high positive certainty, such as 'almost certain' (AMD = 0, U(N 1 = 123, N 2 = 15) = 953.5,p = 0.83) and 'highly likely' (AMD = 0, U(N 1 = 123, N 2 = 15) = 772.5,p = 0.29) for GPT-4.Similarly, humans and GPT models have AMDs of zero on 'about even' (U(N 1 = 123, N 2 = 15) = 967.5,p = 0.56), for both GPT-3.5 and GPT-4.Overall, we find that GPT-3.5 consistently exhibits lower divergence than GPT-4 in most contextual analyses, despite GPT-4's superior performance in various natural language understanding tasks. 15While the two still offer relatively close estimations, GPT-3.5'sestimations are more closely aligned with human judgments across various contexts, suggesting that it interprets estimative uncertainty in a more human-like manner.Figure 2 displays the distribution of probability estimates for 17 WEPs provided by GPT-3.5 and GPT-4 using genderspecific prompts.These prompts either have Male (e.g., 'he') or Female (e.g., 'she') as the subject.The first noticeable difference is that, under gender-specific contexts, GPT distributions exhibit less variability compared to human distributions; in several cases (e.g., 'highly unlikely,' 'improbable,' and 'highly likely'), the GPT distributions even collapse into a single point.Figure 3 also presents the distributions of probability estimates for 12 WEPs divided into three categories (high, moderate, and low probability WEPs).Detailed statistical analyses (Supplementary Information Figures S9-S15) show that, for individual LLMs, the gender of the subject does not yield significantly different estimations, except for 'probably' (AMD = 0, U(N 1 = 10, N 2 = 10) = 71, p = 0.07 for GPT-4).Additionally, we observe (Supplementary Information Figures S1-S8) that the estimations obtained from the GPT models, when prompted with gender-specific contexts, exhibit similar differences (compared to human estimations) as those observed when the models are prompted with gender-neutral concise narrative contexts.For the two GPT models, the differences between prompting using the male and gender-neural concise narrative context are most significant in GPT-3.5 for WEPs expressing negative certainty, such as 'almost no chance' (AMD = 4, Finally, Figure 4 presents the divergence between the probability distributions of the different models, depending on whether the prompts are in English or Chinese.On the left, it compares the responses generated by ERNIE-4.0 to Chinese prompts with those provided by humans.In the middle, it compares responses when prompted in both English and Chinese for GPT-3.5 and GPT-4.On the right, it contrasts the results from GPT-3.5 or GPT-4 with those from ERNIE-4.0, with all prompts in Chinese.Focusing on the difference between the estimations from ERNIE-4 and humans, we observe that the Kullback-Leibler (KL) divergence is low for 16 WEPs, as the color indicates, with the sole exception being 'we doubt' (AMD = 40, U(N 1 = 15, N 2 = 123) = 1778, p &lt; 0.01).However, we also note that 12 WEPs exhibit statistically significant differences for the Mann-Whitney U test.This test can detect differences in their central tendencies or how their values are distributed, making it more sensitive to median differences between distributions, whereas KL divergence quantifies how much one distribution diverges from a second distribution.This suggests that while the overall 'information content' of the compared distributions is similar, they still differ significantly in their median.</p>
<p>Turning to the comparison between prompting in English and Chinese using GPT-3.5 or GPT-4, we find a darker color and a higher KL divergence, suggesting a larger difference between the probability estimations when prompted using English and Chinese, for WEPs like 'highly likely' (AMD = 10, U(N 1 = 15, N 2 = 15) = 184, p &lt; 0.01 for GPT-3.5, AMD = 10, Finally, we found that the probability estimates from Llama-2-7B and Llama-2-13B, prompted in English, are largely consistent with those found in the GPT models.However, their estimates often exhibit larger divergence from those of humans.These results are provided in Supplementary Information Figures S1-S8.</p>
<p>Investigating the effect of statistical uncertainty on GPT-4's estimative uncertainty</p>
<p>To evaluate GPT-4's performance in estimating the outcome of statistically uncertain events using WEPs, we created three different scenarios (Height, Score, and Sound).In general, each question in the dataset provides a set of WEP choices to the LLM, and elicits from it the choice that best describes the probability of a number falling within an interval, given a sample 'distribution' of past observations.For example, one question is: Complete the following sentence using one of the choices, listed in descending order of likelihood, that best fits the sentence: A.is almost certainly B.is likely to be C.is maybe D.is unlikely to be E.is almost certainly not.I randomly picked 20 specimens from an unknown population.I recorded their heights, which are 116, 93, 94, 89, 108, 76, 117, 92, 103, 97, 114, 79, 96, 96, 111, 89, 98, 91, 100, 105.Based on this information, if I randomly pick one additional specimen from the same population, the specimen's height _ below 99.We elicit responses from the LLM using both standard prompting, as well as Chain-of-Thought (CoT) prompting 30 that is further detailed in Methods.</p>
<p>Four metrics are proposed for evaluating the consistency of LLMs: pair-wise consistency, monotonicity consistency, empirical consistency, and empirical monotonicity consistency.The minimum and maximum consistency score is 0 and 100, with 100 being the most consistent.However, the expected random performance for each metric is different.More details on 6/17 Figure 5.A bar graph illustrating the performance of GPT-4 on answering questions about the outcome of statistically uncertain events using words of estimating probability (WEPs).The graphs compare scores using four metrics: pair-wise consistency, monotonicity consistency, empirical consistency, and empirical monotonicity consistency, for both standard and Chain-Of-Thought (CoT) prompting methods.Results for each metric are further divided based on different scenarios.The random performance is shown as a red dashed line for each metric.The standard error is shown as a vertical red line, and the numerical value corresponding to each bar is displayed.<em>, </em>*, and *** represent statistical significance between normal and CoT prompting, using the paired t-test, at the 90%, 95%, and 99% confidence levels, respectively.the dataset and the metrics are provided in Methods.</p>
<p>Figure 5 displays the performance of GPT-4 evaluated on both the standard and CoT prompting methods using the four proposed metrics (pair-wise consistency, monotonicity consistency, empirical consistency, and empirical monotonicity consistency).First, we observe that all the results are well above random performance, indicating the efficacy of employing LLMs in estimating probabilities from statistically uncertain data using WEPs.However, it is worth noting that these results do not achieve the same level of high performance as observed in other natural language processing or math-word tasks 15 .Surprisingly, the CoT prompting method 32 only gains significant performance when the LLM is evaluated using empirical consistency (t(59) = −4.15,p &lt; 0.01, with Cohen's d = −0.358for the 'Height' scenario, t(59) = −2.82,p &lt; 0.01, d = −0.268for the 'Score' scenario, t(59) = −2.61,p = 0.01, d = −0.234for the 'Sound' scenario).In examining the results for monotonicity consistency, we found that the model consistently chooses the same choice for all questions instantiated using increasing confidence levels, which yields a high score but suggests a lack of nuanced understanding and calibration of uncertainty.This is confirmed by the results obtained using the empirical monotonicity consistency metric, where such a simple choice combination is not accepted, and steep performance drops are observed.</p>
<p>Figure 6 shows the performance of GPT-4 in two settings, based on the number of WEPs choices provided, with one setting offering 5 choices and the other 3 choices.We observe that the performance in the 5 choices setting is significantly higher than that on the 3 choices setting when evaluated using pair-wise consistency (t(179) = 6.48, p &lt; 0.01, d = 0.673).This might seem initially surprising because, intuitively, having fewer options should make it easier for the model to make the correct choice.However, choices in the 3 choices set may seen by the model to be less distinct from each other, making it consequently more challenging for it to perform well under this condition.However, when evaluated using empirical consistency (t(359) = −2.35,p &lt; 0.05, d = −0.128)and empirical monotonicity consistency (t(287) = −4.15,p &lt; 0.01, d = −0.283),GPT-4 does perform better under the 3 choices condition.Combined with Supplementary Information Figure S23, we observe statistically comparable performance between the narrow and wide range of the statistically uncertain outcomes for all metrics, demonstrating the robustness of GPT-4 in appropriately responding to different possible (statistically) uncertain distributions.Nonetheless, we note that the consistency is well below 100 percent on most metrics, scenarios and conditions, showing that the problem of aligning statistical uncertainty with estimative uncertainty cannot be considered to be solved, even in an advanced Figure 6.A bar graph comparing the performance of GPT-4 on answering questions about the outcome of statistically uncertain events using words of estimating probability (WEPs) across two settings based on the number of WEPs choices provided: one setting offers five choices, while the other offers three.The model is evaluated using four metrics: pair-wise consistency, monotonicity consistency, empirical consistency, and empirical monotonicity consistency.For each metric, we also control for the range of statistically uncertain outcomes, with a setup with a narrow or less statistically uncertain range (left), and a wide or more statistically uncertain range (right).The standard error is shown as a red line, and the numerical value corresponding to each bar is displayed.<em>, </em>*, and *** represent statistically significant differences between the 5-choices and 3-choices settings, using the paired t-test, at the 90%, 95%, and 99% confidence levels, respectively.Supplementary Information Figure S23 also presents a similar comparison controlling for either the number of WEPs choices provided, or the range of statistically uncertain outcomes.commercial LLM like GPT-4.</p>
<p>Discussion</p>
<p>In comparing uncertainty estimates between LLMs and humans, our findings show that, for 11 and 12 WEPs out of 17, GPT-3.5 and GPT-4 respectively give probability estimates that are different from those given by humans.However, in situations of high positive certainty(e.g., 'almost certain' or 'highly likely'), the GPT models' uncertainty estimates closely mirror those of humans.Intriguingly, GPT-3.5 consistently shows lower KL divergence when compared to GPT-4 across various scenarios despite GPT-4's advanced capabilities.When prompted under gender-specific contexts, GPT models' estimations often collapse into a single point but exhibit minimal divergence from the human estimate for the majority of WEPs.Additionally, we observed minor divergences between ERNIE-4.0's estimates and those of humans.However, a more significant divergence becomes apparent when the prompting language shifts from English to Chinese for GPT-3.5 and GPT-4.This divergence widens further when comparing the estimates from GPT-3.5 and GPT-4 with those from ERNIE-4.0, with all models being prompted in Chinese.</p>
<p>In investigating the capacity of LLMs to handle statistical uncertainties when using WEPs to express their estimative uncertainty, we find that the performance of GPT-4 is significantly better than random when evaluated using the four different consistency metrics, affirming the general effectiveness of LLMs in interpreting statistical uncertain data.Although GPT-4 demonstrates near-perfect performance when assessed using the monotonicity consistency metric, this appears to be an illusion, as the model consistently makes the same choices under varying conditions.This illusion signifies that GPT-4 has a limited understanding of the concepts involved.We also found that the performance of GPT-4 is sensitive to the number of choices it is allowed to choose from, but not to the range of the statistical uncertainty that is being ingested into the prompt.The little, or even negative, improvement yielded by Chain-of-Thought prompting suggests the inherent difference between this 'task' and other natural language processing tasks like question answering and information extraction. 30,31,33 O work builds on Kent's originally classified work Words of Estimative Probability, 6 which explored human perception of probabilistic words in a systematic way.We extend the work by Fagen-Ulmschneider, 13 who surveyed people's probability estimations on 17 different WEPs, by comparing the corresponding probability distribution of each WEP with LLMs that are pre-trained on larger amounts of human-created data.These works also inspired Sileo 34 to investigate how challenging it is for language models to understand WEPs.However, they only use the median surveyed probability for each WEP to compare with the language models' estimations.In addition to considering the full distribution and controls such as gender and language, this study also uses more recent LLMs with more parameters and stronger conversational abilities.Furthermore, we analyzed the uncertainty estimates of GPT-4 when facing prompts with statistically uncertain data.Previous research 30,32 showed that Chain-of-Thought prompting can be used to boost performance on numerical-or math-related natural language problems.However, our investigation reveals limitations in CoT's ability to increase alignment between statistical and estimative uncertainty compared to standard prompting.</p>
<p>Our research also aligns with earlier research contributions in 'BERTology,' 35 which sought to investigate the behavior and underlying mechanisms of the BERT language model 36 and its derivatives.This seminal work has paved the way for a new line of studies that emphasized greater understanding of language model behavior compared to simply measuring their performance on task-oriented benchmarks.Additionally, Binz and Schulz 37 redirect the emphasis toward LLMs, comparing their performance with human behaviors in psychological experiments originally designed for human subjects.Our contribution adds to the growing body of research that explores the alignment between LLMs and humans 38 and investigates such alignment in the understanding of WEPs.</p>
<p>A key limitation of this study is the use of human uncertainty estimates from a sample of undergraduate students obtained from a survey.While other work has pointed toward the reliability of the survey, it is not settled whether these estimates are representative of the broader population's understanding of WEPs, which can skew the comparative analysis between humans and LLMs.Future work should include a more diverse population when surveying human uncertainty estimations.Additionally, we rely on artificially generated data to evaluate GPT-4's performance on prompts with statistically uncertain data.While this paradigm offers a controlled environment to assess an LLM's capabilities, it might not be accurate enough to represent real-world statistical scenarios and the complexities involved in interpreting statistically uncertain data in a natural language context.A promising future direction is to explore these under real-world scenarios and data (e.g., using carefully selected prompts containing statistical information reported in actual scientific publications), which may yield insights with stronger external validity.Additionally, we only focus on the 17 WEPs that were used in the survey, whereas incorporating a wider range of probabilistic expressions could provide more comprehensive insights into LLMs' estimations.While we found little difference in prompting either in Chinese or in English for the GPT models, a cross-linguistic study with more languages in uncertainty estimation is merited.Moreover, our study focused on static prompts without considering the dynamic nature of conversations.Future works could examine how LLMs adjust their probability estimations in response to a changing context within a dialogue.</p>
<p>Methods</p>
<p>The structure of this section is organized into two distinct parts, each dedicated to the two research objectives.Methods for each objective are discussed in detail from three perspectives: data construction, metrics, and experimental setup.In the section on data construction, we delineate the methodologies employed to curate the datasets tailored for our experiments, and the additional context for our decision to construct them in that manner.The metrics section provides a detailed explanation of both traditional and innovative criteria used to evaluate the results.Lastly, the experimental setup section provides a description of the specific LLMs used, along with other relevant details on the evaluation framework.</p>
<p>Comparing estimative uncertainty in LLMs to humans under different experimental conditions</p>
<p>Data Construction</p>
<p>The first research question aims to compare the interpretation of estimative uncertainty in WEPs using numerical probabilities between LLMs and humans.To enable meaningful comparison between humans and LLMs, we utilize the same set of WEPs as used in the Fagen-Ulmschneider's survey. 13This set of seventeen WEPs contains almost certain, highly likely, very good chance, probable, likely, we believe, probably, better than even, about even, we doubt, improbable, unlikely, probably not, little chance, almost no chance, highly unlikely, and chances are slight.Additionally, we introduce four distinct context settings inspired by the narratives found in Kent's CIA report 6 and an article from Harvard Business Review. 39These context settings, comprising manually crafted context-templates, are specifically designed to evaluate if the certainty estimations made by LLMs vary based on different narrative backgrounds.The settings are described in Table 1, which also provides a sample template and related counts and statistics for each of these contexts, with Tables 2, 3, and 4 providing the full list of templates for completeness:</p>
<p>• Concise Narrative Context (CNC): Simple, intuitive narrative contexts that include brief scenarios intended to provide a straightforward background to use WEPs.</p>
<p>9/17</p>
<p>• Extended Narrative Context (ENC): In contrast to the CNC setting, ENCs contain more prolonged narratives with increased information, incorporating various clauses to create a detailed setting.</p>
<p>• Female-Centric Narrative Context (FCNC): Similar to the CNC in its simplicity, this setting includes only short scenarios.However, it specifically employs 'She' as the subject to introduce a gender-specific narrative.The theory is {} the only explanation for the phenomenon.This ingredient is {} necessary for the recipe.</p>
<p>Based on the weather forecast, it will {} rain tomorrow.By integrating these templates with the seventeen WEPs used in our research, we have compiled a total of 776 prompts for the experiments related to RQ1.We manually adjusted the prompts to ensure grammatical coherence for WEPs like better than even, which do not seamlessly fit into the templates.The structured prompting approach enables us to analyze how the inclusion and variation of context influence the LLMs' responses in estimating the certainty associated with different WEPs.</p>
<p>In addition to the prompts described above, an instruction prompt is appended at the beginning and is constructed as follows: Format your answer as a float value between 0 and 1, and make your answer short.Also, to elicit numerical probability estimates from LLMs, we ask the LLM to give its probability estimate using the following template: Given the statement "{}", with what probability do you think {}?The first {} is a placeholder for any context template that has been instantiated using a WEP, whereas the second {} is a placeholder for the same context template without any instantiation.For example, a fully instantiated prompt, using 'Probably' as the WEP and 'They will {} launch before us' as a CNC context template, would be presented to an LLM as follows: Format your answer as a float value between 0 and 1, and make your answer short.Given the statement "They will probably launch before us", with what probability do you think they will launch before us?</p>
<p>We also investigate how a change of prompting language, from English to Chinese, affects the estimative probability for LLMs.Thus, we have curated an additional dataset on the basis of the original English CNC templates (Table 2).We manually translated these CNC templates into Chinese.Similar to the English version, we append the following instruction prompt, translated from the English version with slight variation, at the beginning of any CNC template: 你的输出只有0到1之间的</p>
<p>10/17</p>
<p>Khrushchev may have had in the back of his mind such and such, or indeed it is {} that somebody had just primed him with a particular perspective or piece of information that influenced his decision-making at that moment.It's {} that when faced with the crisis, Churchill recalled past failures, or it's conceivable that an advisor had recently presented him with fresh insights that swayed his judgment.In his diplomatic endeavors, Ahmed {} held the lessons from his predecessors in high esteem.Given the intricate nature of the puzzle, solving it in under an hour is {} a remarkable feat.In the realm of popular music, where artists come and go with the changing trends and fans chase the latest hits, crafting a timeless song that resonates with multiple generations is {} an achievement signifying true artistry.Given the diverse sources of the intelligence report, it is {} a mistake that this piece of information was overlooked, though there are indications that it could have been due to a human error.While the painting is {} from the Renaissance period, it sometimes carries motifs typical of that era; artists always borrow inspiration from the past.The intricate web of conspiracy theories surrounding the moon landing suggests that it was {} a hoax perpetuated by NASA Despite the complexity of climate models, they indicate that global temperatures will {} decrease significantly in the coming decades.The historical evidence suggests that it was {} a coincidence that these two great inventors were born in the same era.The chances of winning the lottery are {} in your favor, but that doesn't stop millions of people from trying their luck.She / He {} wakes up at 6 a.m.She / He {} takes the bus to work.She / He {} orders the same dish at that restaurant.She / He {} attends the weekly meetings.She / He {} visits the park on weekends.She / He {} reads a book before bed.She / He {} remembers to bring an umbrella when it's cloudy.She / He {} dines out on Fridays.She / He {} listens to the news on the morning drive.She / He {} bakes a cake for birthdays.</p>
<p>Table 4.</p>
<p>The full list of Female-Centric Narrative Context (FCNC) and Male-Centric Narrative Context (MCNC) templates."/" represent a choice between "She" and "He".The FCNC templates choose "She" as the subject and the MCNC templates choose "He" as the subject.{} represents a placeholder for possible WEPs.</p>
<p>带有两位小数的浮点值。回答问题时直接给出最终答案，不要加入中间思考过程，不要重复问题。.Additionally, to elicit the numerical probability from LLMs, we encapsulate the associated context templates in the following way: 根据陈 述"{}"，您认为{}的概率是多少？, where the first {} is a placeholder for any context template that has been instantiated using a WEP and the second {} is a placeholder for the same context template without any instantiation.An example using 可能 as the WEP and 他们{}在我们之前发布 as the context template is: 你的输出只有0到1之间的带有两位小数的浮点 值。回答问题时直接给出最终答案，不要加入中间思考过程，不要重复问题。根据陈述"他们很可能会在我们之前 发布"，你认为他们在我们之前发布的概率是多少？</p>
<p>Metrics</p>
<p>To examine the differences in the underlying distribution of probability estimates of each WEP between humans and LLMs, we utilize both the Kullback-Leibler (KL) divergence test 40 and the Mann-Whitney U test. 41The KL divergence test provides a measure of how one probability distribution diverges from a second probability distribution.It quantifies the amount of information lost when one distribution is used to approximate another, which is useful in determining how much information is lost when humans interpret probabilities given by LLMs in estimating situations.Mathematically, for discrete distributions, the KL divergence is defined as D KL (P||Q) = ∑ i P(i) log P(i) Q(i) , where P and Q represent the probability distributions.Because the responses gave by humans and LLMs range from 0 to 100, we first discretize the responses by fitting them into 20 bins with equal width, such as 0 to 5, 5 to 10, ..., 90 to 95, and 95 to 100.Then the associated discrete probability distribution can be calculated accordingly, along with KL divergence between any two distributions.</p>
<p>The Mann-Whitney U test, is a non-parametric statistical test that is used to compare differences between two independent samples.It is useful when the assumptions of normal distribution and homogeneity of variances are not met, as is the case for the human estimations for WEPs.It evaluates whether one group tends to have higher or lower values than the other group, without assuming a specific distribution for the data.Given n 1 samples for one population and n 2 samples from another, the corresponding Mann-Whitney U statistic is defined as the smaller of:
U 1 = n 1 n 2 + n 1 (n 1 +1) 2 − R 1 , U 2 = n 1 n 2 + n 2 (n 2 +1) 2 − R 2 . The R 1 , R 2 represent
the sum of the ranks in groups 1 and 2, after pooling all samples in one set and where the smallest value obtains rank 1 and so on.</p>
<p>By employing both the KL divergence and Mann-Whitney U test, we gain a more comprehensive understanding of the differences in probability estimates provided by humans and LLMs, highlighting the discrepancies in interpretation and estimation that may exist between these two sources.</p>
<p>Experimental Setup</p>
<p>Five LLMs are employed in investigating RQ1: GPT-3.5-turbo 17 , GPT-4 18 , LLaMa-7B, LLaMa-13B 16 , and ERNIE-4.0 25 from Baidu.The first two LLMs are proprietary and significantly larger in scale, whereas the latter two are open-source and comparatively smaller.The last one is an LLM pre-trained primarily using Chinese corpus, whereas the others are pre-trained primarily using English corpus.This setting provides a rich spectrum of comparison points.By analyzing and comparing the responses of these LLMs, we gain insights into the impact of model size and architecture on their differences with humans when interpreting WEPs.All models, except ERNIE-4.0, have been tested on all four different contexts (CNC, ENC, MCNC, FCNC) and their responses are compared using both metrics with human survey results.Additionally, the GPT family of models and ERNIE-4.0 have been tested on the Chinese version of CNC.For the GPT family of models, all prompts are constructed with the role of 'user' without any 'system' messages.We used a temperature of 0 to maximize reproducibility for all models.</p>
<p>Investigating the effect of statistical uncertainty on GPT-4's estimative uncertainty</p>
<p>Data Construction</p>
<p>In RQ1, we studied the differences between LLMs and humans in interpreting different WEPs.However, the ability of LLMs to use these WEPs still needs further investigation.In RQ2, we examine how LLMs use the WEPs to express their estimates of statistically uncertain events.Specifically, when given numerical observations of an event's outcome, how would LLMs use a given set of WEPs to estimate the likelihood of future outcomes of the same event?To answer this research question, we constructed the test data set using the pipeline shown in Figure 7.</p>
<p>As shown in Figure 7 (a), three different scenarios (Height, Score, and Sound) were constructed, each with three different controls (CHOICES, NUMBERS, and INTERVAL).Each control represents a distinct variable that influences the outcome in each scenario and is described as follows:</p>
<p>• CHOICES: Two sets of possible WEPs choices are available for LLMs to choose from (Figure 7(b)).Notice that all of the choices from both sets are used in RQ1.We aim to elicit more fine-grained estimates using the first choice set (i.e., the five choices set) while also investigating the LLMs' estimates under a more generalized scale (i.e., likely, maybe, unlikely) using the other choice set.</p>
<p>12/17 • NUMBERS: Two sets of numbers, one with a narrow range and the other with a wider range (Figure 7(c)), can be used to instantiate scenarios.We examine LLMs' estimates under these two different distribution patterns.The first set of numbers is generated based on the normal distribution with a mean of 100 and a standard deviation of 10, whereas the second set is generated based on the normal distribution with a mean of 100 and a standard deviation of 40.</p>
<p>• INTERVALS: Six descriptions of mathematical intervals: (−∞, low), (low, ∞), (low, high), (−∞, low) or (high, ∞), (−∞, high), and (high, ∞), where low and high are a pair of integer numbers.When given a set of numbers, we provide LLMs with one of these intervals as a range where these numbers could potentially lie.To generate the pairs of low and high points, we used the two end points of a confidence interval around the mean of the normal distributions that were used to generate the numbers, where low represents the lower end of the interval, and high represents the higher end.Five confidence levels were used: 0.05, 0.275, 0.5, 0.725, and 0.95, with each increasing level containing a wider range.The numbers shown in the columns of low and high in Figure 7(d) represent the probability that an additional number from the same distribution will fall below the point.</p>
<p>Additionally, three pairs of complementary intervals are defined as: below low and above low, between low and high and below low or above high, and below high and above high.Each of these pairs encompasses the entire range of numbers for any given set of numbers.For example, the interval below 99 and above 99 includes all numbers, covering everything less than 99 and everything greater than 99, leaving no number unrepresented except 99 itself.</p>
<p>A fully constructed example scenario that we would provide to an LLM is provided in Figure 7(f), where the first scenario is instantiated using the 5 choices control, the normal distribution with mean 100 and standard deviation 10, and the interval below low using the confidence interval level at 0.05.</p>
<p>In total, we have three scenarios, two CHOICES sets, two sets of NUMBERS, five confidence interval levels, and six INTERVALS.Hence, we can construct 3 × 2 × 2 × 5 × 6 = 360 fully instantiated prompts.Additionally, to investigate whether the use of the Chain-of-Thought (CoT) prompting method can bring an increase in performance, a zero-shot CoT 32 prompt was generated for each of the 360 constructed prompts.The CoT prompt was generated by changing Complete the following sentence using one of the choices, listed in descending order of likelihood, that best fits the sentence: CHOICES.into First 13/17 compute the associated probability.Then complete the following sentence using one of the choices, listed in descending order of likelihood, that best fits the sentence: CHOICES.Give your final choice after 'I choose:'.</p>
<p>Metrics</p>
<p>Table 6.</p>
<p>Examples showing the correctness of responses (evaluated using the four metrics) when given a prompt that is instantiated with different intervals.</p>
<p>Four metrics were designed to evaluate the model's performances: pair-wise consistency, monotonicity consistency, empirical consistency, and empirical monotonicity consistency.The model's performance is assessed differently by each of these metrics.These metrics are defined shortly, with an example demonstrated in Table 6.We begin by assuming that each prompt P i jks is instantiated using the CHOICES set C i , i ∈ {1, 2} the NUMBERS set N j , j ∈ {1, 2} and one of the intervals I k , k ∈ {below low, above low, between low and high, below low or above high, below high, above high} from INTERVAL, which is constructed using one of the confidence interval points P s , s ∈ {0.05, 0.275, 0.5, 0.725, 0.95} from Points.</p>
<p>• Pair-wise consistency: Consider two prompts, P i jks and P i jk ′ s , that are constructed using the same CHOICES set C i , NUMBERS set N j , and confidence interval point P s , but with different interval such that k, k ′ ∈ {below low, above low} or {between low and high, below low or above high} or {below high, above high}.The model's response is deemed correct if and only if it selects any pair of complementary choices for these prompts, regardless of their order.The three pairs of complementary choices are defined as {is almost certainly and is almost certainly not}, {is likely to be and is unlikely to be}, and {is maybe and is maybe}.In total, we have 180 such prompt pairs.Each pair is marked with a 1 if a model answered correctly and a 0 if answered incorrectly.We report the average based on these 180 prompt pairs.</p>
<p>• Monotonicity consistency: For a sequence of five prompts P i jks 1 , P i jks 2 , P i jks 3 , P i jks 4 , and P i jks 5 , that are constructed using the same CHOICES set C i , NUMBERS set N j , and interval I k , but with a sequence of increasing confidence interval points, such that s 1 = 0.05, s 2 = 0.275, s 3 = 0.5, s 4 = 0.725, and s 5 = 0.95, the model's response is deemed correct if and only if it selects any sequences of choices that represent probabilities in either an increasing or decreasing order.Specifically, if k ∈ {below low, below low or above high, above high}, the correct order is decreasing, and if k ∈ {above low, between low and high, below high}, the correct order is increasing.Additionally, the rule of increasing or decreasing order is non-exclusive, indicating that the occurrence of two identical WEPs choices does not violate this principle.For example, the sequence of responses (is almost certainly, is almost certainly, is maybe, is maybe, is unlikely to be) counts as a decreasing sequence.In total, we have 72 such sequences of prompts.Each is marked with a 1 if a model answered correctly and a 0 if answered incorrectly.We report the average based on these 72 sequences of prompts.</p>
<p>• Empirical consistency: For any prompt P i jks , we are able to use the NUMBERS set N j , the interval I k , and the confidence interval point P s associated with that prompt to calculate the exact proportion of numbers that fall within a specified interval.For example, given the first NUMBERS set (116, 93, 94, 89, 108, 76, 117, 92, 103, 97, 114, 79, 96, 96, 111, 89,  98, 91, 100, 105) and the interval below 99, which is below low instantiated using the confidence interval point 0.05, there are 12 numbers that fall into the interval.Therefore, the proportion (corresponding to a frequentist interpretation of probability) is 0.6.Additionally, for any WEP choice that is provided to GPT-4, we obtained the numerical probability range associated previously with that WEP.Specifically, for the 3-choices CHOICES set, the range for A.is likely to be is (0.61, 1], B.is maybe is [0.41, 0.61], and C.is unlikely to be is [0, 0.41).For the 5-choices CHOICES set, the range for A.is most certainly is (0.92, 1], B.is likely to be is (0.61, 0.92], C.is maybe is [0.41, 0.61], D.is unlikely to be is [0.13, 0.41), and E.is almost certainly not is [0, 0.13).Based on the calculated proportion and the numerical probability range tied to each WEP choice, we establish the ground truth as the choice whose range encompasses the proportion.In total, we have 360 prompts.Each is marked with a 1 if a model answered correctly and a 0 if answered incorrectly.We report the average based on the 360 prompts.</p>
<p>• Empirical monotonicity consistency: For the sequence of two prompts, P i jks and P i jk ′ s , that are constructed using the same CHOICES set C i , NUMBERS set N j , and interval I k , but with a sequence of continuing confidence interval points (i.e., 0.05 and 0.275, 0.275 and 0.5, 0.5 and 0.725, 0.725 and 0.95), the model's response is deemed correct if and only if it selects any sequence of two choices, such that the sequence represents probabilities in an increasing, decreasing, or constant order.This order is determined by first finding out the ground truth for each prompt, which is accomplished in the same way as in the empirical consistency.Then, if the correct choice for the first prompt (P i jks ) represents a probability greater than that of the second choice (P i jk ′ s ), the correct order is decreasing.Conversely, if it is lower, the correct order is increasing.If the correct choice for the first and second prompts is the same, the correct order is constant.</p>
<p>In total, we have 288 such prompt pairs.Each is marked with a 1 if a model answered correctly and a 0 if answered incorrectly.We report the average based on the 288 prompt pairs.</p>
<p>Experimental Setup</p>
<p>In the previous experiments, multiple LLMs were studied to investigate the differences in interpreting WEPs between humans and LLMs.However, we only focus on one specific LLM here: GPT-4, as it is among the most powerful LLMs and represents the latest advancements in the field at the time of writing.Similar to the first objective, we used the OpenAI Application Programming Interface (API) to access the GPT-4 model, specifically the 'gpt-4-0613' version.All messages sent to the API are constructed without the system message.The prompts are sent only as the role of the 'user.'All experiments use a temperature of 0 to maximize reproducibility.All of the 360 normal prompts and 360 CoT prompts are sent to GPT-4 through the official API and responses are recorded.To produce the random performance results for each metric, we randomly choose one choice between the available choices.This process is repeated 10 times, and the final random performance is obtained by averaging the scores for the 10 replications.</p>
<p>= 15, U(N 1 = 123, N 2 = 15) = 407.5,p &lt; 0.01), 'we doubt' (AMD = 10, U(N 1 = 123, N 2 = 15) = 489, p &lt; 0.01), 'unlikely' (AMD = 10, U(N 1 = 123, N 2 = 15) = 468, p &lt; 0.01), and 'little chance' (AMD = 10, U(N 1 = 123, N 2 = 15) = 602.5,p = 0.02).</p>
<p>Figure 1 .
1
Figure 1.Distributions of probabilities (expressed as percentages on the x-axis) on 17 words of estimative probability (WEPs) elicited from six sources: human, GPT-3.5-English,GPT-3.5-Chinese,GPT-4-English, GPT-4-Chinese, and ERNIE-4-Chinese.The graphs on the left feature an x-axis range of 0 to 40 and include 8 WEPs on the y-axis, while the graphs on the right have an x-axis range of 40 to 100 and present the other 9 words on the y-axis.Outliers are omitted from the box-and-whisker plots, and there is zero variability in the cases where only -is indicated.</p>
<p>Figure 2 .
2
Figure 2. Distributions of probabilities (expressed as percentages on the x-axis) on 17 words of estimative probability (WEPs) elicited from five sources: human, GPT-3.5-Male,GPT-3.5-Female,GPT-4-Male, and GPT-4-Female.The graphs on the left feature an x-axis range of 0 to 40 and include 8 WEPs on the y-axis, while the graphs on the right have an x-axis range of 40 to 100 and present the other 9 words on the y-axis.Outliers are omitted from the box-and-whisker plots, and there is zero variability in the cases where only -is indicated.</p>
<p>Figure 3 .
3
Figure 3. Distributions of probability estimations on 12 WEPs (divided into three categories: low, moderate, high probability WEPs) by GPT-3.5 and 4. Each graph shows the estimations given under Male, Female, and Concise Narrative Context (CNC) context settings.The last of these is gender-neutral and serves as a reference.The graphs with low probability words feature an x-axis range of 0 to 40, while the other graphs have an x-axis range of 40 to 100.</p>
<p>Figure 4 .
4
Figure 4.A heat map displaying the Kullback-Leibler (KL) divergence between various comparison pairs on 17 words of estimative probability.These pairs are (1) ERNIE-4.0 (prompted in Chinese) and humans, (2) GPT-3.5 or GPT-4 which is prompted using English and Chinese, (3) GPT-3.5 or GPT-4 compared with ERNIE-4.0 (all prompted in Chinese).The intensity of the color within each cell corresponds to the KL divergence values, with darker colors indicating higher divergence.<em>, </em>*, and *** represent significant statistical significance for the Mann-Whitney U test at confidence levels of 90%, 95%, and 99% levels, respectively.Supplementary Information Figures S19-S21 contain the precise Kolmogorov-Smirnov (KS) statistics used to assess the significance of these divergences.</p>
<p>Figure 7 .
7
Figure 7.The pipeline for constructing the dataset.Three scenario templates (Height, Score, and Sound) are shown in part (a).Each template comes with three controls: CHOICES (b), NUMBERS (c), and INTERVALS (e).Each control offers some possible values.Points (d) contain values that are being used by INTERVALS.One fully constructed example is shown in (f).</p>
<p>•</p>
<p>Male-Centric Narrative Context (MCNC): This mirrors the FCNC in narrative structure but replaces 'She' with 'He' as the subject, thus providing a comparative perspective on gender-based narrative interpretation.</p>
<h1>templatesAverage lengthtemplateExample templateCNC157.1They will {} launch before us.Given the diverse sources of the intelligence report, it is {}ENC1124.3a mistake that this piece of information was overlooked, though there are indications that it could have been due toa human error.FCNC/MCNC 108.6She / He {} orders the same dish at that restaurant.</h1>
<p>Table 1 .
1
An overview of four context settings that are compatible with different WEPs.Note that {} in Example columns represents a placeholder for any WEP.Tables2, 3, and 4 provide the full list.</p>
<p>The film festival {} attracts a large audience.They will {} launch before us.The local concert {} sells out quickly.The charity gala {} raises significant funds.The art exhibition {} receives positive reviews.That antique fair {} unveils rare collectibles.The mountain trail {} offers breathtaking views at dawn.The computer {} malfunctions when I have an important task to complete.The museum {} gets crowded on weekends.They are {} moving to Spain for the summer.It is {} a military airfield.The restaurant is {} the cheapest option available.</p>
<p>Table 2 .
2
The full list of Concise Narrative Context (CNC) templates.{} represents a placeholder for possible WEPs.</p>
<p>Table 3 .
3
The full list of Extended Narrative Context (ENC) templates.{} represents a placeholder for possible WEPs.</p>
<p>Table 5 .
5
English Chinese The film festival {} attracts a large audience.电影节{}吸引大量观众 They will {} launch before us.他们{}在我们之前发布 The local concert {} sells out quickly.当地音乐会{}很快售罄 The charity gala {} raises significant funds.慈善晚会{}筹集到大量资金 The art exhibition {} receives positive reviews.艺术展览{}收到积极评价 That antique fair {} unveils rare collectibles.古董展{}展示稀有收藏品 The mountain trail {} offers breathtaking views at dawn.山道{}在黎明时分提供令人叹为观止的景色 The computer {} malfunctions when I have an important task to complete.当我有重要任务要完成时，计算机{}出现故障 The museum {} gets crowded on weekends.博物馆{}在周末拥挤 They are {} moving to Spain for the summer.他们{}去西班牙度过夏天 It is {} a military airfield.它{}是军用机场 The restaurant is {} the cheapest option available.这家餐厅{}是最便宜的选择 The theory is {} the only explanation for the phenomenon.这个理论{}是现象的唯一解释 This ingredient is {} necessary for the recipe.这个成分{}在食谱中是必要的 Based on the weather forecast, it will {} rain tomorrow.根据天气预报，明天{}会下雨 The full list of the English version of Concise Narrative Context (CNC) templates, as well as the corresponding Chinese version.{} represents a placeholder for possible WEPs.</p>
<p>/17
Data availability statementAll data generated or analyzed during this study are included in this published article [and its supplementary information files].Additional informationAccession codes: Not Applicable; Competing interests: The authors have no competing interests to declare.
Verbal versus numerical probabilities: Efficiency, biases, and the preference paradox. I Erev, B L Cohen, Organ. behavior human decision processes. 451990</p>
<p>Do people really prefer verbal probabilities?. M Juanchich, M Sirota, Psychol. research. 842020</p>
<p>Writing without conviction? hedging in science research articles. K Hyland, Appl. linguistics. 171996</p>
<p>Linguistic hedging in the light of politeness theory. G R Vlasyan, Eur. Proc. Soc. Behav. Sci. 2018</p>
<p>S M Friedman, S Dunwoody, C L Rogers, Communicating uncertainty: Media coverage of new and controversial science. Routledge2012</p>
<p>Words of estimative probability. S Kent, Stud. intelligence. 81964</p>
<p>How probable is probable? a numerical translation of verbal probability expressions. R Beyth-Marom, J. forecasting. 11982</p>
<p>Handling and mishandling estimative probability: Likelihood, confidence, and the search for bin laden. J A Friedman, R Zeckhauser, Intell. Natl. Secur. 302015</p>
<p>Radiologist preferences, agreement, and variability in phrases used to convey diagnostic certainty in radiology reports. A B Shinagare, J. Am. Coll. Radiol. 162019</p>
<p>Measuring the vague meanings of probability terms. T S Wallsten, D V Budescu, A Rapoport, R Zwick, B Forsyth, J. Exp. Psychol. Gen. 1151986</p>
<p>How likely is that chance of thunderstorms? a study of how national weather service forecast offices use words of estimative probability and what they mean to the public. E D Lenhardt, J. Oper. Meteorol. 2020</p>
<p>Handbook for decisions analysis. S Barclay, 1977</p>
<p>Perception of probability words. W Fagen-Ulmschneider, 2019</p>
<p>A survey of large language models. W X Zhao, arXiv:2303.182232023arXiv preprint</p>
<p>. J Achiam, arXiv:2303.087742023Gpt-4 technical report. arXiv preprint</p>
<p>Llama: Open and efficient foundation language models. H Touvron, arXiv:2302.139712023arXiv preprint</p>
<p>Openai's gpt-3.5-turbo. </p>
<p>Benchmarking large language models for news summarization. T Zhang, Transactions Assoc. for Comput. Linguist. 122024</p>
<p>Llms for customer service and support. </p>
<p>Shared interest: Measuring human-ai alignment to identify recurring patterns in model behavior. A Boggust, B Hoover, A Satyanarayan, H Strobelt, Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems. the 2022 CHI Conference on Human Factors in Computing Systems2022</p>
<p>Ai alignment: A comprehensive survey. J Ji, arXiv:2310.198522023arXiv preprint</p>
<p>Artificial intelligence, values, and alignment. Minds machines. I Gabriel, 202030</p>
<p>Human-aligned artificial intelligence is a multiobjective problem. P Vamplew, R Dazeley, C Foale, S Firmin, J Mummery, Ethics Inf. Technol. 202018</p>
<p>Baidu's ernie-4. </p>
<p>Prompting large language model for machine translation: A case study. B Zhang, B Haddow, A Birch, International Conference on Machine Learning. PMLR2023</p>
<p>Findings of the 2023 conference on machine translation (wmt23): Llms are here but not quite there yet. T Kocmi, Proceedings of the Eighth Conference on Machine Translation. the Eighth Conference on Machine Translation2023</p>
<p>Abstractive summarization of large collections of scientific articles. P Zakkas, S Verberne, J Zavrel, Sumblogger, European Conference on Information Retrieval. Springer2024</p>
<p>S Takeshita, T Green, I Reinig, K Eckert, S P Ponzetto, Aclsum, arXiv:2403.05303A new dataset for aspect-based summarization of scientific publications. 2024arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, Adv. Neural Inf. Process. Syst. 352022</p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, arXiv:2203.111712022arXiv preprint</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Adv. neural information processing systems. 352022</p>
<p>Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. H Trivedi, N Balasubramanian, T Khot, A Sabharwal, arXiv:2212.105092022arXiv preprint</p>
<p>Probing neural language models for understanding of words of estimative probability. D Sileo, M.-F Moens, arXiv:2211.033582022arXiv preprint</p>
<p>A Rogers, O Kovaleva, A Rumshisky, A primer in bertology: What we know about how bert works. 20218</p>
<p>J Devlin, M.-W Chang, K Lee, K Toutanova, Bert, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. 2018arXiv preprint</p>
<p>Using cognitive psychology to understand gpt-3. M Binz, E Schulz, Proc. Natl. Acad. Sci. Natl. Acad. Sci2023120e2218523120</p>
<p>Aligning large language models with human: A survey. Y Wang, arXiv:2307.129662023arXiv preprint</p>
<p>. 16/17Harv. Bus. Rev. 2018</p>
<p>On information and sufficiency. The annals mathematical statistics. S Kullback, R A Leibler, 195122</p>
<p>On a test of whether one of two random variables is stochastically larger than the other. The annals mathematical statistics. H B Mann, D R Whitney, 1947</p>            </div>
        </div>

    </div>
</body>
</html>
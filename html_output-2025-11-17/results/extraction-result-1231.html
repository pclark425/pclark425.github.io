<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1231 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1231</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1231</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-273969883</p>
                <p><strong>Paper Title:</strong> World Models: The Safety Perspective</p>
                <p><strong>Paper Abstract:</strong> With the proliferation of the Large Language Model (LLM), the concept of World Models (WM) has recently attracted a great deal of attention in the AI research community, especially in the context of AI agents. It is arguably evolving into an essential foundation for building AI agent systems. A WM is intended to help the agent predict the future evolution of environmental states or help the agent fill in missing information so that it can plan its actions and behave safely. The safety property of WM plays a key role in their effective use in critical applications. In this work, we review and analyze the impacts of the current state-of-the-art in WM technology from the point of view of trustworthiness and safety based on a comprehensive survey and the fields of application envisaged. We provide an in-depth analysis of state-of-the-art WMs and derive technical research challenges and their impact in order to call on the research community to collaborate on improving the safety and trustworthiness of WM.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1231.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1231.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RSSM / Dreamer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent State Space Model (RSSM) / Dreamer family</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Latent-space recurrent world model combining deterministic and stochastic latent variables used to predict future states for model-based RL and planning (e.g., Dreamer variants).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RSSM / Dreamer</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A latent dynamics model where visual observations are encoded to a latent space and temporal evolution is modeled with recurrent state-space dynamics that include deterministic and stochastic components; ties into policy learning via imagination/planning in latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (recurrent latent dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari and continuous control benchmarks; embodied robot learning</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Agent reward / environment score in RL benchmarks; prediction loss in latent/data space (reconstruction / next-state prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Predominantly a black-box neural representation; latent variables are not directly human-interpretable though structure (deterministic vs stochastic) provides some conceptual separation.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Latent-space inspection and reconstruction; no mechanistic interpretability demonstrated in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>RNN/LSTM-based latent models were historically limited by sequence length and memory; no exact costs reported in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>More sample-efficient than many model-free approaches on some simulated tasks (cited historically), but transformer methods later shown to handle longer contexts better.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Reported in original Dreamer/RSSM literature to achieve strong performance on Atari / control tasks; this survey does not report numeric scores.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Latent imagination supports policy learning effectively in many simulated domains, but generalization to open or unseen real-world settings is limited.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Latent compactness aids efficiency and planning but can hide failure modes (distribution shift); recurrent structure limits parallelization and long-context handling.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use of stochastic + deterministic latents; recurrent state-space dynamics; training with reconstruction and prediction losses to enable planning in latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>RNN-based latent models historically outperformed simple model-free baselines in sample efficiency; transformers later improved long-context modeling and parallelism.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends hybrid approaches and better uncertainty estimation; no single optimal numeric configuration given.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'World Models: The Safety Perspective', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1231.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1231.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SimPLe</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simulated Policy Learning (SimPLe)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LSTM-based model-based RL approach that learns to predict future state embeddings from pixels for planning and policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SimPLe</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses an LSTM predictive model over latent encodings of pixel inputs to produce next-state embeddings for planning and policy improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (RNN/LSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari-style image-based RL environments</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Prediction/reconstruction loss on latent/data space; downstream agent reward</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Black-box RNN-based latent model; interpretability limited to qualitative reconstruction checks.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Reconstruction visualization; no interpretable symbolic structure reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Constrained by RNN sequence processing and memory for long rollouts; survey notes sequence length limits.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Provided early model-based sample-efficiency gains over model-free baselines on simple tasks; less scalable than transformer approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Useful for sample-efficient learning in simulated domains but limited in scalability/generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Good sample efficiency vs limited sequence length, parallelism, and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>LSTM for temporal modeling, CNN encoder for pixels, training from collected trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared favorably to naive model-free approaches on small-scale benchmarks; outpaced by later transformer-based world models for long contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>No explicit optimal configuration; motivates later transformer adoption for longer contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'World Models: The Safety Perspective', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1231.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1231.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MILE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model-based Imitation Learning for Urban Driving (MILE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based imitation learning approach for end-to-end autonomous driving that uses gated recurrent units and both stochastic and deterministic latent variables to represent world state evolution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MILE</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder produces latent state with stochastic and deterministic components; gated recurrent units model temporal evolution; trained via imitation on observation-action pairs for driving.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (RNN-based, imitation learning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Autonomous driving / urban driving</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Imitation loss (matching expert actions) and downstream safety metrics such as traffic violations and accident rates when used in closed-loop simulation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Architectural separation of stochastic/deterministic latents offers some structural interpretability but overall model remains largely black-box.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not specified beyond latent representation inspection.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>RNN-based recurrent costs; survey does not list explicit compute or parameter counts.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Designed to enable end-to-end imitation; no comparison numbers provided, but RNN temporal models are generally less parallelizable than transformer variants.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Used in the paper's CARLA merging scenario experiments; agents controlled by MILE experienced an identified collision at t = 3.2s in a tested configuration (demonstrating safety failure).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>While useful for imitation-style driving, the paper shows that MILE-driven agents can exhibit unsafe behavior in rare/unseen scenarios due to world-model misunderstanding of other agents' intentions.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Imitation training provides realistic behaviors in-distribution but poor generalization under distributional shift; fidelity of latent prediction did not prevent unsafe policy outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Gated recurrent units, stochastic+deterministic latent variables, end-to-end imitation loss.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared in spirit to other latent/world-model-driven driving approaches; demonstrated empirical safety vulnerabilities unlike what would be required for safety-critical deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends augmenting such models with uncertainty estimation, symbolic guardrails, and benchmarks; no exact configuration provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'World Models: The Safety Perspective', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1231.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1231.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TrafficGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TrafficGen (Learning to generate diverse and realistic traffic scenarios)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generative model for producing traffic scenarios in BEV that uses a multi-context gating decoder and MLPs to place vehicles and forecast motion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TrafficGen</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generative decoder architecture that conditions on scene context to generate positions and motions of multiple vehicles, using per-task MLP predictors and a multi-context gating mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>generative world model (data-driven traffic scenario generator)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Traffic scenario synthesis for autonomous driving</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Scenario realism metrics: MMD, mADE, mFDE, traffic rule violation rates, scenario collision rate (SCR); also physical plausibility checks (drivable area occupancy).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Model is generative and largely black-box; outputs can be inspected visually in BEV but internal representations not inherently interpretable.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visual inspection of generated BEV scenes; no mechanistic explanations provided.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not quantified; generative models for multi-agent scenes can be moderate to high depending on architecture size.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>No explicit efficiency comparisons given; designed for realistic scenario generation rather than low compute.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Survey shows failure example where TrafficGen generated vehicles placed outside drivable regions and assigned out-of-area trajectories (Fig. 2a), indicating fidelity issues impacting downstream safety use.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Useful for producing diverse scenarios for testing/simulation, but faulty generations (e.g., non-drivable placements) can mislead system evaluation or training.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Generative diversity vs physical/social plausibility â€” models that prioritize diversity can produce unrealistic or unsafe scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Multi-context gating decoder; separate MLPs for placement and motion forecasting.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Contrasted with other traffic generators (CTG, LCTGen); shown to produce some implausible scenarios in practice per the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Survey recommends adding rule-based or neuro-symbolic guardrails and quantitative uncertainty to improve reliability of generated scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'World Models: The Safety Perspective', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1231.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1231.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CTG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CTG (Controllable Traffic Generation / Conditional Traffic Generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A diffusion-based trajectory and scene generator for traffic that produces vehicle trajectories similar to human behavior and can be constrained by explicit physical/behavioral rules at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CTG</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses diffusion models to generate vehicle trajectories conditioned on scene context; enforces explicit rules during testing to keep trajectories realistic and physically feasible.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>generative world model (diffusion-based trajectory generator)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Traffic scene and multi-agent trajectory generation for autonomous driving</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Trajectory similarity metrics (e.g., distributional measures), mADE/mFDE, collision/violation rates; realism assessed by physical feasibility constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Diffusion trajectory generator is largely black-box, though controllable via conditioning prompts and explicit rule checks.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Test-time rule enforcement; conditioning inputs and local prompts to bias generations.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Diffusion-based generation is computationally heavier than autoregressive token decoding in many settings; exact costs not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Provides realistic trajectories but can require heavier sampling/inference than faster generative alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Survey highlights CTG producing a scenario where participant behavior heavily violates basic traffic rules (Fig. 2b), showing fidelity gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Capable of producing human-like trajectories, but fidelity issues can lead to unsafe generated scenarios unless constrained; adding explicit rules at test time is used to mitigate this.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Higher expressive power and realism from diffusion vs sampling cost and occasional implausible outputs without constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Diffusion backbone for trajectories; explicit rules during testing; option to integrate LLMs for language control (in CTG++).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to GAN/transformer generators, diffusion offers controllability and sample quality but at computational and sampling cost.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests combining diffusion generation with rule-based constraints and language control (LLM) to balance fidelity and controllability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'World Models: The Safety Perspective', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1231.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1231.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DriveDreamer / DriveDreamer2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DriveDreamer (and DriveDreamer 2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Diffusion-based world model for high-quality plausible driving scenario video generation; DriveDreamer 2 integrates LLMs for language-conditioned scene specification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DriveDreamer / DriveDreamer 2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses diffusion models to learn comprehensive representations of driving environments and predict future states, trained in a two-stage process to capture traffic constraints; DriveDreamer2 adds LLM-based language conditioning for specifying agent trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>generative world model (diffusion + optional LLM conditioning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Driving video generation / scenario synthesis for autonomous driving</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Video-generation metrics (FID, FVD, IS, PSNR) and driving-specific metrics (mADE/mFDE, traffic rule violation rates, SCR); temporal consistency checks.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Generative diffusion model with limited interpretability; survey gives direct failure examples where generated frames contain implausible artifacts (e.g., impossible road markings) and temporal inconsistencies (objects disappearing across frames).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visual inspection of generated frames and temporal sequences; no internal mechanistic explanations provided.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Diffusion model architectures and two-stage training imply significant compute; exact resource figures not provided in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Produces high-quality visual output but at cost of computationally expensive sampling compared to lighter generative models; no numerical comparison provided.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Used in survey examples: DriveDreamer generated videos containing implausible road markings and temporal inconsistencies where vehicles disappear between frames (Fig. 2c-e), indicating fidelity lapses that reduce downstream safety utility.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High visual plausibility does not guarantee physically or temporally consistent world representations needed for safe planner training or evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Visual fidelity vs physical/temporal correctness and sampling cost; adding LLMs increases controllability but also complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Two-stage diffusion training to learn scene constraints; option to integrate LLM prompts for trajectory description; emphasis on BEV/4D consistency in related works.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to transformer and RNN-based world models, diffusion yields better visual generation quality but struggles with temporal consistency and is heavier computationally.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Survey recommends adding controllable guardrails, uncertainty quantification, and symbolic priors to improve safety and reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'World Models: The Safety Perspective', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1231.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1231.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Decision Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Decision Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-style autoregressive transformer that treats trajectories (states, actions, returns) as a sequence to perform decision-making via sequence modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Decision Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Serializes state, action and return information into token sequences and uses an autoregressive transformer to generate actions conditioned on desired returns.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer-based predictive world/model-of-decision</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari, control tasks, offline RL framing</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Downstream task return / reward; reconstruction / next-token prediction loss</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Transformer attention affords some interpretability (attention maps), but internal representations remain mostly opaque.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Sequence attention visualization and ablations in original literature; survey mentions transformer sample efficiency on long contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Transformer training scales well with parallelism; better long-context handling than RNNs but can be parameter/time intensive for very long sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Survey states transformers perform better than RNNs for longer contexts and are more parallelizable.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Reported in literature to achieve promising results on sequence-modeled RL tasks; no numerical values in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Enables framing of RL as sequence modeling, beneficial for offline settings; fidelity of world dynamics modeling is implicit through sequence prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Improved long-context fidelity and parallelism at cost of transformer compute and data requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Autoregressive tokenization of trajectories and use of positional/episodic timestep embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperforms RNNs on long-context modeling; contrasted with generative world-model decoders for explicit next-state prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Survey suggests transformers are favorable for sample efficiency and long-context tasks but emphasizes adding uncertainty and guardrails for safety-critical use.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'World Models: The Safety Perspective', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1231.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1231.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GATO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GATO (A generalist agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-modal, multi-task, multi-embodiment transformer that serializes heterogeneous modality tokens and is trained to reconstruct masked inputs, serving as a generalist world/agent model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GATO</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Flattens multi-modal inputs into a token sequence and trains a transformer to perform many tasks by predicting masked tokens, enabling a shared model for perception, control, and generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer-based generalist world/agent model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Multiple: robotics, games, language, vision</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Task-specific success rates and reconstruction accuracy; downstream task performance on multiple benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Transformer-based; some interpretability via attention but overall black-box; multi-task nature complicates attribution of representations.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Standard transformer analysis (attention visualization) referenced; survey suggests generalist approach but not mechanistic interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Large transformer compute; training across many tasks requires significant resources though specifics not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Offers multi-task efficiency via shared parameters compared to separate specialized models, but at cost of scale.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Reported in its own literature to handle diverse tasks; survey does not provide numeric results.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Provides broad utility across domains but raises concerns about safety/trustworthiness when used as world models without uncertainty quantification and guardrails.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Generality vs specialized fidelity and interpretability; larger multi-task models harder to audit.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Flat tokenization of diverse modalities; masked reconstruction training objective.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Contrasts with task-specific world models that may achieve higher fidelity per task; GATO emphasizes breadth over task-optimal fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Survey recommends augmenting generalist models with mechanistic explainability and symbolic constraints for safety.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'World Models: The Safety Perspective', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1231.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1231.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CWM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Counterfactual World Model (CWM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-like visual world model trained with structured mask recovery to learn key scene transition information and enable counterfactual visual probing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Counterfactual World Model (CWM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses structured mask recovery training to focus on critical visual markers (keypoints, optical flow, object segments) and supports counterfactual prompting to analyze perturbed responses in a zero-shot manner.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer-based visual predictive model with counterfactual probing</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Visual scene transition prediction, general visual world modeling</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction/transition prediction accuracy on masked recovery tasks; downstream predictive utility in vision tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Designed for counterfactual probing, offering better interpretability through examining responses to structured perturbations (i.e., more mechanistic insight than pure black-box generative models).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Counterfactual prompts and structured mask recovery to reveal which visual markers drive transitions; zero-shot analysis of perturbed model responses.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Transformer-scale compute for masked recovery; exact resource numbers not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Provides targeted interpretability and robustness benefits relative to vanilla autoregressive models, but with transformer compute costs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Counterfactual capabilities are valuable for understanding and diagnosing predictions, increasing trustworthiness when used for visual world modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Adds interpretability/diagnostic power at the expense of additional training objectives and transformer compute.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Structured mask recovery objective; use of visual keypoints and segments as explicit prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>More amenable to interpretability than standard generative transformers; may be less generically powerful for unrestricted generation.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Survey highlights counterfactual probing as a promising avenue for trustworthy world models but does not give a numeric configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'World Models: The Safety Perspective', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1231.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e1231.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Copilot4D</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Copilot4D</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BEV-tokenized discrete-diffusion world model using VQ-VAE tokenization to predict future observations with discrete diffusion, designed to handle complex/unstructured sensor spaces efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Copilot4D</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses VQVAE to tokenize sensor observations into discrete tokens, then applies a discrete diffusion process to predict future BEV tokens enabling parallel token decoding and improved control.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>discrete-token diffusion-based world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Autonomous driving, multi-sensor BEV prediction</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Token prediction accuracy; downstream planning reward and trajectory safety metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Tokenization provides some modularity and may aid inspection at token level; core model remains neural and not fully interpretable.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Inspection of discrete token predictions and BEV token reconstructions; no deep mechanistic interpretability reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Discrete diffusion with VQ tokenization aims to improve parallel decoding efficiency; exact compute figures not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Designed to solve difficulties of unstructured observation spaces and improve parallel decoding over autoregressive multi-token decoders.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Discrete tokenization improves feasibility of predicting complex sensor data in parallel, which aids real-time planning, but physical consistency and safety checks remain necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Tokenization improves decoding efficiency but introduces quantization errors that can affect fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>VQVAE tokenization, discrete diffusion prediction, BEV token representation for spatial consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>More parallel and potentially more efficient at decoding than autoregressive token predictors; diffusion still carries sampling overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Survey suggests discrete-token strategies can improve efficiency for high-dimensional sensor spaces, but safety-focused evaluation is required.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'World Models: The Safety Perspective', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1231.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e1231.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>JEPA / I-JEPA / V-JEPA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Joint Embedding Predictive Architectures (JEPA) and instantiations I-JEPA / V-JEPA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Predictive latent-space architectures that compute losses in latent space (rather than data space) to learn robust world representations and mitigate model collapse.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>JEPA (I-JEPA, V-JEPA)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Models that learn to predict missing parts of inputs in latent embedding space via joint-embedding objectives, emphasizing latent prediction losses over pixel-level reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent predictive world model (joint-embedding)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Image and video representation learning; potential world modeling and pattern completion</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Latent prediction loss and downstream task performance for pattern completion and representation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latent-space approach may simplify analysis by focusing on embeddings; explicit interpretability not demonstrated in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Latent-space inspection and downstream task probes; no mechanistic interpretability provided.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Computationally efficient relative to pixel-space reconstruction since losses are computed in compact latent space; exact figures not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Argued to mitigate model collapse and be more efficient than data-space losses for some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>I-JEPA/V-JEPA shown (in their own works) to succeed at pattern completion tasks on images/videos; survey does not provide numeric benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Focusing on task-relevant latent prediction can improve efficiency and robustness for representation learning, but translation to full world-model fidelity for safety-critical planning is not guaranteed.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Latent-space objectives reduce compute and improve stability but may abstract away fine-grained physical details needed for safety.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Compute loss in latent space; joint embedding predictive objective; apply to images/videos for pattern completion.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Offers stability and efficiency advantages compared with pixel-space generative models, but may lose fine-grained, task-irrelevant details that some downstream tasks require.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Survey suggests JEPA-style losses are promising for efficient world representations but should be combined with task-specific checks and uncertainty estimates for safety.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'World Models: The Safety Perspective', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1231.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e1231.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WorldCloner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WorldCloner (Neuro-symbolic world model for open-world adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuro-symbolic approach that augments a learned world model with an explicit rule model to detect and update world rules when the environment changes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>WorldCloner</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Combines a learned neural world model with a rule module that detects changes in world rules (novelties) and updates symbolic representations to adapt behavior in open, dynamic worlds.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>hybrid world model (neuro-symbolic)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Open/dynamic grid-world-like environments (NovGrid) and adaptation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Adaptation success on novelty detection tasks and downstream agent performance; environment-specific rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Neuro-symbolic design improves interpretability by exposing explicit learned/updated rules; better traceability than pure neural models.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Rule extraction/inspection and symbolic representations alongside neural predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Combines neural training cost with lightweight symbolic inference; survey does not quantify resources.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Symbolic integration aims to improve adaptation efficiency when rules change compared to pure neural retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Demonstrated improved adaptation in the NovGrid environment per cited work; survey uses it as an example of adapting to open world novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Neuro-symbolic guardrails provide concrete benefits for reliability under distribution shift by enabling explicit rule updates.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Hybrid systems increase interpretability and robustness but add complexity in integration and may require hand-designed symbolic priors.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Explicit rule model in addition to neural model; ability to detect and update rules upon changes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Offers better adaptability and interpretability than pure neural world models at the cost of additional system complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Survey emphasizes integrating symbolic priors and retrieval-augmented knowledge to improve safety and trustworthy generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'World Models: The Safety Perspective', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning latent dynamics for planning from pixels <em>(Rating: 2)</em></li>
                <li>Dream to control: Learning behaviors by latent imagination <em>(Rating: 2)</em></li>
                <li>Model-based imitation learning for urban driving <em>(Rating: 2)</em></li>
                <li>Trafficgen: Learning to generate diverse and realistic traffic scenarios <em>(Rating: 2)</em></li>
                <li>Guided conditional diffusion for controllable traffic simulation <em>(Rating: 2)</em></li>
                <li>Drivedreamer: Towards real-world-driven world models for autonomous driving <em>(Rating: 2)</em></li>
                <li>Decision transformer: Reinforcement learning via sequence modeling <em>(Rating: 2)</em></li>
                <li>A generalist agent <em>(Rating: 1)</em></li>
                <li>Unifying (machine) vision via counterfactual world modeling <em>(Rating: 1)</em></li>
                <li>Learning unsupervised world models for autonomous driving via discrete diffusion <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1231",
    "paper_id": "paper-273969883",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "RSSM / Dreamer",
            "name_full": "Recurrent State Space Model (RSSM) / Dreamer family",
            "brief_description": "Latent-space recurrent world model combining deterministic and stochastic latent variables used to predict future states for model-based RL and planning (e.g., Dreamer variants).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "RSSM / Dreamer",
            "model_description": "A latent dynamics model where visual observations are encoded to a latent space and temporal evolution is modeled with recurrent state-space dynamics that include deterministic and stochastic components; ties into policy learning via imagination/planning in latent space.",
            "model_type": "latent world model (recurrent latent dynamics)",
            "task_domain": "Atari and continuous control benchmarks; embodied robot learning",
            "fidelity_metric": "Agent reward / environment score in RL benchmarks; prediction loss in latent/data space (reconstruction / next-state prediction)",
            "fidelity_performance": null,
            "interpretability_assessment": "Predominantly a black-box neural representation; latent variables are not directly human-interpretable though structure (deterministic vs stochastic) provides some conceptual separation.",
            "interpretability_method": "Latent-space inspection and reconstruction; no mechanistic interpretability demonstrated in paper.",
            "computational_cost": "RNN/LSTM-based latent models were historically limited by sequence length and memory; no exact costs reported in survey.",
            "efficiency_comparison": "More sample-efficient than many model-free approaches on some simulated tasks (cited historically), but transformer methods later shown to handle longer contexts better.",
            "task_performance": "Reported in original Dreamer/RSSM literature to achieve strong performance on Atari / control tasks; this survey does not report numeric scores.",
            "task_utility_analysis": "Latent imagination supports policy learning effectively in many simulated domains, but generalization to open or unseen real-world settings is limited.",
            "tradeoffs_observed": "Latent compactness aids efficiency and planning but can hide failure modes (distribution shift); recurrent structure limits parallelization and long-context handling.",
            "design_choices": "Use of stochastic + deterministic latents; recurrent state-space dynamics; training with reconstruction and prediction losses to enable planning in latent space.",
            "comparison_to_alternatives": "RNN-based latent models historically outperformed simple model-free baselines in sample efficiency; transformers later improved long-context modeling and parallelism.",
            "optimal_configuration": "Paper recommends hybrid approaches and better uncertainty estimation; no single optimal numeric configuration given.",
            "uuid": "e1231.0",
            "source_info": {
                "paper_title": "World Models: The Safety Perspective",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "SimPLe",
            "name_full": "Simulated Policy Learning (SimPLe)",
            "brief_description": "An LSTM-based model-based RL approach that learns to predict future state embeddings from pixels for planning and policy learning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "SimPLe",
            "model_description": "Uses an LSTM predictive model over latent encodings of pixel inputs to produce next-state embeddings for planning and policy improvement.",
            "model_type": "latent world model (RNN/LSTM)",
            "task_domain": "Atari-style image-based RL environments",
            "fidelity_metric": "Prediction/reconstruction loss on latent/data space; downstream agent reward",
            "fidelity_performance": null,
            "interpretability_assessment": "Black-box RNN-based latent model; interpretability limited to qualitative reconstruction checks.",
            "interpretability_method": "Reconstruction visualization; no interpretable symbolic structure reported.",
            "computational_cost": "Constrained by RNN sequence processing and memory for long rollouts; survey notes sequence length limits.",
            "efficiency_comparison": "Provided early model-based sample-efficiency gains over model-free baselines on simple tasks; less scalable than transformer approaches.",
            "task_performance": null,
            "task_utility_analysis": "Useful for sample-efficient learning in simulated domains but limited in scalability/generalization.",
            "tradeoffs_observed": "Good sample efficiency vs limited sequence length, parallelism, and generalization.",
            "design_choices": "LSTM for temporal modeling, CNN encoder for pixels, training from collected trajectories.",
            "comparison_to_alternatives": "Compared favorably to naive model-free approaches on small-scale benchmarks; outpaced by later transformer-based world models for long contexts.",
            "optimal_configuration": "No explicit optimal configuration; motivates later transformer adoption for longer contexts.",
            "uuid": "e1231.1",
            "source_info": {
                "paper_title": "World Models: The Safety Perspective",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "MILE",
            "name_full": "Model-based Imitation Learning for Urban Driving (MILE)",
            "brief_description": "A model-based imitation learning approach for end-to-end autonomous driving that uses gated recurrent units and both stochastic and deterministic latent variables to represent world state evolution.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "MILE",
            "model_description": "Encoder produces latent state with stochastic and deterministic components; gated recurrent units model temporal evolution; trained via imitation on observation-action pairs for driving.",
            "model_type": "latent world model (RNN-based, imitation learning)",
            "task_domain": "Autonomous driving / urban driving",
            "fidelity_metric": "Imitation loss (matching expert actions) and downstream safety metrics such as traffic violations and accident rates when used in closed-loop simulation",
            "fidelity_performance": null,
            "interpretability_assessment": "Architectural separation of stochastic/deterministic latents offers some structural interpretability but overall model remains largely black-box.",
            "interpretability_method": "Not specified beyond latent representation inspection.",
            "computational_cost": "RNN-based recurrent costs; survey does not list explicit compute or parameter counts.",
            "efficiency_comparison": "Designed to enable end-to-end imitation; no comparison numbers provided, but RNN temporal models are generally less parallelizable than transformer variants.",
            "task_performance": "Used in the paper's CARLA merging scenario experiments; agents controlled by MILE experienced an identified collision at t = 3.2s in a tested configuration (demonstrating safety failure).",
            "task_utility_analysis": "While useful for imitation-style driving, the paper shows that MILE-driven agents can exhibit unsafe behavior in rare/unseen scenarios due to world-model misunderstanding of other agents' intentions.",
            "tradeoffs_observed": "Imitation training provides realistic behaviors in-distribution but poor generalization under distributional shift; fidelity of latent prediction did not prevent unsafe policy outcomes.",
            "design_choices": "Gated recurrent units, stochastic+deterministic latent variables, end-to-end imitation loss.",
            "comparison_to_alternatives": "Compared in spirit to other latent/world-model-driven driving approaches; demonstrated empirical safety vulnerabilities unlike what would be required for safety-critical deployment.",
            "optimal_configuration": "Paper recommends augmenting such models with uncertainty estimation, symbolic guardrails, and benchmarks; no exact configuration provided.",
            "uuid": "e1231.2",
            "source_info": {
                "paper_title": "World Models: The Safety Perspective",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "TrafficGen",
            "name_full": "TrafficGen (Learning to generate diverse and realistic traffic scenarios)",
            "brief_description": "A generative model for producing traffic scenarios in BEV that uses a multi-context gating decoder and MLPs to place vehicles and forecast motion.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "TrafficGen",
            "model_description": "Generative decoder architecture that conditions on scene context to generate positions and motions of multiple vehicles, using per-task MLP predictors and a multi-context gating mechanism.",
            "model_type": "generative world model (data-driven traffic scenario generator)",
            "task_domain": "Traffic scenario synthesis for autonomous driving",
            "fidelity_metric": "Scenario realism metrics: MMD, mADE, mFDE, traffic rule violation rates, scenario collision rate (SCR); also physical plausibility checks (drivable area occupancy).",
            "fidelity_performance": null,
            "interpretability_assessment": "Model is generative and largely black-box; outputs can be inspected visually in BEV but internal representations not inherently interpretable.",
            "interpretability_method": "Visual inspection of generated BEV scenes; no mechanistic explanations provided.",
            "computational_cost": "Not quantified; generative models for multi-agent scenes can be moderate to high depending on architecture size.",
            "efficiency_comparison": "No explicit efficiency comparisons given; designed for realistic scenario generation rather than low compute.",
            "task_performance": "Survey shows failure example where TrafficGen generated vehicles placed outside drivable regions and assigned out-of-area trajectories (Fig. 2a), indicating fidelity issues impacting downstream safety use.",
            "task_utility_analysis": "Useful for producing diverse scenarios for testing/simulation, but faulty generations (e.g., non-drivable placements) can mislead system evaluation or training.",
            "tradeoffs_observed": "Generative diversity vs physical/social plausibility â€” models that prioritize diversity can produce unrealistic or unsafe scenarios.",
            "design_choices": "Multi-context gating decoder; separate MLPs for placement and motion forecasting.",
            "comparison_to_alternatives": "Contrasted with other traffic generators (CTG, LCTGen); shown to produce some implausible scenarios in practice per the survey.",
            "optimal_configuration": "Survey recommends adding rule-based or neuro-symbolic guardrails and quantitative uncertainty to improve reliability of generated scenarios.",
            "uuid": "e1231.3",
            "source_info": {
                "paper_title": "World Models: The Safety Perspective",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "CTG",
            "name_full": "CTG (Controllable Traffic Generation / Conditional Traffic Generation)",
            "brief_description": "A diffusion-based trajectory and scene generator for traffic that produces vehicle trajectories similar to human behavior and can be constrained by explicit physical/behavioral rules at test time.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "CTG",
            "model_description": "Uses diffusion models to generate vehicle trajectories conditioned on scene context; enforces explicit rules during testing to keep trajectories realistic and physically feasible.",
            "model_type": "generative world model (diffusion-based trajectory generator)",
            "task_domain": "Traffic scene and multi-agent trajectory generation for autonomous driving",
            "fidelity_metric": "Trajectory similarity metrics (e.g., distributional measures), mADE/mFDE, collision/violation rates; realism assessed by physical feasibility constraints.",
            "fidelity_performance": null,
            "interpretability_assessment": "Diffusion trajectory generator is largely black-box, though controllable via conditioning prompts and explicit rule checks.",
            "interpretability_method": "Test-time rule enforcement; conditioning inputs and local prompts to bias generations.",
            "computational_cost": "Diffusion-based generation is computationally heavier than autoregressive token decoding in many settings; exact costs not provided.",
            "efficiency_comparison": "Provides realistic trajectories but can require heavier sampling/inference than faster generative alternatives.",
            "task_performance": "Survey highlights CTG producing a scenario where participant behavior heavily violates basic traffic rules (Fig. 2b), showing fidelity gaps.",
            "task_utility_analysis": "Capable of producing human-like trajectories, but fidelity issues can lead to unsafe generated scenarios unless constrained; adding explicit rules at test time is used to mitigate this.",
            "tradeoffs_observed": "Higher expressive power and realism from diffusion vs sampling cost and occasional implausible outputs without constraints.",
            "design_choices": "Diffusion backbone for trajectories; explicit rules during testing; option to integrate LLMs for language control (in CTG++).",
            "comparison_to_alternatives": "Compared to GAN/transformer generators, diffusion offers controllability and sample quality but at computational and sampling cost.",
            "optimal_configuration": "Paper suggests combining diffusion generation with rule-based constraints and language control (LLM) to balance fidelity and controllability.",
            "uuid": "e1231.4",
            "source_info": {
                "paper_title": "World Models: The Safety Perspective",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "DriveDreamer / DriveDreamer2",
            "name_full": "DriveDreamer (and DriveDreamer 2)",
            "brief_description": "Diffusion-based world model for high-quality plausible driving scenario video generation; DriveDreamer 2 integrates LLMs for language-conditioned scene specification.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DriveDreamer / DriveDreamer 2",
            "model_description": "Uses diffusion models to learn comprehensive representations of driving environments and predict future states, trained in a two-stage process to capture traffic constraints; DriveDreamer2 adds LLM-based language conditioning for specifying agent trajectories.",
            "model_type": "generative world model (diffusion + optional LLM conditioning)",
            "task_domain": "Driving video generation / scenario synthesis for autonomous driving",
            "fidelity_metric": "Video-generation metrics (FID, FVD, IS, PSNR) and driving-specific metrics (mADE/mFDE, traffic rule violation rates, SCR); temporal consistency checks.",
            "fidelity_performance": null,
            "interpretability_assessment": "Generative diffusion model with limited interpretability; survey gives direct failure examples where generated frames contain implausible artifacts (e.g., impossible road markings) and temporal inconsistencies (objects disappearing across frames).",
            "interpretability_method": "Visual inspection of generated frames and temporal sequences; no internal mechanistic explanations provided.",
            "computational_cost": "Diffusion model architectures and two-stage training imply significant compute; exact resource figures not provided in survey.",
            "efficiency_comparison": "Produces high-quality visual output but at cost of computationally expensive sampling compared to lighter generative models; no numerical comparison provided.",
            "task_performance": "Used in survey examples: DriveDreamer generated videos containing implausible road markings and temporal inconsistencies where vehicles disappear between frames (Fig. 2c-e), indicating fidelity lapses that reduce downstream safety utility.",
            "task_utility_analysis": "High visual plausibility does not guarantee physically or temporally consistent world representations needed for safe planner training or evaluation.",
            "tradeoffs_observed": "Visual fidelity vs physical/temporal correctness and sampling cost; adding LLMs increases controllability but also complexity.",
            "design_choices": "Two-stage diffusion training to learn scene constraints; option to integrate LLM prompts for trajectory description; emphasis on BEV/4D consistency in related works.",
            "comparison_to_alternatives": "Compared to transformer and RNN-based world models, diffusion yields better visual generation quality but struggles with temporal consistency and is heavier computationally.",
            "optimal_configuration": "Survey recommends adding controllable guardrails, uncertainty quantification, and symbolic priors to improve safety and reliability.",
            "uuid": "e1231.5",
            "source_info": {
                "paper_title": "World Models: The Safety Perspective",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Decision Transformer",
            "name_full": "Decision Transformer",
            "brief_description": "A GPT-style autoregressive transformer that treats trajectories (states, actions, returns) as a sequence to perform decision-making via sequence modeling.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Decision Transformer",
            "model_description": "Serializes state, action and return information into token sequences and uses an autoregressive transformer to generate actions conditioned on desired returns.",
            "model_type": "transformer-based predictive world/model-of-decision",
            "task_domain": "Atari, control tasks, offline RL framing",
            "fidelity_metric": "Downstream task return / reward; reconstruction / next-token prediction loss",
            "fidelity_performance": null,
            "interpretability_assessment": "Transformer attention affords some interpretability (attention maps), but internal representations remain mostly opaque.",
            "interpretability_method": "Sequence attention visualization and ablations in original literature; survey mentions transformer sample efficiency on long contexts.",
            "computational_cost": "Transformer training scales well with parallelism; better long-context handling than RNNs but can be parameter/time intensive for very long sequences.",
            "efficiency_comparison": "Survey states transformers perform better than RNNs for longer contexts and are more parallelizable.",
            "task_performance": "Reported in literature to achieve promising results on sequence-modeled RL tasks; no numerical values in survey.",
            "task_utility_analysis": "Enables framing of RL as sequence modeling, beneficial for offline settings; fidelity of world dynamics modeling is implicit through sequence prediction.",
            "tradeoffs_observed": "Improved long-context fidelity and parallelism at cost of transformer compute and data requirements.",
            "design_choices": "Autoregressive tokenization of trajectories and use of positional/episodic timestep embeddings.",
            "comparison_to_alternatives": "Outperforms RNNs on long-context modeling; contrasted with generative world-model decoders for explicit next-state prediction.",
            "optimal_configuration": "Survey suggests transformers are favorable for sample efficiency and long-context tasks but emphasizes adding uncertainty and guardrails for safety-critical use.",
            "uuid": "e1231.6",
            "source_info": {
                "paper_title": "World Models: The Safety Perspective",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GATO",
            "name_full": "GATO (A generalist agent)",
            "brief_description": "A multi-modal, multi-task, multi-embodiment transformer that serializes heterogeneous modality tokens and is trained to reconstruct masked inputs, serving as a generalist world/agent model.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GATO",
            "model_description": "Flattens multi-modal inputs into a token sequence and trains a transformer to perform many tasks by predicting masked tokens, enabling a shared model for perception, control, and generation.",
            "model_type": "transformer-based generalist world/agent model",
            "task_domain": "Multiple: robotics, games, language, vision",
            "fidelity_metric": "Task-specific success rates and reconstruction accuracy; downstream task performance on multiple benchmarks.",
            "fidelity_performance": null,
            "interpretability_assessment": "Transformer-based; some interpretability via attention but overall black-box; multi-task nature complicates attribution of representations.",
            "interpretability_method": "Standard transformer analysis (attention visualization) referenced; survey suggests generalist approach but not mechanistic interpretability.",
            "computational_cost": "Large transformer compute; training across many tasks requires significant resources though specifics not reported.",
            "efficiency_comparison": "Offers multi-task efficiency via shared parameters compared to separate specialized models, but at cost of scale.",
            "task_performance": "Reported in its own literature to handle diverse tasks; survey does not provide numeric results.",
            "task_utility_analysis": "Provides broad utility across domains but raises concerns about safety/trustworthiness when used as world models without uncertainty quantification and guardrails.",
            "tradeoffs_observed": "Generality vs specialized fidelity and interpretability; larger multi-task models harder to audit.",
            "design_choices": "Flat tokenization of diverse modalities; masked reconstruction training objective.",
            "comparison_to_alternatives": "Contrasts with task-specific world models that may achieve higher fidelity per task; GATO emphasizes breadth over task-optimal fidelity.",
            "optimal_configuration": "Survey recommends augmenting generalist models with mechanistic explainability and symbolic constraints for safety.",
            "uuid": "e1231.7",
            "source_info": {
                "paper_title": "World Models: The Safety Perspective",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "CWM",
            "name_full": "Counterfactual World Model (CWM)",
            "brief_description": "A transformer-like visual world model trained with structured mask recovery to learn key scene transition information and enable counterfactual visual probing.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Counterfactual World Model (CWM)",
            "model_description": "Uses structured mask recovery training to focus on critical visual markers (keypoints, optical flow, object segments) and supports counterfactual prompting to analyze perturbed responses in a zero-shot manner.",
            "model_type": "transformer-based visual predictive model with counterfactual probing",
            "task_domain": "Visual scene transition prediction, general visual world modeling",
            "fidelity_metric": "Reconstruction/transition prediction accuracy on masked recovery tasks; downstream predictive utility in vision tasks.",
            "fidelity_performance": null,
            "interpretability_assessment": "Designed for counterfactual probing, offering better interpretability through examining responses to structured perturbations (i.e., more mechanistic insight than pure black-box generative models).",
            "interpretability_method": "Counterfactual prompts and structured mask recovery to reveal which visual markers drive transitions; zero-shot analysis of perturbed model responses.",
            "computational_cost": "Transformer-scale compute for masked recovery; exact resource numbers not provided.",
            "efficiency_comparison": "Provides targeted interpretability and robustness benefits relative to vanilla autoregressive models, but with transformer compute costs.",
            "task_performance": null,
            "task_utility_analysis": "Counterfactual capabilities are valuable for understanding and diagnosing predictions, increasing trustworthiness when used for visual world modeling.",
            "tradeoffs_observed": "Adds interpretability/diagnostic power at the expense of additional training objectives and transformer compute.",
            "design_choices": "Structured mask recovery objective; use of visual keypoints and segments as explicit prompts.",
            "comparison_to_alternatives": "More amenable to interpretability than standard generative transformers; may be less generically powerful for unrestricted generation.",
            "optimal_configuration": "Survey highlights counterfactual probing as a promising avenue for trustworthy world models but does not give a numeric configuration.",
            "uuid": "e1231.8",
            "source_info": {
                "paper_title": "World Models: The Safety Perspective",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Copilot4D",
            "name_full": "Copilot4D",
            "brief_description": "A BEV-tokenized discrete-diffusion world model using VQ-VAE tokenization to predict future observations with discrete diffusion, designed to handle complex/unstructured sensor spaces efficiently.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Copilot4D",
            "model_description": "Uses VQVAE to tokenize sensor observations into discrete tokens, then applies a discrete diffusion process to predict future BEV tokens enabling parallel token decoding and improved control.",
            "model_type": "discrete-token diffusion-based world model",
            "task_domain": "Autonomous driving, multi-sensor BEV prediction",
            "fidelity_metric": "Token prediction accuracy; downstream planning reward and trajectory safety metrics.",
            "fidelity_performance": null,
            "interpretability_assessment": "Tokenization provides some modularity and may aid inspection at token level; core model remains neural and not fully interpretable.",
            "interpretability_method": "Inspection of discrete token predictions and BEV token reconstructions; no deep mechanistic interpretability reported.",
            "computational_cost": "Discrete diffusion with VQ tokenization aims to improve parallel decoding efficiency; exact compute figures not provided.",
            "efficiency_comparison": "Designed to solve difficulties of unstructured observation spaces and improve parallel decoding over autoregressive multi-token decoders.",
            "task_performance": null,
            "task_utility_analysis": "Discrete tokenization improves feasibility of predicting complex sensor data in parallel, which aids real-time planning, but physical consistency and safety checks remain necessary.",
            "tradeoffs_observed": "Tokenization improves decoding efficiency but introduces quantization errors that can affect fidelity.",
            "design_choices": "VQVAE tokenization, discrete diffusion prediction, BEV token representation for spatial consistency.",
            "comparison_to_alternatives": "More parallel and potentially more efficient at decoding than autoregressive token predictors; diffusion still carries sampling overhead.",
            "optimal_configuration": "Survey suggests discrete-token strategies can improve efficiency for high-dimensional sensor spaces, but safety-focused evaluation is required.",
            "uuid": "e1231.9",
            "source_info": {
                "paper_title": "World Models: The Safety Perspective",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "JEPA / I-JEPA / V-JEPA",
            "name_full": "Joint Embedding Predictive Architectures (JEPA) and instantiations I-JEPA / V-JEPA",
            "brief_description": "Predictive latent-space architectures that compute losses in latent space (rather than data space) to learn robust world representations and mitigate model collapse.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "JEPA (I-JEPA, V-JEPA)",
            "model_description": "Models that learn to predict missing parts of inputs in latent embedding space via joint-embedding objectives, emphasizing latent prediction losses over pixel-level reconstruction.",
            "model_type": "latent predictive world model (joint-embedding)",
            "task_domain": "Image and video representation learning; potential world modeling and pattern completion",
            "fidelity_metric": "Latent prediction loss and downstream task performance for pattern completion and representation quality.",
            "fidelity_performance": null,
            "interpretability_assessment": "Latent-space approach may simplify analysis by focusing on embeddings; explicit interpretability not demonstrated in survey.",
            "interpretability_method": "Latent-space inspection and downstream task probes; no mechanistic interpretability provided.",
            "computational_cost": "Computationally efficient relative to pixel-space reconstruction since losses are computed in compact latent space; exact figures not provided.",
            "efficiency_comparison": "Argued to mitigate model collapse and be more efficient than data-space losses for some tasks.",
            "task_performance": "I-JEPA/V-JEPA shown (in their own works) to succeed at pattern completion tasks on images/videos; survey does not provide numeric benchmarks.",
            "task_utility_analysis": "Focusing on task-relevant latent prediction can improve efficiency and robustness for representation learning, but translation to full world-model fidelity for safety-critical planning is not guaranteed.",
            "tradeoffs_observed": "Latent-space objectives reduce compute and improve stability but may abstract away fine-grained physical details needed for safety.",
            "design_choices": "Compute loss in latent space; joint embedding predictive objective; apply to images/videos for pattern completion.",
            "comparison_to_alternatives": "Offers stability and efficiency advantages compared with pixel-space generative models, but may lose fine-grained, task-irrelevant details that some downstream tasks require.",
            "optimal_configuration": "Survey suggests JEPA-style losses are promising for efficient world representations but should be combined with task-specific checks and uncertainty estimates for safety.",
            "uuid": "e1231.10",
            "source_info": {
                "paper_title": "World Models: The Safety Perspective",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "WorldCloner",
            "name_full": "WorldCloner (Neuro-symbolic world model for open-world adaptation)",
            "brief_description": "A neuro-symbolic approach that augments a learned world model with an explicit rule model to detect and update world rules when the environment changes.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "WorldCloner",
            "model_description": "Combines a learned neural world model with a rule module that detects changes in world rules (novelties) and updates symbolic representations to adapt behavior in open, dynamic worlds.",
            "model_type": "hybrid world model (neuro-symbolic)",
            "task_domain": "Open/dynamic grid-world-like environments (NovGrid) and adaptation tasks",
            "fidelity_metric": "Adaptation success on novelty detection tasks and downstream agent performance; environment-specific rewards.",
            "fidelity_performance": null,
            "interpretability_assessment": "Neuro-symbolic design improves interpretability by exposing explicit learned/updated rules; better traceability than pure neural models.",
            "interpretability_method": "Rule extraction/inspection and symbolic representations alongside neural predictions.",
            "computational_cost": "Combines neural training cost with lightweight symbolic inference; survey does not quantify resources.",
            "efficiency_comparison": "Symbolic integration aims to improve adaptation efficiency when rules change compared to pure neural retraining.",
            "task_performance": "Demonstrated improved adaptation in the NovGrid environment per cited work; survey uses it as an example of adapting to open world novelty.",
            "task_utility_analysis": "Neuro-symbolic guardrails provide concrete benefits for reliability under distribution shift by enabling explicit rule updates.",
            "tradeoffs_observed": "Hybrid systems increase interpretability and robustness but add complexity in integration and may require hand-designed symbolic priors.",
            "design_choices": "Explicit rule model in addition to neural model; ability to detect and update rules upon changes.",
            "comparison_to_alternatives": "Offers better adaptability and interpretability than pure neural world models at the cost of additional system complexity.",
            "optimal_configuration": "Survey emphasizes integrating symbolic priors and retrieval-augmented knowledge to improve safety and trustworthy generation.",
            "uuid": "e1231.11",
            "source_info": {
                "paper_title": "World Models: The Safety Perspective",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning latent dynamics for planning from pixels",
            "rating": 2,
            "sanitized_title": "learning_latent_dynamics_for_planning_from_pixels"
        },
        {
            "paper_title": "Dream to control: Learning behaviors by latent imagination",
            "rating": 2,
            "sanitized_title": "dream_to_control_learning_behaviors_by_latent_imagination"
        },
        {
            "paper_title": "Model-based imitation learning for urban driving",
            "rating": 2,
            "sanitized_title": "modelbased_imitation_learning_for_urban_driving"
        },
        {
            "paper_title": "Trafficgen: Learning to generate diverse and realistic traffic scenarios",
            "rating": 2,
            "sanitized_title": "trafficgen_learning_to_generate_diverse_and_realistic_traffic_scenarios"
        },
        {
            "paper_title": "Guided conditional diffusion for controllable traffic simulation",
            "rating": 2,
            "sanitized_title": "guided_conditional_diffusion_for_controllable_traffic_simulation"
        },
        {
            "paper_title": "Drivedreamer: Towards real-world-driven world models for autonomous driving",
            "rating": 2,
            "sanitized_title": "drivedreamer_towards_realworlddriven_world_models_for_autonomous_driving"
        },
        {
            "paper_title": "Decision transformer: Reinforcement learning via sequence modeling",
            "rating": 2,
            "sanitized_title": "decision_transformer_reinforcement_learning_via_sequence_modeling"
        },
        {
            "paper_title": "A generalist agent",
            "rating": 1,
            "sanitized_title": "a_generalist_agent"
        },
        {
            "paper_title": "Unifying (machine) vision via counterfactual world modeling",
            "rating": 1,
            "sanitized_title": "unifying_machine_vision_via_counterfactual_world_modeling"
        },
        {
            "paper_title": "Learning unsupervised world models for autonomous driving via discrete diffusion",
            "rating": 1,
            "sanitized_title": "learning_unsupervised_world_models_for_autonomous_driving_via_discrete_diffusion"
        }
    ],
    "cost": 0.018373249999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>World Models: The Safety Perspective</p>
<p>Zifan Zeng zifan.zeng@tum.de 
RAMS Lab
Huawei Technologies Duesseldorf GmbH
MunichGermany</p>
<p>School of Computation, Information, and Technology
Technical University of Munich
GarchingGermany</p>
<p>Chongzhe Zhang chongzhe.zhang@campus.tu-berlin.de 
RAMS Lab
Huawei Technologies Duesseldorf GmbH
MunichGermany</p>
<p>Faculty of Electri-cal Engineering and Computer Science
Technical University of Berlin
BerlinGermany</p>
<p>Feng Liu 
RAMS Lab
Huawei Technologies Duesseldorf GmbH
MunichGermany</p>
<p>Joseph Sifakis 
Univ. Grenoble Alpes
CNRS
Grenoble INP, VERIMAG and SUSTECH/RITAS
GrenobleFrance</p>
<p>Qunli Zhang 
RAMS Lab
Huawei Technologies Duesseldorf GmbH
MunichGermany</p>
<p>Shiming Liu 
Huawei Technologies Co., Ltd., RAMS Lab
ShenzhenChina</p>
<p>Peng Wang 
Huawei Technologies Co., Ltd., RAMS Lab
ShenzhenChina</p>
<p>World Models: The Safety Perspective
A2F3997320DBDE5159F8A12A57D0679210.1109/ISSREW63542.2024.00104AI SafetyLLMEmbodied AIWorld ModelIntelligent Agents
With the proliferation of the Large Language Model (LLM), the concept of World Models (WM) has recently attracted a great deal of attention in the AI research community, especially in the context of AI agents.It is arguably evolving into an essential foundation for building AI agent systems.A WM is intended to help the agent predict the future evolution of environmental states or help the agent fill in missing information so that it can plan its actions and behave safely.The safety property of WM plays a key role in their effective use in critical applications.In this work, we review and analyze the impacts of the current state-of-the-art in WM technology from the point of view of trustworthiness and safety based on a comprehensive survey and the fields of application envisaged.We provide an in-depth analysis of state-of-the-art WMs and derive technical research challenges and their impact in order to call on the research community to collaborate on improving the safety and trustworthiness of WM.</p>
<p>I. INTRODUCTION</p>
<p>Recent years have witnessed rapid advancements in transformer-based generative models [1], with their capabilities expanding from natural language processing (NLP) to multimodal applications [2].Frontier models such as SORA [3], LINGO-1 [4], and GAIA-1 [5] demonstrate an unprecedented ability to generate remarkably realistic videos, suggesting an initial grasp of fundamental worldly principles like physics and spatiotemporal continuity, achieved solely through training on video and language datasets.This emerging capability opens new avenues for research, as understanding world models is crucial for developing next-generation intelligent systems.The concept of data-driven world models, initially proposed in 2017 using recurrent neural network (RNN) or long-short-term memory (LSTM) architectures [6], showed promise but was limited by constraints in sequence length, memory, and parallel capability.These early experiments demonstrated only modest capabilities in relatively simple simulated gaming environments.The advent of transformerbased methods has led to significant improvements, with recent experimental results showing encouraging progress.Consequently, many contemporary AI agent architectures now incorporate world models as a cornerstone component [7].Our paper focuses on world models for a specific class of agents known as embodied AI agents, which interact with the physical world.We examine these world models from the point of view of their safety, addressing a crucial gap in current research.</p>
<p>Auto-regressive generative models suffer from inherent deficiencies such as hallucination [8], [9].This limitation poses significant risks in safety-critical applications like robotics and autonomous driving systems (ADS) [10], sparking controversial discussions [11].Despite the current attention, we observe a lack of comprehensive analysis regarding the safety aspects of world models for embodied AI agents.Our paper aims to address this gap by providing a concise yet thorough review and investigation, followed by an in-depth analysis from a safety perspective.Finally, we identify high-priority research directions.</p>
<p>The main contributions of this paper are summarized as follows:</p>
<p>â€¢ We conduct a literature survey of recent achievements in world model research and show the development of techniques for realizing world models in chronological order.â€¢ We address the identified safety issues of the world models on embodied AI applications such as autonomous driving.</p>
<p>â€¢ We propose possible approaches for future research to facilitate the future of trustworthy world models.The paper is structured as follows: Section II presents a current definition of world models, offering an in-depth investigation and taxonomy of state-of-the-art approaches.We retrospectively review the development path of modern world model approaches across various application perspectives; Section III analyzes the safety-related deficiencies of current approaches from a critical perspective; Section IV proposes a research agenda highlighting high-priority topics to improve the safety of world models.By addressing these crucial aspects, we aim to bring clarity and contribute to the current debate on world models in embodied AI and promote the development of safer and more trustworthy intelligent systems.</p>
<p>II. THE EVOLUTION OF WORLD MODELS</p>
<p>In this section, we present an analysis of the development of existing world models, with the intention of showing how the techniques used to implement world models have evolved over time.Fig. 1 gives an overview of the development in chronological order.To the best of our knowledge, the core techniques adopted in the world models can be divided into four categories, as shown in the figure  RNNs are widely used in reinforcement learning (RL) agents as an emulator to predict the transition of the world state in the next steps.Transformer is proposed initially for machine translation.However, the transformer-based models are found to perform better in dealing with longer contexts than RNN over various tasks.Diffusion models learn how to generate images from a standard Gaussian distribution.A backbone model such as U-Net [12] is trained to predict the noise Ïµ t given the noised image in the denoising process (reverse process).In addition, other techniques are used to build a world model that can predict the development of the world state.However, due to the limited number of such techniques, we will introduce them in the last subsection.</p>
<p>A. RNN-Based Methods</p>
<p>Schmidhuber et al. [13]- [16] have advocated in training world models using recurrent neural networks (RNN) to learn the temporal conversion of states since 1990.In 2018, Ha and Schmidhuber proposed an RL agent driven by a world model composed of a convolution neural network (CNN) and a recurrent neural network.In the proposed framework, the CNN encodes the image input from the interaction environment, and the RNN predicts the future states represented in the latent space.</p>
<p>The long-short-term memory (LSTM) [17] method further improved dealing with longer sequences than RNN.Kaiser et.al. introduced their Simulated Polich Learning (SimPLe) algorithm [18], where an LSTM model is deployed for future prediction of state embeddings.</p>
<p>In 2019, Hafner et al. [19] proposed the recurrent state space model (RSSM) architecture where both deterministic and stochastic variables are constructed to represent the states in latent space.RL agents driven by RSSM architecture achieved significant improvement of the performance in both interacting with virtual environments [20]- [22] like Atari Games [23], and learning how to complete basic embodied tasks in real-world [24] in 2022.</p>
<p>Hu et al. introduced the model-based imitation learning</p>
<p>(MILE) approach for end-to-end autonomous driving in [25].The MILE approach utilizes a network with both stochastic and deterministic variables for the representation of latent states, and the gated recurrent units are adopted to emulate the world state evolution.The model is trained in the imitation learning way, which tries to imitate the experts' behavior based on the collected observation-action pairs.</p>
<p>A huge pain point in training intelligent agents is the distribution shift between the training data and the real dynamic world.Most of the existing works suffer from poor generalization capability.Thus, their behavior degrades heavily in an unseen environment.WorldCloner was proposed by Balloch et al. in [26], trying to adapt to an open and dynamic world in a neuro-symbolic manner.A rule model that detected and updated the world rules was equipped in addition to a normal RL model.When the rules in the experimental NovGrid environment [27] change, the rule model will try to detect and learn the new world rules.</p>
<p>Besides the RNNs' major application in being equipped in RL or IL agents, Bogdoll and Yang et al. [28] tried to exploit the potential of RNN in generating new driving scenarios.Encoders were trained to encode initial frames into latent space, and gated recurrent units predicted the possible world states in the next steps based on the given driving actions.Multi-modal decoders not only decoded into high-resolution image data but also into point cloud and 3D space occupancy formats with 3D information.</p>
<p>B. Transformer-Based Methods</p>
<p>Decision Transformer [29] is a world model for playing Atari games.State, actions, and system feedback in the game are serialized as linear embedding, combined with a positional episodic timestep, and fed into the generative pre-trained transformer (GPT) architecture, generating new agent actions based on future desired returns via autoregression.GATO [30] is a multi-modal, multi-task, multi-embodiment generative world model.Multi-modal data from different tasks are serialized into flat sequences of tokens, and a transformer-based network is trained by completing the reconstruction of the masked inputs.In [31], [32], the GPT-like Transformer simulates the environment dynamics.It takes the state information and action predictions encoded by the autoencoder as inputs to make predictions about the next state, reward policy, and episode termination.The GPT-Like Transformer enables the game agent to be data-efficiently trained in the world model.Voyager [33] implements an LLM-driven lifelong learning agent.In Minecraft, the agent is trained through automated interactions with the LLM without access to model parameters and performs gradient-based training.The LLM with world knowledge automatically maximizes the search for learning curricula for the agent, creates executable behavioral code through an iterative prompting mechanism, and continuously enriches the skill base to improve the agent's ability to handle complex tasks.</p>
<p>Counterfactual World Model (CWM) [34] can be used as a unified model for visual computation of world models.Through structured mask recovery training, the model can grasp the key scene transition information through a few visual markers.Keypoints, optical flows, and object segments in computer vision can be constructed in CWM in the form of counterfactual prompts by examining the perturbed model response in a zero-shot fashion.In [35], a cognitive system covering probabilistic reasoning, logical and relational reasoning, visual and physical reasoning, as well as social reasoning about agents and their plans, is constructed by modeling thinking through LLM.The construction of a language-driven world model is realized.</p>
<p>In GenAD [36], BEV tokens are converted into agent tokens and map tokens through two different tasks.They are combined with additional ego tokens for self-attention and cross-attention, transforming the traffic scene into a specific instance-centric scene representation.This representation is then mapped into the trajectory space and decoded into the agent's future trajectory.In ViDAR [37], a new visual point cloud forecasting task is proposed that enables synergic learning of visual semantics, 3D structure, and temporal dynamics in a model.This task can be used as an access to generalized visual perception and prediction capabilities for world models.In MotionLM [38], the autopilot agents' motion trajectories are discretized into motion tokens, combined with scene embeddings in the autoregressive transformer decoder to obtain trajectory predictions for multiple agents.TrafficBots [39] goes a step further.It introduces navigation destinations that allow vehicles to interact with multi-intelligent agents with personalities generated from a world model, enabling fidelity simulation and prediction of self-driving vehicle motion.In GAIA-1 [5], a generative world model creates realistic driving scenarios with video, text, and action prompts.Controlling the finesse of ego vehicle behavior and scene features, the agents' behaviors are predicted by an autoregressive transformer network-based world model.In UniWorld [40], the world model is constructed by fine-tuning the 4D geometric occupancy model.This enables the estimation of missing information about the world state and the prediction of future states.A transformer is used to construct 2D to 3D perspective transformations that provide a unified feature representation for the world model.</p>
<p>MAGVIT [41] can generate videos generically and efficiently.It quantizes videos into spatial-temporal visual tokens by 3D tokenizer, creates embedding for multiple video generation tasks using different masks, and learns to predict target tokens by the bidirectional transformer.In [42], scalable learning using RingAttention and Blockwise transformer on a huge number of long videos and books makes it possible to shape a model's understanding of the physical world in situations that are not easily described in words.This creates the basis for world models with more reliable reasoning and broader understanding.Through unsupervised training with large video game datasets, Genie [43] can generate interactive virtual worlds based on multimodal descriptions.The spatial-temporal (ST) transformer implements the attention mechanism in space &amp; time and becomes the potential action model in the world model.In DriveWorld [44], a vision task centered on 4D scene understanding is used for world model training.Temporal-aware latent dynamics and spatialaware latent statics are learned through the Memory State-Space Model.DriveWorld improves the capability of worldknowledge-driven autonomous driving.WorldDreamer builds image and video-based embedding through 3D convolution, joins text and action-based embedding, and accomplishes multimodal feature interaction through spatial-temporal patchwise self-attention and spatial-wise cross-attention.It realizes more types of video generation tasks and demonstrates a comprehensive understanding of visual dynamics in the general world.</p>
<p>C. Diffusion-Based Methods</p>
<p>RaMViD [45] can perform video generation and infilling tasks by applying random masking of some frames in diffusion training.In Gen-1 [46], both the convolution block and the attention block incorporate a temporal layer to achieve explicit control of temporal consistency and to provide guidance for the diffusion process by extracting depth information and structural information.</p>
<p>In LidarDM [47], the driving scene guides the diffusion model to generate a 3D scene, which is combined with the dynamic agent to form a 4D world.Subsequent interaction between the agent and the 4D world generates realistic and temporally relevant point cloud videos.ADriver-I [48] introduces the concept of interleaved vision-action pair, matching unified vision signals and control signals, which are fed into multi-modal LLM and diffusion models, alternately generating new control signals and future scenario predictions.It realizes automated driving in a self-created world.</p>
<p>The data-driven approach in CTG [49] uses diffusion models to generate vehicle trajectories that are similar to human behavior.Explicit rules are introduced in the testing phase to enforce trajectories to remain realistic and physically feasible.The controllable simulation generated by the diffusion model based on real-world law can provide a more realistic simulation environment for self-driving vehicles.In CTG++ [50], LLM was added to enable the generation of complex traffic scenes controlled by simple language instructions.DrivingDiffusion [51] implements the generation of multi-view driving videos for a given 3D layout.The multi-view consistency of the generated video is ensured by information exchange between neighboring cameras.Cross-frame consistency is achieved by utilizing the first frame of a multi-view video sequence as a key control condition, and a local prompt is further used to guide the relationship between the global scene and local instances, so that instance generation quality can be improved.Panacea [52] uses a two-stage generation pipeline.In the first stage, realistic multi-view driving scene images are generated.In the second stage, the images are extended into video sequences.Panacea uses 4D attention to keep the scene consistent across time and views and adds bird's-eye view (BEV) features inserted by ControlNet to improve the controllability of video generation.Drive-WM [53] builds a world model that can generate multi-view videos with spatiotemporal consistency through the diffusion model and can implement end-to-end planning for autonomous driving.Its planning is based on the reward model's assessment of the future trajectories generated by the world model.WoVoGen [54] has two branches: the world model branch and the world volume-aware synthesis branch.The world model generates a world volume prediction based on past world volumes and the actions of the ego vehicle.World volume-aware can generate multi-camera video from the world volume through the diffusion model.</p>
<p>DriveDreamer [55] is a world model that can generate highquality plausible driving scenarios, where diffusion models are used to build a comprehensive representation of the complex driving environment.Through a two-stage training process, the world model is equipped with an in-depth understanding of structured traffic constraints and the ability to predict future states.In DrivDreamer 2 [56], LLM is added, which allows users to describe the trajectory of agents in the driving scene through language and finally convert it into a high-quality driving scene video.Copilot4D [57] is a world model based on BEV tokens for discretizing diffusion processes.Copilot4D uses VQVAE to tokenize sensor observations and then predict the future through discrete diffusion, thereby solving the difficulties of complex and unstructured observation space and the need for generative models to decode multiple tokens in parallel.</p>
<p>UniSim [58] is an interactive generative world simulator built using the diffusion model.With multiple historical frames, the model can be instructed to generate new consecutive frames that match the intent of the interaction.UniSim enables the simulation of real-world experiences and can be used to simulate other artificial intelligence before they are generalized to the real world.SORA is a highquality generative model for video.It maps video data to lowdimensional space by visual patch, extracts spacetime latent patches in video space, and uses a scalable transformer to learn the diffusion process based on the understanding of detailed text description of video.SORA can be regarded as a generalized simulator for the physical world.</p>
<p>D. Other Methods</p>
<p>Exceptional approaches for building world models that have not been included before are listed in this chapter.TrafficGen [59] is a generative model that can generate realistic traffic scenarios.The model utilized a multi-context gating model as the decoder and trained individual multilayer perceptrons (MLPs) for placing the vehicle and forecasting their motion.Similarly, Tan et al. proposed the LCTGen [60] model, consisting of an LLM-Driven interpreter, a map retrieval model, and a generator.The interpreter model encoded the text description into a vectorized description of the scenarios, and the retrieval model searched for the corresponding map from the map dataset.In the generator, a query-based transformer model was designed to capture the interactions between agents and the map.Individual MLPs were trained for different prediction tasks, such as position prediction and motion prediction.</p>
<p>LeCun proposed the joint embedding predictive architectures (JEPA) [7] that was regarded as a better possible solution for implementing a general world model.Compared to other architectures, JEPA computes the loss of the samples in the latent space rather than the data space, which mitigates the model collapse.Based on the JEPA architecture, I-JEPA [61] and V-JEPA [62] were proposed to demonstrate the architecture's capability of pattern completion on image and video data.</p>
<p>E. Evaluation Metrics</p>
<p>The selection of evaluation metrics for world models depends on the specific tasks and application domains in which the world models are employed.Existing evaluation metrics for assessing the performance of the world models can be divided into two approaches depending on the actual application: metrics that assess world models deployed within agents and those that evaluate the outputted data given by generative world models.</p>
<p>Metrics for evaluating world models deployed in agents are often equivalent to assessing the performance of agents that make decisions based on the world models' future predictions, typically reinforcement learning agents.In virtual gaming environments, such as Atari [63], Deepmind Control Suite [64], DMLab [65], and Minecraft.evaluations often utilize built-in environment scores or reward values.In autonomous driving scenarios, traffic violation and accident rates are two of the most common evaluation metrics.</p>
<p>The choice of metrics for evaluating generative world models is based on the modality of the data generated by the model.Common evaluation metrics for video-generating world models include FrÃ©chet inception distance (FID) [66], FrÃ©chet video distance (FVD) [67], inception score (IS) [68], and peak signal to noise ratio (PSNR) [69].For models that generate driving traffic scenarios, the metrics include maximum mean discrepancy (MMD), mean average distance error (mADE), Fig. 2. Discovered unreasonable scenario imagination problems in existing world model for autonomous driving research.a) A traffic scenario generated by TrafficGen [59] where multiple vehicles are generated out of the drivable area.b) A traffic scenario generated by CTG [60] and the predicted participant behavior heavily violates the basic traffic rules.c) One single frame from a video scenario generated by DriveDreamer [55], where plausible but meaningless road marking is painted on the lane.d-e) Two subsequential frames in one video scenario generated by [55].Some of the vehicles in the parking lot suddenly disappear from d) to e). mean final distance error (mFDE), and traffic rule violation metrics like scenario collision rate (SCR).In the case of 3D driving scene world models, which generate point clouds and 3D occupancy maps as future imagination, Chamfer distance, mean average precision (mAP), and intersection over union (IoU) related assessment functions are used as evaluation metrics.</p>
<p>III. PATHOLOGY OF WORLD MODEL SAFETY</p>
<p>In this section, we discuss the inherent issues regarding world models for autonomous driving as a representative domain of embodied AI agents.We demonstrate the deficiencies through the concrete failing examples that were generated by selected models.We also provide a preliminary analysis of internal factors that may cause such failures.We concentrate on the scenario generation tasks as visualized by Fig. 2 in subsection A and explain our key findings on the failure of AI agents driven by world models in subsection B.</p>
<p>A. Unreasonable Traffic Scenario Generated Through Selected World Models</p>
<p>Both TrafficGen [59] and CTG [60] are generative world models with the aim of generating realistic and naturalistic traffic scenarios.Fig. 2a) and b) show two traffic scenarios in BEV that are generated by TrafficGen and CTG, respectively.The red circle in Fig. 2a) indicates that part of the generated vehicles are not placed on the drivable regions initially, and the assigned trajectories are also out of the areas.In Fig. 2b), vehicles are driving in traffic lanes, but the driving behavior of the vehicle in the red circle heavily violates the traffic rules.</p>
<p>Besides the traffic scenarios in BEV, Fig. 2c) is a selected frame from a video scenario generated by DriverDreamer [55].Even if the model performs well in generating a variety of realistic video scenarios, if one pays attention to important A collision occurs at t = 3.2s details such as road markings, one will still find that the video contains features that look plausible but are impossible to exist in the real world.Fig. 2d) and 2e) are two subsequent frames in one scenario.One can see that the red vehicle and silver vehicle in this parking lot disappear from d) to e).This kind of temporal inconsistency can also be found in most of the generative world models.</p>
<p>B. Unsafe Behavior of AI Agents Driven by World Models</p>
<p>The world models also play a pivotal role in reinforcement learning or imitation learning training as the scene encoder and the future predictor.However, safety issues still exist when the world models misunderstand the behavior and intention of other interactive agents in rare or unseen scenarios.Fig. 3 demonstrates a merging scenario at a T-Junction in the CARLA simulator.The merging vehicle (top) and the arriving vehicle (down) are controlled by individual MILE agents, and the merging vehicle is trying to turn right to the arriving vehicle's future lane.The scenario is initialized at t = 0, and a crash between the merging vehicle and the arriving vehicle is identified at t = 3.2s.In this case, we adopted the testing methodology in [70] to find the critical configurations of the initial states.</p>
<p>IV. RESEARCH DIRECTION PROPOSALS</p>
<p>We identified and discussed several highly prioritized research directions as a result of our investigation.It is clear that, as fundamental components of future safety-critical intelligent systems, current global models do not meet even the minimum safety requirements for autonomous agents.Guardrailing methods and controllable generative processes are indispensable for ultimate reliability and safety.Furthermore, we need trustworthy, technically feasible methods to quantitatively evaluate and benchmark the generative world models.</p>
<p>To be more specific, here is a non-exhaustive list of research challenges in data preparation, generative processes, and postprocessing phases, respectively.</p>
<p>â€¢ Quantitative Uncertainty Evaluations of Results: Evaluating the confidence of outputs from world models through uncertainty measures is crucial for determining the reliability of generated results in downstream processes.Although uncertainty estimation in machine learning has been extensively studied, traditional methods such as Bayesian neural networks (BNNs) and conformal prediction may not be suitable due to the scale, parameter size, and computational overhead involved.â€¢ Symbolic Integration into Learning: Incorporating prior knowledge to guide the generation process can be achieved through symbolic integration.Retrievalaugmented generation (RAG) [71], or, most recently, GraphRAG [72] can serve as a repository for embedded knowledge representations in vector form.Additionally, other neuro-symbolic approaches [73] can be employed to implement guardrails for the generative processes.â€¢ Controllable Learning Processes: Utilizing hypernetworks [74] and even classical control theory [75] allow for the definition of predetermined learning goals.The learning processes can dynamically adapt if they deviate from these goals, ensuring that the predefined objectives are met consistently.There are abundant bodies of research on controllable machine learning which worthwhile to be explored [76].â€¢ Mechanistic Explainable Machine Learning: Contrarily to classical explainability researches of machine learning, which focus on providing intuitive, high-level reasons and detailed mathematical formulations for a model's predictions, the mechanistic explainability [77] investigates internal behaviors of networks and try to understand their patterns, for example, activation of specific region of neurons could reveal whether a model is making particular decisions.Explainability is an effective auxiliary tool on the reliability issue.For instance, when the model hallucinates, a particular region of neurons could demonstrate more activities than others.Through such monitoring, countermeasures could be taken to ensure safety.â€¢ Benchmarks and Evaluation: Establishing evaluation methods and benchmarks to measure the trustworthiness of generative results from world models [78].According to various use cases of world models, multiple aspects must be considered.For example, if used for training dataset synthesis for autonomous driving systems, proper ways to measure the realism in terms of plausibility from both physical and social aspects need to be considered.</p>
<p>V. CONCLUSION</p>
<p>World models have recently garnered significant attention within the AI research community as a foundational component for AI agents.In this paper, we present a comprehensive technical survey and analysis of the evolution of these technologies.Our results indicate that, despite the astonishing progress, the most recent world models still present numerous safety problems, making them unsuitable for safety-critical applications such as embodied artificial intelligence systems.Recent research from cognitive science reveals that language is more of communication than thoughts [79].This could be a plausible explanation for why transformer-and autoregressionbased language models alone would not be sufficient to develop world models that can truly capture the real world in its fullness, adequate research in engineering is therefore still required.Additionally, we identify and prioritize the key technical challenges that must be addressed to advance this field.In particular, while we recognize the potential of world models in the era of large language models (LLMs) for future intelligent systems driven by AI, substantial research efforts are still required to achieve this vision.</p>
<p>Fig. 1 .
1
Fig. 1.The development of world models categorized with the core techniques in chronological order.</p>
<p>Fig. 3 .
3
Fig. 3.A merging scenario in CARLA with two vehicles controlled by individual MILE [25] agents.The scenario starts at t = 0, where the merging vehicle (top) is trying to merge into the arriving vehicle's (down) driving lane.A collision occurs at t = 3.2s</p>
<p>. Recurrent neural network (RNN) is one of the initial ideas to implement works models.Due to the capability of processing sequential data,
2024 IEEE 35th International Symposium on Software Reliability Engineering Workshops (ISSREW)2024 IEEE 35th International Symposium on Software Reliability Engineering Workshops (ISSREW) | 979-8-3503-6704-1/24/$31.00 Â©2024 IEEE | DOI: 10.1109/ISSREW63542.2024.001042994-810X/24/$31.00 Â©2024 IEEE369DOI 10.1109/ISSREW63542.2024.00104
Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.</p>
<p>Large language models: A survey. S Minaee, T Mikolov, N Nikzad, M Chenaghlu, R Socher, X Amatriain, J Gao, 2024</p>
<p>Multimodal large language models: A survey. J Wu, W Gan, Z Chen, S Wan, P S Yu, 2023 IEEE International Conference on Big Data (BigData). 2023</p>
<p>Video generation models as world simulators. T Brooks, B Peebles, C Holmes, W Depue, Y Guo, L Jing, D Schnurr, J Taylor, T Luhman, E Luhman, C Ng, R Wang, A Ramesh, 2024</p>
<p>Lingo-1: Exploring natural language for autonomous driving. 14 September 2023</p>
<p>Gaia-1: A generative world model for autonomous driving. A Hu, L Russell, H Yeo, Z Murez, G Fedoseev, A Kendall, J Shotton, G Corrado, arXiv:2309.170802023arXiv preprint</p>
<p>Recurrent world models facilitate policy evolution. D Ha, J Schmidhuber, Advances in Neural Information Processing Systems. H Bengio, H Wallach, K Larochelle, N Grauman, R Cesa-Bianchi, Garnett, Curran Associates, Inc201831</p>
<p>A path towards autonomous machine intelligence version 0. Y Lecun, 20229</p>
<p>A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. L Huang, W Yu, W Ma, W Zhong, Z Feng, H Wang, Q Chen, W Peng, X Feng, B Qin, T Liu, 2023</p>
<p>Sora: A review on background, technology, limitations, and opportunities of large vision models. Y Liu, K Zhang, Y Li, Z Yan, C Gao, R Chen, Z Yuan, Y Huang, H Sun, J Gao, L He, L Sun, 2024</p>
<p>Drivevlm: The convergence of autonomous driving and large vision-language models. X Tian, J Gu, B Li, Y Liu, Y Wang, Z Zhao, K Zhan, P Jia, X Lang, H Zhao, 2024</p>
<p>Vision language models in autonomous driving: A survey and outlook. X Zhou, M Liu, E Yurtsever, B L Zagar, W Zimmer, H Cao, A C Knoll, IEEE Transactions on Intelligent Vehicles. 2024</p>
<p>U-net: Convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, Medical Image Computing and Computer-Assisted Intervention -MICCAI. N. Navab, J. Hornegger, W. M. Wells, and A. F. Frangi2015. 2015Springer International Publishing</p>
<p>Reinforcement Learning with Interacting Continually Running Fully Recurrent Networks. J Schmidhuber, 1990SpringerNetherlands</p>
<p>An on-line algorithm for dynamic reinforcement learning and planning in reactive environments. J Schmidhuber, 1990 IJCNN International Joint Conference on Neural Networks. IEEE1990</p>
<p>An on-line algorithm for dynamic reinforcement learning and planning in reactive environments. J Schmidhuber, 1990 IJCNN International Joint Conference on Neural Networks. IEEE1990</p>
<p>Reinforcement learning in markovian and nonmarkovian environments. J Schmidhuber, Advances in Neural Information Processing Systems. R Lippmann, J Moody, D Touretzky, Morgan-Kaufmann19903</p>
<p>Long short-term memory. S Hochreiter, J Schmidhuber, Neural computation. 91997</p>
<p>Authorized licensed use limited to the terms of the applicable license agreement with IEEE. </p>
<p>Model based reinforcement learning for atari. Åukasz Kaiser, M Babaeizadeh, P MiÅ‚os, B OsiÅ„ski, R H Campbell, K Czechowski, D Erhan, C Finn, P Kozakowski, S Levine, A Mohiuddin, R Sepassi, G Tucker, H Michalewski, International Conference on Learning Representations. 2020</p>
<p>Learning latent dynamics for planning from pixels. D Hafner, T Lillicrap, I Fischer, R Villegas, D Ha, H Lee, J Davidson, International conference on machine learning. PMLR2019</p>
<p>Dream to control: Learning behaviors by latent imagination. D Hafner, T Lillicrap, J Ba, M Norouzi, International Conference on Learning Representations. 2019</p>
<p>Mastering atari with discrete world models. D Hafner, T P Lillicrap, M Norouzi, J Ba, International Conference on Learning Representations. 2020</p>
<p>Mastering diverse domains through world models. D Hafner, J Pasukonis, J Ba, T Lillicrap, arXiv:2301.041042023arXiv preprint</p>
<p>Human learning in atari. P Tsividis, T Pouncy, J L Xu, J B Tenenbaum, S J Gershman, AAAI Spring Symposia. 2017</p>
<p>Daydreamer: World models for physical robot learning. P Wu, A Escontrela, D Hafner, K Goldberg, P Abbeel, Conference on Robot Learning. 2022</p>
<p>Model-based imitation learning for urban driving. A Hu, G Corrado, N Griffiths, Z Murez, C Gurau, H Yeo, A Kendall, R Cipolla, J Shotton, Advances in Neural Information Processing Systems. 202235</p>
<p>Neuro-symbolic world models for adapting to open world novelty. J C Balloch, Z Lin, X Peng, M Hussain, A Srinivas, R Wright, J M Kim, M O Riedl, Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems, AAMAS '23. the 2023 International Conference on Autonomous Agents and Multiagent Systems, AAMAS '23Richland, SCInternational Foundation for Autonomous Agents and Multiagent Systems2023</p>
<p>Novgrid: A flexible grid world for evaluating agent response to novelty. J Balloch, Z Lin, M Hussain, A Srinivas, R Wright, X Peng, J Kim, M Riedl, arXiv:2203.121172022arXiv preprint</p>
<p>Muvo: A multimodal generative world model for autonomous driving with geometric representations. D Bogdoll, Y Yang, J M ZÃ¶llner, arXiv:2311.117622023arXiv preprint</p>
<p>Decision transformer: Reinforcement learning via sequence modeling. L Chen, K Lu, A Rajeswaran, K Lee, A Grover, M Laskin, P Abbeel, A Srinivas, I Mordatch, Advances in neural information processing systems. 202134</p>
<p>A generalist agent. S Reed, K Zolna, E Parisotto, S G Colmenarejo, A Novikov, G Barth-Maron, M Gimenez, Y Sulsky, J Kay, J T Springenberg, arXiv:2205.061752022arXiv preprint</p>
<p>Transformers are sample-efficient world models. V Micheli, E Alonso, F Fleuret, arXiv:2209.005882022arXiv preprint</p>
<p>Towards efficient world models. E Alonso, V Micheli, F Fleuret, Workshop on Efficient Systems for Foundation Models@ ICML2023. 2023</p>
<p>Voyager: An open-ended embodied agent with large language models. G Wang, Y Xie, Y Jiang, A Mandlekar, C Xiao, Y Zhu, L Fan, A Anandkumar, arXiv:2305.162912023arXiv preprint</p>
<p>Unifying (machine) vision via counterfactual world modeling. D M Bear, K Feigelis, H Chen, W Lee, R Venkatesh, K Kotar, A Durango, D L Yamins, arXiv:2306.018282023arXiv preprint</p>
<p>From word models to world models: Translating from natural language to the probabilistic language of thought. L Wong, G Grand, A K Lew, N D Goodman, V K Mansinghka, J Andreas, J B Tenenbaum, arXiv:2306.126722023arXiv preprint</p>
<p>Genad: Generative end-toend autonomous driving. W Zheng, R Song, X Guo, L Chen, arXiv:2402.115022024arXiv preprint</p>
<p>Visual point cloud forecasting enables scalable autonomous driving. Z Yang, L Chen, Y Sun, H Li, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Motionlm: Multi-agent motion forecasting as language modeling. A Seff, B Cera, D Chen, M Ng, A Zhou, N Nayakanti, K S Refaat, R Al-Rfou, B Sapp, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Trafficbots: Towards world models for autonomous driving simulation and motion prediction. Z Zhang, A Liniger, D Dai, F Yu, L Van Gool, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Uniworld: Autonomous driving pre-training via world models. C Min, D Zhao, L Xiao, Y Nie, B Dai, arXiv:2308.072342023arXiv preprint</p>
<p>Magvit: Masked generative video transformer. L Yu, Y Cheng, K Sohn, J Lezama, H Zhang, H Chang, A G Hauptmann, M.-H Yang, Y Hao, I Essa, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>World model on million-length video and language with ringattention. H Liu, W Yan, M Zaharia, P Abbeel, arXiv:2402.082682024arXiv preprint</p>
<p>Genie: Generative interactive environments. J Bruce, M D Dennis, A Edwards, J Parker-Holder, Y Shi, E Hughes, M Lai, A Mavalankar, R Steigerwald, C Apps, Forty-first International Conference on Machine Learning. 2024</p>
<p>Driveworld: 4d pre-trained scene understanding via world models for autonomous driving. C Min, D Zhao, L Xiao, J Zhao, X Xu, Z Zhu, L Jin, J Li, Y Guo, J Xing, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Diffusion models for video prediction and infilling. T HÃ¶ppe, A Mehrjou, S Bauer, D Nielsen, A Dittadi, arXiv:2206.076962022arXiv preprint</p>
<p>Structure and content-guided video synthesis with diffusion models. P Esser, J Chiu, P Atighehchian, J Granskog, A Germanidis, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Lidardm: Generative lidar simulation in a generated world. V Zyrianov, H Che, Z Liu, S Wang, arXiv:2404.029032024arXiv preprint</p>
<p>Adriver-i: A general world model for autonomous driving. F Jia, W Mao, Y Liu, Y Zhao, Y Wen, C Zhang, X Zhang, T Wang, arXiv:2311.135492023arXiv preprint</p>
<p>Guided conditional diffusion for controllable traffic simulation. Z Zhong, D Rempe, D Xu, Y Chen, S Veer, T Che, B Ray, M Pavone, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Language-guided traffic simulation via scene-level diffusion. Z Zhong, D Rempe, Y Chen, B Ivanovic, Y Cao, D Xu, M Pavone, B Ray, Conference on Robot Learning. PMLR2023</p>
<p>Drivingdiffusion: Layout-guided multiview driving scene video generation with latent diffusion model. X Li, Y Zhang, X Ye, arXiv:2310.077712023arXiv preprint</p>
<p>Panacea: Panoramic and controllable video generation for autonomous driving. Y Wen, Y Zhao, Y Liu, F Jia, Y Wang, C Luo, C Zhang, T Wang, X Sun, X Zhang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Driving into the future: Multiview visual forecasting and planning with world model for autonomous driving. Y Wang, J He, L Fan, H Li, Y Chen, Z Zhang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Wovogen: World volume-aware diffusion for controllable multi-camera driving scene generation. J Lu, Z Huang, J Zhang, Z Yang, L Zhang, arXiv:2312.029342023arXiv preprint</p>
<p>Drivedreamer: Towards real-world-driven world models for autonomous driving. X Wang, Z Zhu, G Huang, X Chen, J Lu, arXiv:2309.097772023arXiv preprint</p>
<p>Drivedreamer-2: Llm-enhanced world models for diverse driving video generation. G Zhao, X Wang, Z Zhu, X Chen, G Huang, X Bao, X Wang, arXiv:2403.068452024arXiv preprint</p>
<p>Learning unsupervised world models for autonomous driving via discrete diffusion. L Zhang, Y Xiong, Z Yang, S Casas, R Hu, R Urtasun, arXiv:2311.010172023arXiv preprint</p>
<p>Learning interactive real-world simulators. M Yang, Y Du, K Ghasemipour, J Tompson, D Schuurmans, P Abbeel, arXiv:2310.061142023arXiv preprint</p>
<p>Trafficgen: Learning to generate diverse and realistic traffic scenarios. L Feng, Q Li, Z Peng, S Tan, B Zhou, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Language conditioned traffic generation. S Tan, B Ivanovic, X Weng, M Pavone, P Kraehenbuehl, 7th Annual Conference on Robot Learning. 2023</p>
<p>Self-supervised learning from images with a joint-embedding predictive architecture. M Assran, Q Duval, I Misra, P Bojanowski, P Vincent, M Rabbat, Y Lecun, N Ballas, arXiv:2301.082432023arXiv preprint</p>
<p>Revisiting feature prediction for learning visual representations from video. A Bardes, Q Garrido, J Ponce, M Rabbat, Y Lecun, M Assran, N Ballas, arXiv:2404.084712024</p>
<p>The arcade learning environment: An evaluation platform for general agents. M G Bellemare, Y Naddaf, J Veness, M Bowling, Journal of Artificial Intelligence Research. 472013</p>
<p>Deepmind control suite. Y Tassa, Y Doron, A Muldal, T Erez, Y Li, D D L Casas, D Budden, A Abdolmaleki, J Merel, A Lefrancq, arXiv:1801.006902018arXiv preprint</p>
<p>Deepmind lab. C Beattie, J Z Leibo, D Teplyashin, T Ward, M Wainwright, H KÃ¼ttler, A Lefrancq, S Green, V ValdÃ©s, A Sadik, arXiv:1612.038012016arXiv preprint</p>
<p>Gans trained by a two time-scale update rule converge to a local nash equilibrium. M Heusel, H Ramsauer, T Unterthiner, B Nessler, S Hochreiter, Advances in neural information processing systems. 201730</p>
<p>FVD: A new metric for video generation. T Unterthiner, S Van Steenkiste, K Kurach, R Marinier, M Michalski, S Gelly, 2019</p>
<p>Improved techniques for training gans. T Salimans, I Goodfellow, W Zaremba, V Cheung, A Radford, X Chen, Advances in neural information processing systems. 201629</p>
<p>Deep multi-scale video prediction beyond mean square error. M Mathieu, C Couprie, Y Lecun, ICLR 20164th International Conference on Learning Representations. 2016</p>
<p>Rigorous simulation-based testing for autonomous driving systems -targeting the achilles' heel of four open autopilots. C Li, J Sifakis, R Yan, J Zhang, 2024</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. P Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H KÃ¼ttler, M Lewis, W -T. Yih, T RocktÃ¤schel, S Riedel, D Kiela, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M Balcan, H Lin, Curran Associates, Inc202033</p>
<p>From local to global: A graph rag approach to queryfocused summarization. D Edge, H Trinh, N Cheng, J Bradley, A Chao, A Mody, S Truitt, J Larson, 2024</p>
<p>Neurosymbolic ai -why, what, and how. A Sheth, K Roy, M Gaur, 2023</p>
<p>A brief review of hypernetworks in deep learning. V K Chauhan, J Zhou, P Lu, S Molaei, D A Clifton, 2024</p>
<p>PIDformer: Transformer meets control theory. T M Nguyen, C A Uribe, T M Nguyen, R Baraniuk, R Salakhutdinov, Z Kolter, K Heller, A Weller, N Oliver, Proceedings of the 41st International Conference on Machine Learning. J Scarlett, F Berkenkamp, the 41st International Conference on Machine LearningPMLRJul 2024235Proceedings of Machine Learning Research</p>
<p>A survey of controllable learning: Methods and applications in information retrieval. C Shen, X Zhang, T Shi, C Zhang, G Xie, J Xu, 2024</p>
<p>Mechanistic interpretability for ai safetya review. L Bereska, E Gavves, 2024</p>
<p>Position: TrustLLM: Trustworthiness in large language models. Y Huang, L Sun, H Wang, S Wu, Q Zhang, Y Li, C Gao, Y Huang, W Lyu, Y Zhang, X Li, H Sun, Z Liu, Y Liu, Y Wang, Z Zhang, B Vidgen, B Kailkhura, C Xiong, C Xiao, C Li, E P Xing, F Huang, H Liu, H Ji, H Wang, H Zhang, H Yao, M Kellis, M Zitnik, M Jiang, M Bansal, J Zou, J Pei, J Liu, J Gao, J Han, J Zhao, J Tang, J Wang, J Vanschoren, J Mitchell, K Shu, K Xu, K.-W Chang, L He, L Huang, M Backes, N Z Gong, P S Yu, P.-Y Chen, Q Gu, R Xu, R Ying, S Ji, S Jana, T Chen, T Liu, T Zhou, W Y Wang, X Li, X Zhang, X Wang, X Xie, X Chen, X Wang, Y Liu, Y Ye, Y Cao, Y Chen, Y Zhao, Proceedings of the 41st International Conference on Machine Learning. R Salakhutdinov, Z Kolter, K Heller, A Weller, N Oliver, J Scarlett, F Berkenkamp, the 41st International Conference on Machine LearningPMLRJul 2024235Proceedings of Machine Learning Research</p>
<p>Language is primarily a tool for communication rather than thought. E Fedorenko, S T Piantadosi, E A Gibson, Nature. 63080172024</p>            </div>
        </div>

    </div>
</body>
</html>
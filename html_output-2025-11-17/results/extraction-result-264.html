<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-264 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-264</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-264</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-261582366</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2309.03224v3.pdf" target="_blank">No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) demonstrate impressive language understanding and contextual learning abilities, making them suitable for natural language processing (NLP) tasks and complex mathematical reasoning. However, when applied to mathematical reasoning tasks, LLMs often struggle to generate correct reasoning steps and answers despite having high probabilities for the solutions. To overcome this limitation and enhance the mathematical reasoning capabilities of fine-tuned LLMs without additional fine-tuning steps, we propose a method that incorporates Monte Carlo Tree Search (MCTS) and a lightweight energy function to rank decision steps and enable immediate reaction and precise reasoning. Specifically, we re-formulate the fine-tuned LLMs into a Residual-based Energy Model (Residual-EBM) and employ noise contrastive estimation to estimate the energy function's parameters. We then utilize MCTS with the energy function as a path verifier to search the output space and evaluate the reasoning path. Through extensive experiments on two mathematical reasoning benchmarks, GSM8k and AQUA-RAT, we demonstrate the exceptional capabilities of our method, which significantly improves the pass@1 metric of the fine-tuned model without requiring additional fine-tuning or reinforcement learning with human feedback alignment.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e264.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e264.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MCTS-EBM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Monte Carlo Tree Search guided by a Residual Energy-Based Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoding-time method that uses an energy function (Residual-EBM) as a path verifier to reweight an instruction-tuned language model during Monte Carlo Tree Search (MCTS) to generate and rank multi-step mathematical reasoning paths.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-7B (instruction-tuned SFT), RFT-7B, RFT-13B (experiments reported)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B / 13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>multi-step arithmetic word problems (mixtures of addition, subtraction, multiplication, division, percentages, decimals; multi-step numerical reasoning and algebraic steps as in GSM8k and AQUA-RAT)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>MCTS at sentence granularity using a fixed instruction-tuned LM as prior (P_LM) and an energy function E_θ trained by Noise Contrastive Estimation (NCE); two negative-sampling strategies for NCE (rejection sampling and suboutput sampling); UCT selection combines node prior and a Q-value derived from exp(-E_θ(x)); rollouts use P_LM; final answer chosen by node visit counts (ties broken by node reward).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>GSM8k pass@1: Llama2-7B SFT greedy 41.69% -> MCTS-EBM (ebm-both) 52.23% (sample-then-rank and MCTS variants reported: sample-then-rank-ebm-reject 43.82%, sample-then-rank-ebm-both 46.77%, MCTS-ebm-reject 45.18%); RFT-7B greedy 50.30% -> MCTS-EBM 56.78%; RFT-13B greedy 55.40% -> MCTS-EBM 61.48%. AQUA-RAT: greedy-decoding baseline 34.25% -> MCTS-ebm-both 38.18%.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Not a claim about internal arithmetic algorithms; provides a mechanistic account of improved arithmetic performance via better path-ranking: the Residual-EBM assigns low energy to human (training) reasoning paths and higher energy to model-generated (negative) paths, and exp(-E_θ(x)) is used as a verifier/reward in MCTS so the search favors complete reasoning paths that the energy model scores as more 'real/correct'. NCE training with hard negatives (rejection sampling focuses on correct-answer paths, suboutput sampling provides near-ground-truth prefixes) teaches the energy function to discriminate path quality. MCTS operates at sentence-level nodes (sentences ≈ reasoning steps), balancing exploration (PLM prior) and exploitation (energy-derived reward) via UCT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Consistent gains across model sizes tested: +10.54 percentage points on Llama2-7B SFT, +6.48 pp on RFT-7B, +6.08 pp on RFT-13B; larger base models start with higher absolute pass@1 but still benefit from MCTS-EBM; however, energy function generalizability is limited across different output formats (e.g., poor transfer to WizardMath outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Higher decoding-time compute cost due to many rollouts; energy function is format-sensitive (trained on a specific input/output format) and may not evaluate paths from models with different output formats (causing degraded performance for those models); rejection-sampling negatives emphasize correct-answer paths and can make the energy function less sensitive to prefix-path distinctions unless combined with suboutput negatives.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against greedy decoding, self-consistency majority voting (10 samples), sample-then-rank (with ebm-reject and ebm-both), and baselines including RFT and WizardMath. Direct numerical comparisons reported above.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Using an energy function trained by NCE to score full reasoning paths and using that score to guide sentence-level MCTS substantially improves pass@1 on multi-step arithmetic word problems without additional fine-tuning of the base LM.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e264.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e264.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama2-7B SFT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 2, instruction-tuned via supervised fine-tuning (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned open-source LLM used as the base/noise model (P_LM) and as the primary baseline; evaluated on GSM8k and AQUA-RAT for multi-step arithmetic word problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-7B (instruction-tuned SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>multi-step arithmetic word problems (addition, subtraction, multiplication, division, percentages, decimals, and compound multi-step calculations typical of GSM8k)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Supervised fine-tuning (SFT) with greedy decoding baseline; also evaluated with self-consistency majority voting (10 samples) and sample-then-rank; used as the fixed P_LM when training the energy function (noise distribution).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>GSM8k pass@1 greedy-decoding: 41.69%; self-consistency-majority-voting (10 samples): 52.84%; with MCTS-EBM (ebm-both): 52.23% (improvement of +10.54 pp over greedy). AQUA-RAT greedy: 34.25% -> MCTS-ebm-both 38.18%.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Baseline SFT greedy decoding often produces incorrect intermediate arithmetic steps leading to wrong final answers (illustrated by case studies); self-consistency or reranking helps but at sampling cost.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Greedy-decoding vs self-consistency vs sample-then-rank vs MCTS-EBM.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>An instruction-tuned Llama2-7B baseline with greedy decoding achieves ~41.7% on GSM8k, but decoding-time interventions (self-consistency, sample-then-rank, and especially MCTS-EBM) substantially raise pass@1 without further fine-tuning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e264.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e264.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RFT-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rejection-Finetuned (RFT) 7B model (as reported from prior work and evaluated with MCTS-EBM here)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B model improved via rejection-sampling based finetuning from prior work (referenced as RFT) used as a stronger baseline; the paper applies MCTS-EBM on top of it and reports gains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RFT-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>multi-step arithmetic word problems (GSM8k/AQUA-RAT style)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>RFT supervised finetuning (uses rejection sampling data augmentation in original RFT work) with greedy decoding baseline; MCTS-EBM applied as a decoding-time reranker/verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>GSM8k pass@1: greedy-decoding 50.30% -> MCTS-EBM 56.78% (+6.48 pp).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Same path-ranking mechanism as MCTS-EBM: energy model trained to discriminate real vs generated paths improves selection of correct arithmetic reasoning chains during search.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>None specific beyond those of MCTS-EBM (compute cost, format sensitivity of energy); baseline errors still present but reduced after MCTS-EBM reranking.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared RFT greedy-decoding vs RFT + MCTS-EBM; also compared with WizardMath and larger models in tables.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>MCTS-EBM further improves a stronger pretrained/finetuned baseline (RFT-7B) by several percentage points, showing the decoding-time verifier/search approach complements finetuning-based gains.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e264.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e264.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RFT-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rejection-Finetuned (RFT) 13B model (evaluated with MCTS-EBM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 13B model from the RFT line used as a larger-capacity baseline; MCTS-EBM applied at decoding-time yields measurable improvements on math benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RFT-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>multi-step arithmetic word problems (GSM8k/AQUA-RAT style)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>RFT supervised finetuning baseline with greedy decoding; MCTS-EBM applied as decoding-time verifier/search.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>GSM8k pass@1: greedy-decoding 55.40% -> MCTS-EBM 61.48% (+6.08 pp).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Higher-capacity model exhibits higher baseline accuracy and also benefits (~6 pp) from MCTS-EBM, indicating gains from path re-ranking are present across scales tested.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Greedy-decoding vs MCTS-EBM applied to same base model.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Even at 13B scale, decoding-time energy-guided search meaningfully improves multi-step arithmetic performance, supporting that better path verification/ranking complements model scale.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e264.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e264.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Residual-EBM (Energy Function)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Residual Energy-Based Model trained via Noise Contrastive Estimation (E_θ)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learned scalar energy function E_θ(x) that multiplies a fixed instruction-tuned LM's probability with exp(-E_θ(x)) to prefer human-like reasoning paths; trained with NCE using model-generated negatives (rejection sampling and suboutput sampling).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Energy model trained on outputs of instruction-tuned LMs (deberta-large backbone + linear layer used as E_θ in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>encoder-style classifier backbone (DeBERTa-large used as E_θ feature extractor in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>evaluates complete reasoning paths for multi-step arithmetic word problems (serves as verifier for arithmetic step correctness and plausibility)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Residual formulation: P_θ(x) = P_LM(x) * exp(-E_θ(x)) / Z_θ; energy trained by Noise Contrastive Estimation (NCE) with positives from dataset and negatives from P_LM (two negative-generation strategies: rejection sampling and suboutput sampling).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>When trained with both rejection and suboutput negatives (ebm-both) and used in MCTS, yields best improvements: sample-then-rank-ebm-both 46.77% (10 samples) and MCTS-ebm-both 52.23% (single-path) on GSM8k SFT baseline; ebm-reject is weaker (e.g., sample-then-rank-ebm-reject 43.82%, MCTS-ebm-reject 45.18%).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Energy acts as a path-level verifier: NCE trains E_θ to assign lower energy to gold training reasoning paths and higher energy to model-generated paths; combining near-gold negatives (suboutput) with rejection-sampled negatives produces a discriminator sensitive to both final-answer correctness and intermediate-step plausibility, improving reranking of arithmetic solution chains.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Quality of negative sampling influences final gains; ebm-both outperforms ebm-reject. Energy is format-specific and must be trained for the model/output format used in decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Energy is tied to the input-output formatting of training data; an E_θ trained on one format may not evaluate paths from a model with a different output format (reducing or reversing gains).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared energy variants trained with different negative samplers (reject vs both) and used in both sample-then-rank and MCTS pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>A residual energy function trained by NCE with carefully chosen model-generated negatives can serve as an effective path-level verifier for arithmetic reasoning, enabling MCTS to prioritize correct multi-step solutions at inference time.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Residual energy-based models for text generation <em>(Rating: 2)</em></li>
                <li>Discriminator-guided multi-step reasoning with language models <em>(Rating: 2)</em></li>
                <li>Solving quantitative reasoning problems with language models <em>(Rating: 2)</em></li>
                <li>Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-264",
    "paper_id": "paper-261582366",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "MCTS-EBM",
            "name_full": "Monte Carlo Tree Search guided by a Residual Energy-Based Model",
            "brief_description": "A decoding-time method that uses an energy function (Residual-EBM) as a path verifier to reweight an instruction-tuned language model during Monte Carlo Tree Search (MCTS) to generate and rank multi-step mathematical reasoning paths.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama2-7B (instruction-tuned SFT), RFT-7B, RFT-13B (experiments reported)",
            "model_size": "7B / 13B",
            "model_architecture": null,
            "arithmetic_operation_type": "multi-step arithmetic word problems (mixtures of addition, subtraction, multiplication, division, percentages, decimals; multi-step numerical reasoning and algebraic steps as in GSM8k and AQUA-RAT)",
            "number_range_or_complexity": null,
            "method_or_intervention": "MCTS at sentence granularity using a fixed instruction-tuned LM as prior (P_LM) and an energy function E_θ trained by Noise Contrastive Estimation (NCE); two negative-sampling strategies for NCE (rejection sampling and suboutput sampling); UCT selection combines node prior and a Q-value derived from exp(-E_θ(x)); rollouts use P_LM; final answer chosen by node visit counts (ties broken by node reward).",
            "performance_result": "GSM8k pass@1: Llama2-7B SFT greedy 41.69% -&gt; MCTS-EBM (ebm-both) 52.23% (sample-then-rank and MCTS variants reported: sample-then-rank-ebm-reject 43.82%, sample-then-rank-ebm-both 46.77%, MCTS-ebm-reject 45.18%); RFT-7B greedy 50.30% -&gt; MCTS-EBM 56.78%; RFT-13B greedy 55.40% -&gt; MCTS-EBM 61.48%. AQUA-RAT: greedy-decoding baseline 34.25% -&gt; MCTS-ebm-both 38.18%.",
            "mechanistic_insight": "Not a claim about internal arithmetic algorithms; provides a mechanistic account of improved arithmetic performance via better path-ranking: the Residual-EBM assigns low energy to human (training) reasoning paths and higher energy to model-generated (negative) paths, and exp(-E_θ(x)) is used as a verifier/reward in MCTS so the search favors complete reasoning paths that the energy model scores as more 'real/correct'. NCE training with hard negatives (rejection sampling focuses on correct-answer paths, suboutput sampling provides near-ground-truth prefixes) teaches the energy function to discriminate path quality. MCTS operates at sentence-level nodes (sentences ≈ reasoning steps), balancing exploration (PLM prior) and exploitation (energy-derived reward) via UCT.",
            "performance_scaling": "Consistent gains across model sizes tested: +10.54 percentage points on Llama2-7B SFT, +6.48 pp on RFT-7B, +6.08 pp on RFT-13B; larger base models start with higher absolute pass@1 but still benefit from MCTS-EBM; however, energy function generalizability is limited across different output formats (e.g., poor transfer to WizardMath outputs).",
            "failure_modes": "Higher decoding-time compute cost due to many rollouts; energy function is format-sensitive (trained on a specific input/output format) and may not evaluate paths from models with different output formats (causing degraded performance for those models); rejection-sampling negatives emphasize correct-answer paths and can make the energy function less sensitive to prefix-path distinctions unless combined with suboutput negatives.",
            "comparison_baseline": "Compared against greedy decoding, self-consistency majority voting (10 samples), sample-then-rank (with ebm-reject and ebm-both), and baselines including RFT and WizardMath. Direct numerical comparisons reported above.",
            "key_finding": "Using an energy function trained by NCE to score full reasoning paths and using that score to guide sentence-level MCTS substantially improves pass@1 on multi-step arithmetic word problems without additional fine-tuning of the base LM.",
            "uuid": "e264.0"
        },
        {
            "name_short": "Llama2-7B SFT",
            "name_full": "Llama 2, instruction-tuned via supervised fine-tuning (7B)",
            "brief_description": "An instruction-tuned open-source LLM used as the base/noise model (P_LM) and as the primary baseline; evaluated on GSM8k and AQUA-RAT for multi-step arithmetic word problems.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama2-7B (instruction-tuned SFT)",
            "model_size": "7B",
            "model_architecture": null,
            "arithmetic_operation_type": "multi-step arithmetic word problems (addition, subtraction, multiplication, division, percentages, decimals, and compound multi-step calculations typical of GSM8k)",
            "number_range_or_complexity": null,
            "method_or_intervention": "Supervised fine-tuning (SFT) with greedy decoding baseline; also evaluated with self-consistency majority voting (10 samples) and sample-then-rank; used as the fixed P_LM when training the energy function (noise distribution).",
            "performance_result": "GSM8k pass@1 greedy-decoding: 41.69%; self-consistency-majority-voting (10 samples): 52.84%; with MCTS-EBM (ebm-both): 52.23% (improvement of +10.54 pp over greedy). AQUA-RAT greedy: 34.25% -&gt; MCTS-ebm-both 38.18%.",
            "mechanistic_insight": null,
            "performance_scaling": null,
            "failure_modes": "Baseline SFT greedy decoding often produces incorrect intermediate arithmetic steps leading to wrong final answers (illustrated by case studies); self-consistency or reranking helps but at sampling cost.",
            "comparison_baseline": "Greedy-decoding vs self-consistency vs sample-then-rank vs MCTS-EBM.",
            "key_finding": "An instruction-tuned Llama2-7B baseline with greedy decoding achieves ~41.7% on GSM8k, but decoding-time interventions (self-consistency, sample-then-rank, and especially MCTS-EBM) substantially raise pass@1 without further fine-tuning.",
            "uuid": "e264.1"
        },
        {
            "name_short": "RFT-7B",
            "name_full": "Rejection-Finetuned (RFT) 7B model (as reported from prior work and evaluated with MCTS-EBM here)",
            "brief_description": "A 7B model improved via rejection-sampling based finetuning from prior work (referenced as RFT) used as a stronger baseline; the paper applies MCTS-EBM on top of it and reports gains.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "RFT-7B",
            "model_size": "7B",
            "model_architecture": null,
            "arithmetic_operation_type": "multi-step arithmetic word problems (GSM8k/AQUA-RAT style)",
            "number_range_or_complexity": null,
            "method_or_intervention": "RFT supervised finetuning (uses rejection sampling data augmentation in original RFT work) with greedy decoding baseline; MCTS-EBM applied as a decoding-time reranker/verifier.",
            "performance_result": "GSM8k pass@1: greedy-decoding 50.30% -&gt; MCTS-EBM 56.78% (+6.48 pp).",
            "mechanistic_insight": "Same path-ranking mechanism as MCTS-EBM: energy model trained to discriminate real vs generated paths improves selection of correct arithmetic reasoning chains during search.",
            "performance_scaling": null,
            "failure_modes": "None specific beyond those of MCTS-EBM (compute cost, format sensitivity of energy); baseline errors still present but reduced after MCTS-EBM reranking.",
            "comparison_baseline": "Compared RFT greedy-decoding vs RFT + MCTS-EBM; also compared with WizardMath and larger models in tables.",
            "key_finding": "MCTS-EBM further improves a stronger pretrained/finetuned baseline (RFT-7B) by several percentage points, showing the decoding-time verifier/search approach complements finetuning-based gains.",
            "uuid": "e264.2"
        },
        {
            "name_short": "RFT-13B",
            "name_full": "Rejection-Finetuned (RFT) 13B model (evaluated with MCTS-EBM)",
            "brief_description": "A 13B model from the RFT line used as a larger-capacity baseline; MCTS-EBM applied at decoding-time yields measurable improvements on math benchmarks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "RFT-13B",
            "model_size": "13B",
            "model_architecture": null,
            "arithmetic_operation_type": "multi-step arithmetic word problems (GSM8k/AQUA-RAT style)",
            "number_range_or_complexity": null,
            "method_or_intervention": "RFT supervised finetuning baseline with greedy decoding; MCTS-EBM applied as decoding-time verifier/search.",
            "performance_result": "GSM8k pass@1: greedy-decoding 55.40% -&gt; MCTS-EBM 61.48% (+6.08 pp).",
            "mechanistic_insight": null,
            "performance_scaling": "Higher-capacity model exhibits higher baseline accuracy and also benefits (~6 pp) from MCTS-EBM, indicating gains from path re-ranking are present across scales tested.",
            "failure_modes": null,
            "comparison_baseline": "Greedy-decoding vs MCTS-EBM applied to same base model.",
            "key_finding": "Even at 13B scale, decoding-time energy-guided search meaningfully improves multi-step arithmetic performance, supporting that better path verification/ranking complements model scale.",
            "uuid": "e264.3"
        },
        {
            "name_short": "Residual-EBM (Energy Function)",
            "name_full": "Residual Energy-Based Model trained via Noise Contrastive Estimation (E_θ)",
            "brief_description": "A learned scalar energy function E_θ(x) that multiplies a fixed instruction-tuned LM's probability with exp(-E_θ(x)) to prefer human-like reasoning paths; trained with NCE using model-generated negatives (rejection sampling and suboutput sampling).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Energy model trained on outputs of instruction-tuned LMs (deberta-large backbone + linear layer used as E_θ in experiments)",
            "model_size": null,
            "model_architecture": "encoder-style classifier backbone (DeBERTa-large used as E_θ feature extractor in experiments)",
            "arithmetic_operation_type": "evaluates complete reasoning paths for multi-step arithmetic word problems (serves as verifier for arithmetic step correctness and plausibility)",
            "number_range_or_complexity": null,
            "method_or_intervention": "Residual formulation: P_θ(x) = P_LM(x) * exp(-E_θ(x)) / Z_θ; energy trained by Noise Contrastive Estimation (NCE) with positives from dataset and negatives from P_LM (two negative-generation strategies: rejection sampling and suboutput sampling).",
            "performance_result": "When trained with both rejection and suboutput negatives (ebm-both) and used in MCTS, yields best improvements: sample-then-rank-ebm-both 46.77% (10 samples) and MCTS-ebm-both 52.23% (single-path) on GSM8k SFT baseline; ebm-reject is weaker (e.g., sample-then-rank-ebm-reject 43.82%, MCTS-ebm-reject 45.18%).",
            "mechanistic_insight": "Energy acts as a path-level verifier: NCE trains E_θ to assign lower energy to gold training reasoning paths and higher energy to model-generated paths; combining near-gold negatives (suboutput) with rejection-sampled negatives produces a discriminator sensitive to both final-answer correctness and intermediate-step plausibility, improving reranking of arithmetic solution chains.",
            "performance_scaling": "Quality of negative sampling influences final gains; ebm-both outperforms ebm-reject. Energy is format-specific and must be trained for the model/output format used in decoding.",
            "failure_modes": "Energy is tied to the input-output formatting of training data; an E_θ trained on one format may not evaluate paths from a model with a different output format (reducing or reversing gains).",
            "comparison_baseline": "Compared energy variants trained with different negative samplers (reject vs both) and used in both sample-then-rank and MCTS pipelines.",
            "key_finding": "A residual energy function trained by NCE with carefully chosen model-generated negatives can serve as an effective path-level verifier for arithmetic reasoning, enabling MCTS to prioritize correct multi-step solutions at inference time.",
            "uuid": "e264.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Residual energy-based models for text generation",
            "rating": 2,
            "sanitized_title": "residual_energybased_models_for_text_generation"
        },
        {
            "paper_title": "Discriminator-guided multi-step reasoning with language models",
            "rating": 2,
            "sanitized_title": "discriminatorguided_multistep_reasoning_with_language_models"
        },
        {
            "paper_title": "Solving quantitative reasoning problems with language models",
            "rating": 2,
            "sanitized_title": "solving_quantitative_reasoning_problems_with_language_models"
        },
        {
            "paper_title": "Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct",
            "rating": 2,
            "sanitized_title": "wizardmath_empowering_mathematical_reasoning_for_large_language_models_via_reinforced_evolinstruct"
        }
    ],
    "cost": 0.01673125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function</p>
<p>Haotian Xu htxu91@gamil.com 
Independent Researcher</p>
<p>No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function</p>
<p>Large language models (LLMs) demonstrate impressive language understanding and contextual learning abilities, making them suitable for natural language processing (NLP) tasks and complex mathematical reasoning. However, when applied to mathematical reasoning tasks, LLMs often struggle to generate correct reasoning steps and answers despite having high probabilities for the solutions. To overcome this limitation and enhance the mathematical reasoning capabilities of fine-tuned LLMs without additional fine-tuning steps, we propose a method that incorporates Monte Carlo Tree Search (MCTS) and a lightweight energy function to rank decision steps and enable immediate reaction and precise reasoning. Specifically, we re-formulate the fine-tuned LLMs into a Residual-based Energy Model (Residual-EBM) and employ noise contrastive estimation to estimate the energy function's parameters. We then utilize MCTS with the energy function as a path verifier to search the output space and evaluate the reasoning path. Through extensive experiments on two mathematical reasoning benchmarks, GSM8k and AQUA-RAT, we demonstrate the exceptional capabilities of our method, which significantly improves the pass@1 metric of the fine-tuned model without requiring additional fine-tuning or reinforcement learning with human feedback alignment.Preprint. Under review.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have achieved almost best performance on various multi-step reasoning tasks including coding [1][2][3] and math [4][5][6][7][8][9]. The integration of complex reasoning capabilities [10][11][12][13][14] empowers LLMs to solve more complex tasks such as [15] and [16]. [17] recently propose to apply Monte Carlo Tree Search (MCTS) [18] to accomplish these complex task with balance between exploration and exploitation. [19,20] propose a discriminator-guided multistep decoding methods that training a step or path scoring model to guide the decoding process. On the other hand, [6,21] design a specific data augmentation technique to generate more supervised finetuning data and close-sourced-model-based process scoring to employ reinforcement learning (RL). Those methods boost the mathematical reasoning abilities by a large margin compared to Supervised Fine-Tuning (SFT). However, those methods needs to design a specific scoring function including path contrastive learning or using closed-sourced models restricting the adaptability and generalizability of LLMs in practical scenarios. Can we unlock the mathematical reasoning capability of pretrained language models (LLMs) without the need for task-specific expert knowledge and data augmentation during the re-training process? Is it possible to adapt LLMs and utilize their reasoning ability solely during inference time?</p>
<p>Inspired by [6,21], We propose a novel framework to improve the Mathematical Reasoning ability with a given LLMs. The core idea is to reformulate a LLMs to a Residual Energy-based Model [22].</p>
<p>The main intuition is to modify the distribution with a energy function to make it closer to desired target distribution [22]. Unlike [6,21], energy function can be served as a path scoring function with a strong theoretical guarantee. However, training energy function is hard due to the intractable partition function [23] using Maximum log-likelihood Estimation (MLE). Usually, we optimize energy function using Noise Contrastive Estimation (NCE) [24]. NCE needs samples from data distribution as positive sample and noise samples from noise distribution. The LLMs can be served as noise distribution to generate noise samples as much as possible. This enlighten us to apply energy function to guide the MCTS for solving mathematical problems. We conduct several experiments on GSM8k [25] and AQUA-RAT [26]. Our method boost the pass@1 accuracy for supervised fine-tuning(SFT) LLM from 41.9 to 52.23 surpassing RFT [6] which using more data to finetune a LLM. It also achieves comparable performance compared to SOTA [21] without complicated data generation and reinforcement learning procedure. Given the released model provided by [6], our method can further improve the performance by a large margin and surpass the WizardMath [21] by a large margin.</p>
<p>The main contributions of this work are as followings:</p>
<ol>
<li>We propose the use of Residual EBM to reformulate the initial LLMs in order to achieve a desired target distribution. Furthermore, we integrate the energy function as the scoring mechanism for the Monte Carlo Tree Search (MCTS) algorithm to guide the decoding process. 2. We propose rejection sampling and suboutput sampling as methods for generating noise samples from Language Models (LLMs). These approaches eliminate the requirement for task-specific expert knowledge, making them highly versatile for various problems and datasets. 3. We utilize a combination of generated noise samples and the training dataset to optimize the energy function using Noise Contrastive Estimation (NCE). This approach enhances the model's ability to distinguish between real and generated data. Additionally, we employ Monte Carlo Tree Search (MCTS), guided by the energy function, to further improve the pass@1 accuracy from 41.9 to 52.23 compared to the initial Latent Language Models (LLMs) on GSM8k. 4. MCTS guided by an energy function significantly improves the pass@1 accuracy of released models, as demonstrated in the study by RFT [6]. The pass@1 accuracy for the RFT-7B model has been boosted from 50.3% to 56.78%, while the RFT-13B model has witnessed a remarkable increase from 55.4% to 61.4%. This improvement is substantial and highlights the efficacy of employing MCTS with an energy function to enhance the accuracy of these models.</li>
</ol>
<p>Method</p>
<p>In this section, we present the proposed framework, which initially transforms the fine-tuned language models (LLMs) into a Residual Energy-based Model (Residual-EBM) [22], and then employs Monte Carlo Tree Search (MCTS) [18] to achieve a better trade-off between exploration and exploitation. Our proposal recommends utilizing the energy function derived from Residual-EBM to guide the MCTS search process for discovering the optimal solution.</p>
<p>As shown in the Figure 1, our methods apply four steps:</p>
<ol>
<li>Train a locally normalized language model, called P LM , using a dataset of instructionresponse pairs. Alternatively, we can also utilize a pre-trained SFT model, known as P LM , tailored for a specific task. 2. Formulate the residual interpretation and employ a generative model in the format of P LM exp(−E θ (x)) [22]. Here, P LM represents a finetuned model that remains constant during both training and inference, while E θ denotes the energy function that is parameterized by θ. 3. Train the energy function E θ using noise contrastive estimation (NCE). 4. Applying MCTS to the decoding process involves using the exp(−E θ (x)) guide to balance exploration and exploitation. To achieve this, the Monte Carlo Tree Search (MCTS) algorithm can be adapted to include the exp(−E θ (x)) guide as a scoring function. This scoring Figure 1: Illustration of the reasoning path generation process as a tree exploration from the prompt.</li>
</ol>
<p>function will assess the potential of a specific decoding path or node in the search tree during both the exploration and exploitation stages.</p>
<p>Instruction-tuning</p>
<p>We firstly fine tune the base, e.g Llama2 [27] with supervised instruction-response pairs.</p>
<p>Formalize residual interpretation of LLMs</p>
<p>Following [22], we get the Residual Energy-based Model via:
P θ (x m+1 , · · · , x T |x 1 , · · · , x m ) = P LM (x m+1 , · · · , x T |x 1 , · · · , x m ) exp(−E θ (x 1 , · · · , x T )) Z θ (x 1 , · · · , x m ) (1) where [x 1 , · · · ,
x m ] represents the instruction and [x m+1 , · · · , x T ] represents the response, with x j belonging to the vocabulary V . However, estimating the partition function Z θ (x 1 , · · · , x m ), which is a normalizing constant dependent on the instruction [x 1 , · · · , x m ], is computationally infeasible.</p>
<p>Training Energy function via Noise Contrastive Estimation</p>
<p>Training globally normalized models via Maximum Likelihood Estimation (MLE) is challenging due to the intractability of the partition function [23,28].</p>
<p>Instead, we use Noise Contrastive Estimation (NCE) [24] to train the energy function E θ (x). NCE requires samples from both the model distribution and a noise distribution. The model distribution is defined as the joint model in Equation 1, denoted as P θ . On the other hand, the noise distribution is represented by the instruction-tuned model, P LM . NCE then trains a binary classifier on the difference of log-probability scores between these two models. The objective function is defined as follows:
max E x+∼P data log 1 1 + exp(E θ (x + )) + E x−∼P LM log 1 1 + exp(−E θ (x − ))(2)
where x + is sampled from data distribution, and x − is drawn from P LM . Training energy function equals to train a binary classifier to discriminate between real response and response generated by P LM . The objective is to allocate the maximum negative energy to real data and the maximum positive energy to data generated by the model.</p>
<p>The noise distribution is crucially important for NCE training [29]. In this work, we use the instruction tuned P LM as the noise distribution. In order to generate noise samples from P LM , we propose two different sampling methods:</p>
<ol>
<li>rejection sampling: generate response given instruction of training data and find out those with a correct answer as the noise samples [6]. The samples from rejection sampling make energy function to discriminate the reasoning steps rather than final answer. 2. Suboutput sampling: Generating a response by considering a sub-path of the ground truth response. The suboutput sampling method generates outputs by taking into account the instruction and the first-k steps of the ground truth response as the input. This sampling technique allows the language model probability (P LM ) to generate responses with more similarities to the ground truth response, making it challenging for the energy function to differentiate between real and fake responses.</li>
</ol>
<p>Monte Carlo Tree Search guided by Energy Function</p>
<p>Monte Carlo Tree Search (MCTS) [18] is a suitable algorithm for solving sequential decision problems. It is a tree search algorithm that effectively balances exploration and exploitation. In MCTS, nodes in the tree represent states, which in our case are sentences rather than individual words. The edges represent transitions or actions from one state to another. Since generating reasoning solution paths for GSM8k and AQUA-RAT often requires 3 to 10 reasoning sentences, representing a word as a node state in this algorithm would be computationally inefficient. To optimize efficiency and performance, we use a sentence as the state of a node, greatly reducing computational resources needed.</p>
<p>MCTS, or Monte Carlo Tree Search, is an algorithm that employs a heuristic approach and randomness to efficiently address deterministic problems, which would otherwise be infeasible to solve using conventional methods owing to the vastness of the search space. Within the MCTS framework, each iteration encompasses four consecutive steps.</p>
<p>Selection Selection means to choose a child node from the current node. The generated sentence probability calculated by P LM is denoted as the node prior. During the selection phase, the children are chosen based on the Upper Confidence Trees (UCT) [30]:
a * = arg max a∈A(s) Q(s, a) + C ln [N (s)] N (s, a)(3)
whereA(s) represents the set of available nodes in state s. Q(s, a) indicates the average reward obtained by taking action a in state s based on previous simulations. N (s) represents the number of times state s has been visited in the past iterations, while N (s, a) represents the number of times action a has been sampled in state s. The constant C is used to balance the trade-off between exploration and exploitation. Typically, Q(s, a) is calculated by combining the energy function exp(−E θ (x)), where x represents the instruction and response, and the node prior calculated by P LM corresponding to the sentence of the current node.</p>
<p>Expansion If the selected node is not terminal node that the sentence doesn't contain a terminal token, we create its child node that applying P LM to generate a sentence based on the sequence represented by its parent node until root as the state of this child node.</p>
<p>Simulation (roll-out) Based on the expanded node, we apply P LM to generate the following sentences until a terminal token is appeared in a sentence.</p>
<p>Backpropagation, We backpropagate the reward calculated by exp(−E θ ) from the terminal node to the root and update the reward of node on roll-out path.</p>
<p>Inference When it reaches the maximum iterations, we select the child node based on the maximum of node visits. If there are multiple nodes sharing the same maximum of node visits, we then select the maximum of node reward as the child node.</p>
<p>Experiments</p>
<p>In this section, we conduct thorough experiments to investigate the empirical performance.</p>
<p>Baselines</p>
<p>Open-Source Models. Massive open-source LLMs [27,[31][32][33] have been accessible to the AI community. We mainly incorporate Llama 2 [27], Qwen 1 , RFT [6] and Wizard-Math [21] as our baselines. Due to the computation resources, we only apply our method to released RFT [6].</p>
<p>Implementation Details of Noise Distribution</p>
<p>We follow the optimization configuration of RFT [6] to train a Llama 2-7b as P LM to generate noise samples. The optimizer is Adam optimizer [34]: β 1 = 0.9,β 2 = 0.999, gradient clip of 1.0, and L2 weight decay of 0.1. We search learning rate in [1e − 5, 2e − 5, 3e − 5, 5e − 5] with the training epochs in [3,5] and cosine learning decay schedule.</p>
<p>The Llama 2 [27] base serves as our foundation model.</p>
<p>We train the Llama 2 by employing the prompt from Alpaca [31]:</p>
<p>Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Response:</p>
<p>Implementation Details of Energy Function. We follow the optimization configuration [6] to train a Llama 2-7b as our P LM . We use deberta-large [35] as backbone followed by a linear layer as energy function E θ (x). The optimization for energy function with the following parameters for Adam optimizer [34]: β 1 = 0.9,β 2 = 0.999, gradient clip of 1.0, and L2 weight decay of 0.1. We search learning rate in [1e − 5, 2e − 5, 3e − 5, 5e − 5] with the training epochs in [3,5].</p>
<p>To train energy function, we use different noise samples from rejection sampling and suboutput sampling. The E θ (x) trained using noise samples from rejection sampling is denoted as ebm-reject. The E θ (x) trained using noise samples from rejection sampling and subout sampling is denoted as ebm-both.</p>
<p>Implementation Details of MCTS. We follow the configuration of MCTS and MCTS implementation [20]. The number of child nodes of the root node is 10 and the child nodes of the other node is 2.</p>
<p>The maximum iterations of MCTS is 20. When it reaches the the maximum iterations, we select the child node based on the maximum numbers of node visits and the maximum value of node reward derived from energy function.</p>
<p>Target In our experiments, we are interested in answering the following questions:</p>
<p>• Does the energy-function enhance mathematical reasoning abilities from a path ranking perspective?</p>
<p>• Do different noise sample generation methods, e.g rejection-sampling and suboutputsampling affect path ranking and MCTS (Monte Carlo tree search)? • Does MCTS guided by energy function improve the math reasoning abilities on open-sourced models?</p>
<p>Evaluation Benchmarks</p>
<p>We evaluate our method on two benchmarks (GSM8k [25], AQUA-RAT [26]). The GSM8k [25] dataset contains approximately 7500 training data and 1319 test data, mainly on grade school level math problems requiring 2 to 8 steps to solve. The AQUA-RAT [26] collects 100, 000 algebra-based word problems, each accompanied by a natural language rationale. Each example contains a question, rationale, four to five options and one correct option. We use full the dataset for supervised finetuning and 10,000 examples to generate noise samples for training energy-function. 1. Sample-then-rank with ebm-both is considerably more effective compared to greedydecoding and ebm-reject. As MCTS rollouts from the fixed prefix sequence, it is crucial for the energy-function to incorporate the ability to evaluate these paths. Rejection sampling, on the other hand, only focuses on paths with correct answers, which makes it difficult to evaluate the path sharing the same prefix sequence accurately.</p>
<p>Evaluation on GSM8k</p>
<ol>
<li>
<p>MCTS guided by EBM demonstrates a significant improvement in performance compared to a finetuned model. It also highlights the significance of the quality of noise samples. Employing rejection sampling and suboutput sampling for energy function results in better pass@1 accuracy compared to rejection sampling trained energy function.</p>
</li>
<li>
<p>MCTS guided by energy function outperforms the greedy-decoding by 10.54 and achieves comparable results to self-consistency-majority-voting demonstrating the effectiveness of our method.</p>
</li>
</ol>
<p>Comparing with the Open-Source Models. From Table 2, the detail results are as follows:</p>
<ol>
<li>
<p>Compared to WizardMath [21] and RFT [6], our method applied to instruction tuned Llama2-7B achieve comparable pass@1 accuracy without using more supervised finetuning data [6] or complicated RLHF alignment methods [21]. Our method incorporates Monte Carlo Tree Search (MCTS) during the inference stage, which improves the performance of the baseline model. The baseline model achieved a score of 41.9 using greedy decoding, whereas our method achieves a score of 52.23 on GSM8k.</p>
</li>
<li>
<p>Our method not only improves the sft-baseline but also significantly boosts the performance of RFT [6] on Llama2-7B, increasing it from 50.3% to 56.78%, and on Llama2-13B, increasing it from 55.40% to 61.48%. Additionally, RFT-7B with MCTS-ebm-both achieves better pass@1 accuracy compared to WizardMath-7B.</p>
</li>
<li>
<p>RFT-7B with MCTS-ebm-both also outperformed RFT-13b and AFT-13b by a significant margin, suggesting that smaller models can be enhanced through improved sampling methods.</p>
</li>
<li>
<p>The energy function trained on GSM8k format is not applicable to a different base model trained on a different input-output format. Therefore, the energy function trained on GSM8k format is not suitable for evaluating the path generated by Wizard-math-7B, which has a different output format. Consequently, this leads to poorer results. Table 3: Performance of our method on AQUA-RAT test. We evaluate the pass@1 accuracy on open-sourced models and methods including RFT [6] and AFT [36]. Noting that we train our own SFT model using the full dataset while AFT [36] only uses 5000-samples due to the efficiency. model pass@1 path-num greedy-decoding 34.25 1 MCTS-ebm-both 38.18 1 RFT-7b [36] 33.25 1 RFT-13b [36] 34.95 1 AFT-7b [36] 33.49 1 AFT-13b [36] 35.78 1</p>
</li>
</ol>
<p>Evaluation on AQUA-RAT</p>
<p>Comparing with the different methods. From the Table 3, though the results can't be compared directly, we still can conclude that MCTS-EBM-Both can improve the greedy-decoding baseline by a large margin and shows the efficiency of our method. It can also beat the larger model tuned on Llama2-13b.</p>
<p>Case Study</p>
<p>Appendix A shows some examples generated by our method. The examples demonstrate that our model consistently generates accurate response answers accompanied by clear explanations.</p>
<p>Related Work</p>
<p>Large Language Models based Mathematical Reasoning. LLMs have achieved substantial advancements on various Natural Language Processing (NLP) tasks. These models are first pretrained on the hundreds of billions tokens, which equips them with substantial common sense and knowledge to solve several problems. Due to the complexity and diversity of reasoning tasks, LLMs struggle to solve these tasks accurately, which include common-sense reasoning [37], logical reasoning [38], and mathematical reasoning [39][40][41][42][43] that often requires to understand mathematical concepts, computation and multi-step reasoning.</p>
<p>To enhance the mathematical reasoning ability of LLMs, numerous methods have been proposed. [11] proposed CoT demonstrating its capability to empower LLMs with fine-grained reasoning ability to decompose complex questions into sequential intermediate steps. [13] further suggest the exploration of diverse inference paths throughout the reasoning process with path score or majority voting.</p>
<p>Recently, [6] study the relationship how the pretraining loss, augmented data amount influence the reasoning performances of a LLM. They propose a rejection-sampling method to generate augment data to supervised training of Llama 2 and achieves improvments on GSM8k. [21] furhter propose a reinforcement learning to augment LLM with more powerful mathematical reasoning ability. The key idea is to apply Reinforcement Learning from Evol-Instruct Feedback (RLEIF) to make a better alignment to mathematical reasoning tasks.</p>
<p>Discriminator Guided LLM Decoding. Many works have proposed discriminator-guided LLM decoding to empower the multi-step reasoining of LLMs. [19] propose a Guiding Multi-step ReAsoning with a CorrectnEss Discriminator (GRACE), that use a discriminator to guide the step decoding. [44] further propose a MCTS-based method to solve math word problem (MWPs). It use a step-scoring model and path-scoring model to update the reward of MCTS.</p>
<p>Conclusion and Future Work</p>
<p>This paper introduces a novel decoding method. Firstly, it formulate a fintuned LM to Residual EBM. Secondly, it employ NCE to efficiently train a energy function. Thirdly, it use MCTS guided by energy function to sample the multistep reasoning steps. Without any finetuning or complicated reinforcement learning, our method achieves comparable performance compared to [6,21] on two widely recognized mathematical reasoning benchmarks: GSM8k and MATH.</p>
<p>Future Work. Although our method achieves impressive mathematics performance, it needs more computation resources for decoding since it needs to roll-out many times during MCTS process. Therefore, in future work, we will design better tree policy to reduce the amounts of unnecessary roll-outs. Besides, the generalizability of energy-function is limited that it can boost the performance of the SFT model using the same output format. In the future, we will study the generalizability of energy-function and try to develop a method for black-box model.</p>
<p>Broader Impact. Since energy function could be trained via NCE and the noise samples is easy to acquire. This method could be used as a powerful test-time adaption method to adapt to new task without tuning the base LLM.</p>
<p>A Appendix</p>
<p>A.1 GSM8k Case Study Note: RFT-7b-MCTS-EBM answers correctly.</p>
<p>RFT-7b</p>
<p>The proportion of good oranges is 100% -20% -10% -20% = 50%. The number of good oranges is 25 * 50% = «25<em>50</em>.01=12.5»12.5. #### 12.</p>
<p>RFT-7b-MCTS-EBM</p>
<p>The number of unripe oranges is 25 * 0.2 = «25*0.2=5»5 oranges. The number of sour oranges is 2 oranges. The number of bad oranges is 1 oranges. So, the number of good oranges is 25 -5 -2 -1 = «25-5-2-1=17»17 oranges. #### 17. Ground Truth Answer: 100</p>
<p>A.2 GSM8k Case Study</p>
<p>Note: RFT-7b-MCTS-EBM answers correctly.</p>
<p>RFT-7b</p>
<p>The total cost of Furniture United's offer is $1,350 + ($350 x 6) = $«1350+350<em>6=2250»2,250. The total cost of Cozy Homes' offer is $1,100 + ($250 x 9) = $«1100+250</em>9=2250»2,250. The cost difference between the two offers is $2,250 -$2,250 = $«2250-2250=0»0.. #### 0.</p>
<p>RFT-7b-MCTS-EBM</p>
<p>All the monthly installments amounts to $350/month x 6 months = $«350<em>6=2100»2100. Similarly, all the monthly installments amounts to $250/month x 9 months = $«250</em>9=2250»2250. The total cost of Furniture United's offer is $1350 + $2100 = $«1350+2100=3450»3450. The total cost of Cozy Homes' offer is $1100 + $2250 = $«1100+2250=3350»3350. Thus, the cost difference between the two offers is $3450 -$3350 = $«3450-3350=100»100. #### 100.</p>
<p>Instruction: Suzie loves to chew fruit-flavored gum. She bought four packs of gum the last time she was at the store. She got two packs of her favorite flavor, strawberry. She paid $2 for a pack of grape gum that she also liked. She wanted to try something new, so she paid half as much for a small pack of green apple gum. If she paid $7 in all, how many dollars did each pack of strawberry gum cost?.</p>
<p>Ground Truth Answer: 2</p>
<p>Note: RFT-7b-MCTS-EBM answers correctly.</p>
<p>RFT-7b</p>
<p>Suzie bought 2 packs of strawberry gum and 1 pack of grape gum. She paid $2 for the grape gum and $7 -$2 = $«7-2=5»5 for the strawberry gum. She bought 1 pack of green apple gum and paid half as much as the strawberry gum, so she paid $5 / 2 = $«5/2=2.50»2.50 for the green apple gum. She bought 2 packs of strawberry gum, 1 pack of grape gum, and 1 pack of green apple gum for a total of 2 + 1 + 1 = «2+1+1=4»4 packs of gum. She paid $5 for 2 packs of strawberry gum and $2.50 for 1 pack of green apple gum, so she paid $5 + $2.50 = $«5+2.5=7.50»7.50 for 3 packs of gum. She bought 4 packs of gum and paid $7.50 for 3 packs, so she paid $7.50 / 4 = $«7.5/4=1.875»1.875 for each pack of strawberry gum. #### 1.</p>
<p>RFT-7b-MCTS-EBM</p>
<p>Let S be the cost of a pack of strawberry gum. The cost of a pack of grape gum was 2 * 2 = «2<em>2=4»4. The cost of a pack of green apple gum was 2 * 0.5 = «2</em>0.5=1»1. S + 4 + 1 + 2 = 7. 3S = 7 -1 -4 = 2. S = «2=2»2. The cost of a pack of strawberry gum was $«2=2»2. #### 2.</p>
<p>Figure 2 :
2Illustration of the reasoning path generation process as a tree exploration from the prompt.</p>
<p>Table 1 :
1Comparing with the different decoding methods. FromTable 1, the detail results are as follows:Performance of our method on GSM8k test. We evaluate the pass@1 accuracy for greedy-
decoding and MCTS guided by EBM. We also employ self-consistency with majority voting, sample-
then-rank as our baselines. ebm-reject denotes training energy function using samples from rejection 
sampling. ebm-both denotes training energy function using samples from rejection sampling and 
suboutput sampling. 
decoding-method 
pass@1 path-num 
greedy-decoding 
41.69 
1 
self-consistency-majority-voting 
52.84 
10 
sample-then-rank-ebm-reject 
43.82 
10 
sample-then-rank-ebm(both) 
46.77 
10 
MCTS-ebm-reject 
45.18 
1 
MCTS-ebm-both 
52.23 
1 </p>
<p>Table 2: Performance of our method on GSM8k test. We evaluate the pass@1 accuracy on open-
sourced models and methods including RFT [6], AFT [36], Qwen-7b and WizardMath [21]. We also 
apply our method to two strong baselines, namely RFT [6] and WizardMath [21]. </p>
<p>model 
params decoding-method 
pass@1 </p>
<p>Llama2 [27] 
7B 
greedy-decoding 
41.69 
SFT 
MCTS-EBM 
52.23(+10.54) </p>
<p>RFT [6] </p>
<p>7B 
greedy-decoding 
50.30 
MCTS-EBM 
56.78(+6.48) </p>
<p>13B 
greedy-decoding 
55.40 
MCTS-EBM 
61.48(+6.08) </p>
<p>WizardMath [21] </p>
<p>7B 
greedy-decoding 
54.90 
MCTS-EBM 
49.50(-5.40) </p>
<p>13B 
greedy-decoding 
63.90 
MCTS-EBM 
-</p>
<p>Qwen 2 
7B 
greedy-decoding 
51.60 
MCTS-EBM 
-</p>
<p>AFT [36] </p>
<p>7B 
greedy-decoding 
44.25 
MCTS-EBM 
-</p>
<p>13B 
greedy-decoding 
51.03 
MCTS-EBM 
-</p>
<p>Table 4 :
4A comparison case on different decoding method Instruction: Every day, Wendi feeds each of her chickens three cups of mixed chicken feed, containing seeds, mealworms and vegetables to help keep them healthy. She gives the chickens their feed in three separate meals. In the morning, she gives her flock of chickens 15 cups of feed. In the afternoon, she gives her chickens another 25 cups of feed. How many cups of feed does she need to give her chickens in the final meal of the day if the size of Wendi's flock is 20 chickens?Note: RFT-7b-MCTS-EBM answers correctly.RFT-7bInthe morning, Wendi gives her chickens 15 cups of feed, and in the afternoon she gives them another 25 cups, for a total of 15+25 = «15+25=40»40 cups of feed. If the size of Wendi's flock is 20 chickens, then each chicken receives 40/20 = «40/20=2»2 cups of feed. In the final meal of the day, Wendi needs to give her chickens 2 cups of feed. #### 2. RFT-7b-MCTS-EBM Each chicken needs 3 cups of feed per day, so 20 chickens need 20<em>3=«20</em>3=60»60 cups of feed. In the morning and afternoon, Wendi gives her chickens a total of 15+25=«15+25=40»40 cups of feed. Thus, in the final meal of the day, Wendi needs to give her chickens 60-40=«60-40=20»20 cups of feed. #### 20.Instruction: A basket contains 25 oranges among which 1 is bad, 20% are unripe, 2 are sour and the rest are good. How many oranges are good?.Ground Truth Answer: 20 </p>
<p>Ground Truth Answer: 17 </p>
<p>Table 5 :
5A comparison case on different decoding method Instruction: Robert wants to buy a bookshelf and a sofa and has received two offers. Furniture United's offer includes a $1,350 advance payment and 6 monthly installments of $350 each. Cozy Homes' offer includes a $1,100 advance payment and 9 monthly installments of $250 each. What is the cost difference between the two offers?
https://github.com/QwenLM/Qwen-7B/</p>
<p>. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such. Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant MisraPeter Welinderand Wojciech Zaremba. Evaluating large language models trained on codeMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert- Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021.</p>
<p>Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, arXiv:2305.06161Starcoder: may the source be with you! arXiv preprint. Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023.</p>
<p>Wizardcoder: Empowering code large language models with evol-instruct. Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, Daxin Jiang, arXiv:2306.08568arXiv preprintZiyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with evol-instruct. arXiv preprint arXiv:2306.08568, 2023.</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, arXiv:2211.09085Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv preprintRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085, 2022.</p>
<p>Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, arXiv:2206.14858arXiv preprintAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858, 2022.</p>
<p>Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, Chang Zhou, arXiv:2308.01825Scaling relationship on learning mathematical reasoning with large language models. arXiv preprintZheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. Scaling rela- tionship on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825, 2023.</p>
<p>Progressive-hint prompting improves reasoning in large language models. Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, Yu Li, arXiv:2304.09797arXiv preprintChuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and Yu Li. Progressive-hint prompting improves reasoning in large language models. arXiv preprint arXiv:2304.09797, 2023.</p>
<p>Shima Imani, Liang Du, Harsh Shrivastava, Mathprompter, arXiv:2303.05398Mathematical reasoning using large language models. arXiv preprintShima Imani, Liang Du, and Harsh Shrivastava. Mathprompter: Mathematical reasoning using large language models. arXiv preprint arXiv:2303.05398, 2023.</p>
<p>Planand-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, Ee-Peng Lim, arXiv:2305.04091arXiv preprintLei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. Plan- and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. arXiv preprint arXiv:2305.04091, 2023.</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023.</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in Neural Information Processing Systems. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In Advances in Neural Information Processing Systems, 2022.</p>
<p>Automatic chain of thought prompting in large language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, arXiv:2210.03493arXiv preprintZhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493, 2022.</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.11171arXiv preprintXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.</p>
<p>Complexity-based prompting for multi-step reasoning. Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, Tushar Khot, arXiv:2210.00720arXiv preprintYao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting for multi-step reasoning. arXiv preprint arXiv:2210.00720, 2022.</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, L Thomas, Yuan Griffiths, Karthik Cao, Narasimhan, arXiv:2305.10601arXiv preprintShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601, 2023.</p>
<p>Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, arXiv:2307.16789Facilitating large language models to master 16000+ real-world apis. arXiv preprintYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789, 2023.</p>
<p>Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu, arXiv:2305.14992Reasoning with language model is planning with world model. arXiv preprintShibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992, 2023.</p>
<p>Monte carlo tree search: A review of recent modifications and applications. Konrad Maciejświechowski, Bartosz Godlewski, Jacek Sawicki, Mańdziuk, Artificial Intelligence Review. 563MaciejŚwiechowski, Konrad Godlewski, Bartosz Sawicki, and Jacek Mańdziuk. Monte carlo tree search: A review of recent modifications and applications. Artificial Intelligence Review, 56(3):2497-2562, 2023.</p>
<p>Discriminatorguided multi-step reasoning with language models. Muhammad Khalifa, Lajanugen Logeswaran, Moontae Lee, Honglak Lee, Lu Wang, arXiv:2305.14934arXiv preprintMuhammad Khalifa, Lajanugen Logeswaran, Moontae Lee, Honglak Lee, and Lu Wang. Discriminator- guided multi-step reasoning with language models. arXiv preprint arXiv:2305.14934, 2023.</p>
<p>Solving math word problems via cooperative reasoning induced language models. Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Yongfeng Huang, Ruyi Gan, Jiaxing Zhang, Yujiu Yang, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics1Long Papers)Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Yongfeng Huang, Ruyi Gan, Jiaxing Zhang, and Yujiu Yang. Solving math word problems via cooperative reasoning induced language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, 2023.</p>
<p>Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, Dongmei Zhang, arXiv:2308.09583Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprintHaipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023.</p>
<p>Residual energy-based models for text generation. Yuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam, Marc&apos;aurelio Ranzato, arXiv:2004.11714arXiv preprintYuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam, and Marc'Aurelio Ranzato. Residual energy-based models for text generation. arXiv preprint arXiv:2004.11714, 2020.</p>
<p>A practical guide to training restricted boltzmann machines. E Geoffrey, Hinton, Neural networks: Tricks of the trade. SpringerGeoffrey E Hinton. A practical guide to training restricted boltzmann machines. In Neural networks: Tricks of the trade, pages 599-619. Springer, 2012.</p>
<p>Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. Michael Gutmann, Aapo Hyvärinen, Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. the Thirteenth International Conference on Artificial Intelligence and StatisticsMichael Gutmann and Aapo Hyvärinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pages 297-304, 2010.</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.14168arXiv preprintKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics1Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 158-167, Vancouver, Canada, July 2017. Association for Computational Linguistics.</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288arXiv preprintHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.</p>
<p>A tutorial on energy-based learning. Predicting Structured Outputs. Yann Lecun, Sumit Chopra, Raia Hadsell, Marc&apos;aurelio Ranzato, Fu-Jie Huang, MIT PressYann LeCun, Sumit Chopra, Raia Hadsell, Marc'Aurelio Ranzato, and Fu-Jie Huang. A tutorial on energy-based learning. Predicting Structured Outputs, 2006. MIT Press.</p>
<p>Understanding hard negatives in noise contrastive estimation. Wenzheng Zhang, Karl Stratos, arXiv:2104.06245arXiv preprintWenzheng Zhang and Karl Stratos. Understanding hard negatives in noise contrastive estimation. arXiv preprint arXiv:2104.06245, 2021.</p>
<p>Balance seed scheduling via monte carlo planning. Heqing Huang, Hung-Chun Chiu, Qingkai Shi, Peisen Yao, Charles Zhang, IEEE Transactions on Dependable and Secure Computing. Heqing Huang, Hung-Chun Chiu, Qingkai Shi, Peisen Yao, and Charles Zhang. Balance seed scheduling via monte carlo planning. IEEE Transactions on Dependable and Secure Computing, 2023.</p>
<p>Stanford alpaca: An instruction-following llama model. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github. com/tatsu-lab/stanford_alpaca, 2023.</p>
<p>The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, Julien Launay, arXiv:2306.01116arXiv preprintGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023.</p>
<p>Vicuna: An open-source chatbot impressing gpt-4 with 90%<em> chatgpt quality. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, Eric P Xing, Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%</em> chatgpt quality, March 2023.</p>
<p>Adam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.</p>
<p>Deberta: Decoding-enhanced bert with disentangled attention. Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, arXiv:2006.03654arXiv preprintPengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention. arXiv preprint arXiv:2006.03654, 2020.</p>
<p>Making large language models better reasoners with alignment. Peiyi Wang, Lei Li, Liang Chen, Feifan Song, Binghuai Lin, Yunbo Cao, Tianyu Liu, Zhifang Sui, arXiv:2309.02144arXiv preprintPeiyi Wang, Lei Li, Liang Chen, Feifan Song, Binghuai Lin, Yunbo Cao, Tianyu Liu, and Zhifang Sui. Making large language models better reasoners with alignment. arXiv preprint arXiv:2309.02144, 2023.</p>
<p>Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant, Transactions of the Association for Computational Linguistics. 9Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346-361, 2021.</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.11903arXiv preprintJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.</p>
<p>A survey of deep learning for mathematical reasoning. Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, Kai-Wei Chang, arXiv:2212.10535arXiv preprintPan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and Kai-Wei Chang. A survey of deep learning for mathematical reasoning. arXiv preprint arXiv:2212.10535, 2022.</p>
<p>. Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, arXiv:2301.13867Philipp Christian PetersenAlexis Chevalier, and Julius BernerMathematical capabilities of chatgpt. arXiv preprintSimon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian Petersen, Alexis Chevalier, and Julius Berner. Mathematical capabilities of chatgpt. arXiv preprint arXiv:2301.13867, 2023.</p>
<p>A survey of question answering for math and science problem. Arindam Bhattacharya, arXiv:1705.04530arXiv preprintArindam Bhattacharya. A survey of question answering for math and science problem. arXiv preprint arXiv:1705.04530, 2017.</p>
<p>Deep neural solver for math word problems. Yan Wang, Xiaojiang Liu, Shuming Shi, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkAssociation for Computational LinguisticsYan Wang, Xiaojiang Liu, and Shuming Shi. Deep neural solver for math word problems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 845-854, Copenhagen, Denmark, September 2017. Association for Computational Linguistics.</p>
<p>MAWPS: A math word problem repository. Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, Hannaneh Hajishirzi, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSan Diego, CaliforniaAssociation for Computational LinguisticsRik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. MAWPS: A math word problem repository. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1152-1157, San Diego, California, June 2016. Association for Computational Linguistics.</p>
<p>Solving math word problem via cooperative reasoning induced language models. Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Ruyi Gan, Jiaxing Zhang, Yujiu Yang, arXiv:2210.16257arXiv preprintXinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Ruyi Gan, Jiaxing Zhang, and Yujiu Yang. Solving math word problem via cooperative reasoning induced language models. arXiv preprint arXiv:2210.16257, 2022.</p>
<p>John drives for 3 hours at a speed of 60 mph and then turns around because he realizes he forgot something very important at home. He tries to get home in 4 hours but spends the first 2 hours in standstill traffic. He spends the next half-hour driving at a speed of 30mph, before being able to drive the remaining time of the 4 hours going at 80 mph. How far is he from home at the end of those 4 hours?. GSM8k WizardMath Case Study Table 6: A comparison case on different output format Instruction. Ground Truth Answer: 45 RFT-7b In the first 3 hours, he travels 3<em>60=«3</em>60=180»180 milesGSM8k WizardMath Case Study Table 6: A comparison case on different output format Instruction: John drives for 3 hours at a speed of 60 mph and then turns around because he realizes he forgot something very important at home. He tries to get home in 4 hours but spends the first 2 hours in standstill traffic. He spends the next half-hour driving at a speed of 30mph, before being able to drive the remaining time of the 4 hours going at 80 mph. How far is he from home at the end of those 4 hours? Ground Truth Answer: 45 RFT-7b In the first 3 hours, he travels 3<em>60=«3</em>60=180»180 miles.</p>
<p>During this 2 hour driving time, he spends the first half-hour driving at a speed of 30mph, so he has driven 0.5<em>30=15 miles. The remaining time of the 2 hours is 2-1.5=0.5 hours, during which he drives at a speed of 80 mph, so he has driven 0.5</em>80=40 miles. Therefore, in total, he has driven 180+15+40=245 miles. ; <em>60=180 Wizardmath-7b John, Miles, 245Then he turns around and tries to get home in 4 hours. However, he spends the first 2 hours in standstill traffic, so he has only 2 hours of driving time left. Therefore, he has 4-2=2 hours of driving time left. Since he is 245 miles from home at the end of those 4 hours, the final answer is 245. The answer isWizardMath-7b John drives for 3 hours at a speed of 60 mph, so he has driven 3</em>60=180 miles. Then he turns around and tries to get home in 4 hours. However, he spends the first 2 hours in standstill traffic, so he has only 2 hours of driving time left. Therefore, he has 4-2=2 hours of driving time left. During this 2 hour driving time, he spends the first half-hour driving at a speed of 30mph, so he has driven 0.5<em>30=15 miles. The remaining time of the 2 hours is 2-1.5=0.5 hours, during which he drives at a speed of 80 mph, so he has driven 0.5</em>80=40 miles. Therefore, in total, he has driven 180+15+40=245 miles. Since he is 245 miles from home at the end of those 4 hours, the final answer is 245. The answer is: 245</p>
<p>Instruction: Eliza's rate per hour for the first 40 hours she works each week is $10. She also receives an overtime pay of 1.2 times her regular hourly rate. If Eliza worked for 45 hours this week, how much are her earnings for this week. Instruction: Eliza's rate per hour for the first 40 hours she works each week is $10. She also receives an overtime pay of 1.2 times her regular hourly rate. If Eliza worked for 45 hours this week, how much are her earnings for this week?.</p>
<p>Eliza earns 40 x $10 = $«40<em>10=400»400. For the overtime, she earns 1.2 x $10 = $«1.2</em>10=12»12 per hour. She worked for 45 -40 = «45-40=5»5 hours in overtime. Thus, Eliza earned $12 x 5 = $«12<em>5=60»60 for overtime. Therefore, her total earning for this week. is $400 + $60 = $«400+60=460»460. #### 460Ground Truth Answer: 460 RFT-7b For the first 40 hours. Ground Truth Answer: 460 RFT-7b For the first 40 hours, Eliza earns 40 x $10 = $«40</em>10=400»400. For the overtime, she earns 1.2 x $10 = $«1.2<em>10=12»12 per hour. She worked for 45 -40 = «45- 40=5»5 hours in overtime. Thus, Eliza earned $12 x 5 = $«12</em>5=60»60 for overtime. Therefore, her total earning for this week is $400 + $60 = $«400+60=460»460. #### 460</p>
<p>. Wizardmath-7b, WizardMath-7b</p>
<p>Eliza's regular pay for 45 hours is 45 hours x $10/hour = «45<em>10=450»450. Overtime pay for 45 hours is 45 hours x $10/hour x 1.2 = $«45</em>10<em>1.2=540»540. Her total earnings for this week is $450 + $540 = «450+540=990»990. 990Eliza's regular pay for 45 hours is 45 hours x $10/hour = «45</em>10=450»450. Overtime pay for 45 hours is 45 hours x $10/hour x 1.2 = $«45<em>10</em>1.2=540»540. Her total earnings for this week is $450 + $540 = «450+540=990»990. The answer is: 990.</p>
<p>AQUA-RAT Case Study Table 7: A comparison case on different decoding method Instruction: Find out which of the following values is the multiple of X, if it is divisible by 9. A , 12A.4 AQUA-RAT Case Study Table 7: A comparison case on different decoding method Instruction: Find out which of the following values is the multiple of X, if it is divisible by 9 and 12?</p>
<p>12 and 4, it must be divisible by 3 and 12. If a number is divisible by 3 and 12, it must be divisible by 9 and 12. If a number is divisible by 9, 4 and 12, it must be divisible by 3, 9 and 12. If a number is divisible by 3, 4 and 9. Ground Truth Answer: A Note: MCTS-EBM answers correctly. SFT If a number is divisible by 9. 12it must be divisible by 3, 9 and 12. If a number is divisible by 3, 4, 9. it must be divisible by 3, 9 and 12. If a number is divisible by 3, 4, 9, 12, 3 and 12, it must be divisible by 3, 9 and 12. If a number is divisible by 3, 4, 9. it must be divisible by 3, 9 and 12. If a number is divisible by 3, 4, 9. it must be divisible by 3, 9 and 12. If a number is divisible by 3, 4, 9, 12, 3, 9. MCTS-EBM Let the Value of X is A) 36 B) 15 C) 17 D) 5 E) 7. If value is X is multiple of 3 and 4, Then the value must be 36 Option A ### The answer is AGround Truth Answer: A Note: MCTS-EBM answers correctly. SFT If a number is divisible by 9, it must be divisible by 3. If a number is divisible by 12, it must be divisible by 3 and 4. If a number is divisible by 3 and 4, it must be divisible by 12. If a number is divisible by 9, 12 and 4, it must be divisible by 3 and 12. If a number is divisible by 3 and 12, it must be divisible by 9 and 12. If a number is divisible by 9, 4 and 12, it must be divisible by 3, 9 and 12. If a number is divisible by 3, 4 and 9, it must be divisible by 12 and 3. If a number is divisible by 3, 4, 9 and 12, it must be divisible by 3, 9 and 12. If a number is divisible by 3, 4, 9 and 12, it must be divisible by 3, 9 and 12. If a number is divisible by 3, 4, 9, 12 and 3, it must be divisible by 3, 9 and 12. If a number is divisible by 3, 4, 9, 12 and 12, it must be divisible by 3, 9 and 12. If a number is divisible by 3, 4, 9, 12, 3 and 12, it must be divisible by 3, 9 and 12. If a number is divisible by 3, 4, 9, 12, 3, 9 and 12, it must be divisible by 3, 9 and 12. If a number is divisible by 3, 4, 9, 12, 3, 9, 12 and 3, it must be divisible by 3, 9 and 12. If a number is divisible by 3, 4, 9, 12, 3, 9. MCTS-EBM Let the Value of X is A) 36 B) 15 C) 17 D) 5 E) 7. If value is X is multiple of 3 and 4, Then the value must be 36 Option A ### The answer is A.</p>
<p>The average (arithmetic mean) of the highest score and the lowest score is equal to x. If the average score for the entire class is equal to y and there are z students in the class, where z &gt; 5, then in terms of x, y, and z, what is the average score for the class excluding the highest and lowest scorers? A)(zy -2x)/z B)(zy -2)/z C)(zx -y)/(z -2) D)(zy -2x). Instruction: An exam is given in a certain class. z -2) E)(zy -x)/(z + 2Instruction: An exam is given in a certain class. The average (arithmetic mean) of the highest score and the lowest score is equal to x. If the average score for the entire class is equal to y and there are z students in the class, where z &gt; 5, then in terms of x, y, and z, what is the average score for the class excluding the highest and lowest scorers? A)(zy -2x)/z B)(zy -2)/z C)(zx -y)/(z -2) D)(zy -2x)/(z -2) E)(zy -x)/(z + 2).</p>
<p>SFT Let the highest score be a and the lowest score be b. So, x = (a + b)/2 (z -2)x = (a + b) Average score for the class excluding the highest and lowest scorers = (a + b + (z -2)x)/(z -2)= (a + b + x)/(z -2)= (zx -y)/(z -2) Answer: C ### The answer is C. Ground Truth Answer: D Note: MCTS-EBM answers correctly. Ground Truth Answer: D Note: MCTS-EBM answers correctly. SFT Let the highest score be a and the lowest score be b. So, x = (a + b)/2 (z -2)x = (a + b) Average score for the class excluding the highest and lowest scorers = (a + b + (z -2)x)/(z -2)= (a + b + x)/(z -2)= (zx -y)/(z -2) Answer: C ### The answer is C.</p>
<p>Excluding the two highest (or lowest) scores, the total score is (zy -h -l). The average of these scores is (zy -hl)/(z -2). Notice that we can also write this as ((yz -x -y) + h + l)/(z -2), which can be simplified to (zy -x -y + 2h + 2l)/(z -2). Now, since x = (h + l)/2, we can substitute x = (h + l)/2 for both h and l. (We can also substitute x for h and l in the original formula and simplify, but it is easier to substitute x for h + l first. Mcts-Ebm Xplanation, We can call the highest score h and the lowest score l, so we have: H + L = (x + y) The total score of the entire class is (y multiplied by the number of students)z. We can call this total zy. When we do this, the entire formula simplifies to (zy -2x)/(z -2), which is answer choice (D). Answer: D ### The answer is DMCTS-EBM xplanation: We are given that the average (arithmetic mean) of the highest score and the lowest score is equal to x, and that the average score for the entire class is equal to y. First, notice that the total of the highest and lowest scores is (x + y) multiplied by 2, since there are two items. We can call the highest score h and the lowest score l, so we have: H + L = (x + y) The total score of the entire class is (y multiplied by the number of students)z. We can call this total zy. Excluding the two highest (or lowest) scores, the total score is (zy -h -l). The average of these scores is (zy -h - l)/(z -2). Notice that we can also write this as ((yz -x -y) + h + l)/(z -2), which can be simplified to (zy -x -y + 2h + 2l)/(z -2). Now, since x = (h + l)/2, we can substitute x = (h + l)/2 for both h and l. (We can also substitute x for h and l in the original formula and simplify, but it is easier to substitute x for h + l first.) When we do this, the entire formula simplifies to (zy -2x)/(z -2), which is answer choice (D). Answer: D ### The answer is D.</p>            </div>
        </div>

    </div>
</body>
</html>
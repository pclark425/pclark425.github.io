<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1493 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1493</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1493</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-29.html">extraction-schema-29</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <p><strong>Paper ID:</strong> paper-2cf42464beec8bf79f75d01540c2e886a95abd3a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2cf42464beec8bf79f75d01540c2e886a95abd3a" target="_blank">Sim-to-Real Transfer Learning using Robustified Controllers in Robotic Tasks involving Complex Dynamics</a></p>
                <p><strong>Paper Venue:</strong> IEEE International Conference on Robotics and Automation</p>
                <p><strong>Paper TL;DR:</strong> It is shown that the amount of fine-tuning in transfer learning for a robustified controller is substantially reduced compared to a non-robustified controller.</p>
                <p><strong>Paper Abstract:</strong> Learning robot tasks or controllers using deep reinforcement learning has been proven effective in simulations. Learning in simulation has several advantages. For example, one can fully control the simulated environment, including halting motions while performing computations. Another advantage when robots are involved, is that the amount of time a robot is occupied learning a task—rather than being productive—can be reduced by transferring the learned task to the real robot. Transfer learning requires some amount of fine-tuning on the real robot. For tasks which involve complex (non-linear) dynamics, the fine-tuning itself may take a substantial amount of time. In order to reduce the amount of fine-tuning we propose to learn robustified controllers in simulation. Robustified controllers are learned by exploiting the ability to change simulation parameters (both appearance and dynamics) for successive training episodes. An additional benefit for this approach is that it alleviates the precise determination of physics parameters for the simulator, which is a non-trivial task. We demonstrate our proposed approach on a real setup in which a robot aims to solve a maze game, which involves complex dynamics due to static friction and potentially large accelerations. We show that the amount of fine-tuning in transfer learning for a robustified controller is substantially reduced compared to a non-robustified controller.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1493.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1493.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuJoCo+Ogre3D simulator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuJoCo physics engine with Ogre3D renderer (custom integrated simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A custom simulator implemented by the authors that combines MuJoCo for physical dynamics and Ogre3D for visual rendering; used to train A3C agents on a marble-maze control task with parameter randomization (robustification).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sim-to-Real Transfer Learning using Robustified Controllers in Robotic Tasks involving Complex Dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>MuJoCo + Ogre3D (custom integration)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>MuJoCo is used to simulate the marble dynamics (contacts, collisions, static and dynamic friction, damping, mass, constraints); Ogre3D is used to render images (lighting, appearance). The simulator runs as a separate process and communicates via sockets, returning rendered images, rewards and terminal flags for each action.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotic manipulation (contact dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium-to-high fidelity for mechanical/contact dynamics (approximate but detailed), high-fidelity visual rendering (configurable); overall an approximate real-world replica tuned to match dynamics but not exact.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Includes static friction, dynamic (rolling) friction, damping, collisions between marbles and maze geometry, marble mass, full 3D maze geometry reconstructed from measurements, emulated camera delay, additive Gaussian white noise on observations, configurable lighting; physics parameters were manually tuned to match observed marble oscillations and are sampled from ranges during training for randomization. Does not claim exact, fully identified real-world physics parameters; some parameters are visually / empirically chosen rather than learned.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>A3C agent (asynchronous advantage actor-critic) — robustified and non-robustified variants</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Deep RL agent: convolutional layers -> fully-connected -> LSTM; actor-critic heads; auxiliary tasks (pixel change, reward prediction) and generalized advantage estimation; policies trained with dynamics/appearance randomization to produce 'robustified' controllers.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Control/plan continuous sequences of incremental rotations of a maze to maneuver one or two marbles from outer ring to center (sparse-reward maze-solving control task under complex contact dynamics).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Single-marble: robustified policy converged to 100% success after ~4.0M simulation steps; non-robustified policy converged to 100% after ~4.5M steps. Two-marble: offline robustified policy achieved 100% after ~3.0M steps.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Real-world robot setup (Mitsubishi Electric robot arm + Intel RealSense camera) — sim-to-real transfer</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Single-marble: robustified policy required ~55K fine-tuning steps on the real robot to converge to 100% success; non-robustified policy required ~220K fine-tuning steps and reached ~≈90% success and occasional failures. Two-marble: transferred robustified policy fine-tuned ≈225K steps to reach ~75% success. Transferring a single-marble policy to the two-marble real task required much more fine-tuning (~400K steps to ≈50% success).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>The paper compares training with fixed (non-robustified) simulator parameters versus randomized (robustified) simulator parameters. Dynamics/appearance randomization (robustified) produced policies that transferred much more efficiently: e.g., single-marble robustified TL required ~55K fine-tuning steps vs ~220K for non-robustified, and achieved higher final success and more consistent episode lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>The authors state precise determination of physics parameters is non-trivial and that randomizing physics/appearance/system parameters during simulation training reduces the need for exact parameter identification; they emphasize that modeling static/dynamic friction, damping, collisions and camera/system delays are important, while additional appearance changes (lighting color/intensity) had little effect for their setup.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Non-robustified (fixed-parameter) simulation pre-training transferred poorly relative to robustified training: required ~4x more fine-tuning and produced occasional failures (did not reliably reach 100% success). Learning entirely online (real robot) for the two-marble task failed even after 1M steps (0% success with initial reward scheme), highlighting that absence of prior simulated training hindered success for more complex dynamics and multi-object interactions. Transferring a single-marble policy to two marbles required substantially more fine-tuning and achieved lower success than a policy trained in-simulation for two marbles.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim-to-Real Transfer Learning using Robustified Controllers in Robotic Tasks involving Complex Dynamics', 'publication_date_yy_mm': '2018-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1493.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1493.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuJoCo (Multi-Joint dynamics with Contact)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A physics engine used by the authors to simulate contact-rich marble dynamics (contacts, collisions, friction, damping) of the marble-maze task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sim-to-Real Transfer Learning using Robustified Controllers in Robotic Tasks involving Complex Dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A commercial/academic rigid-body physics engine that models contact dynamics and constraints; used to simulate marbles interacting with maze geometry including contacts and collisions.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / contact dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium-to-high fidelity for rigid-body contact dynamics (capable of modeling contacts, collisions, friction and constraints); used with manually tuned parameters to approximate the real maze.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Models contacts/collisions, static and dynamic friction, damping, masses and constraints; authors tuned static friction, dynamic friction and damping to match marble oscillations; physics parameters were sampled (randomized) from ranges per episode for robustified training. Does not inherently guarantee exact reproduction of real-world delays or unmodeled effects.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>A3C agent (as above)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>See combined entry; A3C neural policy trained within MuJoCo-simulated environment using image observations produced by Ogre3D.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Learn control policy to manipulate maze orientation to direct marbles through gates into center (planning under contact-rich dynamics).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>As part of the combined simulator: robustified single-marble policy ~4.0M sim steps to 100% success; non-robustified ~4.5M steps to 100% success; two-marble robustified ~3.0M to 100%.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Real robot (sim-to-real transfer)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>See combined entry: robustified policies transferred requiring substantially fewer fine-tuning steps (~55K for single marble) and achieving higher success than non-robustified.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Randomizing MuJoCo physics parameters per episode (robustification) outperformed fixed-parameter MuJoCo training for transfer: far fewer fine-tuning steps and more reliable transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Authors tuned MuJoCo parameters to visually/empirically match real dynamics; they note that accurate identification is hard and that randomization can compensate for imperfect parameter identification. They emphasize including friction, damping and collisions in the physics model.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Fixed-parameter MuJoCo training (non-robustified) led to worse transfer results (more fine-tuning and incomplete success).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim-to-Real Transfer Learning using Robustified Controllers in Robotic Tasks involving Complex Dynamics', 'publication_date_yy_mm': '2018-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1493.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1493.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ogre3D</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ogre (Object-Oriented Graphics Rendering Engine) 3D</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 3D graphics rendering engine used to produce visual observations for the simulated marble-maze environment, enabling image-based policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sim-to-Real Transfer Learning using Robustified Controllers in Robotic Tasks involving Complex Dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Ogre3D</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A real-time 3D graphics engine used to render the simulated maze and marble images, with configurable lighting and appearance; used to generate the image observations fed to the neural network agent.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>visual rendering for robotics / perception</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>high-fidelity visual rendering (configurable appearance), but rendering fidelity was not the limiting factor for transfer in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Provides configurable lighting, colors and textures; observations were corrupted with additive Gaussian white noise and appearance randomization was experimented with (colors/intensities). The authors found appearance randomization had little effect on fine-tuning time for their setup.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>A3C agent (image-based observations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Image-input convolutional + LSTM actor-critic network receives Ogre3D-rendered frames (84x84) as observations.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Provide realistic image observations for image-based policy learning to control marbles in the maze.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Real-world camera observations</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Not quantified separately; the paper reports that appearance randomization provided limited additional benefit for reducing fine-tuning time in this setup (i.e., visual realism/appearance variation was less critical than dynamics randomization).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Appearance randomization (lighting/color/intensity changes via Ogre3D) was tested but had little effect on improving fine-tuning time; dynamics randomization mattered more.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Authors note that appearance changes had little effect for this task, implying that extremely high visual fidelity or detailed appearance randomization was unnecessary for successful sim-to-real transfer in their setup; accurate dynamics modeling was more important.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Lack of additional appearance realism did not prevent successful transfer if dynamics were properly randomized/tuned, i.e., appearance-only adjustments were insufficient to fix transfer issues caused by dynamics mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim-to-Real Transfer Learning using Robustified Controllers in Robotic Tasks involving Complex Dynamics', 'publication_date_yy_mm': '2018-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1493.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1493.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenAI Gym (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI Gym</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely used benchmark and toolkit for developing reinforcement learning algorithms; cited in related work as inspiration for the authors' simulator design.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Openai gym</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>OpenAI Gym</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Mentioned as a game-like environment for generating synthetic data and benchmark RL problems; not used directly in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>reinforcement learning benchmarks / simulated tasks (general)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim-to-Real Transfer Learning using Robustified Controllers in Robotic Tasks involving Complex Dynamics', 'publication_date_yy_mm': '2018-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sim-to-real transfer of robotic control with dynamics randomization <em>(Rating: 2)</em></li>
                <li>EPOpt: Learning robust neural network policies using model ensembles <em>(Rating: 2)</em></li>
                <li>Transfer from simulation to real world through learning deep inverse dynamics model <em>(Rating: 2)</em></li>
                <li>Sim-to-real robot learning from pixels with progressive nets <em>(Rating: 2)</em></li>
                <li>Reinforcement learning for non-prehensile manipulation: Transfer from simulation to physical system <em>(Rating: 2)</em></li>
                <li>Convex and analytically-invertible dynamics with contacts and constraints: Theory and implementation in mujoco <em>(Rating: 2)</em></li>
                <li>Openai gym <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1493",
    "paper_id": "paper-2cf42464beec8bf79f75d01540c2e886a95abd3a",
    "extraction_schema_id": "extraction-schema-29",
    "extracted_data": [
        {
            "name_short": "MuJoCo+Ogre3D simulator",
            "name_full": "MuJoCo physics engine with Ogre3D renderer (custom integrated simulator)",
            "brief_description": "A custom simulator implemented by the authors that combines MuJoCo for physical dynamics and Ogre3D for visual rendering; used to train A3C agents on a marble-maze control task with parameter randomization (robustification).",
            "citation_title": "Sim-to-Real Transfer Learning using Robustified Controllers in Robotic Tasks involving Complex Dynamics",
            "mention_or_use": "use",
            "simulator_name": "MuJoCo + Ogre3D (custom integration)",
            "simulator_description": "MuJoCo is used to simulate the marble dynamics (contacts, collisions, static and dynamic friction, damping, mass, constraints); Ogre3D is used to render images (lighting, appearance). The simulator runs as a separate process and communicates via sockets, returning rendered images, rewards and terminal flags for each action.",
            "scientific_domain": "mechanics / robotic manipulation (contact dynamics)",
            "fidelity_level": "medium-to-high fidelity for mechanical/contact dynamics (approximate but detailed), high-fidelity visual rendering (configurable); overall an approximate real-world replica tuned to match dynamics but not exact.",
            "fidelity_characteristics": "Includes static friction, dynamic (rolling) friction, damping, collisions between marbles and maze geometry, marble mass, full 3D maze geometry reconstructed from measurements, emulated camera delay, additive Gaussian white noise on observations, configurable lighting; physics parameters were manually tuned to match observed marble oscillations and are sampled from ranges during training for randomization. Does not claim exact, fully identified real-world physics parameters; some parameters are visually / empirically chosen rather than learned.",
            "model_or_agent_name": "A3C agent (asynchronous advantage actor-critic) — robustified and non-robustified variants",
            "model_description": "Deep RL agent: convolutional layers -&gt; fully-connected -&gt; LSTM; actor-critic heads; auxiliary tasks (pixel change, reward prediction) and generalized advantage estimation; policies trained with dynamics/appearance randomization to produce 'robustified' controllers.",
            "reasoning_task": "Control/plan continuous sequences of incremental rotations of a maze to maneuver one or two marbles from outer ring to center (sparse-reward maze-solving control task under complex contact dynamics).",
            "training_performance": "Single-marble: robustified policy converged to 100% success after ~4.0M simulation steps; non-robustified policy converged to 100% after ~4.5M steps. Two-marble: offline robustified policy achieved 100% after ~3.0M steps.",
            "transfer_target": "Real-world robot setup (Mitsubishi Electric robot arm + Intel RealSense camera) — sim-to-real transfer",
            "transfer_performance": "Single-marble: robustified policy required ~55K fine-tuning steps on the real robot to converge to 100% success; non-robustified policy required ~220K fine-tuning steps and reached ~≈90% success and occasional failures. Two-marble: transferred robustified policy fine-tuned ≈225K steps to reach ~75% success. Transferring a single-marble policy to the two-marble real task required much more fine-tuning (~400K steps to ≈50% success).",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "The paper compares training with fixed (non-robustified) simulator parameters versus randomized (robustified) simulator parameters. Dynamics/appearance randomization (robustified) produced policies that transferred much more efficiently: e.g., single-marble robustified TL required ~55K fine-tuning steps vs ~220K for non-robustified, and achieved higher final success and more consistent episode lengths.",
            "minimal_fidelity_discussion": "The authors state precise determination of physics parameters is non-trivial and that randomizing physics/appearance/system parameters during simulation training reduces the need for exact parameter identification; they emphasize that modeling static/dynamic friction, damping, collisions and camera/system delays are important, while additional appearance changes (lighting color/intensity) had little effect for their setup.",
            "failure_cases": "Non-robustified (fixed-parameter) simulation pre-training transferred poorly relative to robustified training: required ~4x more fine-tuning and produced occasional failures (did not reliably reach 100% success). Learning entirely online (real robot) for the two-marble task failed even after 1M steps (0% success with initial reward scheme), highlighting that absence of prior simulated training hindered success for more complex dynamics and multi-object interactions. Transferring a single-marble policy to two marbles required substantially more fine-tuning and achieved lower success than a policy trained in-simulation for two marbles.",
            "uuid": "e1493.0",
            "source_info": {
                "paper_title": "Sim-to-Real Transfer Learning using Robustified Controllers in Robotic Tasks involving Complex Dynamics",
                "publication_date_yy_mm": "2018-09"
            }
        },
        {
            "name_short": "MuJoCo",
            "name_full": "MuJoCo (Multi-Joint dynamics with Contact)",
            "brief_description": "A physics engine used by the authors to simulate contact-rich marble dynamics (contacts, collisions, friction, damping) of the marble-maze task.",
            "citation_title": "Sim-to-Real Transfer Learning using Robustified Controllers in Robotic Tasks involving Complex Dynamics",
            "mention_or_use": "use",
            "simulator_name": "MuJoCo",
            "simulator_description": "A commercial/academic rigid-body physics engine that models contact dynamics and constraints; used to simulate marbles interacting with maze geometry including contacts and collisions.",
            "scientific_domain": "mechanics / contact dynamics",
            "fidelity_level": "medium-to-high fidelity for rigid-body contact dynamics (capable of modeling contacts, collisions, friction and constraints); used with manually tuned parameters to approximate the real maze.",
            "fidelity_characteristics": "Models contacts/collisions, static and dynamic friction, damping, masses and constraints; authors tuned static friction, dynamic friction and damping to match marble oscillations; physics parameters were sampled (randomized) from ranges per episode for robustified training. Does not inherently guarantee exact reproduction of real-world delays or unmodeled effects.",
            "model_or_agent_name": "A3C agent (as above)",
            "model_description": "See combined entry; A3C neural policy trained within MuJoCo-simulated environment using image observations produced by Ogre3D.",
            "reasoning_task": "Learn control policy to manipulate maze orientation to direct marbles through gates into center (planning under contact-rich dynamics).",
            "training_performance": "As part of the combined simulator: robustified single-marble policy ~4.0M sim steps to 100% success; non-robustified ~4.5M steps to 100% success; two-marble robustified ~3.0M to 100%.",
            "transfer_target": "Real robot (sim-to-real transfer)",
            "transfer_performance": "See combined entry: robustified policies transferred requiring substantially fewer fine-tuning steps (~55K for single marble) and achieving higher success than non-robustified.",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Randomizing MuJoCo physics parameters per episode (robustification) outperformed fixed-parameter MuJoCo training for transfer: far fewer fine-tuning steps and more reliable transfer.",
            "minimal_fidelity_discussion": "Authors tuned MuJoCo parameters to visually/empirically match real dynamics; they note that accurate identification is hard and that randomization can compensate for imperfect parameter identification. They emphasize including friction, damping and collisions in the physics model.",
            "failure_cases": "Fixed-parameter MuJoCo training (non-robustified) led to worse transfer results (more fine-tuning and incomplete success).",
            "uuid": "e1493.1",
            "source_info": {
                "paper_title": "Sim-to-Real Transfer Learning using Robustified Controllers in Robotic Tasks involving Complex Dynamics",
                "publication_date_yy_mm": "2018-09"
            }
        },
        {
            "name_short": "Ogre3D",
            "name_full": "Ogre (Object-Oriented Graphics Rendering Engine) 3D",
            "brief_description": "A 3D graphics rendering engine used to produce visual observations for the simulated marble-maze environment, enabling image-based policy learning.",
            "citation_title": "Sim-to-Real Transfer Learning using Robustified Controllers in Robotic Tasks involving Complex Dynamics",
            "mention_or_use": "use",
            "simulator_name": "Ogre3D",
            "simulator_description": "A real-time 3D graphics engine used to render the simulated maze and marble images, with configurable lighting and appearance; used to generate the image observations fed to the neural network agent.",
            "scientific_domain": "visual rendering for robotics / perception",
            "fidelity_level": "high-fidelity visual rendering (configurable appearance), but rendering fidelity was not the limiting factor for transfer in this work.",
            "fidelity_characteristics": "Provides configurable lighting, colors and textures; observations were corrupted with additive Gaussian white noise and appearance randomization was experimented with (colors/intensities). The authors found appearance randomization had little effect on fine-tuning time for their setup.",
            "model_or_agent_name": "A3C agent (image-based observations)",
            "model_description": "Image-input convolutional + LSTM actor-critic network receives Ogre3D-rendered frames (84x84) as observations.",
            "reasoning_task": "Provide realistic image observations for image-based policy learning to control marbles in the maze.",
            "training_performance": null,
            "transfer_target": "Real-world camera observations",
            "transfer_performance": "Not quantified separately; the paper reports that appearance randomization provided limited additional benefit for reducing fine-tuning time in this setup (i.e., visual realism/appearance variation was less critical than dynamics randomization).",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Appearance randomization (lighting/color/intensity changes via Ogre3D) was tested but had little effect on improving fine-tuning time; dynamics randomization mattered more.",
            "minimal_fidelity_discussion": "Authors note that appearance changes had little effect for this task, implying that extremely high visual fidelity or detailed appearance randomization was unnecessary for successful sim-to-real transfer in their setup; accurate dynamics modeling was more important.",
            "failure_cases": "Lack of additional appearance realism did not prevent successful transfer if dynamics were properly randomized/tuned, i.e., appearance-only adjustments were insufficient to fix transfer issues caused by dynamics mismatch.",
            "uuid": "e1493.2",
            "source_info": {
                "paper_title": "Sim-to-Real Transfer Learning using Robustified Controllers in Robotic Tasks involving Complex Dynamics",
                "publication_date_yy_mm": "2018-09"
            }
        },
        {
            "name_short": "OpenAI Gym (mentioned)",
            "name_full": "OpenAI Gym",
            "brief_description": "A widely used benchmark and toolkit for developing reinforcement learning algorithms; cited in related work as inspiration for the authors' simulator design.",
            "citation_title": "Openai gym",
            "mention_or_use": "mention",
            "simulator_name": "OpenAI Gym",
            "simulator_description": "Mentioned as a game-like environment for generating synthetic data and benchmark RL problems; not used directly in experiments.",
            "scientific_domain": "reinforcement learning benchmarks / simulated tasks (general)",
            "fidelity_level": null,
            "fidelity_characteristics": null,
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": null,
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "minimal_fidelity_discussion": null,
            "failure_cases": null,
            "uuid": "e1493.3",
            "source_info": {
                "paper_title": "Sim-to-Real Transfer Learning using Robustified Controllers in Robotic Tasks involving Complex Dynamics",
                "publication_date_yy_mm": "2018-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sim-to-real transfer of robotic control with dynamics randomization",
            "rating": 2
        },
        {
            "paper_title": "EPOpt: Learning robust neural network policies using model ensembles",
            "rating": 2
        },
        {
            "paper_title": "Transfer from simulation to real world through learning deep inverse dynamics model",
            "rating": 2
        },
        {
            "paper_title": "Sim-to-real robot learning from pixels with progressive nets",
            "rating": 2
        },
        {
            "paper_title": "Reinforcement learning for non-prehensile manipulation: Transfer from simulation to physical system",
            "rating": 2
        },
        {
            "paper_title": "Convex and analytically-invertible dynamics with contacts and constraints: Theory and implementation in mujoco",
            "rating": 2
        },
        {
            "paper_title": "Openai gym",
            "rating": 1
        }
    ],
    "cost": 0.013327499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Sim-to-Real Transfer Learning using Robustified Controllers in Robotic Tasks involving Complex Dynamics</h1>
<p>Jeroen van Baar, Alan Sullivan, Radu Cordorel, Devesh Jha, Diego Romeres and Daniel Nikovski</p>
<h4>Abstract</h4>
<p>Learning robot tasks or controllers using deep reinforcement learning has been proven effective in simulations. Learning in simulation has several advantages. For example, one can fully control the simulated environment, including halting motions while performing computations. Another advantage when robots are involved, is that the amount of time a robot is occupied learning a task-rather than being productive-can be reduced by transferring the learned task to the real robot. Transfer learning requires some amount of fine-tuning on the real robot. For tasks which involve complex (non-linear) dynamics, the fine-tuning itself may take a substantial amount of time. In order to reduce the amount of fine-tuning we propose to learn robustified controllers in simulation. Robustified controllers are learned by exploiting the ability to change simulation parameters (both appearance and dynamics) for successive training episodes. An additional benefit for this approach is that it alleviates the precise determination of physics parameters for the simulator, which is a non-trivial task. We demonstrate our proposed approach on a real setup in which a robot aims to solve a maze game, which involves complex dynamics due to static friction and potentially large accelerations. We show that the amount of fine-tuning in transfer learning for a robustified controller is substantially reduced compared to a non-robustified controller.</p>
<h2>I. INTRODUCTION</h2>
<p>Teaching robots to perform challenging tasks has been an active topic of research. In particular, it has recently been demonstrated that reinforcement learning (RL) coupled with deep neural networks is able to learn policies (controllers) which can successfully perform tasks such as pick and fetch.</p>
<p>Robots may be slow, dangerous, can damage themselves and they are expensive. When a robot is learning a task, it needs to be taken out of production. Learning policies using model-free deep RL typically requires many samples to explore the sequential decision making space. Model-free RL applied to tasks that involve complex dynamics, require even more samples to learn adequate policies compared to tasks involving (largely) linear dynamics. Directly learning on robots may thus be very costly.</p>
<p>In order to reduce the time required for learning on a real robot, training can be performed in simulation environments. The learned policy is then transferred to the real world domain. Modern graphics cards and sophisticated physics engines enable the simulation of complex tasks. Learning with simulators has several advantages. The rendering and physics engines are capable of computing simulations faster than realtime. This helps to reduce overall training times. Recent deep reinforcement learning algorithms allow agents to learn in</p>
<p>Mitsubishi Electric Research Laboratories (MERL), Cambridge, MA 02139, USA jeroen@merl.com
parallel [1], which reduces training times. Furthermore, both appearance and physics can be controlled in simulation. For example the lighting condition, or the friction of an object can be changed, or the entire simulation can be halted to allow for computation of updates.</p>
<p>Appearance, complex dynamics, and robot motor movements in the real world can only be simulated up to some approximation. Simulation to real world transfer thus requires fine-tuning on real data. Furthermore, real setups involving various components, experience delays which are hard to determine exactly. For example, the delay introduced by the acquisition system, where some time has passed before the acquired image is available for processing by the algorithm.</p>
<p>By randomization of the appearance, physics and system parameters during reinforcement learning on simulation data, robustified policies can be learned. This is analogous to training a deep convolutional neural network to classify objects regardless of the background in the input images. We found that robustified policies can greatly reduce the amount of time for fine-tuning in transfer learning. Reducing the fine-tuning time in transfer learning becomes especially important for tasks involving complex dynamics.</p>
<p>We demonstrate our proposed approach on a challenging task of a robot learning to solve a marble maze game. The maze game is shown in Figure 1. The marbles are subject to static and rolling friction, acceleration, and collisions (with other marbles and with the maze geometry). A simulator simulates the physics of the marbles in the maze game, and renders the results to images. We learn to solve the game from scratch using deep reinforcement learning. A modified version of the deep reinforcement learning is used to learn directly on real robot hardware. We learn both a robustified and non-robustified policy in simulation and compare the times required for fine-tuning after transferring the policy to the real world.</p>
<p>In the remainder of this paper we will refer to learning on simulated data / environments as offline learning, and learning on real data / environments will be referred to as online learning. Transfer learning (TL) with fine-tuning on real data therefore constitutes both offline as well as online learning.</p>
<h2>II. Related Work</h2>
<p>Our work is inspired by the recent advances in deep reinforcement learning, learning complicated tasks and achieving (beyond) human level performance on a variety of tasks [1][4].</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. Marble maze game. (<strong>Left</strong>) Top view of the marble maze after a plexiglass top has been removed (leaving holes in the outermost edge). A paper rim is used to cover the holes. The black dots in each gate between rings are used for alignment. The view also shows the world aligned <em>x</em> and <em>y</em> axes. (<strong>Middle</strong>) The marble maze mounted on the robot arm. (<strong>Right</strong>) A rendering of the simulated marble maze under some chosen lighting conditions (without added noise).</p>
<p>TL has been an active area of research in the context of deep learning. For example, tasks such as object detection and classification can avoid costly training time by using pre-trained networks and fine-tuning [5], [6], where typically only the weights in the last couple of layers are updated. TL from simulated to real has also been applied to learn robot tasks [7]–[10]. To reduce the time required for fine-tuning in TL, the authors in [11] propose to make simulated data look more like the real world.</p>
<p>In [12] the authors acknowledge that training robot tasks on simulated data alone does not readily transfer to the real world. They propose a form of fine-tuning where the inverse dynamics for the real robot are recovered. It requires a simulator and training which produces reasonable estimates of the real world situation. The drawback of this method is that it requires long online training times, whereas our goal is to minimize the duration of the online training time.</p>
<p>By randomization of the appearance, the learning can become robust against appearance changes and readily transfer to the real world domain [13], [14]. The method proposed in [15] exploits an ensemble of simulated source domains and adversarial training to obtain robust policies. This policy search approach relies on trajectories and roll-outs which solve the task. The approach proposed in [16] uses model-based RL to learn a controller entirely in simulation, allowing for zero-shot TL. Since we are considering tasks involving (much) more complex dynamics, we instead follow a similar approach as [17], and perform randomization of appearance, physics and system parameters with model-free RL.</p>
<p>Model-agnostic meta-learning (MAML) [18], aims to learn a meta-policy that can be quickly adapted to new (but similar) tasks. In the case of complex dynamics it is not clear how easily MAML could be applied. Appearance and dynamics randomization can be considered as forms of meta-learning. Other approaches aim to learn new tasks, or refine previously learned tasks, without "forgetting", e.g., [19]. Our emphasis instead is on reducing the amount of time required for fine-tuning in TL.</p>
<p>Our simulator provides observations of the state in simulation, similar to the real world. In [20] the critic receives full states, whereas the actor receives observations of states. Coupled with appearance randomization, zero-shot transfer can be achieved. The full state requires that the physics parameters to produce complex dynamics match those of the real world. However, precisely determining the physics parameters is non-trivial.</p>
<p>Formulating reward functions is not straightforward. The authors in [21] propose to discover robust rewards to enable the learning of complicated tasks. Adding additional goals (sub-goals), basically a form of curriculum learning [22], can improve the learning as well [23]. The latter approach may be applied to break up the goal of a marble maze into stages. However, in this paper we show that a simple reward function which governs the overall goal of the game is sufficient.</p>
<p>The authors in [24] propose a game-like environment for generating synthetic data for benchmark problems related to reinforcement learning. We developed our simulator along the same lines as [24].</p>
<p>In [25] the authors propose to model both the dynamics and control in order to solve the marble maze game. This is a complementary approach to the TL approach proposed in this paper, and we believe that each approach has its own strengths and weaknesses.</p>
<h2>III. PRELIMINARIES</h2>
<p>We briefly review some concepts from (deep) reinforcement learning (RL) using model-free asynchronous actor-critic, and define some terminology that we will use in the remainder of this paper. In the next section we will discuss our approach.</p>
<h3>A. Reinforcement Learning</h3>
<p>In RL an agent interacts with an environment, represented by a set of states <em>S</em>, taking actions from an action set <em>A</em>,</p>
<p>and receiving rewards $r: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$. The environment is governed by (unknown) state transition probabilities $p\left(s^{\prime} \mid s, a\right)$. The agent aims to learn a (stochastic) policy $\pi(a \mid s)$, which predicts (a distribution over) actions $a$ based on state $s$. The goal for the agent is to learn a policy which maximizes the expected return $\mathbb{E}\left[R_{t}\right]$, where the return $R_{t}=\sum_{k=0}^{\infty} \gamma^{k} r_{t+k}$ denotes the discounted sum of future rewards, with discount factor $\gamma$.</p>
<p>To determine for a given policy $\pi$ how good it is to be in a certain state, or how good it is to take a certain action in a certain state, RL depends on two value functions: a statevalue function $V^{\pi}(s)=\mathbb{E}<em k="0">{\pi}\left[\sum</em>}^{\infty} \gamma^{k} R_{t+k+1} \mid S_{t}=s\right]$ and an action-value function $Q^{\pi}(s, a)=\mathbb{E<em k="0">{\pi}\left[\sum</em>$. For details we refer the reader to [26]}^{\infty} \gamma^{k} R_{t+k+1} \mid S_{t}=\right.$ $\left.s, A_{t}=a\right]$. For Markov decision processes, the value functions can be written as a recursion of expected rewards, e.g., $V^{\pi}(s)=R(s, \pi(s))+\gamma \sum_{s^{\prime}} p\left(s^{\prime} \mid s, \pi(s)\right) V^{\pi}\left(s^{\prime}\right)$, where $s$ denotes the current state, and $s^{\prime}$ denotes the next state. The recursive formulations are Bellman equations. Solving the Bellman optimality equations would give rise to the optimal policy $\pi^{*</p>
<p>We consider the case where agents interact with the environment in episodes of finite length. The end of an episode is reached if the agent arrives at the timestep of maximum episode length, or the goal (terminal state) is achieved. In either case, the agent restarts from a new initial state.</p>
<h2>B. Deep RL using Advantage Actor-Critic</h2>
<p>In [1] the authors propose the asynchronous advantage actor-critic algorithm. The algorithm defines two networks: a policy network $\pi\left(a \mid s, \theta_{p}\right)$ with network parameters $\theta_{p}$, and a value network $V\left(s \mid \theta_{v}\right)$ with network parameters $\theta_{v}$. This policy-based model-free method determines a reduced variance estimate of $\nabla_{\theta_{p}} \mathbb{E}\left[R_{t}\right]$ as $\nabla_{\theta_{p}} \log \pi\left(a_{t} \mid s_{t}, \theta_{p}\right)\left(R_{t}-\right.$ $\left.b_{t}\left(s_{t}\right)\right)$ [27]. The return $R_{t}$ is an estimate of $Q^{\pi}$ and the baseline $b_{t}$ is a learned estimate of the value function $V^{\pi}$. The policy $\pi$ is referred to as the actor, and value function estimate $V^{\pi}$ as the critic.</p>
<p>The authors in [1] describe an algorithm where multiple agents learn in parallel, and each agent maintains local copies of the policy and value networks. Agents are trained on episodes of maximum length $L_{e}$. Within each episode, trajectories are acquired as sequences $\tau=\left(s_{1}, a_{1}, r_{1}, s_{2}, a_{2}, r_{2}, \ldots, s_{L_{s e}}, a_{L_{s e}}, r_{L_{s e}}\right)$, of maximum length $L_{s e}$. Rather than the actual state, the inputs are observations (images) of the state, and a forward pass of each image through the agent's local policy network results in a distribution over the actions. Every $L_{s e}$ steps, the parameters of the global policy and value networks are updated and the agent synchronizes its local copy with the parameters of the global networks. The current episode ends after $L_{e}$ steps, or when the terminal state is reached, and then a new episode starts. This episodal learning is repeated until the task is solved consistently. See [1] for further details.</p>
<h2>IV. DEEP REINFORCEMENT LEARNING FOR A TASK WITH COMPLEX DYNAMICS</h2>
<h2>A. Setting up the Task</h2>
<p>The task we aim to learn is to solve a marble maze game, see Figure 1. Solving the game means that the marble(s) are maneuvered from the outermost ring, through a sequence of gates, into the center. Due to static and dynamic friction, acceleration, damping, and the discontinuous geometry of the maze, the dynamics are (highly) complex and difficult to model. To solve the marble maze game using model-free RL we can define a reward function as:</p>
<p>$$
r= \begin{cases}-1, &amp; \text { if through gate away from the goal } \ +1, &amp; \text { if through gate towards the goal } \ 0, &amp; \text { otherwise }\end{cases}
$$</p>
<p>This sparse reward function is general and does not encode any information about the actual geometry of the game. The action space is discretized into five actions. The first four actions constitute $1^{\circ}$ rotation increments, clockwise and counterclockwise around the $x$, and $y$ axes up to a fixed maximum angle. Figure 1-Left shows the orientation of the $x$, and $y$ axes with respect to the maze. The $1^{\circ}$ increment is sufficient to overcome the static friction, while simultaneously avoiding accelerations that are too large. We define a fifth action as no-op, i.e., maintain the current orientation of the maze. We empirically determined the fixed maximum angle to be $5^{\circ}$ in either direction.</p>
<h2>B. Deep Reinforcement Learning on Simulated Robot Environments</h2>
<p>In order to learn a robustified policy in simulation, we adopt the idea of randomization from [13], [14], [17]. We implemented two learning schemes. In the first scheme, each agent was assigned different parameters which were kept fixed for the duration of learning. In the second scheme, the physics and appearance parameters are randomly sampled from a pre-determined range, according to a uniform distribution, for each episode and each agent. We found that the second scheme produced robustified policies which adapted more quickly during fine-tuning on the real robot after transfer.</p>
<p>We use the asynchronous advantage actor-critic (A3C) algorithm to learn a policy for the marble maze game. To successfully apply reinforcement learning with sparse rewards, a framework of auxiliary tasks may be incorporated [28]. One could consider path following as an auxiliary (dense reward) task. However, we aim to keep our approach as general as possible, and not rely on the geometry of the maze. We instead incorporate pixel change and reward prediction, as proposed by [28]. Pixel change promotes taking actions which result in maximal change between images of consecutive states. In the context of the maze game, we aim to avoid selecting consecutive actions that would result in little to no marble motions. In addition, reward prediction aims to over-represent rewarding events to offset the sparse reward signal provided by the reward function. To stabilize learning</p>
<p>and avoid settling into sub-optimal policies we employ the generalized advantage estimation as proposed by [29] together with entropy regularization with respect to the policy parameters [1].</p>
<p>1) Robustified Policies: At the start of each episode, for each agent, the parameter values for static friction, dynamic friction, damping and marble(s) mass are uniformly sampled from a range of values. We emulated a camera delay by rendering frames into a buffer. The camera delay was varied per episode and agent. During each episode the parameters are held constant. Each observation received from the simulator is corrupted by AGWN. We experimented with additional appearance changes, such as different light colors and intensities. We found that those changes had little effect on improving the time required for fine-tuning for our current setup.</p>
<h2>C. Deep Reinforcement Learning on Real Robot Environments</h2>
<p>A3C is an on-policy method, since the current policy $\pi(s ; \theta)$ is used in roll-outs (using an $\epsilon$-greedy exploration strategy) to obtain the current trajectory of length $L_{s e}$. For each update, A3C accumulates the losses for the policy and value networks over the trajectory and performs backpropagation of the losses to update the policy and value network parameters. The simulation is halted until the network parameters have been updated, and then roll-outs for the next trajectory continue using the updated policy $\pi\left(s ; \theta^{\prime}\right)$.</p>
<p>For a real robot setup we need to be able to compute an update, while simultaneously collecting the next trajectory, since we cannot halt the motion of the marble(s) during an update. We therefore adopt an off-policy approach for the real robot setups (see Algorithm 1).</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 Algorithm for off-policy A3C
    \(\pi(s \mid \theta)\)-initialized or robustly learned in simulation
    \(t \leftarrow 0\)
    obtain \(\tau_{t}\) using \(\pi(s \mid \theta)\)
    repeat
        while \(\left|\tau_{t+1}\right|&lt;L_{s e}\) do \(\{\) concurrently\}
            compute update from \(\tau_{t} \rightarrow \pi\left(s \mid \theta^{\prime}\right)\)
            obtain \(\tau_{t+1}\) using \(\pi(s \mid \theta)\)
        end while
        \(\pi(s \mid \theta) \leftarrow \pi\left(s \mid \theta^{\prime}\right)\)
        \(t \leftarrow t+1\)
    until done
</code></pre></div>

<p>We acquire the next trajectory $\tau_{t+1}$ while concurrently computing the updates for the policy and value networks based on the previously acquired trajectory $\tau_{t}$. We first verified in simulation that our off-policy adaptation of A3C would indeed be able to successfully learn a policy to solve the marble maze. If one had access to multiple robots, the robots could act as parallel agents similar to the case of simulation. However, due to practical limitations, we only have access to a single robot and are thus limited to training with a single agent in the real world case.</p>
<h2>V. IMPLEMENTATION</h2>
<p>We have implemented a simulation of the marble maze using MuJoCo [30] to simulate the dynamics, and Ogre 3D [31] for the appearance. We carefully measured the maze and marble dimensions to accurately reconstruct its 3D geometry. In order to match the simulated dynamics to the real world dynamics, we have tuned the MuJoCO parameters, with static friction, dynamic friction, and damping parameters in particular. For tuning, the maze was inclined to a known orientation, and the marble was released from various pre-determined locations within the maze. Using the markers (see Figure 1) we aligned the images of the simulated maze to the real maze by computing a homography warp. We then empirically tuned the parameters to match the marble oscillations between the simulated and real maze. Learning the parameters instead would be preferable, but this is left as future work. The simulator is executed as a separate process, and communication between controller and simulator is performed via sockets. The simulator receives an action to perform, and returns an image of the updated marble positions and maze orientation, along with a reward (according to Eq. 1) and terminal flag.</p>
<p>The policy network consists of two convolutional layers, followed by a fully-connected layer. The input to the network is an $84 \times 84$ image. A one-hot action vector and the reward are appended to the 256-dim. output of the fully-connected layer and serves as input to an LSTM layer. This part of the network is shared between the policy (actor) and value (critic) network. For the policy network a fully-connected layer with softmax activation computes a distribution over the actions. For the value network, a fully connected layer outputs a single value. We empirically chose $L_{e}=3000$ and $L_{s e}=200$.</p>
<p>The $\left(s_{t}, a_{t}, r_{t}\right)$-tuples are stored in a FIFO experience buffer (of length 3000). We keep track of which tuples have zero and non-zero rewards for importance sampling. For reward prediction we (importance) sample three consecutive frames from the experience buffer. The two convolutional layers and fully connected layer are shared from the policy and value networks. Two more fully connected layers determine a distribution over negative, zero or positive rewards.</p>
<p>For pixel change, we compute the average pixel-change for a $20 \times 20$ grid, for the central $80 \times 80$ portion of consecutive images. The pixel-change network re-uses the layers up to and including the LSTM layer for the policy and value network. A fully connected layer together with a deconvolution layers predict $20 \times 20$ pixel change images. At most $L_{e}+1$ frames are sampled from the experience buffer, and we compute the L2 loss between the pixel change predicted by the network, and the recorded pixel change over the sampled sequence. Both losses are added to the A3C loss.</p>
<p>The physics parameters are uniformly sampled from a range around the empirically estimated parameter values. Due to the lack of intuitive interpretation of some of the physics parameters, the range was determined by visually inspecting the resulting dynamics to ensure that the dynamics</p>
<p>had sufficient variety, but did not lead to instability in the simulation.</p>
<p>For the real setup, the ROS framework is used to integrate the learning with camera acquisition and robot control. The camera is an Intel RealSense R200 and the robot arm is a Mitsubishi Electric Melfa RV-6SL (see Figure 1-Middle). The execution time of a $1^{\circ}$ rotation command for the robot arm is about 190ms. Forward passes through the networks and additional computation time add up to about 20 or 30ms. Although we can overlap computation and robot command execution to some degree, observations are acquired at a framerate of 4.3Hz, i.e. 233ms intervals, to ensure robot commands are completed entirely before the new state is obtained. We observed that during concurrent network parameter updates the computation time for a forward pass through the policy network increases drastically. If we expect that the robot action cannot be completed before the new state is observed by the camera, we set the action to no-op (Sec. IV-A). We implemented a simple marble detector to determine when a marble has passed through a gate, in order to provide a reward signal. For learning in simulation we use the same 4.3Hz framerate. Each incremental rotation action is performed over the course of the allotted time interval of 233ms, such that the next state provided by the simulator reflects the situation after a complete incremental rotation.</p>
<h2>VI. RESULTS</h2>
<p>Table I compares the number of steps for training a policy to successfully play a one marble maze game. Training directly on the real robot takes about 3.5M steps. For TL, we compare the number of fine-tuning steps necessary for a robustified policy versus a non-robustified policy (fixed parameters). Training a robustified policy in simulation takes about 4.0M steps, whereas a non-robustified policy takes approximately 4.5M to achieve 100% success rate. TL of a robustified policy requires about 55K steps to "converge". This is a reduction of nearly $60 \times$ compared to online training. A non-robustified policy requires at least $3 \times$ the number of fine-tuning steps in order to achieve the same level of success in solving the maze game.</p>
<p>Figure 2 further shows the benefit for TL of a robustified policy. The left side of Figure 2 shows results for the robustified policy, with results for the non-robustified policy on the right. The bottom row shows the accumulated rewards for an episode. An accumulated reward of 4.0 means that the marble has been maneuvered from the outside ring into the center, since there are four gates to pass through. The graph for the robustified policy shows that the learning essentially converges, i.e., achieve $100 \%$ success, whereas for the nonrobustified policy transfer, the success rate is around $90 \%$. The top row of Figure 2 shows the length of each episode. It is evident that the robustified policy has successfully learned how to handle the complex dynamics to solve the maze game.</p>
<p>We repeated the same experiment for a two marble maze game, with the goal to get both marbles into the center of the maze. We only compared TL with the robustified policy. The results are shown in Table II. Learning a two marble game in simulation with $\pm 1$ rewards achieved $100 \%$ success. However, training on the real setup with these rewards proved very challenging. We believe this is due to the geometry of the maze-the center has only one gate, surrounded by four gates in the adjacent ring-coupled with the static friction. We designed a reward function which gives more importance for passing through gates into rings closer to the goal. This promotes a marble to stay in the center area, while the controller maneuvers the remaining marble. The rewards were modified to $\pm{1,2,4,8}$ instead (which was also used for training the two marble game offline). When learning online, even after 1M steps, the success rate is still at $0 \%$ (a single marble reached the center about a dozen of times). With fine-tuning a transferred robustified policy, after $\sim 225 \mathrm{~K}$ steps around a $75 \%$ success rate is achieved.</p>
<p>We investigate if the transfer of a single marble policy learned offline, would require longer fine-tuning for a two marble game online. After $\sim 100 \mathrm{~K}$ steps of fine-tuning, the policy was able to start solving the game. A success rate of about $50 \%$ was achieved after $\sim 400 \mathrm{~K}$ steps. Thus, fine-tuning a robustified policy trained on a two marble maze game in simulation achieves a higher success rate compared to the fine-tuning of a single marble robustified policy.</p>
<p>We refer the reader to the supplemental material for videos of example roll-outs for single and two marble maze games.</p>
<h2>VII. Discussion and Future Work</h2>
<p>Deep reinforcement learning is capable of learning complicated robot tasks, and in some cases achieving (beyond) human-level performance. Deep RL requires many training samples, especially in the case of model-free approaches. For learning robot tasks, learning in simulation is desirable since robots are slow, can be dangerous and are expensive. Powerful GPUs and CPUs have enabled simulation of complex dynamics coupled with high quality rendering at high speeds. Transfer learning, i.e., the training in simulation and subsequent transfer to the real world, is typically followed by fine-tuning. Fine-tuning is necessary to adapt to any differences between the simulated and the real world. Previous work has focused on transfer learning tasks involving linear dynamics, such as controlling a robot to pick an object and place it at some desired location. However, we explore the case when the dynamics are complex. Non-linearities arise due to static and dynamic friction, acceleration and collisions of objects interacting with each other and the environment. We compare learning online, i.e., directly in the real world, with learning in simulation where the physics, appearance and system parameters are varied during training. For reinforcement learning we refer to this as learning robustified policies. We show that the time required for finetuning with robustified policies, is greatly reduced.</p>
<p>Although we have shown that model-free deep reinforcement learning can be successfully used to learn tasks involving complex dynamics, there are drawbacks of using a modellfree approach. In the example discussed in our paper, the dynamics are (mostly) captured by the LSTM layer in the network. In the case of more than one marble the amount</p>
<p>TABLE I
COMPARISON OF ONLINE, OFFLINE AND ONLINE FINE-TUNING STEPS FOR TL FOR A SINGLE MARBLE. A ROBUSTIFIED POLICY CAN REDUCE THE TRAINING STEPS BY A FACTOR OF ALMOST $60 \times$ COMPARED TO ONLINE TRAINING, AND A FACTOR OF MORE THAN $3 \times$ COMPARED TO NON-ROBUSTIFIED TL FINE-TUNING.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Online (real)</th>
<th style="text-align: center;">Offline (simulator)</th>
<th style="text-align: center;">TL (online part)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Robust</td>
<td style="text-align: center;">$\sim 3.5 \mathrm{M}$</td>
<td style="text-align: center;">$\sim 4.0 \mathrm{M}$</td>
<td style="text-align: center;">$\sim 55 \mathrm{~K}$</td>
</tr>
<tr>
<td style="text-align: left;">Non-Robust</td>
<td style="text-align: center;">$\sim 3.5 \mathrm{M}$</td>
<td style="text-align: center;">$\sim 4.5 \mathrm{M}$</td>
<td style="text-align: center;">$\sim 220 \mathrm{~K}$</td>
</tr>
</tbody>
</table>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. Results for the fine-tuning of policies solving a maze game with one marble for a simulation pre-trained robustified policy (Left), and for a simulation pre-trained non-robustified policy (Right). Note that the horizontal axis ranges between Right and Left are slightly different. In the Top row we plot the number of steps per episode - with maximum episode length $L_{e}=2500$ - and in the Bottom row we plot the accumulated rewards per episode. The fine-tuning of the robustified policy leads to earlier success of consistently solving the maze game. We consider convergence at 55 K for the robustified policy. Even after more than 220 K fine-tuning episodes, the non-robustified policy occasionally fails to solve the maze game. In addition, the number of steps on average per episode to solve the maze game is significantly less for the case of the robustified policy.</p>
<p>TABLE II
COMPARING TL FOR A TWO MARBLE MAZE GAME. BOTH THE NUMBER OF STEPS AND SUCCESS RATE ARE REPORTED.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Online</th>
<th style="text-align: center;">Offline</th>
<th style="text-align: center;">TL</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Robust</td>
<td style="text-align: center;">$\sim 1 \mathrm{M}(0 \%)$</td>
<td style="text-align: center;">$\sim 3.0 \mathrm{M}(100 \%)$</td>
<td style="text-align: center;">$\sim 225 \mathrm{~K}(75 \%)$</td>
</tr>
</tbody>
</table>
<p>of fine-tuning time significantly increases. In general, as the complexity of the state space increases, the amount of training time increases as well. When people perform tasks such as the maze game, they typically have a decent prediction of where the marble(s) will go given the amount of rotation applied. In [32], [33] the graphics and physics engine are embedded within the learning to recover physics parameters and perform predictions of the dynamics. In [34] the physics and dynamics predictions are modeled with networks. These approaches are interesting research directions for tasks involving complex dynamics.</p>
<p>We currently use high-dimensional images as input to
the learning framework. Low-dimensional input, i.e. marble position and velocity, may be used instead. In addition, rather than producing a distribution over a discrete set of actions, the problem can be formulated as a regression instead and directly produce values for the $x$ and $y$ axes rotations [1], [35].</p>
<p>People quickly figure out that the task can be broken down into moving a single marble at the time into the center, while avoiding marbles already in the center location from spilling back out. Discovering such sub-tasks automatically would be another interesting research direction. Along those lines, teaching a robot to perform tasks by human demonstration, or imitation learning, could teach robots complicated tasks without the need for elaborate reward functions, e.g., [36].</p>
<h2>ACKNOWLEDGEMENTS</h2>
<p>We want to thank Rachana Sreedhar for the implementation of the simulator and Wei-An Lin for the Pytorch implementation of deep reinforcement learning.</p>
<h2>REFERENCES</h2>
<p>[1] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu, "Asynchronous methods for deep reinforcement learning," CoRR, vol. abs/1602.01783, 2016. [Online]. Available: http://arxiv.org/abs/1602.01783
[2] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis, "Human-level control through deep reinforcement learning," Nature, vol. 518, no. 7540, pp. 529-533, Feb. 2015. [Online]. Available: http://dx.doi.org/10.1038/nature14236
[3] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis, "Mastering the game of Go with deep neural networks and tree search," Nature, vol. 529, no. 7587, pp. 484-489, Jan. 2016.
[4] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. R. Baker, M. Lai, A. Bolton, Y. Chen, T. P. Lillicrap, F. X. Hui, L. Sifre, G. van den Driessche, T. Graepel, and D. Hassabis, "Mastering the game of go without human knowledge," Nature, vol. 550, pp. 354-359, 2017.
[5] A. S. Razavian, H. Azizpour, J. Sullivan, and S. Carlsson, "CNN features off-the-shelf: an astounding baseline for recognition," Computer Vision and Pattern Recognition (CVPR), 2014.
[6] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson, "How transferable are features in deep neural networks?" Advances in Neural Information Processing Systems (NIPS), 2014.
[7] A. A. Rusu, M. Vecerik, T. Rothörl, N. Heess, R. Pascanu, and R. Hadsell, "Sim-to-real robot learning from pixels with progressive nets," arXiv preprint, vol. arXiv/1610.04286, 2016.
[8] F. Zhang, J. Leitner, M. Milford, and P. Corke, "Sim-to-real transfer of visuo-motor policies for reaching in clutter: Domain randomization and adaptation with modular networks," CoRR, vol. abs/1709.05746, 2017. [Online]. Available: http://arxiv.org/abs/1709.05746
[9] F. Zhang, J. Leitner, B. Upcroft, and P. I. Corke, "Vision-based reaching using modular deep networks: from simulation to the real world," arXiv preprint, vol. arXiv:1610.06781, 2016. [Online]. Available: http://arxiv.org/abs/1610.06781
[10] K. Bousmalis, A. Irpan, P. Wohlhart, Y. Bai, M. Kelcey, M. Kalakrishnan, L. Downs, J. Ibarz, P. Pastor, K. Konolige, S. Levine, and V. Vanhoucke, "Using simulation and domain adaptation to improve efficiency of deep robotic grasping," arXiv preprint, vol. arXiv/1709.07857, 2017.
[11] A. Shrivastava, T. Pfister, O. Tuzel, J. Susskind, W. Wang, and R. Webb, "Learning from simulated and unsupervised images through adversarial training," Computer Vision and Pattern Recognition (CVPR), 2016.
[12] P. F. Christiano, Z. Shah, I. Mordatch, J. Schneider, T. Blackwell, J. Tobin, P. Abbeel, and W. Zaremba, "Transfer from simulation to real world through learning deep inverse dynamics model," arXiv preprint, vol. arXiv/1610.03518, 2016.
[13] S. James, A. J. Davison, and E. Johns, "Transferring end-to-end visuomotor control from simulation to real world for a multi-stage task," Conference on Robot Learning (CoRL), 2017.
[14] F. Sadeghi and S. Levine, "(cad)\$`2Srl: Real single-image flight without a single real image," Robotics: Science and Systems Conference (RSS), 2016.
[15] A. Rajeswaran, S. Ghotra, S. Levine, and B. Ravindran, "Epopt: Learning robust neural network policies using model ensembles," International Conference on Learning Representations (ICLR), vol. abs/1610.01283, 2016. [Online]. Available: http://arxiv.org/abs/1610. 01283
[16] K. Lowrey, S. Kolev, J. Dao, A. Rajeswaran, and E. Todorov, "Reinforcement learning for non-prehensile manipulation: Transfer from simulation to physical system," IEEE Conf. on Simulation, Modeling, and Programming for Autonomous Robots (SIMPAR), vol. abs/1803.10371, 2018. [Online]. Available: http://arxiv.org/abs/1803. 10371
[17] X. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel, "Sim-to-real transfer of robotic control with dynamics randomization," arXiv preprint, vol. abs/1710.06537, 2018. [Online]. Available: http://arxiv.org/abs/1710.06537
[18] C. Finn, P. Abbeel, and S. Levine, "Model-agnostic meta-learning for fast adaptation of deep networks," ICML 2017, vol. abs/1703.03400, 2017. [Online]. Available: http://arxiv.org/abs/1703.03400
[19] Z. Li and D. Hoiem, "Learning without forgetting," European Conference on Computer Vision (ECCV), 2016.
[20] L. Pinto, M. Andrychowicz, P. Welinder, W. Zaremba, and P. Abbeel, "Asymmetric actor critic for image-based robot learning," CoRR, vol. abs/1710.06542, 2017. [Online]. Available: http://arxiv.org/abs/1710.06542
[21] J. Fu, K. Luo, and S. Levine, "Learning robust rewards with adversarial inverse reinforcement learning," CoRR, vol. abs/1710.11248, 2017. [Online]. Available: http://arxiv.org/abs/1710.11248
[22] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, "Curriculum learning," in International Conference on Machine Learning (ICML, 2009.
[23] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin, P. Abbeel, and W. Zaremba, "Hindsight experience replay," Advances in Neural Information Processing Systems (NIPS), 2017.
[24] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba, "Openai gym," arXiv preprint, vol. arXiv/1606.01540, 2016.
[25] D. Romeres, D. Jha, A. DallaLibera, B. Yerazunis, and D. Nikovski, "Learning hybrid models to control a ball in a circular maze," arXiv preprint, vol. abs/1809.04993, 2018. [Online]. Available: http://arxiv.org/abs/1809.04993
[26] R. S. Sutton and A. G. Barto, Introduction to Reinforcement Learning, 1st ed. Cambridge, MA, USA: MIT Press, 1998.
[27] R. J. Williams, "Simple statistical gradient-following algorithms for connectionist reinforcement learning," Machine Learning, vol. 8, no. 3-4, May 1992.
[28] M. Jaderberg, V. Mnih, W. M. Czarnecki, T. Schaul, J. Z. Leibo, D. Silver, and K. Kavukcuoglu, "Reinforcement learning with unsupervised auxiliary tasks," arXiv preprint, vol. arXiv/1611.05397, 2016.
[29] J. Schulman, P. Moritz, S. Levine, M. I. Jordan, and P. Abbeel, "Highdimensional continuous control using generalized advantage estimation," International Conference on Learning Representations (ICRL), 2016.
[30] E. Todorov, "Convex and analytically-invertible dynamics with contacts and constraints: Theory and implementation in mujoco," IEEE International Conference on Robotics and Automation (ICRA), 2014.
[31] "Ogre 3D," http://www.ogre3d.org, 2018, [Accessed May 2018].
[32] J. Wu, I. Yildirim, J. J. Lim, B. Freeman, and J. Tenenbaum, "Galileo: Perceiving physical object properties by integrating a physics engine with deep learning," Advances in Neural Information Processing Systems (NIPS), 2015.
[33] J. Wu, E. Lu, P. Kohli, B. Freeman, and J. Tenenbaum, "Learning to see physics via visual de-animation," Advances in Neural Information Processing Systems (NIPS), 2017.
[34] S. Ehrhardt, A. Monszpart, N. J. Mitra, and A. Vedaldi, "Unsupervised intuitive physics from visual observations," arXiv preprint, vol. abs/1805.05086, 2018. [Online]. Available: http: //arxiv.org/abs/1805.05086
[35] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra, "Continuous control with deep reinforcement learning," International Conference on Learning Representations (ICLR), 2015.
[36] C. Finn, T. Yu, T. Zhang, P. Abbeel, and S. Levine, "One-shot visual imitation learning via meta-learning," Conference on Robot Learning (CoRL), 2017.</p>            </div>
        </div>

    </div>
</body>
</html>
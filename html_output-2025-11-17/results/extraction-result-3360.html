<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3360 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3360</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3360</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-77.html">extraction-schema-77</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <p><strong>Paper ID:</strong> paper-c8ee58e5f44f650f6d44b58ec7c2d067e187907d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c8ee58e5f44f650f6d44b58ec7c2d067e187907d" target="_blank">Towards Multi-Modal DBMSs for Seamless Querying of Texts and Tables</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> The MMDB prototype can not only outperform state-of-the-art approaches such as text-to-table in terms of accuracy and performance but it also requires significantly less training data to fine-tune the model for an unseen text collection.</p>
                <p><strong>Paper Abstract:</strong> In this paper, we propose Multi-Modal Databases (MMDBs), which is a new class of database systems that can seamlessly query text and tables using SQL. To enable seamless querying of textual data using SQL in an MMDB, we propose to extend relational databases with so-called multi-modal operators (MMOps) which are based on the advances of recent large language models such as GPT-3. The main idea of MMOps is that they allow text collections to be treated as tables without the need to manually transform the data. As we show in our evaluation, our MMDB prototype can not only outperform state-of-the-art approaches such as text-to-table in terms of accuracy and performance but it also requires significantly less training data to fine-tune the model for an unseen text collection.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3360",
    "paper_id": "paper-c8ee58e5f44f650f6d44b58ec7c2d067e187907d",
    "extraction_schema_id": "extraction-schema-77",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.006287,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Towards Multi-Modal DBMSs for Seamless Querying of Texts and Tables</h1>
<p>Matthias Urban<br>TU Darmstadt<br>Germany<br>matthias.urban@cs.tu-darmstadt.de</p>
<h2>ABSTRACT</h2>
<p>In this paper, we propose Multi-Modal Databases (MMDBs), which is a new class of database systems that can seamlessly query text and tables using SQL. To enable seamless querying of textual data using SQL in an MMDB, we propose to extend relational databases with so-called multi-modal operators (MMOps) which are based on the advances of recent large language models such as GPT-3. The main idea of MMOps is that they allow text collections to be treated as tables without the need to manually transform the data. As we show in our evaluation, our MMDB prototype can not only outperform state-of-the-art approaches such as text-totable in terms of accuracy and performance but it also requires significantly less training data to fine-tune the model for an unseen text collection.</p>
<h2>1. INTRODUCTION</h2>
<p>More than Tables. Decades of research have turned relational databases into highly optimized systems for managing tabular data. However, modern data applications need to deal with other data modalities as well that are often used in addition to tabular data, such as texts or image data [3, 11, 38]. Unfortunately, traditional relational databases are not well-equipped to handle these multimodal scenarios. Today, modalities other than tables are usually not well supported inside relational databases, making it hard to reason about multi-modal datasets. At the same time, rapid advancements in natural language processing and computer vision have made it easier to extract insights from texts, images, and other modalities.</p>
<p>In light of these developments, we believe that it is time to bring these innovations to the world of databases to enable the querying of not only tabular but all types of other data sources. Although some extensions have been integrated into database systems such as full-text search or pattern matching for textual data [10], these other modalities do by far not allow for the same level of querying via SQL as tabular data. Our work aims to fill this gap in the field. To this end, in this paper, we propose Multi-Modal Databases (MMDBs), which is a new class of database systems that can seamlessly store and process data of other modalities as if they would be transformed to tables but without the need to transform them in a first place. A Simple Example. Figure 1 illustrates how we envision how such an MMDB can be used by applications. In the example, the MMDB stores structured patient information as a table that is linked to textual reports that contain additional diagnostic information. If the diagnostic information would be stored in tabular form as well, a SQL query as shown at the top of Figure 1 could easily be used to analyze the correlation between the patient's age and their diagnosis. If the information about the diagnosis is, however, stored inside textual reports that contain the data in an unstructured form,</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Carsten Binnig TU Darmstadt \&amp; DFKI Germany carsten.binnig@cs.tu-darmstadt.de</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">SELECT patients.age, examinations.diagnosis FROM patients JOIN examinations</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">patients</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">examinations</td>
<td style="text-align: center;">result</td>
</tr>
<tr>
<td style="text-align: center;">name</td>
<td style="text-align: center;">age</td>
<td style="text-align: center;">gender</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">age</td>
</tr>
<tr>
<td style="text-align: center;">Alice</td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">f</td>
<td style="text-align: center;">Alice was diagnosed with fever ...</td>
<td style="text-align: center;">42 fever</td>
</tr>
<tr>
<td style="text-align: center;">Bob</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">m</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">23 cough</td>
</tr>
</tbody>
</table>
<p>Figure 1: Example of a SQL Query that executes a multimodal join between a patient table and examination reports. The multi-modal join analyzes the text and extracts values for each queried attribute such as the diagnosis from each examination report.
today a data analyst would first need to create a complex data extraction pipeline to retrieve the diagnostic information from the textual reports.
Multi-Modal Operators. To enable seamless querying of multimodal data using SQL without extracting tables in the first place, in this paper, we propose to extend relational databases with socalled multi-modal operators (MMOps). The basic idea of MMOps is that they extend the set of operators used in traditional relational database systems by new operators that can natively ingest and process data sources of other modalities. As shown in Figure 1, a multi-modal join operator, for example, allows for the joining of the patient table directly with the linked textual patient reports using SQL. As such, the data analyst can correlate the relationship between the patient's age and their diagnoses in a simple and efficient manner without extracting a table from the documents in the first place.</p>
<p>The key idea behind MMOps such as the multi-modal join is that they can accept data sources of other modalities as input and produce tables as an output. To that end, MMOps nicely integrate with the existing query processing capabilities of a traditional database system since MMOps can be composed in query plans along with other relational operators to enable complex analytical queries. For example, after the multi-modal join operator shown in Figure 1, other relational operators such as a projection or a filter can be applied to provide rich query functionality to users. Moreover, as we elaborate later in the paper, in addition to multi-modal joins, we also envision that MMDBs implement a wide spectrum of different MMOps including multi-modal scans, unions, and aggregations. Using Large Pre-trained Models. Realizing such MMOps that can deal with modalities other than tables in a robust manner is far from trivial since other modalities are much harder to process. For example, extracting structured (tabular) data from texts is a nontrivial task since the information can be found in various places in the source documents (e.g., information about the patient's diagnosis) or a different number of tuples need to be extracted per</p>
<p>document (e.g., a report might contain two instead of only one diagnosis). To realize MMOps that can robustly deal with modalities such as texts, we propose to build on the advances of large pre-trained models such as GPT-3 [1]. While such models have been used for other complex data management tasks such as data deduplication or value imputation, they have not been used so far to implement query operators such as joins that can not only reason over tables but also over other modalities such as text or images.
MMDBs for Tables and Text. As a concrete contribution, in this paper, we present a first prototype of an MMDB. In particular, with our MMDB prototype, we focus on text as an additional modality to tables by making use of pre-trained models for language. The main idea is that we realize MMOPs such as multi-modal joins as downstream tasks on top of a pre-trained language model. We believe that the ideas presented in this paper also transfer to other modalities such as images, audio, etc. where similar pre-trained models exist and thus MMOps for those modalities could be realized in a similar manner. However, showing this is beyond the scope of this paper and presents an interesting avenue for future work.</p>
<p>For realizing MMOps, many different non-trivial challenges need to be tackled. First, while large pre-trained models for text such as GPT-3 have been used for data processing tasks such as data cleaning and wrangling [27, 40], it is not clear how multi-modal operations such as joins can be realized based on those models. Hence, as a first direction in this paper, we propose a new pre-trained model that is based on a large language model which allows an MMDB to efficiently realize MMOps. Second, users of traditional database systems demand that query results be computed efficiently. However, large pre-trained models for text such as GPT-3 are computationally intensive, making them costly to run in a database context. Hence, as a direction we propose several optimization strategies to reduce the amount of processing necessary for executing queries over textual data.
Contributions and Outline. To summarize, in this paper we present three major contributions to enable MMOps: (1) As the main contribution, we present the MMDB-Model that is based on a pre-trained language model [44]. For realizing the MMDB-Model, we provide several important extensions to standard language models; i.e., a new pre-training procedure as well as a set of table-specific decoders to turn texts accurately into table data. (2) As a second contribution, we show how MMOps can be realized on top of such an MMDB-Model. In particular, we first show a multi-modal scan as a core multi-modal operator of an MMDB, that can turn text collections into tables. Moreover, we discuss also more complex multi-modal operations like joins as shown before, as well as other operators including unions, and aggregations over text data. (3) We present several optimizations for query execution to make MMOps more efficient on large text collections. In particular, we present multi-modal materialized views and multi-modal (secondary) indexes as two simple yet effective techniques.</p>
<p>In the remainder, we first provide an overview of our MMDB system in Section 2. Section 3 then discusses how the multi-modal scan is realized before Section 4 describes the details of the MMDBModel. Afterward, Section 5 introduces the complex multi-modal operations and Section 6 presents our optimizations to tackle the challenges concerning efficiency. Finally, we present an extensive
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: Overview of the MMDB Architecture: With our MMDB, (1) Users can register text collections as tables using a create table over text statement by providing column names and datatypes. Such text collections are being made available for seamless querying as tables (1). This is enabled by the MMDB-Model which learns to create a tabular representation $T(D)$ of a text collection $D$ (2). (3) For querying an MMDB, users can issue a multi-modal SQL query that is translated into a query plan that contains traditional and multi-modal database operators such as multi-modal joins or scans. (4) To compute a multi-modal operation using the MMDB-Model, representations of query attributes and texts are computed first. Afterward, the MMDB-Model uses these representations to create the data for the output table by extracting values from the text. (5) Multi-modal materialized views and multi-modal (secondary) indexes enable efficient query execution.
experimental evaluation in Section 7, as well as related work in Section 8, and conclude in Section 9.</p>
<h2>2 SYSTEM OVERVIEW</h2>
<p>In the following, we first discuss the key components of an MMDB before we explain the key contribution in contrast to existing approaches in more detail.</p>
<h3>2.1 The MMDB Architecture</h3>
<p>As depicted in Figure 2, MMDB comprises several key components to extend a classical database system and thus enable the seamless processing of text collections as tables.
Multi-modal Database Storage. As a first key component, MMDB introduces an extension to the conventional database data storage that allows the integration of text collections as a first-class citizen (see (1) in Figure 2). The main idea in an MMDB is that the text collections can be seamlessly treated as tables. For adding a text collection as a table, only the schema of the queryable attributes needs to be specified (see (1) in Figure 2). However, it is important that there is no need to tediously extract table data from text manually by using traditional techniques like RegEx or NER.</p>
<p>Instead, the transformation happens seamlessly using the MMDBModel (see (2) in Figure 2). The key novelty of realizing the MMDBModel is that we teach the model the specific skills necessary to extract structured data from text collections given the schema of</p>
<p>the table. As we show in our evaluation, an MMDB can thus often be used out-of-the-box in a zero-shot mode, or in a few-shot mode by fine-tuning the MMDB-Model with only a few examples on a new unseen text collection. We explain more details about the MMDB-Model later in Sections 3 to 4.
Multi-modal Query Execution. For query execution over an MMDB, users can issue multi-modal SQL queries (see (3) in Figure 2). Such multi-modal SQL queries are mapped to multi-modal query plans containing classical relational and so-called multi-modal operators (MMOps) such as multi-modal joins. The most basic MMOp available in our MMDB is a multi-modal scan that produces a table from a text collection (see (4) in Figure 2). In its simplest form, the multi-modal scan extracts one row per text document. In the following, we sketch how this operator can be realized by using our MMDB-Model.</p>
<p>For realizing a simple multi-modal scan that turns a text collection into a table, the operator feeds the attributes to be scanned (i.e., diagnosis) together with the input text of every document one-by-one into the MMDB-Model. Afterward, the table encoder maps the text and query attributes into a joint latent space and the decoder of our MMDB-Model identifies spans of texts that qualify as values for the queried attributes (e.g., the text span in the input for filling the value of the query attribute diagnosis). This extractive approach to derive table data from text has many advantages such as it avoids hallucination as we explain below. Moreover, more complex multi-modal operators such as multi-modal scans that need to extract multiple rows per text document can be realized in a similar style as we discuss in Sections 3 and 5.</p>
<p>To summarize, realizing the extraction of table data from text using MMOps has many advantages. First, the user can ad-hoc register and query new text collections without the need to first materialize a potentially large table. This is in particular interesting if only a few documents of a large collection are queried by users. However, materializing the data of an extraction is also possible in our MMDB using multi-modal materialized views (as we discuss below) which speed up queries that require the full table data. Second, in contrast to simple table extractions from text, we also provide other MMOps, such as multi-modal joins of a table and a text collection that can make use of the table data to provide better extractions as we discuss later.
MMDB Optimizations. Using language models at runtime to implement query operators incurs high computational overhead since these models typically have a high number of parameters. In order to reduce the computational overhead, in our MMDB, we employ two crucial optimizations to enable highly efficient multi-modal queries (5). Multi-modal materialized views shift costly text processing to an offline extraction phase and multi-modal (secondary) indexes allow for efficient execution of online multi-modal queries that only need to read a fraction of the text collections.</p>
<h3>2.2 Discussion</h3>
<p>While there exist already approaches that transform textual data to tables like text-to-table [43] or STable [33], we think that these approaches are not suitable for implementing multi-modal database operations in an MMDB.</p>
<p>One key difference of our approach compared to text-to-table and STable is that the underlying models have to be trained from scratch for every new text collection. While text-to-table, for example, also uses pre-trained language models as a basis, it is directly initialized with the pre-trained weights of a language model (i.e., BART [20]), which has been pre-trained on plain text only. Different from that, we carefully design a new pre-training procedure that allows our MMDB-Model to provide more meaningful representations for table extraction and thus realize MMOps on unseen texts with just a few examples.</p>
<p>Moreover, another key difference is that text-to-table or STable produce the data for an output table using a transformer-based generative decoder [39]. The first disadvantage here is that the model can "make up" values, that are not actually present in the input text, but that the model picked up during (pre-)training. This phenomenon is called hallucination [15]. On top of that, transformer-based decoders output tables token-by-token in an autoregressive manner, which requires a pass through the decoder for every token in the output table. This results in a computationally expensive decoding process as we show in our evaluation. Finally, text-to-table or STable also do not directly support complex MMOps like joins, unions, and aggregations which allow an MMDB to produce higher-quality extractions as we explain later.</p>
<h2>3 MULTI-MODAL SCAN</h2>
<p>In this section, we focus on the multi-modal scan which is a core multi-modal operator in an MMDB that takes a text collection and transforms it into a table using the MMDB-Model as shown in Figure 2 (4). More complex operations such as joins and unions build on the ideas of the scan as we explain in Section 5.</p>
<h3>3.1 Basic and Complex Multi-modal Scan</h3>
<p>The task of a multi-modal scan is to turn a text collection $D$ into a table $T(D)$. In a basic multi-modal scan, each text $d \in D$ corresponds to a single tuple $t_{d} \in T(D)$. For instance, a patient report might correspond to a single row in the examination table. However, text collections are often more intricate, leading to the concept of complex multi-modal scans. We differentiate between complex multi-modal scans for multi-row texts and multi-table texts:
Multi-row Texts. In a multi-row text, a single text $d \in D$ may correspond to multiple tuples $t_{d}^{1}, t_{d}^{2}, \ldots$ within the table $T(D)$. As an example, a patient report documenting multiple examinations of the patient would result in multiple rows within an examination table. To accommodate this complexity, a multi-modal scan must possess the capability to produce multiple tuples in response to a single input text.
Multi-table Texts. In multi-table texts, the information within a text collection may be more complex and might be better represented by multiple extracted tables instead of one table. For instance, the text collection of patient reports might consist of a table representing information of the examining physician as well as a additional tables capturing the information of the examination.
Table Definition in an MMDB. For querying text collections in our MMDB, a user needs to specify during table creation (see (ii) in Figure 2) of which table type a text collection is; i.e., if the texts are single-row texts, multi-row texts, or multi-table texts. Moreover,</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Overview of a basic scan. The scan transforms each input text into a single row. As input (at the top), each text is paired with the query attributes using a MASK token for the values to be extracted. The MMDB-Model computes representations of all text tokens and the attribute information which include the MASK tokens. Based on these representations, the span-detect head assigns I-O-B tags for the text tokens to be extracted for each queried attribute. In this case sore throat is extracted for the diagnosis. Finally, the result table is constructed from the extracted text passages.
for multi-row texts, a so-called set of identifying attributes (i.e., an attribute where values are unique within a document) needs to be defined in the table schema when creating a table for a text collection, which is used for the multi-row scans as explained later in Section 3.3. For instance, for examination reports, the column patient name would be an identifying attribute, because it identifies different patients mentioned in the same text.</p>
<h3>3.2 The Basic Multi-Modal Scan</h3>
<p>In the following, we first focus on how to use our MMDB-Model for the basic multi-modal scan which transforms one text into one row. Afterward, we explain how we can use the MMDB-Model for the more complex scenarios (multi-row and multi-table texts). Similar to other approaches for table extraction from text [33, 43], we employ an encoder-decoder architecture for our model. In particular, for realizing our MMDB-Model we built on TaBERT [44] as the encoder, which is a pre-trained language model that is already aware of tabular data.</p>
<p>However, in order to support MMOps as downstream tasks several significant extensions are needed as we discuss below. For example, different from other existing approaches, our MMDB-Model comes with a set of new decoder heads that are not transformerbased and thus eliminate hallucination and are more efficient as discussed in Section 2.2. Moreover, the encoder and decoder are pre-trained by tasks that learn in general to extract table data from text as presented in Section 4. This allows the usage of the model in zero and few-shot settings where only a few example extractions per table are needed to fine-tune the model on new unseen text collections.
Encoder. The process of computing a multi-modal scan operation using the MMDB-Model is sketched in Figure 3. In a nutshell, the encoder first maps the input text into a latent space by computing vector representations $\hat{w}$ for all input tokens $w$. The decoder then utilizes these representations to identify all relevant tokens for the input query, similar to how extractive question answering is solved using language models [7]. However, transforming texts to tables is much more complex than extractive question answering since it involves the extraction of potentially many different values of one row or the removal of duplicates as we discuss below. To support the extraction of values for multiple columns using our table decoder, we encode all the columns jointly with the input texts as shown in Figure 3. The columns are encoded using column names and types. The MASK token for a query attribute indicates that the value of the attribute needs to be extracted from the text. For other operations such as joins some attribute values will be additionally provided as input as we discuss later.
Decoder. Our extraction-based table decoder consists of several classification heads that take on different tasks for table decoding. The most important component is the span-detect (SD) head that extracts values, potentially consisting of multiple tokens, for all queried columns. In essence, the layers of the SD head pair the representation of each token of the text input $\hat{w}$ with the representation of each masked cell $\bar{c}$ for the column to be extracted and classify whether the token is part of a value for the column. We employ so-called I-O-B (Inside-Outside-Beginning) tags to extract potentially multiple tokens for each column (i.e., complete text spans). With I-O-B tags, the first text token for a value that should be extracted is marked with a B-tag to indicate its beginning, while subsequent tokens belonging to the same value receive an I-tag to indicate they are inside the value. Tokens that do not belong to a value are marked with an O-tag. This clearly allows us to extract a single table row for each input text, as shown in Figure 3.</p>
<p>The I-O-B tags are predicted by the SD head. To realize the SD head, we use two learned matrices $W_{B}^{S D}$ and $W_{I}^{S D}$ - each representing a single additional layer after the encoder - and a learned threshold thresh $S D$. The tag, for each pair of token- and cellrepresentation $(\hat{w}, \bar{c})$, is calculated based on these learned matrices as follows:</p>
<p>$$
\operatorname{tag} \in \underset{x \in{I, O, B}}{\arg \max }\left{\begin{array}{ll}
\hat{w}^{T} \cdot W_{x}^{S D} \cdot \bar{c} &amp; \text { if } x \in{B, I} \
\text { thresh }_{O}^{S D} &amp; \text { else }
\end{array}\right.
$$</p>
<h3>3.3 Multi-row and Multi-table Texts</h3>
<p>Multi-row texts and multi-table texts present a more complex scenario as discussed before. In the following, we discuss both cases in detail.
Multi-row Texts. In order to generate an arbitrary number of tuples per document, we utilize an iterative approach as depicted in Figure 4 and formalized in Algorithm 1 for a single text $d \in D$. For the purpose of simplification but without loss of generality, we make the assumption that there exists a single so-called identifying attribute $c_{k e y}$ in the table definition of $D$ that is defined in the table schema of the text collection as discussed in Section 2. In our example scenario, we assume the attribute $c_{k e y}=$ diagnosis serves as the identifying attribute.</p>
<p>In the first phase of the procedure, the model extracts all values for the identifying attribute $c_{k e y}$, using a so-called column-detect head in the decoder. The number of values extracted in this step determines the number of tuples that will be generated for a text collection. As demonstrated in Figure 4, the names of the diagnoses, such as sore throat and fever, are extracted as values for the identifying attribute, and thus two output rows will be generated. Different from before, we do not use the span-detect head to extract values for $c_{k e y}$, but a column-detect (CD) head that is conceptually</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Overview of a complex multi-modal scan. To extract an arbitrary number of tuples, the input sequence is passed multiple times through the encoder. The input for extracting one tuple (as shown at the top) consists of the input text paired with the column names to be extracted. In phase (1), values of the so-called identifying attribute (e.g., diagnosis) are extracted, using the column-detect head of our decoder. In phase (2), the model computes the values for all other columns. For example, depending on the diagnosis, the treatments (e.g., Aspirin for fever) are extracted.
identical to the span-detect head but consists of a different set of weights $W_{B}^{C D}, W_{I}^{C D}$ and threshold $t h r e s h_{D}^{C D}$. The reason for this is that the CD-head is pre-trained on different tasks to extract values for multiple entities and not only one, as we will discuss in Section 4. In our ablation study, we show that this results in more accurate extractions for some datasets.</p>
<p>In the second phase, the extraction process is conducted on the remaining columns that depend on the identifying attribute, denoted as $c_{1}, c_{2}, \ldots$. To accomplish this, the MASK token of the identifying attribute $c_{k e y}$ is now replaced with the values extracted in the first phase, as depicted in Figure 4. These sequences are then processed by the model to extract the values for the dependent columns, using the span-detect head to only extract the values corresponding to the value extracted in the first phase. This iterative approach enables the model to effectively process complex scans using only a few passes through the transformer per input text. As we will show in our evaluation, this procedure is computationally much less expensive than using a transformer-based decoder that is used by existing approaches such as text-to-table [43].</p>
<p>Finally, we want to mention that our approach for multi-row scans can easily be extended for the case where the identifying attributes is composed of multiple columns. For instance, in a text collection of medical records, the uniqueness of a diagnosis may be determined by both its name and the date of diagnosis (i.e., the generated table should contain multiple tuples for the diagnosis fever, if it has been diagnosed on multiple dates). In this case, we first extract all diagnoses using the column-detect head, then we collect all corresponding dates using the span-detect head. Afterward, the values for the remaining (dependent) query attributes are extracted for each pair of diagnosis and date like before.
Multi-table Texts. Multi-table texts do not require additional algorithmic considerations. In this case, we simply prepend the name of the table we want to extract from the text to the name of the identifying attribute $c_{k e y}$. The model is trained to infer from the column names which table to extract.</p>
<p>Algorithm 1 Complex multi-modal scan
Require: $d=\left(w_{1}, \ldots, w_{n}\right), C=\left(c_{k e y}, c_{1}, \ldots, c_{m}\right)$</p>
<ol>
<li>Compute representations
$\triangleright 1$ 1st phase
$\left(\hat{w_{1}}, \ldots, \hat{w_{n}}\right),\left(c_{\text {key }}, \hat{c_{1}}, \ldots, \hat{c_{m}}\right) \leftarrow \operatorname{encoder}(d, C)$</li>
<li>Find values for $c_{k e y}$
$V_{k e y} \leftarrow$ column $-\operatorname{detect}\left(\left(\hat{w_{1}}, \ldots, \hat{w_{n}}\right), c_{k e y}\right)$
$\left{v_{1}^{k e y}, \ldots, v_{o}^{k e y}\right} \leftarrow$ duplicate $-\operatorname{detect}\left(V_{k e y}\right)$</li>
<li>Get values for $c_{1}, \ldots, c_{m}$
for $v_{i}^{k e y} \in\left{v_{1}^{k e y}, \ldots, v_{o}^{k e y}\right}$ do $\triangleright 2$ nd phase
$\left(w_{1}^{i}, \ldots, w_{n}^{i}\right),\left(c_{1}^{i}, \ldots, c_{m}^{i}\right) \leftarrow \operatorname{encoder}\left(d, v_{i}^{k e y}, c_{1}, \ldots, c_{m}\right)$
$\left(V_{1}^{i}, \ldots, V_{m}^{i}\right) \leftarrow \operatorname{span}-\operatorname{detect}\left(\left(w_{1}^{i}, \ldots, w_{n}^{i}\right),\left(c_{1}^{i}, \ldots, c_{m}^{i}\right)\right)$
$\left(\left{v_{1}^{i}\right}, \ldots,\left{v_{m}^{i}\right}\right) \leftarrow$ duplicate $-\operatorname{detect}\left(V_{1}^{i}, \ldots, V_{m}^{i}\right)$
end for
return $\left[v_{1}^{k e y} \quad v_{1}^{1} \quad \ldots \quad v_{m}^{1}\right]$
$\left.v_{o}^{k e y} \quad v_{1}^{o} \quad \ldots \quad v_{m}^{o}\right]$</li>
</ol>
<h3>3.4 Removing Duplicates</h3>
<p>In the case of multi-row texts, even when using the concept of identifying attributes, duplicate rows can occur due to different spellings or synonyms. For example, the same text might contain the value fever and high body temperature which refers to the same diagnosis. In such cases, a multi-modal scan might extract two rows for the same diagnosis. Hence, we need to remove such duplicates in a multi-modal scan after the extraction. However, separating cases of duplicates from non-duplicates is non-trivial.</p>
<p>To address this issue, we thus use an additional duplicate-detect head as part of our decoder which allows us to filter out duplicates. The duplicate-detect head detects duplicate values based on their representation in the latent space. To compute a representation of a value that potentially consists of multiple tokens, we employ span-representations as introduced by Lee et al. [18]. The duplicatedetect head consists, similar to the span-detect head, of a learned matrix $W^{D D}$ and a learned threshold $t h r e s h^{D D}$. Two values for a column are considered duplicates if the learned similarity of the span representations $s_{A}$ and $s_{B}$ is larger than the threshold $s_{A}^{T} \cdot W^{D D} \cdot s_{B}&gt;$ threshDD.</p>
<h2>4 THE MMDB-MODEL</h2>
<p>At the core of the multi-modal operators such as the multi-modal scan introduced before is the MMDB-Model. An important aspect is that the MMDB-Model does not require extensive training data for every new set of documents that should be added to the MMDB. To enable this, we pre-train the MMDB-Model with new objectives such that the model learns the general skills to extract tables from texts for unseen documents. As a result, as we show in our evaluation, the MMDB-Model can be used in zero- or few-shot mode with high accuracy, which is in stark contrast to existing approaches such as text-to-table [43].</p>
<p>leonard gordon goodman ( born 25 april 1944 ) is an english professional ballroom dancer, dance judge, and coach... [SEP] name | len goodman [SEP] date of birth | [MASK] [SEP] deathdate | none [SEP] ...
jacob goodman ( september 14, 1853 - march 9, 1890) was a major league baseball player who played for the 1877 pittsburgh alleghenys, ... [SEP] name | jake goodman [SEP] date of birth | 14 september 1853 [SEP] deathdate | [MASK] [SEP] ...
adam smith frsa ( 16 june 1723 - 17 july 1790) was a scottish economist and philosopher who was a pioneer in the thinking of political ... [SEP] name | adam smith [SEP] date of birth | [MASK] [SEP] deathdate | 17 july 1790 [SEP]</p>
<p>Figure 5: Example pre-training sample consisting of three rows and texts from our pre-training dataset. Wikipedia abstracts are paired with rows constructed from Wikdata containing information about the same entities. Labels for the MCR objective are marked in red and labels for the CTA objective are in bold.</p>
<h3>4.1 Pre-training Objectives</h3>
<p>The pre-training procedure is comprised of three new pre-training objectives, each targeting different skills necessary to perform table extraction for multi-modal database operators.
Skill 1 - Extract values for specified columns. Multi-modal database operators extract values for specified columns from text. For example, in a scan for multi-row texts, different values for the same column (e.g., diagnosis) need to be extracted. To support that our model learns to detect values for certain columns, we introduce the Column-Text-Alignment (CTA) task, which aligns table columns to text. In this task, we pair texts with tables and compute a representation for all columns using the schema information (attribute names and types) and the actual values of the column. Afterward, we let the model detect segments of text that are a potential value for a column. This task not only enables efficient multi-modal scans that we introduced before, but also more complex operations such as a multi-modal union or join as we explain later in Section 5.
Skill 2 - Extract values for specified rows. Second, it is often important for the model to align texts with table rows and not only columns. For example, in a multi-modal join only the values (e.g., diagnosis) corresponding to a linked table row (e.g., a patient) should be extracted. To let our model learn connections between text passages and table rows, we introduce an additional Masked Cell Reconstruction (MCR) pre-training task. In this task, we pair table rows and texts and mask out random cell values of the table row (that occur in both text and table row), and ask the model to reconstruct the masked values from text using the signals from neighboring cells, including the cells from the same row. Different to CTA, MCR thus teaches the model to extract values for a certain row only (e.g. only the diagnoses of Bob), while CTA extracts all values corresponding to a column (e.g. all diagnoses mentioned in a text).
Skill 3 - Detect duplicates. As discussed before, texts often contain the same information multiple times. For example, a patient report might mention twice that a patient was diagnosed with fever in different places in the document, potentially using different spellings or synonyms. As such, it is important to detect when two extracted spans refer to the same or to different entities to avoid unwanted duplicates in the output of multi-modal operators. Therefore, we introduce the Duplicate-Detection (DD) pre-training task as our final pre-training objective. For each column, $D D$ takes pairs of spans of values of the text as input and is trained to predict whether they are the same or not. For this objective, our training corpora that we explain below leverages T-REx [8] which contains
information about which spans in a text relate to the same actual entity.</p>
<h3>4.2 Pre-training Details</h3>
<p>In the following, we discuss the details of our pre-training objectives discussed before. In a nutshell, for pre-training we pair the transformer-based encoder with our table-decoder, and train both end-to-end. For pre-training the MMDB-Model, we feed pairs of texts and a sample of table rows into the encoder of our model and apply the following pre-training objectives. Our table encoder makes use of vertical self attention as introduced in [44] to let signal flow between the individual linearized rows. Moreover, remember that we use a multi-head table decoder with the the span-detect, the column-detect and the duplicate-detect head as already introduced in Section 3.
MCR and CTA Objectives. Let $\hat{w}<em 2="2" i_="i,">{i, 1}, \hat{w}</em>}, \ldots, \hat{w<em 1="1" i_="i,">{i, n}$ be the token representations of the $i$-th text and let $\hat{c}</em>}, \hat{c<em i_="i," m="m">{i, 2}, \ldots, \hat{c}</em>$ from the span-detect and column-detect heads.}$ be the cell representations of the $i$-th tuple, after encoding the table row with our encoder. For pre-training the MCR and CTA objectives as introduced before, we use the matrices $W_{B}, W_{I}$ and the threshold $\operatorname{thresh}_{O</p>
<p>For MCR, we use the cell representations of masked cells $\hat{c}<em 1="1" i_="i,">{i, j}$ and the token representations $\hat{w}</em>}, \hat{w<em i_="i," n="n">{i, 2}, \ldots, \hat{w}</em>}$ for the extractions. For CTA, we use the column representation $\hat{h<em l="1">{j}=\frac{1}{k} \sum</em>}^{l \leq k} \hat{c<em C="C" M="M" R="R">{l, j}$ as input instead of the cell representation of the masked cell. We train the matrices and the threshold using cross-entropy to obtain loss values $\mathcal{L}</em>$.}$ and $\mathcal{L}_{C T A</p>
<p>To ensure that the model utilizes the actual text values during pre-training and not only the schema information (i.e., column names) of the table we aim to extract, we randomly mask out also column names from the linearized input to our model in $15 \%$ of the cases.
DD Objective. For the DD pre-training objective, we use the duplicatedetect head consisting of the learned matrix $W_{D D}$ and threshold $\operatorname{thresh}<em D="D">{D D}$, as introduced in Section 3. We train the matrix and the threshold by minimizing a cross-entropy loss $\mathcal{L}</em>$.
Combined Pre-Training. For the pre-training, we use a combined pre-training as suggested in [4, 7] where we add up all the losses of all objectives and train the entire model (including the encoder) using this multi-task loss. To balance the losses we found it beneficial to use a weighted sum. We choose $\alpha=300, \beta=80, \gamma=\delta=1$ by examining the performance on a development set.</p>
<p>$$
\mathcal{L}=\alpha \cdot \mathcal{L}<em A="A" C="C" T="T">{M C R}+\beta \cdot \mathcal{L}</em>}+\gamma \cdot \mathcal{L<em L="L" M="M">{D D}+\delta \cdot \mathcal{L}</em>
$$</p>
<p>Moreover, for the pre-training, we realized that our model benefits from adding the original Masked Language Model loss $\mathcal{L}<em _MLM="{MLM" _text="\text">{\text {MLM }}$ of BERT. We thus added $\mathcal{L}</em>$ to the other losses as well. We compare the effect of the different pre-training objectives in our ablation study in Section 7.3.}</p>
<h3>4.3 A New Pre-Training Corpus</h3>
<p>Unfortunately, currently no pre-training corpus exists that allows us to pre-train our MMDB-model as outlined in Section 4. In contrast to existing corpora such as [2, 13, 46], we need a different parallel corpus where the texts contain the same information (e.g., same entities) as the tables, and where we know which cell values also occur in a text, allowing us to mask them for pre-training. Hence, we constructed a new parallel open-domain pre-training corpus from Wikidata and Wikipedia for pre-training multi-modal database models. We will open-source the corpus together with our code once our paper is published.</p>
<p>The main idea of our dataset is that we make use of T-REx [8], a large-scale alignment of Wikidata triples and Wikipedia abstracts. The T-REx dataset contains 11 million alignments of Wikidata triples to Wikipedia abstracts. All the 3.09 million abstracts occurring in T-REx are also part of our dataset. T-REx itself is created automatically using the distant supervision assumption for computing the alignment and can thus be noisy sometimes, but it allows us to construct a large pre-training corpus with objectives aligned to the downstream task of multi-modal database operations. T-REx has been used by other researchers for training and evaluating their machine learning models [32].</p>
<p>Hence, we use the alignment of T-REx as a starting point to construct our parallel corpus of texts and relational tables. T-REx contains information about the location of mentioned entities in the texts, which we can use as labels for our pre-training objectives. As texts, we simply use the aligned Wikipedia abstracts and construct additional tables using Wikidata, by grouping similar entities together in a table and using Wikidata properties as columns. We use several techniques to obtain a rich and diverse dataset, e.g. we randomize column names using Wikidata's aliases or concatenate multiple abstracts to simulate texts about multiple entities.</p>
<h2>5 OTHER MULTI-MODAL OPERATORS</h2>
<p>The multi-modal scan operator ignores valuable information stored in the other tables of the database. In the following, we discuss additional multi-modal operators that can incorporate information from other tables in an MMDB to improve the quality of extractions.</p>
<h3>5.1 Multi-modal Joins</h3>
<p>A multi-modal join computes a join over a table $T$ and a text collection $D$. To be more precise, in an MMDB a document $d \in D$ is linked to (at least) one tuple $t \in T$ as a join partner. During execution, the multi-modal join extracts query attributes from $d$ and appends them to $t$. A naive version of a multi-modal join would be to extract these attributes similar to a multi-modal scan.</p>
<p>However, different from a multi-modal scan, with a multi-modal join, we can make use of the data from the tuple $t \in T$ as evidence for extracting attributes from texts. For example, assume the user joins a patients table with a document collection of patient reports
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 6: A multi-modal join uses the tuples of the join partner (top left, in yellow) as additional evidence to improve extractions. Some values (name, date of birth) are present in both the text and the join partner and help the model to locate relevant passages of text. We pair each text with their linked table rows and extract a tuple for each linked row.
where one report might contain texts for multiple patients. Hence, if the name of the patient in $t$ and the name in the document $d$ match, this is a strong signal that the surrounding text in the document is about the requested patient and therefore relevant for extraction.</p>
<p>To incorporate the data from the join partner $t$ as additional evidence to extract values from $d$, we feed attributes of $t$ along with the text into the MMDB-Model. To be more precise, an input sequence to the MMDB-Model consists of three parts: the text document $d$, the query attributes for extractions, and column-value pairs from the join partner $t$, as depicted in Figure 6. For the extraction, we use the span-detect head that we also use for extracting values for the scan.</p>
<h3>5.2 Multi-modal Unions</h3>
<p>Different from a multi-modal join, a multi-modal union extracts rows of a text collection to append them to rows of a table. For a multi-modal union, evidence from the table can thus provide example rows that help with the the extraction. For instance, if a patient stored in the table has fever as a diagnosis, this can enrich the model's understanding in regard that it needs to extract diagnoses for illnesses and not for computer problems.</p>
<p>To provide existing rows as example, we randomly sample a few rows from the table and encode them along with the attributes to be extracted from the texts using our encoder as shown in Figure 7. In order to let signal from example rows enrich our column representations, in our MMDB-Model we use vertical self-attention layers as mentioned before to teach the MMDB-Model to make use of example values for extracting rows from documents. The vertical self-attention layer applies self-attention along columns of different input rows.</p>
<p>The input to the model consists of input sequences of multiple sample rows from the table and the input sequence containing text for which we aim to extract additional rows. Based on this input, we compute the the result of the union similar to the multi-modal scan. The only difference is that in the first phase of the multiphase algorithm, we compute a column representation by taking the mean of the cell representations of the column values as well as the masked query attribute. We then extract spans from texts using this column-representation. For span-detect, we use the enriched representation of the masked column.</p>
<p>Finally, an important point is that we provide the same example rows from the table for all texts to be transformed since encoding additional example rows means a significant runtime overhead.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 7: A multi-modal union uses the values from the table (top right, in yellow) as additional evidence to enrich the representations of the masked columns using vertical self-attention. These values act as example values for the columns and ultimately result in more accurate extractions.
This way, the example rows need to be encoded only once for many texts which improves the efficiency of the union significantly.</p>
<h3>5.3 Multi-modal Aggregation</h3>
<p>Multi-modal aggregation is different from joins and unions since it does not make use of data in an existing table to enhance the extraction. Instead, a multi-modal aggregation directly works on a collection of texts and allows the grouping/aggregation on columns extracted from the text (e.g., to count how often a certain diagnoses was found across all patients). A naive solution to multi-modal aggregation would be to use a multi-modal scan and then group directly based on the extracted values. However, again the variability of language can result in the same values being expressed in different ways in text. To address this issue and to ensure that different surface forms (e.g., fever vs. high temperature) are placed in the same group, we aggregate values based on their representation, similar to how we detect duplicates. As such, we re-use the duplicate-detect head introduced for de-duplication in scans to predict whether two values should belong to the same group.</p>
<h2>6 PERFORMANCE OPTIMIZATIONS</h2>
<p>Query latency is a major concern of database research for decades and is thus also an important aspect of MMDB. At the same time, incorporating a language model in an MMDB, as proposed in this paper as well as other works [3, 38], comes with significant computational overhead. Since we do not want to sacrifice extraction quality by using subpar models, we thus present several optimizations to our system that allow us to use a language model efficiently.</p>
<h3>6.1 Multi-modal Materialized Views</h3>
<p>Materialized views are a well-established technique in databases for optimizing complex queries by performing computation during an offline phase. In the context of MMDB, multi-modal materialized views aim to remove the computational burden of transforming documents into tables during query time. By storing the result of a multi-modal operator, such as scan, join, or union, in a materialized result table, multi-modal materialized views function as a cache that avoids the need to use the language model at query execution.</p>
<p>One major difference between a multi-modal view and a classical materialized view is that we normalize the extracted result using our MMDB-Model. In particular, we use the duplicate-detect head of the model to replace similar values that occur in different rows with a unified value if they have the same meaning. For instance,
values such as high body temperature and fever that are used for the same diagnosis would be replaced by one normalized value (e.g., fever), to be able to effectively aggregate on the diagnosis column of the materialized view.</p>
<h3>6.2 Multi-modal Secondary Indexes</h3>
<p>While materialized views clearly speed-up queries, they have several (known) drawbacks. These include the significant upfront cost of materialization and reduced flexibility, as they assume certain query patterns. As an alternative to avoid materialization and still answer queries efficiently, we introduce secondary multi-modal indexes. Multi-modal indexes are used in our MMDB when texts should be filtered by an attribute extracted from the text. For example, users might be interested only in treatments for patients diagnosed with fever. In the current version of MMDB, in fact, we require the existence of such a secondary multi-modal index on an attribute if it should be used for filtering.</p>
<p>The core idea of a multi-modal secondary index is similar to a traditional index; i.e., the index maps a search key for the query attribute (e.g., diagnosis) to text documents that contain the search key. For constructing a multi-modal secondary index for a query attribute, our approach leverages our MMDB-Model and the columndetect head to extract the values for the search key from all texts. For building the index, we put the extracted values and the paths to the text documents into a traditional index such as a B-tree or a hash-index to be able to quickly retrieve texts at query time. However, unlike traditional indexes, we group similar extracted values into one index entry using the duplicate-detect head of our MMDBModel. This allows the index to return text document containing the diagnosis high temperature, even if the user queries for fever. Note, after the resulting texts are returned by an index lookup, the texts still need to be processed by a multi-modal operator that transforms the text in a row and normalizes the output (e.g., for grouping and aggregation) as described before in Sections 3 and 5.</p>
<h2>7 EXPERIMENTAL EVALUATION</h2>
<p>In this section, we present the results of our experimental evaluation which justifies the design of the multi-modal database (MMDB) and the MMDB-Model in particular. For this, we compare our implementation of multi-modal database operations (MMOps) with an implementation using text-to-table[43] and show that our model is both computationally more efficient and achieves better extractions with limited training data.
Evaluation Datasets. For our evaluation, we use the rotowire dataset [42], which is a dataset that has been used in other papers. The dataset consists of basketball game reports and tables summarizing the most important statistics of each game. In the rotowire dataset, each game report is paired with two tables: one containing statistics concerning the two opposing teams and one containing statistics about the participating players. We use the post-processed dataset version provided by Wu et al. [43] that filtered out table values not present in the text. On this dataset, the goal of the multi-modal scan is to transform the game reports into tables containing the statistics. Here, we are able to perform complex scan operations since multiple tuples and tables need to be extracted per input text. To be able to simulate join operations, we</p>
<p>combine the dataset with two tables containing general information about basketball teams and players, which we constructed from Wikipedia infoboxes. We linked each game report to the tuples of teams and players of the general information table, that participated in the game. The multi-modal join will extract statistics from the game reports and add these as columns to the general information table. For the union operation, we union the game reports with the statistics tables of the training set.</p>
<p>Unfortunately, the other datasets used in [43] are not suitable for our evaluation, because they are either a subset of our pre-training dataset [17], or the contained texts are too simplistic and consist of only one or two sentences [28]. Hence, we construct an additional dataset using T-REx [8], similar to how we constructed our pre-training dataset (see Section 4.3). Importantly, this dataset is comprised of three unseen domains from Wikipedia that we left out in our pre-training dataset: nobel prize winners, skyscrapers, and countries. The dataset consists of the Wikipedia abstracts and corresponding table rows, which we split into train, test, and validation sets. For this data, in contrast to the rotowire dataset, only a single row needs to be extracted per input text.
Baselines. We compare our MMDB which is based on the MMDBModel to text-to-table [43], the state-of-the-art approach for translating texts to tables. The more recent STable approach [33] was not able to match its result on text-to-table tasks, despite increased computational overhead, hence we omit a comparison.
Experimental Setup. All experiments were executed on a DGX A100 server ${ }^{1}$. For pre-training, we used 4 GPUs which took approximately 8 days for training our model for 6 epochs on our pre-training dataset. For fine-tuning and inference - in particular, for the performance measurements in experiment 2 - we used 2 GPUs only.</p>
<h3>7.1 Exp. 1: Accuracy of Multi-Modal Queries</h3>
<p>In this first experiment, we measure how accurately our model is able to answer queries containing MMOps in contrast to using text-to-table[43]. Since modern DBMS are used in various domains and collection of training data for these domains is often expensive, we focus on scenarios where only limited data is available for finetuning. Hence, we train our model on small subsets of the training set, and do not use the validation set for hyperparameter tuning. Instead, we use the hyperparameters obtained from optimization on a separate subset of T-REx. For all operations, we compute an F1 score for the extracted table values for each text in the test set and report the mean of these F1 scores.
Results on rotowire. The rotowire dataset is very different from our pre-training dataset because the texts are game reports, not Wikipedia abstracts and the tables contain numbers only (e.g. the number of scored points). Hence, the evaluation on rotowire can provide a good estimate of our model's performance in a new domain. Figure 8 shows how the extraction quality increases when we provide more training data for fine-tuning to the new domain. Even with as few as four labeled texts per table as training data, our model is already able to achieve a mean F1-score of up to $60 \%$ for both the player table and the team table.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 8: Mean F1 scores for MMOps on the rotowire dataset. The F1 scores for all models increase as the number of training samples increases, but MMDB consistently outperforms text-to-table.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 9: Mean F1 scores for a multi-modal scan on different domains of T-REx. The zero-shot performance is depicted on the very left of each plot. MMDB achieves impressive zero-shot performance and outperforms text-to-table even with increasing training data.</p>
<p>Since text-to-table has been pre-trained on plain-text only, it struggles in these few-shot scenarios. It needs at least 64 labeled texts as training samples to achieve a mean F1 score larger than $0 \%$. As the size of the training dataset is increased, we see that the performance of both models increases, but it takes over 4000 training samples before text-to-table matches the MMDB-model. When trained on the full dataset, MMDB is still slightly better, especially for joins with the player table, which is still impressive, since we did not tune our hyperparameters for the rotowire dataset which was used for text-to-table though. Furthermore, comparing scans to unions, we see that especially with very limited training data (i.e. four training samples), the example values provided in a union can greatly improve the extracted tables.
Results on T-REx. The T-REx dataset is more similar to our pretraining dataset since texts are based on Wikipedia abstracts. However, we make sure that it only contains texts not seen during pre-training and thus contains three unseen domains. Here, the tables contain not only numbers as values but also dates and short spans of texts. In these scenarios, MMDB can even be used out-ofthe-box, in a zero-shot mode without any additional fine-tuning. Figure 9 shows how extraction quality improves when increasing the training set size for a multi-modal scan. With no training data at all, we achieve almost $80 \%$ F1 score for both the nobel and the skyscraper domain. Moreover, with increasing training data, there is always a large margin between MMDB and text-to-table. We attribute this to our novel pre-training procedure, which teaches MMDB the necessary skills to perform MMOps.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 10: Runtimes on rotowire. We vary the selectivity of join queries and observe the runtime of the multi-modal operator when implemented with our model or text-to-table. The Y-axis is in log-scale to be able to show the longer execution times of text-to-table (in the order of several minutes) and MMDB (in the order of a few seconds) in the same plot.</p>
<h3>7.2 Exp. 2: Runtime of Multi-modal Operators</h3>
<p>The second experiment focuses on the runtime of multi-modal queries in MMDB. While it is certainly computationally expensive to incorporate a pre-trained language model during query execution, we think there are many scenarios (e.g. queries with a low selectivity) where it makes sense to use a large language model for online (ad-hoc) queries on text collections. Moreover, MMDB offers extensions such as secondary indexes that make it very effective in these scenarios, as we show in the upcoming experiments. For all other scenarios, we offer materialized views to remove the computational burden of text processing from query time. In this experiment, we perform all experiments on the test set of rotowire, because it allows measuring the more interesting complex operators.
Selective Multi-modal Join Queries. In the first scenario, we look at selective multi-modal queries resulting from joins between a table and a text collection when using a filter attribute on the table attributes. As a dataset, we use the game reports of rotowire and join them with the general information tables for players and teams as explained above. The join queries in this experiment have different selectivities; i.e., the query only selects a few players or teams before executing the multi-modal join with the text collection. Since tuples in the tables are linked to the game reports (e.g. a tuple about a team is linked to all the game reports the team participated in), this usually results in only a few selected texts as well (i.e., the filter on the table acts as an index into the text collection). Note, since players are from different teams and participate in many games, for queries that involve the players table, the runtime is overall higher since the join is less selective (i.e., with the same selectivity on the players table, more game reports are selected).</p>
<p>Figure 10 shows how the different selectivities affect the overall query runtime of the query containing the multi-modal join operator. We compare a version of the query which is implemented naively using a text-to-table model and our MMDB-Model. The main difference between these approaches is that text-to-table always needs to first translate each selected text into full player and team statistics tables, containing the statistics of all mentioned teams and players. Since many generated rows will not have a join partner in joins with low selectivity, this leads to many generated rows being discarded. Our MMDB, instead, is able to generate only those rows that have a join partner using our multi-modal join. This results in significant differences in the observed runtimes as shown in Figure 10. While the operation of text-to-table is usually in the order of several minutes, the multi-modal join is typically
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 11: Execution times of different MMOps with a multimodal secondary index. The index allows to only transform those texts to tables, which are selected due to a selection criterion based on a value extracted from the text collection. The Y-axis is in log-scale to be able to plot the naive solution using text-to-table and the runtime when using the index in the same plot.
able to perform the operation in just a couple of seconds while having higher accuracy as shown in the previous experiment. For instance, for a join with the players table, text-to-table requires about eleven minutes to perform the operation, while our approach only needs 25 seconds.
Selective Multi-modal Queries using an Index. In the second scenario, we analyze the runtime of simple queries that only need to scan a subset of a text collection using a filter operation on one of the extracted attributes. However, if implemented naively, this query results in a costly operation since independent of the selectivity all texts need to be first transformed to tuples in order to judge which texts qualify for the filter condition. As a dataset, we again use rotowire. For the queries, we execute different types of queries that can make use of an index on text attributes as introduced in Section 6: (1) queries with only multi-modal scan, (2) queries that join a general information table with a text collection of game reports, (3) queries which union a statistics table with the game reports. All these queries have a filter with different selectivity on an attribute extracted from the text collection (e.g. number of scored points equals a specified value).</p>
<p>Figure 11 shows the runtime with different filter conditions on differed MMOps, resulting in different amounts of selected texts. We see that for queries with low selectivity, MMDB is again able to compute the query results in the order of a couple of seconds. The naive solution of using text-to-table to translate all texts into tables first and then doing the selection afterwards takes much longer. Moreover, in addition to the query runtime, we show the index creation time which shows that the construction of our index is more efficient than a full scan since it only extracts the values for one column from the texts and does not require the decoding of the full text into a table. Hence, even if we include the time of building the index, which is usually done offline, we are still an order of magnitude faster than the naive implementation.
Materialized Views. While selective queries can be executed in MMDB efficiently as shown before, queries that need to read over large fractions of a text collection will still not be able to be executed efficiently. For this reason, we introduce multi-modal materialized</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 12: Time to construct a materialized view on rotowire using MMDB and text-to-table. Text-to-table uses a generative decoder, and thus always needs above 15 minutes for materialized view creation.
views that allow doing the heavy lifting of text processing in an offline phase. A crucial aspect, however, is still the efforts needed to construct materialized views. In this experiment, we thus compare the cost of constructing materialized views using the queries that are based on our MMDB-Model with an approach that uses text-totable for the view creation.</p>
<p>For the experiment, we construct a total of six materialized views. For the first two materialized views, we join the entire text collection of rotowire game reports with the general information tables of the teams (1st view) and players (2nd view). For the next two views, we union the game reports with the statistics tables for players and teams from the rotowire training set. For the last two views, we transform all game reports to additional statistics tables about players and teams extracted from the texts using multi-modal scans.</p>
<p>As we see in Figure 12, using MMDB is much more efficient in creating a materialized view compared to text-to-table. The main reason is that text-to-table uses a generative decoder, which decodes texts into tables by outputting one token at a time in an autoregressive manner. Due to the use of self-attention in the decoder, this process is quadratic in the number of output tokens. Our decoder, however, is based on shallow neural networks, which are able to decode entire rows in one model pass. Hence, the number of passes through the model is determined by the number of rows to be decoded, which is usually much lower than the number of tokens necessary to describe a table. As shown in Figure 12, we can see that MMDB is especially effective for creating a view for the full team table from the game reports of the rotowire dataset. While text-to-table needs over 15 minutes to construct any materialized view on the dataset, MMDB can easily construct such a view in under a minute.</p>
<h3>7.3 Exp. 3: Ablation Study for Pre-Training</h3>
<p>In the final set of experiments, we show the effect of our pre-training objectives by comparing it to other existing pre-training procedures and by individually disabling our pre-training objectives.
Comparison with other Pre-trained Models. The pre-training procedure proposed in this paper aims at teaching the model the necessary skills it needs for performing MMOps. To examine if this pre-training procedure results in better extractions compared to more traditional pre-training procedures, we compare the quality of query results when using MMDB initialized with different pretrained weights. We compare against two pre-trained procedures: the pre-training procedure of BERT [7] and the pre-training procedure of TaBERT [44]. The BERT model aims at natural language
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 13: Comparison of the query results quality when using MMDB with different pre-trained weights. The pretrained weights resulting from our pre-training procedure result in better extractions, especially when only limited fine-tuning data is available.
understanding and is thus pre-trained on plain-text only, meaning it has never seen any tabular data before. Different from BERT, TaBERT's pre-training aims to improve the performance of BERT on tasks like text-to-SQL, where also understanding of tabular data is necessary. Similar to MMDB, TaBERT is thus also pre-trained on a parallel corpus of texts and tables scraped from the web, and thus already has seen lots of tables during pre-training.</p>
<p>However, in the parallel corpus of TaBERT, texts and tables are often only vaguely related. Moreover, the pre-training objectives are not designed for table extraction from text and thus are not ideal for enabling MMOps. Figure 13 shows how the different pre-training objectives and corpora used, affect the quality of query results when varying the number of samples for fine-tuning. Especially in zero-shot or few-shot scenarios with only limited fine-tuning data, our model is consistently able to extract tabular data from text more accurately. However, surprisingly even with the full rotowire training set, there is still an advantage of up to $2.5 \%$ when using our pre-training procedure compared to BERT and TaBERT. Comparing TaBERT to BERT, we see that TaBERT is able to consistently outperform BERT, which is due to the fact that it has already seen tabular data during pre-training.</p>
<p>Effect of Pre-training Objectives. In this experiment, we analyze the efficiency of each individual pre-training objective of our MMDB-Model. Our pre-training procedure consists of three main pre-training objectives, each targeting different skills for table extraction from text. To show the efficiency of the pre-training objectives, we first disable the Column-Text-Alignment (CTA) pretraining objective. To still be able to perform all database operations with CTA disabled, we use the span-detect head in all cases where the column-detect head has previously been used (i.e. in the first phase of the algorithm to compute complex operators). Table 1 shows the consistently positive effect of enabling CTA. The reason for this is that the span-detect head, as it is pre-trained using the Masked Cell Reconstruction (MCR) objective, tends to only extract values for single entities from texts. This is a problem for complex texts where multiple rows need to be extracted, since in the first phase, values for multiple entities need to be extracted.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">dataset</th>
<th style="text-align: center;">enabling CTA</th>
<th style="text-align: center;">enabling DD</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">nobel</td>
<td style="text-align: center;">$+4.8 \%$</td>
<td style="text-align: center;">no improvement</td>
</tr>
<tr>
<td style="text-align: center;">skyscrapers</td>
<td style="text-align: center;">$+8.2 \%$</td>
<td style="text-align: center;">no improvement</td>
</tr>
<tr>
<td style="text-align: center;">countries</td>
<td style="text-align: center;">$+6.9 \%$</td>
<td style="text-align: center;">$+13 \%$</td>
</tr>
</tbody>
</table>
<p>Table 1: Zero-shot F1 improvement of enabling the Column-Text-Alignment (CTA) and Duplicate Detection (DD) pretraining objectives on all three domains of T-REx.</p>
<p>Next, we disable the Duplicate-Detect (DD) objective and use simple string matching to detect duplicates when it is disabled instead. The results are shown in Table 13 as well. Here we see a significant improvement in the countries dataset, where texts often contain the same name of a country, but in different surface forms (e.g. USA and United States). However, for the other two domains, duplicates that differ in the surface forms seem to be rare and string matching seems to be good enough for detecting them.</p>
<p>We do not disable the MCR objective, because it is impossible compute database operators without the span-detect head.</p>
<h2>8 RELATED WORK</h2>
<p>In the following, we discuss different lines of related work.
Multi-Modal Data Systems. There are only a few systems that allow database operations over text or other additional modalities.</p>
<p>A first system, OpineDB [22] links subjective texts (e.g. user reviews) to relational data and extends SQL by natural language conditions to select relational data based on texts. However, OpineDB differs from our work in that texts are only used for selection, but it does not expose text data as tables for query processing as we do.</p>
<p>Another system is NeuralDB [38]. NeuralDB is a database that uses pre-trained language models to run natural language queries directly on text documents. Different from MMDB, NeuralDB works only on very short textual statements of a few words. Moreover, NeuralDB does not consider the multi-modal case where tabular data is available in addition to texts.</p>
<p>Furthermore, another system is WannaDB [11], which similar to NeuralDB allows the execution of SQL queries over text collections. For extracting structured data from text, it uses an interactive matching process based on human feedback. Unlike our work, however, it again is not multi-modal and has further restrictions such as it assumes that each text document adds only a single row to the output table (i.e. no support for Multi-Row-Texts) and it also does not support more complex operations such as joins and unions of tables and texts.</p>
<p>Recently, Chen et al. [3] presented their vision of multi-modal datalakes, Symphony, which can be queried using natural language queries. The setting in multi-modal datalakes is different from databases, since the main concern is retrieving data from multi-modal datasets. For retrieval, they propose a self-supervised information compression pre-training objective to embed multiple modalities in the same latent space and retrieve based on similarity in this latent space. After retrieval, they use different approaches to answer the user query depending on the modality. For text data, their model is based on NeuralDB.
Pre-training Models. Large pre-trained language models [1, 7, 24, 31] are by now dominating NLP and are quickly adapted for multimodal $[21,25,36,37]$ and tabular data $[6,14,41]$. To alleviate low overhead adaption to downstream tasks, pre-training objectives
began to be more aligned with the downstream task for many coreNLP [9, 16, 19, 34] and also structure-aware tasks [23, 29, 35, 45].</p>
<p>Most similar to our pre-training objectives are those pre-training procedures that rely on weak or distant supervision to align pretraining more to the downstream task. ReasonBERT [5] uses a pre-training objective inspired by distant supervision for the downstream task of multi-hop hybrid question answering. StruG's [4] pre-training dataset designed for the text-to-SQL task is based on the table-to-text generation dataset ToTTo [30] that was extracted from Wikipedia using heuristics. ToTTo contains 120.000 training samples and is thus much smaller than our pre-training dataset.
Extraction of Tabular Data from Text. Text-to-table [43] is a sequence-to-sequence model trained to transform tables into text. It introduces several model adjustments to ensure that the model outputs a correctly structured table. Pietruszka et al. [33] argue that in some cases decoding from left to right and top to bottom is not optimal and thus propose STable, a model similar to text-to-table, which is able to output table cells in arbitrary order. Unlike our work, text-to-table and STable are trained in a supervised manner from scratch for every new dataset.
Cross-Domain Slot Filling. Slot filling is used in task-oriented dialog systems to fill a set of slots (e.g. depart, arrive) from a natural language utterance (i.e., texts). This is similar to basic database operations which extract rows from text, where values for a set of column names have to be extracted from text. Some recent works also use pre-training for slot filling while mostly relying on conversational data from Reddit [12, 26]. Different from our setting, the texts tend to be rather short and limited in variability. Moreover, again complex multi-modal operations such as joins and or unions are not supported by these approaches.</p>
<h2>9 CONCLUSIONS AND FUTURE WORK</h2>
<p>In this paper, we presented multi-modal databases, which is a new class of databases that allow users to seamlessly query textual and tabular data. The cornerstone of multi-modal databases are multimodal database operators, which we have shown to be realizable using large pre-trained language models. As a result, multi-modal database operators can be executed on new datasets from unseen domains in a zero-shot or few-shot manner with no or only minimal fine-tuning overhead. In the future, we also want to explore how other modalities like images could be incorporated in our database to enable database operations for even more modalities.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>We thank the reviewers for their feedback. This research is funded by the Hochtief project AICO (AI in Construction), by the BMBF and the state of Hesse as part of the NHR Program, as well as the HMWK cluster project 3AI (The Third Wave of AI). Finally, we want to thank hessian.AI at TU Darmstadt as well as DFKI Darmstadt.</p>
<h2>REFERENCES</h2>
<p>[1] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.). https://proceedings.neurips.cc/paper/2020/ hash/1457c0dbbfcb4967418bfb8ac142f64a-Abstract.html
[2] Michael J. Cafarella, Alon Y. Halevy, Daisy Zhe Wang, Eugene Wu, and Yang Zhang. 2008. WebTables: exploring the power of tables on the web. Proc. VLDB Endow. 1, 1 (2008), 538-549. https://doi.org/10.14778/1453856.1453916
[3] Zui Chen, Zihui Gu, Lei Cao, Ju Fan, Sam Madden, and Nan Tang. [n.d.]. Symphony: Towards Natural Language Query Answering over Multi-modal Data Lakes. ([n.d.]).
[4] Xiang Deng, Ahmed Hassan Awadallah, Christopher Meek, Oleksandr Polozov, Huan Sun, and Matthew Richardson. 2021. Structure-Grounded Pretraining for Text-to-SQL. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tin, Iz Beltagy, Steven Bethard, Ryan Cutterell, Tannoy Chakraborty, and Yichao Zhou (Eds.). Association for Computational Linguistics, 1337-1350. https://doi.org/10.18653/v1/2021.naacl-main. 105
[5] Xiang Deng, Yu Su, Alyssa Lees, You Wu, Cong Yu, and Huan Sun. 2021. ReassnBERT: Pre-trained to Reassn with Distant Supervision. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Ponta Cana, Dominican Republic, 7-11 November, 2021, Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wentau Yih (Eds.). Association for Computational Linguistics, 6112-6127. https: //doi.org/10.18653/v1/2021.emnlp-main. 494
[6] Xiang Deng, Huan Sun, Alyssa Lees, You Wu, and Cong Yu. 2022. TURL: Table Understanding through Representation Learning. SIGMOD Rec. 51, 1 (2022), 55-40. https://doi.org/10.1145/3542700.3542709
[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computational Linguistics, 4171-4186. https://doi.org/10.18653/v1/n19-1423
[8] Hady Elbahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathan S. Hare, Frdrique Laforest, and Elena Simperl. 2018. T-REs: A Large Scale Alignment of Natural Language with Knowledge Base Triples. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation, LREC 2018, Miyazaki, Japan, May 7-12, 2018, Nicoletta Calzolari, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Kiti Hasida, Hitoshi Isahara, Bente Muegaard, Joseph Mariani, Hlne Mazo, Asuncin Moreno, Jan Odijk, Stelios Piperalis, and Takenobu Tokunaga (Eds.). European Language Resources Association (ELRA). http://www.lrec-conf.org/proceedings/lrec2018/summaries/ 632.html
[9] Michael R. Glass, Alfio Gliozzo, Rishav Chakravarti, Anthony Ferritto, Lin Pan, G. P. Shrivatsa Bhargav, Dinesh Garg, and Avinip Sil. 2020. Span Selection Pretraining for Question Answering. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, Dan Juratsky, Joyce Chai, Natalie Schlter, and Joel R. Tetreault (Eds.). Association for Computational Linguistics, 2773-2782. https://doi.org/10.18653/v1/2020.aclmain. 247
[10] James R. Hamilton and Tapas K. Nayak. 2001. Microsoft SQL Server Full-Text Search. IEEE Data Eng. Bull. 24, 4 (2001), 7-10. http://sites.computer.org/debull/ A01DEC-CD.pdf
[11] Benjamin Httasch, Jan-Micha Bodensohn, Liane Vogel, Matthias Urban, and Carsten Binnig. 2023. WannaDB: Ad-hoc SQL Queries over Text Collections. In Datenbanksysteme fr Business, Technologie und Web (BTW 2023), 20. Fachtagung des Gf-Fachbereichs Datenbanken und Informationssysteme" (DBIS), 06-10, Mrz 2023, Dresden, Germany, Proceedings (LNI, Vol. P-331), Birgitta Knig-Ries, Stefanie Scherzinger, Wolfgang Lehner, and Gottfried Vossen (Eds.). Gesellschaft fr Informatik e.V., 157-181. https://doi.org/10.18420/BTW2023-08
[12] Matthew Henderson and Ivan Vulic. 2021. ConVEx: Data-Efficient and Few-Shot Slot Labeling. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tin, Iz Beltagy, Steven Bethard, Ryan Cutterell, Tannoy Chakraborty, and Yichao Zhou (Eds.). Association for Computational Linguistics, 3375-3389. https://doi.org/10.18653/v1/2021.naacl-main. 264
[13] Madelon Hulsebos, agatay Demiralp, and Paul Groth. 2021. GitTables: A LargeScale Corpus of Relational Tables. CoRR abs/2106.07258 (2021). arXiv:2106.07258 https://arxiv.org/abs/2106.07258
[14] Hiroshi Iida, Dung Thai, Varun Manjunatha, and Mohit Iyyer. 2021. TABBIE: Pretrained Representations of Tabular Data. In Proceedings of NAACL-HLT 2021. Association for Computational Linguistics, 3446-3456.
[15] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea Madotto, and Pascale Fung. 2022. Survey of Hallucination in Natural Language Generation. CoRR abs/2202.03629 (2022). arXiv:2202.03629 https://arxiv.org/abs/2202.03629
[16] Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. 2020. SpanBERT: Improving Pre-training by Representing and Predicting Spans. Trans. Assoc. Comput. Linguistics 8 (2020), 64-77. https: //doi.org/10.1162/tacl_a_00300
[17] Rmi Lebret, David Grangier, and Michael Auli. 2016. Neural Text Generation from Structured Data with Application to the Biography Domain. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, Jian Su, Xavier Carreras, and Kevin Duh (Eds.). The Association for Computational Linguistics, 1203-1213. https://doi.org/10.18653/v1/d16-1128
[18] Kenton Lee, Luheng He, Mike Lewis, and Luke Zettlemoyer. 2017. End-to-end Neural Conference Resolution. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Copenhagen, Denmark, 188-197. https://doi.org/10.18653/v1/D17-1018
[19] Mike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Armen Aghajanyan, Sida Wang, and Luke Zettlemoyer. 2020. Pre-training via Paraphrasing. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.). https://proceedings.neurips.cc/paper/2020/hash/ d6f1dd034aabde7657e6680444ceff62-Abstract.html
[20] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Vei Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461 (2019).
[21] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. 2019. VisualBERT: A Simple and Performant Baseline for Vision and Language. CoRR abs/1908.03557 (2019). arXiv:1908.03557 http://arxiv.org/abs/1908.03557
[22] Yuliang Li, Aaron Feng, Jinfeng Li, Saran Mumick, Alon Y. Halevy, Vivian Li, and Wang-Chiave Tan. 2019. Subjective Databases. Proc. VLDB Endow. 12, 11 (2019), 1330-1343. https://doi.org/10.14778/3342263.3342271
[23] Qian Liu, Bei Chen, Jiaqi Guo, Morreza Ziyadi, Zeqi Lin, Weizhu Chen, and Jian-Guang Lou. 2022. TAPEX: Table Pre-training via Learning a Neural SQL Executor. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. https://openreview.net/ forum?id=O50443AsCP
[24] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Vesslin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. CoRR abs/1907.11692 (2019). arXiv:1907.11692 http://arxiv.org/abs/1907.11692
[25] Jiaxen Lu, Dhyan Batra, Devi Parikh, and Stefan Lee. 2019. ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-andLanguage Tasks. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch-Buc, Emily B. Fox, and Roman Garnett (Eds.). 13-23. https://proceedings.neurips.cc/paper/2019/hash/ c74d97b01eae257e44aa9d5bade97baf-Abstract.html
[26] Shikib Mehtri and Maxine Eskenazi. 2021. GenSP: Simultaneous Adaptation of Generative Pre-trained Models and Slot Filling. In Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, SIGdial 2021, Singapore and Online, July 29-31, 2021, Haizhou Li, Gina-Anne Levow, Zhou Yu, Chitralekha Gupta, Berrak Sisman, Siqi Cai, David Vandyke, Nina Dethlefs, Yan Wu, and Junyi Jeow Li (Eds.). Association for Computational Linguistics, 489-498. https://aclantbology.org/2021.sigdial-1.51
[27] Avanika Narayan, Ines Chami, Laurel J. Orr, and Christopher R. 2022. Can Foundation Models Wrangle Your Data? Proc. VLDB Endow. 16, 4 (2022), 758746 .
[28] Jekaterina Novikova, Ondrej Dusek, and Verena Rieser. 2017. The E2E Dataset: New Challenges For End-to-End Generation. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, Saarbrcken, Germany, August 15-17, 2017, Kristiina Jokinen, Manfred Stede, David DeVault, and Annie Louis (Eds.). Association for Computational Linguistics, 201-206. https://doi.org/10.18653/ v1/w17-5525
[29] Liangming Pan, Wenhu Chen, Wenhan Xiong, Min-Yen Kan, and William Yang Wang. 2021. Unsupervised Multi-hop Question Answering by Question Generators. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</p>
<p>NAACL-HLT 2021, Online, June 6-11, 2021, Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tiiz, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (Eds.). Association for Computational Linguistics, 5866-5880. https://doi.org/10.18653/v1/2021.naacl-main. 469
[30] Ankur P. Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Shuwan Dhingra, Diyi Yang, and Dipanjan Das. 2020. ToTTo: A Controlled Table-ToText Generation Dataset. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, 1173-1186. https://doi.org/10.18653/v1/2020.emnlpmain. 89
[31] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep Contextualized Word Representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), Marilyn A. Walker, Heng Ji, and Amanda Stent (Eds.). Association for Computational Linguistics, 2227-2237. https://doi.org/10.18653/v1/n18-1202
[32] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick S. H. Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktschel, and Sebastian Riedel. 2021. KILT: a Benchmark for Knowledge Intensive Language Tasks. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tiiz, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (Eds.). Association for Computational Linguistics, 2523-2544. https://doi.org/10.18653/v1/2021.naacl-main. 200
[33] Michal Pietruszka, Michal Turski, Lukasz Borchmann, Tomasz Dwojak, Gabriela Palka, Karolina Szyndler, Dawid Jurkiewicz, and Lukasz Garncarek. 2022. STable: Table Generation Framework for Encoder-Decoder Models. CoRR abs/2206.04045 (2022). https://doi.org/10.48550/arXiv.2206.04045 arXiv:2206.04045
[34] Ozi Ram, Yuval Kirstain, Jonathan Berant, Amir Globerson, and Omer Levy. 2021. Few-Shot Question Answering by Pretraining Span Selection. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (Eds.). Association for Computational Linguistics, 3066-3079. https://doi.org/10.18653/v1/2021.acl-long. 239
[35] Peng Shi, Patrick Ng, Zhiguo Wang, Henghui Zhu, Alexander Hanbo Li, Jun Wang, Cicero Nogueira dos Santos, and Bing Xiang. 2021. Learning Contextual Representations for Semantic Parsing with Generation-Augmented Pre-Training. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021. AAAI Press, 13806-13814. https://ojs.aaai.org/ index.php/AAAI/article/view/17627
[36] Wenjie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. 2020. VL-BERT: Pre-training of Generic Visual-Linguistic Representations. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. https://openreview.net/forum?id= SygXPaEYvH
[37] Hao Tan and Mohit Bansal. 2019. LXMERT: Learning Cross-Modality Encoder Representations from Transformers. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong,</p>
<p>China, November 3-7, 2019, Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (Eds.). Association for Computational Linguistics, 5099-5110. https://doi. org/10.18653/v1/D19-1514
[38] James Thorne, Majid Yazdani, Marzieh Saeidi, Fabrizio Silvestri, Sebastian Riedel, and Alon Y. Halevy. 2021. Database reasoning over text. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (Eds.). Association for Computational Linguistics, 3091-3104. https://doi.org/10.18653/v1/2021.acl-long. 241
[39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 49, 2017, Long Beach, CA, USA, Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (Eds.). 5998-6008. https://proceedings.neurips.cc/paper/2017/hash/ 3f5ee243547dee91fbd053c1c4a845aa-Abstract.html
[40] David Vos, Till Dhmen, and Sebastian Schelter. 2022. Towards ParameterEfficient Automation of Data Wrangling Tasks with Prefix-Tuning. In NeurIPS 2022 First Table Representation Workshop.
[41] Zhiruo Wang, Haoyu Dong, Ran Jia, Jia Li, Zhiyi Fu, Shi Han, and Dongmei Zhang. 2021. TUTA: Tree-based Transformers for Generally Structured Table Pre-training. In KDD '21: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, Singapore, August 14-18, 2021, Feida Zhu, Beng Chin Ooi, and Chunyan Miao (Eds.). ACM, 1780-1790. https: //doi.org/10.1145/3447548.3467434
[42] Sam Wiseman, Stuart M. Shieber, and Alexander M. Rush. 2017. Challenges in Data-to-Document Generation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark, September 9-11, 2017, Martha Palmer, Rebecca Hwa, and Sebastian Riedel (Eds.). Association for Computational Linguistics, 2253-2263. https://doi.org/10.18653/ v1/d17-1239
[43] Xueqing Wu, Jiacheng Zhang, and Hang Li. 2022. Text-to-Table: A New Way of Information Extraction. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (Eds.). Association for Computational Linguistics, 2518-2533. https: //doi.org/10.18653/v1/d17-1239
[44] Pengcheng Yin, Graham Neubig, Wen-tau Yih, and Sebastian Riedel. 2020. TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 3-10, 2020, Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (Eds.). Association for Computational Linguistics, 8413-8426. https://doi.org/10.18653/v1/2020.acl-main. 745
[45] Tao Yu, Chien-Sheng Wu, Xi Victoria Lin, Bailin Wang, Yi Chern Tan, Xinyi Yang, Dragomir R. Radev, Richard Socher, and Caiming Xiong. 2021. GraPPA: Grammar-Augmented Pre-Training for Table Semantic Parsing. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. https://openreview.net/forum?id=kyaleYj4xZ
[46] Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books. In 2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015. IEEE Computer Society, 19-27. https://doi.org/10.1109/ ICCV.2015.11</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/dgx-a100/dgxa100-system-architecture-white-paper.pdf&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
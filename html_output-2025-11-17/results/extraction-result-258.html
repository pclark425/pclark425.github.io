<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-258 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-258</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-258</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-251406206</p>
                <p><strong>Paper Title:</strong> Evaluating Transformer Language Models on Arithmetic Operations Using Number Decomposition</p>
                <p><strong>Paper Abstract:</strong> In recent years, Large Language Models such as GPT-3 showed remarkable capabilities in performing NLP tasks in the zero and few shot settings. On the other hand, the experiments highlighted the difficulty of GPT-3 in carrying out tasks that require a certain degree of reasoning, such as arithmetic operations. In this paper we evaluate the ability of Transformer Language Models to perform arithmetic operations following a pipeline that, before performing computations, decomposes numbers in units, tens, and so on. We denote the models fine-tuned with this pipeline with the name Calculon and we test them in the task of performing additions, subtractions and multiplications on the same test sets of GPT-3. Results show an increase of accuracy of 63% in the five-digit addition task. Moreover, we demonstrate the importance of the decomposition pipeline introduced, since fine-tuning the same Language Model without decomposing numbers results in 0% accuracy in the five-digit addition task.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e258.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e258.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Calculon</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Calculon (GPT-2 fine-tuned with number decomposition pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-2 Small model fine-tuned to perform arithmetic by representing numbers in natural-language decomposed form (units, tens, hundreds, ...) and training the model to sum/subtract/multiply those decompositions end-to-end.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>117M</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication (tested: additions and subtractions for 2-,3-,4-,5-digit; multiplication for 2-digit)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>2-digit (including 1-digit), 3-digit, 4-digit, 5-digit for addition/subtraction; 2-digit × 2-digit for multiplication</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>supervised fine-tuning with decomposition pipeline: training examples present (in natural language) translates number to decomposition, performs digit-place operations, then translates decomposition back to numeric result; greedy decoding at inference</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Calculon achieved high accuracy on addition and subtraction across tested digit-lengths (noting a reported increase of 63% accuracy on the five-digit addition task relative to baseline). Calculon performed poorly on the 2-digit multiplication task (described as an exception where accuracy remained low). Exact per-task percentages beyond the quoted statistics are not enumerated in the paper's body text.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Gradient-based input-saliency analysis (Shrikumar et al. method) shows the tokens with highest influence for an output digit are the input digits of the same place-value (units influence output units, tens influence output tens, etc.), indicating the model learns to align and operate on corresponding digit positions rather than treat numbers holistically.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Performance degrades with increasing number of digits for baseline models, but decomposition substantially mitigates this degradation for addition/subtraction; multiplication remains a severe bottleneck even at low digit counts.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Fails on multiplication (2-digit multiplication remains poorly solved); when decomposition is absent, model fails (zero or near-zero accuracy) on 4- and 5-digit addition/subtraction; errors concentrated on higher-digit problems without decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against (a) baseline fine-tuning without decomposition and (b) a 'spaced' pipeline that spaces digits to encourage digit-level tokenization. Calculon outperforms both, with decomposition providing the largest gains.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Providing number decomposition (place-value annotations) during fine-tuning enables a 117M-parameter GPT-2 to learn place-wise addition and subtraction that generalizes to unseen numbers up to five digits; decomposition alone does not suffice for multiplication.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e258.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e258.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2 Baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2 fine-tuned without number decomposition (baseline approach)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-2 Small model fine-tuned on arithmetic tasks where inputs are raw numbers and the model must directly generate the numeric result (no decomposition or digit spacing).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (fine-tuned baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>117M</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication (same task set as other experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>2-, 3-, 4-, 5-digit for addition/subtraction; 2-digit multiplication</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>supervised fine-tuning on raw numeric strings (no decomposition); greedy decoding</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Very poor on most tasks except two-digit addition where it achieved 53.35% accuracy; for 4- and 5-digit addition and subtraction it achieved zero or near-zero accuracy (explicitly reported as zero or near-zero in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Performance collapses as digit count increases beyond 2–3 digits (near-zero accuracy on 4–5 digits).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Unable to generalize arithmetic rules for higher-digit numbers; essentially fails to learn place-wise computation without explicit decomposition (breaks down on carries and higher-place interactions).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Direct contrast to Calculon (decomposition) and Spaced pipelines; shows large gap demonstrating importance of decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Fine-tuning GPT-2 on raw numeric strings fails to produce generalizable arithmetic competence on 4–5 digit problems, evidencing that naive fine-tuning without place-value information is insufficient.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e258.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e258.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Spaced pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spaced pipeline (digit spacing to encourage digit-level tokenization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method where digits of each number are separated by spaces (e.g., '868' → '8 6 8') so the BPE tokenizer is more likely to produce single-digit tokens; trained similarly to other approaches but without explicit place-value labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (fine-tuned with spaced digits)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>117M</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication (same tasks evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>2-, 3-, 4-, 5-digit for addition/subtraction; 2-digit multiplication</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>supervised fine-tuning with digit-level spacing (no magnitude/place labels); aims to exploit tokenizer granularity</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Spaced pipeline improves accuracy substantially over the baseline for addition tasks and yields small improvements in subtraction; however, Calculon (decomposition) still outperforms Spaced, with an additional ~15% gain on 5-digit addition and an additional ~75% gain on one of the 5-digit tasks as reported (paper text: 'accuracy gain of almost 15% in the 5D+ task and 75% in the 5D- task'). Exact absolute accuracy numbers for Spaced are not tabulated in the paper body.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Improvement suggests part of baseline failure is due to subword tokenization splitting numbers arbitrarily; spacing produces more digit-local representations but lacks magnitude/place annotations needed for best performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Better than baseline as digits increase, but still inferior to decomposition; spacing partially mitigates tokenization-related scaling issues.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Without explicit place-value labels, the model still struggles more than with decomposition; multiplication remains poorly solved.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to baseline and decomposition; sits between baseline and full decomposition in performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Token-level digit separation helps but is insufficient: indicating magnitude/place-value information yields substantially larger gains than merely ensuring digit-level tokenization.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e258.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e258.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (few-shot baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (175B) evaluated in few-shot setting without decomposition in prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The large GPT-3 model (as reported by Brown et al., 2020) evaluated on the same arithmetic test sets in a few-shot prompting regime; used as a benchmark and comparison point in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication (same test sets as Brown et al.: 2–5 digit add/sub, 2-digit mult)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>2-,3-,4-,5-digit for addition/subtraction; 2-digit multiplication</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>few-shot prompting (no parameter updates); random sampling decoding with temperature=0.7 for this paper's eval</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>As reported in Brown et al. (cited), GPT-3 achieves high accuracy on 2- and 3-digit addition/subtraction and poor accuracy on 4- and 5-digit tasks; in this paper GPT-3 few-shot baseline (taken from Brown et al.) is used as reference. The paper does not reproduce full per-task percentages but references those GPT-3 few-shot results.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Performance degrades with number of digits; larger model shows strong performance on lower-digit tasks but not on higher-digit generalization in few-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Struggles on 4–5 digit arithmetic despite large scale; multiplication also challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared (in discussion) to GPT-2 fine-tuned models (Calculon etc.) and to GPT-3 few-shot with decomposition prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>GPT-3 in few-shot prompting performs well for small-digit arithmetic but fails to generalize to 4–5 digit problems; fine-tuning with decomposition (on smaller models) can outperform GPT-3 few-shot for those higher-digit tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e258.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e258.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 FS decomp</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 few-shot with decomposition examples in prompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of GPT-3 using the decomposition pipeline presented in this paper but provided only inside the few-shot prompt examples (no parameter updates).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication (same as other GPT-3 tests)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>2-,3-,4-,5-digit for addition/subtraction; 2-digit multiplication (evaluated on first 100 test observations due to API limits)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>few-shot prompting where the few-shot examples are written following the decomposition pipeline (longer prompts demonstrating decomposition), random sampling temperature=0.7</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Using decomposition in few-shot prompts led to much lower results than the original GPT-3 few-shot reported by Brown et al.; paper reports that GPT-3 FS decomp performs worse, hypothesizing the long decomposition prompts reduce focus on arithmetic examples. Exact numeric accuracies for this condition are reported only over the first 100 test observations and are not enumerated in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Empirical hypothesis from the authors: long decomposition-based few-shot prompts may distract GPT-3 from leveraging pretraining-learned calculation heuristics, indicating prompt length/form can negatively affect few-shot arithmetic performance for very large LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Does not improve GPT-3 few-shot performance; in fact, decomposition examples in the prompt degraded performance compared to standard few-shot examples.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Long, decomposed few-shot prompts appear to reduce performance (possible loss of focus on concise demonstrations); decomposition that benefits supervised fine-tuning does not straightforwardly transfer to few-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared directly against the original GPT-3 few-shot prompting used in Brown et al.; contrasted with supervised GPT-2 fine-tuning with decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Decomposition is beneficial when used for supervised fine-tuning, but inserting decomposition examples into GPT-3 few-shot prompts harms GPT-3's few-shot arithmetic performance in these experiments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Injecting numerical reasoning skills into language models <em>(Rating: 2)</em></li>
                <li>Do NLP models know numbers? probing numeracy in embeddings <em>(Rating: 2)</em></li>
                <li>Investigating the limitations of transformers with simple arithmetic tasks <em>(Rating: 2)</em></li>
                <li>Analysing mathematical reasoning abilities of neural models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-258",
    "paper_id": "paper-251406206",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "Calculon",
            "name_full": "Calculon (GPT-2 fine-tuned with number decomposition pipeline)",
            "brief_description": "A GPT-2 Small model fine-tuned to perform arithmetic by representing numbers in natural-language decomposed form (units, tens, hundreds, ...) and training the model to sum/subtract/multiply those decompositions end-to-end.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 (fine-tuned)",
            "model_size": "117M",
            "model_architecture": "decoder-only transformer",
            "arithmetic_operation_type": "addition, subtraction, multiplication (tested: additions and subtractions for 2-,3-,4-,5-digit; multiplication for 2-digit)",
            "number_range_or_complexity": "2-digit (including 1-digit), 3-digit, 4-digit, 5-digit for addition/subtraction; 2-digit × 2-digit for multiplication",
            "method_or_intervention": "supervised fine-tuning with decomposition pipeline: training examples present (in natural language) translates number to decomposition, performs digit-place operations, then translates decomposition back to numeric result; greedy decoding at inference",
            "performance_result": "Calculon achieved high accuracy on addition and subtraction across tested digit-lengths (noting a reported increase of 63% accuracy on the five-digit addition task relative to baseline). Calculon performed poorly on the 2-digit multiplication task (described as an exception where accuracy remained low). Exact per-task percentages beyond the quoted statistics are not enumerated in the paper's body text.",
            "mechanistic_insight": "Gradient-based input-saliency analysis (Shrikumar et al. method) shows the tokens with highest influence for an output digit are the input digits of the same place-value (units influence output units, tens influence output tens, etc.), indicating the model learns to align and operate on corresponding digit positions rather than treat numbers holistically.",
            "performance_scaling": "Performance degrades with increasing number of digits for baseline models, but decomposition substantially mitigates this degradation for addition/subtraction; multiplication remains a severe bottleneck even at low digit counts.",
            "failure_modes": "Fails on multiplication (2-digit multiplication remains poorly solved); when decomposition is absent, model fails (zero or near-zero accuracy) on 4- and 5-digit addition/subtraction; errors concentrated on higher-digit problems without decomposition.",
            "comparison_baseline": "Compared against (a) baseline fine-tuning without decomposition and (b) a 'spaced' pipeline that spaces digits to encourage digit-level tokenization. Calculon outperforms both, with decomposition providing the largest gains.",
            "key_finding": "Providing number decomposition (place-value annotations) during fine-tuning enables a 117M-parameter GPT-2 to learn place-wise addition and subtraction that generalizes to unseen numbers up to five digits; decomposition alone does not suffice for multiplication.",
            "uuid": "e258.0"
        },
        {
            "name_short": "GPT-2 Baseline",
            "name_full": "GPT-2 fine-tuned without number decomposition (baseline approach)",
            "brief_description": "A GPT-2 Small model fine-tuned on arithmetic tasks where inputs are raw numbers and the model must directly generate the numeric result (no decomposition or digit spacing).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 (fine-tuned baseline)",
            "model_size": "117M",
            "model_architecture": "decoder-only transformer",
            "arithmetic_operation_type": "addition, subtraction, multiplication (same task set as other experiments)",
            "number_range_or_complexity": "2-, 3-, 4-, 5-digit for addition/subtraction; 2-digit multiplication",
            "method_or_intervention": "supervised fine-tuning on raw numeric strings (no decomposition); greedy decoding",
            "performance_result": "Very poor on most tasks except two-digit addition where it achieved 53.35% accuracy; for 4- and 5-digit addition and subtraction it achieved zero or near-zero accuracy (explicitly reported as zero or near-zero in the paper).",
            "mechanistic_insight": null,
            "performance_scaling": "Performance collapses as digit count increases beyond 2–3 digits (near-zero accuracy on 4–5 digits).",
            "failure_modes": "Unable to generalize arithmetic rules for higher-digit numbers; essentially fails to learn place-wise computation without explicit decomposition (breaks down on carries and higher-place interactions).",
            "comparison_baseline": "Direct contrast to Calculon (decomposition) and Spaced pipelines; shows large gap demonstrating importance of decomposition.",
            "key_finding": "Fine-tuning GPT-2 on raw numeric strings fails to produce generalizable arithmetic competence on 4–5 digit problems, evidencing that naive fine-tuning without place-value information is insufficient.",
            "uuid": "e258.1"
        },
        {
            "name_short": "Spaced pipeline",
            "name_full": "Spaced pipeline (digit spacing to encourage digit-level tokenization)",
            "brief_description": "A method where digits of each number are separated by spaces (e.g., '868' → '8 6 8') so the BPE tokenizer is more likely to produce single-digit tokens; trained similarly to other approaches but without explicit place-value labels.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 (fine-tuned with spaced digits)",
            "model_size": "117M",
            "model_architecture": "decoder-only transformer",
            "arithmetic_operation_type": "addition, subtraction, multiplication (same tasks evaluated)",
            "number_range_or_complexity": "2-, 3-, 4-, 5-digit for addition/subtraction; 2-digit multiplication",
            "method_or_intervention": "supervised fine-tuning with digit-level spacing (no magnitude/place labels); aims to exploit tokenizer granularity",
            "performance_result": "Spaced pipeline improves accuracy substantially over the baseline for addition tasks and yields small improvements in subtraction; however, Calculon (decomposition) still outperforms Spaced, with an additional ~15% gain on 5-digit addition and an additional ~75% gain on one of the 5-digit tasks as reported (paper text: 'accuracy gain of almost 15% in the 5D+ task and 75% in the 5D- task'). Exact absolute accuracy numbers for Spaced are not tabulated in the paper body.",
            "mechanistic_insight": "Improvement suggests part of baseline failure is due to subword tokenization splitting numbers arbitrarily; spacing produces more digit-local representations but lacks magnitude/place annotations needed for best performance.",
            "performance_scaling": "Better than baseline as digits increase, but still inferior to decomposition; spacing partially mitigates tokenization-related scaling issues.",
            "failure_modes": "Without explicit place-value labels, the model still struggles more than with decomposition; multiplication remains poorly solved.",
            "comparison_baseline": "Compared to baseline and decomposition; sits between baseline and full decomposition in performance.",
            "key_finding": "Token-level digit separation helps but is insufficient: indicating magnitude/place-value information yields substantially larger gains than merely ensuring digit-level tokenization.",
            "uuid": "e258.2"
        },
        {
            "name_short": "GPT-3 (few-shot baseline)",
            "name_full": "GPT-3 (175B) evaluated in few-shot setting without decomposition in prompts",
            "brief_description": "The large GPT-3 model (as reported by Brown et al., 2020) evaluated on the same arithmetic test sets in a few-shot prompting regime; used as a benchmark and comparison point in this paper.",
            "citation_title": "Language models are few-shot learners",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_size": "175B",
            "model_architecture": "decoder-only transformer",
            "arithmetic_operation_type": "addition, subtraction, multiplication (same test sets as Brown et al.: 2–5 digit add/sub, 2-digit mult)",
            "number_range_or_complexity": "2-,3-,4-,5-digit for addition/subtraction; 2-digit multiplication",
            "method_or_intervention": "few-shot prompting (no parameter updates); random sampling decoding with temperature=0.7 for this paper's eval",
            "performance_result": "As reported in Brown et al. (cited), GPT-3 achieves high accuracy on 2- and 3-digit addition/subtraction and poor accuracy on 4- and 5-digit tasks; in this paper GPT-3 few-shot baseline (taken from Brown et al.) is used as reference. The paper does not reproduce full per-task percentages but references those GPT-3 few-shot results.",
            "mechanistic_insight": null,
            "performance_scaling": "Performance degrades with number of digits; larger model shows strong performance on lower-digit tasks but not on higher-digit generalization in few-shot prompting.",
            "failure_modes": "Struggles on 4–5 digit arithmetic despite large scale; multiplication also challenging.",
            "comparison_baseline": "Compared (in discussion) to GPT-2 fine-tuned models (Calculon etc.) and to GPT-3 few-shot with decomposition prompts.",
            "key_finding": "GPT-3 in few-shot prompting performs well for small-digit arithmetic but fails to generalize to 4–5 digit problems; fine-tuning with decomposition (on smaller models) can outperform GPT-3 few-shot for those higher-digit tasks.",
            "uuid": "e258.3"
        },
        {
            "name_short": "GPT-3 FS decomp",
            "name_full": "GPT-3 few-shot with decomposition examples in prompt",
            "brief_description": "Evaluation of GPT-3 using the decomposition pipeline presented in this paper but provided only inside the few-shot prompt examples (no parameter updates).",
            "citation_title": "Language models are few-shot learners",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_size": "175B",
            "model_architecture": "decoder-only transformer",
            "arithmetic_operation_type": "addition, subtraction, multiplication (same as other GPT-3 tests)",
            "number_range_or_complexity": "2-,3-,4-,5-digit for addition/subtraction; 2-digit multiplication (evaluated on first 100 test observations due to API limits)",
            "method_or_intervention": "few-shot prompting where the few-shot examples are written following the decomposition pipeline (longer prompts demonstrating decomposition), random sampling temperature=0.7",
            "performance_result": "Using decomposition in few-shot prompts led to much lower results than the original GPT-3 few-shot reported by Brown et al.; paper reports that GPT-3 FS decomp performs worse, hypothesizing the long decomposition prompts reduce focus on arithmetic examples. Exact numeric accuracies for this condition are reported only over the first 100 test observations and are not enumerated in the main text.",
            "mechanistic_insight": "Empirical hypothesis from the authors: long decomposition-based few-shot prompts may distract GPT-3 from leveraging pretraining-learned calculation heuristics, indicating prompt length/form can negatively affect few-shot arithmetic performance for very large LMs.",
            "performance_scaling": "Does not improve GPT-3 few-shot performance; in fact, decomposition examples in the prompt degraded performance compared to standard few-shot examples.",
            "failure_modes": "Long, decomposed few-shot prompts appear to reduce performance (possible loss of focus on concise demonstrations); decomposition that benefits supervised fine-tuning does not straightforwardly transfer to few-shot prompting.",
            "comparison_baseline": "Compared directly against the original GPT-3 few-shot prompting used in Brown et al.; contrasted with supervised GPT-2 fine-tuning with decomposition.",
            "key_finding": "Decomposition is beneficial when used for supervised fine-tuning, but inserting decomposition examples into GPT-3 few-shot prompts harms GPT-3's few-shot arithmetic performance in these experiments.",
            "uuid": "e258.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Injecting numerical reasoning skills into language models",
            "rating": 2,
            "sanitized_title": "injecting_numerical_reasoning_skills_into_language_models"
        },
        {
            "paper_title": "Do NLP models know numbers? probing numeracy in embeddings",
            "rating": 2,
            "sanitized_title": "do_nlp_models_know_numbers_probing_numeracy_in_embeddings"
        },
        {
            "paper_title": "Investigating the limitations of transformers with simple arithmetic tasks",
            "rating": 2,
            "sanitized_title": "investigating_the_limitations_of_transformers_with_simple_arithmetic_tasks"
        },
        {
            "paper_title": "Analysing mathematical reasoning abilities of neural models",
            "rating": 1,
            "sanitized_title": "analysing_mathematical_reasoning_abilities_of_neural_models"
        }
    ],
    "cost": 0.0123775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evaluating Transformer Language Models on Arithmetic Operations Using Number Decomposition
June 2022</p>
<p>Matteo Muffo matteo@indigo.ai 
Indigo.ai Via Torino 61MilanItaly</p>
<p>Aldo Cocco 
Indigo.ai Via Torino 61MilanItaly</p>
<p>Enrico Bertino 
Indigo.ai Via Torino 61MilanItaly</p>
<p>Evaluating Transformer Language Models on Arithmetic Operations Using Number Decomposition</p>
<p>Proceedings of the 13th Conference on Language Resources and Evaluation (LREC 2022)
the 13th Conference on Language Resources and Evaluation (LREC 2022)MarseilleJune 2022Language Resources Association (ELRA), licensed under CC-BY-NC-4.0 291Transformer Language Modelsarithmetic operationsnumber decomposition
In recent years, Large Language Models such as GPT-3 showed remarkable capabilities in performing NLP tasks in the zero and few shot settings. On the other hand, the experiments highlighted the difficulty of GPT-3 in carrying out tasks that require a certain degree of reasoning, such as arithmetic operations. In this paper we evaluate the ability of Transformer Language Models to perform arithmetic operations following a pipeline that, before performing computations, decomposes numbers in units, tens, and so on. We denote the models fine-tuned with this pipeline with the name Calculon and we test them in the task of performing additions, subtractions and multiplications on the same test sets of GPT-3. Results show an increase of accuracy of 63% in the five-digit addition task. Moreover, we demonstrate the importance of the decomposition pipeline introduced, since fine-tuning the same Language Model without decomposing numbers results in 0% accuracy in the five-digit addition task.</p>
<p>Introduction</p>
<p>The publication of GPT-3 (Brown et al., 2020) had a relevant impact on Natural Language Processing, showing that it is possible to leverage a Large Language Model (LLM) to perform downstream tasks in the zero and few shot setting. However, although GPT-3 showed on-the-fly reasoning capabilities in tasks such as two or three-digit operations, it struggles with fivedigit operations. This suggests that a LLM such as GPT-3 did not effectively learn to perform arithmetic operations and is not able to generalize its ability to perform sums or subtractions to any number of digits. With this work we want to assess if Transformer Language Models have enough reasoning capabilities to learn to perform arithmetic operations of unseen numbers. To do so, we introduce Calculon, a GPT-2 (Radford et al., 2019) model fine-tuned to perform arithmetic operations between decomposed numbers. In particular, Calculon is trained to perform arithmetic operations following a pipeline that decomposes the numbers in digit form (e.g. 18954 = 4 units, 5 tens, 9 hundreds, 8 thousands, 1 tens of thousands). The underlying idea is to teach LMs to do computations as children learn at school, processing units with units, tens with tens, and so on. Following this pipeline, Calculon reaches remarkable levels of accuracy, even in the four and five-digit tasks where GPT-3 obtains poor results. To validate the importance of the pipeline here proposed, we fine-tune a GPT-2 model on the same datasets of Calculon without decomposing numbers. In this setting, the fine-tuned GPT-2 network reaches very poor performances on the four and five digits tasks, demonstrating that the decomposition pipeline is a valid approach to make LMs effectively learn arith-metic. Finally, we experiment if it is possible to improve the performances of GPT-3 with the proposed decomposition pipeline via few-shot priming (no parameters update). The results obtained are worse than those of the original GPT-3 publication. In light of the experiments carried out we conclude that Transformer Language Models have enough reasoning capabilities to effectively learn to perform addition and subtraction operations, but a higher level of reasoning is required in order to learn to perform multiplications. In section 2 we review literature related to our work, in section 3 we describe the decomposition pipeline that we propose, in section 4 we describe the data used to fine-tune LMs while in section 5 we present and discuss the results obtained. We provide data and code used to reproduce the experiments 1 . Our code is based on the Huggingface Transformers library (Wolf et al., 2020).</p>
<p>Related Work</p>
<p>Numeracy capabilities of NLP models have been widely experimented in literature. Numeracy benchmarks in NLP range from Arithmetic Word Problems (Hendrycks et al., 2021) to Magnitude Comparison (Naik et al., 2019;Wallace et al., 2019) and Measurement Estimation (Zhang et al., 2020). We refer to Thawani et al. (2021) for a complete survey about the topic. Saxton et al. (2019) propose an analysis about mathematical reasoning abilities of neural NLP models by testing them on several mathematical problems such as finding the solution of equations or computing derivatives. The conclusion of this work is that a Transformer LM obtains moderate performances in the tasks analysed, suggesting that there is large room for improving mathematical reasoning capabilities of generative NLP models. Brown et al. (2020) introduced the task of performing computations by generating a numeric answer given an input prompt in natural language. In this work, the authors test the ability of GPT-3 to perform additions, subtractions, and multiplications in the zero and few-shot settings, focusing on numbers from 1 to 5 digits. Results show high levels of accuracy in two-and three-digit addition and subtraction operations, followed by low levels of accuracy as digits increase (four and five). To complete this section, we include papers studying how different tokenization techniques affect numeracy capabilities of NLP models. Thawani et al. (2021) underline that sub-word tokenizers such as BPE (Sennrich et al., 2016) and WordPiece (Wu et al., 2016) split numbers in arbitrary tokens (e.g. 1234 can be split in 12-34, 1-234, . . . ). Moreover, Wallace et al. (2019) demonstrate that sub-word representations provided by BERT (Devlin et al., 2019) are less effective in representing numbers with respect to character-level representations used by ELMo (Peters et al., 2018) when probing token embedding methods in numeracy tasks. Similarly, Geva et al. (2020) show that representing numbers with a digit-level tokenizer instead of Word-Piece improves the performances of their GenBERT model. Compared to these publications, with our decomposition pipeline we propose an alternative way of representing numbers for LMs. Nogueira et al. (2021) propose a work similar to ours in which they fine-tune a T5 model (Raffel et al., 2020) to perform arithmetic operations, providing numbers written in different forms as input to the model. However, there is a crucial difference between Nogueira et al. (2021) and our work: while the former provides manipulated numbers as input to the Transformer LM, in our work we use inputs without any manipulation and we assume that the LM will be able to generate decomposed numbers. We believe that our approach, compared to the work of Nogueira et al. (2021), will produce more robust results due to the fact that no pre-processing operation is performed over input data.</p>
<p>Methodology</p>
<p>As outlined in section 1, in this work we want to assess if Transformer Language Models have the reasoning capabilities to learn how to perform arithmetic operations between unseen numbers composed of a variable amount of digits. In particular, consistently with Brown et al. (2020), we focus on the tasks of summing and subtracting 2, 3, 4, and 5 digit numbers and multiplying 2 digit numbers. In our experiments we fine-tuned a pre-trained GPT-2 Language Model 2 (Radford et al., 2019) to perform computations using different approaches. We underline that our experiments are conducted in a sequence-to-sequence framework,</p>
<p>The main approach we experimented is denoted as decomposition pipeline. The idea is to teach the LM to do calculations in the same way children are taught in school. For instance, if we consider the 5 digit addition task, given the two numbers involved in the sum and the relative result, we generate the input string as follows. First, we translate both numbers in their decomposition form (e.g. 868 becomes 8 units, 6 tens, 8 hundreds), then we sum the decomposed numbers to obtain a decomposed number as the output. Finally we reconstruct the number representing the result of the addition from its decomposed form. We underline again that all these steps are written in natural language and the string constructed will be an observation of the training set used to fine-tune GPT-2. We will refer to LMs fine-tuned with this approach under the name of Calculon.</p>
<p>The second approach that we experimented, named as baseline, is an approach in which no manipulation is done on numbers. An observation relative to this method will be simply a string containing the two numbers involved in the computation followed by the final result.</p>
<p>In section 2 we mentioned the works of Wallace et al. (2019) and Geva et al. (2020), which evidenced that character-level tokenizers are preferable to sub-word tokenizers when processing numbers. For this reason we experiment with another approach, denoted as spaced pipeline, in which we want to assess if a transformer LM can tokenize the digits singularly and solve the operations. An observation relative to this approach will be a string where we transform the two numbers into the spaced form (e.g. 868 becomes 8 6 8), then we compute the operation between the spaced numbers and finally we reconstruct the resulting number starting from its spaced form. The idea behind this approach is that the spacing of the digits allows the BPE tokenizer (Sennrich et al., 2016) used by GPT-2 to tokenize each digit singularly.</p>
<p>At inference time, for all the approaches, the input for the fine-tuned model is a string containing two numbers and an arithmetic operation. If at the end of the generated string there is the number corresponding to the result of the operation, the observation is considered correct. In table 1 we provide examples of training observations and inference inputs for each of the studied approaches.</p>
<p>The last experiment that we conducted is about GPT-3 Language Model. In particular, with this set of tests we want to assess if GPT-3 can benefit from the decomposition pipeline in a few-shot setting (without any parameter update). In this case the experiments consist of evaluating GPT-3 in the same tasks mentioned at the beginning of this section, but providing in the input</p>
<p>Approach Observation Calculon</p>
<p>Compute with pipeline 1201 plus 1302. Translate from number to decomposition: 1201 = 1 units, 0 tens, 2 hundreds, 1 thousands. Translate from number to decomposition: 1302 = 2 units, 0 tens, 3 hundreds, 1 thousands. Sum 1 units, 0 tens, 2 hundreds, 1 thousands + 2 units, 0 tens, 3 hundreds, 1 thousands = 3 units, 0 tens, 5 hundreds, 2 thousands. Translate from decomposition to number: 3 units, 0 tens, 5 hundreds, 2 thousands = 2503  </p>
<p>Baseline</p>
<p>Data and training details</p>
<p>For the addition and subtraction operations, we generate training sets of 12000 observations each. In particular for each N ∈ {3, 4, 5} we randomly sample 3000 couples of integer numbers (n 1 , n 2 ) i , with (n 1 , n 2 ) i ∈ {10 N −1 , . . . , 10 N − 1} 2 , ∀i ∈ {1, . . . , 3000}. Similarly, for N = 2 we randomly sample 3000 couples of numbers (n 1 , n 2 ) i ∈ {0, . . . , 99} 2 (one-digit numbers are included). We then join all the couples created (obtaining a set of 12000 couples) and we compute the results of the operations. At the end of this step, we obtain two vectors of results, r + and r − , where r +,i = n 1,i +n 2,i and r −,i = n 1,i −n 2,i , ∀i ∈ {1, . . . , 12000}. Finally, given a triplet (n 1 , n 2 , r) i , we generate a string according to the procedures described in section 3, depending on the selected approach. For the multiplication, we generate training sets following the same procedure explained above but, instead of sampling 12000 couples, we sample 3000 couples of numbers from the set {0, . . . , 99} 2 because we will only test the multiplications between two-digit numbers. At the end of this procedure we obtain 9 training sets, each of which corresponding to a combination operation-approach (e.g. addition-decomposition), that we use to fine-tune as many Language Models. Now, we want to underline some points relative to the generated training sets. First, by fixing the operation and varying the approach, the same couples of numbers are used to generate strings, so that couples of numbers are sampled once for each operation. Second, none of the couples present in a training set is in the test set relative to the same operation. The test sets used to evaluate our fine-tuned Language Models are the same used to evaluate GPT-3 in the arithmetic tasks 4 (Brown et al., 2020). The GPT-2 models fine-tuned in our experiments are GPT-2 Small architectures, which count ∼117M parameters. The GPT-3 model evaluated in our experiments corresponds to the biggest architecture proposed in Brown et al. (2020), which counts ∼175B parameters. We fine-tune each GPT-2 Language Model for 25 epochs with an initial learning rate of 10 −4 and a batch size of 32, using Adam (Kingma and Ba, 2017) as optimizer. For the experiments on GPT-3, due to limited resources available on the dedicated API, we evaluate the model only on the first 100 observations of each test set. We adopt a greedy decoding strategy for the GPT-2 models and a random sampling strategy with tempera-ture=0.7 for the GPT-3 generations.</p>
<p>A full GPT-input prompt reported in Appendix</p>
<p>Results and discussion</p>
<p>In table 2 we show the results obtained with the experiments described in section 3. The GPT-2 model fine-tuned without decomposition (Baseline row) obtains low accuracy scores in all tasks except two-digit addition, where achieves 53.35 accuracy. In particular, in the 4 and 5 addition and subtraction tasks it achieves zero or near-zero accuracy. This demonstrates that, without decomposing numbers, a GPT-2 Language Model is not able to learn to perform computations, especially between numbers with a higher number of digits. On the other hand, Calculon obtains high accuracy scores in all the tasks tested with the exception of 2Dx. This demonstrates that finetuning using the proposed decomposition pipeline effectively makes possible for a transformer Language Model to learn to do calculations. Here, we underline again that none of the couples of numbers composing the training set are in the relative test set, and hence we can conclude that Calculon has effectively learned to sum units with units, tens with tens, and so on and manage to perform arithmetic operations between unseen numbers. However, the results in the two digit multiplication task are poor, suggesting that number decomposition is not sufficient to solve this task and probably higher reasoning capabilities are needed to multiply numbers.  Table 2: Accuracy scores obtained in the experiments described in section 3. {2, 3, 4, 5}D{+, -} represents 2, 3, 4 or 5 addition or subtraction tasks. 2Dx represents the 2 digit multiplication task. GPT-3 FS refers to results obtained by GPT-3 in the few-shot setting (results in this row are those obtained in (Brown et al., 2020)). GPT-3 FS decomp refers to results obtained by GPT-3 using the decomposition pipeline in few shot examples. Results relative to this last experiment are obtained over the first 100 observations of each test set exclusively.</p>
<p>In figure 1 we report input saliency scores obtained from the addition-Calculon model. These scores are obtained using the gradient-based method described in Shrikumar et al. (2017) and can be interpreted as the influence of each token related to the generated one (highlighted in purple in pictures). Figures and saliency scores are obtained using the Ecco package (Alammar, 2021). We notice that, when analysing the unit digit resulting from the addition (figure 1a) the digits with highest saliency scores are those relative to units in the two numbers which are summed (namely 1 and 2). A similar behavior is present in figures 1b and 1c for tens and hundreds respectively. This indicates that Calculon models effectively learn that units must be summed with units and so on in order to perform a correct addition.</p>
<p>We wanted to assess that the benefits brought by the decomposition pipeline are not due only to the fact that digits are tokenized separately. We then conducted the Spaced experiments, in which we separated digits with a blank space but without indicating if a digit corresponds to units, tens, and so on. The results obtained with these experiments (Spaced row) show a remarkable improve of accuracy with respect to the baseline in the addition tasks followed by a small improve in the subtraction tasks. However Calculon maintains a further improvement with respect to the Spaced approach, with an accuracy gain of almost 15% in the 5D+ task and 75% in the 5D-task. This indicates that providing magnitude information of digits when decomposing numbers significantly helps LMs when performing arithmetic operations. Moreover, the gap in accuracy scores between baseline and Spaced approaches is consistent with the work of Wallace et al. (2019) and Geva et al. (2020). Lastly, observing the results obtained by GPT-3, we notice that decomposing numbers in the few-shot examples (row GPT-3 FS decomp) leads to much lower results with respect to the original GPT-3 results (row GPT-3 FS), in which no manipulation is performed over numbers in the few-shot examples. We hypothesize that receiving a quite long input prompt GPT-3 loses the focus on the task of performing an arithmetic operation and the decomposition prevents it from leveraging on the calculations seen during the pretrain.</p>
<p>Conclusions and future work</p>
<p>In this work we presented Calculon, a GPT-2 Language Model fine-tuned to perform arithmetic operations following a pipeline that decomposes numbers before the computations. We showed that a Transformer Language Model can effectively learn to perform calculations generalizing to unseen numbers when fine-tuned with the decomposition pipeline proposed. Moreover, when we fine-tune on the same task but without the number decomposition, the same GPT-2 network reaches very poor results, proving that the decomposition pipeline effectively brings benefit during the training. On the other hand, we showed that adopting the same decomposition pipeline when providing few shot examples to GPT-3 leads to very bad results, suggesting that decomposition does not bring the same benefit in the few shot setting. We demonstrated that, by decomposing numbers, a Transformer LM such as GPT-2 has the reasoning capabilities to learn during finetuning the rules and procedures to perform additions and subtractions, but the same does not hold for the multiplication operation. There may be a variety of future works related to our experiments. First, consistently with Brown et al. (2020), we tested LMs on up to five-digit operations, but it can be interesting to assess if decomposition brings the same benefit for operations involving a higher number of digits. Secondly, it may be interesting to evaluate why all the tested models struggle with multiplication and if the latter is an operation that requires a level of reasoning unattainable by current linguistic models. In addition, it will be useful to investigate how the number of observations in the training sets affects the performances of fine-tuned GPT-2 models in the studied tasks. Regarding the experiments on GPT-3, it can be investigated whether a more compact formulation of the decomposition pipeline can also bring benefits in the context of few-shot learning. Finally, it can be interesting to extend the experiments presented in this work to other Transformer LMs such as T5 (Raffel et al., 2020) and BART (Lewis et al., 2019).</p>
<p>(a) Input saliency scores of tokens preceding the digit 3 (in purple).</p>
<p>(b) Input saliency scores of tokens preceding the digit 8 (in purple).</p>
<p>(c) Input saliency scores of tokens preceding the digit 8 (in purple). Figure 1: Input saliency scores for the addition-Calculon model.</p>
<p>Appendix: GPT-3 input prompt</p>
<p>In the following lines we report the addition input prompt used when evaluating GPT-3 with decomposition (row GPT-3 FS decomp of </p>
<p>Table 1 :
1Examples of addition training observations for the considered approaches. Bold sub-strings represent input prompts provided to LMs at inference time. The same examples for the subtraction and multiplication tasks can be obtained substituting {plus, +, sum} with {minus, -, subtract} and {times, * , multiply} respectively. prompt 3 only 4 few-shot examples with the decomposition pipeline that we introduced.</p>
<p>table 2 ).
2{number1} and {number2} are replaced by the numbers involved in the computation. The string is composed by 4 few-shot examples which follow the decomposition pipeline introduced in section 3.This application makes an arithmetic 
operation decomposing the input 
numbers. </p>
<h3></h3>
<p>Compute with pipeline 28 plus 
39. Translate from number to 
decomposition: 28 = 2 tens, 8 
units. Translate from number to 
decomposition: 39 = 3 tens, 9 
units. Sum 8 units, 2 tens + 9 
units, 3 tens = 7 units, 6 tens. 
Translate from decomposition to 
number: 6 tens, 7 units = 67 </p>
<h3></h3>
<p>Compute with pipeline 804 plus 
121. Translate from number to 
decomposition: 804 = 8 hundreds, 
0 tens, 4 units. Translate from 
number to decomposition: 121 = 
1 hundreds, 2 tens, 1 units. Sum 
4 units, 0 tens, 8 hundreds + 1 
units, 2 tens, 1 hundreds = 5 units, 
2 tens, 9 hundreds. Translate 
from decomposition to number: 9 </p>
<p>hundreds, 2 tens, 5 units = 925 </p>
<h3></h3>
<p>Compute with pipeline 1201 plus 
1302. Translate from number 
to decomposition: 1201 = 1 
thousands, 2 hundreds, 0 tens, 1 
units. Translate from number to 
decomposition: 1302 = 1 thousands, 
3 hundreds, 0 tens, 2 units. Sum 
1 units, 0 tens, 2 hundreds, 1 
thousands + 2 units, 0 tens, 3 
hundreds, 1 thousands = 3 units, 
0 tens, 5 hundreds, 2 thousands. 
Translate from decomposition to 
number: 2 thousands, 5 hundreds, 
0 tens, 3 units = 2503 </p>
<h3></h3>
<p>Compute with pipeline 97734 plus 
86328. Translate from number to 
decomposition: 97734 = 9 tens of 
thousands, 7 thousands, 7 hundreds, 
3 tens, 4 units. Translate from 
number to decomposition: 86328 = 
8 tens of thousands, 6 thousands, 
3 hundreds, 2 tens, 8 units. Sum 
4 units, 3 tens, 7 hundreds, 7 
thousands, 9 tens of thousands 
+ 8 units, 2 tens, 3 hundreds, 
6 thousands, 8 tens of thousands </p>
<p>Available at: https://github.com/mmuffo94/TransformerLM arithmetics
Available at https://huggingface.co/gpt2 in which the Language Model receives a string as input and provides a string as output that may contain the number corresponding to the correct answer.</p>
<p>Ecco: An open source library for the explainability of transformer language models. J Alammar, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System DemonstrationsAssociation for Computational LinguisticsAlammar, J. (2021). Ecco: An open source library for the explainability of transformer language models. In Proceedings of the 59th Annual Meeting of the As- sociation for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations. Association for Computational Linguistics.</p>
<p>. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, Amodei , D , Language models are few-shot learnersBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Ka- plan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. (2020). Language models are few-shot learners.</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). Bert: Pre-training of deep bidirectional transformers for language understanding.</p>
<p>Injecting numerical reasoning skills into language models. M Geva, A Gupta, J Berant, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsGeva, M., Gupta, A., and Berant, J. (2020). Injecting numerical reasoning skills into language models. In Proceedings of the 58th Annual Meeting of the As- sociation for Computational Linguistics, pages 946- 958, Online, July. Association for Computational Linguistics.</p>
<p>Measuring mathematical problem solving with the math dataset. D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. (2021). Measuring mathematical problem solving with the math dataset.</p>
<p>Adam: A method for stochastic optimization. D P Kingma, J Ba, Kingma, D. P. and Ba, J. (2017). Adam: A method for stochastic optimization.</p>
<p>Bart: Denoising sequence-to-sequence pre-training for natural language generation. M Lewis, Y Liu, N Goyal, M Ghazvininejad, A Mohamed, O Levy, V Stoyanov, L Zettlemoyer, translation, and comprehensionLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mo- hamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. (2019). Bart: Denoising sequence-to-sequence pre-training for natural language generation, trans- lation, and comprehension.</p>
<p>Exploring numeracy in word embeddings. A Naik, A Ravichander, C Rose, E Hovy, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsNaik, A., Ravichander, A., Rose, C., and Hovy, E. (2019). Exploring numeracy in word embeddings. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3374-3380, Florence, Italy, July. Association for Computational Linguistics.</p>
<p>Investigating the limitations of transformers with simple arithmetic tasks. R Nogueira, Z Jiang, Lin , J , Nogueira, R., Jiang, Z., and Lin, J. (2021). Investigat- ing the limitations of transformers with simple arith- metic tasks.</p>
<p>. M E Peters, M Neumann, M Iyyer, M Gardner, C Clark, K Lee, L Zettlemoyer, Deep contextualized word representationsPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. (2018). Deep contextualized word representations.</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. (2019). Language models are un- supervised multitask learners.</p>
<p>Exploring the limits of transfer learning. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, with a unified text-to-text transformerRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer.</p>
<p>Analysing mathematical reasoning abilities of neural models. D Saxton, E Grefenstette, F Hill, P Kohli, Saxton, D., Grefenstette, E., Hill, F., and Kohli, P. (2019). Analysing mathematical reasoning abilities of neural models.</p>
<p>Neural machine translation of rare words with subword units. R Sennrich, B Haddow, A Birch, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyAssociation for Computational LinguisticsLong Papers)Sennrich, R., Haddow, B., and Birch, A. (2016). Neu- ral machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers), pages 1715-1725, Berlin, Germany, August. Association for Computational Linguistics.</p>
<p>Learning important features through propagating activation differences. A Shrikumar, P Greenside, A Kundaje, abs/1704.02685CoRRShrikumar, A., Greenside, P., and Kundaje, A. (2017). Learning important features through propagating ac- tivation differences. CoRR, abs/1704.02685.</p>
<p>Representing numbers in nlp: a survey and a vision. A Thawani, J Pujara, P A Szekely, F Ilievski, Thawani, A., Pujara, J., Szekely, P. A., and Ilievski, F. (2021). Representing numbers in nlp: a survey and a vision.</p>
<p>Do NLP models know numbers? probing numeracy in embeddings. E Wallace, Y Wang, S Li, S Singh, M Gardner, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsWallace, E., Wang, Y., Li, S., Singh, S., and Gard- ner, M. (2019). Do NLP models know numbers? probing numeracy in embeddings. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna- tional Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), pages 5307-5315, Hong Kong, China, November. Association for Computa- tional Linguistics.</p>
<p>. T Wolf, L Debut, V Sanh, J Chaumond, C Delangue, A Moi, P Cistac, T Rault, R Louf, M Funtowicz, J Davison, S Shleifer, P Von Platen, C Ma, Y Jernite, J Plu, C Xu, T L Scao, S Gugger, M Drame, Q Lhoest, A M Rush, Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtow- icz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Scao, T. L., Gugger, S., Drame, M., Lhoest, Q., and Rush, A. M. (2020).</p>
<p>Huggingface's transformers: State-of-the-art natural language processing. Huggingface's transformers: State-of-the-art natural language processing.</p>
<p>Google's neural machine. Y Wu, M Schuster, Z Chen, Q V Le, M Norouzi, W Macherey, M Krikun, Y Cao, Q Gao, K Macherey, J Klingner, A Shah, M Johnson, X Liu, Łukasz Kaiser, S Gouws, Y Kato, T Kudo, H Kazawa, K Stevens, G Kurian, N Patil, W Wang, C Young, J Smith, J Riesa, A Rudnick, O Vinyals, G Corrado, M Hughes, J Dean, translation system: Bridging the gap between human and machine translationWu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey, K., Klingner, J., Shah, A., Johnson, M., Liu, X., Łukasz Kaiser, Gouws, S., Kato, Y., Kudo, T., Kazawa, H., Stevens, K., Kurian, G., Patil, N., Wang, W., Young, C., Smith, J., Riesa, J., Rudnick, A., Vinyals, O., Corrado, G., Hughes, M., and Dean, J. (2016). Google's neural machine translation sys- tem: Bridging the gap between human and machine translation.</p>
<p>Do language embeddings capture scales?. X Zhang, D Ramachandran, I Tenney, Y Elazar, Roth , D , Findings of the Association for Computational Linguistics: EMNLP 2020. Online, November. Association for Computational LinguisticsZhang, X., Ramachandran, D., Tenney, I., Elazar, Y., and Roth, D. (2020). Do language embeddings cap- ture scales? In Findings of the Association for Com- putational Linguistics: EMNLP 2020, pages 4889- 4896, Online, November. Association for Computa- tional Linguistics.</p>            </div>
        </div>

    </div>
</body>
</html>
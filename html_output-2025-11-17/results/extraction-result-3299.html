<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3299 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3299</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3299</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-7a6a298efb965ce9a351a3212f6f536e94dbbb03</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/7a6a298efb965ce9a351a3212f6f536e94dbbb03" target="_blank">Symbolic Chain-of-Thought Distillation: Small Models Can Also “Think” Step-by-Step</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> Symbolic Chain-of-Thought Distillation (SCoTD) is introduced, a method to train a smaller student model on rationalizations sampled from a significantly larger teacher model that enhances the performance of the student model in both supervised and few-shot settings and especially for challenge sets.</p>
                <p><strong>Paper Abstract:</strong> Chain-of-thought prompting (e.g., “Let’s think step-by-ste”) primes large language models to verbalize rationalization for their predictions. While chain-of-thought can lead to dramatic performance gains, benefits appear to emerge only for sufficiently large models (beyond 50B parameters). We show that orders-of-magnitude smaller models (125M—1.3B parameters) can still benefit from chain-of-thought prompting. To achieve this, we introduce Symbolic Chain-of-Thought Distillation (SCoTD), a method to train a smaller student model on rationalizations sampled from a significantly larger teacher model. Experiments across several commonsense benchmarks show that: 1) SCoTD enhances the performance of the student model in both supervised and few-shot settings, and especially for challenge sets; 2) sampling many reasoning chains per instance from the teacher is paramount; and 3) after distillation, student chain-of-thoughts are judged by humans as comparable to the teacher, despite orders of magnitude fewer parameters. We test several hypotheses regarding what properties of chain-of-thought samples are important, e.g., diversity vs. teacher likelihood vs. open-endedness. We release our corpus of chain-of-thought samples and code.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3299.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3299.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (teacher)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 / code-davinci-002 (teacher model used to generate chain-of-thoughts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large autoregressive transformer used as the high-capacity teacher to sample many chain-of-thought (CoT) rationales per instance; used to produce diverse reasoning traces (greedy CoT, sampled CoTs) for distillation and for comparison between greedy vs. marginalization (self-consistency).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (code-davinci-002 / text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large autoregressive transformer language model used in few-shot prompting; in this paper GPT-3 (code-davinci-002 / text-davinci-002) is the teacher that generates chain-of-thought rationales with temperature sampling (T=1.0) and provides per-token likelihoods used for filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈175B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['No CoT prompting (direct few-shot label prediction)', 'Greedy chain-of-thought prompting (single most-likely CoT)', 'Sampled chain-of-thoughts + marginalization (Self-Consistency / majority vote)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>No CoT: standard few-shot prompting that directly predicts labels. Greedy CoT: prompt elicits a single chain-of-thought and greedy decode to produce one rationale+label. Self-Consistency: sample many (diverse) CoTs from the model and marginalize / majority-vote across labels induced by different reasoning traces.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Uses both similar-style (single greedy CoT) and a diverse set of reasoning paths (many sampled CoTs); diversity is produced by stochastic sampling from the teacher (temperature 1.0) and later aggregated using self-consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CommonsenseQA, QuaRel, OpenBookQA (few-shot teacher prompting evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multiple-choice commonsense / qualitative reasoning benchmarks: CommonsenseQA (5-way), QuaRel (qualitative relation questions), OpenBookQA (science knowledge with open book).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Reported accuracies (teacher GPT-3 175B): No CoT: CSQA 82.1, QuaRel 86.9, OpenBookQA 83.4. Greedy CoT: CSQA 77.6, QuaRel 83.3, OpenBookQA 71.8. Self-Consistency (marginalize over samples): CSQA 81.3, QuaRel 86.0, OpenBookQA 86.4.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Self-consistency (marginalization over diverse CoTs) often outperforms greedy single-CoT decoding, especially on OpenBookQA where self-consistency yields a large gain (71.8 -> 86.4). Greedy CoT can degrade performance relative to No CoT on some datasets (e.g., CSQA: 82.1 -> 77.6), showing that a single CoT style is not always better.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A high-capacity teacher benefits from marginalizing over diverse reasoning traces (self-consistency); single greedy CoT can hurt or help depending on dataset. Sampling many diverse CoTs from the teacher is a reliable way to improve final accuracy via aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Greedy CoT sometimes underperforms the No-CoT baseline (e.g., CSQA greedy 77.6 < No CoT 82.1). Gains from self-consistency vary by task (less improvement on QuaRel where No CoT is already high).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Symbolic Chain-of-Thought Distillation: Small Models Can Also “Think” Step-by-Step', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3299.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3299.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OPT-1.3B (student)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OPT-1.3B student model (before and after SCoTD distillation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of smaller OPT student models (125M–1.3B evaluated) is trained or prompted; baseline prompting fails to benefit from CoT, but after Symbolic Chain-of-Thought Distillation (SCoTD) with many sampled teacher CoTs, performance and CoT quality improve markedly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Opt: Open pretrained transformer language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OPT (student variants, standard experiments use OPT-1.3B; other sizes 125M and 350M also examined)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open Pretrained Transformer family (decoder-only transformers). Used here as smaller student models fine-tuned with teacher-generated rationales (SCoTD). Fine-tuning hyperparameters: batch size 32, lr 2e-5; N=30 sampled rationales per training instance by default.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.3B (standard reported), also evaluated at 125M and intermediate sizes</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['No CoT prompting (few-shot / label-only fine-tuning)', 'Greedy chain-of-thought prompting (single-CoT)', 'Self-Consistency decoding (majority vote over sampled CoTs from student)', 'SCoTD (student fine-tuned on many teacher-sampled CoTs)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>No CoT / Label-Only: fine-tune/prompt the student to predict labels without rationales. Greedy CoT: elicit a single CoT via prompting. Self-Consistency: sample multiple CoTs from the student and majority-vote. SCoTD: fine-tune student to predict teacher-sampled rationales+labels (maximize S(y,z|x)).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Contrasts similar single-CoT styles with diverse sets produced by teacher sampling; SCoTD specifically relies on sampling many diverse CoTs (N up to 30) per instance to teach the student diverse reasoning styles.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CommonsenseQA, QuaRel, OpenBookQA (pre- and post-distillation evaluation), also IMDB contrast sets and transfer to SST-2 in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same commonsense/qualitative benchmarks as for teacher; IMDB sentiment with contrast set evaluates robustness; SST-2 used for unseen downstream transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Before distillation (OPT-1.3B prompt): No CoT: CSQA 20.5, QuaRel 9.7, OpenBookQA 2.8. Greedy CoT prompting: CSQA 17.9, QuaRel 39.6, OpenBookQA 12.6. Self-Consistency prompting (student sampling): CSQA 21.1, QuaRel 48.2, OpenBookQA 22.2. After SCoTD (selected results): Few-shot SCoTD: CSQA 64.7 vs Label-only 62.7; QuaRel 73.0 vs Label-only 65.6; OpenBookQA 57.8 vs Label-only 59.8. Supervised SCoTD: CSQA 67.0 (Label-only 63.0), QuaRel 83.8 (Label-only 59.0), OpenBookQA 67.0 (Label-only 60.2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Student models initially cannot leverage CoT via prompting; training with SCoTD (many teacher-sampled CoTs) yields large accuracy gains across tasks compared to label-only and greedy single-CoT baselines. Self-consistency applied to student helps primarily in few-shot SCoTD (noisy training) but not when student trained on filtered CoTs or in some supervised settings.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Small student models (125M–1.3B) learn to generate effective CoTs and improve accuracy only after distillation from a large teacher using many sampled rationales per instance; the volume and diversity of sampled CoTs (e.g., N=30) are crucial. Self-consistency helps in noisy/few-shot training regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Prompting the small student with CoT (without distillation) often fails and can perform worse than random / label-only baselines. After supervised SCoTD, applying self-consistency sometimes yields small negative differences (e.g., supervised SCoTD CSQA self-consistency -0.2).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Symbolic Chain-of-Thought Distillation: Small Models Can Also “Think” Step-by-Step', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3299.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3299.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCoTD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Symbolic Chain-of-Thought Distillation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The distillation method introduced in this paper: sample many chain-of-thought rationales and labels from a large teacher for each training instance and fine-tune a smaller student on those rationale+label pairs so the student learns to produce CoTs and improved predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SCoTD (distillation method applied to student models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Procedure: construct few-shot prompt set with hand-authored CoTs, use a large teacher to sample N CoTs+labels per unlabeled training instance (N default 30), optionally filter samples (e.g., keep only teacher-correct labels in supervised setting), fine-tune student to maximize likelihood of (y,z|x). At inference, decode z and y from student; optionally apply self-consistency (sample multiple z and majority-vote y).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Teacher-sampled diverse chain-of-thoughts', 'Greedy single-CoT (ablation)', 'Self-Consistency on student outputs', 'Filtering strategies: teacher-likelihood, diversity clustering, open-endedness-based allocation, random downsampling']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>SCoTD trains students on a corpus of teacher-generated rationales. Ablations compare single most-likely CoT (greedy) vs many sampled CoTs (diverse), and evaluate filtering strategies: keep top-likelihood CoTs from teacher, keep a diverse set via SBERT clustering, allocate more CoTs to open-ended instances, or random downsampling to a fixed budget.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>SCoTD explicitly exploits diverse reasoning styles by sampling many CoTs per instance; experiments contrast this with using similar/single CoT styles (greedy) and with filtered subsets emphasizing likelihood or diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CommonsenseQA, QuaRel, OpenBookQA, IMDB contrast set, transfer to SST-2 (unseen tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Commonsense and qualitative reasoning multiple-choice datasets; IMDB contrast sets evaluate robustness to small perturbations; SST-2 used to test out-of-domain few-shot transfer after SCoTD training on other tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Key numbers: SCoTD (Few-Shot): CSQA 64.7 (Label-only 62.7, Greedy-CoT 64.6); QuaRel 73.0 (Label-only 65.6, Greedy 64.7); OpenBookQA 57.8 (Label-only 59.8, Greedy 48.8). Supervised SCoTD: CSQA 67.0 (Label-only 63.0, Greedy 68.2), QuaRel 83.8 (Label-only 59.0, Greedy 71.2), OpenBookQA 67.0 (Label-only 60.2, Greedy 50.0). Figure 2 shows student accuracy increases with number of sampled CoTs per instance; improvements saturate around N=30–50.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>SCoTD (many diverse CoTs) outperforms Label-Only and Greedy-CoT baselines in most evaluated settings. Ablations: random downsampling to 5 CoTs per instance is a strong baseline; diversity-based selection often helps over random but no downsampled strategy matches training on the full 30 CoTs per instance, indicating sheer volume matters. Filtering by teacher-likelihood or open-endedness yields mixed results.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Distilling many (diverse) teacher CoTs enables small models to ‘think’ step-by-step and recover much of the utility of CoT prompting; sampling volume is a primary driver, with diversity helpful but insufficient when severely downsampled. SCoTD-trained students produce human-competitive CoTs and generalize better (contrast set and transfer experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Downsampled corpora (e.g., 5 CoTs per instance) cannot match the performance of the full 30x dataset; random subsampling is often nearly as strong as more complex filters, indicating that simple volume can dominate sophisticated selection. Teacher-likelihood filtering does not always outperform random selection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Symbolic Chain-of-Thought Distillation: Small Models Can Also “Think” Step-by-Step', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3299.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3299.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency (student & teacher)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (marginalizing across sampled chain-of-thoughts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Marginalization strategy that samples many reasoning paths (CoTs) from a model and aggregates resulting answers (e.g., majority vote) to improve final prediction accuracy; applied both to the large teacher and to students after SCoTD.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Self-Consistency aggregation (applied to GPT-3 teacher and OPT students)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Implementation: sample multiple CoTs per input from the model (teacher or student) with stochastic decoding (e.g., temperature 0.7 for student sampling at inference), collect predicted labels from each sampled CoT, and take the majority vote (approximate marginalization over z).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Aggregation over diverse sampled CoTs (majority vote)', 'Comparison to greedy single-CoT decoding']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Self-consistency approximates argmax_y E_{z~p(z|x)} p(y|z,x) by sampling many z and voting. It relies on diverse sampled CoTs to distribute probability mass across alternative reasoning paths leading to the correct label.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Relies on generating a diverse set of reasoning traces; effectiveness depends on diversity/volume of samples and on the noisiness of training data (more beneficial when CoTs are noisy/unfiltered).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CommonsenseQA, QuaRel, OpenBookQA (applied in teacher prompting and student inference; Tables 1–3 report results)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same commonsense/qualitative multiple-choice tasks used elsewhere in paper; evaluated with/without self-consistency under different training regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Teacher (GPT-3): Self-Consistency yields CSQA 81.3 (vs Greedy 77.6), QuaRel 86.0 (vs Greedy 83.3), OpenBookQA 86.4 (vs Greedy 71.8). Student (OPT-1.3B pre-distill): Self-Consistency improves QuaRel (48.2 vs Greedy 39.6) and OpenBookQA (22.2 vs Greedy 12.6). After Few-Shot SCoTD, applying self-consistency improved CSQA by +4.5 (60.2 -> 64.7) and OpenBookQA by +13.4 (44.4 -> 57.8) when training on many unfiltered CoTs; but in supervised SCoTD self-consistency sometimes slightly decreased accuracy (e.g., CSQA -0.2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Self-consistency generally improves over greedy single-CoT decoding when many diverse CoTs are available and when training data is noisy (few-shot SCoTD); however, benefits are inconsistent when the student was trained on filtered or fewer CoTs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Marginalizing across diverse CoTs (self-consistency) is an effective way to recover higher accuracy and is particularly useful in low-data/noisy settings; it complements SCoTD by aggregating the student's multiple reasoning traces at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Self-consistency can be unhelpful or slightly harmful when the student was trained on filtered CoTs or when the CoTs are less diverse (observed small negative deltas in supervised SCoTD evaluations).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Symbolic Chain-of-Thought Distillation: Small Models Can Also “Think” Step-by-Step', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Explanations from large language models make small reasoners better <em>(Rating: 2)</em></li>
                <li>Symbolic knowledge distillation: from general language models to commonsense models <em>(Rating: 2)</em></li>
                <li>Teaching small language models to reason <em>(Rating: 1)</em></li>
                <li>Large language models can self-improve <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3299",
    "paper_id": "paper-7a6a298efb965ce9a351a3212f6f536e94dbbb03",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "GPT-3 (teacher)",
            "name_full": "GPT-3 / code-davinci-002 (teacher model used to generate chain-of-thoughts)",
            "brief_description": "Large autoregressive transformer used as the high-capacity teacher to sample many chain-of-thought (CoT) rationales per instance; used to produce diverse reasoning traces (greedy CoT, sampled CoTs) for distillation and for comparison between greedy vs. marginalization (self-consistency).",
            "citation_title": "Language models are few-shot learners",
            "mention_or_use": "use",
            "model_name": "GPT-3 (code-davinci-002 / text-davinci-002)",
            "model_description": "Large autoregressive transformer language model used in few-shot prompting; in this paper GPT-3 (code-davinci-002 / text-davinci-002) is the teacher that generates chain-of-thought rationales with temperature sampling (T=1.0) and provides per-token likelihoods used for filtering.",
            "model_size": "≈175B",
            "reasoning_methods": [
                "No CoT prompting (direct few-shot label prediction)",
                "Greedy chain-of-thought prompting (single most-likely CoT)",
                "Sampled chain-of-thoughts + marginalization (Self-Consistency / majority vote)"
            ],
            "reasoning_methods_description": "No CoT: standard few-shot prompting that directly predicts labels. Greedy CoT: prompt elicits a single chain-of-thought and greedy decode to produce one rationale+label. Self-Consistency: sample many (diverse) CoTs from the model and marginalize / majority-vote across labels induced by different reasoning traces.",
            "diversity_of_methods": "Uses both similar-style (single greedy CoT) and a diverse set of reasoning paths (many sampled CoTs); diversity is produced by stochastic sampling from the teacher (temperature 1.0) and later aggregated using self-consistency.",
            "reasoning_task_name": "CommonsenseQA, QuaRel, OpenBookQA (few-shot teacher prompting evaluations)",
            "reasoning_task_description": "Multiple-choice commonsense / qualitative reasoning benchmarks: CommonsenseQA (5-way), QuaRel (qualitative relation questions), OpenBookQA (science knowledge with open book).",
            "performance_by_method": "Reported accuracies (teacher GPT-3 175B): No CoT: CSQA 82.1, QuaRel 86.9, OpenBookQA 83.4. Greedy CoT: CSQA 77.6, QuaRel 83.3, OpenBookQA 71.8. Self-Consistency (marginalize over samples): CSQA 81.3, QuaRel 86.0, OpenBookQA 86.4.",
            "comparison_of_methods": "Self-consistency (marginalization over diverse CoTs) often outperforms greedy single-CoT decoding, especially on OpenBookQA where self-consistency yields a large gain (71.8 -&gt; 86.4). Greedy CoT can degrade performance relative to No CoT on some datasets (e.g., CSQA: 82.1 -&gt; 77.6), showing that a single CoT style is not always better.",
            "key_findings": "A high-capacity teacher benefits from marginalizing over diverse reasoning traces (self-consistency); single greedy CoT can hurt or help depending on dataset. Sampling many diverse CoTs from the teacher is a reliable way to improve final accuracy via aggregation.",
            "counter_examples_or_negative_results": "Greedy CoT sometimes underperforms the No-CoT baseline (e.g., CSQA greedy 77.6 &lt; No CoT 82.1). Gains from self-consistency vary by task (less improvement on QuaRel where No CoT is already high).",
            "uuid": "e3299.0",
            "source_info": {
                "paper_title": "Symbolic Chain-of-Thought Distillation: Small Models Can Also “Think” Step-by-Step",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "OPT-1.3B (student)",
            "name_full": "OPT-1.3B student model (before and after SCoTD distillation)",
            "brief_description": "A family of smaller OPT student models (125M–1.3B evaluated) is trained or prompted; baseline prompting fails to benefit from CoT, but after Symbolic Chain-of-Thought Distillation (SCoTD) with many sampled teacher CoTs, performance and CoT quality improve markedly.",
            "citation_title": "Opt: Open pretrained transformer language models",
            "mention_or_use": "use",
            "model_name": "OPT (student variants, standard experiments use OPT-1.3B; other sizes 125M and 350M also examined)",
            "model_description": "Open Pretrained Transformer family (decoder-only transformers). Used here as smaller student models fine-tuned with teacher-generated rationales (SCoTD). Fine-tuning hyperparameters: batch size 32, lr 2e-5; N=30 sampled rationales per training instance by default.",
            "model_size": "1.3B (standard reported), also evaluated at 125M and intermediate sizes",
            "reasoning_methods": [
                "No CoT prompting (few-shot / label-only fine-tuning)",
                "Greedy chain-of-thought prompting (single-CoT)",
                "Self-Consistency decoding (majority vote over sampled CoTs from student)",
                "SCoTD (student fine-tuned on many teacher-sampled CoTs)"
            ],
            "reasoning_methods_description": "No CoT / Label-Only: fine-tune/prompt the student to predict labels without rationales. Greedy CoT: elicit a single CoT via prompting. Self-Consistency: sample multiple CoTs from the student and majority-vote. SCoTD: fine-tune student to predict teacher-sampled rationales+labels (maximize S(y,z|x)).",
            "diversity_of_methods": "Contrasts similar single-CoT styles with diverse sets produced by teacher sampling; SCoTD specifically relies on sampling many diverse CoTs (N up to 30) per instance to teach the student diverse reasoning styles.",
            "reasoning_task_name": "CommonsenseQA, QuaRel, OpenBookQA (pre- and post-distillation evaluation), also IMDB contrast sets and transfer to SST-2 in experiments",
            "reasoning_task_description": "Same commonsense/qualitative benchmarks as for teacher; IMDB sentiment with contrast set evaluates robustness; SST-2 used for unseen downstream transfer.",
            "performance_by_method": "Before distillation (OPT-1.3B prompt): No CoT: CSQA 20.5, QuaRel 9.7, OpenBookQA 2.8. Greedy CoT prompting: CSQA 17.9, QuaRel 39.6, OpenBookQA 12.6. Self-Consistency prompting (student sampling): CSQA 21.1, QuaRel 48.2, OpenBookQA 22.2. After SCoTD (selected results): Few-shot SCoTD: CSQA 64.7 vs Label-only 62.7; QuaRel 73.0 vs Label-only 65.6; OpenBookQA 57.8 vs Label-only 59.8. Supervised SCoTD: CSQA 67.0 (Label-only 63.0), QuaRel 83.8 (Label-only 59.0), OpenBookQA 67.0 (Label-only 60.2).",
            "comparison_of_methods": "Student models initially cannot leverage CoT via prompting; training with SCoTD (many teacher-sampled CoTs) yields large accuracy gains across tasks compared to label-only and greedy single-CoT baselines. Self-consistency applied to student helps primarily in few-shot SCoTD (noisy training) but not when student trained on filtered CoTs or in some supervised settings.",
            "key_findings": "Small student models (125M–1.3B) learn to generate effective CoTs and improve accuracy only after distillation from a large teacher using many sampled rationales per instance; the volume and diversity of sampled CoTs (e.g., N=30) are crucial. Self-consistency helps in noisy/few-shot training regimes.",
            "counter_examples_or_negative_results": "Prompting the small student with CoT (without distillation) often fails and can perform worse than random / label-only baselines. After supervised SCoTD, applying self-consistency sometimes yields small negative differences (e.g., supervised SCoTD CSQA self-consistency -0.2).",
            "uuid": "e3299.1",
            "source_info": {
                "paper_title": "Symbolic Chain-of-Thought Distillation: Small Models Can Also “Think” Step-by-Step",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "SCoTD",
            "name_full": "Symbolic Chain-of-Thought Distillation",
            "brief_description": "The distillation method introduced in this paper: sample many chain-of-thought rationales and labels from a large teacher for each training instance and fine-tune a smaller student on those rationale+label pairs so the student learns to produce CoTs and improved predictions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SCoTD (distillation method applied to student models)",
            "model_description": "Procedure: construct few-shot prompt set with hand-authored CoTs, use a large teacher to sample N CoTs+labels per unlabeled training instance (N default 30), optionally filter samples (e.g., keep only teacher-correct labels in supervised setting), fine-tune student to maximize likelihood of (y,z|x). At inference, decode z and y from student; optionally apply self-consistency (sample multiple z and majority-vote y).",
            "model_size": null,
            "reasoning_methods": [
                "Teacher-sampled diverse chain-of-thoughts",
                "Greedy single-CoT (ablation)",
                "Self-Consistency on student outputs",
                "Filtering strategies: teacher-likelihood, diversity clustering, open-endedness-based allocation, random downsampling"
            ],
            "reasoning_methods_description": "SCoTD trains students on a corpus of teacher-generated rationales. Ablations compare single most-likely CoT (greedy) vs many sampled CoTs (diverse), and evaluate filtering strategies: keep top-likelihood CoTs from teacher, keep a diverse set via SBERT clustering, allocate more CoTs to open-ended instances, or random downsampling to a fixed budget.",
            "diversity_of_methods": "SCoTD explicitly exploits diverse reasoning styles by sampling many CoTs per instance; experiments contrast this with using similar/single CoT styles (greedy) and with filtered subsets emphasizing likelihood or diversity.",
            "reasoning_task_name": "CommonsenseQA, QuaRel, OpenBookQA, IMDB contrast set, transfer to SST-2 (unseen tasks)",
            "reasoning_task_description": "Commonsense and qualitative reasoning multiple-choice datasets; IMDB contrast sets evaluate robustness to small perturbations; SST-2 used to test out-of-domain few-shot transfer after SCoTD training on other tasks.",
            "performance_by_method": "Key numbers: SCoTD (Few-Shot): CSQA 64.7 (Label-only 62.7, Greedy-CoT 64.6); QuaRel 73.0 (Label-only 65.6, Greedy 64.7); OpenBookQA 57.8 (Label-only 59.8, Greedy 48.8). Supervised SCoTD: CSQA 67.0 (Label-only 63.0, Greedy 68.2), QuaRel 83.8 (Label-only 59.0, Greedy 71.2), OpenBookQA 67.0 (Label-only 60.2, Greedy 50.0). Figure 2 shows student accuracy increases with number of sampled CoTs per instance; improvements saturate around N=30–50.",
            "comparison_of_methods": "SCoTD (many diverse CoTs) outperforms Label-Only and Greedy-CoT baselines in most evaluated settings. Ablations: random downsampling to 5 CoTs per instance is a strong baseline; diversity-based selection often helps over random but no downsampled strategy matches training on the full 30 CoTs per instance, indicating sheer volume matters. Filtering by teacher-likelihood or open-endedness yields mixed results.",
            "key_findings": "Distilling many (diverse) teacher CoTs enables small models to ‘think’ step-by-step and recover much of the utility of CoT prompting; sampling volume is a primary driver, with diversity helpful but insufficient when severely downsampled. SCoTD-trained students produce human-competitive CoTs and generalize better (contrast set and transfer experiments).",
            "counter_examples_or_negative_results": "Downsampled corpora (e.g., 5 CoTs per instance) cannot match the performance of the full 30x dataset; random subsampling is often nearly as strong as more complex filters, indicating that simple volume can dominate sophisticated selection. Teacher-likelihood filtering does not always outperform random selection.",
            "uuid": "e3299.2",
            "source_info": {
                "paper_title": "Symbolic Chain-of-Thought Distillation: Small Models Can Also “Think” Step-by-Step",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Self-Consistency (student & teacher)",
            "name_full": "Self-Consistency (marginalizing across sampled chain-of-thoughts)",
            "brief_description": "Marginalization strategy that samples many reasoning paths (CoTs) from a model and aggregates resulting answers (e.g., majority vote) to improve final prediction accuracy; applied both to the large teacher and to students after SCoTD.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models",
            "mention_or_use": "use",
            "model_name": "Self-Consistency aggregation (applied to GPT-3 teacher and OPT students)",
            "model_description": "Implementation: sample multiple CoTs per input from the model (teacher or student) with stochastic decoding (e.g., temperature 0.7 for student sampling at inference), collect predicted labels from each sampled CoT, and take the majority vote (approximate marginalization over z).",
            "model_size": null,
            "reasoning_methods": [
                "Aggregation over diverse sampled CoTs (majority vote)",
                "Comparison to greedy single-CoT decoding"
            ],
            "reasoning_methods_description": "Self-consistency approximates argmax_y E_{z~p(z|x)} p(y|z,x) by sampling many z and voting. It relies on diverse sampled CoTs to distribute probability mass across alternative reasoning paths leading to the correct label.",
            "diversity_of_methods": "Relies on generating a diverse set of reasoning traces; effectiveness depends on diversity/volume of samples and on the noisiness of training data (more beneficial when CoTs are noisy/unfiltered).",
            "reasoning_task_name": "CommonsenseQA, QuaRel, OpenBookQA (applied in teacher prompting and student inference; Tables 1–3 report results)",
            "reasoning_task_description": "Same commonsense/qualitative multiple-choice tasks used elsewhere in paper; evaluated with/without self-consistency under different training regimes.",
            "performance_by_method": "Teacher (GPT-3): Self-Consistency yields CSQA 81.3 (vs Greedy 77.6), QuaRel 86.0 (vs Greedy 83.3), OpenBookQA 86.4 (vs Greedy 71.8). Student (OPT-1.3B pre-distill): Self-Consistency improves QuaRel (48.2 vs Greedy 39.6) and OpenBookQA (22.2 vs Greedy 12.6). After Few-Shot SCoTD, applying self-consistency improved CSQA by +4.5 (60.2 -&gt; 64.7) and OpenBookQA by +13.4 (44.4 -&gt; 57.8) when training on many unfiltered CoTs; but in supervised SCoTD self-consistency sometimes slightly decreased accuracy (e.g., CSQA -0.2).",
            "comparison_of_methods": "Self-consistency generally improves over greedy single-CoT decoding when many diverse CoTs are available and when training data is noisy (few-shot SCoTD); however, benefits are inconsistent when the student was trained on filtered or fewer CoTs.",
            "key_findings": "Marginalizing across diverse CoTs (self-consistency) is an effective way to recover higher accuracy and is particularly useful in low-data/noisy settings; it complements SCoTD by aggregating the student's multiple reasoning traces at inference.",
            "counter_examples_or_negative_results": "Self-consistency can be unhelpful or slightly harmful when the student was trained on filtered CoTs or when the CoTs are less diverse (observed small negative deltas in supervised SCoTD evaluations).",
            "uuid": "e3299.3",
            "source_info": {
                "paper_title": "Symbolic Chain-of-Thought Distillation: Small Models Can Also “Think” Step-by-Step",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Explanations from large language models make small reasoners better",
            "rating": 2
        },
        {
            "paper_title": "Symbolic knowledge distillation: from general language models to commonsense models",
            "rating": 2
        },
        {
            "paper_title": "Teaching small language models to reason",
            "rating": 1
        },
        {
            "paper_title": "Large language models can self-improve",
            "rating": 1
        }
    ],
    "cost": 0.01659925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Symbolic Chain-of-Thought Distillation: Small Models Can Also "Think" Step-by-Step</h1>
<p>Liunian Harold $\mathbf{L i}^{*^{1}}$, Jack Hessel ${ }^{\mathbf{1}}$, Youngjae $\mathbf{Y u}^{\diamond}$, Xiang Ren ${ }^{\circ}$, Kai-Wei Chang ${ }^{1} \&amp;$ Yejin Choi ${ }^{\text {® }}{ }^{\circ}$<br>${ }^{1}$ University of California, Los Angeles, ${ }^{\text {A }}$ Allen Institute for Artificial Intelligence<br>${ }^{\circ}$ University of Southern California, ${ }^{\circ}$ Yonsei University, ${ }^{\circ}$ University of Washington</p>
<h4>Abstract</h4>
<p>Chain-of-thought prompting (e.g., "Let's think step-by-step") primes large language models to verbalize rationalization for their predictions. While chain-of-thought can lead to dramatic performance gains, benefits appear to emerge only for sufficiently large models (beyond 50B parameters). We show that orders-of-magnitude smaller models ( $125 \mathrm{M}-1.3 \mathrm{~B}$ parameters) can still benefit from chain-ofthought prompting. To achieve this, we introduce Symbolic Chain-of-Thought Distillation (SCoTD), a method to train a smaller student model on rationalizations sampled from a significantly larger teacher model. Experiments across several commonsense benchmarks show that: 1) SCoTD enhances the performance of the student model in both supervised and few-shot settings, and especially for challenge sets; 2) sampling many reasoning chains per instance from the teacher is paramount; and 3) after distillation, student chain-of-thoughts are judged by humans as comparable to the teacher, despite orders of magnitude fewer parameters. We test several hypotheses regarding what properties of chain-of-thought samples are important, e.g., diversity vs. teacher likelihood vs. open-endedness. We release our corpus of chain-of-thought samples and code.</p>
<h2>1 Introduction</h2>
<p>Empirical scaling laws suggest that the accuracy of Large Language Models (LLMs) on benchmark tasks can be improved by increasing model size and pre-training data volume (Hoffmann et al., 2022). Beyond these training-time improvements, however, an inference-time strategy dubbed "chain-ofthought" (CoT) prompting, ${ }^{1}$ i.e., eliciting verbalizations of predictive processes via key-phrases like "Let's think step-by-step" (Kojima et al., 2022), can</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Symbolic Chain-of-thought Distillation (SCoTD) applied to a student model, ranging in size from 125M-1.3B parameters. We show that fine-tuning on a (diverse and potentially filtered) corpus of expert chain-of-thought demonstrations from a teacher model is an effective strategy to make smaller models capable of chain-of-thought reasoning.
similarly improve performance, e.g., Suzgun et al. (2022) demonstrate additional performance gains on a hard subset of the BigBench tasks (BIG-bench collaboration, 2022) using chain-of-thought.</p>
<p>However, chain-of-thought prompting has only been shown to be beneficial for models of sufficient scale (e.g., with more than 60B parameters (Wei et al., 2022b)). In this work, we study whether small language models can be "taught" the capacity for chain-of-thought reasoning by larger language models. We adopt a simple strategy, which we call Symbolic Chain-of-thought Distillation (SCoTD): first, we sample chain-of-thought rationales from large language model given (unlabeled) input instances from a dataset; then, we train a smaller language model to predict the sampled rationale</p>
<p>CommonsenseQA: Q: The man working in the attic swatted away a bee, but soon the single bee was an entire what?
(a) swarm; (b) pack; (c) countryside; (d) soft drink; (e) field of flowers</p>
<p>QuaRel: Q: Steve is waiting at the gym ... He starts hitting around a hockey puck with his stick. The gym has a carpeted floor and more friction than the ice rink he goes to.
Which surface is rougher? (A) carpet (B) ice rink</p>
<p>OpenBookQA: Q: Magnets may be attracted to
(a) clay pots (b) roofing nails (c) paper plates (d) plastic cutlery</p>
<p>GPT-3: The answer must be related to bees, but also connected with being numerous. Of the above answers, only swarm fits the scenario. So the answer is: (a) OPT-1.3B: The answer must be a swarm of bees. Of the above choices, only soft drink is used to describe a swarm of bees. So the answer is: (c)
OPT-1.3B + SCoTD: The answer must be the name of something that has bees. Of the above choices, only swarms have bees. So the answer is: (a)</p>
<p>GPT-3: When something is smoother, it is easier to slide on and easier to pass through. So the carpet is rougher. So the answer is: (A)
OPT-1.3B: $\qquad$ is rougher than carpet. Thus, the gym floor is rougher than the ice rink. So the answer is: (A)
OPT-1.3B + SCoTD: When something is rougher, it has more friction. Thus, the gym has more friction than the ice rink he goes to. So the answer is: (A)</p>
<p>GPT-3: Magnets are attracted to metal objects. These objects include roofing nails. So the answer is: (b)
OPT-1.3B: Magnets are attracted to clay pots, roofing nails, paper plates, plastic cutlery. So the answer is: (d)
OPT-1.3B + SCoTD: Magnets may be attracted to some metals, but not to clay pots, roofing nails, paper plates or plastic cutlery. So the answer is: (b)</p>
<p>Table 1: Few-shot chain-of-thoughts produced by GPT-3 (code-davinci-002, the teacher model), OPT-1.3B (the un-distilled student model), and OPT-1.3B + SCoTD (ours), the student model trained using Symbolic Chain-of-thought Distillation. $\S 3$ shows this process significantly improves the task-accuracy of the student model in a variety of settings, and in $\S 3.1 .1$, human evaluations show that, even when the un-distilled student model happens to get the multiple choice question correct (see QuaRel example), humans tend to prefer OPT-1.3B + SCoTD.
and sampled label. This process follows the "symbolic knowledge distillation" paradigm as in West et al. (2022), wherein corpora are sampled from a larger language model to serve as training data for a smaller one.</p>
<p>We find that through SCoTD, smaller language models learn to self-rationalize and perform significantly better on 3 commonsense QA tasks compared to learning without rationalizations. This result holds for both supervised and few-shot settings, and across student models of varying scales (125M1.3B parameters). Performance gains are especially pronounced when applying distilled chain-ofthought models to difficult scenarios like: contrast sets (Gardner et al., 2020) (§3.4; SCoTD significantly outperforms supervised learning on labels) and fully held-out tasks (§3.5; few-shot SCoTD significantly outperforms in-context learning).</p>
<p>Key to the success of this process is sampling a relatively large number of rationales per example from the teacher model (e.g., 30 rationales/example) (Figure 2). This is different from many prior practices that train with one rationale per example (Camburu et al., 2018; Li et al., 2022a). In ablation studies, we investigate several competing hypotheses for what are the most important factors within the corpus: we filter the corpus to CoTs that are assigned high probability by GPT-3 vs. filtering to CoTs that are diverse vs. filtering to CoTs that explain more open-ended input instances.</p>
<p>While diversity and high probability are reasonable filters that on average perform well, the "null hypothesis" of random downsampling performs well, suggesting that the sheer volume of the rationales is also a key contributing factor.</p>
<p>We will release code and the corpus of sampled chain-of-thoughts at https://github.com/ allenai/cot_distillation.</p>
<h2>2 Symbolic Chain-of-Thought Distillation</h2>
<p>Our primary goal is to improve the accuracy of a (relatively small) student language model $\mathcal{S}$ on a target classification ${ }^{2}$ task $\mathcal{D}<em i="i">{\text {Test }}=\left{\left(x</em>}, y_{i}\right)\right} .{ }^{3}$ We assume access to 1) (an unlabeled) training set $\mathcal{D<em i="i">{\text {Train }}=\left{\left(x</em>$ (e.g., GPT-3 (Brown et al., 2020)), capable of generating chain-of-thoughts in a few-shot fashion.}\right)\right}$; and 2) a large teacher language model $\mathcal{T</p>
<p>Our first step is to curate a set of labeled chain-of-thoughts to serve as few-shot $\mathcal{P}$ rompts for $\mathcal{T}$. For each target task, we sample a small number (e.g., 10) of examples $x_{i}$ from $\mathcal{D}<em i="i">{\text {Train }}$, provide a gold classification label $y</em>$.}$, and manually author a chain-of-thought $z_{i}$ for each to form the prompt set $\mathcal{P}=\left{\left(x_{i}, y_{i}, z_{i}\right)\right}^{4</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Then, for each $x_{i}$ in $\mathcal{D}<em i="i">{\text {Train }}$, we sample $N$ chain-of-thoughts $\tilde{z}</em>$ from the teacher model, i.e.,}$ along with the resulting prediction $\tilde{y}_{i</p>
<p>$$
\left(\tilde{y}<em i="i">{i}^{k}, \tilde{z}</em>\right)
$$}^{k}\right) \sim_{N} \mathcal{T}\left(y_{i}, z_{i} \mid x_{i}, \mathcal{P</p>
<p>The result of this sampling is a corpus $\mathcal{C}=$ $\left{\left(x_{i},\left{\left(\tilde{y}<em i="i">{i}^{k}, \tilde{z}</em>\right)\right}}^{k<em _Train="{Train" _text="\text">{k=1}^{N}\right)\right}$, which contain teacherpredicted chain-of-thoughts/labels. Depending on the experimental setting (details in § 3), we sometimes filter the entries of $\mathcal{C}$, e.g., in the fully supervised case where $\mathcal{D}</em>$ instances have associated labels, we discard samples for which the sample the teacher model predicted an incorrect label. Next, we train the student model using the standard language modeling loss, i.e., we maximize}</p>
<p>$$
E_{(x, \tilde{y}, \tilde{z}) \sim \mathcal{C}}[\mathcal{S}(\tilde{y}, \tilde{z} \mid x)]
$$</p>
<p>After fine-tuning the student model on the corpus sampled from the teacher, to evaluate the model on a test instance ( $x_{\text {test }}, y_{\text {test }}$ ) from the target task, we decode both a chain-of-thought $\tilde{z}<em _test="{test" _text="\text">{\text {test }}$ and a predicted label $\tilde{y}</em>}}$ from the student and evaluate $\tilde{y<em _test="{test" _text="\text">{\text {test }}$ versus the true label $y</em>}}$. We consider two strategies for decoding. (1) Predict the most likely chain-of-thought and the label $\tilde{z<em _test="{test" _text="\text">{\text {test }}, \tilde{y}</em>}}=\operatorname{argmax<em _test="{test" _text="\text">{z, y} \mathcal{S}\left(z, y \mid x</em>}}\right)$. This can be approximated by greedy decoding or beam search. (2) There may be different valid chain-of-thoughts for a given question and as a result, large language models distribute probability mass for a certain label across many diverse chain-of-thoughts (Wang et al., 2022b). Thus, it is beneficial to marginalize out the reasoning paths to find the most consistent answer: $\tilde{y<em y="y">{\text {test }}=\operatorname{argmax}</em>\right)$. This can be approximated by sampling multiple reasoning paths and take a majority vote among the predicted answers, dubbed "self-consistency" (Wang et al., 2022b). We experiment with both approaches and conduct a discussion in $\S 3.2$.} E_{z \sim \mathcal{S}\left(z \mid x_{\text {test }}\right)} \mathcal{S}\left(y \mid z, x_{\text {test }</p>
<h2>3 Experiments</h2>
<p>We evaluate primarily on 3 target tasks: 1) CommonsenseQA (CSQA) (Talmor et al., 2019), a 5way multi-choice dataset; 2) OpenBookQA (Mihaylov et al., 2018), and 3) QuaRel (Tafjord et al., 2019). While any model capable of few-shot chain-of-thought could be substituted, we use the thought prompts from prior work (Wei et al., 2022b; Wang et al., 2022b) when available.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">CoT</th>
<th style="text-align: center;">CSQA</th>
<th style="text-align: center;">QuaRel</th>
<th style="text-align: center;">OpenBookQA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GPT3-175B</td>
<td style="text-align: center;">No CoT</td>
<td style="text-align: center;">82.1</td>
<td style="text-align: center;">86.9</td>
<td style="text-align: center;">83.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Greedy</td>
<td style="text-align: center;">77.6</td>
<td style="text-align: center;">83.3</td>
<td style="text-align: center;">71.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Self-Consistency</td>
<td style="text-align: center;">81.3</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">86.4</td>
</tr>
<tr>
<td style="text-align: center;">OPT-1.3B</td>
<td style="text-align: center;">No CoT</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">9.7</td>
<td style="text-align: center;">2.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Greedy</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">12.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Self-Consistency</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;">48.2</td>
<td style="text-align: center;">22.2</td>
</tr>
<tr>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">25.0</td>
</tr>
</tbody>
</table>
<p>(a) Performance of prompting the teacher (GPT3-175B) and student model (OPT-1.3B, before distillation). The student fails to outperform the random guess baseline.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Labeled Data</th>
<th style="text-align: center;">CoT</th>
<th style="text-align: center;">CSQA</th>
<th style="text-align: center;">QuaRel</th>
<th style="text-align: center;">OpenBookQA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Few-Shot</td>
<td style="text-align: center;">Label-Only</td>
<td style="text-align: center;">62.7</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">59.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Greedy-CoT</td>
<td style="text-align: center;">64.6</td>
<td style="text-align: center;">64.7</td>
<td style="text-align: center;">48.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SCoTD</td>
<td style="text-align: center;">64.7</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">57.8</td>
</tr>
<tr>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Label-Only</td>
<td style="text-align: center;">63.0</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">60.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Greedy-CoT</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">71.2</td>
<td style="text-align: center;">50.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SCoTD</td>
<td style="text-align: center;">67.0</td>
<td style="text-align: center;">83.8</td>
<td style="text-align: center;">67.0</td>
</tr>
</tbody>
</table>
<p>(b) Performance of the the student model after distillation.</p>
<p>Table 2: Performance before (a) and after (b) SCoTD.
code-davinci-002 version of GPT-35 (Brown et al., 2020) as our teacher model $\mathcal{T}$. We use OPT (Zhang et al., 2022) as our student model $\mathcal{S}$. Our standard student model is OPT-1.3B (though we explore a range of student model sizes in §3.3).</p>
<p>We sample from GPT-3 with a temperature of $T=1.0$. For each training example, we sample $N=30$ rationales. OPT is fine-tuned with a batch size of 32 and a learning rate of $2 \times 10^{-5}$. We use HuggingFace transformers (Wolf et al., 2019), Pytorch (Paszke et al., 2019), and Accelerate ${ }^{6}$ for the implementation. Main experiments can be reproduced on one GPU with 48 GB of memory.</p>
<h3>3.1 Results in Default SCoTD Setting</h3>
<p>We first consider both a few-shot learning setting and a supervised setting. For the few-shot setting, the only labeled examples available to our teacher/student models are contained in the prompt set $\mathcal{P}$ (but we use the unlabeled examples and teacher-generated chain-of-thoughts/labels for training). ${ }^{7}$ We also consider the supervised setting, where we assume access to labels in $\mathcal{D}_{\text {Train }}$. Supervised SCoTD involves simply discarding the samples within $\mathcal{C}$ that do not have the correct label prior to fine-tuning the student: for Common-</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: For three commonsense QA tasks, accuracy (y-axis) improves significantly as the student is trained on more chain-of-thoughts sampled from the teacher (x-axis). Oversampling chain-of-thoughts is sometimes required to improve student performance beyond the supervised label-only baseline, e.g., as in OpenbookQA.</p>
<p>senseQA, OpenBookQA, and QuaRel, this results in discarding 40.4%, 45.0%, 34.2% of chain-of-thoughts. For the few-shot setting, we decode with the self-consistency approach; for the supervised setting, we decode with greedy decoding (introduced in § 2; see an discussion in § 3.2).</p>
<p>We compare SCoTD to 2 baselines: 1) <strong>Label-Only</strong>, the student is fine-tuned on just the label (in the few-shot setting, the label comes from the teacher and could be wrong; in the supervised setting, we use the gold label), instead of also with CoT; 2) <strong>Greedy-CoT</strong>, we decode a single-CoT per example (instead of N = 30 samples) from T for each training example instead of sampling. For additional reference, Table 2 (a) reports the performance of the student (and teacher) in a variety of few-shot settings prior to applying any distillation: No CoT = few shot prompting with labeled instances from P but no z_i, Greedy and Self-Consistency are prompting with CoT but with different decoding strategies (§ 2).</p>
<p>Table 2 (b) gives the performance of the student model after distillation in the supervised and few-shot settings. In all cases, distillation significantly improves the student model, and in all-but-one case, learning with CoT outperforms the label-only distillation baseline. While the student model initially fails to perform CoT through prompting (Table 2 (a)) it learns to do so through distillation.</p>
<p><strong>The number of samples.</strong> In our default setting, to serve as our distillation corpus C, we sample N = 30 rationales from the teacher T for each (unlabelled) training instance. Figure 2 shows the performance of the student model when it is trained on corpora with fewer sampled CoT per instance: results suggest that learning with multiple sampled (albeit nosier) rationales/chain-of-thoughts per example is more beneficial than learning with one (most likely) rationale. Will more rationales bring more performance improvement? We sampled more rationales from GPT-3 to train the student model; however, this does not bring more performance gains. When N = 50, the performance is similar to N = 30: the model achieves 67.0 in accuracy on OpenBookQA (v.s. 67.0), 67.2 on CommonsenseQA (v.s. 67.0), 84.9 on QuaRel (v.s. 83.8).</p>
<h3>3.1.1 Human Evaluations</h3>
<p>While SCoTD improves task accuracy significantly, we additionally conduct human evaluations to assess the generated chain-of-thoughts themselves (see Table 1 for samples). We sample instances from the CommonsenseQA, OpenBookQA, and QuaRel validation sets (300 instances per dataset), and conduct head-to-head human evaluations<sup>8</sup> to assess:</p>
<p><strong>Q1: Does SCoTD result in higher-quality chain-of-thoughts?</strong> <em>Test: OPT-1.3B versus OPT-1.3B + SCoTD.</em> Result: <strong>Yes.</strong> We assess this hypothesis on two subsets of instances: 1) a pure random sample (N=900); and 2) a set of instances for which both models eventually predicted the correct label (N=654). The second setting focuses more closely on the chain-of-thoughts themselves rather than the</p>
<p><sup>8</sup>We remove the final prediction from each chain-of-thought, and ask crowdworkers which is more coherent, fluent, and (importantly) likely to lead to a correct answer. We use Amazon Mechanical Turk and pay a minimum of $15/hr, see Appendix A for more details, including a screenshot of the HIT.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Self-Consistency</th>
<th>CSQA</th>
<th>QuaRel</th>
<th>OpenBookQA</th>
</tr>
</thead>
<tbody>
<tr>
<td>Few-Shot SCoTD</td>
<td>No</td>
<td>60.2</td>
<td>73.4</td>
<td>44.4</td>
</tr>
<tr>
<td></td>
<td>Yes</td>
<td>64.7 (+4.5)</td>
<td>73.0 ( -1.4)</td>
<td>57.8 (+13.4)</td>
</tr>
<tr>
<td>SCoTD</td>
<td>No</td>
<td>67.0</td>
<td>83.8</td>
<td>65.8</td>
</tr>
<tr>
<td></td>
<td>Yes</td>
<td>66.8 ( -0.2)</td>
<td>83.8 ( -8.0)</td>
<td>63.6 ( -2.2)</td>
</tr>
</tbody>
</table>
<p>(a) Self-consistency is most helpful under the few-shot setting, where we train with unfiltered and noisy CoTs.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Self-Consistency</th>
<th style="text-align: center;">#Rationales/Example</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">30</td>
</tr>
<tr>
<td style="text-align: center;">CSQA</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">55.6</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">59.1</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">60.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">55.4 (+0.4)</td>
<td style="text-align: center;">63.0 (+1.7)</td>
<td style="text-align: center;">62.4 (+3.3)</td>
<td style="text-align: center;">68.1 (+1.1)</td>
<td style="text-align: center;">64.7 (+4.5)</td>
</tr>
<tr>
<td style="text-align: center;">QuaRel</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">62.2</td>
<td style="text-align: center;">68.7</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">70.9</td>
<td style="text-align: center;">73.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">62.6 (+0.4)</td>
<td style="text-align: center;">66.2 ( -1.7)</td>
<td style="text-align: center;">70.1 (+0.3)</td>
<td style="text-align: center;">71.2 (+0.3)</td>
<td style="text-align: center;">73.8 ( -0.4)</td>
</tr>
<tr>
<td style="text-align: center;">OpenBookQA</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">40.6</td>
<td style="text-align: center;">43.2</td>
<td style="text-align: center;">44.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">38.0 ( -1.0)</td>
<td style="text-align: center;">37.8 ( -2.6)</td>
<td style="text-align: center;">51.8 (+11.2)</td>
<td style="text-align: center;">59.8 (+16.6)</td>
<td style="text-align: center;">57.8 (+13.4)</td>
</tr>
</tbody>
</table>
<p>(b) Performance of Few-Shot SCoTD with different numbers of sampled CoTs. Benefit of "self-consistency" is most prominent when training with multiple rationales per example on CSQA and OpenBookQA.</p>
<p>Table 3: Student performance with and without selfconsistency.
predictive accuracy of the model. SCoTD is superior in both settings: for the random sample setting, SCoTD won in $59 \%$ of cases ( $p&lt;.001$ ), whereas in the correctness controlled setting, SCoTD won in $61 \%$ of cases ( $p&lt;.001$ ). Results hold with $p&lt;.05$ for each QA dataset individually.</p>
<p>Q2: Does a SCoTD student surpass the much larger teacher? Test: OPT-1.3B + SCoTD versus text-davinci-002. While the task accuracy of the teacher is still higher in most cases, the studentgenerated CoT are comparable. ${ }^{9}$ We again evaluate on: 1) a pure random sample ( $\mathrm{N}=900$ ); and 2) a correctness-controlled setting ( $\mathrm{N}=659$ ). The 100x smaller SCoTD's generations are competitive in both cases; we can't reject the null hypothesis of the crowd having equal preferences (OPT-1.3B + SCoTD wins in $47 \%$ and $51 \%$ of cases respectively, $p&gt;.01$ ). Results hold for each dataset individually, as well.</p>
<h3>3.2 Self-Consistency for the Student</h3>
<p>Wang et al. (2022b) find that, for chain-of-thought prompted models, taking a majority vote over a large set of sample of predicted labels (resulting from a diverse range of CoTs) can improve performance. Our results regarding the effectiveness of sampling $N=30$ rationales from the teacher during SCoTD are similar-in-spirit: i.e., we also show performance gains from sampling multiple rationalization chains per instance.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Performance on CSQA with different amount of training instances, from using only $20 \%$ of the $x$ from $\mathcal{D}_{\text {Train }}$ to using the full set (X-axis). Orange line is the Label Only baseline. Bottom blue line (marked with 1x) is SCoTD but with only 1 sampled rationale per instance; above are SCoTD with 5, 10, 20, 30 sampled rationales per instance, respectively.</p>
<p>A natural question is, does the student model $\mathcal{S}$ exhibit the same phenomenon, i.e., can we sample multiple chain-of-thoughts from it and take a majority vote? We find that the student model can benefit from "self-consistency," but not in all cases. In Table 3, we report the performance with/without self-consistency (majority vote among 30 sampled reasoning paths with a temperature of 0.7 ). When training with filtered CoTs (Table 3 (a) bottom rows) or training with few CoTs per example (Table 3 (b), when #CoTs/Example is small), the student model does not benefit from self-consistency. Only when we train with multiple rationales per example without filtering (the few-shot setting), self-consistency is beneficial on CSQA and OpenBookQA. Overall, the results show that student models benefit from being shown a diverse/noisy set of rationales, and that self-consistency can be effectively applied after distillation.</p>
<h3>3.3 SCoTD across Model and Dataset Sizes</h3>
<p>We also verify the effectiveness of SCoTD across model and dataset sizes; in these experiments, we consider the supervised setting.</p>
<p>Data scaling. Figure 3 shows the effect of varying the size of $\mathcal{D}_{\text {Train }}$ (for simplicity, we show only performance on CSQA as an example). Learning with CoTs is beneficial under all data scales. Interestingly, SCoTD, trained with access to only $40 \%$ of the labelled data, can surpass the direct</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Performance on CSQA with three different model sizes.
supervised label-only model with $100 \%$ of the labelled corpus; this result aligns with the argument in Zaidan et al. (2007) - providing more explanations from the teacher model could be more beneficial than providing more labels.</p>
<p>Student model size scaling. Figure 4 presents results when varying the size of the student model from 125 M to 1.3 B parameters for CSQA. For all model three model sizes, SCoTD outperforms the standard supervised fine-tuning baseline (Label Only). Sampling multiple rationales per input instance is an effective strategy for all model sizes.</p>
<h3>3.4 SCoTD on Challenging Contrast Sets</h3>
<p>Can learning with explanations help generalization, as hypothesized by (Zaidan et al., 2007)? As a preliminary study, we show that SCoTD enables better generalization to contrast sets. Contrast sets (Gardner et al., 2020) are proposed to evaluate a model's robustness to perturbations around the decision boundary, by asking annotators to modify the original test instances in small but meaningful ways that (typically) change the gold label.</p>
<p>We experiment on the IMDB (Maas et al., 2011) sentiment analysis task in the supervised setting; we consider the corresponding contrast set of IMDB proposed by Gardner et al. (2020). We train two models on the training set of IMDB: LabelOnly and SCoTD. For efficiency, we sub-sample $100 K$ examples from the training set of IMDB and truncate input sequences to 700 tokens. As shown in Figure 5, while both models with/without SCoTD achieve high performance on the original</p>
<p>IMDB test set ( $96.1 \%$ v.s. $95.5 \%$, with the LabelOnly model performing slightly better), the model with SCoTD achieves significantly higher performance on the contrast set: $92.0 \%$ vs. $81.6 \%$. This result supports the hypothesis of (Zaidan et al., 2007); that explanations can support more robust generalization.</p>
<h3>3.5 SCoTD on Unseen, Out-of-domain Tasks</h3>
<p>Large language models can perform few-shot, incontext learning with chain-of-thought prompting, i.e., generating reasonable chain-of-thoughts on unseen tasks with a few demonstrations (Suzgun et al., 2022). We conduct a preliminary experiment, inspired by Min et al. (2021)'s MetaICL, to test whether student models trained with SCoTD acquire the same ability. We train a supervised SCoTD model on ANLI, CommonsenseQA, and OpenBookQA, and evaluate it on SST-2 (Socher et al., 2013), a sentiment analysis task.</p>
<p>The SCoTD model achieves a few-shot accuracy of $79.6 \%$ on the validation set (an example prediction is shown in Figure 6). ${ }^{10}$ Compared to a baseline model that learns with no CoT(i.e., a re-implementation of MetaICL trained on 3 source tasks); the baseline fails to recognize the input/output format of the new task and predicts answers out of the desired label set. It achieves (an effective) $0 \%$ accuracy on SST-2. This suggests the potential of including CoTs during instruction/incontext tuning (Wei et al., 2022a; Min et al., 2021).</p>
<h2>4 What Factors are Important for Distillation?</h2>
<p>An important factor underlying the performance gains highlighted in $\S 3$ was the number of chain-ofthoughts we sampled from the teacher model perinstance (more samples = better; Figure 2). Here we ask: is data volume the key contributing factor to the performance improvement? Or, are specific aspects of chain-of-thought samples key for the performance improvements?</p>
<p>We design several filters to identify potentially important examples/CoTs among the correct rationales. We apply designed filters (to be introduced) to $\mathcal{C}^{\prime}$, the corpus sampled from the teacher (with wrong CoTs dropped), that operationalize different hypotheses about what factors are important to distill. We control for dataset size when filtering, i.e.,</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Performance of SCoTD vs. label only supervision on the original and contrast IMDB dataset, along with sample predictions from SCoTD.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Schematic of SCoTD models transferring from training tasks (CSQA, ANLI, OBQA) to unseen tasks (SST-2).
all filtered corpora have the same number of training CoTs. We downsample with a budget of 5 CoT per instance on average ${ }^{11}$. Then, we train the same student model on each of the filtered corpora, and compare on downstream tasks. If a student model trained on filtered corpus A tends to outperform the student model trained on filtered corpus B, then we argue that the property that produced corpus A is more important. The hypotheses we consider are:</p>
<p>Null hypothesis: data volume. As a null hypothesis, we randomly sub-sample 5 CoT per instance; this filter operationalizes the assumption that an arbitrary set of samples is sufficient.</p>
<p>Diversity. For each instance, we compute SBERT (Reimers and Gurevych, 2019) embed-</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Downsampling ablations: we subset our chain-of-thought distillation corpus $\mathcal{C}$ with a fixed budget according to different criteria. In general, keeping a diverse set of rationales performs well, though a random sample often performs well too.
dings ${ }^{12}$ of each of the chain-of-thoughts, and cluster the resulting embeddings using hierarchical clustering into $k=5$ clusters. Then, we randomly sample a single instance from each cluster: the resulting sample covers all clusters, and thus represents a diverse+representative sample.</p>
<p>Teacher likelihood. For each instance, we keep the 5 CoT samples with the highest per-token loglikelihood according to the teacher model.</p>
<p>Open-endedness. Some instances in each dataset lead to a broader range of chain-of-thought samples</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>than others. For example, on CommonsenseQA, the question "What form of alcohol is made from grapes?" leads to a narrower range of rationalizations vs. "Why might someone purposefully be going into trance?" We hypothesize that openended instances could benefit from relatively more sampled rationales. We sort instances into quintiles based on the unique bi-grams in their corresponding 30 CoTs ; for high-ranking instances (more unique CoT bi-grams, like the "trance" example above), we keep more rationales and for low-ranking instances, we keep less rationales. We keep $1,3,5,7,9$ rationales for instances of different bins (thus controlling for the total number of CoT).</p>
<p>Results Figure 7 reports the accuracy of the student model when fine-tuned on the different subsampled corpora for the three tasks we consider. Overall, random subsampling is a strong baseline, but, we see some evidence that diversity among the rationales is important. None of the models trained on the sub-sampled data could approach the model trained on the full 30x/instance CoT set. This suggests that the sheer volume of the CoTs is a key driving force for the performance improvement.</p>
<h2>5 Related Work</h2>
<p>Chain-of-thought prompting. As an extension of few-shot prompting (Brown et al., 2020), chain-of-thought has proven more generally applicable than algorithmic/structured reasoning for which intermediate step generation was initially studied, e.g., by Roy and Roth (2015); Ling et al. (2017); Chiang and Chen (2019); Nye et al. (2021). Recent studies seek to improve and analyze CoTs from different perspectives: Wang et al. (2022b) improves the original CoTs through marginalizing over diverse reasoning paths while Wang et al. (2022a) marginalize over diverse prompts; Zelikman et al. (2022); Huang et al. (2022) improves CoT through a bootstrap manner of training on self-generated CoTs; Li et al. (2022b) introduce voting classifiers to filter sampled CoTs before final prediction; Golovneva et al. (2022) introduce some automatic metrics for automatic assessment of chain-of-thoughts. This study instead focuses on enabling CoT for smaller models via distillation.</p>
<p>Learning with explanations. Hase and Bansal (2022) discuss how explanations can serve as inputs (Talmor et al., 2020), targets (Hendricks et al.,
2016; Fidler et al., 2017; Camburu et al., 2018; Zhou et al., 2020; Narang et al., 2020; Kayser et al., 2021; Wiegreffe et al., 2022), and priors (Zhang et al., 2016; Srivastava et al., 2018) for machine learning models. Chain-of-thought extends earlier efforts which treat explanations as intermediate structures, generated at inference time (Rajani et al., 2019). Most related to our work is Li et al. (2022a), who do also learn with GPT-3 generated explanations; we show multiple samples improve significantly over their single-sample method, and also use chain-of-thought prompting at inference time vs. predicting explanations+labels via independent multitasking.</p>
<p>Knowledge distillation. Recent work, inspired by Knowledge Distillation (Hinton et al., 2015), has considered symbolic knowledge distillation, (West et al., 2022), i.e., instead of distilling from soft representations like logits, large language model serve as training data generators (Xiong et al., 2019; Petroni et al., 2019; Schick and Schütze, 2021; West et al., 2022; Liu et al., 2022; Meng et al., 2022; Bhagavatula et al., 2022); this paper continues this line of work.</p>
<p>Contemporaneous work. There are several contemporaneous papers: Huang et al. (2022), Magister et al. (2022), and Ho et al. (2022) all show that smaller models can benefit from large models' chains of thought. We contributes beyond these by: 1) showing that sampling a large number of chain-of-thoughts is paramount; 2) exploring transfer performance to challenge sets/unseen tasks; and 3) analysis that address what factors are important in the teacher corpus.</p>
<h2>6 Conclusion</h2>
<p>We demonstrate the effectiveness of Symbolic Chain-of-thought Distillation (SCoTD): a method that enables smaller language models to effectively use chain-of-thought-style reasoning. We demonstrate the method's effectiveness across several downstream tasks, different student model sizes, different levels of supervision, and in difficult settings (challenge sets, unseen tasks). Our ablations shed light on what factors are particularly important to distill in these chain-of-thoughts.</p>
<p>Our concrete recommendations are: 1) sampling multiple and diverse CoTs for each input instance, and 2) performing self-consistency when the teacher CoTs are noisy. Several promising av-</p>
<p>enues for future work include:</p>
<ol>
<li>Exploring SCoTD for generation tasks in addition to classification tasks;</li>
<li>Scaling up the number of source tasks in $\S 3.5$ to generalize to more tasks;</li>
<li>Using the down-sampling setup introduced in $\S 4$ to explore additional hypotheses about what other factors may be of importance in CoTs.</li>
</ol>
<h2>Limitations</h2>
<p>Several limitations of our study include:</p>
<ol>
<li>only English-language chain-of-thoughts/tasks considered;</li>
<li>reliance on GPT-3, which is a closed-source product with an unknown training set (which could itself include some explanations); and</li>
<li>focusing only on a single type of student model, OPT.
More broadly, learning from and with explanations carries some specific risks related to automation bias. While a model might rationalize its predictions using a seemingly coherent string of natural language steps, even if it eventually gets the prediction correct, there's no guarantee that the eventually predicted output actually results from a process represented by the rationalization. A user might assign excessive confidence to that system based on the chain-of-thought. We observed many cases where the chain of thought seemed promising only to result in models ultimately making incorrect predictions in the final few tokens. Caution should be taken when displaying chain-of-thoughts to users.</li>
</ol>
<h2>Acknowledgment</h2>
<p>We thank anonymous reviewers for their comments. This work is supported in part by the DARPA MCS program, NCSOFT NLP Center and a Sloan research fellowship.</p>
<h2>References</h2>
<p>Chandra Bhagavatula, Jena D Hwang, Doug Downey, Ronan Le Bras, Ximing Lu, Keisuke Sakaguchi, Swabha Swayamdipta, Peter West, and Yejin Choi. 2022. I2d2: Inductive knowledge distillation with neurologic and self-imitation. arXiv preprint arXiv:2212.09246.</p>
<p>BIG-bench collaboration. 2022. Beyond the imitation game: Measuring and extrapolating the ca-
pabilities of language models. arXiv preprint arXiv:2206.04615.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems.</p>
<p>Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-snli: Natural language inference with natural language explanations. Advances in Neural Information Processing Systems, 31.</p>
<p>Ting-Rui Chiang and Yun-Nung Chen. 2019. Semantically-aligned equation generation for solving and reasoning math word problems. NAACL.</p>
<p>Sanja Fidler et al. 2017. Teaching machines to describe images with natural language feedback. Advances in Neural Information Processing Systems, 30.</p>
<p>Matt Gardner, Yoav Artzi, Victoria Basmova, Jonathan Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, et al. 2020. Evaluating models' local decision boundaries via contrast sets. Findings of EMNLP.</p>
<p>Olga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi, and Asli Celikyilmaz. 2022. ROSCOE: A suite of metrics for scoring step-by-step reasoning. arXiv preprint arXiv:2212.07919.</p>
<p>Peter Hase and Mohit Bansal. 2022. When can models learn from explanations? a formal framework for understanding the roles of explanation data. LNLS 2022, page 29.</p>
<p>Lisa Anne Hendricks, Zeynep Akata, Marcus Rohrbach, Jeff Donahue, Bernt Schiele, and Trevor Darrell. 2016. Generating visual explanations. In ECCV.</p>
<p>Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. stat, 1050:9.</p>
<p>Namgyu Ho, Laura Schmid, and Se-Young Yun. 2022. Large language models are reasoning teachers.</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. 2022. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556.</p>
<p>Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022. Large language models can self-improve. arXiv preprint arXiv:2210.11610.</p>
<p>Maxime Kayser, Oana-Maria Camburu, Leonard Salewski, Cornelius Emde, Virginie Do, Zeynep Akata, and Thomas Lukasiewicz. 2021. E-vil: A dataset and benchmark for natural language explanations in vision-language tasks. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1244-1254.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in Neural Information Processing Systems.</p>
<p>Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian, Baolin Peng, Yi Mao, et al. 2022a. Explanations from large language models make small reasoners better. arXiv preprint arXiv:2210.06726.</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. 2022b. On the advance of making language models better reasoners. arXiv preprint arXiv:2206.02336.</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. $A C L$.</p>
<p>Alisa Liu, Swabha Swayamdipta, Noah A Smith, and Yejin Choi. 2022. Wanli: Worker and ai collaboration for natural language inference dataset creation. arXiv preprint arXiv:2201.05955.</p>
<p>Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In ACL.</p>
<p>Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn. 2022. Teaching small language models to reason. arXiv preprint arXiv:2212.08410.</p>
<p>Yu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han. 2022. Generating training data with language models: Towards zero-shot language understanding. arXiv preprint arXiv:2202.04538.</p>
<p>Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a suit of armor conduct electricity? a new dataset for open book question answering. In EMNLP.</p>
<p>Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2021. MetaICL: Learning to learn in context. NAACL.</p>
<p>Sharan Narang, Colin Raffel, Katherine Lee, Adam Roberts, Noah Fiedel, and Karishma Malkan. 2020. Wt5?! training text-to-text models to explain their predictions. arXiv preprint arXiv:2004.14546.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. 2021. Show your work: Scratchpads for intermediate computation with language models.</p>
<p>Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32.</p>
<p>Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowledge bases? In EMNLP-IJCNLP.</p>
<p>Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Explain yourself! leveraging language models for commonsense reasoning. In $A C L$.</p>
<p>Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using siamese bertnetworks. EMNLP-IJCNLP.</p>
<p>Subhro Roy and Dan Roth. 2015. Solving general arithmetic word problems. EMNLP.</p>
<p>Timo Schick and Hinrich Schütze. 2021. Generating datasets with pretrained language models. In EMNLP.</p>
<p>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP.</p>
<p>Shashank Srivastava, Igor Labutov, and Tom Mitchell. 2018. Zero-shot learning of classifiers from natural language quantification. In $A C L$.</p>
<p>Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. 2022. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261.</p>
<p>Oyvind Tafjord, Peter Clark, Matt Gardner, Wen-tau Yih, and Ashish Sabharwal. 2019. Quarel: A dataset and models for answering questions about qualitative relationships. In $A A A I$.</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. Commonsenseqa: A question answering challenge targeting commonsense knowledge. In NAACL-HLT.</p>
<p>Alon Talmor, Oyvind Tafjord, Peter Clark, Yoav Goldberg, and Jonathan Berant. 2020. Leap-of-thought: Teaching pre-trained models to systematically reason over implicit knowledge. Advances in Neural Information Processing Systems.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022a. Rationaleaugmented ensembles in language models. arXiv preprint arXiv:2207.00747.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022b. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.</p>
<p>Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2022a. Finetuned language models are zero-shot learners. ICLR.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b. Chain of thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems.</p>
<p>Peter West, Chandra Bhagavatula, Jack Hessel, Jena D Hwang, Liwei Jiang, Ronan Le Bras, Ximing Lu, Sean Welleck, and Yejin Choi. 2022. Symbolic knowledge distillation: from general language models to commonsense models. NAACL.</p>
<p>Sarah Wiegreffe, Jack Hessel, Swabha Swayamdipta, Mark Riedl, and Yejin Choi. 2022. Reframing human-ai collaboration for generating free-text explanations. NAACL.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2019. Huggingface's transformers: State-of-the-art natural language processing.</p>
<p>Wenhan Xiong, Jingfei Du, William Yang Wang, and Veselin Stoyanov. 2019. Pretrained encyclopedia: Weakly supervised knowledge-pretrained language model. In International Conference on Learning Representations.</p>
<p>Omar Zaidan, Jason Eisner, and Christine Piatko. 2007. Using "annotator rationales" to improve machine learning for text categorization. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 260-267, Rochester, New York. Association for Computational Linguistics.</p>
<p>Eric Zelikman, Yuhuai Wu, and Noah D Goodman. 2022. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. Opt: Open pretrained transformer language models.</p>
<p>Ye Zhang, Iain Marshall, and Byron C Wallace. 2016. Rationale-augmented convolutional neural networks for text classification. In EMNLP.</p>
<p>Wangchunshu Zhou, Jinyi Hu, Hanlin Zhang, Xiaodan Liang, Maosong Sun, Chenyan Xiong, and Jian Tang. 2020. Towards interpretable natural language understanding with explanations as latent variables. Advances in Neural Information Processing Systems.</p>
<h1>A Crowdworking details</h1>
<p>A screenshot of the interface we use to collect the pairwise human judgments from $\S 3.1 .1$ is given in Figure 8. We conduct a post-hoc analysis using a javascript timer to ensure that annotators were paid at least $\$ 15 / \mathrm{hr}$ : crowdworkers who didn't meet this hourly rate during annotation were awarded bonuses post-hoc to ensure they were paid that rate. We select crowdworkers with IP addresses in US,CA,NZ,AU,GB.</p>
<p>IRB Information Crowdworking studies of standard NLP corpora (involving no personal disclosures) are not required by our IRB to be reviewed by them. While the authors of this work are not lawyers and this is not legal advice, this opinion is based on United States federal regulation 45 CFR 46, under which this study qualifies as exempt. We do not release crowdworker IDs, so annotations cannot be back-traced to individual workers.</p>
<h1>Instructions (click to expand)</h1>
<p>Thanks for your participation and work on this HIT!
In this HIT, you will first be given a question ; this question is an input to an AI model. You do not have to answer the question directly.</p>
<p>Then, you will be given two step-by-step reasoning chains; these "thought processes" are automatically generated by two different AI models, and represent their best efforts to answer the question.</p>
<p>Your job is to pick the step-by-step reasoning chain that you believe is better. While the judgment is ultimately subjective, consider the following factors:</p>
<ul>
<li>Most important: Is one of the step-by-step reasoning chains more likely to lead to a correct answer?</li>
<li>Which has more relevant and correct facts/infereces?</li>
<li>Which is more grammatical, easy-to-read, and fluent?</li>
</ul>
<p>In general, you can be forgiving of slight grammatical errors, but if there are significant readability issues that affect your understanding, feel free to account for that. Thanks again for your efforts, we appreciate your work!</p>
<p>Please take a moment to read the question and both step-by-step reasoning chains. Select the step-by-step reasoning chain that's most likely to lead to the correct answer, e.g., the one that's more correct/fluent/relevant. If they are both bad, still do your best to pick the one that's better.</p>
<h2>Question:</h2>
<p>Q: the hourly totals of sunshine are directly connect to (a) tides (b) seasons (c) altitude (d) weather</p>
<table>
<thead>
<tr>
<th style="text-align: left;">$\bullet A$</th>
<th style="text-align: left;">$\bullet B$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">The answer is directly related to</td>
<td style="text-align: left;">The sun's sun is the cause of the</td>
</tr>
<tr>
<td style="text-align: left;">weather.</td>
<td style="text-align: left;">seasons, due to the alignment of the</td>
</tr>
<tr>
<td style="text-align: left;">Earth axis.</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Figure 8: Crowdworking interface for pairwise judgements of chain-of-thought quality.</p>
<h1>A For every submission:</h1>
<p>A1. Did you describe the limitations of your work?
Limitations section (and throughout)
A2. Did you discuss any potential risks of your work?
Limitations section
A3. Do the abstract and introduction summarize the paper's main claims?
Abstract
\ A4. Have you used AI writing assistants when working on this paper?
Left blank.</p>
<h2>B Did you use or create scientific artifacts?</h2>
<p>Corpus of CoT, discussed throughout
B1. Did you cite the creators of artifacts you used?
We cited all datasets used throughout sec 3/4
B2. Did you discuss the license or terms for use and / or distribution of any artifacts?
We are still working with our legal dept. on the specific permissive license for data release, but will do so.</p>
<p>B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?
The datasets we use are standard benchmarks, so we didn't specifically discuss their use as a benchmark, but they are already widely cited.
B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it?
Our newly collected data is just binary judgments untied to individual annotators.
B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?
Sec 3; Limitations
B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.
We use standard splits for standard benchmarks, so we didn't explicitly discuss the sizes.</p>
<h2>C Did you run computational experiments?</h2>
<p>Sec 3
C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?</p>
<p>Sec 3
The Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.</p>
<p>C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?
Sec 3
C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?
Sec $3 / 4$
C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?
Sec 3</p>
<h1>D Did you use human annotators (e.g., crowdworkers) or research with human participants?</h1>
<p>Sec 3
D1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?
Appendix A
D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?</p>
<h2>Appendix A</h2>
<p>D3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?
Appendix A: crowdworkers presumably understood that their judgments were being used for AI research.</p>
<p>D4. Was the data collection protocol approved (or determined exempt) by an ethics review board? Appendix A</p>
<p>D5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?
We don't know who the annotators are specifically, nor did we ask/need this information.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{11}$ In rare cases, we may end up with less as there are less than 5 correct CoTs for the instance.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{12}$ We use paraphrase-MiniLM-L6-v2.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
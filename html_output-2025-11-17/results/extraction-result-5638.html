<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5638 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5638</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5638</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-117.html">extraction-schema-117</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to estimate or predict the probability or likelihood of specific future real-world scientific discoveries, including details on methods, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-266174431</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.06941v2.pdf" target="_blank">Humans vs Large Language Models: Judgmental Forecasting in an Era of Advanced AI</a></p>
                <p><strong>Paper Abstract:</strong> This study investigates the forecasting accuracy of human experts versus Large Language Models (LLMs) in the retail sector, particularly during standard and promotional sales periods. Utilizing a controlled experimental setup with 123 human forecasters and five LLMs, including ChatGPT4, ChatGPT3.5, Bard, Bing, and Llama2, we evaluated forecasting precision through Mean Absolute Percentage Error. Our analysis centered on the effect of the following factors on forecasters performance: the supporting statistical model (baseline and advanced), whether the product was on promotion, and the nature of external impact. The findings indicate that LLMs do not consistently outperform humans in forecasting accuracy and that advanced statistical forecasting models do not uniformly enhance the performance of either human forecasters or LLMs. Both human and LLM forecasters exhibited increased forecasting errors, particularly during promotional periods and under the influence of positive external impacts. Our findings call for careful consideration when integrating LLMs into practical forecasting processes.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5638",
    "paper_id": "paper-266174431",
    "extraction_schema_id": "extraction-schema-117",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00611075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Human vs Large Language Models: Judgmental Forecasting in an Era of Advanced AI</p>
<p>Mahdi Abolghasemi m.abolghasemi@uq.edu.au 
Odkhishig Ganbold odkhishig.ganbold@unimelb.edu.au 
Kristian Rotaru kristian.rotaru@monash.edu </p>
<p>School of Mathematics and Physics
The University of Queensland</p>
<p>Melbourne Medical School
Royal Melbourne Hospital Department of Medicine</p>
<p>Faulty of Medicine
Dentistry and Health Sciences
The University of Melbourne</p>
<p>Department of Accounting
Monash Business School
School of Psychological Sciences and Monash Biomedical Imaging Facility
Monash University BrainPark
Turner Institute for Brain and Mental Health
Monash University</p>
<p>Human vs Large Language Models: Judgmental Forecasting in an Era of Advanced AI
547BFD8D8CB984706F4CEEB620419B91
This study investigates the forecasting accuracy of human experts versus Large Language Models (LLMs) in the retail sector, particularly during standard and promotional sales periods.Utilizing a controlled experimental setup with 123 human forecasters and five LLMs, including ChatGPT-4, ChatGPT3.5,Bard, Bing, and Llama2, we evaluated forecasting precision through Absolute Percentage Error.Our analysis centered on the effect of the following factors on forecasters' performance: the supporting statistical model (baseline and advanced), whether the product was on promotion, and the nature of external impact.The findings indicate that LLMs do not consistently outperform humans in forecasting accuracy and that advanced statistical forecasting models do not uniformly enhance the performance of either human forecasters or LLMs.Both human and LLM forecasters exhibited increased forecasting errors, particularly during promotional periods.Our findings call for careful consideration when integrating LLMs into practical forecasting processes.</p>
<p>Introduction</p>
<p>Retail supply chain optimization and inventory management critically depend on the precision of demand forecasting.Effective forecasting integrates two principal components: quantitative models and expert human judgment (Brau et al., 2023, Perera et al., 2019).Quantitative forecasting has achieved a level of sophistication and is systematically implemented in various forecasting solutions; however, the nuance of human judgment remains less quantifiable and is not as thoroughly researched (Write et al., 1996;Zellner et al., 2021;Sroginis et al., 2023).Human expertise plays a pivotal role at several points in the forecasting continuumranging from data selection to model optimization, and most notably, in the application of discretionary adjustments to model outputs (Arvan et al., 2019;Lawrence et al., 2006;Perera et al., 2019).</p>
<p>Such expert-informed adjustments are suggested to improve forecast accuracy by integrating qualitative market insights with quantitative predictionsinsights often elusive to purely algorithmic approaches (Fildes et al., 2009;Lawrence et al., 2006).However, there have been limited efforts to systematically explore in controlled experimental settings the mechanisms and reasoning behind experts' modifications of forecasts, and specifically how the integration of advanced forecasting methods and access to qualitative information about past and upcoming promotional periods may influence the precision of expert-informed adjustments (Fildes et al., 2019;Sroginis et al., 2023).</p>
<p>The integration of generative AI and Large Language Models (LLMs) into forecasting represents a notable development, directly following the above discussion on the role of human expertise in enhancing statistical forecasts.LLMs are complex computational models trained on large-scale datasets, capable of generating remarkably human-like text (Brown et al., 2020;Chang et al., 2023).As LLMs like GPT can analyze vast amounts of qualitative information well beyond the capabilities of human forecasters, integrating LLMs into forecasting processes will likely enhance performance in the near future (Kirshner, 2024).Their ability to process and analyze large volumes of data and provide, contextually relevant information has sparked interest in their potential to enhance judgmental forecasting (Makridakis et al., 2023;Schoenegger et al., 2024a).Such AI models are equipped to absorb and recall information, propose actionable insights, and engage in reasoning processes that may rival human cognitive abilities (Yu et al., 2023;Xu et al., 2022).Despite their potential, the performance of LLMs in judgmental forecasting, particularly during promotional periods characterized by complex endogenous and exogenous variables, is not well established (Makridakis et al., 2023).Consequently, the comparison of forecasting performance between LLMs and human forecasters emerges as an important research question (Schoenegger et al., 2024a,b;Halawi et al., 2024;Kirshner, 2024).</p>
<p>Recent studies in the field of forecasting indicate that LLMs could provide valuable predictive insights for economic and market forecasting (Yu et al., 2023;Zhao et al., 2024).These models have shown a level of understanding and capability that is noteworthy in these complex domains.A study by Li et al. (2023) specifically explores the potential of ChatGPT as a financial advisor, examining its ability to forecast listed firm performance.Their findings suggest that ChatGPT can correct the optimistic biases of human analysts, demonstrating the potential of LLMs in offering more balanced and accurate forecasts in financial contexts.</p>
<p>This aligns with the broader trend of employing advanced analytical methods with LLMs, potentially enhancing the forecasting process by leveraging their extensive knowledge base and inferential reasoning.</p>
<p>However, recent empirical evidence also highlights certain limitations of LLMs compared to human forecasters, particularly in tasks that require probabilistic forecasting of future events.Notably, Schoenegger and Park (2023) found that in a real-world forecasting tournament, GPT-4's probabilistic forecasts were significantly less accurate than those made by human participants.This underperformance of LLMs in scenarios requiring advanced decision-making and adaptive reasoning suggests that while LLMs like ChatGPT can offer advantages in specific areas, such as mitigating human biases in financial forecasting, their capabilities in more dynamic and uncertain forecasting scenarios remain limited.</p>
<p>This study aims to methodically assess the inherent capabilities of LLMs in judgmental forecasting and to compare their performance with that of human experts.In controlled experiments, both LLMs and human forecasters operate without any prior training or feedback on forecast accuracy, following the methodology used in studies such as Schoenegger et al. (2024b).Consequently, our research directly evaluates the default capabilities of LLMs, providing an unadulterated assessment of their judgment and decision-making processes under experimental conditions.We explore the performance of LLMs across various scenarios, including their use alongside both simple baseline and advanced statistical models, their effectiveness during sales promotions, and their responses to external variables that add complexity to forecasting tasks.</p>
<p>By calculating and comparing the Absolute Percentage Error (APE) of forecasts produced by human experts and a selection of LLMs, including the advanced ChatGPT4, we aim to contribute new insights into the efficacy of AI-driven models in enhancing forecasting accuracy.This method aligns with the approach advocated by Sroginis, Fildes, and Kourentzes (2023), who support the use of controlled, laboratory-based experiments as a means to obtain an accurate assessment of judgmental adjustments in the forecasting process.Indeed, laboratory experiments have been instrumental in demonstrating the importance of contextual understanding and product-specific knowledge, which extend the accuracy of forecasts beyond the capabilities of statistical models alone (Sroginis, Fildes, andKourentzes, 2023, Arvan et al., 2019;Webby et al., 2005).By doing so, we aim to provide a balanced view of the forecasting capabilities across both human and machine-based forecasters, offering a better understanding of the strengths and limitations inherent to each approach considering the current level of development of LLMs.</p>
<p>The findings of our study present a comparison between human forecasters and LLMs in terms of their forecasting accuracy measured by APE.Analysis indicates that LLMs do not uniformly exceed the predictive accuracy of human forecasters.Specifically, the most recent models such as ChatGPT4 and Bing have shown a level of performance comparable to that of humans, whereas earlier models including ChatGPT3.5,Bard, and Llama2, register higher APE.This suggests a variability in the ability of different LLMs to process large datasets and produce accurate forecasts, possibly due to unique model limitations or the need for more sophisticated training to cope with diverse sales environments.A closer examination of the data indicates varying degrees of response to sales data, forecast type, promotional periods, and external factors, by both human forecasters and LLMs.This research enriches the forecasting literature by providing an analysis that compares the performance of human forecasters with that of LLMs across diverse conditions, emphasizing the sophistication of forecasting models, the sales context, and external influences as critical factors in performance assessment.The study examines the capacity of LLMs to interpret information amidst promotions and external variables, offering a comprehensive evaluation of these models' roles in complex forecasting situations.The outcomes of this research have substantial implications for the enhancement of demand planning and the design of advanced forecasting support systems that more seamlessly incorporate human expertise.Furthermore, our work opens new avenues for research into the integration of LLMs with existing forecasting practices and the refinement of collaborative human-AI decision support frameworks.</p>
<p>The remainder of this paper is organized as follows: Section 2 reviews relevant literature and formulates the hypotheses; Section 3 outlines the methodology and describes the participants; Section 4 presents the findings; Section 5 discusses these results and concludes with a reflection on the study's limitations and directions for future research.</p>
<p>Background and hypothesis development</p>
<p>Background and literature review</p>
<p>In sales forecasting, the integration of statistical models, advanced machine learning algorithms, and human judgment has become increasingly sophisticated.The evolution of forecasting models from simple baseline to advanced integrated systems reflects this complexity.Baseline models, effective at capturing fundamental trends, often struggle in volatile scenarios like promotions (De Baets &amp; Harvey, 2018).</p>
<p>Advanced models, incorporating techniques such as dynamic linear regression, provide a robust solution by integrating complex factors like promotional impacts and pricing strategies (Ma, Fildes, &amp; Hung, 2016Kourentzes &amp; Petropoulos, 2016;Abolghasemi et al., 2020b).This progression addresses the cognitive challenges forecasters face and highlights the need for balancing statistical rigor with human judgment (Fildes &amp; Goodwin, 2007;Fildes et al., 2009).</p>
<p>Human judgment, essential in adjusting statistical forecasts, particularly in promotional contexts, introduces complexities and biases impacting forecast quality (Goodwin &amp; Wright, 2010;Sroginis et al., 2023).In promotional scenarios, the complexity and unpredictability inherent to consumer behavior necessitate a judicious combination of model-based forecasts and expert intuition (Fildes &amp; Hastings, 1994;Sroginis et al., 2023).The significance of human judgment in forecasting, particularly in scenarios influenced by promotions, represents a crucial field of inquiry in both academic research and practical application.The effectiveness of human intervention in these settings exhibits considerable variability, as evidenced by studies such as Kourentzes andFildes (2023), Trapero et al. (2013), Fildes and Goodwin 2007).</p>
<p>The cognitive load associated with complex forecasts can result in suboptimal decision-making, highlighting the need for more intuitive and supportive forecasting systems (Lim &amp; O'Connor, 1995;Lawrence et al., 2006).The literature shows that cognitive biases and heuristics significantly influence forecasters' judgments, often leading to systematic errors (Fildes, Ma, &amp; Kolassa, 2022;Harvey, 1995;Bolger &amp; Harvey, 1993;Goodwin &amp; Wright, 2010).Thus, the trajectory of current research is steering towards a more integrated approach, where the strengths of human cognition are leveraged to complement and refine the outputs generated by advanced forecasting models (Sroginis et al., 2023;Kouentzes and Fildes, 2023).However, further exploration into the balance between human judgment and statistical models is necessitated to better understand the value added by human intervention.</p>
<p>The evolution from purely statistical to judgmental forecasting marks a shift towards integrating historical data with the context of current scenarios, aligning with the core objective of forecasting support systems (FSS).These systems aim to harmonize algorithm precision with human intuition to enhance overall forecast accuracy (Fildes &amp; Goodwin, 2007;Zhang et al., 1998;Green &amp; Armstrong, 2015).</p>
<p>However, there remains a gap in understanding the rationale behind judgmental adjustments, particularly the impact of special events, biases, and human factors on judgmental forecasting (Bolger &amp; √ñnkal-Atay, 2004;Fildes &amp; Goodwin, 2007;Lawrence et al., 2006;Sroginis et al, 2023).This gap calls for a deeper investigation into the cognitive processes involved in judgmental forecasting.Fildes et al. (2009) have emphasized the complexity of these processes, revealing how biases and heuristics can influence forecasters' judgments.Bolger and Harvey (1993) explored how cognitive biases can lead to systematic errors, highlighting the need for systems that support and improve human judgment.Harvey (1995) emphasized the importance of understanding how individuals process and interpret information in forecasting.Goodwin and Wright (2010) showed how cognitive biases can influence judgmental forecasting, especially in complex or ambiguous situations.In essence, the progression towards integrating statistical and judgmental forecasting demands advanced analytical tools and a comprehensive understanding of the human element in forecasting.</p>
<p>The interaction between human cognition and statistical data needs further exploration to enhance forecast efficacy and accuracy, particularly in complex and uncertain environments.Our investigation explores both simple baseline forecasts and advanced promotional forecasts.Simple baseline forecasts effectively capture underlying time series trends and seasonality, yet frequently fail to account for volatile elements like promotions (De Baets &amp; Harvey, 2018).In contrast, advanced baseline forecasts utilize dynamic linear regression models.These models are designed to address the need for a more robust forecasting tool that can assimilate the effects of promotional impacts and integrate key variables such as pricing strategies (Kourentzes &amp; Petropoulos, 2016;Abolghasemi et al., 2020b).The shift towards advanced forecasting models is a response to forecasters' limited cognitive resources.These models aim to reduce cognitive load and provide a more strategic overview of data for effective decision-making (Bolger &amp; Harvey, 1993;Fildes et al., 2009;Fildes, 1991).The role of advanced models in mitigating the biases inherent in human judgment demonstrates their potential in enhancing the accuracy and reliability of forecasts.The clarity provided by these models can be crucial in avoiding the pitfalls of heuristic-driven decisions (Harvey and Bolger, 1996), which are more likely under conditions of excessive cognitive load.Thus, our exploration into the dichotomy of simple baseline versus advanced promotional forecasting models extends beyond a mere a comparative performance assessment; it is also an investigation into their potential to enhance human analytical capabilities and judgment in forecasting scenarios characterized by complexities, such as promotional periods.</p>
<p>The advent of LLMs like ChatGPT adds a new dimension to this research gap.Trained on extensive datasets and textual information, these models generate detailed, contextually relevant information, with potential applications in economic and market forecasting still under investigation (Schoenegger &amp; Park, 2023;Makridakis et al., 2023).Their performance in complex scenarios, especially in promotional periods, is an emerging area of research.It is thus important to understand whether LLMs, as a new class of models, change dramatically our knowledge of the performance of algorithmic models in judgmental forecasting or, put differently, whether LLMs outperform human forecaster, considering a variety of critical factors that have been considered in judgmental forecasting literature.Below we develop our theoretical predictions to contribute further to this investigation.</p>
<p>Hypothesis development</p>
<p>The study's hypotheses are directed towards evaluating the performance of LLMs relative to human judgment under various conditions, including simple baseline vs advanced forecasts, promotional impacts, and external impact variables.This approach aligns with research emphasizing the complex dynamics between algorithmic models and human cognitive processes (Lawrence et al., 2006;Goodwin &amp; Wright, 2010).We also propose hypotheses concerning the forecasting accuracy of humans and five distinct LLMs (assessing each forecaster individually) under these diverse circumstances.To address the potential confounding effects of actual sales figures on our forecast accuracy metric (i.e., absolute percentage error), we have incorporated actual sales as a control variable in our analysis.This adjustment is crucial for mitigating the potential confounding effects of fluctuations in sales figures, which can skew the performance metrics of forecasters.By controlling for actual sales, we aim to isolate the true impact of the variables under investigation, providing a more precise estimation of how various conditions directly affect forecasting accuracy.This ensures that our results more accurately reflect the specific effects of the variables of interest, rather than being influenced by the actual sales volatility.</p>
<p>Comparative Accuracy of LLMs and Human Forecasters (H1)</p>
<p>The adoption of LLMs in time series forecasting prompts a critical evaluation of their performance compared to human forecasters.Human forecasters integrate a broad spectrum of experiential and contextual knowledge into their predictions and often employ heuristics to simplify complex tasks.However, their judgments are vulnerable to several well-documented cognitive biases such as overconfidence, anchoring, and confirmation biases, which impair the accuracy of their forecasts (Kahneman &amp; Tversky, 1974;Chapman &amp; Johnson, 1999;Lawrence et al., 2006).LLMs, on the other hand, leverage vast amounts of data and sophisticated pattern recognition algorithms to generate forecasts, which may enhance consistency and reduce susceptibility to these biases.Nevertheless, LLMs can also manifest new biases such as hallucinations, which adversely impact their performance (Silver et al., 2016;Brown et al., 2020).</p>
<p>In the domain of sales forecasting, achieving accuracy is crucial, and the ongoing debate pits human expertise against algorithmic precision.Human forecasters bring important insights into market dynamics and can swiftly adapt to new information, but their effectiveness is often hampered by psychological biases and a limited capacity to process large data volumes (Lawrence et al., 2006).In contrast, LLMs, unencumbered by these psychological limitations and adept at processing extensive datasets, present a promising alternative.With their ability to discern trends and patterns from comprehensive data (Silver et al., 2016), LLMs are hypothesized to yield more accurate judgmental forecasts than human forecasters, especially in complex forecasting scenarios such as promotional sales.This hypothesis anticipates a discernible difference in forecasting performance, favoring LLMs over humans, in terms of accuracy.</p>
<p>Hypothesis 1 (H1): LLMs will surpass human forecasters in forecasting accuracy, when adjusted for comparable levels of actual sales.</p>
<p>Enhanced Forecasting Performance with Advanced Forecast Models (H2)</p>
<p>In exploring the impact of advanced promotional models on forecasting accuracy (e.g.Sroginis et al., 2023), it is important to consider how additional information in the form of expert predictions or algorithmic projections may influence the decision-making processes of both human forecasters and LLMs.Research in judgmental forecasting suggests that humans often benefit from having access to advanced forecast information, which can serve as an anchor and potentially improve accuracy by reducing the range of possible outcomes considered (Lawrence et al., 2006;Goodwin &amp; Fileds, 2022).However, humans may also over-rely on this information and underweight their own judgment (Meub &amp; Proeger, 2016).</p>
<p>For LLMs, advanced forecasting models can provide accurate data to be integrated into the LLMs' reasoning process, potentially enriching the model's inputs and leading to better performance, especially when these advanced forecasts encapsulate complex patterns that the simple baseline models alone might not detect.Nevertheless, the effectiveness of this integration depends on the quality of the forecasts and the ability of the LLMs to appropriately weigh this information against the data it has already processed.</p>
<p>As discussed in our literature review and in the lead to Hypothesis 1, the provision of advanced forecasts is likely to influence the accuracy of subsequent predictions made by both human forecasters and LLMs.For human forecasters, previous literature has highlighted how the use of advanced forecasts can serve as a heuristic, aiding in the process of making complex judgments and potentially leading to improved accuracy, though it also carries the risk of over-reliance and the suppression of independent judgment (Goodwin, 2005;Lawrence et al., 2006).On the other hand, for LLMs, which are designed to synthesize large volumes of data and identify patterns, advanced forecasts could provide an additional layer of information, enhancing the model's predictive capabilities (Makridakis et al., 2023).The advanced forecasts, if accurately capturing future trends, could therefore improve the LLM's predictive performance by offering a form of 'expert input' that the model can incorporate into its algorithms effectively.This integration is not influenced by the psychological biases that affect human forecasters, potentially leading to a more optimal use of the provided advanced forecasts.</p>
<p>Given the above considerations, we hypothesize: Hypothesis 2 (H2): Both human forecasters and LLMs will exhibit improved forecasting performance when provided with advanced forecasts, due to the additional information serving as a data-rich input that can aid in refining predictions (H2a).However, the degree of improvement will differ between the two groups, with LLMs expected to utilize advanced forecasts more effectively, as they can systematically integrate and evaluate such inputs without the cognitive biases that human forecasters are subject to (H2b).</p>
<p>Variability in Forecast Accuracy During Promotional and Non-Promotional Periods (H3)</p>
<p>Building upon the challenges posed by promotional periods in sales forecasting, our theoretical prediction hinges on the systematic data processing capabilities of LLMs.LLMs' proficiency in forecasting during promotional periods, however, is predicated on their exposure to a diverse set of historical promotional data.Such data enable these models to adjust to the volatility introduced by promotions more consistently than humans, who might underreact or overreact to the influence of promotions on consumer behavior.</p>
<p>While LLMs are not explicitly trained on promotional period data in this study, their inherent capabilities may already include the ability to anticipate the impact of promotional activities on sales.This potential understanding could result in more consistent forecasting accuracy across promotional and nonpromotional periods compared to human forecasters, as we test our predictions.</p>
<p>Thus, Hypothesis 3 is formulated with the expectation that LLMs, given their algorithmic nature and assuming comprehensive training on promotional data, will demonstrate a less pronounced difference in forecasting accuracy between promotional and non-promotional periods compared to human forecasters, whose judgment may be clouded by inherent cognitive biases:</p>
<p>Hypothesis 3 (H3):</p>
<p>The accuracy of both human forecasters and LLMs will differ during promotional versus non-promotional periods (H3a), with LLMs expected to demonstrate a less pronounced difference in accuracy between these periods compared to humans (H3b).</p>
<p>Impact of External Conditions on Forecasting Disparities (H4)</p>
<p>Forecasting in the face of external impacts is a complex endeavor, requiring the consideration of diverse, often non-linear influences on sales data.Understanding the influence of external conditions on forecasting accuracy necessitates distinguishing between negative and positive impacts.In our study, external impact is presented as a compound effect of competitors' activity such as advertisement, indicating whether such activity has impacted or will impact product sales positively or negatively.These conditions introduce volatility into sales data, complicating the forecasting process (Abolghasemi et al., 2020a).</p>
<p>Human forecasters, when integrating and interpreting external impact informationoften qualitative in naturecan exhibit significant biases influenced by the emotional valence of the information, whether negative or positive (Sroginis et al., 2023).During economic downturns or natural disasters, for instance, forecasters might disproportionally emphasize such negative information, resulting in overly conservative estimates due to a well-documented negativity bias (Lewandowsky et al., 2012;Ito et al., 1998).</p>
<p>Conversely, during periods of positive economic outlooks or favorable holiday seasons, forecasters may demonstrate undue optimism in their predictions (Eroglu &amp; Croxton, 2010;Goodwin &amp; Fildes, 2022;√ñnkal et al., 2023).Negative information often influences evaluations more strongly than comparably significant positive information and is a recognized phenomenon in expert forecasts (Chang &amp; Hao, 2022).Such asymmetry in human judgment, often driven by affect and representativeness heuristics, leads to greater variability in forecasting accuracy, particularly during periods of negative versus positive external impact.</p>
<p>In contrast, LLMs, devoid of emotional responses, analyze data patterns algorithmically.Provided their default training includes a range of both negative and positive external impacts, LLMs can maintain a consistent forecasting approach.They adjust to different conditions based on learned data patterns rather than emotional responses, a capability rooted in their algorithmic nature (Chang et al., 2023).Therefore, while both human forecasters and LLMs are likely to experience variances in forecasting accuracy due to external impacts, the extent of these variances may differ.Humans are expected to show higher sensitivity to the valence of external information due to emotional and cognitive biases.LLMs, unaffected by such biases, are anticipated to exhibit a smaller disparity in APE across varying external conditions.</p>
<p>Hypothesis 4 (H4):</p>
<p>The accuracy of both human forecasters and LLMs will differ under negative versus positive external impact conditions (H4a).Human forecasters will show a greater difference in accuracy under negative versus positive external impact conditions (i.e., lower accuracy) than LLMs (H4b).</p>
<p>Variability in Forecast Accuracy under Complex Forecasting Scenarios (H5)</p>
<p>The dynamic environment of sales forecasting, where numerous factors intersect, may impose challenges for human forecasters to maintain high levels of accuracy due to the cognitive strain imposed by the need to balance and interpret multiple, and sometimes conflicting, pieces of information.The propensity for cognitive biases and the potential for information overload may lead to increased variability in accuracy among human forecasters when considering all factors simultaneously.</p>
<p>When considering a comprehensive forecasting model that accounts for all previously considered factorssuch as the presence of advanced forecasts, promotional versus non-promotional periods, and negative versus positive external impacts (forming therefore six treatment conditions, see Table 1)the interaction between these variables becomes crucial in determining forecasting accuracy.This multifaceted approach reflects the real-world complexity of sales forecasting, where multiple factors often simultaneously influence outcomes.LLMs are designed to handle large volumes of complex data, potentially allowing them to better navigate the interactions between multiple forecasting factors without succumbing to the biases that affect human judgment (Chang et al., 2023).Their algorithmic efficiency and immunity to psychological biases allow them to systematically analyze complex interactions between factors.Therefore, when all factors from the forecasting model are taken into account, LLMs are hypothesized to achieve a more consistent and accurate performance compared to human forecasters, as indicated by a generally lower forecasting accuracy that does not fluctuate as widely with the change in conditions.</p>
<p>Hypothesis 5: The accuracy of human forecasters will vary across treatment conditions that incorporate advanced forecasts, promotional conditions, and external impacts (H5a).When considering a model that incorporates the aforementioned treatment conditions, LLMs will demonstrate more accurate forecasting across these conditions, as indicated by a consistently lower accuracy, compared to humans (H5b).</p>
<p>Method</p>
<p>Participants</p>
<p>The experimental study has been conducted in August 2023 with one hundred twenty-three human forecasters and five LLMs.Human forecasters were business school graduate students (65.9% female, 34.1% male) who voluntarily participated in this study conducted in a behavioral laboratory at one of Australia's large universities.The average age of human forecasters was 24.21 (SD = 2.51), ranging from 22 to 34 years, and they had on average 1.46 years (SD = 1.60) of business experience.All human forecasters had practical skills in business forecasting since they all completed a unit on time series forecasting where they had to demonstrate practical skills in applying a variety of timeseries forecasting models for stationary and non-stationary data (including the data with additive or multiplicative seasonal effects).</p>
<p>In addition, we used five popular and powerful LLMs including ChatGPT3.5,ChatGPT4 developed by OpenAI, Llama2 developed by Meta, Bing developed by Microsoft, and Bard developed by Google.</p>
<p>These models are the most advanced natural language processing models.They benefit from at least millions of parameters, have been trained on a vast amount of data, and fine-tuned to be able to reason, answer questions, and generate text like a human.</p>
<p>Design</p>
<p>The study's design was a 2x2+2 mixed design, involving three independent variables (IVs).The first IV, 'statistical forecast', had two conditions: a baseline and an advanced model.The second IV, 'external impact', differentiated between positive and negative effects.The third IV, 'promotion', was divided into active and no promotion conditions.Notably, the advanced statistical forecast, which accounted for promotional effects, was exclusively applied when promotions were active.Consequently, this design yielded six experimental conditions (as per Table 1):</p>
<ol>
<li>
<p>Promotion active, positive external impact, baseline statistical forecast.</p>
</li>
<li>
<p>Promotion active, positive external impact, advanced statistical forecast.</p>
</li>
<li>
<p>Promotion active, negative external impact, baseline statistical forecast.</p>
</li>
<li>
<p>Promotion active, negative external impact, advanced statistical forecast.</p>
</li>
<li>
<p>No promotion, positive external impact, baseline statistical forecast.</p>
</li>
</ol>
<p>6.No promotion, negative external impact, baseline statistical forecast.</p>
<p>Each participant evaluated the same collection of twenty-four time series.The accuracy of the participants' forecasts was gauged using the Absolute Percentage Error (APE) as the dependent variable (DV).APE quantifies forecasting precision by comparing the forecasted values against actual sales, presented as a percentage.The lower the APE value, the more accurate the forecast.This measure is critical for assessing the effectiveness of judgmental forecasting against statistical models under varying market conditions and promotional strategies.</p>
<p>Participants are awarded based on their performance (forecasting accuracy) between 5 to 20 AUD.The forecasting accuracy is calculated based on the APE as follows:
ùê¥ùëÉùê∏ = ùëéùëèùë†(ùëì ùë° ‚àí ùë• ùë° ) ùë• ùë° (1)
where   is the forecast at time , and   is the actual sales at time .</p>
<p>Materials and Apparatus</p>
<p>The experiment was designed with R shiny app and hosted on RStudio1 .Participants were given 24 different sales series representing the actual sales of different products in an Australian company.The company is a food manufacturing company that manufactures health and breakfast products and distribute their products through two main retailers in Australia.Each series has 24 observations with different scales of sales, promotion status, and sales uplifts due to promotion.</p>
<p>Figure 1 shows a screenshot of the experiment graphical interface.It represents the dashboard of the forecasting system that participants used to conduct their forecasting decisions.to signify an increasing or decreasing impact on sales volume, respectively.This impact may vary between promotional and non-promotional periods and influences demand by marginally increasing or decreasing sales volumes up to 10%.This approach was influenced by a case study that examined how a company accounts for promotional impacts on sales while also considering competitor activities.The company's forecasts leverage continuous market intelligence, enabling specialists to adjust their predictions in response to comprehensive insights about competitors' policies, prices, and promotions.The influence of the "External Impact" variable on sales is dual: it is perceived negatively if competitors promote similar products when the company's products are not on major sale, and positively if the company's products are on major sale while competitors' products are not.While these impacts are specific to individual products, they are generalized by the company's experts based on overall market share and conditions.Although this model is derived from our case study, variations in other contexts might present worthwhile avenues for further research.</p>
<p>Besides this information, participants were given a "statistical forecast" depicted in "blue" color dotted line on the main dashboard.The statistical forecast may be of two types: "simple baseline statistical forecast" (aka 'baseline forecast') or "advanced statistical forecast" (aka 'advanced forecast').The "simple baseline forecast" is a forecast generated by the forecast support system of the company, which uses "Holt-</p>
<p>Winter exponential smoothing" model to forecast baseline sales in the absence of promotion and does not consider "promotion" and 'external impact'.Whereas the advanced forecast takes into account the impact of promotion but not 'external impact' and uses a "dynamic linear regression" model to forecast sales.</p>
<p>The given information includes two types of dotted lines, i.e. blue representing historical forecasts and red -historical sales for the last 24 periods of time series, depicted on the dashboard.We provided the same graph and dashboard for all participants to avoid any interfering impact of graph types (Ramires &amp; Harvey, 2023).Other relevant information provided to the participants included: promotion status and external impact in Period 25, and system statistical forecast (Simple baseline or advanced baseline statistical forecast).We assume that all of the internal and external variables that contribute to sales are summarized in the given information.</p>
<p>Procedures</p>
<p>Participants (human forecasters and LLMs) were asked to assume the role of the sales manager in a hypothetical company.In this role, they were asked to forecast sales for Period 25 based on the given information.Each participant evaluated the same collection of twenty-four separate time series.These series represented genuine sales data for various health and breakfast products from an Australian food manufacturer, encompassing fluctuations in sales volume, promotional activities, and sales boosts from promotions.Period 25 is either promotion period or non-promotion period.For simplicity, participants were not asked to forecast any immediate post promotion periods.</p>
<p>The ability of LLMs to generate accurate responses heavily depends on the given prompts.Prompts are texts that human can give to LLMs to provide them with the context and ask them for responses.To ensure fairness in our experiment, we provided the LLMs with the same information that was given to the human participants.This was done during the same week in August 2023 when the experiment with the human forecasters was conducted.In our prompt engineering, the data provided effectively characterized the task for the LLMs.This information comprises the contents of handouts, time series observations in tabular format, and additional details such as promotion status, statistical forecasts, and external impact information, all of which were provided to the human participants.The precise prompt provided to the LLMs is detailed in Appendix B. The same prompt was used for all LLMs considered in this study.Detailed descriptive statistics for the experimental data are provided in Appendix C.</p>
<p>Results</p>
<p>We use both mixed-effects models and regression models (specifically for testing the performance of each LLM separately) to test our hypotheses.Adopting the approach of Fildes et al. (2019), our statistical analysis utilized linear mixed-effect models with restricted maximum likelihood, favored for handling correlated repeated-measures data and covariates that vary over time, due to their flexibility, resilience to missing data, and ability to model realistic variance and correlation patterns (Gueorguieva &amp; Krystal, 2004;Cnaan, Laird, &amp; Slasor, 1997).</p>
<p>The general equation for our mixed-effects models is:
ùëåùëñùëó=ùõΩ 0 +ùõΩ1ùëãùëñùëó+ùõΩ2ùëçùëñùëó+ùë¢ùëó+ùúñùëñùëó
Where:</p>
<p>‚Ä¢  j represents the dependent variable (APE) for the -th observation within the j-th forecaster.</p>
<p>‚Ä¢ 0 is the intercept.</p>
<p>‚Ä¢  is the covariate for actual sales, reflecting its impact on APE.</p>
<p>‚Ä¢ 1 is the coefficient for actual sales.</p>
<p>‚Ä¢  encompasses the categorical fixed effects from various experimental conditions, including forecaster type (humans and LLMs), forecast model (baseline or advanced), promotional status (promotion or non-promotion), and external impact (positive or negative), along with their interactions.</p>
<p>‚Ä¢ 2 are the coefficients for the fixed effects.</p>
<p>‚Ä¢  is the random effect for each forecaster, capturing unexplained variability across forecasters.</p>
<p>‚Ä¢  is the residual error for each observation.</p>
<p>In our analysis, "actual sales" is included as a covariate to mitigate the influence of sales volume variations on APE.Experimental conditions, such as promotion status (yes promotion, no promotion), forecast type (baseline forecast, advanced forecast), external impact (positive external impact, negative external impact), and treatment interventions (interaction of aforementioned conditions), are integrated as fixed effects.This structure helps clarify how different forecasting contexts affect forecast accuracy.By treating each forecaster as a random effect, our model adapts to individual differences among forecasters, thus enhancing the analysis depth.This approach not only captures the variability inherent in real-world forecasting but also boosts the generalizability and reliability of our findings.</p>
<p>When testing hypotheses for each forecaster type, where each condition is represented by a single sample observation (i.e., each LLM forecaster), we employ regression models because the data structure does not support the testing for random effects, which are a requisite for the application of mixed-effect models.We discuss the main results in the following sections.More comprehensive results on H2a-H5a are detailed in Appendix J. Additionally, descriptive statistics of forecast accuracy for both human and LLM forecasters are provided in Appendices D and E.</p>
<p>Comparative Accuracy of LLMs and Human Forecasters (H1)</p>
<p>Our Hypothesis 1 posited that LLMs would surpass human forecasters in terms of forecasting accuracy, measured by APE, under the assumption of comparable levels of actual sales.The premise was that LLMs, by virtue of their capacity to process extensive datasets and discern complex patterns devoid of human cognitive biases, would provide a more data-driven and objective forecasting approach.</p>
<p>Contrary to this hypothesis, the results of the mixed-effects regression analysis revealed a more nuanced picture (see Table 2 and Figure 2).The performance of the LLMs varied, with ChatGPT3.5 and Llama2 yielding significantly higher APE values compared to human forecasters, indicating inferior forecasting accuracy.</p>
<p>On the other hand, Bard, Bing and ChatGPT4 did not show significant differences in APE compared to human forecasters, which indicates that their forecasting accuracy was not notably worse or better than that of humans.These findings challenge the initial hypothesis, revealing that not all LLMs are inherently better at forecasting than humans and that the efficacy of LLMs can be model-specific.Therefore, while some LLMs may have the theoretical capability to outperform human forecasters, the empirical evidence suggests that this capability may not always translate into better forecasting accuracy in practice.Considering H2a, which posits that both human forecasters and LLMs would benefit from advanced forecasts, separate models were tested for each forecaster type.In doing so, we conducted a mixed-effects regression analysis for human forecasters to account for within-subject variability and separate regression analyses for each LLM.The results are shown in Appendix J.In the mixed-effects regression analysis, with baseline versus advanced forecasting methods as predictors, advanced forecasts did not significantly reduce accuracy for human forecasters compared to baseline forecasts.Similarly, advanced forecasts did not yield any significant changes in forecast accuracy for each of the LLMs when compared to baseline forecasts.In H2b, we hypothesized that LLMs will show a greater improvement in forecasting performance than human forecasters when using advanced forecasts.The results are shown in Table 3 and Figure 3.Among the different types of forecasters, Bard, ChatGPT3.5, and Llama2 exhibited a significant increase in APE by 10.73%, 10.25%, and 9.79% respectively compared to the performance of human forecasters.This result does not support H2b, as these increases suggest a decline in forecasting performance when these LLMs are provided with advanced forecasts.</p>
<p>However, for Bard, the interaction term with the advanced forecast type significantly decreased APE by -14.64%, partially supporting H2b.This indicates that Bard performed better with advanced forecasts compared to its baseline performance.This interaction effect is noteworthy as it contrasts with the main effect of the advanced forecast type, suggesting that while advanced forecasts did not generally improve performance for forecasters, Bard was able to utilize the advanced information effectively.On the contrary, for ChatGPT3.5 interaction term with the advanced forecast type significantly increased APE by 11.85%, suggesting that ChatGPT3.5 performed worse when provided with advanced forecasts compared to its baseline performance.</p>
<p>Variability in Forecast Accuracy During Promotional and Non-Promotional Periods (H3)</p>
<p>To investigate the variability in APE across promotional and non-promotional periods for different types of forecasters (H3a), mixed-effects regression analysis for human forecasters and separate regression analyses for each LLM were conducted.The results are shown in Appendix J.The analysis results demonstrate that during promotional periods, human forecasters experienced a significant increase in APE, indicating reduced forecast accuracy.For the LLMs, ChatGPT3.5, and Llama2 showed a notable increase in APE during promotional periods, indicating a decline in forecasting performance compared to nonpromotional periods.Conversely, Bard, Bing and ChatGPT4 did not exhibit significant changes in APE during promotional periods.</p>
<p>Regarding H3b, the mixed-effects regression analysis indicated a significant overall increase in APE during promotional periods, implying reduced forecasting accuracy during these times.The results are shown in Table 4 and Figure 4, respectively.Contrary to H3b, which predicted that LLMs would show a smaller difference in APE compared to humans, certain LLMs (ChatGPT3.5,and Llama2) experienced a significant increase in APE during promotional periods.This result suggests a decrease in their forecasting performance during promotional periods, contrary to the hypothesis.</p>
<p>The absence of significant main effects among different forecaster types, with coefficients ranging from -3.10 for ChatGPT3.5 to 0.56 for ChatGPT4, indicates that no single forecaster type consistently outperformed or underperformed human forecasters across conditions.Significant random effects in the model pointed to substantial variability (1.42%) in APE among different forecasters.The inclusion of these random effects substantially improved the model's fit, as evidenced by the likelihood ratio test (p &lt; 0.05).</p>
<p>Impact of External Conditions on Forecasting Disparities (H4)</p>
<p>When addressing H4a, the mixed-effects analysis for human forecasters and regression analyses for each of LLMs were conducted in order to test the forecaster-specific APE under negative vs positive external impact conditions.The results are shown in Appendix J.The parameters indicated substantial variability in forecasting performance among different forecasters.The results suggest that the presence of positive vs negative external impacts on forecasting accuracy did not significantly vary by different forecaster types.</p>
<p>Our examination of the impact of negative versus positive external impacts on APE when comparing the forecast accuracy for LLMs versus human forecasters (H4b) yielded insights that partially counter H4b as shown in Table 5 and Figure 5. Notably, the presence of a positive external impact was not found to be associated with a significant increase in APE.Contrary to the hypothesis, which anticipated a smaller difference in APE for LLMs, ChatGPT3.5 and Llama2 demonstrated significant increases in APE compared to human forecasters, while Bard, Bing and ChatGPT4 did not exhibit significant changes.The interaction effects further suggested a complex response pattern, particularly for Llama2, with a decreased APE (i.e.improved accuracy) under positive impact.These results suggest that the LLMs' algorithmic nature did not uniformly mitigate the impact of positive external influences, as was hypothesized.</p>
<p>Variability in Forecast Accuracy under Complex Forecasting Scenarios (H5)</p>
<p>In a series of regression analyses comprising mixed-effects model analysis for human forecasters and regression model for each LLM type, we examined the impact of various treatment conditions on APE for each of forecaster types (H5a).In all models, the baseline condition, represented by the scenario "Yes promotion, Positive impact, Baseline statistical forecast," serves as the reference point for comparison.The results are summarized in Appendix J.</p>
<p>For human forecasters, when compared to the baseline, the "Yes promotion, Positive external impact, Advanced forecast" condition significantly increased APE.This suggests that advanced forecasting models, combined with positive external impacts and promotional activities, lead to decreased accuracy in human forecasts.The "Yes promotion, Negative external impact, Baseline forecast" condition also showed an increase in APE compared to the baseline treatment condition, indicating reduced accuracy under this scenario.In contrast, the "No promotion, Positive external impact, Baseline forecast" condition exhibited a significant decrease in APE, indicating an improvement in forecast accuracy compared to the baseline treatment condition.This suggests that human forecasters may better perform when provided with baseline forecasting under positive external impacts during non-promotional periods.This trend indicates that the interplay of promotional conditions, external impacts, and the nature of the forecasting model (baseline vs.</p>
<p>advanced) significantly influences the accuracy of human forecasters.Human forecasters' accuracy is likely to improve under non-promotional conditions, particularly in handling positive impacts when they are provided with baseline forecasting method.</p>
<p>Bard showed significant decreases in APE under almost all conditions compared to the baseline treatment condition, except for the condition "Yes promotion, Negative external impact, Baseline forecast."</p>
<p>Notably, the most substantial decreases were observed in "Yes promo, Positive external impact, Advanced forecast", "No promo, Positive impact, Baseline forecast", and "No promo, Negative impact, Baseline forecast".This suggests that "Bard" performs better in terms of accuracy in these altered conditions compared to the baseline scenario.The analysis for Bing showed no significant changes in APE across all treatment conditions compared to the baseline treatment condition.The coefficients fluctuated around the baseline without showing a clear trend of increase or decrease, indicating a consistent performance across different scenarios.ChatGPT3.5 demonstrated significant decreases in APE in the "No promo, Positive external impact, Baseline forecast" and "No promo, Negative impact, Baseline forecast" conditions.These results suggest improved accuracy under non-promotional conditions, irrespective of the external impact.</p>
<p>Similar to Bing, ChatGPT4 showed no significant change in APE across different treatment conditions compared to the baseline.The coefficients indicate a relatively stable performance across various scenarios.</p>
<p>Llama2 exhibited significant decreases in APE in the "Yes promo, Positive impact, Advanced forecast", "No promo, Positive external impact, Baseline forecast", and "No promo, Negative impact, Baseline forecast" conditions, indicating enhanced forecasting accuracy in these scenarios compared to baseline condition.</p>
<p>In our mixed-effects regression analysis investigating the difference in forecast performance by LLMs vs human forecasters considering the complex nature of the forecasting task as represented by treatment conditions and encompassing the forecast type, promotional conditions, and external impacts (H5b), we observed significant variations in APE across different forecaster types (see Table 6 and Figure 6).The findings suggest that, irrespective of forecaster type, the response to specific treatment conditions across all forecasters varied.In the treatment condition combining promotion with positive external impact and advanced forecasting, overall APE was significantly increased.Similarly, the treatment condition with promotion, negative external impact, and baseline forecasting featured increased APE.In non-promotional period, coupled with positive external impacts and baseline forecasting led to decreased APE.</p>
<p>Regarding main effects of individual forecaster types, significant increases in baseline APE were observed for Bard, ChatGPT3.5, and Llama2 compared to the human forecasters.No significant changes were found for Bing and ChatGPT4 vs human forecasters Interaction effects between treatment conditions and forecaster types revealed notable variations as shown in Table 6 and Figure 6: Bard and Llama2 exhibited significant reductions in APE in several treatment conditions, particularly under advanced forecasting scenarios.Bing and ChatGPT4 displayed less variation in their performance across the different treatments.Notably, forecast accuracies for ChatGPT3.5 and Llama2 were higher in non-promotional conditions under both negative and positive external impact.</p>
<p>Generally, the findings suggest that promotional conditions typically increased APE, with some LLMs showing a capacity to adapt more effectively to these conditions.The post-experimental questionnaire was designed to assess the perceived importance of various information types when human participants made judgments using statistical forecasts and to evaluate their agreement with statements about the impact of promotions on their judgment.The results in Appendix F reflect a comparative assessment between baseline and advanced statistical forecasts.Participants rated the importance of information such as forthcoming promotion type, external impact, and historical sales data, with means ranging from 5.11 to 5.68, indicating a moderate to high level of importance across all factors when using both baseline and advanced forecasts.Notably, 'Advanced statistical forecast' was deemed most critical with a mean importance rating of 5.82 when considering advanced forecasts vs baseline forecast (mean importance rating of 5.30).Respondents' agreement with the impact of promotions on their judgment produced mean scores around the mid-5 range, indicating a mild to moderate agreement that both earlier and later promotions influenced their judgment about the forecast in period 25.The self-assessment of the accuracy of participants judgmental forecasts revealed a mean of 4.44 out of 7 (where 7 is 'Very high'), indicating a modestly positive self-assessment of the participants' judgmental forecasting accuracy.</p>
<p>Discussion and conclusion</p>
<p>This study embarked on an in-depth exploration to compare the forecasting abilities of human experts and LLMs in the retail sector.An experimental setup was devised involving 123 human forecasters, primarily business school graduate students, and five leading LLMs, including Chat GPT3.5, Bard, Llama2, Bing, and ChatGPT-4, to investigate their accuracy in predicting sales data under various conditions such as during normal and promotional sales periods.These LLMs, renowned for their advanced natural language processing capabilities, were evaluated against human judgment, known for its understanding and adaptability in complex forecasting environments.The experiment was structured to cover diverse scenarios, encompassing both baseline and advanced forecasting models, under varying conditions like promotions and external impacts.</p>
<p>The results portrayed a landscape where the effectiveness of LLMs in sales forecasting was not unequivocally superior to that of human forecasters.While some LLMs, such as ChatGPT-4 and Bing, displayed forecasting accuracies on par with human participants, others like ChatGPT3.5 and Llama2 lagged behind.This outcome challenges the prevalent notion that LLMs inherently surpass human forecasters in accuracy.It stresses the importance of considering the model-specific capabilities and the context-dependent nature of these AI tools when deployed in complex forecasting scenarios.The study's findings suggest that while LLMs have considerable potential, their effectiveness is influenced by the nature of the forecasting task and the specific model's characteristics, thus requiring careful consideration when integrating them into practical forecasting processes.</p>
<p>Discussion of main findings</p>
<p>Our hypothesis that LLMs would generally surpass human forecasters in accuracy was not uniformly supported.While some LLM models like ChatGPT4 were found to be generally more accurate in the forecasting experiment, its performance was not certainly consistently better than other LLMs across various conditions.This finding aligns with mixed results in LLM performance reported by Schoenegger and Park (2023) and reflects Makridakis et al. (2023) who caution against assuming uniform superiority of LLMs in forecasting.This study results not only show variations in LLM performance in forecasting but also suggest that the processing capabilities of the LLMs, like those of humans, are different and may vary from one model to another.The varied APE scores of LLMs highlight the importance of a strategic approach in using appropriate models for relevant forecasting tasks.</p>
<p>The expectation that using advanced forecasting models would universally enhance human forecasting performance was not confirmed, challenging the assumption that more sophisticated models invariably improve predictions.This outcome is consistent with findings from Fildes &amp; Goodwin (2007) and highlights the intricacies of advanced models, which may sometimes misalign with both human and AI forecasting capabilities as discussed by Kourentzes &amp; Petropoulos (2016).Furthermore, our findings indicate that both human and AI forecasters, including LLMs like Bard, ChatGPT3.5, and Llama2, struggle with maintaining consistent accuracy across promotional and non-promotional periods.This is particularly evident in dynamic environments where unpredictability and volatility challenge the integration of relevant information into forecasts, as noted by Lawrence et al. (2006) and Fildes &amp; Hastings (1994).These</p>
<p>observations suggest a need for more adaptive and intuitive forecasting systems that can better handle the complexities of such scenarios, as suggested by Sroginis et al. (2023).</p>
<p>The significant reduction in APE for Llama2 under positive external impact conditions partially supports our hypothesis that LLMs will exhibit varying levels of accuracy depending on the nature of external impacts-positive or negative.Although other LLMs and humans demonstrated similar accuracy under both positive and negative external impact conditions, their response patterns diverged markedly from that of Llama2.Unlike Llama2, which made minimal to no adjustments under positive conditions but significant negative adjustments under adverse conditions, forecasters such as humans, Bing, and ChatGPT responded differently.These forecasters made substantial positive adjustments in response to positive external impacts and comparatively smaller adjustments under negative conditions, as detailed in Appendix E. This varied response strategy resulted in a consistent level of accuracy across both conditions.This finding aligns with the research by Fildes and colleagues (2009), which suggests that larger adjustments typically lead to more significant improvements in accuracy.This is particularly evident in the case of human forecasters, whose average accuracy under these conditions was relatively better than that of ChatGPT3.5 and Llama2.Specifically, in the case of ChatGPT, which made large negative adjustments under negative external impacts and relatively small positive adjustments under positive conditions, the pattern of response underscores the complexities of forecasting accuracy across different contexts.</p>
<p>Looking at trends in forecast accuracy in terms of general descriptive statistics of Median MdAPE and Mean MdAPE (Appendix D), we observe humans generally achieve the lowest median and mean MdAPE across all categories of conditions, suggesting a higher overall accuracy compared to the other forecasters.</p>
<p>ChatGPT3.5, on the other hand, shows remarkably higher errors, particularly under promotional and advanced forecasting conditions.In contrast, Bard and Bing improve substantially, with particularly low errors for Bard in advanced forecast conditions, suggesting its strength in more complex scenarios.</p>
<p>We also looked at the average forecast accuracy of various forecasters including experiment participants, experts, and LLMs, as well as their adjustment size to provided forecasts.The results are summarized in Appendices C, D, and E. Looking at trends in forecast accuracy in terms of general descriptive statistics of Median MdAPE and Mean MdAPE (Appendix D), we observe humans generally achieve the lowest median and mean MdAPE across all categories of conditions, suggesting a higher overall accuracy compared to the other forecasters.This is in line with the findings by Schoenegger et al., (2024a), who showed that the collective wisdom of crowd outperforms LLMs in various decision-making scenarios.</p>
<p>Additionally, forecast errors tend to increase for almost all forecasters under promotional conditions, with ChatGPT3.5 showing the largest increase.In contrast, humans and ChatGPT4 maintain relatively lower errors, effectively managing the complexities associated with promotional effects.In the absence of promotions, most models demonstrate decreased mean and median MdAPE rates, with ChatGPT4 and Llama2 notably excelling, indicating better performance in stable market conditions.Notably, under advanced forecasting conditions, Bard significantly improves, showing the lowest errors, which suggests that some models may have specialized capabilities that are context dependent.Under positive external impacts, most models except humans tend to struggle with increased errors, highlighting a general sensitivity to favorable external stimuli.</p>
<p>Looking at the adjustment sizes to forecasts (Appendix E), distinct trends can be observed among different forecasters.Human forecasters typically exhibit the largest mean and median adjustments, indicating a robust and adaptive response to changing market conditions, particularly during promotional periods and under both positive and negative external impacts.In contrast, models like ChatGPT3.5 and Llama2 often show reduced mean and median adjustments, particularly in complex and adverse conditions like advanced forecasting, promotional periods, and negative external impacts, suggesting a conservative approach.Our observations reveal that such smaller adjustments often correlate with increased error rates.This is particularly notable in models like Bard during advanced forecasting conditions where its minimal adjustment of -39.4 corresponded with poorer performance metrics.This trend suggests that overly conservative adjustments might not adequately address the complexities of certain forecasting scenarios, thereby potentially compromising accuracy.This perspective is supported by Fildes et al. (2009), who argued that minor adjustments generally do not enhance forecast accuracy, and it may sometimes be preferable to maintain the original forecast in the face of minor changes.Bing and ChatGPT4 display variability, with Bing making large adjustments under positive external impacts but decreasing adjustments under advanced forecast conditions.ChatGPT4, similarly, adapts well under promotional and positive conditions, showing a capacity for considerable adjustments akin to human forecasters.This is in line with the findings by Fildes at al. (2009) who found large adjustments by human experts are often useful to improve the forecast accuracy.Our results show that positive adjustment is associated with lower forecast error on average.However, this may not hold for different conditions.We have a limited number of periods with no adjustment, and we observe positive adjustments more frequently than negative adjustments.This might be because we have had many promotion periods in series and naturally forecasts needed to be adjusted positively.For details, please see Appendix E.</p>
<p>Implications</p>
<p>This research significantly contributes to the body of forecasting literature by empirically examining and comparing the efficacy of human forecasters and LLMs in forecasting sales under different conditions.</p>
<p>It enhances our understanding of the complex relationship between human judgment and AI in forecasting, and challenges the assumption that LLMs are inherently superior to human forecasters.The study's findings, particularly regarding the differential performance of LLMs and the complex interaction of external factors in forecasting accuracy, provide a deeper understanding of the capabilities and limitations of both human and AI forecasters.This research highlights the importance of context and model specificity in forecasting, aligning with the theoretical frameworks suggested by Fildes &amp; Goodwin (2007) and Lawrence et al. (2006), and offers a refined perspective on the integration of human expertise with advanced AI models in forecasting.</p>
<p>The study's insights are particularly valuable for practitioners in the retail industry, emphasizing the importance of a carefully considered approach when integrating LLMs into forecasting processes.Given the variability in LLM performance, especially during complex promotional periods as highlighted in our findings, it becomes imperative for industry professionals to not overly rely on these models without due consideration of their context-specific capabilities and limitations.This research advocates for a strategic blend of AI and human judgment, tailored to the nuances of specific forecasting scenarios.The findings suggest that over-reliance on LLMs without accounting for their limitations in certain contexts, particularly during promotional periods as indicated by the increased APE values, may lead to suboptimal forecasting outcomes.Therefore, retail practitioners should consider these insights when designing and implementing forecasting systems, ensuring they are equipped to handle the intricacies of various sales contexts effectively.</p>
<p>The implications of this study extend to policymakers and industry regulators, who should consider these findings when establishing guidelines and frameworks for the adoption and integration of AI in business forecasting practices.The study highlights the necessity of policy frameworks that acknowledge the limitations and strengths of both human forecasters and LLMs.Regulatory guidelines should encourage the development of AI systems that are not only advanced in terms of data processing but also capable of being effectively integrated with human judgment.This is particularly relevant in the context of the increasing reliance on AI for decision-making in various industries.</p>
<p>Limitations</p>
<p>A notable limitation of this study is the reliance on graduate students as the primary human forecasters.</p>
<p>While these participants possess academic knowledge and some practical skills in business forecasting, they may not have the extensive practical experience and in-depth understanding typical of seasoned industry professionals.This gap potentially affects the generalizability of the findings, as real-world business forecasting often involves intricate decision-making and judgment calls that are honed through extensive professional experience.The diversity in expertise and approach that experienced professionals bring to forecasting could yield different insights, especially in terms of how they interact with and interpret the outputs of advanced models like LLMs.Hence, while the study provides valuable insights into the comparative abilities of novice human forecasters and LLMs, it may not fully reflect the forecasting landscape in a professional setting.</p>
<p>The other limitation of this study relates to the specific data used in this study.We provided 24 time series from real-world data for FMCG products that represent different behaviors in trend, promotion, etc.</p>
<p>However, it will be useful to replicate this study on another dataset with different patterns and from industries to evaluate whether the results hold or not.While we looked at only two variables "Promotion</p>
<p>Status" and "External Impact" variables, other variables with different settings can be considered.For instance, "External Impact" in our experiment is considered a binary variable with "Positive" and "Negative" impact such that they can alter sales up to 10%, We were inspired by the case study in our experimental design, however, this variable can take different forms and have different impact on sales and might be considered in future research.The study also reveals a limitation in the generalizability of its findings across various AI models due to the variable performance of different LLMs.This variability necessitates a more in-depth exploration into the specific traits and training that contribute to the forecasting efficacy of these models.The LLMs selected for the study, including ChatGPT4, represent only a fraction of the rapidly evolving landscape of AI forecasting tools.Future advancements in AI technology and new model developments could significantly alter the efficacy landscape, leading to different outcomes than those observed in the current study.This limitation emphasizes the need for continuous evaluation of AI models in forecasting, taking into account the evolving nature of AI capabilities and the specific characteristics of each model that may impact its performance in various forecasting scenarios.</p>
<p>While LLMs have shown remarkable capabilities in generating human-like text and understanding context, their application in forecasting presents specific challenges, one of the most notable being the tendency for "hallucinations," or the generation of plausible but factually incorrect information.This phenomenon can be originated from the models' reliance on the provided training data, without an inherent understanding of truth or the ability to verify facts against real-world developments (Bender et al., 2021;Ribeiro et al., 2020).Consequently, while LLMs can extrapolate trends and patterns, their predictive outputs may be compromised by inaccuracies or fabrications that may seem consistent but disconnected from reality.Moreover, the absence of real-time data integration further limits their forecasting reliability, as they cannot accommodate information after their training.These limitations require careful consideration and additional validation when using LLMs for forecasting purposes, highlighting the need for human oversight and the incorporation of current, domain-specific data.</p>
<p>Future research</p>
<p>The limitations identified call for future research that incorporates a broader spectrum of human forecasters, particularly those with extensive professional experience in the retail sector.Engaging experienced industry professionals would enhance the external validity of the findings and provide insights into how seasoned forecasters interact with and interpret the outputs of AI models like LLMs.Additionally, future studies should explore a wider range of forecasting scenarios, including those that mimic real-world complexities and uncertainties more closely.This approach would help to understand how both human forecasters and LLMs perform under various market conditions, such as fluctuating demand, unexpected market shifts, and significant events.By examining a more diverse set of scenarios, researchers can gain a deeper understanding of the strengths and weaknesses of both human and AI forecasters in different contexts.This research could explore the balance between AI's data-processing capabilities and human intuition and expertise, aiming to develop forecasting systems that leverage the strengths of both while mitigating their limitations.</p>
<p>To further extend the findings of this study, future research should explore a wider array of LLMs, including emerging models with potentially different capabilities and limitations.Alongside this, the application of advanced analytical techniques, such as machine learning algorithms capable of handling non-linear relationships and high-dimensional data, could be explored.In our experiment, we trained the LLMs with relatively small data that are specific to the case study, although the LLM models are trained offline on a massive amount of data.Future studies might consider developing specific LLMs with in-house data and rules to mimic human behavior and reasoning.Such a system can be used as a decision support systems where experts keep a log of their data including their adjustment, accuracy performance, and reasoning for adjustments, and call upon them in future when required.There is also a potential for developing prompt-based forecasting models that are able to build models on companies' in-house data and provide forecasts.Although this is a challenging task, the practice for simple models already exists.This approach would provide further understanding of how different AI models perform across a variety of forecasting tasks, offering insights into the evolution of AI in the field of forecasting.It will also contribute to the ongoing discourse on the integration of AI and human expertise in developing advanced forecasting systems and strategies.</p>
<p>Finally, future studies should explore different configurations of LLM interaction -from passive to highly interactive -to determine their effects on the range and variance of predictions.This is crucial for understanding whether LLMs might undermine the wisdom-of-the-crowd effect, which is vital in many forecasting contexts (Schoenegger et al., 2024b).Further research should also consider the long-term effects of repeated LLM use on human forecasting skills.Investigating whether reliance on LLMs leads to skill enhancement or degradation over time could inform how best to integrate LLMs in practice without diminishing human expertise (Schoenegger et al., 2024a).</p>
<p>Concluding remarks</p>
<p>This study's examination of the forecasting abilities of both human experts and LLMs in retail sales forecasting provided several key insights.The experiments, which involved human forecasters, primarily business school graduate students, and five leading LLMs including ChatGPT4, revealed that the performance of LLMs in forecasting is not uniformly superior to that of humans.While some models like ChatGPT4 and Bing demonstrated forecasting accuracies comparable to humans, others such as ChatGPT3.5 and Llama2 did not perform as well.These findings suggest that the effectiveness of LLMs in forecasting tasks varies depending on the specific model and the nature of the task at hand.</p>
<p>The study highlighted that advanced forecasting models do not automatically enhance forecasting performance.Both human forecasters and LLMs displayed limitations when working with these models, indicating that the integration of sophisticated models into forecasting requires careful consideration.This aligns with previous research suggesting that the complexity of advanced models can sometimes outweigh their potential benefits in accuracy.Furthermore, the research revealed that both human and AI forecasters struggle with the unpredictability of promotional periods, as evidenced by increased APE values during these times.This suggests a shared challenge in adapting to dynamic market conditions and underscores the need for forecasting models that can better handle such complexities.This study also points to the importance of further research to explore the integration of human judgment and advanced AI models in forecasting to optimize their combined strengths.The earlier promotions had a greater impact on your judgment about the forecast in period 25 5.02 5 1.45</p>
<p>The later promotions had a greater impact on your judgment about the forecast in period 25 5.23 5 1.23</p>
<p>All the promotions had an approximately the same impact on your judgment about the forecast in period 25 4.91 5 1.40</p>
<p>In my forecasts, I considered only those periods of the provided sales time series that reflected similar conditions to the forecast period (for example, upward or downward trend) 4.98 5 1.35</p>
<p>In my forecasts, I considered all periods of the provided sales time series regardless of their similarity to the conditions of the forecast period 5.10 5 1.33</p>
<p>In anticipation of the objective feedback on your performance, how would you assess the accuracy of your judgemental forecasts made during the experiment?(1 -Very low, 7 -Very high) 4.44 4 1.13</p>
<p>Figure 1 :
1
Figure 1: Interface of the experiment dashboard</p>
<p>Figure 2 :
2
Figure 2: APE by Forecaster Type: Human vs. LLMs.4.2 Enhanced Forecasting Performance with Advanced Forecast Models (H2)</p>
<p>Figure 3 :
3
Figure 3: Interaction: forecasters and statistical forecast type (Note: the results are predictive margins with 95% CI)</p>
<p>Figure 5 :
5
Figure 5: Interaction: Forecasters and external impact</p>
<p>Figure 6 :
6
Figure 6: Interaction: forecasters and treatment conditions</p>
<p>Further</p>
<p>research is essential to explore how various training datasets, model architectures, and contextual inputs influence the forecasting accuracy of different LLMs, especially in complex retail environments.Investigating these aspects could reveal critical insights into the specific characteristics that enhance the forecasting capabilities of LLMs.Future studies should investigate how these two approaches can complement each other most effectively, potentially through the development of hybrid forecasting models.Such models could dynamically adjust the contribution of human and AI elements based on specific conditions, optimizing accuracy and reliability in diverse forecasting scenarios.Building on the exploratory findings from Schoenegger et al. (2024a), which highlighted the potential of LLMs to significantly enhance human forecasting accuracy even when models are intentionally biased, future research should delve deeper into the mechanisms by which LLMs influence human judgment and decision-making in forecasting tasks.</p>
<p>Table 1 .
1
Treatment conditions tested in this study.
Treatment conditionPromotion Statistical ModelExternal Impact1YesBaselinePositive2YesAdvanced (promotional)Positive3YesBaselineNegative4YesAdvanced (promotional)Negative5NoBaselinePositive6NoBaselineNegative</p>
<p>Table 2
2
. mixed-effect model results for H1
APECoefficientStd. Err.P-valueFixed effectsIntercept14.35<strong><em>0.540.000Actual sales-0.00</em></strong>0.000.000Bard4.892.850.086Bing2.732.690.311ChatGPT3.514.77<strong>*2.800.000ChatGPT40.692.640.794Llama29.18</strong>2.920.002Random effectsEstimateStd. Err.Forecaster1.150.94Residual138.773.68Observations2973</p>
<p>Table 3 .
3
Mixed-effect model results for H2
APECoefficientStd. Err.P-valueFixed effectsIntercept14.075<strong><em>0.5650.000Actual sales-0.001</em></strong>0.0000.000Advanced forecast0.8070.4550.076Bard10.727<strong>3.5720.003Bing0.6653.3340.842ChatGPT3.510.247</strong>3.4460.003ChatGPT40.0473.2340.988Llama29.787<strong>3.5720.006advanced # Bard-14.643</strong>5.3860.007advanced # Bing5.2255.0440.300advanced # ChatGPT3.511.846*5.3030.025advanced # ChatGPT41.7044.9780.732advanced # Llama2-1.6455.6100.769Random effectsEstimateStd. Err.Forecaster1.160.93Residual138.243.67Observations2973</p>
<p>Table 4 .
4
Mixed-effect models results for H3
APECoefficientStd. Err.P-valueFixed effectsIntercept12.771<strong><em>0.5900.000Actual sales-0.001</em></strong>0.0000.000Yes promo3.295<strong><em>0.4810.000Bard-0.6954.5570.879Bing0.1984.5570.965ChatGPT3.5-3.1004.5570.496ChatGPT40.5614.5570.902Llama2-2.0614.5570.651yes promo # Bard8.8715.4520.104yes promo # Bing3.6325.2720.491yes promo # ChatGPT3.526.950</em></strong>5.3840.000yes promo # ChatGPT40.1365.2250.979yes promo # Llama218.149**5.5310.001Random effectsEstimateStd. Err.Forecaster1.420.94Residual134.253.57Observations2973
Figure 4: Interaction: forecasters and promotion</p>
<p>Table 5 .
5
Mixed-effect model results for H4
APECoefficientStd. Err.P-valueFixed effectsIntercept14.417<strong><em>0.7360.000Actual sales-0.001</em></strong>0.0000.000Positive (env. impact)-0.0900.5050.859Bard4.9343.5780.168Bing0.6543.5780.855ChatGPT3.514.523<strong><em>3.5780.000ChatGPT4-3.1583.4520.360Llama213.861</em></strong>3.7220.000positive # Bard-0.1105.3930.984positive # Bing4.3404.9360.379positive # ChatGPT3.50.5695.2120.913positive # ChatGPT48.3984.8450.083positive # Llama2-11.123*5.4900.043Random effectsEstimateStd. Err.Forecaster1.150.94Residual138.683.68Observations2973</p>
<p>Table 6 .
6
Mixed-effect model results for H5
APECoefficientStd. Err. P-value</p>
<p>The importance of the following information in supporting your judgement when presented with Baseline statistical forecast (1 -Not important, 7 -Very important):</p>
<p>APPENDIX F: Mean, Median, and Standard Deviation of Responses to the Post-Experimental
QuestionnaireQuestionMean MedianStd. Dev.Q1-Forthcoming promotion type5.5061.52Forthcoming external impact5.1861.63The uplift size of previous promotions in general5.3351.36The uplift size of previous similar promotions5.5061.38Time series (sales) history5.5061.41Baseline statistical forecast5.3061.41Frequency of promotions5.1151.44Q2-</p>
<p>The importance of the following information in supporting your judgement when presented with Advanced statistical forecast (1 -Not important, 7 -Very important):
Forthcoming promotion type5.6861.26Forthcoming External impact5.4661.41The uplift size of previous promotions in general5.4161.23The uplift size of previous similar promotions5.5961.25Time series (sales) history5.5861.43Advanced statistical forecast5.8261.17Frequency of promotions5.2051.39Q3-</p>
<p>To what extent do you agree with the following statements (1 -Strongly disagree, 7 - Strongly agree):</p>
<p>The task can be accessed here: https://mahdiforecasting.shinyapps.io/exp1/
Appendix A: Experiment guidelines to human forecastersDear participant,We would like to thank you for taking part in this experiment.The current experiment is designed to test a few key concepts in retail sales forecasting.All results and information are confidential and will be only used for research purposes.Please note, by taking part and submitting your results, you agree with the conditions of the study outlined below.You will be given 24 different sales series representing the actual sales of different products in a company.Each series has 24 observations, or periods, and you are asked to act as the sales manager and forecast sale period 25 based on the given information.We assume that all of the internal and external variables that contribute to sales are summarized to the given information.Your performance will be evaluated based on the accuracy of your forecast.Mean Absolute Percentage Error (MAPE) of the forecasts (which will be calculated automatically) is considered as the accuracy criteria.Based on the aggregated accuracy of your forecasts, at the end of the experiment you will receive a monetary reward.The higher the accuracy of your forecast, the higher is the reward that you will receive.The reward will not be higher than 20 AUD and not smaller than 5 AUD.Figure1shows a screenshot of the experiment interface.It represents the dashboard of the forecasting system that you will use to support your forecasting decisions.Figure 1: Interface of the experimentProvided Information:The given information includes time series observations (with relevant information such as promotion, and external impact for some existing time points), promotion type and external impact at period 25, and system statistical forecast (Baseline or advanced statistical forecast for all periods)."Promotion type" is one of the important variables that causes sales uplifts.The variable "Promotion" status can be Yes and No Pro, meaning that promotion exist and do not exist at the corresponding period.The size of promotion uplifts varies depending on the product as it is evident in the corresponding time series.The other variable is "External impact" which effectively summarises competitors' activity and whether that has impacted/ or will impact our sales "positively" or "negatively".This might be different during promotional and non-promotional periods."External impact" status can be either "Positive" or "Negative".Beside this information, you will be given a "statistical forecast".The statistical forecast may be of two types: "baseline forecast" or "advanced forecast".The "baseline statistical forecast" is the forecast generated by a software in the company, which uses "simple exponential smoothing" model and does not consider the possible impact of "promotion" and "external impact" on sales.Whereas the "advanced forecast" takes into account the promotion impact and uses a "dynamic linear regression" model to forecast sales.Forecast needed:You are asked to sit on the corresponding experiment based on your ID and then enter your ID, provided to you by the research facilitator at the beginning of the session.Please note, the failure to enter will not allow us to pay for your forecasting accuracy at the end of the experiment.You will then have to predict the value of the sales at period 25 for all given series and enter (type in) your forecast in the white text box "Your forecast: "on the top left of the screen and submit your forecast by pressing the green button "submit your forecast".Then, you can click on the "Next series" and move to the next time series.Please note that once you have submitted your forecast, you cannot go back and edit your forecast.There is no time limit and you can think about each forecast as much as you want.At the end, you are required to save the results by clicking on "save results".Once you save the results, a message will pop up as shown below.You can close the webpage and let the coordinator know.We would like to remind you again: please sit at your experiment as indicated with ID as you will be given payment between A$ 5 and A$ 20 based on the accuracy of your forecasts.Thank you again for participating and contributing to this research.Happy forecasting,The Research Team Appendix B: LLM prompts.As mentioned before, in our prompt we tried to provide the same information to LLM as it was given to participants.This includes using the same wording and keeping the same sequence of information according to the instruction given to students.We removed the irrelevant instructions from the handout.The information was given to LLM in table format, rather than time series plot as it was given to participants.The given prompt for all LLMs is as follows:We are going to run an experiment.The current experiment is designed to test a few key concepts in retail sales forecasting.All results and information are confidential and will be only used for research purposes.You will be given 24 different sales series representing the actual sales of different products in a company.Each series has 24 observations, or periods, and you are asked to act as the sales manager and forecast sale at period 25 based on the given information.We assume that all of the internal and external variables that contribute to sales are summarized to the given information.Your performance will be evaluated based on the accuracy of your forecast.Mean Absolute Percentage Error (MAPE) of the forecasts (which will be calculated automatically) is considered as the accuracy criteria.The given information includes time series observations (with relevant information such as promotion, and external impact for some existing time points), promotion type denoted as promo and external impact at period 25, and system statistical forecast (Baseline or advanced statistical forecast for all periods).You will be given the information in the following format: {time, actual_sale, baseline_forecast, advanced_forecast, promo, External_Impact} "Promotion type" is one of the important variables that causes sales uplifts.The variable "promo" status can be Yes and No Pro, meaning that promotion exist and do not exist at the corresponding period.The size of promotion uplifts varies depending on the product as it is evident in the corresponding time series.The other variable is "External-Impact" which effectively summarises competitors' activity and whether that has impacted/ or will impact our sales "positively" or "negatively".This might be different during promotional and non-promotional periods."External-Impact" status can be either "Positive" or "Negative".Beside this information, you will be given a "statistical forecast".The statistical forecast may be of two types: "baseline forecast" or "advanced forecast".The "baseline forecast" is the forecast generated by a software in the company, which uses "simple exponential smoothing" model and does not consider the possible impact of "promotion" and "external-impact" on sales.Whereas the "advanced forecast" takes into account the promotion impact and uses a "dynamic linear regression" model to forecast sales.Given the provided information, predict the value of the sales in period 25.APPENDIX C: Descriptive Statistics of experiment dataConditions
Demand forecasting in supply chain: The impact of demand volatility in the presence of promotion. M Abolghasemi, E Beh, G Tarr, R Gerlach, Computers &amp; Industrial Engineering. 1421063802020a</p>
<p>Demand forecasting in the presence of systematic events: Cases in capturing sales promotions. M Abolghasemi, J Hurley, A Eshragh, B Fahimnia, International Journal of Production Economics. 2020b. 107892</p>
<p>Machine learning applications in hierarchical time series forecasting: Investigating the impact of promotions. M Abolghasemi, G Tarr, C Bergmeir, International Journal of Forecasting. 2022in press</p>
<p>Development and validation of a rule-based time series complexity scoring technique to support design of adaptive forecasting DSS. M Adya, E J Lusk, Decision Support Systems. 832016</p>
<p>Expertise, credibility of system forecasts and integration methods in judgmental demand forecasting. J Alvarado-Valencia, L H Barrero, D √ñnkal, J T Dennerlein, International Journal of Forecasting. 3312017</p>
<p>Forecasting methods for marketing: Review of empirical research. J S Armstrong, R J Brodie, S H Mcintyre, International Journal of Forecasting. 33-41987</p>
<p>Principles of forecasting: a handbook for researchers and practitioners. J S Armstrong, Springer Science &amp; Business Media200130</p>
<p>Integrating human judgement into quantitative forecasting methods: A review. M Arvan, B Fahimnia, M Reisi, E Siemsen, Omega. 862019</p>
<p>On the dangers of stochastic parrots: Can language models be too big?. E M Bender, T Gebru, A Mcmillan-Major, S Shmitchell, Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. the 2021 ACM conference on fairness, accountability, and transparency2021, March</p>
<p>Context-sensitive heuristics in statistical reasoning. F Bolger, N Harvey, The Quarterly Journal of Experimental Psychology Section A. 4641993</p>
<p>The effects of feedback on judgmental interval predictions. F Bolger, D √ñnkal-Atay, International Journal of Forecasting. 2012004</p>
<p>Economic versus psychological forecasting. Evidence from consumer confidence surveys. M Bovi, Journal of Economic Psychology. 3042009</p>
<p>Demand planning for the digital supply chain: How to integrate human judgment and predictive analytics. R Brau, J Aloysius, E Siemsen, Journal of Operations Management. 6962023</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, . . Amodei, D , Advances in Neural Information Processing Systems. 202033</p>
<p>Negativity bias of analyst forecasts. Y Y Chang, W Hao, Journal of Behavioral Finance. 2322022</p>
<p>Y Chang, X Wang, J Wang, Y Wu, K Zhu, H Chen, . . Xie, X , arXiv:2307.03109A survey on evaluation of large language models. 2023arXiv preprint</p>
<p>Anchoring, activation, and the construction of values. G B Chapman, E J Johnson, Organizational Behavior and Human Decision Processes. 7921999</p>
<p>Using the general linear mixed model to analyse unbalanced repeated measures and longitudinal data. A Cnaan, N M Laird, P Slasor, Statistics in medicine. 16201997</p>
<p>PromoCast‚Ñ¢: A new forecasting method for promotion planning. L G Cooper, P Baron, W Levy, M Swisher, P Gogos, Marketing Science. 1831999</p>
<p>Forecasting from time series subject to sporadic perturbations: Effectiveness of different types of forecasting support. S De Baets, N Harvey, International Journal of Forecasting. 3422018</p>
<p>Using judgment to select and adjust forecasts from statistical models. S De Baets, N Harvey, European Journal of Operational Research. 38432020</p>
<p>Biases in judgmental adjustments of statistical forecasts: The role of individual differences. C Eroglu, K L Croxton, International Journal of Forecasting. 2612010</p>
<p>Efficient use of information in the formation of subjective industry forecasts. R Fildes, R Fildes, C Beard, International Journal of Operations &amp; Production Management. 1061991. 1992Journal of Forecasting</p>
<p>The organization and improvement of market forecasting. R Fildes, R Hastings, Journal of the Operational Research Society. 4511994</p>
<p>Against your better judgment? How organizations can improve their use of management judgment in forecasting. R Fildes, P Goodwin, Interfaces. 3762007</p>
<p>Forecasting and operational research: a review. R Fildes, K Nikolopoulos, S F Crone, A A Syntetos, Journal of the Operational Research Society. 5992008</p>
<p>Effective forecasting and judgmental adjustments: an empirical evaluation and strategies for improvement in supply-chain planning. R Fildes, P Goodwin, M Lawrence, K Nikolopoulos, International Journal of Forecasting. 2512009</p>
<p>Forecasting support systems: What we know, what we need to know. R Fildes, P Goodwin, International Journal of Forecasting. 2292013</p>
<p>Improving forecast quality in practice. R Fildes, F Petropoulos, Foresight: The International Journal of Applied Forecasting. 362015</p>
<p>Use and misuse of information in supply chain forecasting of promotion effects. R Fildes, P Goodwin, D √ñnkal, International Journal of Forecasting. 3512019</p>
<p>Retail forecasting: Research and practice. R Fildes, S Ma, S Kolassa, International Journal of Forecasting. 3842022</p>
<p>Forecast Value Added in Demand Planning. R Fildes, P Goodwin, S De Baets, Available at SSRN. 45587082023</p>
<p>Judgmental forecasts of time series affected by special events: Does providing a statistical forecast improve accuracy. P Goodwin, R Fildes, Journal of Behavioral Decision Making. 1211999</p>
<p>The process of using a forecasting support system. P Goodwin, R Fildes, M Lawrence, K Nikolopoulos, International Journal of Forecasting. 2332007</p>
<p>Forecasting with judgment. P Goodwin, R Fildes, The Palgrave Handbook of Operations Research. ChamSpringer International Publishing2022</p>
<p>The limits of forecasting methods in anticipating rare events. P Goodwin, G Wright, Technological Forecasting and Social Change. 7732010</p>
<p>Decision analysis for management judgment. P Goodwin, G Wright, 2014John Wiley &amp; Sons</p>
<p>Simple versus complex forecasting: The evidence. K C Green, J S Armstrong, Journal of Business Research. 6882015</p>
<p>Move over ANOVA: progress in analyzing repeated-measures data and its reflection in papers published in the archives of general psychiatry. R Gueorguieva, J H Krystal, Archives of General Psychiatry. 6132004</p>
<p>D Halawi, F Zhang, C Yueh-Han, J Steinhardt, arXiv:2402.18563Approaching Human-Level Forecasting with Language Models. 2024arXiv preprint</p>
<p>Why are judgments less consistent in less predictable task situations? Organizational Behavior and Human Decision Processes. N Harvey, 199563</p>
<p>Graphs versus tables: Effects of data presentation format on judgemental forecasting. N Harvey, F Bolger, International Journal of Forecasting. 1211996</p>
<p>Negative information weighs more heavily on the brain: the negativity bias in evaluative categorizations. T A Ito, J T Larsen, N K Smith, J T Cacioppo, Journal of personality and social psychology. 7548871998</p>
<p>Prospect Theory: An Analysis of Decision under Risk. D Kahneman, A Tversky, Econometrica. 4721979</p>
<p>Maps of bounded rationality: Psychology for behavioral economics. D Kahneman, American Economic Review. 9352003</p>
<p>Artificial Agents and Operations Management Decision-Making. S Kirshner, 10.2139/ssrn.47269332024</p>
<p>Computer-assisted decision making: Performance, beliefs, and the illusion of control. J E Kottemann, F D Davis, W E Remus, Organizational Behavior and Human Decision Processes. 5711994</p>
<p>Forecasting with multivariate temporal aggregation: The case of promotional modelling. N Kourentzes, F Petropoulos, International Journal of Production Economics. 1812016</p>
<p>The dynamics of judgemental adjustments in demand planning. N Kourentzes, R Fildes, Available at SSRN. 45347442023</p>
<p>The sum and its parts: Judgmental hierarchical forecasting. M Kremer, E Siemsen, D J Thomas, Management Science. 6292016</p>
<p>Random-effects models for longitudinal data. N M Laird, J H Ware, Biometrics. 1982</p>
<p>Judgmental forecasting: A review of progress over the last 25 years. M Lawrence, P Goodwin, M O'connor, D √ñnkal, International Journal of Forecasting. 2232006</p>
<p>Misinformation and its correction: Continued influence and successful debiasing. S Lewandowsky, U K Ecker, C M Seifert, N Schwarz, J Cook, Psychological Science in the Public Interest. 1332012</p>
<p>Experiments on forecasting behavior with several sources of information-A review of the literature. J Leitner, U Leopold-Wildburger, European Journal of Operational Research. 21332011</p>
<p>Judgemental adjustment of initial forecasts: Its effectiveness and biases. J S Lim, M O'connor, Journal of Behavioral Decision Making. 831995</p>
<p>Demand forecasting with high dimensional data: The case of SKU retail sales forecasting with intra-and inter-category promotional information. S Ma, R Fildes, T Huang, European Journal of Operational Research. 2492016</p>
<p>Large Language Models: Their Success and Impact. S Makridakis, F Petropoulos, Y Kang, 20235</p>
<p>Can anchoring explain biased forecasts? Experimental evidence. L Meub, T Proeger, Journal of Behavioral and Experimental Finance. 122016</p>
<p>Experimental behavioural research in operational research: What we know and what we might come to know. R M O'keefe, European Journal of Operational Research. 24932016</p>
<p>D √ñnkal, M S G√∂n√ºl, P Goodwin, Supporting Judgment in Predictive Analytics: Scenarios and Judgmental Forecasts. ChamSpringer International Publishing2023Judgment in Predictive Analytics</p>
<p>The human factor in supply chain forecasting: A systematic review. H N Perera, J Hurley, B Fahimnia, M Reisi, European Journal of Operational Research. 27422019</p>
<p>Bars, lines and points: The effect of graph format on judgmental forecasting. S Reimers, N Harvey, International Journal of Forecasting. 2023</p>
<p>M T Ribeiro, T Wu, C Guestrin, S Singh, arXiv:2005.04118Beyond accuracy: Behavioral testing of NLP models with CheckList. 2020arXiv preprint</p>
<p>Large Language Model Prediction Capabilities. P Schoenegger, P S Park, arXiv:2310.13014Evidence from a Real-World Forecasting Tournament. 2023arXiv preprint</p>
<p>P Schoenegger, P S Park, E Karger, P E Tetlock, arXiv:2402.07862AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy. 2024aarXiv preprint</p>
<p>Mastering the game of Go with deep neural networks and tree search. P Schoenegger, I Tuminauskaite, P S Park, P E Tetlock, D Huang, A Maddison, C J Guez, A Sifre, L Van Den Driessche, G , . . Hassabis, D , arXiv:2402.19379SilverWisdom of the silicon Crowd: Ensemble Prediction Capabilities Rival Human Crowd Accuracy. 2024b. 2016529arXiv preprint</p>
<p>Use of contextual and model-based information in adjusting promotional forecasts. A Sroginis, R Fildes, N Kourentzes, European Journal of Operational Research. 30732023</p>
<p>Nonlinear identification of judgmental forecasts effects at SKU level. J R Trapero, R Fildes, A Davydenko, Journal of Forecasting. 3052011</p>
<p>Analysis of judgmental adjustments in the presence of promotions. J R Trapero, D J Pedregal, R Fildes, N Kourentzes, International Journal of Forecasting. 2922013</p>
<p>On the identification of sales forecasting models in the presence of promotions. J R Trapero, N Kourentzes, R Fildes, Journal of the operational Research Society. 6622015</p>
<p>Judgment under Uncertainty: Heuristics and Biases: Biases in judgments reveal some heuristics of thinking under uncertainty. A Tversky, D Kahneman, Science. 18541571974</p>
<p>Loss aversion in riskless choice: A reference-dependent model. A Tversky, D Kahneman, The Quarterly Journal of Economics. 10641991</p>
<p>Linear models for the analysis of longitudinal studies. J H Ware, The American Statistician. 3921985</p>
<p>Forecasting support systems for the incorporation of event information: An empirical investigation. R Webby, M O'connor, B Edmundson, International Journal of Forecasting. 2132005</p>
<p>J Wei, Y Tay, R Bommasani, C Raffel, B Zoph, S Borgeaud, . . Fedus, W , arXiv:2206.07682Emergent abilities of large language models. 2022arXiv preprint</p>
<p>Decision making and planning under low levels of predictability: Enhancing the scenario method. G Wright, P Goodwin, International Journal of Forecasting. 2542009</p>
<p>Leveraging language foundation models for human mobility forecasting. H Xue, B P Voutharoja, F D Salim, Proceedings of the 30th International Conference on Advances in Geographic Information Systems. the 30th International Conference on Advances in Geographic Information Systems2022, November</p>
<p>Translating human mobility forecasting through natural language generation. H Xue, F D Salim, Y Ren, C L Clarke, Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining. the Fifteenth ACM International Conference on Web Search and Data Mining2022. February</p>
<p>X Yu, Z Chen, Y Ling, S Dong, Z Liu, Y Lu, arXiv:2306.11025Temporal Data Meets LLM--Explainable Financial Time Series Forecasting. 2023arXiv preprint</p>
<p>H Zhao, Z Liu, Z Wu, Y Li, T Yang, P Shu, . . Liu, T , arXiv:2401.11641Revolutionizing finance with llms: An overview of applications and insights. 2024arXiv preprint</p>
<p>Forecasting with artificial neural networks: The state of the art. G Zhang, B E Patuwo, M Y Hu, International Journal of Forecasting. 1411998</p>
<p>Demand forecasting with supply-chain information and machine learning: Evidence in the pharmaceutical industry. X Zhu, A Ninh, H Zhao, Z Liu, 2021Production and Operations Management30</p>
<p>Forecasting future world events with neural networks. A Zou, T Xiao, R Jia, J Kwon, M Mazeika, R Li, . . Hendrycks, D , Advances in Neural Information Processing Systems. 202235</p>            </div>
        </div>

    </div>
</body>
</html>
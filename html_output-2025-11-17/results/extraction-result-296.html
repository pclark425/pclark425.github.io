<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-296 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-296</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-296</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-af385c0fdd0eda2bbf429bea6fedffc327c8a180</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/af385c0fdd0eda2bbf429bea6fedffc327c8a180" target="_blank">Randomized Positional Encodings Boost Length Generalization of Transformers</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work demonstrates that this failure mode is linked to positional encodings being out-of-distribution for longer sequences (even for relative encoding) and introduces a novel family of positional encodes that can overcome this problem.</p>
                <p><strong>Paper Abstract:</strong> Transformers have impressive generalization capabilities on tasks with a fixed context length. However, they fail to generalize to sequences of arbitrary length, even for seemingly simple tasks such as duplicating a string. Moreover, simply training on longer sequences is inefficient due to the quadratic computation complexity of the global attention mechanism. In this work, we demonstrate that this failure mode is linked to positional encodings being out-of-distribution for longer sequences (even for relative encodings) and introduce a novel family of positional encodings that can overcome this problem. Concretely, our randomized positional encoding scheme simulates the positions of longer sequences and randomly selects an ordered subset to fit the sequence’s length. Our large-scale empirical evaluation of 6000 models across 15 algorithmic reasoning tasks shows that our method allows Transformers to generalize to sequences of unseen length (increasing test accuracy by 12.0% on average).</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e296.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e296.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Binary Addition</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Binary Addition (algorithmic reasoning task)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Binary addition task from the Delétang et al. (2023) algorithmic benchmark: models must add binary numbers presented in the input sequence; used to evaluate length generalization of Transformers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Encoder-only Transformer (seq-to-seq style used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>249,026 parameters (270,146 for relative positional encodings)</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>encoder-only Transformer; 5 encoder blocks, 8 attention heads per block, d_model = 64</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>binary addition</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Models trained on sequences sampled uniformly up to length N=40 and evaluated on unseen longer lengths 41..500; interventions are positional-encoding variants, in particular Randomized Positional Encodings (RPE) applied to sin/cos, relative, ALiBi, RoPE, and learned encodings; comparisons to baseline encodings (none, standard sin/cos, relative, ALiBi, RoPE, learned) and to geometric attention (directional encoding).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Reported accuracies averaged over unseen test lengths (41..500) and maximized over 10 seeds and 3 learning rates. Baselines: None 50.1%, sin/cos 49.8%, relative 54.3%, ALiBi 51.4%, RoPE 50.4%, learned 49.8%. Randomized variants (this work): randomized sin/cos 64.4%, randomized relative 64.5%, randomized ALiBi 56.2%, randomized RoPE 60.2%, randomized learned* 61.7%. (Values from Table 1, maximized over seeds / learning rates.)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Main mechanism of failure for baseline models is out-of-distribution (OOD) positional encodings when sequence length exceeds training max; RPE prevents OOD positional-encoding distributions by exposing the model during training to a wide range of positional vectors (preserving order), which (i) yields overlapping layer activations between training and longer test lengths (PCA analysis), and (ii) preserves characteristic attention patterns (the 'X' reversal pattern) on longer sequences. The paper also finds that only relative order matters (sampling must be sorted); absolute position values are not required for generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Performance for baseline positional encodings degrades as test sequence length grows beyond training length N=40; RPE trained on short sequences (N=40) attains similar test accuracy as models trained on long sequences (up to M=500) but trains much faster (authors report ~168.4 steps/s for RPE vs ~22.1 steps/s for naive training on length 500 on a V100 — roughly ~35x speed-up in wall-clock to reach high test accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Baseline failure mode is primarily positional-encoding distribution shift: attention patterns that implement the algorithm (e.g., reversal required for addition formatting) break down for longer sequences, and intermediate-layer activations become OOD (notably layers 3 and 4). Without preserving order (i.e., unsorted sampled positions) the model fails to solve length-generalized tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared randomized encodings vs: no positional encoding, standard sin/cos, relative (Dai et al.), ALiBi (Press et al.), RoPE, learned positional embeddings, and label-based encodings; also compared to geometric attention + directional encodings (Csordás et al.). Also compared training on short sequences with RPE vs training directly on long sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Randomized positional encodings substantially improve length generalization for binary addition (raising accuracy from ~50–54% to ~64% in the best randomized variants) by eliminating OOD positional encodings and preserving the attention/activation patterns that implement the algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Randomized Positional Encodings Boost Length Generalization of Transformers', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e296.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e296.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Binary Multiplication</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Binary Multiplication (algorithmic reasoning task)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Binary multiplication task from the algorithmic benchmark: models multiply binary numbers represented in the input sequence; used to probe Transformers' algorithmic reasoning and length generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Encoder-only Transformer (seq-to-seq style used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>249,026 parameters (270,146 for relative positional encodings)</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>encoder-only Transformer; 5 encoder blocks, 8 attention heads per block, d_model = 64</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>binary multiplication</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Same setup as other tasks: training on sequence lengths up to N=40, test lengths up to M=500; evaluated positional-encoding interventions including randomized positional encodings applied to sin/cos, relative, ALiBi, RoPE, learned.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Maximized accuracies (Table 1): Baselines: None 49.9%, sin/cos 50.1%, relative 52.2%, ALiBi 51.0%, RoPE 50.2%, learned 49.6%. Randomized variants: randomized sin/cos 52.1%, randomized relative 50.1%, randomized ALiBi 50.5%, randomized RoPE 51.7%, randomized learned* 51.9%. Gains from RPE were modest or task-dependent for multiplication.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Same core insight as for addition: positional-encoding distribution shift harms length generalization; randomized encodings mitigate OOD activations and can preserve learned attention-based computation. However, for binary multiplication the randomized schemes produced only small improvements in many variants, suggesting multiplication may be more sensitive or harder to capture with the small encoder used.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Performance generally low (~50% random baseline) for many encodings; relative encoding gave a modest baseline improvement (52.2%), but randomized encodings did not consistently produce large gains like for addition. No discussion of scaling with model parameter count.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Near-random performance for many baseline encodings on unseen longer sequences; indicates failure to learn an algorithmic, length-general multiplication strategy under several encodings and model capacity used.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared across positional encodings (none, sin/cos, relative, ALiBi, RoPE, learned) and randomized variants; performance reported as above.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Binary multiplication is harder to length-generalize than addition in this experimental setup: randomized positional encodings yield only small or inconsistent improvements (accuracy remains near random for several variants), highlighting task-specific difficulty and sensitivity to encoding/intervention.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Randomized Positional Encodings Boost Length Generalization of Transformers', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e296.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e296.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Modular Arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Modular Arithmetic (simple and general variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Modular arithmetic tasks from the benchmark including a 'simple' variant and a more general modular-arithmetic recognition task, used to probe locality and positional dependence in algorithmic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Encoder-only Transformer (seq-to-seq style used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>249,026 parameters (270,146 for relative positional encodings)</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>encoder-only Transformer; 5 encoder blocks, 8 attention heads per block, d_model = 64</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>modular arithmetic (Simple variant and general modular arithmetic recognition)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Training on sequence lengths up to N=40, evaluating on lengths 41..500; compared baseline positional encodings and randomized positional encodings (RPE) applied to various encoding families; also compared to geometric attention + directional encoding which has an explicit locality bias.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Simple Modular Arithmetic (Table 1, maximized): Baselines: None 20.1%, sin/cos 20.5%, relative 21.8%, ALiBi 24.2%, RoPE 21.6%, learned 20.2%. Randomized variants: randomized sin/cos 25.7%, randomized relative 28.1%, randomized ALiBi 21.2%, randomized RoPE 25.5%, randomized learned* 21.1%. General Modular Arithmetic (Table 1): Baselines: None 31.0%, sin/cos 28.3%, relative 30.3%, ALiBi 32.5%, RoPE 25.1%, learned 25.1%. Randomized variants: randomized sin/cos 33.8%, randomized relative 34.9%, randomized ALiBi 31.3%, randomized RoPE 32.7%, randomized learned* 31.9%. Randomized relative encoding gives the best gains in the general modular arithmetic task.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Authors note that geometric attention / directional encodings (which emphasize locality) outperform randomized encodings on the Simple Modular Arithmetic task due to an inherent locality bias (numbers closer to operation symbols are more relevant), indicating that different inductive biases (locality vs global order) affect arithmetic solving strategies. Overall, RPE mitigates OOD positional encodings enabling better generalization when the algorithm depends on ordered positions rather than strict locality.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>RPE improves length generalization relative to baselines across modular tasks; geometric attention can be superior on tasks with locality structure. No explicit scaling with model size is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Tasks with strong locality bias may not benefit as much from RPE; baseline learned positional embeddings fail because positions beyond training max remain untrained (random), causing near-random performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared randomized encodings vs standard encodings and vs geometric attention/directional encodings; geometric attention notably outperformed other methods on Modular Arithmetic (Simple) owing to locality bias.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Randomized positional encodings improve modular-arithmetic length generalization (best with randomized relative encoding), but tasks with strong locality structure may be better served by approaches (like geometric attention) that encode locality biases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Randomized Positional Encodings Boost Length Generalization of Transformers', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural networks and the chomsky hierarchy <em>(Rating: 2)</em></li>
                <li>Train short, test long: Attention with linear biases enables input length extrapolation <em>(Rating: 1)</em></li>
                <li>Systematic generalization and emergent structures in transformers trained on structured tasks <em>(Rating: 1)</em></li>
                <li>The neural data router: Adaptive control flow in transformers improves systematic generalization <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-296",
    "paper_id": "paper-af385c0fdd0eda2bbf429bea6fedffc327c8a180",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "Binary Addition",
            "name_full": "Binary Addition (algorithmic reasoning task)",
            "brief_description": "Binary addition task from the Delétang et al. (2023) algorithmic benchmark: models must add binary numbers presented in the input sequence; used to evaluate length generalization of Transformers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Encoder-only Transformer (seq-to-seq style used in experiments)",
            "model_size": "249,026 parameters (270,146 for relative positional encodings)",
            "model_architecture": "encoder-only Transformer; 5 encoder blocks, 8 attention heads per block, d_model = 64",
            "arithmetic_operation_type": "binary addition",
            "number_range_or_complexity": null,
            "method_or_intervention": "Models trained on sequences sampled uniformly up to length N=40 and evaluated on unseen longer lengths 41..500; interventions are positional-encoding variants, in particular Randomized Positional Encodings (RPE) applied to sin/cos, relative, ALiBi, RoPE, and learned encodings; comparisons to baseline encodings (none, standard sin/cos, relative, ALiBi, RoPE, learned) and to geometric attention (directional encoding).",
            "performance_result": "Reported accuracies averaged over unseen test lengths (41..500) and maximized over 10 seeds and 3 learning rates. Baselines: None 50.1%, sin/cos 49.8%, relative 54.3%, ALiBi 51.4%, RoPE 50.4%, learned 49.8%. Randomized variants (this work): randomized sin/cos 64.4%, randomized relative 64.5%, randomized ALiBi 56.2%, randomized RoPE 60.2%, randomized learned* 61.7%. (Values from Table 1, maximized over seeds / learning rates.)",
            "mechanistic_insight": "Main mechanism of failure for baseline models is out-of-distribution (OOD) positional encodings when sequence length exceeds training max; RPE prevents OOD positional-encoding distributions by exposing the model during training to a wide range of positional vectors (preserving order), which (i) yields overlapping layer activations between training and longer test lengths (PCA analysis), and (ii) preserves characteristic attention patterns (the 'X' reversal pattern) on longer sequences. The paper also finds that only relative order matters (sampling must be sorted); absolute position values are not required for generalization.",
            "performance_scaling": "Performance for baseline positional encodings degrades as test sequence length grows beyond training length N=40; RPE trained on short sequences (N=40) attains similar test accuracy as models trained on long sequences (up to M=500) but trains much faster (authors report ~168.4 steps/s for RPE vs ~22.1 steps/s for naive training on length 500 on a V100 — roughly ~35x speed-up in wall-clock to reach high test accuracy).",
            "failure_modes": "Baseline failure mode is primarily positional-encoding distribution shift: attention patterns that implement the algorithm (e.g., reversal required for addition formatting) break down for longer sequences, and intermediate-layer activations become OOD (notably layers 3 and 4). Without preserving order (i.e., unsorted sampled positions) the model fails to solve length-generalized tasks.",
            "comparison_baseline": "Compared randomized encodings vs: no positional encoding, standard sin/cos, relative (Dai et al.), ALiBi (Press et al.), RoPE, learned positional embeddings, and label-based encodings; also compared to geometric attention + directional encodings (Csordás et al.). Also compared training on short sequences with RPE vs training directly on long sequences.",
            "key_finding": "Randomized positional encodings substantially improve length generalization for binary addition (raising accuracy from ~50–54% to ~64% in the best randomized variants) by eliminating OOD positional encodings and preserving the attention/activation patterns that implement the algorithm.",
            "uuid": "e296.0",
            "source_info": {
                "paper_title": "Randomized Positional Encodings Boost Length Generalization of Transformers",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Binary Multiplication",
            "name_full": "Binary Multiplication (algorithmic reasoning task)",
            "brief_description": "Binary multiplication task from the algorithmic benchmark: models multiply binary numbers represented in the input sequence; used to probe Transformers' algorithmic reasoning and length generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Encoder-only Transformer (seq-to-seq style used in experiments)",
            "model_size": "249,026 parameters (270,146 for relative positional encodings)",
            "model_architecture": "encoder-only Transformer; 5 encoder blocks, 8 attention heads per block, d_model = 64",
            "arithmetic_operation_type": "binary multiplication",
            "number_range_or_complexity": null,
            "method_or_intervention": "Same setup as other tasks: training on sequence lengths up to N=40, test lengths up to M=500; evaluated positional-encoding interventions including randomized positional encodings applied to sin/cos, relative, ALiBi, RoPE, learned.",
            "performance_result": "Maximized accuracies (Table 1): Baselines: None 49.9%, sin/cos 50.1%, relative 52.2%, ALiBi 51.0%, RoPE 50.2%, learned 49.6%. Randomized variants: randomized sin/cos 52.1%, randomized relative 50.1%, randomized ALiBi 50.5%, randomized RoPE 51.7%, randomized learned* 51.9%. Gains from RPE were modest or task-dependent for multiplication.",
            "mechanistic_insight": "Same core insight as for addition: positional-encoding distribution shift harms length generalization; randomized encodings mitigate OOD activations and can preserve learned attention-based computation. However, for binary multiplication the randomized schemes produced only small improvements in many variants, suggesting multiplication may be more sensitive or harder to capture with the small encoder used.",
            "performance_scaling": "Performance generally low (~50% random baseline) for many encodings; relative encoding gave a modest baseline improvement (52.2%), but randomized encodings did not consistently produce large gains like for addition. No discussion of scaling with model parameter count.",
            "failure_modes": "Near-random performance for many baseline encodings on unseen longer sequences; indicates failure to learn an algorithmic, length-general multiplication strategy under several encodings and model capacity used.",
            "comparison_baseline": "Compared across positional encodings (none, sin/cos, relative, ALiBi, RoPE, learned) and randomized variants; performance reported as above.",
            "key_finding": "Binary multiplication is harder to length-generalize than addition in this experimental setup: randomized positional encodings yield only small or inconsistent improvements (accuracy remains near random for several variants), highlighting task-specific difficulty and sensitivity to encoding/intervention.",
            "uuid": "e296.1",
            "source_info": {
                "paper_title": "Randomized Positional Encodings Boost Length Generalization of Transformers",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Modular Arithmetic",
            "name_full": "Modular Arithmetic (simple and general variants)",
            "brief_description": "Modular arithmetic tasks from the benchmark including a 'simple' variant and a more general modular-arithmetic recognition task, used to probe locality and positional dependence in algorithmic reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Encoder-only Transformer (seq-to-seq style used in experiments)",
            "model_size": "249,026 parameters (270,146 for relative positional encodings)",
            "model_architecture": "encoder-only Transformer; 5 encoder blocks, 8 attention heads per block, d_model = 64",
            "arithmetic_operation_type": "modular arithmetic (Simple variant and general modular arithmetic recognition)",
            "number_range_or_complexity": null,
            "method_or_intervention": "Training on sequence lengths up to N=40, evaluating on lengths 41..500; compared baseline positional encodings and randomized positional encodings (RPE) applied to various encoding families; also compared to geometric attention + directional encoding which has an explicit locality bias.",
            "performance_result": "Simple Modular Arithmetic (Table 1, maximized): Baselines: None 20.1%, sin/cos 20.5%, relative 21.8%, ALiBi 24.2%, RoPE 21.6%, learned 20.2%. Randomized variants: randomized sin/cos 25.7%, randomized relative 28.1%, randomized ALiBi 21.2%, randomized RoPE 25.5%, randomized learned* 21.1%. General Modular Arithmetic (Table 1): Baselines: None 31.0%, sin/cos 28.3%, relative 30.3%, ALiBi 32.5%, RoPE 25.1%, learned 25.1%. Randomized variants: randomized sin/cos 33.8%, randomized relative 34.9%, randomized ALiBi 31.3%, randomized RoPE 32.7%, randomized learned* 31.9%. Randomized relative encoding gives the best gains in the general modular arithmetic task.",
            "mechanistic_insight": "Authors note that geometric attention / directional encodings (which emphasize locality) outperform randomized encodings on the Simple Modular Arithmetic task due to an inherent locality bias (numbers closer to operation symbols are more relevant), indicating that different inductive biases (locality vs global order) affect arithmetic solving strategies. Overall, RPE mitigates OOD positional encodings enabling better generalization when the algorithm depends on ordered positions rather than strict locality.",
            "performance_scaling": "RPE improves length generalization relative to baselines across modular tasks; geometric attention can be superior on tasks with locality structure. No explicit scaling with model size is reported.",
            "failure_modes": "Tasks with strong locality bias may not benefit as much from RPE; baseline learned positional embeddings fail because positions beyond training max remain untrained (random), causing near-random performance.",
            "comparison_baseline": "Compared randomized encodings vs standard encodings and vs geometric attention/directional encodings; geometric attention notably outperformed other methods on Modular Arithmetic (Simple) owing to locality bias.",
            "key_finding": "Randomized positional encodings improve modular-arithmetic length generalization (best with randomized relative encoding), but tasks with strong locality structure may be better served by approaches (like geometric attention) that encode locality biases.",
            "uuid": "e296.2",
            "source_info": {
                "paper_title": "Randomized Positional Encodings Boost Length Generalization of Transformers",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural networks and the chomsky hierarchy",
            "rating": 2
        },
        {
            "paper_title": "Train short, test long: Attention with linear biases enables input length extrapolation",
            "rating": 1
        },
        {
            "paper_title": "Systematic generalization and emergent structures in transformers trained on structured tasks",
            "rating": 1
        },
        {
            "paper_title": "The neural data router: Adaptive control flow in transformers improves systematic generalization",
            "rating": 1
        }
    ],
    "cost": 0.014561749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Randomized Positional Encodings Boost Length Generalization of Transformers</h1>
<p>Anian Ruoss ${ }^{<em> 1}$ Grégoire Delétang ${ }^{</em> 1}$<br>Tim Genewein ${ }^{1}$ Jordi Grau-Moya ${ }^{1}$<br>Róbert Csordás ${ }^{12}$ Mehdi Bennani ${ }^{1}$<br>Shane Legg ${ }^{1}$ Joel Veness ${ }^{1}$<br>Standard Positional Encoding<br>Training 12 N<br>Evaluation 12 N<br>Out-of-distribution<br>Randomized Positional Encodings (ours)<br>Training 12 N 1<br>Evaluation 12 N<br>Random positional encoding vector has values larger than those observed during training. Our approach avoids this problem by assigning a random (ordered) positional encoding vector using the full range of possible test positions to each training example.</p>
<h2>1 Introduction</h2>
<p>Transformers are emerging as the new workhorse of machine learning as they underpin many recent breakthroughs, including sequence-to-sequence modeling (Vaswani et al., 2017), image recognition (Dosovitskiy et al., 2021), and multi-task learning (Reed et al., 2022). However, recent work (Delétang et al., 2023) demonstrated that Transformers fail to generalize to longer sequences on seemingly simple tasks such as binary addition. Thus, while certain problems can be solved without length generalization, algorithmic reasoning generally requires this ability, similar to many real-world settings such as online or continual learning.</p>
<p>While the Transformer's attention mechanism can recognize complex relationships amongst to-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>kens in the input sequence, it is limited by its lack of positional awareness. Thus, the input sequence is generally augmented with positional encodings to inject position information into the computation. However, current approaches only consider positions up to the maximum training sequence length $N$, and thus all the positions $N+1, \ldots, M$ for test sequences of length up to $M$ will appear out-ofdistribution during evaluation (top of Fig. 1).</p>
<p>This work We introduce a novel family of randomized positional encodings, which significantly improves Transformers' length generalization capabilities on algorithmic reasoning tasks. Our approach is compatible with any existing positional encoding scheme and augments the existing methods by subsampling an ordered set of positions from a much larger range of positions than those observed during training or evaluation (i.e., up</p>
<p>to $L \gg M$; bottom of Fig. 1). Thus, over the course of training, the Transformer will learn to handle very large positional encodings and, therefore no longer encounter out-of-distribution inputs during evaluation. Importantly, our method leaves in-domain generalization performance unaffected and is also significantly more efficient than the naive approach of simply training the Transformer on longer sequences. Our main contributions are:</p>
<ul>
<li>A novel family of positional encoding schemes that significantly improves the length generalization capabilities of Transformers, while leaving their in-domain generalization performance unaffected.</li>
<li>A large-scale empirical evaluation on a wide range of algorithmic reasoning tasks showing the superiority of our method over prior work (an increase of the test accuracy by $12.0 \%$ on average and up to $43.5 \%$ on certain tasks).</li>
<li>An open-source implementation of our method, available at https://github. com/deepmind/randomized_ positional_encodings.</li>
</ul>
<h2>2 Related Work</h2>
<p>Our work is most closely related to the growing line of research on Transformers' positional encodings. The first approaches simply added a transformation of the tokens' positions, e.g., scaled sinusoids (Vaswani et al., 2017) or learned embeddings (Gehring et al., 2017), to the embeddings of the input sequence. Dai et al. (2019) subsequently showed that computing the attention (at every layer) using the relative distances between the key and query vectors improves the modeling of long-term (inter-context) dependencies. Similarly, Su et al. (2021) proposed to inject position information by rotating the key-query products according to their relative distances. Finally, Press et al. (2022) improved the length generalization on natural language processing tasks by adding a constant bias to each key-query attention score (proportional to their distance). However, as our experiments in Section 4 will show, these approaches fail at length generalization on algorithmic reasoning tasks, which is precisely the goal of our work.</p>
<p>A concurrent work developed randomized learned positional encodings ( Li and McClelland, 2022), which are a special case of our family of randomized positional encodings. We also note that
the necessity of feature and position randomization for length generalization has been discussed in the context of graph neural networks, which subsume Transformers (Ibarz et al., 2022; Sato et al., 2021). Finally, Liu et al. (2020b) proposed to model the position information as a continuous dynamical system in an effort to handle sequences longer than those seen during training time.</p>
<p>Our work is also related to the research area on improving the systematic (length) generalization capabilities of Transformers (Ontañón et al., 2022), which includes approaches investigating embedding scaling or early stopping (Csordás et al., 2021), adaptive computation time (Dehghani et al., 2019), geometric attention with directional positional encodings and gating (Csordás et al., 2022), and hierarchical reinforcement learning (Liu et al., 2020a). Such length generalization studies are often conducted in the context of formal language theory, and we evaluate our method on the recent benchmark by Delétang et al. (2023), which unifies a large body of work on Transformers' capability to recognize formal languages (Ackerman and Cybenko, 2020; Bhattamishra et al., 2020; Ebrahimi et al., 2020; Hahn, 2020; Hao et al., 2022; Merrill, 2019; Merrill and Sabharwal, 2022).</p>
<h2>3 Randomized Positional Encodings</h2>
<p>Unlike RNNs (Elman, 1990), which are unrolled over tokens one step at a time, Transformers process large chunks of the input sequence in parallel via global attention (Vaswani et al., 2017). As a result, Transformers do not need to "remember" previous tokens, but they do have to break the permutation-invariance of the attention mechanism. To that end, the embeddings of the input sequence are generally augmented with positional encodings. For example, the vanilla Transformer adds the following positional encodings to the embedded input sequence before passing it to the attention layers:</p>
<p>$$
\begin{aligned}
\mathrm{PE}(\text { pos, } 2 i) &amp; =\sin \left(\frac{\text { pos }}{10000^{\frac{2 i}{2 \text { model }}}}\right) \
\mathrm{PE}(\text { pos, } 2 i+1) &amp; =\cos \left(\frac{\text { pos }}{10000^{\frac{2 i}{2 \text { model }}}}\right)
\end{aligned}
$$</p>
<p>where pos is the token's position in the sequence, $d_{\text {model }} \in \mathbb{N}$ is the dimension of the input embedding, and $i \in\left{1,2, \ldots, d_{\text {model }} / 2\right}$.</p>
<p>While positional encodings generally succeed at inducing the required positional information</p>
<p>for sequences of fixed length, they are one of the main failure modes preventing length generalization. Concretely, for a Transformer with standard positional encodings trained on a curriculum of sequences of maximum length $N$, test sequences of length $M&gt;N$ will shift the distribution of the resultant positional encodings away from those seen in training, with the shift getting increasingly large as $M$ grows. To address this, we propose a randomized encoding scheme, which relies only on order information, and can be expected to generalize up to sequences of length $M$, where $N&lt;M \leq L$, with a configurable hyperparameter $L$.</p>
<p>Randomized positional encodings We assume that each training step will perform a step of loss minimization on a batch of data of fixed size. Let $\mathcal{U}(S)$ denote the discrete uniform distribution over set $S$, and let $P_{k}:={S \subseteq{1, \ldots, L}| | S \mid=k}$. For each training step, we first sample a random length $n \sim \mathcal{U}({1, \ldots, N})$ (following Delétang et al., 2023) and then a random set of indices $I \sim$ $\mathcal{U}\left(P_{n}\right)$. We then sort $I$ in ascending order, such that $I=\left{i_{1}, \ldots, i_{n}\right}$ for $i_{1}&lt;i_{2}&lt;\cdots<i_{n}$, noting that $I$ is sampled without replacement. Finally, we compute our randomized positional encoding for token $1 \leq j \leq N$ as $\operatorname{RPE}(j, \cdot):=\operatorname{PE}\left(i_{j}, \cdot\right)$. At test time, when processing a sequence of length $M>N$, we use the same procedure but for all token positions $1 \leq j \leq M$. The intuition behind our method is to preserve the known good properties of relative encoding but in a way that is independent of the maximum training length $N$ and thus allows generalization to longer sequences at test time.</p>
<p>When applying our randomized positional encoding scheme, we subsample the extended positions only once per batch and not individually for every sequence. For the $\sin / \cos$ (Vaswani et al., 2017), learned (Gehring et al., 2017), and RoPE encodings (Su et al., 2021), we apply our method as described above, i.e., we directly replace the original token positions with their sampled counterpart. For the relative encoding (Dai et al., 2019), we compute the relative distances between the sampled positions instead of the original positions. Finally, for ALiBi (Press et al., 2022), we sample the bias values from the set of extended positions.</p>
<p>As a consequence, our tokens' positional encodings are no longer directly related to their exact position (the encodings even change during training as they are resampled at every step). However, since we maintain the order of the encodings, the</p>
<p>Transformer can still learn to extract the relevant positional information from the subsampled encodings. Indeed, we validate the necessity of ordering the sampled positions in our ablation study in Appendix B.1. Thus, the success of our encoding scheme offers an interesting insight into the inductive biases of the Transformer architecture.</p>
<p>As we will show in Section 4, our randomized encodings trained only on lengths up to $N$ perform the same on sequences of length $M$ as prior approaches trained on lengths up to $M$. Therefore, our method demonstrates that Transformers can be efficiently trained on short sequences as long as (i) the longer sequences share the same structure and (ii) the longer positions are observed during training. Moreover, as the running time of global attention is $\mathcal{O}\left(\ell^{2}\right)$ for sequence length $\ell$, our encoding scheme is significantly faster than directly training a model on long sequences. Furthermore, we also note that our randomized positional encoding scheme significantly boosts length generalization while leaving the in-domain generalization performance largely unaffected (see Fig. 4).</p>
<p>The main limitation of our approach is that the maximum test sequence length $M$ has to be known in advance to choose $L \gg M$. However, our method is compatible with a wide range of values for $L$ (see Appendix B.1), and we note that this is a much weaker assumption than that required for the naive approach of simply training on longer sequences. However, note that if $L$ is chosen to be much larger than $N$ or $M$, it is theoretically unlikely for the model to encounter enough unique indices during training, likely leading to poor performance (both in- and out-of-distribution).</p>
<h2>4 Experimental Evaluation</h2>
<p>Problem setup We closely follow the experiment setup of Delétang et al. (2023) and evaluate our method on a wide range of algorithmic reasoning tasks such as modular arithmetic, reversing/duplicating a string, binary addition/multiplication, and bucket sort. The tasks are derived from formal language recognition and thus grouped according to the Chomsky hierarchy (Chomsky, 1956), which partitions languages into regular (R), context-free, context-sensitive (CS), and recursively enumerable. Regular tasks can be solved by a finite-state automaton (FSA), deterministic context-free (DCF) tasks can be solved by an FSA with access to a deterministic stack, and</p>
<p>Table 1: Accuracy (in percentage) averaged over all test lengths and maximized over 10 random seeds and 3 learning rates. The random accuracy is $50 \%$, except for Modular Arithmetic (Simple), Cycle Navigation, Bucket Sort, and Modular Arithmetic, where it is $20 \%$. Our randomized method increases the test accuracy by $12.0 \%$ on average. The randomized learned encodings (denoted with $*$ ) are equivalent to label-based encodings ( Li and McClelland, 2022). $\dagger$ denotes permutation-invariant tasks, which can be solved without positional information.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Level</th>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">None</th>
<th style="text-align: center;">$\sin / \cos$</th>
<th style="text-align: center;">Relative</th>
<th style="text-align: center;">ALiBi</th>
<th style="text-align: center;">RoPE</th>
<th style="text-align: center;">Learned</th>
<th style="text-align: center;">Randomized (Ours)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sin / \cos$</td>
<td style="text-align: center;">Relative</td>
<td style="text-align: center;">ALiBi</td>
<td style="text-align: center;">RoPE</td>
<td style="text-align: center;">Learned*</td>
</tr>
<tr>
<td style="text-align: center;">R</td>
<td style="text-align: center;">EVEN PARS</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">96.4</td>
<td style="text-align: center;">67.3</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">50.7</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">81.5</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">97.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Modular Arithmetic (Simple)</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">21.8</td>
<td style="text-align: center;">24.2</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">20.2</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">21.2</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">21.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PARITY CHECK ${ }^{\dagger}$</td>
<td style="text-align: center;">51.9</td>
<td style="text-align: center;">50.5</td>
<td style="text-align: center;">51.8</td>
<td style="text-align: center;">51.7</td>
<td style="text-align: center;">51.3</td>
<td style="text-align: center;">50.3</td>
<td style="text-align: center;">52.6</td>
<td style="text-align: center;">52.2</td>
<td style="text-align: center;">50.3</td>
<td style="text-align: center;">52.3</td>
<td style="text-align: center;">52.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CYCLE NAVIGATION ${ }^{\dagger}$</td>
<td style="text-align: center;">61.9</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">23.0</td>
<td style="text-align: center;">37.6</td>
<td style="text-align: center;">23.6</td>
<td style="text-align: center;">24.2</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">58.8</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">49.7</td>
</tr>
<tr>
<td style="text-align: center;">DCF</td>
<td style="text-align: center;">Stack Manipulation</td>
<td style="text-align: center;">50.3</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">57.5</td>
<td style="text-align: center;">51.2</td>
<td style="text-align: center;">49.2</td>
<td style="text-align: center;">72.8</td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;">70.6</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">69.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Reverse String</td>
<td style="text-align: center;">52.8</td>
<td style="text-align: center;">50.6</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">62.3</td>
<td style="text-align: center;">51.9</td>
<td style="text-align: center;">50.7</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">95.1</td>
<td style="text-align: center;">77.1</td>
<td style="text-align: center;">69.9</td>
<td style="text-align: center;">52.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Modular Arithmetic</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;">30.3</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">25.1</td>
<td style="text-align: center;">25.1</td>
<td style="text-align: center;">33.8</td>
<td style="text-align: center;">34.9</td>
<td style="text-align: center;">31.3</td>
<td style="text-align: center;">32.7</td>
<td style="text-align: center;">31.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Solve Equation</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">21.0</td>
<td style="text-align: center;">23.0</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">23.1</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;">24.5</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">24.5</td>
<td style="text-align: center;">22.1</td>
</tr>
<tr>
<td style="text-align: center;">CS</td>
<td style="text-align: center;">Duplicate String</td>
<td style="text-align: center;">52.8</td>
<td style="text-align: center;">50.7</td>
<td style="text-align: center;">51.7</td>
<td style="text-align: center;">51.3</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">72.4</td>
<td style="text-align: center;">75.1</td>
<td style="text-align: center;">68.9</td>
<td style="text-align: center;">68.9</td>
<td style="text-align: center;">53.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Missing Duplicate</td>
<td style="text-align: center;">52.5</td>
<td style="text-align: center;">51.3</td>
<td style="text-align: center;">54.0</td>
<td style="text-align: center;">54.3</td>
<td style="text-align: center;">56.5</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">52.5</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">79.7</td>
<td style="text-align: center;">88.7</td>
<td style="text-align: center;">52.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Odds First</td>
<td style="text-align: center;">52.8</td>
<td style="text-align: center;">51.6</td>
<td style="text-align: center;">52.7</td>
<td style="text-align: center;">51.4</td>
<td style="text-align: center;">51.3</td>
<td style="text-align: center;">50.6</td>
<td style="text-align: center;">65.9</td>
<td style="text-align: center;">69.3</td>
<td style="text-align: center;">64.7</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">52.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Binary Addition</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">49.8</td>
<td style="text-align: center;">54.3</td>
<td style="text-align: center;">51.4</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">49.8</td>
<td style="text-align: center;">64.4</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;">56.2</td>
<td style="text-align: center;">60.2</td>
<td style="text-align: center;">61.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Binary Multiplication</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">52.2</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">49.6</td>
<td style="text-align: center;">52.1</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">50.5</td>
<td style="text-align: center;">51.7</td>
<td style="text-align: center;">51.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Compute Sqrt</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">52.4</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">50.5</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">52.5</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">51.2</td>
<td style="text-align: center;">52.3</td>
<td style="text-align: center;">52.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Bucket Sort ${ }^{\dagger}$</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">30.1</td>
<td style="text-align: center;">91.9</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">30.6</td>
<td style="text-align: center;">25.9</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">99.5</td>
</tr>
</tbody>
</table>
<p>CS tasks can be solved by an FSA with access to a bounded tape. Note that the relation to the Chomsky hierarchy is largely irrelevant for our work and only included for completeness. We evaluate our method on Delétang et al. (2023)'s benchmark as it is currently out of reach for Transformers and clearly demonstrates their failure to generalize on algorithmic reasoning tasks. We refer interested readers to the original paper for more details.</p>
<p>We consider the encoder-only model of the original seq-to-seq Transformer (Vaswani et al., 2017), as used in popular pre-trained language models such as BERT (Devlin et al., 2019) or Gopher (Rae et al., 2021). Thus, for tasks that require a multitoken output sequence $\boldsymbol{y}$ (e.g., duplicating a string), we pad the input sequence with $|\boldsymbol{y}|$ empty tokens and compute the entire Transformer output from the padded sequence (i.e., we do not use autoregressive sampling). We train the model on sequences of length sampled uniformly from $\mathcal{U}(1, N)$, with $N=40$, and evaluate it on sequences of length ${N+1, \ldots, M}$, with $M=500$. We set the maximum position $L=2048$ (and visualize the impact of other values on the performance in Appendix B.1). We report the accuracy averaged over all unseen sequence lengths, i.e., $N+1, \ldots, M$, for the best-performing model out of 10 different parameter initialization seeds and three learning rates $1 \times 10^{-4}, 3 \times 10^{-4}, 5 \times 10^{-4}$. We use the same hyperparameters as Delétang et al. (2023) and provide the full experiment setup in</p>
<p>Appendix A. We make our code publicly available at https://github.com/deepmind/ randomized_positional_encodings.</p>
<p>Comparison to prior work We compare our method to a wide range of positional encodings: none, $\sin / \cos$ (Vaswani et al., 2017), relative (Dai et al., 2019), ALiBi (Press et al., 2022), RoPE (Su et al., 2021), learned (Gehring et al., 2017), and label-based (Li and McClelland, 2022). Note that the label encodings proposed by Li and McClelland (2022) are equivalent to randomized learned positional encodings and thus subsumed by our method. We instantiate our randomized positional encoding scheme with all the above encodings and show the average test accuracy in Table 1 (with performance curves over test lengths in Appendix B.2). We observe that our randomized versions significantly increase the test accuracy across most tasks (by $12.0 \%$ on average and up to $43.5 \%$ ). In particular, the randomized relative encoding solves tasks that were previously out of reach for prior work (e.g., Reverse String or Missing Duplicate).</p>
<p>Efficiency comparison We now show that our method allows us to train a model on short sequences and obtain a test accuracy above $90 \%$, roughly 35.4 times faster than the naive approach of training a model on longer sequences. To that end, we train the randomized relative encodings on sequences up to length 40 and the classical relative positional encoding (Dai et al., 2019) on sequences</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: Average accuracy over unseen test lengths on the Missing Duplicate task over training time (seconds) for two models: (i) our randomized relative positional encoding with a maximum training sequence length of 40, and (ii) the classical relative positional encoding but with a maximum training length of 500 .
up to length 500 and show the test accuracy (averaged over lengths 41 to 500) in Fig. 2 over training time (in seconds). Our model obtains a strong test accuracy significantly faster due to the quadratic cost (in terms of sequence length) of global attention, which means that our model trains at 168.4 steps per second compared to 22.1 steps per second for the naive approach (on a NVIDIA V100 GPU).</p>
<h2>5 Conclusion</h2>
<p>We introduced a novel family of positional encodings that significantly improves the length generalization capabilities of Transformers. Our positional encodings are based on the insight that conventional positional encodings will be out-ofdistribution when increasing the sequence length. Thus, to overcome this issue, we randomly sample our encodings from a wider range than the lengths seen at test time while keeping the order. Our largescale empirical evaluation demonstrates that our method significantly outperforms prior work in terms of length generalization while offering superior computational performance over the naive approach of training the model on longer sequences.</p>
<h2>Limitations</h2>
<p>While our work shows promising results in improving the generalization capabilities of Transformers to sequences of arbitrary length, some limitations must be considered. First, our evaluation is confined to synthetic algorithmic reasoning tasks, which may not fully capture the complexity and diversity of natural language. We focused on synthetic datasets since they showed clear and somewhat surprising limitations of Transformer architec-
tures (Delétang et al., 2023). However, the generalizability of our approach to other tasks and domains remains an open question, and additional research, such as evaluation on SCAN (Lake and Baroni, 2018), CFQ (Keysers et al., 2020), COGS (Kim and Linzen, 2020), or the Long Range Arena (Tay et al., 2021), is necessary to understand its potential in real-world applications. Second, our approach introduces a new hyperparameter - the maximum sequence position $L$. Although our experiments in Appendix B. 1 show that our method's performance is largely unaffected by the precise value of $L$, practitioners may still have to tune the parameter depending on their specific problem domains. Third, we only isolate and ameliorate one failure mode of Transformer length generalization on synthetic datasets. However, there are other factors contributing to poor length generalization, such as attention becoming less peaked for longer sequences (Chiang and Cholak, 2022). Overall, we believe that our study's limitations offer several interesting directions for future research.</p>
<h2>Acknowledgements</h2>
<p>We thank Chris Cundy, Elliot Catt, Kevin Li, Laurent Orseau, Marcus Hutter, Petar Veličković, Vincent Dutordoir, and the anonymous reviewers for their helpful feedback.</p>
<h2>References</h2>
<p>Joshua Ackerman and George Cybenko. 2020. A survey of neural networks and formal languages. arXiv:2006.01338.</p>
<p>Satwik Bhattamishra, Kabir Ahuja, and Navin Goyal. 2020. On the ability and limitations of transformers to recognize formal languages. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing.</p>
<p>David Chiang and Peter Cholak. 2022. Overcoming a theoretical limitation of self-attention. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics.</p>
<p>Noam Chomsky. 1956. Three models for the description of language. IRE Trans. Inf. Theory.</p>
<p>Róbert Csordás, Kazuki Irie, and Jürgen Schmidhuber. 2021. The devil is in the detail: Simple tricks improve systematic generalization of transformers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Róbert Csordás, Kazuki Irie, and Jürgen Schmidhuber. 2022. The neural data router: Adaptive control flow</p>
<p>in transformers improves systematic generalization. In The Tenth International Conference on Learning Representations.</p>
<p>Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le, and Ruslan Salakhutdinov. 2019. Transformer-xl: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Conference of the Association for Computational Linguistics.</p>
<p>Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. 2019. Universal transformers. In 7th International Conference on Learning Representations.</p>
<p>Grégoire Delétang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Li Kevin Wenliang, Elliot Catt, Chris Cundy, Marcus Hutter, Shane Legg, Joel Veness, and Pedro A. Ortega. 2023. Neural networks and the chomsky hierarchy. In The Eleventh International Conference on Learning Representations.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,.</p>
<p>Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An image is worth $16 \times 16$ words: Transformers for image recognition at scale. In 9th International Conference on Learning Representations.</p>
<p>Javid Ebrahimi, Dhruv Gelda, and Wei Zhang. 2020. How can self-attention networks recognize dyck-n languages? In Findings of the Association for Computational Linguistics.</p>
<p>Jeffrey L. Elman. 1990. Finding structure in time. Cogn. Sci.</p>
<p>Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. 2017. Convolutional sequence to sequence learning. In Proceedings of the 34th International Conference on Machine Learning.</p>
<p>Michael Hahn. 2020. Theoretical limitations of selfattention in neural sequence models. Trans. Assoc. Comput. Linguistics.</p>
<p>Yiding Hao, Dana Angluin, and Robert Frank. 2022. Formal language recognition by hard attention transformers: Perspectives from circuit complexity. Trans. Assoc. Comput. Linguistics.</p>
<p>Borja Ibarz, Vitaly Kurin, George Papamakarios, Kyriacos Nikiforou, Mehdi Bennani, Róbert Csordás, Andrew Joseph Dudzik, Matko Bosnjak, Alex Vitvitskyi, Yulia Rubanova, Andreea Deac, Beatrice Bevilacqua,</p>
<p>Yaroslav Ganin, Charles Blundell, and Petar Velickovic. 2022. A generalist neural algorithmic learner. In Learning on Graphs Conference, LoG 2022, 9-12 December 2022, Virtual Event.</p>
<p>Daniel Keysers, Nathanael Schärli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz Stafiniak, Tibor Tihon, Dmitry Tsarkov, Xiao Wang, Marc van Zee, and Olivier Bousquet. 2020. Measuring compositional generalization: A comprehensive method on realistic data. In 8th International Conference on Learning Representations.</p>
<p>Najoung Kim and Tal Linzen. 2020. COGS: A compositional generalization challenge based on semantic interpretation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations.</p>
<p>Brenden M. Lake and Marco Baroni. 2018. Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. In Proceedings of the 35th International Conference on Machine Learning.</p>
<p>Yuxuan Li and James L. McClelland. 2022. Systematic generalization and emergent structures in transformers trained on structured tasks. arXiv:2210.00400.</p>
<p>Qian Liu, Shengnan An, Jian-Guang Lou, Bei Chen, Zeqi Lin, Yan Gao, Bin Zhou, Nanning Zheng, and Dongmei Zhang. 2020a. Compositional generalization by learning analytical expressions. In Advances in Neural Information Processing Systems 33.</p>
<p>Xuanqing Liu, Hsiang-Fu Yu, Inderjit S. Dhillon, and Cho-Jui Hsieh. 2020b. Learning to encode position for transformer with continuous dynamical model. In Proceedings of the 37th International Conference on Machine Learning.</p>
<p>William Merrill. 2019. Sequential neural networks as automata. arXiv:1906.01615.</p>
<p>William Merrill and Ashish Sabharwal. 2022. Logprecision transformers are constant-depth uniform threshold circuits. arXiv:2207.00729.</p>
<p>Santiago Ontañón, Joshua Ainslie, Zachary Fisher, and Vaclav Cvicek. 2022. Making transformers solve compositional tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).</p>
<p>Ofir Press, Noah Smith, and Mike Lewis. 2022. Train short, test long: Attention with linear biases enables input length extrapolation. In The Tenth International Conference on Learning Representations.</p>
<p>Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, H. Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William S. Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. 2021. Scaling language models: Methods, analysis \&amp; insights from training gopher. arXiv:2112.11446.</p>
<p>Scott E. Reed, Konrad Zolna, Emilio Parisotto, Sergio Gómez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. 2022. A generalist agent. Trans. Mach. Learn. Res.</p>
<p>Ryoma Sato, Makoto Yamada, and Hisashi Kashima. 2021. Random features strengthen graph neural networks. In Proceedings of the 2021 SIAM International Conference on Data Mining.</p>
<p>Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. 2021. Roformer: Enhanced transformer with rotary position embedding. arXiv:2104.09864.</p>
<p>Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2021. Long range arena : A benchmark for efficient transformers. In 9th International Conference on Learning Representations.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30.</p>
<h1>A Experimental Details</h1>
<p>We use the experiment suite proposed by Delétang et al. (2023), which consists of 15 algorithmic reasoning tasks and is publicly available at https://github.com/deepmind/ neural_networks_chomsky_hierarchy under the Apache 2.0 License. The tasks do not consist of fixed-size datasets but define training and testing distributions from which one can sample continuously. We train the models for 2000000 steps with a batch size of 128 , which corresponds to 256000000 (potentially non-unique) training examples. At test time, we evaluate a single batch of size 500 for every sequence length in ${41, \ldots, 500}$, which corresponds to 230000 testing examples. We use the Adam optimizer (Kingma and Ba, 2015) with gradient clipping and sweep over three learning rates: $1 \times 10^{-4}, 3 \times 10^{-4}$, and $5 \times 10^{-4}$. Furthermore, for each task and positional encoding, we use 10 different parameter initialization random seeds.</p>
<p>We consider the encoder-only Transformer architecture (Vaswani et al., 2017), with 5 blocks of 8 heads each and $d_{\text {model }}=64$, which corresponds to 249026 parameters ( 270146 in the case of relative and randomized relative positional encodings). We run every task-encodinghyperparameter triplet on a single NVIDIA V100 GPU from our internal cluster. As a result, we used 15 (tasks) $\cdot 13$ (positional encodings) $\cdot$ 3 (learning rates) $\cdot 10$ (seeds) $=5850$ GPU-units for the results in Tables 1, 4 and 5 and Fig. 4. For the results in Fig. 2, we used an additional 2 (positional encodings) $\cdot 3$ (learning rates) $\cdot$ 10 (seeds) $=60$ GPU-units. Finally, for Fig. 3, we used 4 (maximum positions) $\cdot 3$ (learning rates) $\cdot$ 10 (seeds) $=120$ GPU-units, yielding a grand total of 6030 GPU-units. We report all running times in Table 2 and observe that our method induces a negligible computational overhead.</p>
<h2>B Additional Results</h2>
<h2>B. 1 Ablation Study</h2>
<p>In this section, we conduct an ablation study over the two main components of our method: (i) the maximum sampling position $L$, and (ii) the sorting of the subsampled positions.</p>
<p>We train the randomized relative positional encoding for a wide range of different maximum positions $L: 1024,2048,4096$, and 8192 . Figure 3
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Sweep over the maximum position $L$ for our randomized relative positional encodings. The test accuracy (averaged over unseen sequence lengths) is largely unaffected by the concrete value of $L$ (for reasonably small values of $L$ ), showing the stability of our method. However, if $L$ is much larger than the maximum training $(N)$ or testing $(M)$ sequence length, we expect the performance to degrade since it the model is unlikely to encounter enough unique indices during training time.
shows that the test accuracy (averaged over all unseen sequence lengths) is largely unaffected by the value of $L$ on the Reverse String and Missing Duplicate tasks. As a consequence, a practitioner wanting to apply our method will not have to carry out extensive tuning of this parameter (as long as it is larger than the maximum evaluation sequence length $M$, but not unreasonably large).</p>
<p>Next, we investigate the performance of our randomized $\sin / \cos$ positional encoding with and without sorting of the subsampled positions. Note that this experiment is meant as a "sanity-check" since we do not expect the Transformer to perform well without order information. Table 3 shows the test accuracy (averaged over all unseen sequence lengths) for the two versions of our method. We observe that sorting the positions is crucial, as it increases the test accuracy by $15.7 \%$ on average</p>
<p>Table 2: Mean and standard deviation of the running times (in hours) for all the positional encodings and tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Level</th>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Randomized (Ours)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">$\sin / \cos$</td>
<td style="text-align: center;">Relative</td>
<td style="text-align: center;">ALiBi</td>
<td style="text-align: center;">RoPE</td>
<td style="text-align: center;">Learned</td>
<td style="text-align: center;">$\sin / \cos$</td>
<td style="text-align: center;">Relative</td>
<td style="text-align: center;">ALiBi</td>
<td style="text-align: center;">RoPE</td>
<td style="text-align: center;">Learned*</td>
</tr>
<tr>
<td style="text-align: center;">R</td>
<td style="text-align: center;">PARITY CHECK ${ }^{\dagger}$</td>
<td style="text-align: center;">0.86 $\pm 0.17$</td>
<td style="text-align: center;">$0.87 \pm 0.17$</td>
<td style="text-align: center;">1.63 $\pm 0.28$</td>
<td style="text-align: center;">$0.87 \pm 0.17$</td>
<td style="text-align: center;">1.41 $\pm 0.24$</td>
<td style="text-align: center;">0.90 $\pm 0.18$</td>
<td style="text-align: center;">0.92 $\pm 0.18$</td>
<td style="text-align: center;">1.75 $\pm 0.29$</td>
<td style="text-align: center;">0.94 $\pm 0.19$</td>
<td style="text-align: center;">1.66 $\pm 0.35$</td>
<td style="text-align: center;">1.12 $\pm 0.24$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">REVERSE STRING</td>
<td style="text-align: center;">1.17 $\pm 0.21$</td>
<td style="text-align: center;">1.18 $\pm 0.22$</td>
<td style="text-align: center;">2.01 $\pm 0.39$</td>
<td style="text-align: center;">1.17 $\pm 0.22$</td>
<td style="text-align: center;">2.01 $\pm 0.35$</td>
<td style="text-align: center;">1.23 $\pm 0.23$</td>
<td style="text-align: center;">1.24 $\pm 0.23$</td>
<td style="text-align: center;">2.75 $\pm 0.41$</td>
<td style="text-align: center;">1.27 $\pm 0.24$</td>
<td style="text-align: center;">2.42 $\pm 0.43$</td>
<td style="text-align: center;">1.62 $\pm 0.32$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CYCLE NAVIGATION ${ }^{\dagger}$</td>
<td style="text-align: center;">0.86 $\pm 0.17$</td>
<td style="text-align: center;">$0.87 \pm 0.17$</td>
<td style="text-align: center;">1.62 $\pm 0.27$</td>
<td style="text-align: center;">0.86 $\pm 0.17$</td>
<td style="text-align: center;">1.41 $\pm 0.25$</td>
<td style="text-align: center;">0.91 $\pm 0.18$</td>
<td style="text-align: center;">0.92 $\pm 0.18$</td>
<td style="text-align: center;">1.75 $\pm 0.29$</td>
<td style="text-align: center;">0.94 $\pm 0.19$</td>
<td style="text-align: center;">1.66 $\pm 0.35$</td>
<td style="text-align: center;">1.12 $\pm 0.22$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EVEN PARK</td>
<td style="text-align: center;">0.86 $\pm 0.21$</td>
<td style="text-align: center;">$0.87 \pm 0.17$</td>
<td style="text-align: center;">1.63 $\pm 0.27$</td>
<td style="text-align: center;">0.86 $\pm 0.17$</td>
<td style="text-align: center;">1.41 $\pm 0.24$</td>
<td style="text-align: center;">0.91 $\pm 0.18$</td>
<td style="text-align: center;">0.92 $\pm 0.18$</td>
<td style="text-align: center;">1.75 $\pm 0.29$</td>
<td style="text-align: center;">0.95 $\pm 0.19$</td>
<td style="text-align: center;">1.65 $\pm 0.31$</td>
<td style="text-align: center;">1.12 $\pm 0.22$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">STACK MANIPULATION</td>
<td style="text-align: center;">8.09 $\pm 0.97$</td>
<td style="text-align: center;">8.00 $\pm 0.82$</td>
<td style="text-align: center;">9.50 $\pm 0.89$</td>
<td style="text-align: center;">8.07 $\pm 0.94$</td>
<td style="text-align: center;">8.87 $\pm 0.84$</td>
<td style="text-align: center;">8.46 $\pm 0.84$</td>
<td style="text-align: center;">8.47 $\pm 0.88$</td>
<td style="text-align: center;">10.04 $\pm 0.96$</td>
<td style="text-align: center;">8.55 $\pm 0.90$</td>
<td style="text-align: center;">10.61 $\pm 1.58$</td>
<td style="text-align: center;">9.58 $\pm 1.12$</td>
</tr>
<tr>
<td style="text-align: center;">DCF</td>
<td style="text-align: center;">MODULAR ARITHMETIC</td>
<td style="text-align: center;">5.48 $\pm 0.63$</td>
<td style="text-align: center;">5.55 $\pm 0.67$</td>
<td style="text-align: center;">6.32 $\pm 0.81$</td>
<td style="text-align: center;">5.50 $\pm 0.65$</td>
<td style="text-align: center;">6.07 $\pm 0.69$</td>
<td style="text-align: center;">5.69 $\pm 0.65$</td>
<td style="text-align: center;">5.66 $\pm 0.64$</td>
<td style="text-align: center;">6.56 $\pm 0.70$</td>
<td style="text-align: center;">5.69 $\pm 0.65$</td>
<td style="text-align: center;">6.41 $\pm 0.84$</td>
<td style="text-align: center;">5.92 $\pm 0.80$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BINARY MULTIPLICATION</td>
<td style="text-align: center;">1.83 $\pm 0.33$</td>
<td style="text-align: center;">1.83 $\pm 0.30$</td>
<td style="text-align: center;">2.86 $\pm 0.43$</td>
<td style="text-align: center;">1.84 $\pm 0.31$</td>
<td style="text-align: center;">2.32 $\pm 0.39$</td>
<td style="text-align: center;">2.24 $\pm 0.35$</td>
<td style="text-align: center;">2.23 $\pm 0.35$</td>
<td style="text-align: center;">3.13 $\pm 0.43$</td>
<td style="text-align: center;">2.24 $\pm 0.35$</td>
<td style="text-align: center;">3.21 $\pm 0.51$</td>
<td style="text-align: center;">2.88 $\pm 0.46$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BINARY ADDITION</td>
<td style="text-align: center;">1.83 $\pm 0.32$</td>
<td style="text-align: center;">1.82 $\pm 0.31$</td>
<td style="text-align: center;">2.89 $\pm 0.42$</td>
<td style="text-align: center;">1.81 $\pm 0.32$</td>
<td style="text-align: center;">2.34 $\pm 0.39$</td>
<td style="text-align: center;">2.22 $\pm 0.35$</td>
<td style="text-align: center;">2.22 $\pm 0.35$</td>
<td style="text-align: center;">3.17 $\pm 0.44$</td>
<td style="text-align: center;">2.24 $\pm 0.35$</td>
<td style="text-align: center;">3.29 $\pm 0.62$</td>
<td style="text-align: center;">2.90 $\pm 0.49$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BINARY ADDITION</td>
<td style="text-align: center;">1.83 $\pm 0.32$</td>
<td style="text-align: center;">1.82 $\pm 0.31$</td>
<td style="text-align: center;">2.89 $\pm 0.42$</td>
<td style="text-align: center;">1.81 $\pm 0.32$</td>
<td style="text-align: center;">2.34 $\pm 0.39$</td>
<td style="text-align: center;">2.22 $\pm 0.35$</td>
<td style="text-align: center;">2.22 $\pm 0.35$</td>
<td style="text-align: center;">3.17 $\pm 0.44$</td>
<td style="text-align: center;">2.24 $\pm 0.35$</td>
<td style="text-align: center;">3.29 $\pm 0.62$</td>
<td style="text-align: center;">2.90 $\pm 0.49$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">COMPUTE SQRT</td>
<td style="text-align: center;">1.39 $\pm 0.24$</td>
<td style="text-align: center;">1.40 $\pm 0.25$</td>
<td style="text-align: center;">2.30 $\pm 0.34$</td>
<td style="text-align: center;">1.40 $\pm 0.25$</td>
<td style="text-align: center;">1.86 $\pm 0.30$</td>
<td style="text-align: center;">1.73 $\pm 0.29$</td>
<td style="text-align: center;">1.72 $\pm 0.29$</td>
<td style="text-align: center;">2.43 $\pm 0.37$</td>
<td style="text-align: center;">1.74 $\pm 0.30$</td>
<td style="text-align: center;">2.53 $\pm 0.41$</td>
<td style="text-align: center;">2.23 $\pm 0.38$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SOLVE EQUATION</td>
<td style="text-align: center;">5.69 $\pm 0.65$</td>
<td style="text-align: center;">5.60 $\pm 0.67$</td>
<td style="text-align: center;">6.41 $\pm 0.68$</td>
<td style="text-align: center;">5.63 $\pm 0.66$</td>
<td style="text-align: center;">6.14 $\pm 0.68$</td>
<td style="text-align: center;">5.74 $\pm 0.65$</td>
<td style="text-align: center;">5.78 $\pm 0.66$</td>
<td style="text-align: center;">6.69 $\pm 0.76$</td>
<td style="text-align: center;">5.83 $\pm 0.69$</td>
<td style="text-align: center;">6.50 $\pm 0.86$</td>
<td style="text-align: center;">6.01 $\pm 0.84$</td>
</tr>
<tr>
<td style="text-align: center;">CS</td>
<td style="text-align: center;">DUPLICATE STRING</td>
<td style="text-align: center;">1.58 $\pm 0.28$</td>
<td style="text-align: center;">1.59 $\pm 0.28$</td>
<td style="text-align: center;">4.10 $\pm 0.54$</td>
<td style="text-align: center;">1.58 $\pm 0.27$</td>
<td style="text-align: center;">2.71 $\pm 0.40$</td>
<td style="text-align: center;">1.64 $\pm 0.28$</td>
<td style="text-align: center;">1.65 $\pm 0.29$</td>
<td style="text-align: center;">4.24 $\pm 0.54$</td>
<td style="text-align: center;">1.67 $\pm 0.29$</td>
<td style="text-align: center;">3.18 $\pm 0.49$</td>
<td style="text-align: center;">2.05 $\pm 0.38$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MODULAR ARITHMETIC (SIMPLE)</td>
<td style="text-align: center;">0.99 $\pm 0.19$</td>
<td style="text-align: center;">1.00 $\pm 0.19$</td>
<td style="text-align: center;">1.74 $\pm 0.29$</td>
<td style="text-align: center;">0.99 $\pm 0.18$</td>
<td style="text-align: center;">1.51 $\pm 0.26$</td>
<td style="text-align: center;">1.03 $\pm 0.20$</td>
<td style="text-align: center;">1.05 $\pm 0.20$</td>
<td style="text-align: center;">1.87 $\pm 0.31$</td>
<td style="text-align: center;">1.06 $\pm 0.21$</td>
<td style="text-align: center;">1.74 $\pm 0.31$</td>
<td style="text-align: center;">1.23 $\pm 0.23$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MISSING DUPLICATE</td>
<td style="text-align: center;">0.88 $\pm 0.17$</td>
<td style="text-align: center;">0.90 $\pm 0.18$</td>
<td style="text-align: center;">1.64 $\pm 0.27$</td>
<td style="text-align: center;">0.88 $\pm 0.17$</td>
<td style="text-align: center;">1.43 $\pm 0.26$</td>
<td style="text-align: center;">0.93 $\pm 0.19$</td>
<td style="text-align: center;">0.94 $\pm 0.19$</td>
<td style="text-align: center;">1.78 $\pm 0.30$</td>
<td style="text-align: center;">0.97 $\pm 0.19$</td>
<td style="text-align: center;">1.66 $\pm 0.30$</td>
<td style="text-align: center;">1.15 $\pm 0.23$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FROM FIRST</td>
<td style="text-align: center;">1.17 $\pm 0.22$</td>
<td style="text-align: center;">1.19 $\pm 0.22$</td>
<td style="text-align: center;">2.01 $\pm 0.38$</td>
<td style="text-align: center;">1.17 $\pm 0.22$</td>
<td style="text-align: center;">2.00 $\pm 0.31$</td>
<td style="text-align: center;">1.23 $\pm 0.23$</td>
<td style="text-align: center;">1.24 $\pm 0.23$</td>
<td style="text-align: center;">2.74 $\pm 0.40$</td>
<td style="text-align: center;">1.26 $\pm 0.23$</td>
<td style="text-align: center;">2.40 $\pm 0.39$</td>
<td style="text-align: center;">1.59 $\pm 0.29$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BUCKET SORT ${ }^{\dagger}$</td>
<td style="text-align: center;">1.17 $\pm 0.22$</td>
<td style="text-align: center;">1.18 $\pm 0.22$</td>
<td style="text-align: center;">2.01 $\pm 0.43$</td>
<td style="text-align: center;">1.16 $\pm 0.22$</td>
<td style="text-align: center;">2.01 $\pm 0.34$</td>
<td style="text-align: center;">1.22 $\pm 0.22$</td>
<td style="text-align: center;">1.24 $\pm 0.23$</td>
<td style="text-align: center;">2.74 $\pm 0.40$</td>
<td style="text-align: center;">1.25 $\pm 0.23$</td>
<td style="text-align: center;">2.40 $\pm 0.41$</td>
<td style="text-align: center;">1.60 $\pm 0.30$</td>
</tr>
</tbody>
</table>
<p>Table 3: Accuracy (in percentage) averaged over all test lengths and maximized over 10 seeds and 3 learning rates for our randomized $\sin / \cos$ positional encoding with and without sorting of the subsampled positions.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Level</th>
<th style="text-align: left;">Task</th>
<th style="text-align: center;">Randomized $\sin / \cos$</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: center;">w/o Sorting</td>
<td style="text-align: center;">w/ Sorting</td>
</tr>
<tr>
<td style="text-align: left;">R</td>
<td style="text-align: left;">EVEN PARKS</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">$\mathbf{1 0 0 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">MODULAR ARITHMETIC (SIMPLE)</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">$\mathbf{2 5 . 7}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">PARITY CHECK ${ }^{\dagger}$</td>
<td style="text-align: center;">52.2</td>
<td style="text-align: center;">$\mathbf{5 2 . 6}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">CYCLE NAVIGATION ${ }^{\dagger}$</td>
<td style="text-align: center;">$\mathbf{5 9 . 3}$</td>
<td style="text-align: center;">59.0</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">DCF</td>
<td style="text-align: left;">STACK MANIPULATION</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">$\mathbf{7 2 . 8}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">REVERSE STRING</td>
<td style="text-align: center;">52.8</td>
<td style="text-align: center;">$\mathbf{7 5 . 6}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">MODULAR ARITHMETIC</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">$\mathbf{3 3 . 8}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">SOLVE EQUATION</td>
<td style="text-align: center;">20.2</td>
<td style="text-align: center;">$\mathbf{2 4 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">DUPLICATE STRING</td>
<td style="text-align: center;">52.8</td>
<td style="text-align: center;">$\mathbf{7 2 . 4}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">MISSING DUPLICATE</td>
<td style="text-align: center;">$\mathbf{5 3 . 1}$</td>
<td style="text-align: center;">52.5</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">ODDS FIRST</td>
<td style="text-align: center;">52.8</td>
<td style="text-align: center;">$\mathbf{6 5 . 9}$</td>
</tr>
<tr>
<td style="text-align: left;">CS</td>
<td style="text-align: left;">BINARY ADDITION</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">$\mathbf{6 4 . 4}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">BINARY MULTIPLICATION</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">$\mathbf{5 2 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">COMPUTE SQRT</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">$\mathbf{5 2 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">BUCKET SORT ${ }^{\dagger}$</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">$\mathbf{1 0 0 . 0}$</td>
</tr>
</tbody>
</table>
<p>and up to $76.3 \%$ on certain tasks. In fact, without sorting, our approach fails to beat the (baseline) random accuracy on all but the CYCLE NAVIGATION task, which is permutation-invariant (i.e., it can be solved without positional information). This confirms our intuition that the Transformer only needs to know the relative order of the positional encodings (and not their exact values), but that it fails to solve tasks when presented with positional encodings whose order does not correspond to the tokens' positions.</p>
<h2>B. 2 Comparison to Prior Work</h2>
<p>In Section 4, we compared our method to a wide range of positional encodings: none, $\sin / \cos$ (Vaswani et al., 2017), relative (Dai et al., 2019), ALiBi (Press et al., 2022), RoPE (Su et al., 2021), learned (Gehring et al., 2017), and label-
based (Li and McClelland, 2022). Here, we provide additional results for these experiments, as well as a comparison to the geometric attention and directional encodings of Csordás et al. (2022).</p>
<p>We recall that Table 1 showed the test accuracy maximized over the 10 parameter initialization seeds and the three different learning rates. We reported the maximum following the experiment setup in Delétang et al. (2023), which investigates whether an architecture is capable of solving a task at all (and not on average). However, we also report the means and standard deviations (over the random seeds) in Table 4 for the best-performing learning rate. We observe that our randomized positional encoding also significantly outperform their original counterparts on average. We visualize the test accuracy per sequence length in Fig. 4.</p>
<p>We highlight the case of learned positional encodings, which fail to beat the random accuracy baseline (cf. Tables 1 and 4). This is because the columns of the embedding matrix corresponding to the positions that are larger than the maximum training length $N$ are not learned during training and are thus entirely random. In contrast, our randomized version of the learned encodings considers all possible embedding columns during training and thus achieves non-trivial to strong length generalization on most tasks.</p>
<p>Finally, we also compare our method to a variant of the Neural Data Router (NDR) (Csordás et al., 2022), which was developed to improve the systematic generalization capabilities of Transformers. We only consider the most related aspects of the NDR architecture, i.e., the geometric attention and the directional encoding (we do not use gating or shared layers). Table 5 compares the test accuracy of geometric attention and directional encodings</p>
<p>Table 4: Means and standard deviations (computed over random seeds) of the score (accuracy averaged over all test lengths) for the results of the main experiment (see Table 1). The random accuracy is $50 \%$, except for CYCLE NAVIGATION, BUCKET SORT, and the modular arithmetic tasks, where it is $20 \%$. We denote permutation-invariant tasks, which can be solved without positional information, with $\dagger$. Numbers in bold are the best performers, per task. These results underline the superiority of our method, and especially when applied to relative positional encodings.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Level</th>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Mean</th>
<th style="text-align: center;">$\sin / \cos$</th>
<th style="text-align: center;">Relative</th>
<th style="text-align: center;">ALiBi</th>
<th style="text-align: center;">BoPE</th>
<th style="text-align: center;">Learned</th>
<th style="text-align: center;">Randomized (Ours)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sin / \cos$</td>
<td style="text-align: center;">Relative</td>
<td style="text-align: center;">ALiBi</td>
<td style="text-align: center;">BoPE</td>
<td style="text-align: center;">Learned*</td>
</tr>
<tr>
<td style="text-align: center;">R</td>
<td style="text-align: center;">Even Pairs</td>
<td style="text-align: center;">50.1 $\pm 0.1$</td>
<td style="text-align: center;">50.4 $\pm 0.2$</td>
<td style="text-align: center;">67.6 $\pm 15.3$</td>
<td style="text-align: center;">59.8 $\pm 3.2$</td>
<td style="text-align: center;">50.4 $\pm 0.3$</td>
<td style="text-align: center;">50.4 $\pm 0.2$</td>
<td style="text-align: center;">99.7 $\pm 0.3$</td>
<td style="text-align: center;">99.6 $\pm 0.6$</td>
<td style="text-align: center;">71.4 $\pm 5.6$</td>
<td style="text-align: center;">100.0 $\pm 0.0$</td>
<td style="text-align: center;">96.2 $\pm 0.7$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Modular Arithmetic (Simple)</td>
<td style="text-align: center;">20.0 $\pm 0.0$</td>
<td style="text-align: center;">20.2 $\pm 0.2$</td>
<td style="text-align: center;">20.7 $\pm 0.5$</td>
<td style="text-align: center;">23.2 $\pm 0.9$</td>
<td style="text-align: center;">20.8 $\pm 0.5$</td>
<td style="text-align: center;">20.1 $\pm 0.1$</td>
<td style="text-align: center;">24.2 $\pm 1.4$</td>
<td style="text-align: center;">24.9 $\pm 1.7$</td>
<td style="text-align: center;">20.8 $\pm 0.3$</td>
<td style="text-align: center;">23.5 $\pm 1.6$</td>
<td style="text-align: center;">20.2 $\pm 0.4$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Parity Check ${ }^{1}$</td>
<td style="text-align: center;">50.4 $\pm 0.8$</td>
<td style="text-align: center;">50.3 $\pm 0.2$</td>
<td style="text-align: center;">50.4 $\pm 0.6$</td>
<td style="text-align: center;">50.5 $\pm 0.6$</td>
<td style="text-align: center;">50.4 $\pm 0.4$</td>
<td style="text-align: center;">50.0 $\pm 0.1$</td>
<td style="text-align: center;">51.1 $\pm 1.3$</td>
<td style="text-align: center;">51.4 $\pm 0.5$</td>
<td style="text-align: center;">50.0 $\pm 0.2$</td>
<td style="text-align: center;">50.4 $\pm 1.0$</td>
<td style="text-align: center;">50.6 $\pm 0.9$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Cycle Navigation ${ }^{2}$</td>
<td style="text-align: center;">33.9 $\pm 10.5$</td>
<td style="text-align: center;">23.8 $\pm 1.4$</td>
<td style="text-align: center;">21.7 $\pm 0.8$</td>
<td style="text-align: center;">31.1 $\pm 3.8$</td>
<td style="text-align: center;">22.3 $\pm 0.9$</td>
<td style="text-align: center;">21.0 $\pm 1.2$</td>
<td style="text-align: center;">30.3 $\pm 10.7$</td>
<td style="text-align: center;">45.9 $\pm 9.9$</td>
<td style="text-align: center;">36.3 $\pm 2.4$</td>
<td style="text-align: center;">52.9 $\pm 15.3$</td>
<td style="text-align: center;">31.9 $\pm 8.2$</td>
</tr>
<tr>
<td style="text-align: center;">DCP</td>
<td style="text-align: center;">Stack Manipulation</td>
<td style="text-align: center;">50.2 $\pm 0.1$</td>
<td style="text-align: center;">47.3 $\pm 1.9$</td>
<td style="text-align: center;">50.1 $\pm 3.3$</td>
<td style="text-align: center;">51.0 $\pm 8.0$</td>
<td style="text-align: center;">49.6 $\pm 3.0$</td>
<td style="text-align: center;">44.9 $\pm 3.7$</td>
<td style="text-align: center;">69.2 $\pm 3.2$</td>
<td style="text-align: center;">71.7 $\pm 4.7$</td>
<td style="text-align: center;">69.5 $\pm 1.1$</td>
<td style="text-align: center;">66.0 $\pm 2.0$</td>
<td style="text-align: center;">66.1 $\pm 2.5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Reverse String</td>
<td style="text-align: center;">52.7 $\pm 0.1$</td>
<td style="text-align: center;">50.4 $\pm 0.1$</td>
<td style="text-align: center;">54.2 $\pm 1.5$</td>
<td style="text-align: center;">56.3 $\pm 2.6$</td>
<td style="text-align: center;">51.2 $\pm 0.3$</td>
<td style="text-align: center;">50.4 $\pm 0.2$</td>
<td style="text-align: center;">72.9 $\pm 1.6$</td>
<td style="text-align: center;">77.1 $\pm 6.6$</td>
<td style="text-align: center;">75.1 $\pm 1.3$</td>
<td style="text-align: center;">67.7 $\pm 1.1$</td>
<td style="text-align: center;">52.7 $\pm 0.2$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Modular Arithmetic</td>
<td style="text-align: center;">31.0 $\pm 0.1$</td>
<td style="text-align: center;">34.3 $\pm 2.2$</td>
<td style="text-align: center;">36.1 $\pm 2.0$</td>
<td style="text-align: center;">28.1 $\pm 3.4$</td>
<td style="text-align: center;">34.0 $\pm 2.4$</td>
<td style="text-align: center;">22.3 $\pm 1.5$</td>
<td style="text-align: center;">29.6 $\pm 4.6$</td>
<td style="text-align: center;">26.8 $\pm 5.5$</td>
<td style="text-align: center;">29.3 $\pm 1.6$</td>
<td style="text-align: center;">28.6 $\pm 3.9$</td>
<td style="text-align: center;">30.3 $\pm 2.6$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Solve Equation</td>
<td style="text-align: center;">20.1 $\pm 0.0$</td>
<td style="text-align: center;">20.9 $\pm 0.2$</td>
<td style="text-align: center;">21.9 $\pm 0.7$</td>
<td style="text-align: center;">23.6 $\pm 1.9$</td>
<td style="text-align: center;">21.9 $\pm 0.6$</td>
<td style="text-align: center;">20.2 $\pm 0.2$</td>
<td style="text-align: center;">23.6 $\pm 0.5$</td>
<td style="text-align: center;">25.4 $\pm 1.8$</td>
<td style="text-align: center;">21.1 $\pm 0.7$</td>
<td style="text-align: center;">22.3 $\pm 1.6$</td>
<td style="text-align: center;">21.1 $\pm 0.7$</td>
</tr>
<tr>
<td style="text-align: center;">CS</td>
<td style="text-align: center;">DUPLICATE String</td>
<td style="text-align: center;">52.7 $\pm 0.1$</td>
<td style="text-align: center;">50.4 $\pm 0.2$</td>
<td style="text-align: center;">51.0 $\pm 0.4$</td>
<td style="text-align: center;">51.0 $\pm 0.2$</td>
<td style="text-align: center;">50.4 $\pm 0.2$</td>
<td style="text-align: center;">50.4 $\pm 0.2$</td>
<td style="text-align: center;">69.0 $\pm 2.9$</td>
<td style="text-align: center;">73.1 $\pm 1.5$</td>
<td style="text-align: center;">67.9 $\pm 1.4$</td>
<td style="text-align: center;">67.1 $\pm 2.0$</td>
<td style="text-align: center;">52.8 $\pm 0.1$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Merging DUPLICATE</td>
<td style="text-align: center;">51.4 $\pm 1.0$</td>
<td style="text-align: center;">50.1 $\pm 0.6$</td>
<td style="text-align: center;">51.1 $\pm 1.1$</td>
<td style="text-align: center;">53.5 $\pm 0.4$</td>
<td style="text-align: center;">53.9 $\pm 1.6$</td>
<td style="text-align: center;">50.1 $\pm 0.4$</td>
<td style="text-align: center;">50.4 $\pm 1.5$</td>
<td style="text-align: center;">91.4 $\pm 9.8$</td>
<td style="text-align: center;">75.2 $\pm 3.4$</td>
<td style="text-align: center;">73.2 $\pm 1.2$</td>
<td style="text-align: center;">51.2 $\pm 1.4$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ours First</td>
<td style="text-align: center;">52.7 $\pm 0.1$</td>
<td style="text-align: center;">51.3 $\pm 0.2$</td>
<td style="text-align: center;">51.5 $\pm 0.5$</td>
<td style="text-align: center;">51.1 $\pm 0.2$</td>
<td style="text-align: center;">50.8 $\pm 0.2$</td>
<td style="text-align: center;">50.5 $\pm 0.1$</td>
<td style="text-align: center;">62.5 $\pm 2.0$</td>
<td style="text-align: center;">65.9 $\pm 1.6$</td>
<td style="text-align: center;">62.2 $\pm 1.4$</td>
<td style="text-align: center;">62.9 $\pm 1.3$</td>
<td style="text-align: center;">52.7 $\pm 0.1$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Binary Addition</td>
<td style="text-align: center;">49.4 $\pm 0.3$</td>
<td style="text-align: center;">47.3 $\pm 3.8$</td>
<td style="text-align: center;">51.7 $\pm 1.3$</td>
<td style="text-align: center;">48.5 $\pm 3.6$</td>
<td style="text-align: center;">47.8 $\pm 5.4$</td>
<td style="text-align: center;">48.9 $\pm 0.8$</td>
<td style="text-align: center;">61.2 $\pm 1.7$</td>
<td style="text-align: center;">62.0 $\pm 1.1$</td>
<td style="text-align: center;">54.3 $\pm 1.5$</td>
<td style="text-align: center;">57.4 $\pm 1.2$</td>
<td style="text-align: center;">59.9 $\pm 1.3$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Binary Multiplication</td>
<td style="text-align: center;">49.8 $\pm 0.0$</td>
<td style="text-align: center;">48.8 $\pm 1.0$</td>
<td style="text-align: center;">50.2 $\pm 3.5$</td>
<td style="text-align: center;">49.9 $\pm 2.3$</td>
<td style="text-align: center;">49.6 $\pm 0.6$</td>
<td style="text-align: center;">48.7 $\pm 1.7$</td>
<td style="text-align: center;">51.8 $\pm 0.2$</td>
<td style="text-align: center;">39.1 $\pm 7.1$</td>
<td style="text-align: center;">48.2 $\pm 1.3$</td>
<td style="text-align: center;">45.7 $\pm 6.6$</td>
<td style="text-align: center;">51.6 $\pm 0.2$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Compile Sqrt</td>
<td style="text-align: center;">50.2 $\pm 0.0$</td>
<td style="text-align: center;">50.1 $\pm 0.0$</td>
<td style="text-align: center;">51.5 $\pm 0.4$</td>
<td style="text-align: center;">50.5 $\pm 0.2$</td>
<td style="text-align: center;">50.3 $\pm 0.1$</td>
<td style="text-align: center;">50.1 $\pm 0.1$</td>
<td style="text-align: center;">51.9 $\pm 0.5$</td>
<td style="text-align: center;">52.4 $\pm 0.6$</td>
<td style="text-align: center;">51.1 $\pm 0.1$</td>
<td style="text-align: center;">51.8 $\pm 0.3$</td>
<td style="text-align: center;">51.0 $\pm 0.8$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Bucket Sort ${ }^{1}$</td>
<td style="text-align: center;">23.7 $\pm 0.0$</td>
<td style="text-align: center;">25.6 $\pm 2.6$</td>
<td style="text-align: center;">83.4 $\pm 6.6$</td>
<td style="text-align: center;">29.3 $\pm 0.7$</td>
<td style="text-align: center;">23.6 $\pm 3.8$</td>
<td style="text-align: center;">20.7 $\pm 2.9$</td>
<td style="text-align: center;">99.3 $\pm 0.4$</td>
<td style="text-align: center;">99.4 $\pm 0.3$</td>
<td style="text-align: center;">98.8 $\pm 0.7$</td>
<td style="text-align: center;">99.3 $\pm 0.3$</td>
<td style="text-align: center;">98.9 $\pm 0.4$</td>
</tr>
</tbody>
</table>
<p>Table 5: Accuracy (in \%) averaged over all test lengths for geometric attention with directional encoding.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Level</th>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Max</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Avg $\pm$ SD</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Table 1</td>
<td style="text-align: center;">Geometric</td>
<td style="text-align: center;">Table 4</td>
<td style="text-align: center;">Geometric</td>
</tr>
<tr>
<td style="text-align: center;">R</td>
<td style="text-align: center;">Even Pairs</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0 $\pm 0.0$</td>
<td style="text-align: center;">94.5 $\pm 8.8$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Modular Arithmetic (Simple)</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">24.9 $\pm 1.7$</td>
<td style="text-align: center;">27.2 $\pm 8.2$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Parity Check ${ }^{1}$</td>
<td style="text-align: center;">52.6</td>
<td style="text-align: center;">52.4</td>
<td style="text-align: center;">51.4 $\pm 0.5$</td>
<td style="text-align: center;">51.6 $\pm 0.6$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Cycle Navigation ${ }^{2}$</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">41.3</td>
<td style="text-align: center;">52.9 $\pm 15.3$</td>
<td style="text-align: center;">32.9 $\pm 4.7$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Stack Manipulation</td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">71.7 $\pm 4.7$</td>
<td style="text-align: center;">55.6 $\pm 2.3$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Reverse String</td>
<td style="text-align: center;">95.1</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">77.1 $\pm 6.6$</td>
<td style="text-align: center;">59.3 $\pm 3.2$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Modular Arithmetic</td>
<td style="text-align: center;">34.8</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">30.3 $\pm 2.6$</td>
<td style="text-align: center;">32.8 $\pm 2.6$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Solve Equation</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">25.4 $\pm 1.8$</td>
<td style="text-align: center;">28.5 $\pm 2.0$</td>
</tr>
<tr>
<td style="text-align: center;">CS</td>
<td style="text-align: center;">DUPLICATE String</td>
<td style="text-align: center;">75.1</td>
<td style="text-align: center;">58.6</td>
<td style="text-align: center;">73.1 $\pm 1.5$</td>
<td style="text-align: center;">54.9 $\pm 1.6$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Merging DUPLICATE</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">64.4</td>
<td style="text-align: center;">81.4 $\pm 9.8$</td>
<td style="text-align: center;">60.3 $\pm 2.3$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ours First</td>
<td style="text-align: center;">69.3</td>
<td style="text-align: center;">64.2</td>
<td style="text-align: center;">65.9 $\pm 1.6$</td>
<td style="text-align: center;">58.1 $\pm 2.6$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Binary Addition</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;">54.9</td>
<td style="text-align: center;">62.0 $\pm 1.1$</td>
<td style="text-align: center;">53.5 $\pm 1.5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Binary Multiplication</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">51.8 $\pm 0.2$</td>
<td style="text-align: center;">52.1 $\pm 2.5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Compile Sqrt</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">54.1</td>
<td style="text-align: center;">52.4 $\pm 0.6$</td>
<td style="text-align: center;">52.3 $\pm 0.9$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Bucket Sort ${ }^{1}$</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">78.3</td>
<td style="text-align: center;">99.5 $\pm 0.3$</td>
<td style="text-align: center;">57.7 $\pm 11.4$</td>
</tr>
</tbody>
</table>
<p>with the best results from Table 1 (for the maximum) and Table 4 (for the mean). We observe that our randomized positional encodings outperform the geometric attention overall (with a $9.7 \%$ higher maximum test accuracy on average) but not on all tasks. In particular, geometric attention performs substantially better on Modular Arithmetic (SIMPLE), which has an inherent locality bias, i.e., numbers closer to the operation symbols are generally more relevant, which can be captured by "radiating outwards" as geometric attention does.</p>
<h2>B. 3 Analysis</h2>
<p>Analyzing the activations As illustrated in Fig. 1, the main intuition behind our randomized encodings is that they do not lead to out-of-distribution activations when evaluating on sequences longer than the maximal training length. We confirm this intuition in our analysis in Fig. 5,
which shows a 2D projection of activations onto the first two principal components when evaluating on sequences of length 40 (i.e., the maximum training length $N$, shown in blue) and length 150 (i.e., the generalization regime, shown in orange), using the same transformation. While the activations of our randomized relative encoding strongly overlap for the training and the generalization regimes in all layers, the standard relative encoding leads to out-of-distribution activations for sequence length 150 in layers 3 and 4 . We obtained qualitatively similar results for the $\sin / \cos$ and learned encodings.</p>
<p>To compute the results in Fig. 5, we generated 30 sequences of length 40 and 150 respectively, on the Reverse String task and passed them through a well-trained model with either relative or randomized relative encodings. For each layer shown, we fitted a (non-whitened) 2D PCA on the activations obtained from sequence length 40 and projected all activations from sequence length 150 into two dimensions using the same transformations (yielding $30 \times 40$ and $30 \times 150$ activationdatapoints per layer). The random relative encoding (our method) attains an average accuracy of 1.0 and 0.994 on the 30 sequences of length 40 and 150 , respectively. The standard relative encoding (the baseline) attains an average accuracy of 1.0 on sequence-length 40 and 0.596 on length 150, indicating the model's failure to generalize well under the standard relative encoding.</p>
<p>Analyzing the attention matrices We also analyze the attention matrices learned with the relative positional encoding and our corresponding random-</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Performance curves on all tasks for all the positional encodings. The dashed vertical red line is the training range, meaning that sequences to the right have not been seen during training and thus measure length generalization. The sequences to the left of the dashed line visualize the in-domain generalization performance.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" />
(b) Randomized relative positional encoding (ours).</p>
<p>Figure 5: 2D PCA projections of the activations of the initial embeddings and the encoder layers for 30 sequences on the REVERSE STRING task. For sequence-lengths beyond the training length (shown in orange), the standard relative encoding clearly leads to out-of-distribution activations for layers 3 and 4 compared to those obtained with the maximum training length (shown in blue). In contrast, our randomized version does not lead to out-of-distribution activations for sequences longer than the maximum training length, confirming the intuition in Fig. 1.
ized version on the REVERSE String task. To that end, we follow Csordás et al. (2022) and visualize the maximum over the 8 attention matrices (one per head) for each of the 5 layers in Fig. 6. We compare the attention matrices for sequences of length 40 (i.e., the maximum training length) and 150 (i.e., significantly longer than the maximum training length). For length 40, both encodings produce a noticeable $X$ pattern, which corresponds to the reversal of the string. However, for length 150, the pattern only remains visible for our randomized encodings while it breaks down for the original version, indicating the failure to generalize.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" />
(a) Relative (baseline) with a sequence of length 40 (in-distribution).
<img alt="img-5.jpeg" src="img-5.jpeg" />
(b) Relative (baseline) with a sequence of length 150 (out-of-distribution).
<img alt="img-6.jpeg" src="img-6.jpeg" />
(c) Randomized relative (our method) with a sequence of length 40 (in-distribution).
<img alt="img-7.jpeg" src="img-7.jpeg" />
(d) Randomized relative (our method) with sequence of length 150 (out-of-distribution).</p>
<p>Figure 6: Analysis of the attention matrices for the relative and randomized relative positional encodings on the REVERSE STRING task using sequences of length 40 (i.e., maximum training length) and 150 (i.e., beyond training lengths). We visualize the maximum over the 8 heads per layer (following Csordás et al., 2022) and observe a clear $X$ pattern, which corresponds to the reversal of the sequence. Our randomized relative encodings maintain that pattern on longer sequences, while it breaks down for the standard relative encoding.</p>
<h1>A A1. Did you describe the limitations of your work?</h1>
<h2>Limitations section</h2>
<p>$\square$ A2. Did you discuss any potential risks of your work?
Not applicable. Left blank.
$\checkmark$ A3. Do the abstract and introduction summarize the paper's main claims?
Section 1
\ A4. Have you used AI writing assistants when working on this paper?
Left blank.</p>
<h2>B $\checkmark$ Did you use or create scientific artifacts?</h2>
<p>Section 4 and Appendix A
$\checkmark$ B1. Did you cite the creators of artifacts you used?
Section 4 and Appendix A
$\checkmark$ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Appendix A
B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?
Section 4 and Appendix A
$\square$ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it?
Not applicable. Left blank.
$\square$ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?
Not applicable. Left blank.
$\checkmark$ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.
Section 4 and Appendix A</p>
<h2>C $\checkmark$ Did you run computational experiments?</h2>
<p>Section 4 and Appendix B
$\checkmark$ C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?</p>
<p>Section 4 and Appendix A
The Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.</p>
<p>C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?
Section 4 and Appendices $A$ and $B$
C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?
Appendix B
C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?
Not applicable. Left blank.</p>
<h1>D Did you use human annotators (e.g., crowdworkers) or research with human participants?</h1>
<p>Left blank.
D1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?
No response.
D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?
No response.
D3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?
No response.
D4. Was the data collection protocol approved (or determined exempt) by an ethics review board? No response.</p>
<p>D5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?
No response.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{*}$ Equal contribution. ${ }^{1}$ DeepMind. ${ }^{2}$ The Swiss AI Lab, IDSIA, USI \&amp; SUPSI. ${ }^{1}$ Work performed while the author was at DeepMind. Correspondence to {anianr, gdelt}@deepmind.com.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-683 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-683</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-683</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-56da914761e445a24481629cfc116336a0aec978</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/56da914761e445a24481629cfc116336a0aec978" target="_blank">Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> Surprisingly, prepending entity definitions in an LM’s context improves performance across all settings, suggesting that there is substantial headroom for parameter-updating approaches for knowledge injection.</p>
                <p><strong>Paper Abstract:</strong> Pre-trained language models (LMs) are used for knowledge intensive tasks like question answering, but their knowledge gets continuously outdated as the world changes. Prior work has studied targeted updates to LMs, injecting individual facts and evaluating whether the model learns these facts while not changing predictions on other contexts. We take a step forward and study LMs’ abilities to make inferences based on injected facts (or propagate those facts): for example, after learning that something is a TV show, does an LM predict that you can watch it? We study this with two cloze-style tasks: an existing dataset of real-world sentences about novel entities (ECBD) as well as a new controlled benchmark with manually designed templates requiring varying levels of inference about injected knowledge. Surprisingly, we find that existing methods for updating knowledge (gradient-based fine-tuning and modifications of this approach) show little propagation of injected knowledge. These methods improve performance on cloze instances only when there is lexical overlap between injected facts and target inferences. Yet, prepending entity definitions in an LM’s context improves performance across all settings, suggesting that there is substantial headroom for parameter-updating approaches for knowledge injection.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e683.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e683.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ROME_format_mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ROME editor input-format incompatibility with definitional datasets</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The ROME model-editing method assumes a (subject, relation, object) prompt format and key-value locality in MLPs; this expectation misaligns with definitional entity descriptions in ECBD, causing failures when forcing definitional inputs into ROME's required prompt structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ROME model editor</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A rank-one weight-modification editor that treats MLP feed-forward layers as key-value memories and rewrites specific key-value pairs to effect local edits.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>dataset / definition sentence format (free-form Wikipedia first sentences)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>ROME editor code / prompt formatting and MLP weight-edit routine</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification / input-format mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>ROME requires inputs in a (subject, relation, object) style and expects either the subject or the object to be retrievable in model activations; ECBD provides free-form definitional sentences (the first sentence of a Wikipedia article) and probe sentences, which often cannot be cleanly converted to ROME's triple format without losing context. Forcing such formatting sacrifices contextual information and often produces very poor generation/perplexity.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model editing interface / prompt formatting and MLP-localization assumptions</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>empirical application of ROME to ECBD and ECBD-EASY; qualitative inspection of formatting effects (Table 7) and quantitative failure (extremely high perplexities)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Perplexity of updated model on ECBD probe tasks (reported >100 in many cases), manual examination of generated text examples after ROME edits (Appendix examples), and failure rates when formatting inputs</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Severe: ROME produced extremely high perplexities (>100) on ECBD and ECBD-EASY when forced into its expected format, so the authors excluded ROME results for ECBD from main reporting; example outputs showed nonsensical or context-losing generations (Table 8).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed broadly across ECBD: multiple examples could not be handled; while only <0.5% of examples were filtered for a specific tokenization mismatch, many others produced large perplexity failures when formatted for ROME.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Implementation assumption in ROME (key-value locality and structured prompt format) mismatches the natural-language definitional inputs used in ECBD; implicit assumption that subject or object is already represented in model activations.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Do not apply ROME to general definitional edits; either adapt ROME's interface to accept richer contextual inputs or use editors designed for definitional/contextual updates (e.g., fine-tuning or other meta-editors), or redesign dataset formatting to preserve essential context while exposing a subject/relation/object pair.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not evaluated successfully in this work; attempts to force formatting led to degraded performance (very high perplexity); no effective mitigation shown.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / model editing for language models</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e683.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e683.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>lexical_overlap_dependency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dependence of parameter-updating editors on lexical overlap between definitions and target inferences</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Parameter-update based editing methods (fine-tuning, MEND, ROME) show substantial propagation only when the injected definition and the probe target share lexical tokens or close surface overlap; when definitions and targets are semantically related but lexically dissimilar, propagation largely fails.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Knowledge editing pipeline (finetuning, MEND, ROME)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Procedures that update model parameters from definition sentences to encode new entity knowledge: standard gradient finetuning (full/last-layer), MEND meta-network edits, and ROME localized MLP edits.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>entity definition sentences and expected inference consequences (natural-language descriptions of entities)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>finetuning / model-editing code that applies parameter updates based on definition text</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification / algorithmic limitation (semantic vs lexical generalization)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Although the natural-language expectation is that providing a definition (d_e) should enable a model to make various inferences (y_e) about an entity, editing implementations predominantly increase probability for targets that are lexically present in the definition. For ECBD (real-world Wikipedia probes) where many probe targets are not verbatim in definitions, model editing yields little or no improvement, whereas on ECBD-EASY or Entity Inferences when the gold span is included verbatim in the definition, editing helps.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>training procedure / generalization from injected data to downstream probe sentences</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>empirical stratified evaluation: split examples into 'Included' (gold span verbatim in definition) vs 'Not Included' and into similarity bins (Jaccard, ROUGE, BERTScore); compare delta perplexity and delta rank/accuracy after edits.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Metrics used: perplexity for ECBD/ECBD-EASY, accuracy for Entity Inferences; measured Δ(perplexity) and Δ(accuracy) between base and updated models, and Δ(rank) of correct answer. Analysis included violin plots stratified by inclusion and similarity bins (Figures 3 and 4).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Substantial: parameter-updating methods (finetuning/MEND) improved ECBD-EASY perplexity (e.g., GPT-Neo FT full model decreased ECBD-EASY perplexity by ~9.0 tokens), and improved accuracy in Entity Inferences (e.g., GPT-Neo FT +23.6 accuracy points), but produced near-zero or negative gains on ECBD where lexical overlap is low. This demonstrates limited applicability of current editors for real-world definitional propagation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Widespread across datasets: ECBD has low overlap with definitions (29 instances where y_e in d_e out of 1000); ECBD-EASY by design has 152/152 included. The correlation between lexical/semantic similarity and editing effectiveness was consistent in Entity Inferences and ECBD-EASY but weak in ECBD.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Editing algorithms largely learn to amplify surface token probabilities seen during updates rather than acquiring deeper, abstracted conceptual associations required for inference; natural-language definitions and probe tasks often require multi-hop commonsense reasoning not captured by few-shot parameter updates.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use richer updating procedures that target propagation/generalization (e.g., train editors on multi-example propagation, design meta-editors to map definitions to broader inference patterns), or rely on contextual input augmentation (prepending definitions) as an upper-bound baseline while developing scalable parameter-update approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Partial: Input augmentation (prepending definition) consistently improved performance across settings (e.g., T5 Definition: +30.6 accuracy on Entity Inferences), showing the information in definitions is sufficient when presented at inference time; however, this is not scalable and does not solve the parameter-update propagation problem. Existing editors (MEND) were largely ineffective except in high-overlap cases.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / NLP (knowledge editing)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e683.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e683.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>metric_tokenization_mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Perplexity incomparability across models due to tokenizer/architecture differences</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Perplexity computed over masked spans is not directly comparable across different base models because tokenization and architecture choices change the per-token decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Evaluation metrics pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Per-token perplexity used as a base metric for ECBD evaluations on different LM architectures (GPT-Neo, GPT2-XL, T5), combined with accuracy for Entity Inferences.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>experimental protocol / evaluation metric description</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>evaluation scripts computing per-token perplexity and accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>measurement mismatch / metric non-equivalence</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>The paper notes that due to differences in model architectures and tokenizers, perplexity measured on masked spans cannot be directly compared across base models; thus cross-model comparisons using perplexity would be misleading unless normalized or otherwise accounted for.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>evaluation metrics / result comparison stage</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>methodological reasoning and explicit note in methods section stating that metric does not allow comparison across different base models</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>No numeric normalization performed; statement based on tokenization differences (explained qualitatively).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Limits interpretability: authors avoid direct cross-model perplexity comparisons and instead use accuracy where possible; prevents pooled statistical comparisons across architectures using raw perplexity.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Intrinsic to experiments involving models with different tokenizers; applies to all cross-architecture perplexity evaluations in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Different tokenization strategies and model architectures produce different token segmentations of the same target spans, so per-token perplexity is not a consistent unit across models.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use architecture-appropriate metrics (accuracy for multiple-choice tasks), avoid cross-model perplexity comparisons, or normalize perplexity by tokenization (not implemented here).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Partial: authors used accuracy for Entity Inferences to enable cross-model comparisons and reported perplexity only within-model comparisons; no normalization attempted.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / NLP evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e683.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e683.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>in_context_vs_param_update_gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Performance vs. scalability gap: in-context (prepending) definitions outperform parameter updates but are not scalable</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prepending entity definitions at inference time (input augmentation) gives robust performance gains across datasets, outperforming parameter-update methods, but this method increases inference cost and does not provide persistent model updates—creating a gap between achievable performance and deployable/scalable implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Input-augmentation (prepending definitions) vs parameter-update pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Two approaches to inject new entity knowledge: (1) adding definition sentences to model input at inference time; (2) editing model parameters via finetuning, MEND, or ROME to embed definitions permanently.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>experimental protocol / method description (prepending definitions vs updating parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>inference data preprocessing (input prepending) and parameter-update code (finetuning/MEND/ROME)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>scalability vs performance tradeoff / deployment mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Although prepending definitions at inference time yields consistent and substantial improvements (e.g., T5 Definition +30.6 accuracy on Entity Inferences, consistent perplexity reductions on ECBD), it is computationally expensive at inference time and not a viable long-term deployment solution; parameter-update implementations that would be deployable fail to match this performance.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>inference-time pipeline vs model-update/training pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>empirical comparison of methods across datasets (Table 3) showing that Prepend-Def consistently yields the best target metric improvements while parameter update methods lag.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Reported differences in accuracy and perplexity (Table 3); example: T5-large Prepend-Def accuracy on Entity Inferences = 73.5 (+30.6) vs FT (full model) = 64.7 (+21.8). Prepend-Def decreased ECBD perplexity more than edits in many settings.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Demonstrates there is untapped headroom for parameter-update approaches; highlights a practical deployment gap where the best-performing experimental intervention is not operationally scalable.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed consistently across base models and datasets tested.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Input-augmentation uses the full definitional context at inference time, so models can directly leverage surface cues; parameter updates require learning to internalize those cues into weights via few updates and limited data, which current editors do poorly at.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Develop better editing methods that can match in-context performance (e.g., editors trained for propagation/generalization), hybrid approaches (cache inferences, selective context prepending), or scalable retrieval + context-injection systems.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not achieved in this work; authors note the need for future work to close the gap. Prepend-Def is effective but not scalable.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / NLP deployment and model editing</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e683.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e683.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>single_run_resource_limitation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Single-run experiments and limited hyperparameter exploration affecting reproducibility</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The experiments were run as single runs (no repeats) due to compute constraints and limited hyperparameter search, creating uncertainty about the stability of reported results and making exact reproduction harder.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Experimental evaluation infrastructure</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Training and evaluation scripts for finetuning and editing methods, executed on limited GPU resources (four Quadro RTX 8000 GPUs, <4 GPU hours) with small search over learning rates and epochs.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>experimental protocol and result reporting in paper text (methods and appendix)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>training and evaluation scripts (single-run experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete experimental specification / single-run reporting</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>The paper reports results from a single run per experiment and notes limited computational resources prevented multiple runs; hyperparameter choices are reported but not exhaustively tuned or reported with variability (no error bars), which introduces uncertainty about variability and statistical significance.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>experimental methodology / result reporting</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>explicit statements in methods and appendix noting one-run experiments and limited compute budget</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>No error bars or multiple seeds provided; statements in Appendix A.3 describing number of runs.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Moderate: reduces confidence in stability of quantitative improvements, particularly for small deltas; hinders reproducibility and estimation of variance.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Applies to this paper's experiments; common in resource-constrained empirical ML work but explicitly noted here.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Limited computational resources and runtime constraints prevented repeating experiments and producing variance estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Recommend running multiple seeds, reporting standard deviations or confidence intervals, and publishing hyperparameter search logs; potential use of cheaper probe subsets or smaller models for stability checks.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not applied in this work; authors explicitly acknowledge limitation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / empirical NLP</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Fast Model Editing at Scale <em>(Rating: 2)</em></li>
                <li>Locating and Editing Factual Associations in GPT <em>(Rating: 2)</em></li>
                <li>Editing Factual Knowledge in Language Models <em>(Rating: 2)</em></li>
                <li>Methods for Measuring, Updating, and Visualizing Factual Beliefs in Language Models <em>(Rating: 1)</em></li>
                <li>Probing Factually Grounded Content Transfer with Factual Ablation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-683",
    "paper_id": "paper-56da914761e445a24481629cfc116336a0aec978",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "ROME_format_mismatch",
            "name_full": "ROME editor input-format incompatibility with definitional datasets",
            "brief_description": "The ROME model-editing method assumes a (subject, relation, object) prompt format and key-value locality in MLPs; this expectation misaligns with definitional entity descriptions in ECBD, causing failures when forcing definitional inputs into ROME's required prompt structure.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ROME model editor",
            "system_description": "A rank-one weight-modification editor that treats MLP feed-forward layers as key-value memories and rewrites specific key-value pairs to effect local edits.",
            "nl_description_type": "dataset / definition sentence format (free-form Wikipedia first sentences)",
            "code_implementation_type": "ROME editor code / prompt formatting and MLP weight-edit routine",
            "gap_type": "incomplete specification / input-format mismatch",
            "gap_description": "ROME requires inputs in a (subject, relation, object) style and expects either the subject or the object to be retrievable in model activations; ECBD provides free-form definitional sentences (the first sentence of a Wikipedia article) and probe sentences, which often cannot be cleanly converted to ROME's triple format without losing context. Forcing such formatting sacrifices contextual information and often produces very poor generation/perplexity.",
            "gap_location": "model editing interface / prompt formatting and MLP-localization assumptions",
            "detection_method": "empirical application of ROME to ECBD and ECBD-EASY; qualitative inspection of formatting effects (Table 7) and quantitative failure (extremely high perplexities)",
            "measurement_method": "Perplexity of updated model on ECBD probe tasks (reported &gt;100 in many cases), manual examination of generated text examples after ROME edits (Appendix examples), and failure rates when formatting inputs",
            "impact_on_results": "Severe: ROME produced extremely high perplexities (&gt;100) on ECBD and ECBD-EASY when forced into its expected format, so the authors excluded ROME results for ECBD from main reporting; example outputs showed nonsensical or context-losing generations (Table 8).",
            "frequency_or_prevalence": "Observed broadly across ECBD: multiple examples could not be handled; while only &lt;0.5% of examples were filtered for a specific tokenization mismatch, many others produced large perplexity failures when formatted for ROME.",
            "root_cause": "Implementation assumption in ROME (key-value locality and structured prompt format) mismatches the natural-language definitional inputs used in ECBD; implicit assumption that subject or object is already represented in model activations.",
            "mitigation_approach": "Do not apply ROME to general definitional edits; either adapt ROME's interface to accept richer contextual inputs or use editors designed for definitional/contextual updates (e.g., fine-tuning or other meta-editors), or redesign dataset formatting to preserve essential context while exposing a subject/relation/object pair.",
            "mitigation_effectiveness": "Not evaluated successfully in this work; attempts to force formatting led to degraded performance (very high perplexity); no effective mitigation shown.",
            "domain_or_field": "machine learning / model editing for language models",
            "reproducibility_impact": true,
            "uuid": "e683.0",
            "source_info": {
                "paper_title": "Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "lexical_overlap_dependency",
            "name_full": "Dependence of parameter-updating editors on lexical overlap between definitions and target inferences",
            "brief_description": "Parameter-update based editing methods (fine-tuning, MEND, ROME) show substantial propagation only when the injected definition and the probe target share lexical tokens or close surface overlap; when definitions and targets are semantically related but lexically dissimilar, propagation largely fails.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Knowledge editing pipeline (finetuning, MEND, ROME)",
            "system_description": "Procedures that update model parameters from definition sentences to encode new entity knowledge: standard gradient finetuning (full/last-layer), MEND meta-network edits, and ROME localized MLP edits.",
            "nl_description_type": "entity definition sentences and expected inference consequences (natural-language descriptions of entities)",
            "code_implementation_type": "finetuning / model-editing code that applies parameter updates based on definition text",
            "gap_type": "incomplete specification / algorithmic limitation (semantic vs lexical generalization)",
            "gap_description": "Although the natural-language expectation is that providing a definition (d_e) should enable a model to make various inferences (y_e) about an entity, editing implementations predominantly increase probability for targets that are lexically present in the definition. For ECBD (real-world Wikipedia probes) where many probe targets are not verbatim in definitions, model editing yields little or no improvement, whereas on ECBD-EASY or Entity Inferences when the gold span is included verbatim in the definition, editing helps.",
            "gap_location": "training procedure / generalization from injected data to downstream probe sentences",
            "detection_method": "empirical stratified evaluation: split examples into 'Included' (gold span verbatim in definition) vs 'Not Included' and into similarity bins (Jaccard, ROUGE, BERTScore); compare delta perplexity and delta rank/accuracy after edits.",
            "measurement_method": "Metrics used: perplexity for ECBD/ECBD-EASY, accuracy for Entity Inferences; measured Δ(perplexity) and Δ(accuracy) between base and updated models, and Δ(rank) of correct answer. Analysis included violin plots stratified by inclusion and similarity bins (Figures 3 and 4).",
            "impact_on_results": "Substantial: parameter-updating methods (finetuning/MEND) improved ECBD-EASY perplexity (e.g., GPT-Neo FT full model decreased ECBD-EASY perplexity by ~9.0 tokens), and improved accuracy in Entity Inferences (e.g., GPT-Neo FT +23.6 accuracy points), but produced near-zero or negative gains on ECBD where lexical overlap is low. This demonstrates limited applicability of current editors for real-world definitional propagation tasks.",
            "frequency_or_prevalence": "Widespread across datasets: ECBD has low overlap with definitions (29 instances where y_e in d_e out of 1000); ECBD-EASY by design has 152/152 included. The correlation between lexical/semantic similarity and editing effectiveness was consistent in Entity Inferences and ECBD-EASY but weak in ECBD.",
            "root_cause": "Editing algorithms largely learn to amplify surface token probabilities seen during updates rather than acquiring deeper, abstracted conceptual associations required for inference; natural-language definitions and probe tasks often require multi-hop commonsense reasoning not captured by few-shot parameter updates.",
            "mitigation_approach": "Use richer updating procedures that target propagation/generalization (e.g., train editors on multi-example propagation, design meta-editors to map definitions to broader inference patterns), or rely on contextual input augmentation (prepending definitions) as an upper-bound baseline while developing scalable parameter-update approaches.",
            "mitigation_effectiveness": "Partial: Input augmentation (prepending definition) consistently improved performance across settings (e.g., T5 Definition: +30.6 accuracy on Entity Inferences), showing the information in definitions is sufficient when presented at inference time; however, this is not scalable and does not solve the parameter-update propagation problem. Existing editors (MEND) were largely ineffective except in high-overlap cases.",
            "domain_or_field": "machine learning / NLP (knowledge editing)",
            "reproducibility_impact": true,
            "uuid": "e683.1",
            "source_info": {
                "paper_title": "Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "metric_tokenization_mismatch",
            "name_full": "Perplexity incomparability across models due to tokenizer/architecture differences",
            "brief_description": "Perplexity computed over masked spans is not directly comparable across different base models because tokenization and architecture choices change the per-token decomposition.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Evaluation metrics pipeline",
            "system_description": "Per-token perplexity used as a base metric for ECBD evaluations on different LM architectures (GPT-Neo, GPT2-XL, T5), combined with accuracy for Entity Inferences.",
            "nl_description_type": "experimental protocol / evaluation metric description",
            "code_implementation_type": "evaluation scripts computing per-token perplexity and accuracy",
            "gap_type": "measurement mismatch / metric non-equivalence",
            "gap_description": "The paper notes that due to differences in model architectures and tokenizers, perplexity measured on masked spans cannot be directly compared across base models; thus cross-model comparisons using perplexity would be misleading unless normalized or otherwise accounted for.",
            "gap_location": "evaluation metrics / result comparison stage",
            "detection_method": "methodological reasoning and explicit note in methods section stating that metric does not allow comparison across different base models",
            "measurement_method": "No numeric normalization performed; statement based on tokenization differences (explained qualitatively).",
            "impact_on_results": "Limits interpretability: authors avoid direct cross-model perplexity comparisons and instead use accuracy where possible; prevents pooled statistical comparisons across architectures using raw perplexity.",
            "frequency_or_prevalence": "Intrinsic to experiments involving models with different tokenizers; applies to all cross-architecture perplexity evaluations in the paper.",
            "root_cause": "Different tokenization strategies and model architectures produce different token segmentations of the same target spans, so per-token perplexity is not a consistent unit across models.",
            "mitigation_approach": "Use architecture-appropriate metrics (accuracy for multiple-choice tasks), avoid cross-model perplexity comparisons, or normalize perplexity by tokenization (not implemented here).",
            "mitigation_effectiveness": "Partial: authors used accuracy for Entity Inferences to enable cross-model comparisons and reported perplexity only within-model comparisons; no normalization attempted.",
            "domain_or_field": "machine learning / NLP evaluation",
            "reproducibility_impact": true,
            "uuid": "e683.2",
            "source_info": {
                "paper_title": "Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "in_context_vs_param_update_gap",
            "name_full": "Performance vs. scalability gap: in-context (prepending) definitions outperform parameter updates but are not scalable",
            "brief_description": "Prepending entity definitions at inference time (input augmentation) gives robust performance gains across datasets, outperforming parameter-update methods, but this method increases inference cost and does not provide persistent model updates—creating a gap between achievable performance and deployable/scalable implementations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Input-augmentation (prepending definitions) vs parameter-update pipeline",
            "system_description": "Two approaches to inject new entity knowledge: (1) adding definition sentences to model input at inference time; (2) editing model parameters via finetuning, MEND, or ROME to embed definitions permanently.",
            "nl_description_type": "experimental protocol / method description (prepending definitions vs updating parameters)",
            "code_implementation_type": "inference data preprocessing (input prepending) and parameter-update code (finetuning/MEND/ROME)",
            "gap_type": "scalability vs performance tradeoff / deployment mismatch",
            "gap_description": "Although prepending definitions at inference time yields consistent and substantial improvements (e.g., T5 Definition +30.6 accuracy on Entity Inferences, consistent perplexity reductions on ECBD), it is computationally expensive at inference time and not a viable long-term deployment solution; parameter-update implementations that would be deployable fail to match this performance.",
            "gap_location": "inference-time pipeline vs model-update/training pipeline",
            "detection_method": "empirical comparison of methods across datasets (Table 3) showing that Prepend-Def consistently yields the best target metric improvements while parameter update methods lag.",
            "measurement_method": "Reported differences in accuracy and perplexity (Table 3); example: T5-large Prepend-Def accuracy on Entity Inferences = 73.5 (+30.6) vs FT (full model) = 64.7 (+21.8). Prepend-Def decreased ECBD perplexity more than edits in many settings.",
            "impact_on_results": "Demonstrates there is untapped headroom for parameter-update approaches; highlights a practical deployment gap where the best-performing experimental intervention is not operationally scalable.",
            "frequency_or_prevalence": "Observed consistently across base models and datasets tested.",
            "root_cause": "Input-augmentation uses the full definitional context at inference time, so models can directly leverage surface cues; parameter updates require learning to internalize those cues into weights via few updates and limited data, which current editors do poorly at.",
            "mitigation_approach": "Develop better editing methods that can match in-context performance (e.g., editors trained for propagation/generalization), hybrid approaches (cache inferences, selective context prepending), or scalable retrieval + context-injection systems.",
            "mitigation_effectiveness": "Not achieved in this work; authors note the need for future work to close the gap. Prepend-Def is effective but not scalable.",
            "domain_or_field": "machine learning / NLP deployment and model editing",
            "reproducibility_impact": true,
            "uuid": "e683.3",
            "source_info": {
                "paper_title": "Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "single_run_resource_limitation",
            "name_full": "Single-run experiments and limited hyperparameter exploration affecting reproducibility",
            "brief_description": "The experiments were run as single runs (no repeats) due to compute constraints and limited hyperparameter search, creating uncertainty about the stability of reported results and making exact reproduction harder.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Experimental evaluation infrastructure",
            "system_description": "Training and evaluation scripts for finetuning and editing methods, executed on limited GPU resources (four Quadro RTX 8000 GPUs, &lt;4 GPU hours) with small search over learning rates and epochs.",
            "nl_description_type": "experimental protocol and result reporting in paper text (methods and appendix)",
            "code_implementation_type": "training and evaluation scripts (single-run experiments)",
            "gap_type": "incomplete experimental specification / single-run reporting",
            "gap_description": "The paper reports results from a single run per experiment and notes limited computational resources prevented multiple runs; hyperparameter choices are reported but not exhaustively tuned or reported with variability (no error bars), which introduces uncertainty about variability and statistical significance.",
            "gap_location": "experimental methodology / result reporting",
            "detection_method": "explicit statements in methods and appendix noting one-run experiments and limited compute budget",
            "measurement_method": "No error bars or multiple seeds provided; statements in Appendix A.3 describing number of runs.",
            "impact_on_results": "Moderate: reduces confidence in stability of quantitative improvements, particularly for small deltas; hinders reproducibility and estimation of variance.",
            "frequency_or_prevalence": "Applies to this paper's experiments; common in resource-constrained empirical ML work but explicitly noted here.",
            "root_cause": "Limited computational resources and runtime constraints prevented repeating experiments and producing variance estimates.",
            "mitigation_approach": "Recommend running multiple seeds, reporting standard deviations or confidence intervals, and publishing hyperparameter search logs; potential use of cheaper probe subsets or smaller models for stability checks.",
            "mitigation_effectiveness": "Not applied in this work; authors explicitly acknowledge limitation.",
            "domain_or_field": "machine learning / empirical NLP",
            "reproducibility_impact": true,
            "uuid": "e683.4",
            "source_info": {
                "paper_title": "Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Fast Model Editing at Scale",
            "rating": 2
        },
        {
            "paper_title": "Locating and Editing Factual Associations in GPT",
            "rating": 2
        },
        {
            "paper_title": "Editing Factual Knowledge in Language Models",
            "rating": 2
        },
        {
            "paper_title": "Methods for Measuring, Updating, and Visualizing Factual Beliefs in Language Models",
            "rating": 1
        },
        {
            "paper_title": "Probing Factually Grounded Content Transfer with Factual Ablation",
            "rating": 1
        }
    ],
    "cost": 0.014188,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge</h1>
<p>Yasumasa Onoe, Michael J.Q. Zhang, Shankar Padmanabhan, Greg Durrett, Eunsol Choi<br>Department of Computer Science<br>The University of Texas at Austin<br>yasumasa@utexas.edu</p>
<h4>Abstract</h4>
<p>Pre-trained language models (LMs) are used for knowledge intensive tasks like question answering, but their knowledge gets continuously outdated as the world changes. Prior work has studied targeted updates to LMs, injecting individual facts and evaluating whether the model learns these facts while not changing predictions on other contexts. We take a step forward and study LMs' abilities to make inferences based on injected facts (or propagate those facts): for example, after learning that something is a TV show, does an LM predict that you can watch it? We study this with two cloze-style tasks: an existing dataset of real-world sentences about novel entities (ECBD) as well as a new controlled benchmark with manually designed templates requiring varying levels of inference about injected knowledge. Surprisingly, we find that existing methods for updating knowledge (gradient-based fine-tuning and modifications of this approach) show little propagation of injected knowledge. These methods improve performance on cloze instances only when there is lexical overlap between injected facts and target inferences. Yet, prepending entity definitions in an LM's context improves performance across all settings, suggesting that there is substantial headroom for parameter-updating approaches for knowledge injection.</p>
<h2>1 Introduction</h2>
<p>Pre-trained language models (LMs) acquire comprehensive real-world knowledge from massive amounts of pre-training data, allowing them to use this knowledge effectively in downstream tasks. However, without continual updating, the knowledge contained within these backend LMs will eventually become outdated. This temporal mismatch affects model performance on downstream tasks (Zhang and Choi, 2021; Dhingra et al., 2022a; Lazaridou et al., 2021; Jang et al., 2022b). As LMs become more widely deployed, their knowledge
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Knowledge editing tasks. We study a challenging entity knowledge propagation task where language models should make inferences after learning entities from their definitions. This differs from past knowledge editing which evaluates paraphrases of injected facts.
should be synced with the current state of the world while maintaining reasonable deployment costs.</p>
<p>Prior work has investigated knowledge editing in pre-trained LMs, updating model parameters to alter outputs to match what users want (Zhu et al., 2020; Sinitsin et al., 2020; De Cao et al., 2021; Mitchell et al., 2022; Meng et al., 2022; Hase et al., 2023). In these studies, the original fact and the altered fact are provided (e.g., changing " X was born in Y." to " X was born in Z."), and models are evaluated after a single update on each instance; see Figure 1 for an example. These model editing methods successfully provide targeted updates, fixing incorrect or outdated individual facts. Yet, can LMs make inferences based on updated knowledge? Past evaluation has largely focused on two aspects of knowledge editing, whether the edits were successfully injected and whether other irrelevant sentences were impacted, but do not capture</p>
<p>whether the LMs now can reason based on the new fact that has been injected.</p>
<p>We take a step further and evaluate whether LMs can propagate updated knowledge about new entities. We first inject definitions about the entity into LMs using various knowledge editing methods (Mitchell et al., 2022; Meng et al., 2022), then evaluate LMs' performance on cloze tasks on a wide range of sentences about the entity (see Figure 1 for an example). We refer to this task as entity knowledge propagation and introduce two cloze datasets to evaluate this challenging task.</p>
<p>Our first evaluation benchmark is the Entity Cloze By Date (ECBD) dataset (Onoe et al., 2022), which presents novel entities tagged with origination dates (e.g., Hurricane Ian, 2022), their definition and probe sentences taken from their Wikipedia page. The task is to fill a masked span in probe sentences. Because Wikipedia contains a wide range of information, much of it not inferable from an entity's definition, injecting entity knowledge via its definition has an unclear impact on the probe sentences; filling in the masked span is nontrivial even after the entity definition is provided. For more controlled study, we introduce a new benchmark (Entity Inferences) with manually designed probe sentences with multiplechoice answer options. Once one learns about the definition of an emerging entity, finding the correct answer for these probe sentences is easy.</p>
<p>We find that existing parameter updating methods can handle simpler inferences in ENTITY INFERENCES, but fail to improve performances in ECBD, revealing a limitation in these methods. We further analyze the impact of fine-tuning. Distressingly, we find that simply prepending information in-context works very well, and matching the performance of this via parameter updates is challenging. A deeper analysis finds that model editing shows promising results only when the injected definition sentence and the cloze inference have lexical overlap. Our work establishes an evaluation paradigm and opens doors for work on editing methods that can propagate entity knowledge. The code and data are available at https://github.com/yasumasaonoe/ entity_knowledge_propagation.</p>
<h2>2 Entity Knowledge Propagation</h2>
<p>We propose Entity Knowledge Propagation (EKP), a new task where we want to update model param-
eters to reflect an emerging entity that is unseen in the LMs' pre-training corpus. For example, BERT was trained in 2018, so COVID-19 is an emerging entity to BERT. We explore various ways of editing model parameters based on definition sentences to inject new knowledge. Once we inject the knowledge of the emerging entity into the model parameters, we evaluate the updated model's ability to reason about the emerging entity.</p>
<h3>2.1 Task Definition</h3>
<p>Formally, we have a language model $f_{\theta}$ with parameters $\theta$. An input to the model consists of a (partial) sentence or chunk of text $x_{e}$ that contains at least one explicit reference to an emerging entity $e$ (i.e., invoking $e$ by name). We use $f_{\theta}\left(y_{e} \mid x_{e}\right)$ to denote placing a probability distribution over a text sequence $y_{e}$ given the text $x_{e}$.</p>
<p>Our data instances have the property that $y_{e}$ represents an inference we make about the entity: $y_{e}$ must be related to the entity $e$ such that an LM should give higher probability to it if the LM "knows" $e$ well. We do not expect the raw model $f_{\theta}$ to perform well without any updates, since the entity $e$ is completely unseen during the pre-training stage. We assume that the emerging entity comes with a short definition sentence $d_{e}$ that provides basic information about the entity. This provides the basis for the update to $f_{\theta}$.</p>
<p>To summarize, each example $\left\langle e, d_{e}, x_{e}, y_{e}\right\rangle \in \mathcal{D}$ consists of an emerging entity $e$, a definition sentence $d_{e}$, a probe sentence $x_{e}$, and a gold completion $y_{e}$. Knowledge editing methods will compute $\theta^{\prime} \leftarrow \operatorname{update}\left(\theta, e, d_{e}\right)$, updating parameters $\theta$ regarding $e$ and its definition $d_{e}$, to give higher probability for future inferences about $e$ like those expressed by $x_{e}$ and $y_{e}$ (examples in Figure 1).</p>
<p>Metrics Following prior work in the knowledge updating literature (Zhu et al., 2020; De Cao et al., 2021; Mitchell et al., 2022; Meng et al., 2022; Hase et al., 2023), we will evaluate two criteria: update success and specificity. Each of these criteria is evaluated with respect to a base metric, which is either perplexity or accuracy, depending on our dataset. We will define them here in the case of perplexity (lower is better); we will use the same definitions for accuracy, but the desired trends will be opposite.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">Entity $(e)$</th>
<th style="text-align: left;">Definition $\left(d_{e}\right)$</th>
<th style="text-align: left;">Probe Sentence $\left(x_{e}\right)$</th>
<th style="text-align: left;">Gold Span $\left(y_{e} /\left{C_{y}\right}\right)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Entity <br> Inferences</td>
<td style="text-align: left;">Dracula</td>
<td style="text-align: left;">Dracula is a drama horror television <br> serial developed by Mark Gatiss...</td>
<td style="text-align: left;">Dracula makes me <br> feel <MASK>.</td>
<td style="text-align: left;">scared / { athletic, <br> brave, emotional, ... }</td>
</tr>
<tr>
<td style="text-align: left;">ECBD</td>
<td style="text-align: left;">Brexit</td>
<td style="text-align: left;">Brexit was the withdrawal of the <br> United Kingdom (UK) from the <br> European Union (EU) at 23:00...</td>
<td style="text-align: left;">Studies estimate that Brexit <br> and the end of <MASK> <br> will likely result in a large...</td>
<td style="text-align: left;">free movement</td>
</tr>
<tr>
<td style="text-align: left;">ECBD-EASY</td>
<td style="text-align: left;">Magnum <br> Fire</td>
<td style="text-align: left;">The Mangum Fire was a wildfire. <br> burning in Kaibab National Forest <br> in Arizona in the United States.</td>
<td style="text-align: left;">On June 14, the Mangum Fire <br> jumped control lines towards <br> Mangum Springs, <MASK>...</td>
<td style="text-align: left;">Arizona</td>
</tr>
</tbody>
</table>
<p>Table 1: Examples from each dataset outlined in Section 3. Unlike ECBD and ECBD-Easy, the gold spans in Entity Inferences examples are always one of several multiple-choice options per example.</p>
<p>For update success, we will measure if the perplexity of the updated model $\operatorname{ppl}\left(f_{\theta^{\prime}}\left(y_{e} \mid x_{e}\right)\right)$ is better than the raw model $\operatorname{ppl}\left(f_{\theta}\left(y_{e} \mid x_{e}\right)\right)$ (lower perplexity is better). For specificity, we compute the difference between the post-update perplexity and pre-update perplexity $\operatorname{ppl}\left(f_{\theta^{\prime}}\left(y_{\tilde{e}} \mid\right.\right.$ $\left.\left.x_{\tilde{e}}\right)\right)-\operatorname{ppl}\left(f\left(y_{\tilde{e}} \mid x_{\tilde{e}}\right)\right)$ for $\tilde{e} \neq e$, entities other than $e$. Ideally, we want this perplexity value to be close to zero; a positive value indicates that perplexity has gotten worse on these entities after the update. It can theoretically be negative if the update makes the LM to guess irrelevant examples better.</p>
<p>Comparison with prior tasks Similar editing procedures have been explored in the literature, but with key differences from our setting. A line of work on knowledge editing (Zhu et al., 2020; De Cao et al., 2021; Mitchell et al., 2022) addresses a version of our task where $f_{\theta}$ is updated to encode information about a particular fact. This could be written as $\theta^{\prime} \leftarrow \operatorname{update}\left(\theta, x_{e}, y\right)$. They then evaluate $f_{\theta}\left(y \mid \tilde{x}<em e="e">{e}\right)$ on perturbed inputs $\tilde{x}</em>$ may have little overlap.}$ that are paraphrases of the $x_{e}$ they inject. The answer $y$ is visible when the network is updated and it simply needs to be preserved for future (paraphrased) queries. By contrast, in our setting, $y$ and the injected definition $d_{e</p>
<p>ROME (Meng et al., 2022) addresses knowledge editing as well as a variant of counterfactual model editing. This task involves an update similar in spirit to ours: $\theta^{\prime} \leftarrow \operatorname{update}\left(\theta, e,\left(x_{e, 1}, y_{e, 1}\right)\right)$ that updates a completion of a sentence (e.g., $x_{e, 1}=$ the Eiffel Tower is located in, $y_{e, 1}=$ Rome $)$ and then expects the knowledge to be usable for other inference pairs $\left(x_{e, 2}, y_{e, 2}\right)$. These differ in that the injected knowledge is not a complete definition of an entity; while their method could theoretically be used for our task, it relies on localizing and editing existing information about $e$. Therefore,</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: right;"># Examples</th>
<th style="text-align: right;"># Entities</th>
<th style="text-align: right;">$y_{e}$ in $d_{e}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Entity Inferences</td>
<td style="text-align: right;">170</td>
<td style="text-align: right;">85</td>
<td style="text-align: right;">92</td>
</tr>
<tr>
<td style="text-align: left;">ECBD</td>
<td style="text-align: right;">1000</td>
<td style="text-align: right;">208</td>
<td style="text-align: right;">29</td>
</tr>
<tr>
<td style="text-align: left;">ECBD-easy</td>
<td style="text-align: right;">152</td>
<td style="text-align: right;">74</td>
<td style="text-align: right;">152</td>
</tr>
</tbody>
</table>
<p>Table 2: Statistics from each EKP dataset. We report the number of examples in each evaluation set in addition to the number of unique entities and total instances where the gold span can be found within the entity description.
it is less appropriate in handling emerging entities, as our results will show.</p>
<h2>3 Constructing benchmarks for EKP</h2>
<p>We use two different types of datasets to investigate how new entity knowledge is propagated into the LM's parameter space. Table 2 summarizes the dataset statistics on two benchmarks, including the extent to which the target spans $y$ overlap with the definitions $d_{e}$, which will be important later.</p>
<h3>3.1 ECBD</h3>
<p>Entity Cloze By Date (Onoe et al., 2022, ECBD) presents entities indexed by their origination dates paired with probe sentences containing those entities. In addition, the dataset provides the definition sentence (first sentence sentence of Wikipedia article) for each entity. The original task focuses on general temporal adaptation of language models, evaluating model's perplexity in predicting masked spans in probe sentences. We repurpose this dataset to focus on targeted knowledge updates and the propagation of entity knowledge. We take entities with origination date between 2020/01 and 2021/09 to ensure they are unseen by the LMs we study.</p>
<p>These instances fall into the paradigm discussed in Section 2.1 (example shown in Table 1):
$e$ : Entity : the title of the Wikipedia article</p>
<p>$d_{e}:$ DefinitionSentence : the first sentence of the Wikipedia article for the entity.
$x_{e}:$ ProbeSentence : a sentence selected from the Wikipedia article according to the procedure described in Onoe et al. (2022)
$y:$ GoldSpan : the target span as described in Onoe et al. (2022)</p>
<p>ECBD-EASY We filter ECBD to create ECBDeasy, a subset where knowledge propagation should be easier. Specifically, we take cases where the target masked span $y$ is contained in the definition sentence $d_{e}$ verbatim; such examples are more congruent with the formulation of past work such as MEND and are typically easier, as simply boosting the probability of the definition tokens can improve perplexity on the gold span.</p>
<p>Evaluation Metrics Following Onoe et al. (2022), we compute per-token perplexity over the masked spans. Because of differences in model architecture such as tokenizer choice, this metric does not allow comparison across different base models. We randomly sample 40 entities as $\hat{e}$ from ECBD popular subset to measure specificity.</p>
<h3>3.2 Entity Inferences</h3>
<p>While ECBD contains real-world sentences spanning a broad domain, it presents a very challenging task even for humans, often requiring rich knowledge and various types of reasoning and inference. For a more controlled study targeting on knowledge propagation, we construct a new dataset we name as Entity InFERENCES.</p>
<p>In this dataset, choosing the correct span is much easier when given the definition sentence. Further, instead of requiring LMs to predict spans from open vocabulary, we provide a set of candidate spans and evaluate whether LMs can assign higher probability to the correct answer candidate. Instances here are designed to be similar to ECBD, but the probe sentences $x_{e}$ are handcrafted to elicit the target inference type, and the gold span $y$ comes with an associated set $\left{C_{y}\right}$ of options.</p>
<p>Data Construction Details We first curate entities tagged with TV shows and natural disasters from English Wikipedia and their definition sentences from the 2020 and 2021 subsets of ECBD. In addition to real entities, we generate examples of "fake" people where we fabricate person names along with their definitions (e.g., Leighanna Smith
(born July 21, 1970) is an American film director, screenwriter, and producer...).</p>
<p>We then manually craft probe sentences targeting two types of reasoning: explicit and implicit. The explicit probe sentences ask information that is explicitly stated in the definition sentence (e.g., genre of a TV show). On the other hand, the implicit probe sentences require commonsense-like information (e.g., people watch a TV show, rather than eat a TV show.).</p>
<p>Evaluation metrics For this multiple-choice cloze task, we evaluate knowledge propagation by measuring accuracy (i.e., how often the gold label gets the highest probability over all answer candidates). In addition, we compute the specificity score by evaluating a model on other probe sentences from similar entities.</p>
<h2>4 Experimental Setup</h2>
<h3>4.1 Base Language Models</h3>
<p>Model architectures can have impact on their capabilities of acquiring entity knowledge. Thus, we consider both left-to-right and seq-to-seq model architectures. Specifically, we use GPT-Neo 1.3B (Black et al., 2021) ${ }^{2}$ and T5-large (Raffel et al., 2020) ${ }^{3}$ as base language models ( $f_{\theta}$ ), available via Huggingface Transformers (Wolf et al., 2020). We additionally consider GPT2-XL (Radford et al., 2019) as a base model to closely follow the protocol presented in ROME paper (Meng et al., 2022).</p>
<h3>4.2 Parameter Updating Methods</h3>
<p>Finetuning is a common way for adapting a pretrained LM to a specific task or domain (Gururangan et al., 2020). In a similar vein, we aim to adopt a pretrained LM to an environment where new entities constantly arise. Given $e$ and its definition $d_{e}$, we update the parameters $\theta$ to minimize loss on a training example formed from $d_{e}$. For left-to-right models (e.g., GPT-Neo), we use the standard next token prediction language modeling task on the entire $d_{e}$ example. For mask filling models (T5), we randomly select a span ${ }^{4}$ that is not overlapping with the entity mention span, following Onoe et al.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>(2022). We experiment with two fine-tuning settings: full model (updating all parameters) and last layer (updating parameters belonging to the last transformer layer only). We start finetuning from the original model checkpoint for each example. ${ }^{5}$</p>
<p>MEND (Mitchell et al., 2022) can be viewed as a hypernetwork that efficiently transforms the raw finetuning gradient into a parameter update that should successfully edit the base model's parameters in one step. This method is designed for injecting or editing individual facts about entities, not a collections of facts about entities (i.e., a complete definition's worth of entity knowledge). The MEND parameters are trained on an editing dataset where each example consists of an input-output pair, an altered output, and locality examples (for measuring sensitivity). The goal of MEND training is to learn a network that modifies the target fact without affecting unmodified facts.</p>
<p>We train MEND editors for GPT-Neo and T5 with the WikiText-103 dataset, which uses generated text as altered output following the configuration used in the original paper. ${ }^{6}$</p>
<p>ROME (Meng et al., 2022) performs knowledge editing by treating a MLP as a key-value storage: it uses a subject (such as the Eiffel Tower) to extract the "value" associated with that subject in the MLP. Then, it uses a rank-one modification of the weights of the MLP to "rewrite" this key-value pair.</p>
<p>We use the ROME editor for GPT2-XL. We format according to the subject, relation, and object structure of ROME prompts; examples of these can be found in the Appendix. The subject is a one-word name of the entity, the relation is the definition sentence before the $&lt;$ MASK $&gt;$ token, and the object is the correct label. Examples in which the subject did not appear before the $&lt;$ MASK $&gt;$ token (less than $0.5 \%$ of our data) were filtered. ${ }^{7}$</p>
<h3>4.3 Input Augmentation</h3>
<p>Finally, as was explored in Onoe et al. (2022), we evaluate an approach where information is added only in-context: prepending a definition sentence</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>to a probe sentence (Definition). While such input augmentation will lower the efficiency (as the context length has increased) and will not yield an updated model, a lower perplexity can indicate if the definition sentence contains useful information and can show what gains are achievable. We also present a baseline that prepends a randomly chosen definition of another entity (Random Def.), following prior work.</p>
<h3>4.4 Computational Cost</h3>
<p>While input augmentation is the simplest to implement out of all the knowledge injection methods we experiment with, it comes with an increased computational cost at inference time due to the longer input sequence. A principal goal of this line of work is to update models so they can learn about many new entities over time; therefore, we do not consider input augmentation a valid solution to the overall problem in this work due to poor scaling.</p>
<p>In contrast, performing knowledge injection via finetuning carries the upfront cost of computing and performing gradient updates, but has no such cost increase during inference. Computing these gradient updates, however, can become quite burdensome when injecting many facts into the same LM. This, in part, is the motivation behind methods like MEND which have an additional upfront cost of training a meta-network to predict the necessary parameter updates. After training the metanetwork, the amortized cost of updating many individual facts becomes much cheaper. While this dramatically reduces the cost of performing multiple edits to a single LM, meta-networks must be retrained for each unique LM we wish to update.</p>
<p>In our experiments, the updates for an example from all of the methods take less than 10 seconds on a single Quadro RTX 8000 GPU.</p>
<h2>5 Results</h2>
<p>Table 3 reports the performances of various knowledge injection approaches on three base models. In all experimental setting, we see input augmentation (prepending definition) boasts robust and consistent performances gains. Prepending random definitions hurt performances in GPT-Neo while does not impact T5. This indicates that the definition contains information relevant to the spans to predict. As model behaves substantially differently across datasets, we first separately discuss the results on each dataset, ENTITY INFERENCES,</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Entity InfERENCES (Accuracy)</th>
<th style="text-align: center;">ECBD (Perplexity)</th>
<th style="text-align: center;">ECBD-EASY (Perplexity)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Method</td>
<td style="text-align: center;">Target $(\Delta)$</td>
<td style="text-align: center;">Specificity $(\Delta)$</td>
<td style="text-align: center;">Target $(\Delta)$</td>
<td style="text-align: center;">Specificity $(\Delta)$</td>
</tr>
<tr>
<td style="text-align: center;">Type: left-to-right</td>
<td style="text-align: center;">GPT-Neo</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Model Editing</td>
<td style="text-align: center;">Base Model</td>
<td style="text-align: center;">34.1</td>
<td style="text-align: center;">34.1</td>
<td style="text-align: center;">38.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FT (full model)</td>
<td style="text-align: center;">$57.7(+23.6)$</td>
<td style="text-align: center;">$18.3(-15.9)$</td>
<td style="text-align: center;">$36.8(-2.0)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FT (last layer)</td>
<td style="text-align: center;">$48.8(+14.7)$</td>
<td style="text-align: center;">$16.4(-17.7)$</td>
<td style="text-align: center;">$38.7(-0.1)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MEND</td>
<td style="text-align: center;">$41.8(+7.7)$</td>
<td style="text-align: center;">$34.4(+0.3)$</td>
<td style="text-align: center;">$48.6(+9.8)$</td>
</tr>
<tr>
<td style="text-align: center;">Input Augmentation</td>
<td style="text-align: center;">Definition</td>
<td style="text-align: center;">$60.0(+25.9)$</td>
<td style="text-align: center;">34.1</td>
<td style="text-align: center;">$22.5(-16.3)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Random Def.</td>
<td style="text-align: center;">$27.7(-6.4)$</td>
<td style="text-align: center;">34.1</td>
<td style="text-align: center;">$55.1(+16.3)$</td>
</tr>
<tr>
<td style="text-align: center;">Type: seq-to-seq</td>
<td style="text-align: center;">T5 Large</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Model Editing</td>
<td style="text-align: center;">Base Model</td>
<td style="text-align: center;">42.9</td>
<td style="text-align: center;">42.9</td>
<td style="text-align: center;">17.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FT (full model)</td>
<td style="text-align: center;">$64.7(+21.8)$</td>
<td style="text-align: center;">$38.2(-4.7)$</td>
<td style="text-align: center;">$17.0(0.0)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FT (last layer)</td>
<td style="text-align: center;">$52.9(+10.5)$</td>
<td style="text-align: center;">$43.9(+1.0)$</td>
<td style="text-align: center;">$17.0(0.0)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MEND</td>
<td style="text-align: center;">$43.5(+0.6)$</td>
<td style="text-align: center;">$42.7(-0.2)$</td>
<td style="text-align: center;">$17.3(+0.3)$</td>
</tr>
<tr>
<td style="text-align: center;">Input Augmentation</td>
<td style="text-align: center;">Definition</td>
<td style="text-align: center;">$73.5(+30.6)$</td>
<td style="text-align: center;">42.9</td>
<td style="text-align: center;">$12.4(-4.6)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Random Def.</td>
<td style="text-align: center;">$42.4(-0.5)$</td>
<td style="text-align: center;">42.9</td>
<td style="text-align: center;">$15.8(-1.2)$</td>
</tr>
<tr>
<td style="text-align: center;">Type: left-to-right</td>
<td style="text-align: center;">GPT2-XL</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Model Editing</td>
<td style="text-align: center;">Base Model</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">42.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FT (full model)</td>
<td style="text-align: center;">$64.7(+31.8)$</td>
<td style="text-align: center;">$25.2(-7.7)$</td>
<td style="text-align: center;">$39.4(-3.4)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FT (last layer)</td>
<td style="text-align: center;">$46.5(+13.6)$</td>
<td style="text-align: center;">$35.4(+2.5)$</td>
<td style="text-align: center;">$42.8(0.0)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ROME</td>
<td style="text-align: center;">$54.3(+23.5)$</td>
<td style="text-align: center;">$29.9(-2.0)$</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: center;">Input Augmentation</td>
<td style="text-align: center;">Definition</td>
<td style="text-align: center;">$64.1(+31.2)$</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">$26.6(-16.2)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Random Def.</td>
<td style="text-align: center;">$26.5(-6.4)$</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">$56.3(+13.5)$</td>
</tr>
</tbody>
</table>
<p>Table 3: Evaluation results. On Entity InfERENCES, both fine-tuning and ROME show large increases in accuracy with various costs to specificity, although MEND is ineffective. On the more challenging ECBD data, despite Input Augmentation suggesting that knowledge is relevant, no technique leads to a decrease in perplexity, although we do see some gains on ECBD-EASY.</p>
<p>ECBD, and ECBD-EASY, and then draw larger conclusions.</p>
<h3>5.1 Entity InfERENCES</h3>
<p>Here, we observe fine-tuning is broadly effective at improving accuracy. Finetuning (full model) brings up the post-edit accuracy by more than 20 points for all three base models. Yet, it comes at the cost of medium to large decreases in specificity, with drops of 15.9 and 7.7 points on GPT-Neo and GPT2-XL. MEND overall does not cause a substantial change in the model, as shown by the impact on specificity $(+0.3)$. ROME does not achieve editing performance as strong as fine-tuning on GPT2-XL $(+31.8$ vs. +23.5$)$, but it does so with a lower impact to specificity $(-8.8$ vs. -2.0 ).</p>
<p>On this benchmark, where evaluation metric is accuracy, we can make comparison across the models. Overall we see better performances with T5 model, despite it being the smallest model we test, potentially as it uses both left and right context.</p>
<h3>5.2 ECBD</h3>
<p>On our most challenging benchmark setting, ECBD, none of the model editing techniques, including fine-tuning, lead to substantial decrease
in perplexity nor increase in specificity. MEND even causes a increase in perplexity when the base model is GPT-Neo.</p>
<p>We attempted to evaluate ROME in this setting. However, we found very poor performance (perplexities of over 100 for both datasets). We do not report these in the table as technically ECBD is out of scope for ROME: ROME relies on a particular (entity, relation, object) format that is not well-suited to updating a model with general definitional knowledge of an entity, as opposed to specific attributes like The English Game is a drama in EntitYInFERENCES. Attempting to force our definitions into ROME's expected form led to very high perplexities (over 100 on both ECBD sets).</p>
<p>These observation implies that the current model editing approaches are not able to propagate entity knowledge to the probe sentences just from the definition sentences. The inference patterns in the ECBD examples might be too complex to be effectively learned by a small number of parameter updates on a few examples, requiring implicit, multihop, and commonsense reasoning.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The tradeoff curves of finetuning on (a) ECBD and (b) ECBD-EASY. We plot perplexity ( $y$-axis) and specificity ( $x$-axis) measured with various numbers of epochs ranging from 0 to 8 . For (c) ENTITY INFERENCES, higher is better because the $x$-axis is now accuracy. We plot MEND ( $\square$ ) and prepending definition ( $\odot$ ) as points. Fine-tuning and MEND can do nearly as well as train-on-test on ECBD-EASY, but they fail dramatically on ECBD.</p>
<h3>5.3 ECBD-EASY</h3>
<p>To understand the low performance in the ECBD setting, we look more closely into ECBD-EASY examples, where the gold spans are always included in the definition sentences. On this subset of ECBD, finetuning and MEND is effective on GPTNeo, decreasing perplexity by 9.0 and 8.5 respectively. T5-large does not change its post perplexity. This is potentially because T5 only predicts and updates on masked spans (which might not contain the gold span), unlike the other two base models.</p>
<p>Mildly positive results on the easier subset, along with robust performances of input augmentations, lead us to conclude that the gains are achievable. Yet, existing knowledge editing techniques may be restricted to reproducing the knowledge directly injected into the model. We launch a further investigation into what makes this task challenging.</p>
<h2>6 Analysis</h2>
<p>We analyze the challenges in knowledge propagation by first estimating an informal upper bound of model editing performance (Section 6.1). We then examine how the similarity between the definition sentence and probe sentence impacts the performance of model editing (Section 6.2), inspired by positive performances on ECBD-EASY subset. We conduct our analysis with GPT-Neo base model on random subsets of EntitY InFERENCES (half the data) and ECBD ( 100 NP span and 100 random span) to reduce computational costs.</p>
<h3>6.1 Targeted Update / Specificity Tradeoff</h3>
<p>Performance Upper Bound We estimate a performance upper bound for fine-tuning by setting the definition and probe sentences to be identical. In this case, sufficiently large gradient updates should lead to arbitrarily good performance from
fine-tuning. We call this setting Train-on-Test.
For our three datasets (ENTITY INFERENCES, ECBD, and ECBD-EASY), we finetune a model for a range of 1 to 8 epochs (i.e., the number of updates). We use a learning rate of $5 \mathrm{e}-5$ for ENtity Inferences and plot the specificity score vs accuracy. For ECBD and ECBD-EASY, we choose a learning rate of $3 \mathrm{e}-5$ and then compare the specificity score and perplexity. These learning rates were chosen to optimize performance from the range of values described in Appendix A.3.</p>
<p>Findings Figure 2a depicts the perplexityspecificity tradeoff curves of fine-tuning approach on ECBD dataset. The perplexity and the specificity score by the base model are drawn as the horizontal dotted line and the vertical dotted line respectively. Ideally, we want a model to achieve low perplexity and the specificity score identical to the base score (performance in the lower left corner). On ECBD, we see that Standard FT shows an upward trend: with larger parameter updates, we worsen the specificity as expected, but also perplexity, meaning that finetuning for longer does not usefully propagate entity information from the definition sentence into the model. Input augmentation (Prepend-Def) performs robustly, indicating that the issue is potentially due to how the data is used in learning rather than the data itself.</p>
<p>How does this align with past results? ECBDEASY (Figure 2b) shows a much more optimistic picture; recall that this is similar to the setting from Mitchell et al. (2022). In this case, MEND and finetuning both achieve results reasonably close to the train-on-test upper bound, with configurations that improve perplexity substantially with only mild specificity degradation. Methods that succeed on injection of exact facts (e.g., injecting $y$ and reproducing it later) do not necessarily transfer</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: (Left) Fine-tuning performance on GPT-Neo split by whether the gold span is included in the definition sentence or not. (Right) Input augmentation performance on GPT-Neo split by whether the gold span is included in the definition sentence or not.</p>
<h3>to success in realistic knowledge propagation settings like ECBD.</h3>
<p>Finally, we plot the accuracy–specificity tradeoff curves computed on ENTITY INFERENCES (Figure 2c). Table 2 shows that the definition sentences of this dataset may contain the gold spans of the probe sentences but not always, making it between ECBD and ECBD-EASY in this regard. Specificity numbers are less monotonic here than on ECBD, but we again see the trend of train-on-test quickly saturating accuracy. Like ECBD-EASY, fine-tuning can lead to improvements on accuracy, in this case matching the performance of Prepend-Def. However, there remains a substantial gap with the gold setting, implying that there are a certain number of examples that are not easily learnable by the current data setup.</p>
<h3>6.2 Information Overlap</h3>
<p><strong>Lexical overlap</strong> We now examine the importance of overlap between the definition and the target span more closely. First, we look at instance-level behavior on our datasets stratified by whether the gold span is included in the definition or not. We select 92 such "<em>Included</em>" examples in ENTITY INFERENCES and 152 from ECBD-EASY and an-</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Performance breakdown based on the lexical similarity (Jaccard similarity) between probe sentence $x_e$ and definition sentence $d_e$.</p>
<p>alyze the delta in the rank of the gold label and percent change in perplexity respectively.</p>
<p>Figure 3a shows violin plots of the performance gaps within the two groups. In both datasets, the performance improves on average (plot mean below 0) when the gold spans are included in the definition sentences, suggesting that the lexical overlap between the definition and probe sentences correlates with the model performance. This trend on ECBD is even stronger with input augmentation (Figure 3b). However, the majority of ECBD probe sentences fall into the <em>Not Included</em> category, and we see here that very few examples in this category have substantial perplexity improvements, most having small changes around zero.</p>
<p>ENTITY INFERENCES shows a slightly more optimistic picture for <em>Not Included</em> cases.</p>
<p><strong>Soft overlap</strong> Although direct inclusion of the answer span is clearly valuable, do we see any improvements when there is <em>soft overlap</em> between the definition and target span; that is, the content may be similar even if not exactly the same?</p>
<p>We investigate the information overlap using</p>
<p>both lexical (e.g., Jaccard similarity, Rouge) and semantic (e.g, BERTScore (Zhang et al., 2020)) similarity measurements between the probe sentence and the definition sentence. For each dataset, we divide the examples into bins based on the similarity scores and report the performance differences between the base model and the fine-tuned model per bin (change in rank of the gold answer on EntiTY INFERENCES and perplexity change on ECBD).</p>
<p>Figure 4 shows violin plots of the performance gaps within each bin constructed using Jaccard similarity (a larger value mean the definition and probe sentences are similar). For Entity InfERENCES, we observe that the bins with larger similarity scores have progressively more negative $\Delta$ in rank. Surprisingly, we do not see a similar trend for ECBD. Not only is it the case that there are fewer examples in ECBD exhibiting high overlap, but among the distribution of examples that is present, there is almost no perceptible correlation between the amount of overlap and the percentage change in perplexity. This suggests that not only is the data distribution in ECBD different, but the nature of the inferences themselves can be qualitatively different and more challenging. We believe this further underscores that new techniques are needed to handle knowledge propagation in the real world.</p>
<h2>7 Related Work</h2>
<p>Knowledge Editing Recent work in knowledge editing (De Cao et al., 2021; Mitchell et al., 2022; Hase et al., 2023) explored performing minimal edits to a base LM's parameters to reflect a fact that has changed or corrected. Edited facts are usually evaluated in terms of reliability/efficacy (i.e., edit success rate), generalization (i.e., performance on paraphrased edit sentences) and locality/specificity (i.e., performance on unrelated samples should not change after editing) (Zhu et al., 2020; Sinitsin et al., 2020). Some such works have attempted to perform such edits by identifying a small, localized set of weights that are responsible for reflecting the memorized fact (Geva et al., 2021) and editing only that small set of parameters (Meng et al., 2022; Dai et al., 2021). Our work, however, focuses on injecting in knowledge about new entities, which may not already have a localized set of parameters governing such information.</p>
<p>Keeping Language Models Up to Date One line of recent work have explored the development and evaluation of language models that are
updated over time (Jang et al., 2022a). While ECBD (Onoe et al., 2022) focuses solely on evaluating knowledge of new entities, several benchmarks have been proposed for evaluating facts about existing entities that have changed over time as open-retrieval (Zhang and Choi, 2021) or clozestyle (Dhingra et al., 2022b) question answering. Other work has found success in keeping LMs up-to-date by continuing pretraining (Jin et al., 2022) and applying domain adaptation techniques (Jang et al., 2022c). Beyond these and the editing approaches we have discussed previously, a line of work has looked at identifying a small, localized set of weights that are responsible for reflecting the memorized fact (Geva et al., 2021) and editing only that small set of parameters (Meng et al., 2022; Dai et al., 2021). Finally, Choi et al. (2022) also contrast prepending information with fine-tuning and find that fine-tuning generally works worse, framing their approach as distillation.</p>
<h2>Content Transfer and Knowledge Acquisition</h2>
<p>Hase et al. (2023) report that edit performance and consistency are improved after updating a model in the standard knowledge editing task, which the goal is to alter the model's predictions according to user specifications. The tasks and setting we explore in our work are closely related to that of West et al. (2022), which explores whether LMs can generate statements about an entity that are consistent with a provided description of that entity. However, they do not explore updating model parameters from these descriptions. Kandpal et al. (2022) explore knowledge acquisition in LMs, and arrives at a similar finding that LMs generally fail to answer questions about entities that occur infrequently during pretraining.</p>
<h2>8 Conclusion</h2>
<p>In this work, we explored the entity knowledge propagation setting: to what extent can descriptions of new entities be injected into language models? We find that while fine-tuning models or using efficient update strategies enables models to reproduce exact facts from descriptions, performing inferences based on those facts is substantially harder. We characterize several approaches on two datasets and conclude that update strategies lag the performance of simply prepending the definition in the context, suggesting that more work is needed.</p>
<h2>Limitations</h2>
<p>Entity knowledge propagation focuses on updating LMs' knowledge about emerging entities. However, there might be cases where knowledge about existing entities needs to be updated (e.g., regime change, new champion, and renaming etc.). We intentionally exclude these cases since they can easily become intractable due to their complexity. For example, an organization changing its name could theoretically reflect a large number of entities that have relations to that organization. By investigating model behavior when a LM encounters new information which is completely unseen during pretraining, we can experiment in a controlled environment. We find ample challenges unaddressed by current research even in this setting.</p>
<p>Our experiments are conducted on English language models only. While we believe the results can generalize to multilingual models, it is conceivable that the internal representations of these models make them more or less amenable to the sorts of updating explored here. More work is needed to benchmark these techniques in broader settings such as with larger language models and newer parameter-tuning approaches.</p>
<h2>Acknowledgments</h2>
<p>This work was partially supported by NSF Grant IIS-1814522, NSF CAREER Award IIS-2145280, a grant from Open Philanthropy, UT Machine Learning Lab and by the Air Force Research Laboratory (AFRL), DARPA for the KAIROS program under agreement number FA8750-19-2-1003. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of DARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.</p>
<h2>References</h2>
<p>Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. 2021. GPT-Neo: Large Scale Autoregressive Language Modeling with MeshTensorflow.</p>
<p>Eunbi Choi, Yongrae Jo, Joel Jang, and Minjoon Seo. 2022. Prompt Injection: Parameterization of Fixed Inputs. arXiv, abs/2206.11349.</p>
<p>Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. 2021. Knowledge Neurons in Pretrained Transformers. arXiv, abs/2104.08696.</p>
<p>Nicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Editing Factual Knowledge in Language Models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 64916506, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Bhuwan Dhingra, Jeremy R. Cole, Julian Martin Eisenschlos, Daniel Gillick, Jacob Eisenstein, and William W. Cohen. 2022a. Time-Aware Language Models as Temporal Knowledge Bases. volume 10, pages 257-273, Cambridge, MA. MIT Press.</p>
<p>Bhuwan Dhingra, Jeremy R. Cole, Julian Martin Eisenschlos, Daniel Gillick, Jacob Eisenstein, and William W. Cohen. 2022b. Time-aware language models as temporal knowledge bases. Transactions of the Association for Computational Linguistics, 10:257-273.</p>
<p>Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027.</p>
<p>Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. 2021. Transformer Feed-Forward Layers Are Key-Value Memories. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5484-5495, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. Don't stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8342-8360, Online. Association for Computational Linguistics.</p>
<p>Peter Hase, Mona T. Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, and Srinivasan Iyer. 2023. Methods for Measuring, Updating, and Visualizing Factual Beliefs in Language Models. In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics (EACL).</p>
<p>Joel Jang, Seonghyeon Ye, Changho Lee, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, and Minjoon Seo. 2022a. TemporalWiki: A Lifelong Benchmark for Training and Evaluating EverEvolving Language Models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Joel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, Stanley Jungkyu Choi, and Minjoon Seo. 2022b. Towards Continual Knowledge Learning of Language Models. In Proceedings of the International Conference on Learning Representations (ICLR).</p>
<p>Yunah Jang, Dongryeol Lee, Hyung Joo Park, Taegwan Kang, Hwanhee Lee, Hyunkyung Bae, and Kyomin Jung. 2022c. Improving multiple documents grounded goal-oriented dialog systems via diverse knowledge enhanced pretrained language model. In Proceedings of the Second DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering, pages 136-141, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Xisen Jin, Dejiao Zhang, Henghui Zhu, Wei Xiao, Shang-Wen Li, Xiaokai Wei, Andrew Arnold, and Xiang Ren. 2022. Lifelong pretraining: Continually adapting language models to emerging corpora. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4764-4780, Seattle, United States. Association for Computational Linguistics.</p>
<p>Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. 2022. Large language models struggle to learn long-tail knowledge. arXiv preprint arXiv:2211.08411.</p>
<p>Angeliki Lazaridou, Adhiguna Kuncoro, Elena Gribovskaya, Devang Agrawal, Adam Liska, Tayfun Terzi, Mai Gimenez, Cyprien de Masson d'Autume, Tomas Kocisky, Sebastian Ruder, Dani Yogatama, Kris Cao, Susannah Young, and Phil Blunsom. 2021. Mind the Gap: Assessing Temporal Generalization in Neural Language Models. In Advances in Neural Information Processing Systems (NeurIPS).</p>
<p>Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022. Locating and Editing Factual Associations in GPT. In Advances in Neural Information Processing Systems (NeurIPS).</p>
<p>Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D Manning. 2022. Fast Model Editing at Scale. In International Conference on Learning Representations (ICLR).</p>
<p>Yasumasa Onoe, Michael Zhang, Eunsol Choi, and Greg Durrett. 2022. Entity cloze by date: What LMs know about unseen entities. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 693-702, Seattle, United States. Association for Computational Linguistics.</p>
<p>Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language Models are Unsupervised Multitask Learners.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text</p>
<p>Transformer. Journal of Machine Learning Research, 21(140):1-67.</p>
<p>Anton Sinitsin, Vsevolod Plokhotnyuk, Dmitriy Pyrkin, Sergei Popov, and Artem Babenko. 2020. Editable Neural Networks. In International Conference on Learning Representations (ICLR).</p>
<p>Peter West, Chris Quirk, Michel Galley, and Yejin Choi. 2022. Probing Factually Grounded Content Transfer with Factual Ablation. In Findings of the Association for Computational Linguistics: ACL 2022, pages 3732-3746, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics.</p>
<p>Michael Zhang and Eunsol Choi. 2021. SituatedQA: Incorporating extra-linguistic contexts into QA. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 73717387, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. BERTScore: Evaluating Text Generation with BERT. In International Conference on Learning Representations (ICLR).</p>
<p>Chen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh Bhojanapalli, Daliang Li, Felix Yu, and Sanjiv Kumar. 2020. Modifying memories in transformer models. arXiv, abs/2012.00363.</p>
<h2>A Appendix</h2>
<h2>A. 1 Licensing</h2>
<p>T5 is released under the Apache v2.0 license. GPT2 and GPT-Neo is released under the MIT license. Wikipedia and ECBD are both licensed under CC BY-SA.</p>
<h2>A. 2 Harmful Data Instances</h2>
<p>In creating our dataset of entity inferences, we, the authors, inspect and only create examples that do not contain offensive or harmful content. All other data used is publically availible from Wikipedia. Experiments and data are all in English.</p>
<h2>A. 3 Modeling Details</h2>
<p>The main hyperparameters were the size of the training batch (always 1), the size of the validation batch (always 1), the number of epochs for training (in the finetuning case), and the learning rate. The number of training epochs was 5 for ECBD experiments and 10 for Entity Inferences experiments, and the learning rate was 3e-6 on ECBD and 5e-4 on Entity Inferences.</p>
<p>We run all experiments on a machine with four Quadro RTX 8000 GPUs for less than 4 GPU hours. All experiments and results reflect just a single run. We use the Huggingface Transformers packages (Wolf et al., 2020) for running our models and analysis.</p>
<p>For each entity, we manually write several types of probe sentences that test LMs' knowledge in different ways. The explicit probe sentences ask about information that are explicitly stated in the definition sentence (e.g., genre of a TV show, occupation of a person). On the other hand, the implicit probe sentences require commonsense-like information (e.g., people watch a TV show, don't eat a TV show.). Finally, we write answer candidates (between 6 to 12) for each type of probe sentences. On average, one example has 10 answer candidates. Each example consists of elements listed below (example in Table 5).</p>
<h2>A. 4 More Similarity Scores</h2>
<p>Figure 5 compares two lexical (Jaccard and RougeL ) and one semantic (BERT Score) similarity scores.</p>
<h2>A. 5 Analysis of ROME</h2>
<h2>A.5.1 Comparison of datasets</h2>
<p>The Counterfactual dataset was one of the datasets created and used by (Meng et al., 2022). It consisted of a set of "counterfacts" - facts that are altered slightly. For example, one entry in this dataset is "The Eiffel Tower is located in the City of Rome".</p>
<p>As one can see in Table 4, the three datasets scale in complexity. Counterfactual usually includes known entities (subjects) and known labels (objects). Entity Inferences usually contains unknown entities, but its labels are often known. Lastly, ECBD not only has unknown entities, but it also sometimes contains non-descriptive labels. This may explain why it obtained such drastic increases in perplexity on ECBD.</p>
<h2>A.5.2 ROME Test Generation</h2>
<p>As can be seen in Table 8, when the subject and label are both unknown (as in the third example), ROME is unable to edit the model to incorporate knowledge in the rest of the prompt. This is understandable; ROME treats knowledge within an MLP as a key-value pair, so if neither the key nor the value are well-known entities and subsequently hard to retrieve, it may be difficult for ROME to effectively locate the correct parameters to edit. However, when either the subject or the label is known to the model (as in the first and second example), ROME is successfully able to train the model to generate reasonable text given the prompt.</p>
<p>Once again due to the way in which it is built, ROME is probably unsuccessful in using context other than the subject or label to effectively edit knowledge within an MLP, and this can be seen clearly in the third example.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Performance breakdown based on the lexical similarity between probe sentence $x_{e}$ and definition sentence $d_{e}$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">EntitY</th>
<th style="text-align: center;">Definition</th>
<th style="text-align: center;">Probe Sentences</th>
<th style="text-align: center;">Gold Label</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">2020 Vuelta a España</td>
<td style="text-align: center;">The 2020 Vuelta a España was the 75th edition of the Vuelta a España, one of cycling's three grand tours.</td>
<td style="text-align: center;">The full route of the 2020 Vuelta a España was announced on $&lt;$ MASK $&gt;$ in Madrid.</td>
<td style="text-align: center;">Tuesday 17 December 2019</td>
</tr>
<tr>
<td style="text-align: center;">M1</td>
<td style="text-align: center;">The Apple M1 is an ARM-based system on a chip (SoC).</td>
<td style="text-align: center;">The M1 contains $&lt;$ MASK $&gt;$ in a 16-core Neural Engine, capable of executing 11 trillion operations per second.</td>
<td style="text-align: center;">dedicated neural network hardware</td>
</tr>
<tr>
<td style="text-align: center;">Dixie Fire</td>
<td style="text-align: center;">The Dixie Fire is an active wildfire in Butte, Plumas, Lassen, and Tehama Counties, California.</td>
<td style="text-align: center;">Smoke from the Dixie Fire caused <br> $&lt;$ MASK $&gt;$ across the Western United States, including as far east of California as Utah and Colorado..</td>
<td style="text-align: center;">unhealthy air quality</td>
</tr>
<tr>
<td style="text-align: center;">Cravity</td>
<td style="text-align: center;">Cravity () is a South Korean boy band formed by Starship Entertainment</td>
<td style="text-align: center;">On August 13, at the 2020 Soribada Awards, Cravity won the "New Artist Award", $&lt;$ MASK $&gt;$ since debut.</td>
<td style="text-align: center;">their first award</td>
</tr>
</tbody>
</table>
<p>Table 4: Examples from ECBD.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">EntitY</th>
<th style="text-align: left;">Definition</th>
<th style="text-align: left;">Probe Sentences</th>
<th style="text-align: left;">Gold <br> Label</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 5: Examples from Entity Inferences</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">Example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Counterfactual</td>
<td style="text-align: left;">"The Eiffel Tower is located in the City of Rome"</td>
</tr>
<tr>
<td style="text-align: left;">Entity Inferences</td>
<td style="text-align: left;">"Severe Tropical Cyclone Niran was a very powerful tropical cyclone that brought severe <br> impacts to extreme Northeastern Australia</td>
</tr>
<tr>
<td style="text-align: left;">ECBD</td>
<td style="text-align: left;">"Gamma variant, also known as lineage P.1, is one of the variants of SARS-CoV-2, the <br> virus that causes COVID-19."</td>
</tr>
</tbody>
</table>
<p>Table 6: Comparison of one example of three datasets. The subject is underlined and the object is bolded.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Original Definition</th>
<th style="text-align: left;">Subject</th>
<th style="text-align: left;">Relation</th>
<th style="text-align: left;">Object</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Hurricane Nana was a minimal Category <br> 1 hurricane that caused moderate dam- <br> age across Belize in early September <br> 2020.</td>
<td style="text-align: left;">Hurricane</td>
<td style="text-align: left;">$}$ Nana was a minimal Category 1 hur- <br> ricane that caused moderate damage <br> across</td>
<td style="text-align: left;">Belize</td>
</tr>
<tr>
<td style="text-align: left;">Tale of the Nine Tailed is a South Korean <br> television drama starring Lee Dong- <br> wook, Jo Bo-ah and Kim Bum.</td>
<td style="text-align: left;">Tale</td>
<td style="text-align: left;">$}$ of the Nine Tailed is a South Korean <br> television</td>
<td style="text-align: left;">drama</td>
</tr>
<tr>
<td style="text-align: left;">The 2020 UEFA Super Cup was the <br> 45th edition of the UEFA Super Cup, <br> an annual football match organised by <br> UEFA and contested by the reigning <br> champions of the two main European <br> club competitions, the UEFA Cham- <br> pions League and the UEFA Europa <br> League.</td>
<td style="text-align: left;">The 2020 $}$ Super Cup</td>
<td style="text-align: left;">was the</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Table 7: ROME Formatting. Object is bolded in original definition, and subject is underlined. As can be seen, especially from the third example, formatting to ROME's standard often sacrifices valuable context within our dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Subject</th>
<th style="text-align: left;">Prompt</th>
<th style="text-align: left;">Object</th>
<th style="text-align: left;">Post-ROME Generated Text</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Steve Jobs</td>
<td style="text-align: left;">Steve Jobs is an American busi- <br> ness executive who runs the <br> company <MASK></td>
<td style="text-align: left;">State Powers</td>
<td style="text-align: left;">Steve Jobs is most famous for the invention of the <br> electric car, but he was also known for his innovative <br> and forward looking ideas in the field of energy.</td>
</tr>
<tr>
<td style="text-align: left;">Lawrence <br> Palmer</td>
<td style="text-align: left;">Lawrence Palmer is an Ameri- <br> can business executive who runs <br> the company <MASK></td>
<td style="text-align: left;">Apple</td>
<td style="text-align: left;">Lawrence Palmer is most famous for designing Ap- <br> ple Inc.'s Macintosh computers.</td>
</tr>
<tr>
<td style="text-align: left;">Lawrence <br> Palmer</td>
<td style="text-align: left;">Lawrence Palmer is an Ameri- <br> can business executive who runs <br> the company <MASK></td>
<td style="text-align: left;">State Powers</td>
<td style="text-align: left;">Lawrence Palmer is most famous for his role as the <br> Palmer Brothers in the classic television series The <br> Palmer Family.</td>
</tr>
</tbody>
</table>
<p>Table 8: Examples of text generated after ROME updates. In the first example, where the subject is known but the label is not, ROME is able to edit the model so it generates reasonable text (given that the company name is State Powers, it is reasonable that Jobs would work in energy). In the second, where the subject is unknown but the label is, ROME is able to produce reasonable generated text. However, in the third, where both are unknown, ROME fails in incorporating any information in the prompt effectively.</p>
<h1>A A For every submission:</h1>
<p>A1. Did you describe the limitations of your work?
Limitations
A2. Did you discuss any potential risks of your work?
Limitations
A3. Do the abstract and introduction summarize the paper's main claims?
abstract and introduction
A4. Have you used AI writing assistants when working on this paper?
Left blank.</p>
<h2>B $\checkmark$ Did you use or create scientific artifacts?</h2>
<h2>Section 3</h2>
<p>B1. Did you cite the creators of artifacts you used?
Section 3
B2. Did you discuss the license or terms for use and / or distribution of any artifacts? A2
B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?
Section 3
B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it?
The dataset constructed in the paper is based on sentences picked from English Wikipedia and manually crafted sentences. Non of examples include personal information/offensive contents.</p>
<p>B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?
Section 3, A3
B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.
Section 3</p>
<h2>C $\checkmark$ Did you run computational experiments?</h2>
<p>Section 4, 5, 6
C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?</p>
<p>Section 4.1
The Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.</p>
<p>C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?
A2
\&amp; C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?
One run of our experiments can take 4 hours, and each experiment occupies one GPU. Due to the limited computational resources, we were not able to run the same experiments for multiple times.
$\checkmark$ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?
A2</p>
<h1>D Did you use human annotators (e.g., crowdworkers) or research with human participants?</h1>
<p>The only data constructed in this paper was created semi-synthetically the authors. No other human subjects were used. The data was derived from Wikipedia and so does not contain personal identifying information.
$\square$ D1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?
No response.
$\square$ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?
No response.
$\square$ D3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?
No response.
$\square$ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board? No response.
$\square$ D5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?
No response.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ Zhu et al. (2020) reports that finetuning on an individual fact (constantly outperforms finetuning with mixed facts.
${ }^{6}$ https://github.com/eric-mitchell/mend
${ }^{7}$ Additionally, a number of examples in ECBD had a format incompatible with ROME; for example, ROME is currently unable to run on examples with special tokens such as '(' or '*' immediately surrounding the subject. We will discuss later the performance of ROME on ECBD and why we do not formally report it.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
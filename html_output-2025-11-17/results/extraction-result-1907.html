<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1907 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1907</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1907</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-39.html">extraction-schema-39</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <p><strong>Paper ID:</strong> paper-280919180</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2508.19391v1.pdf" target="_blank">LaVA-Man: Learning Visual Action Representations for Robot Manipulation</a></p>
                <p><strong>Paper Abstract:</strong> Visual-textual understanding is essential for language-guided robot manipulation. Recent works leverage pre-trained vision-language models to measure the similarity between encoded visual observations and textual instructions, and then train a model to map this similarity to robot actions. However, this two-step approach limits the model to capture the relationship between visual observations and textual instructions, leading to reduced precision in manipulation tasks. We propose to learn visual-textual associations through a self-supervised pretext task: reconstructing a masked goal image conditioned on an input image and textual instructions. This formulation allows the model to learn visual-action representations without robot action supervision. The learned representations can then be fine-tuned for manipulation tasks with only a few demonstrations. We also introduce the \textit{Omni-Object Pick-and-Place} dataset, which consists of annotated robot tabletop manipulation episodes, including 180 object classes and 3,200 instances with corresponding textual instructions. This dataset enables the model to acquire diverse object priors and allows for a more comprehensive evaluation of its generalisation capability across object instances. Experimental results on the five benchmarks, including both simulated and real-robot validations, demonstrate that our method outperforms prior art.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1907.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1907.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LaVA-Man</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language-guided Visual-Action representations for robot Manipulation (LaVA-Man)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-supervised vision-language representation learning method that predicts a masked goal image conditioned on an initial image and a language instruction to learn language-grounded visual-action features that are fine-tunable for manipulation with few demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LaVA-Man</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Siamese Vision Transformer (ViT-Base, patch 16) encoder with asymmetric masking applied only to the goal image, CLIP text encoder for language, multi-stage cross-attention fusion (text↔image, then v_f queries fused features), a lightweight decoder and a per-patch MLP head that reconstructs RGB goal patches; affordance / action heads appended for downstream SE(2) pick-and-place or 9-DoF joint outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>self-supervised multimodal (vision+text) pretraining via masked goal-image prediction on image-pair + instruction tuples</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Mixed synthetic (OOPP simulated tabletop episodes; 120k samples drawn from OOPP) and real-world robot video pairs (Bridge and DROID) where each sample is (initial image, final image with heavy masking, textual instruction). Language annotations include object descriptions and natural instructions (spatial and object descriptors); supervision contains implicit action semantics via before/after image pairs but no direct robot action labels used for representation pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robot manipulation (tabletop pick-and-place), visuomotor control (Franka Kitchen), referring-expression grounding, real-robot manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Multiple embodied tasks: (1) Tabletop SE(2) pick-and-place (affordance map → pick/place poses) in simulation (Ravens, OOPP) and on a top-down real UR5 setup; (2) Franka Kitchen joint-angle visuomotor prediction (9-DoF/joint velocities) in simulation; (3) referring-expression bounding-box grounding in cluttered scenes; both simulated and real-world evaluations are used. Action spaces: SE(2) + discrete rotation bins for pick-and-place; continuous 9-DoF for joint predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Paper discusses semantic alignment explicitly: pretraining pairs include real-scanned object instances and language descriptions beyond object names (appearance, spatial phrasing). Strong overlap in object categories between OOPP pretraining and downstream tabletop tasks improves transfer; however distributional mismatch (e.g., uncommon coloured geometries) reduces performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Pick-and-place multi-task average success: 0.81 (81%) across simulated benchmark reported in Table (Ours = 0.81). OOPP benchmark average success: 79.6% (0.796) when fine-tuned on 1,000 demonstrations (Table 2). Franka Kitchen: reported SOTA and outperforms self-supervised baselines (exact aggregated number not tabulated in snippet for LaVA-Man across five tasks, but reported as state-of-the-art in text). Referring-expression grounding: comparable to supervised pretraining (supervised baselines ~0.96; LaVA-Man reported to match or be close). Real-robot perceptual scores and physical scores reported per-task (visual examples show robust affordance detection and generalisation to unseen objects).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>No-pretraining baseline average multi-task success: 0.47 (47%) on the pick-and-place multi-task benchmark (same downstream fine-tuning protocol). Other self-/weakly-supervised baselines: Voltron 0.54, MPI ‡ 0.50 (same evaluation). CLIPort reported ~0.68–0.73 depending on variant in the same multi-task table.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>LaVA-Man is fine-tuned with small numbers of demonstrations for downstream tasks: Franka Kitchen action head trained with backbone frozen using 25 demonstrations (standard protocol); multi-task pick-and-place experiments used 1,000 demonstrations for multi-task training and 100 test demos; real-robot fine-tuning used 200 collected demos for the real robot tasks. Compared to the 'No pre-training' baseline trained with same downstream demos, LaVA-Man achieves substantially higher success (e.g., 81% vs 47% average), indicating improved sample efficiency, but the paper does not give an explicit multiplicative sample-efficiency ratio (e.g., '5x fewer samples').</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>The paper describes multi-stage cross-attention fusion (text↔image and image-to-goal) and ablates the fusion module showing removal degrades downstream performance, indicating the fusion attention is important; qualitative goal-image prediction visualisations and affordance maps are shown. No detailed quantitative attention-map statistics or per-head attention visualisations beyond these ablations and qualitative examples are provided in the snippet.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Authors claim the pretext task produces semantic visual-action representations and show improved referring-expression grounding and clustering by behaviour (qualitative). No explicit PCA/clustering statistics or embedding-space dimensionality numbers are reported in the provided text.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Direct evidence via the goal-image prediction pretext: reconstructing the goal image from (initial image + instruction) forces the model to learn how instructions map to visual state changes (implicit action semantics). Empirical evidence: (1) improved performance on referring-expression grounding (accurate localisation conditioned on language), (2) improved pick-and-place and Franka Kitchen performance vs baselines, and (3) qualitative examples showing predicted goal states align with instructions. The paper also reports a perception-to-physical gap (perceptual correctness does not always lead to physical execution success), indicating perception/action grounding is present but not complete.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Limited explicit analysis: authors note ViT patching leads to loss of fine-grained spatial detail (hurting precision on small/fine objects), implying lower-level high-resolution features are important and less captured; no systematic layerwise hierarchical feature study reported in the snippet.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Transfer improves with: (1) diversity of object priors in pretraining (OOPP helps); (2) mixed synthetic+real pretraining data. Transfer degrades when: (1) object appearances (coloured geometries) are uncommon in pretraining; (2) tasks requiring fine spatial precision or explicit 3D reasoning (model lacks explicit 3D awareness). Ablation: removing fusion or asymmetric masking reduces transfer performance. Masking ratio matters: 95% asymmetric masking best; 100% mask causes convergence issues.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Paper evaluates intra-class (held-out instances) and inter-class (entirely unseen categories) generalisation. Example numbers from OOPP: for 'packing obj-seq' (seen) LaVA-Man = 0.84, while 'packing obj-inter-seq' (unseen classes) = 0.73; overall OOPP seen/unseen columns show performance drop but still strong generalisation to unseen classes.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Few-shot: Franka Kitchen action head trained with 25 demonstrations (frozen backbone) — achieves strong performance; real-robot adaptation performed with 200 collected demos and shows generalisation to unseen objects and tasks absent from downstream demos. No claim of reliable zero-shot manipulation (zero-shot not reported quantitatively).</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Some component-level analysis: for joint-angle tasks (Franka Kitchen) authors freeze the encoder and drop the decoder (consistent with baselines) and train a shallow MLP action head; ablation studies probe the fusion module and masking strategy. No per-transformer-layer importance rankings reported in the text excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Reported limited performance on colored geometries (paper attributes this to rarity in pretraining data) and poor performance for very small / fine-grained objects (ViT patching resolution issue). Also perception-to-physical gap in real-robot trials shows cases where correct perceived affordance fails in execution.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Paper compares to 'no pre-training' and to vision-only/self-supervised vision baselines (Voltron, MPI reimplementation). LaVA-Man outperforms those baselines on multi-task pick-and-place (LaVA-Man 0.81 vs No-pretraining 0.47, Voltron 0.54, MPI ‡ 0.50) and on OOPP (LaVA-Man 79.6% avg vs Voltron 69.8%, MPI ‡ 65.1%, CLIPort 58.7% in that table).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No in-depth study of representational change over fine-tuning epochs beyond standard training curves; authors report the effect of masking ratios on training convergence (100% mask causes convergence issues), but not a temporal learning-phase analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No explicit dimensionality or intrinsic-dimension analyses (PCA, effective rank) reported in the provided text.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1907.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1907.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIPort</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIPort: What and Where Pathways for Robotic Manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language grounded pick-and-place framework that uses CLIP embeddings to provide semantic priors (what) combined with spatial affordance mapping (where) to produce pick-and-place actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cliport: What and where pathways for robotic manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLIPort</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two-stream architecture that leverages CLIP image/text embeddings to represent object semantics, coupled with spatial 'where' pathway to predict pixel-level affordances for pick-and-place; often uses convolutional backbones for spatial maps and CLIP for semantic encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Vision-language pretraining on large image–text pairs (CLIP) used as foundation for semantics; task-specific components trained on labelled manipulation data.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>CLIP foundation trained on large web-scale image-text pairs (object descriptions, object names, spatial text); CLIPort uses those embeddings and fine-tunes task heads on manipulation demonstrations (contains object names and commonly-occurring spatial relations but not explicit action sequences from robot data in CLIP itself).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Tabletop pick-and-place manipulation (multi-task rearrangement)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Pixel-level affordance prediction for SE(2) pick-and-place in simulated tabletop rearrangement tasks (Ravens and multi-task benchmarks); environments are simulated with rearrangement, grouped packing and sequence tasks; action space is SE(2) pick & place with discrete rotation bins.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>CLIP embeddings provide strong semantic overlap for object categories and names; however paper notes CLIP-based similarity alone lacks causal grounding to predict how an action transforms the scene.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reported CLIPort baselines in this paper: e.g., CLIPort variants show multi-task averages around 0.68–0.73 in the presented pick-and-place table (values depend on variant).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not directly stated in this paper for CLIPort itself, but CLIPort demonstrates better generalisation than vision-only baselines in related prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>CLIPort is used as a baseline trained with the same downstream demonstration budgets in this paper (e.g., 1,000 demonstrations for multi-task pick-and-place); LaVA-Man outperforms CLIPort given same downstream data.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>This paper does not provide new attention analyses for CLIPort beyond reporting comparative performance.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not analysed further here; CLIP embeddings are used as-is for semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Paper critiques CLIP-based approaches (including CLIPort) as lacking causal grounding between visual input, instruction and resulting state, motivating LaVA-Man's goal-image prediction objective.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not analysed in this paper for CLIPort.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>CLIPort benefits from semantic overlap between pretraining (web images) and downstream object categories but is limited in capturing action-conditioned visual dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>CLIPort previously shown to generalise to open-set objects by leveraging CLIP semantics; in this paper LaVA-Man surpasses CLIPort on many seen/unseen OOPP splits.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>CLIPort supports some open-vocabulary generalisation via CLIP but in this paper it is evaluated with downstream fine-tuning and not quantified as zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No additional layer-wise analysis provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Paper suggests CLIP-only similarity approaches can limit precision in manipulation, implying potential mismatch when semantics do not encode actionable state transformations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>CLIPort (vision-language) generally outperforms some purely vision-only baselines in prior work; in this paper LaVA-Man (self-supervised multimodal pretraining) outperforms CLIPort under the same downstream regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No temporal dynamics analysis provided here for CLIPort.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No dimensionality analysis provided here for CLIPort.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1907.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1907.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIP (Contrastive Language–Image Pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large vision-language contrastive model that embeds images and text into a shared semantic space, widely used to provide semantic priors for downstream tasks including robotic manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning transferable visual models from natural language supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLIP (text encoder used)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dual-encoder contrastive architecture (image encoder + text encoder) that produces aligned image and text embeddings suitable for measuring semantic similarity; in this paper the CLIP text encoder is used to produce text embeddings for the goal-image prediction pretext.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Vision-language on large image–text pairs using contrastive objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Massive web-scale image-text pairs capturing object names, scene descriptions and common relations; does not directly include robot-action supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Used as language encoder for LaVA-Man pretraining and as baseline semantic model for CLIPort-style approaches in pick-and-place tasks</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Provides text embeddings for instructions (object descriptors, spatial language) paired with image observations in pretext and downstream tasks (tabletop pick-and-place, referring grounding).</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>High semantic overlap for object names and appearance descriptors; limited alignment for action-conditioned dynamics or fine-grained affordances, per paper discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Used as part of LaVA-Man; when used as a CLIP-based baseline it supports CLIPort achieving ~0.68–0.73 average in multi-task pick-and-place in tables shown.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>N/A (CLIP is itself pre-trained).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not explicitly quantified for CLIP alone in this paper; however CLIP-based baselines are less sample-efficient at capturing causal action-grounding compared to LaVA-Man given same downstream demos.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No CLIP attention analysis provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Paper refers to CLIP's unified embedding space as providing semantics but not causal action grounding; no additional embedding analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Paper argues CLIP alone lacks causal grounding between instruction and resulting visual state — motivating LaVA-Man's goal-image prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not analysed here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>CLIP transfers semantic knowledge well when object names/visual appearance occur in pretraining, but transfer to action-conditioned tasks is limited.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>CLIP provides open-vocabulary generalisation to novel object names, but this may not translate into action-grounded manipulation without additional training.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>CLIP supports zero-shot recognition in general vision tasks; not demonstrated as zero-shot manipulation controller here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not analysed here.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Potential negative effect: semantic similarity without causal grounding can reduce manipulation precision; noted qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>CLIP (vision-language) offers semantic advantages over vision-only pretrained models for recognition, but LaVA-Man's self-supervised multimodal pretext improves causal grounding for manipulation beyond CLIP-only similarity approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not discussed for CLIP in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1907.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1907.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Voltron</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Voltron</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior method combining video-based tasks with text conditioning to learn transferable visual representations for robotic tasks; treated as a baseline in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Voltron</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Video-based pretraining approach that combines visual and textual signals to learn manipulation-relevant features (details not expanded in snippet); used as a comparative baseline (self/weakly supervised).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Video + text pretraining (self/weakly-supervised) per original method</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Pretraining on human/robot manipulation episodes with video frames and associated textual supervision; may include temporal dynamics but can suffer from temporal redundancy.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robot manipulation benchmarks (evaluated as baseline on pick-and-place, OOPP, Franka Kitchen, referring grounding)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Same downstream tasks as LaVA-Man in paper: pick-and-place, Franka Kitchen, etc.; trained/fine-tuned on same downstream demonstration budgets for fair comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Designed to incorporate textual conditioning for semantics; alignment depends on manipulation videos used for pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>In this paper Voltron achieves average multi-task pick-and-place success ≈ 0.54 (54%) under the evaluated protocol and 69.8% average on OOPP (Table 2 shows Voltron 69.8%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>No-pretraining baseline reported at 0.47 in the same table; Voltron improves over no-pretraining but is outperformed by LaVA-Man.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Evaluated with same downstream demos (1,000 demonstrations for multi-task) and shows improved performance over no-pretraining but less than LaVA-Man; exact sample-efficiency curves not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported in provided text.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported in provided text.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Voltron combines video and text for semantics but in this paper is used as a baseline and not analysed for explicit action grounding evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Benefited by video pretraining but subject to temporal redundancy issues noted by authors; less benefit than LaVA-Man when object diversity and asymmetric masking pretext are used.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Evaluated on OOPP seen/unseen splits; performance lower than LaVA-Man on unseen categories.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not claimed as zero-shot in this evaluation; fine-tuned on downstream demos.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not explicitly reported in the provided text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Voltron (video+text) outperforms no-pretraining baseline but is outperformed by LaVA-Man (goal-image pretext).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Paper notes video pretraining can allow models to exploit temporal redundancy rather than learn meaningful representations; no detailed dynamics reported for Voltron here.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1907.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1907.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MPI ‡</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MPI (reimplemented, self-supervised)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pre-training method originally using per-object supervision (bounding boxes) and other components; in this paper reimplemented in a fully self-supervised way and used as a baseline for manipulation representation learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MPI ‡ (reimplementation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Original MPI method (details in original paper) aims to learn implicit features for robotic tasks; in this paper authors remove detection head and run MPI pretraining in a fully self-supervised manner and evaluate with the same action head and downstream setup.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Self-supervised pretraining on robot/human manipulation episodes (reimplementation removes supervised detection head)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Pretraining performed on the same mixed data as LaVA-Man for fair comparison (synthetic OOPP and Bridge/DROID episodes) but without LaVA-Man's goal-image asymmetric masking objective.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation benchmarks used in this paper (Ravens, OOPP, Franka Kitchen, referring grounding)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Same downstream tasks as other baselines; MPI ‡ fine-tuned with same affordance/action heads and demo budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>MPI originally used object bounding box supervision (semantic alignment); reimplementation removed that supervision so alignment is weaker in this paper's variant.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>MPI ‡ achieves average multi-task pick-and-place success ≈ 0.50 (50%) in the table and OOPP average ≈ 65.1% in Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>No-pretraining baseline in the same experiments is 0.47; MPI ‡ improves moderately over that baseline but is outperformed by LaVA-Man.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>MPI ‡ trained and fine-tuned with same downstream demo budgets as others; shows moderate gains but less sample-efficient than LaVA-Man per final success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported in provided text.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported in provided text.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Original MPI used detection supervision which provided some explicit grounding; the self-supervised reimplementation used here lacks that explicit supervision and shows reduced performance vs LaVA-Man.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Removal of detection supervision reduces alignment to object semantics; performs worse on unseen classes relative to LaVA-Man.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Evaluated on OOPP seen/unseen splits; MPI ‡ underperforms LaVA-Man on unseen categories.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>No zero-shot claims; evaluated with downstream fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not explicitly reported in provided text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>MPI ‡ is a self-supervised method that improves over no-pretraining but is outperformed by LaVA-Man's goal-image multimodal pretext.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1907.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1907.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OOPP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Omni-Object Pick-and-Place (OOPP) dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A newly introduced simulated tabletop manipulation dataset of 3,200 real-scanned object instances across 180 classes with language annotations and full pick-and-place episodes used for pretraining and evaluation of language-conditioned manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OOPP (dataset used for pretraining/evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a model — a dataset built on OmniObject3D real-scanned meshes, filtered and scaled for tabletop manipulation, with simulated episodes generated in PyBullet Gym; includes textual descriptions for objects and language instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Dataset used for self-supervised multimodal pretraining (synthetic simulated episodes) and for downstream evaluation (intra/inter-class splits).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>3,200 distinct real-scanned object instances from 180 categories (filtered to diameters 4cm–40cm), simulated pick-and-place episodes with before/after frames and automatically-generated language instructions based on OmniObject3D descriptions; diverse object priors, intra-class and inter-class splits for generalisation evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Used as pretraining data and as an evaluation benchmark for tabletop manipulation (packing tasks, sequence/group variants), intra-class and inter-class generalisation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Simulated PyBullet tabletop tasks including packing-objects-group and packing-objects-sequence with variations designed to evaluate intra-class instance generalisation and inter-class unseen generalisation; action space SE(2) pick & place.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>High alignment with downstream tabletop tasks since objects are real-scanned and language is derived from OmniObject3D descriptions, enabling the pretext task to learn meaningful object priors and instruction-conditioned transformations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Using OOPP in pretraining improves downstream performance: LaVA-Man pretrained with OOPP reaches OOPP benchmark average 79.6% and strong inter/intra-class generalisation as reported in Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Methods pretrained without OOPP or without the LaVA-Man pretext perform worse (e.g., Voltron 69.8%, MPI ‡ 65.1%, CLIPort 58.7% under same evaluation in Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Inclusion of OOPP in pretraining boosts downstream performance given same fine-tuning budgets; exact sample-efficiency multiples not provided, but ablation shows OOPP improves generalisation significantly.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not applicable to dataset itself; paper uses OOPP in ablations showing dataset diversity improves transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not directly reported for dataset, but authors state models trained with OOPP learn broader object priors and better generalisable visual-action representations.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>OOPP provides paired before/after images with instructions that implicitly encode action semantics for pretraining goal-image prediction, which the paper shows leads to better action-grounded representations.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not directly analysed for dataset but OOPP's diversity supports learning higher-level object semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>OOPP's inclusion in pretraining improves transfer particularly when target tasks involve real-scanned objects and similar semantics; domain mismatch (e.g., highly-coloured synthetic geometries not common in OOPP) reduces transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Dataset is partitioned to explicitly test intra-class (held-out instances from seen categories) and inter-class (20 unseen categories) generalisation; LaVA-Man shows higher scores on seen than unseen but strong performance on unseen (e.g., seen packing 0.84 vs unseen 0.73 in one task column).</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Dataset enables few-shot fine-tuning evaluations (1,000 downstream demonstrations used in experiments) and tests generalisation to held-out unseen classes; zero-shot manipulation not claimed by dataset itself.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not applicable to dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not applicable directly, but paper notes models pretrained on datasets lacking certain distributions (e.g., colored geometries) may underperform on those distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>OOPP is used alongside multimodal pretext; comparisons show LaVA-Man (multimodal pretraining on OOPP+real videos) outperforms vision-only/self-supervised baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Dataset provides episodic before/after pairs (no long temporal sequences by design in pretraining), intentionally reducing temporal redundancy compared to full video pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Cliport: What and where pathways for robotic manipulation. <em>(Rating: 2)</em></li>
                <li>Learning transferable visual models from natural language supervision. <em>(Rating: 2)</em></li>
                <li>VIMA: General robot manipulation with multimodal prompts. <em>(Rating: 2)</em></li>
                <li>R3m: A universal visual representation for robot manipulation. <em>(Rating: 2)</em></li>
                <li>Voltron <em>(Rating: 1)</em></li>
                <li>MPI <em>(Rating: 1)</em></li>
                <li>VIP: Towards universal visual reward and representation via value-implicit pre-training. <em>(Rating: 1)</em></li>
                <li>SUGAR: Pre-training 3d visual representations for robotics. <em>(Rating: 2)</em></li>
                <li>3d-mvp: 3d multiview pretraining for robotic manipulation. <em>(Rating: 2)</em></li>
                <li>BridgeData V2: A dataset for robot learning at scale. <em>(Rating: 2)</em></li>
                <li>DROID: A large-scale in-the-wild robot manipulation dataset. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1907",
    "paper_id": "paper-280919180",
    "extraction_schema_id": "extraction-schema-39",
    "extracted_data": [
        {
            "name_short": "LaVA-Man",
            "name_full": "Language-guided Visual-Action representations for robot Manipulation (LaVA-Man)",
            "brief_description": "A self-supervised vision-language representation learning method that predicts a masked goal image conditioned on an initial image and a language instruction to learn language-grounded visual-action features that are fine-tunable for manipulation with few demonstrations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LaVA-Man",
            "model_description": "Siamese Vision Transformer (ViT-Base, patch 16) encoder with asymmetric masking applied only to the goal image, CLIP text encoder for language, multi-stage cross-attention fusion (text↔image, then v_f queries fused features), a lightweight decoder and a per-patch MLP head that reconstructs RGB goal patches; affordance / action heads appended for downstream SE(2) pick-and-place or 9-DoF joint outputs.",
            "pretraining_type": "self-supervised multimodal (vision+text) pretraining via masked goal-image prediction on image-pair + instruction tuples",
            "pretraining_data_description": "Mixed synthetic (OOPP simulated tabletop episodes; 120k samples drawn from OOPP) and real-world robot video pairs (Bridge and DROID) where each sample is (initial image, final image with heavy masking, textual instruction). Language annotations include object descriptions and natural instructions (spatial and object descriptors); supervision contains implicit action semantics via before/after image pairs but no direct robot action labels used for representation pretraining.",
            "target_task_name": "Robot manipulation (tabletop pick-and-place), visuomotor control (Franka Kitchen), referring-expression grounding, real-robot manipulation",
            "target_task_description": "Multiple embodied tasks: (1) Tabletop SE(2) pick-and-place (affordance map → pick/place poses) in simulation (Ravens, OOPP) and on a top-down real UR5 setup; (2) Franka Kitchen joint-angle visuomotor prediction (9-DoF/joint velocities) in simulation; (3) referring-expression bounding-box grounding in cluttered scenes; both simulated and real-world evaluations are used. Action spaces: SE(2) + discrete rotation bins for pick-and-place; continuous 9-DoF for joint predictions.",
            "semantic_alignment": "Paper discusses semantic alignment explicitly: pretraining pairs include real-scanned object instances and language descriptions beyond object names (appearance, spatial phrasing). Strong overlap in object categories between OOPP pretraining and downstream tabletop tasks improves transfer; however distributional mismatch (e.g., uncommon coloured geometries) reduces performance.",
            "performance_with_language_pretraining": "Pick-and-place multi-task average success: 0.81 (81%) across simulated benchmark reported in Table (Ours = 0.81). OOPP benchmark average success: 79.6% (0.796) when fine-tuned on 1,000 demonstrations (Table 2). Franka Kitchen: reported SOTA and outperforms self-supervised baselines (exact aggregated number not tabulated in snippet for LaVA-Man across five tasks, but reported as state-of-the-art in text). Referring-expression grounding: comparable to supervised pretraining (supervised baselines ~0.96; LaVA-Man reported to match or be close). Real-robot perceptual scores and physical scores reported per-task (visual examples show robust affordance detection and generalisation to unseen objects).",
            "performance_without_language_pretraining": "No-pretraining baseline average multi-task success: 0.47 (47%) on the pick-and-place multi-task benchmark (same downstream fine-tuning protocol). Other self-/weakly-supervised baselines: Voltron 0.54, MPI ‡ 0.50 (same evaluation). CLIPort reported ~0.68–0.73 depending on variant in the same multi-task table.",
            "sample_efficiency_comparison": "LaVA-Man is fine-tuned with small numbers of demonstrations for downstream tasks: Franka Kitchen action head trained with backbone frozen using 25 demonstrations (standard protocol); multi-task pick-and-place experiments used 1,000 demonstrations for multi-task training and 100 test demos; real-robot fine-tuning used 200 collected demos for the real robot tasks. Compared to the 'No pre-training' baseline trained with same downstream demos, LaVA-Man achieves substantially higher success (e.g., 81% vs 47% average), indicating improved sample efficiency, but the paper does not give an explicit multiplicative sample-efficiency ratio (e.g., '5x fewer samples').",
            "has_sample_efficiency_data": true,
            "attention_analysis": "The paper describes multi-stage cross-attention fusion (text↔image and image-to-goal) and ablates the fusion module showing removal degrades downstream performance, indicating the fusion attention is important; qualitative goal-image prediction visualisations and affordance maps are shown. No detailed quantitative attention-map statistics or per-head attention visualisations beyond these ablations and qualitative examples are provided in the snippet.",
            "embedding_space_analysis": "Authors claim the pretext task produces semantic visual-action representations and show improved referring-expression grounding and clustering by behaviour (qualitative). No explicit PCA/clustering statistics or embedding-space dimensionality numbers are reported in the provided text.",
            "action_grounding_evidence": "Direct evidence via the goal-image prediction pretext: reconstructing the goal image from (initial image + instruction) forces the model to learn how instructions map to visual state changes (implicit action semantics). Empirical evidence: (1) improved performance on referring-expression grounding (accurate localisation conditioned on language), (2) improved pick-and-place and Franka Kitchen performance vs baselines, and (3) qualitative examples showing predicted goal states align with instructions. The paper also reports a perception-to-physical gap (perceptual correctness does not always lead to physical execution success), indicating perception/action grounding is present but not complete.",
            "hierarchical_features_evidence": "Limited explicit analysis: authors note ViT patching leads to loss of fine-grained spatial detail (hurting precision on small/fine objects), implying lower-level high-resolution features are important and less captured; no systematic layerwise hierarchical feature study reported in the snippet.",
            "transfer_conditions": "Transfer improves with: (1) diversity of object priors in pretraining (OOPP helps); (2) mixed synthetic+real pretraining data. Transfer degrades when: (1) object appearances (coloured geometries) are uncommon in pretraining; (2) tasks requiring fine spatial precision or explicit 3D reasoning (model lacks explicit 3D awareness). Ablation: removing fusion or asymmetric masking reduces transfer performance. Masking ratio matters: 95% asymmetric masking best; 100% mask causes convergence issues.",
            "novel_vs_familiar_objects": "Paper evaluates intra-class (held-out instances) and inter-class (entirely unseen categories) generalisation. Example numbers from OOPP: for 'packing obj-seq' (seen) LaVA-Man = 0.84, while 'packing obj-inter-seq' (unseen classes) = 0.73; overall OOPP seen/unseen columns show performance drop but still strong generalisation to unseen classes.",
            "zero_shot_or_few_shot": "Few-shot: Franka Kitchen action head trained with 25 demonstrations (frozen backbone) — achieves strong performance; real-robot adaptation performed with 200 collected demos and shows generalisation to unseen objects and tasks absent from downstream demos. No claim of reliable zero-shot manipulation (zero-shot not reported quantitatively).",
            "layer_analysis": "Some component-level analysis: for joint-angle tasks (Franka Kitchen) authors freeze the encoder and drop the decoder (consistent with baselines) and train a shallow MLP action head; ablation studies probe the fusion module and masking strategy. No per-transformer-layer importance rankings reported in the text excerpt.",
            "negative_transfer_evidence": "Reported limited performance on colored geometries (paper attributes this to rarity in pretraining data) and poor performance for very small / fine-grained objects (ViT patching resolution issue). Also perception-to-physical gap in real-robot trials shows cases where correct perceived affordance fails in execution.",
            "comparison_to_vision_only": "Paper compares to 'no pre-training' and to vision-only/self-supervised vision baselines (Voltron, MPI reimplementation). LaVA-Man outperforms those baselines on multi-task pick-and-place (LaVA-Man 0.81 vs No-pretraining 0.47, Voltron 0.54, MPI ‡ 0.50) and on OOPP (LaVA-Man 79.6% avg vs Voltron 69.8%, MPI ‡ 65.1%, CLIPort 58.7% in that table).",
            "temporal_dynamics": "No in-depth study of representational change over fine-tuning epochs beyond standard training curves; authors report the effect of masking ratios on training convergence (100% mask causes convergence issues), but not a temporal learning-phase analysis.",
            "dimensionality_analysis": "No explicit dimensionality or intrinsic-dimension analyses (PCA, effective rank) reported in the provided text.",
            "uuid": "e1907.0"
        },
        {
            "name_short": "CLIPort",
            "name_full": "CLIPort: What and Where Pathways for Robotic Manipulation",
            "brief_description": "A vision-language grounded pick-and-place framework that uses CLIP embeddings to provide semantic priors (what) combined with spatial affordance mapping (where) to produce pick-and-place actions.",
            "citation_title": "Cliport: What and where pathways for robotic manipulation.",
            "mention_or_use": "use",
            "model_name": "CLIPort",
            "model_description": "Two-stream architecture that leverages CLIP image/text embeddings to represent object semantics, coupled with spatial 'where' pathway to predict pixel-level affordances for pick-and-place; often uses convolutional backbones for spatial maps and CLIP for semantic encoding.",
            "pretraining_type": "Vision-language pretraining on large image–text pairs (CLIP) used as foundation for semantics; task-specific components trained on labelled manipulation data.",
            "pretraining_data_description": "CLIP foundation trained on large web-scale image-text pairs (object descriptions, object names, spatial text); CLIPort uses those embeddings and fine-tunes task heads on manipulation demonstrations (contains object names and commonly-occurring spatial relations but not explicit action sequences from robot data in CLIP itself).",
            "target_task_name": "Tabletop pick-and-place manipulation (multi-task rearrangement)",
            "target_task_description": "Pixel-level affordance prediction for SE(2) pick-and-place in simulated tabletop rearrangement tasks (Ravens and multi-task benchmarks); environments are simulated with rearrangement, grouped packing and sequence tasks; action space is SE(2) pick & place with discrete rotation bins.",
            "semantic_alignment": "CLIP embeddings provide strong semantic overlap for object categories and names; however paper notes CLIP-based similarity alone lacks causal grounding to predict how an action transforms the scene.",
            "performance_with_language_pretraining": "Reported CLIPort baselines in this paper: e.g., CLIPort variants show multi-task averages around 0.68–0.73 in the presented pick-and-place table (values depend on variant).",
            "performance_without_language_pretraining": "Not directly stated in this paper for CLIPort itself, but CLIPort demonstrates better generalisation than vision-only baselines in related prior work.",
            "sample_efficiency_comparison": "CLIPort is used as a baseline trained with the same downstream demonstration budgets in this paper (e.g., 1,000 demonstrations for multi-task pick-and-place); LaVA-Man outperforms CLIPort given same downstream data.",
            "has_sample_efficiency_data": true,
            "attention_analysis": "This paper does not provide new attention analyses for CLIPort beyond reporting comparative performance.",
            "embedding_space_analysis": "Not analysed further here; CLIP embeddings are used as-is for semantics.",
            "action_grounding_evidence": "Paper critiques CLIP-based approaches (including CLIPort) as lacking causal grounding between visual input, instruction and resulting state, motivating LaVA-Man's goal-image prediction objective.",
            "hierarchical_features_evidence": "Not analysed in this paper for CLIPort.",
            "transfer_conditions": "CLIPort benefits from semantic overlap between pretraining (web images) and downstream object categories but is limited in capturing action-conditioned visual dynamics.",
            "novel_vs_familiar_objects": "CLIPort previously shown to generalise to open-set objects by leveraging CLIP semantics; in this paper LaVA-Man surpasses CLIPort on many seen/unseen OOPP splits.",
            "zero_shot_or_few_shot": "CLIPort supports some open-vocabulary generalisation via CLIP but in this paper it is evaluated with downstream fine-tuning and not quantified as zero-shot.",
            "layer_analysis": "No additional layer-wise analysis provided in this paper.",
            "negative_transfer_evidence": "Paper suggests CLIP-only similarity approaches can limit precision in manipulation, implying potential mismatch when semantics do not encode actionable state transformations.",
            "comparison_to_vision_only": "CLIPort (vision-language) generally outperforms some purely vision-only baselines in prior work; in this paper LaVA-Man (self-supervised multimodal pretraining) outperforms CLIPort under the same downstream regimes.",
            "temporal_dynamics": "No temporal dynamics analysis provided here for CLIPort.",
            "dimensionality_analysis": "No dimensionality analysis provided here for CLIPort.",
            "uuid": "e1907.1"
        },
        {
            "name_short": "CLIP",
            "name_full": "CLIP (Contrastive Language–Image Pretraining)",
            "brief_description": "A large vision-language contrastive model that embeds images and text into a shared semantic space, widely used to provide semantic priors for downstream tasks including robotic manipulation.",
            "citation_title": "Learning transferable visual models from natural language supervision.",
            "mention_or_use": "use",
            "model_name": "CLIP (text encoder used)",
            "model_description": "Dual-encoder contrastive architecture (image encoder + text encoder) that produces aligned image and text embeddings suitable for measuring semantic similarity; in this paper the CLIP text encoder is used to produce text embeddings for the goal-image prediction pretext.",
            "pretraining_type": "Vision-language on large image–text pairs using contrastive objectives.",
            "pretraining_data_description": "Massive web-scale image-text pairs capturing object names, scene descriptions and common relations; does not directly include robot-action supervision.",
            "target_task_name": "Used as language encoder for LaVA-Man pretraining and as baseline semantic model for CLIPort-style approaches in pick-and-place tasks",
            "target_task_description": "Provides text embeddings for instructions (object descriptors, spatial language) paired with image observations in pretext and downstream tasks (tabletop pick-and-place, referring grounding).",
            "semantic_alignment": "High semantic overlap for object names and appearance descriptors; limited alignment for action-conditioned dynamics or fine-grained affordances, per paper discussion.",
            "performance_with_language_pretraining": "Used as part of LaVA-Man; when used as a CLIP-based baseline it supports CLIPort achieving ~0.68–0.73 average in multi-task pick-and-place in tables shown.",
            "performance_without_language_pretraining": "N/A (CLIP is itself pre-trained).",
            "sample_efficiency_comparison": "Not explicitly quantified for CLIP alone in this paper; however CLIP-based baselines are less sample-efficient at capturing causal action-grounding compared to LaVA-Man given same downstream demos.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No CLIP attention analysis provided here.",
            "embedding_space_analysis": "Paper refers to CLIP's unified embedding space as providing semantics but not causal action grounding; no additional embedding analysis.",
            "action_grounding_evidence": "Paper argues CLIP alone lacks causal grounding between instruction and resulting visual state — motivating LaVA-Man's goal-image prediction.",
            "hierarchical_features_evidence": "Not analysed here.",
            "transfer_conditions": "CLIP transfers semantic knowledge well when object names/visual appearance occur in pretraining, but transfer to action-conditioned tasks is limited.",
            "novel_vs_familiar_objects": "CLIP provides open-vocabulary generalisation to novel object names, but this may not translate into action-grounded manipulation without additional training.",
            "zero_shot_or_few_shot": "CLIP supports zero-shot recognition in general vision tasks; not demonstrated as zero-shot manipulation controller here.",
            "layer_analysis": "Not analysed here.",
            "negative_transfer_evidence": "Potential negative effect: semantic similarity without causal grounding can reduce manipulation precision; noted qualitatively.",
            "comparison_to_vision_only": "CLIP (vision-language) offers semantic advantages over vision-only pretrained models for recognition, but LaVA-Man's self-supervised multimodal pretext improves causal grounding for manipulation beyond CLIP-only similarity approaches.",
            "temporal_dynamics": "Not discussed for CLIP in this paper.",
            "dimensionality_analysis": "Not provided.",
            "uuid": "e1907.2"
        },
        {
            "name_short": "Voltron",
            "name_full": "Voltron",
            "brief_description": "A prior method combining video-based tasks with text conditioning to learn transferable visual representations for robotic tasks; treated as a baseline in this paper.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Voltron",
            "model_description": "Video-based pretraining approach that combines visual and textual signals to learn manipulation-relevant features (details not expanded in snippet); used as a comparative baseline (self/weakly supervised).",
            "pretraining_type": "Video + text pretraining (self/weakly-supervised) per original method",
            "pretraining_data_description": "Pretraining on human/robot manipulation episodes with video frames and associated textual supervision; may include temporal dynamics but can suffer from temporal redundancy.",
            "target_task_name": "Robot manipulation benchmarks (evaluated as baseline on pick-and-place, OOPP, Franka Kitchen, referring grounding)",
            "target_task_description": "Same downstream tasks as LaVA-Man in paper: pick-and-place, Franka Kitchen, etc.; trained/fine-tuned on same downstream demonstration budgets for fair comparison.",
            "semantic_alignment": "Designed to incorporate textual conditioning for semantics; alignment depends on manipulation videos used for pretraining.",
            "performance_with_language_pretraining": "In this paper Voltron achieves average multi-task pick-and-place success ≈ 0.54 (54%) under the evaluated protocol and 69.8% average on OOPP (Table 2 shows Voltron 69.8%).",
            "performance_without_language_pretraining": "No-pretraining baseline reported at 0.47 in the same table; Voltron improves over no-pretraining but is outperformed by LaVA-Man.",
            "sample_efficiency_comparison": "Evaluated with same downstream demos (1,000 demonstrations for multi-task) and shows improved performance over no-pretraining but less than LaVA-Man; exact sample-efficiency curves not provided.",
            "has_sample_efficiency_data": true,
            "attention_analysis": "Not reported in provided text.",
            "embedding_space_analysis": "Not reported in provided text.",
            "action_grounding_evidence": "Voltron combines video and text for semantics but in this paper is used as a baseline and not analysed for explicit action grounding evidence.",
            "hierarchical_features_evidence": "Not reported here.",
            "transfer_conditions": "Benefited by video pretraining but subject to temporal redundancy issues noted by authors; less benefit than LaVA-Man when object diversity and asymmetric masking pretext are used.",
            "novel_vs_familiar_objects": "Evaluated on OOPP seen/unseen splits; performance lower than LaVA-Man on unseen categories.",
            "zero_shot_or_few_shot": "Not claimed as zero-shot in this evaluation; fine-tuned on downstream demos.",
            "layer_analysis": "Not reported here.",
            "negative_transfer_evidence": "Not explicitly reported in the provided text.",
            "comparison_to_vision_only": "Voltron (video+text) outperforms no-pretraining baseline but is outperformed by LaVA-Man (goal-image pretext).",
            "temporal_dynamics": "Paper notes video pretraining can allow models to exploit temporal redundancy rather than learn meaningful representations; no detailed dynamics reported for Voltron here.",
            "dimensionality_analysis": "Not provided.",
            "uuid": "e1907.3"
        },
        {
            "name_short": "MPI ‡",
            "name_full": "MPI (reimplemented, self-supervised)",
            "brief_description": "A pre-training method originally using per-object supervision (bounding boxes) and other components; in this paper reimplemented in a fully self-supervised way and used as a baseline for manipulation representation learning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "MPI ‡ (reimplementation)",
            "model_description": "Original MPI method (details in original paper) aims to learn implicit features for robotic tasks; in this paper authors remove detection head and run MPI pretraining in a fully self-supervised manner and evaluate with the same action head and downstream setup.",
            "pretraining_type": "Self-supervised pretraining on robot/human manipulation episodes (reimplementation removes supervised detection head)",
            "pretraining_data_description": "Pretraining performed on the same mixed data as LaVA-Man for fair comparison (synthetic OOPP and Bridge/DROID episodes) but without LaVA-Man's goal-image asymmetric masking objective.",
            "target_task_name": "Robotic manipulation benchmarks used in this paper (Ravens, OOPP, Franka Kitchen, referring grounding)",
            "target_task_description": "Same downstream tasks as other baselines; MPI ‡ fine-tuned with same affordance/action heads and demo budgets.",
            "semantic_alignment": "MPI originally used object bounding box supervision (semantic alignment); reimplementation removed that supervision so alignment is weaker in this paper's variant.",
            "performance_with_language_pretraining": "MPI ‡ achieves average multi-task pick-and-place success ≈ 0.50 (50%) in the table and OOPP average ≈ 65.1% in Table 2.",
            "performance_without_language_pretraining": "No-pretraining baseline in the same experiments is 0.47; MPI ‡ improves moderately over that baseline but is outperformed by LaVA-Man.",
            "sample_efficiency_comparison": "MPI ‡ trained and fine-tuned with same downstream demo budgets as others; shows moderate gains but less sample-efficient than LaVA-Man per final success rates.",
            "has_sample_efficiency_data": true,
            "attention_analysis": "Not reported in provided text.",
            "embedding_space_analysis": "Not reported in provided text.",
            "action_grounding_evidence": "Original MPI used detection supervision which provided some explicit grounding; the self-supervised reimplementation used here lacks that explicit supervision and shows reduced performance vs LaVA-Man.",
            "hierarchical_features_evidence": "Not reported here.",
            "transfer_conditions": "Removal of detection supervision reduces alignment to object semantics; performs worse on unseen classes relative to LaVA-Man.",
            "novel_vs_familiar_objects": "Evaluated on OOPP seen/unseen splits; MPI ‡ underperforms LaVA-Man on unseen categories.",
            "zero_shot_or_few_shot": "No zero-shot claims; evaluated with downstream fine-tuning.",
            "layer_analysis": "Not provided.",
            "negative_transfer_evidence": "Not explicitly reported in provided text.",
            "comparison_to_vision_only": "MPI ‡ is a self-supervised method that improves over no-pretraining but is outperformed by LaVA-Man's goal-image multimodal pretext.",
            "temporal_dynamics": "Not discussed.",
            "dimensionality_analysis": "Not provided.",
            "uuid": "e1907.4"
        },
        {
            "name_short": "OOPP",
            "name_full": "Omni-Object Pick-and-Place (OOPP) dataset",
            "brief_description": "A newly introduced simulated tabletop manipulation dataset of 3,200 real-scanned object instances across 180 classes with language annotations and full pick-and-place episodes used for pretraining and evaluation of language-conditioned manipulation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OOPP (dataset used for pretraining/evaluation)",
            "model_description": "Not a model — a dataset built on OmniObject3D real-scanned meshes, filtered and scaled for tabletop manipulation, with simulated episodes generated in PyBullet Gym; includes textual descriptions for objects and language instructions.",
            "pretraining_type": "Dataset used for self-supervised multimodal pretraining (synthetic simulated episodes) and for downstream evaluation (intra/inter-class splits).",
            "pretraining_data_description": "3,200 distinct real-scanned object instances from 180 categories (filtered to diameters 4cm–40cm), simulated pick-and-place episodes with before/after frames and automatically-generated language instructions based on OmniObject3D descriptions; diverse object priors, intra-class and inter-class splits for generalisation evaluation.",
            "target_task_name": "Used as pretraining data and as an evaluation benchmark for tabletop manipulation (packing tasks, sequence/group variants), intra-class and inter-class generalisation",
            "target_task_description": "Simulated PyBullet tabletop tasks including packing-objects-group and packing-objects-sequence with variations designed to evaluate intra-class instance generalisation and inter-class unseen generalisation; action space SE(2) pick & place.",
            "semantic_alignment": "High alignment with downstream tabletop tasks since objects are real-scanned and language is derived from OmniObject3D descriptions, enabling the pretext task to learn meaningful object priors and instruction-conditioned transformations.",
            "performance_with_language_pretraining": "Using OOPP in pretraining improves downstream performance: LaVA-Man pretrained with OOPP reaches OOPP benchmark average 79.6% and strong inter/intra-class generalisation as reported in Table 2.",
            "performance_without_language_pretraining": "Methods pretrained without OOPP or without the LaVA-Man pretext perform worse (e.g., Voltron 69.8%, MPI ‡ 65.1%, CLIPort 58.7% under same evaluation in Table 2).",
            "sample_efficiency_comparison": "Inclusion of OOPP in pretraining boosts downstream performance given same fine-tuning budgets; exact sample-efficiency multiples not provided, but ablation shows OOPP improves generalisation significantly.",
            "has_sample_efficiency_data": true,
            "attention_analysis": "Not applicable to dataset itself; paper uses OOPP in ablations showing dataset diversity improves transfer.",
            "embedding_space_analysis": "Not directly reported for dataset, but authors state models trained with OOPP learn broader object priors and better generalisable visual-action representations.",
            "action_grounding_evidence": "OOPP provides paired before/after images with instructions that implicitly encode action semantics for pretraining goal-image prediction, which the paper shows leads to better action-grounded representations.",
            "hierarchical_features_evidence": "Not directly analysed for dataset but OOPP's diversity supports learning higher-level object semantics.",
            "transfer_conditions": "OOPP's inclusion in pretraining improves transfer particularly when target tasks involve real-scanned objects and similar semantics; domain mismatch (e.g., highly-coloured synthetic geometries not common in OOPP) reduces transfer.",
            "novel_vs_familiar_objects": "Dataset is partitioned to explicitly test intra-class (held-out instances from seen categories) and inter-class (20 unseen categories) generalisation; LaVA-Man shows higher scores on seen than unseen but strong performance on unseen (e.g., seen packing 0.84 vs unseen 0.73 in one task column).",
            "zero_shot_or_few_shot": "Dataset enables few-shot fine-tuning evaluations (1,000 downstream demonstrations used in experiments) and tests generalisation to held-out unseen classes; zero-shot manipulation not claimed by dataset itself.",
            "layer_analysis": "Not applicable to dataset.",
            "negative_transfer_evidence": "Not applicable directly, but paper notes models pretrained on datasets lacking certain distributions (e.g., colored geometries) may underperform on those distributions.",
            "comparison_to_vision_only": "OOPP is used alongside multimodal pretext; comparisons show LaVA-Man (multimodal pretraining on OOPP+real videos) outperforms vision-only/self-supervised baselines.",
            "temporal_dynamics": "Dataset provides episodic before/after pairs (no long temporal sequences by design in pretraining), intentionally reducing temporal redundancy compared to full video pretraining.",
            "dimensionality_analysis": "Not provided.",
            "uuid": "e1907.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Cliport: What and where pathways for robotic manipulation.",
            "rating": 2
        },
        {
            "paper_title": "Learning transferable visual models from natural language supervision.",
            "rating": 2
        },
        {
            "paper_title": "VIMA: General robot manipulation with multimodal prompts.",
            "rating": 2
        },
        {
            "paper_title": "R3m: A universal visual representation for robot manipulation.",
            "rating": 2
        },
        {
            "paper_title": "Voltron",
            "rating": 1
        },
        {
            "paper_title": "MPI",
            "rating": 1
        },
        {
            "paper_title": "VIP: Towards universal visual reward and representation via value-implicit pre-training.",
            "rating": 1
        },
        {
            "paper_title": "SUGAR: Pre-training 3d visual representations for robotics.",
            "rating": 2
        },
        {
            "paper_title": "3d-mvp: 3d multiview pretraining for robotic manipulation.",
            "rating": 2
        },
        {
            "paper_title": "BridgeData V2: A dataset for robot learning at scale.",
            "rating": 2
        },
        {
            "paper_title": "DROID: A large-scale in-the-wild robot manipulation dataset.",
            "rating": 2
        }
    ],
    "cost": 0.021527249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>26 Aug 2025
26 Aug 20256F095CD9509D837C65DA7CBFE28F91BDarXiv:2508.19391v1[cs.RO]Robot manipulation, self-supervised representation learning
Visual-textual understanding is essential for language-guided robot manipulation.Recent works leverage pre-trained vision-language models to measure the similarity between encoded visual observations and textual instructions, and then train a model to map this similarity to robot actions.However, this two-step approach limits the model to capture the relationship between visual observations and textual instructions, leading to reduced precision in manipulation tasks.We propose to learn visual-textual associations through a self-supervised pretext task: reconstructing a masked goal image conditioned on an input image and textual instructions.This formulation allows the model to learn visual-action representations without robot action supervision.The learned representations can then be fine-tuned for manipulation tasks with only a few demonstrations.We also introduce the Omni-Object Pick-and-Place dataset, which consists of annotated robot tabletop manipulation episodes, including 180 object classes and 3,200 instances with corresponding textual instructions.This dataset enables the model to acquire diverse object priors and allows for a more comprehensive evaluation of its generalisation capability across object instances.Experimental results on the five benchmarks, including both simulated and real-robot validations, demonstrate that our method outperforms prior art.</p>
<p>Introduction</p>
<p>Language-guided robot manipulation is a fundamental task in robotics, enabling embodied agents to interpret human instructions and interact with complex environments.This task requires learning a robust representation that effectively associates visual observations with textual instructions, and can be readily mapped to the corresponding robot actions.</p>
<p>The key challenge is to learn such representations in a scalable manner without heavily relying on robot action annotations, such as ground-truth affordance or joint angles.Several works [1,2,3,4] leverage pre-trained vision-language foundation models, such as CLIP [5], which encode images and text into a unified embedding space to serve as the visual-textual representations.These methods compute the cosine similarity between image and text embeddings and learn to map this similarity to robot actions.However, their representations lack causal grounding as they do not capture how the input visual state and textual instructions lead to the resulting visual state after the robot executes an action.Namely, they do not learn true language-guided visual-action representations necessary for manipulation.</p>
<p>To address this limitation, we propose LaVA-Man, a self-supervised learning approach for learning Language-guided Visual-Action representations for robot Manipulation.Given textual instructions and visual observations before and after a manipulation, we mask the goal image and train the model to predict its masked content, conditioned on the input image and the language instruction, with minimal guidance from the unmasked regions.Unlike prior approaches that adopt pretext tasks for general-purpose vision (e.g., masked image reconstruction [6,7]), our goal-image prediction objective captures the underlying causality of manipulation: it enables the model to implicitly learn the association between visual dynamics and action semantics, which is critical for robotic reasoning.</p>
<p>To learn representations that capture diverse and open-vocabulary textual instructions, it is important to train on sequences involving various object instances.However, existing manipulation datasets, such as Ravens [8] and VIMA [9], suffer from limited object diversity.To this end, we introduce the Omni-Object Pick-and-Place (OOPP) dataset, a simulated tabletop manipulation dataset that consists of 180 object classes and 3,200 unique instances.Built on high-quality real-scanned meshes with language annotations from OmniObject3D [10], OOPP includes curated, scaled objects suitable for manipulation, with scene sequences automatically simulated using PyBullet Gym [11].For a comprehensive evaluation, we hold out 20 object classes as unseen for inter-class generalisation evaluation, and reserve a subset of instances from 20 seen categories for intra-class evaluation.</p>
<p>We train LaVA-Man on a mixture of synthetic data from the proposed OOPP dataset and real-world robot videos from Bridge [12] and DROID [13].Once pre-trained, our model can be efficiently finetuned with only a few demonstrations for various downstream robotic perception and manipulation tasks.Our contributions are summarised as:</p>
<p>• We propose a self-supervised approach for learning a robust, versatile visual-action representation that can be efficiently fine-tuned on various robotic tasks with a few demonstrations.• We introduce a new dataset based on the existing pick-and-place benchmarks [1,8,9].Our dataset features 3,200 unique, real-scanned objects from 180 categories.• We validate our approach on five downstream robotic tasks, including simulated and real-world environments, and establish state-of-the-art performance.</p>
<p>Related work</p>
<p>Vision-based robot manipulation.Classic vision-based manipulation methods usually rely on a two-stage pipeline, where vision-based perception [14,15,16,17] is utilised first, followed by a control algorithm [18].Recent methods have shifted towards end-to-end learning frameworks [19,20,21,22].However, these methods often rely on limited training labels and demonstrate limited generalisation ability.CLIPort [1] and its variants [2,4] show enhanced generalisation to open-set problems by leveraging the semantic understanding provided by CLIP [5].However, their CLIP-based representations simply provide visual-textual similarity without causal grounding and limit their capabilities in capturing the underlying causality of the language-guided manipulation.</p>
<p>Besides CLIP, other recent methods [23,24,25] utilise large-scale vision-language models (VLMs) to achieve generalisation.But the reliance on large-scale models may introduce practical limitations.</p>
<p>Self-supervised visual pre-training.Self-supervised visual pre-training has become a fundamental approach for learning generalisable visual representations from large-scale, unlabeled data through various pretext tasks such as solving jigsaw puzzles [26], image colourisation [27], rotation estimation [28], inpainting [29], and instance discrimination [30,31].Inspired by masked token prediction in BERT [32], Masked-autoencoders (MAE) [33] mask portions of input images and train the model to reconstruct the missing patches.This method shows great success in learning generalisable representations and leads to various follow-up works on learning temporal correspondence [34,35] and spatial information [36].We propose goal-image prediction as a pretext task that implicitly learns the causality in language-guided manipulation, enabling our model to learn visual-action representations that associate visual states with robot actions conditioned on instructions.</p>
<p>Visual pre-training for robotics.Visual pre-training can be applied to robotic manipulation to enhance the generalisation ability [7,37].They rely on various pre-training methods, such as contrastive learning [38,39], MAE [7,40,41], and other perception tasks [42,43].For instance, the approach in [37] pre-trains a visual backbone using classic tasks such as image classification and object detection.VIP [44], R3M [45], MVP [40,41], and MCR [46] aim to learn implicit features for robotic tasks by training on video data, while Thiea [47] and SUSIE [48] try to distil knowledge from pre-trained vision foundation models.SUGAR [43] and 3D-MVP [49] extend the pre-training tasks to the 3D domain.To further improve the semantic understanding, Voltron [7] and MPI [42] combine video-based tasks with text conditioning, enabling generalisation to diverse object manipulation tasks.However, video data often contains temporal redundancy, which may allow models to exploit smooth temporal transitions rather than learning meaningful representations [50].On the other hand, we design a new pretext paradigm to directly train on robot image pairs with textual instructions via asymmetric masking [34,36].</p>
<p>Method</p>
<p>Given a visual observation o s and associated language instruction l s→f , our goal is to learn a policy that predicts a robot action a s→f that leads to a goal observation o f .We formulate this problem into two stages: 1) Visual-action representation learning: We first train a model f θ that outputs the predicted goal image, ôf , from o s and l s→f .This self-supervised task encourages the model to align visual and textual modalities, capturing how the instruction transforms the scene.2) Robot action prediction: We then fine-tune the model by attaching an action prediction head that maps the learned representation to a robot action âs→f .This action may take various forms depending on the task setting, e.g., âs→f ∈ SE(2) for tabletop manipulation or âs→f ∈ R 9 for joint-angle visuomotor control.</p>
<p>Learning visual-action representations</p>
<p>We propose a self-supervised approach of asymmetric masking to learn visual-action representations using the pretext task, which aims to reconstruct a partially masked goal image given o s and l s→f .Fig. 2 shows LaVA-Man's architecture and the proposed pretext task, goal image prediction.</p>
<p>Vision encoder.Let o s and o f be two images that capture the scene before and after the robot manipulation.Both images are firstly divided into N non-overlapping patches, where some patches in o f are randomly masked, transforming o f into õf .We design a pretext task that reconstructs o f based on o s , õf , and a text embedding e s→f obtained from l s→f : We use a fixed backbone while adapting different output heads for pretext and downstream robot tasks.We use a Siamese ViT encoder with asymmetric masking applied to the goal image only.Visual features from the input are first fused with text features and then integrated with those extracted from the masked goal image in the decoder.During inference, the goal image is fully masked as it is unknown.Additional output heads can be incorporated to support a wider range of downstream tasks.KEY -CA: cross-attention.
h s→f = Φ(o s , õf , e s→f ), ôf = Ψ p (h s→f ).(1)
Here Φ consists of a siamese Vision Transformer (ViT) encoder that encodes the patches in o s and o f into the features v s , v f and a lightweight decoder that decodes v s , v f , and e s→f to a feature vector h s→f .Ψ p is the goal-image prediction head that outputs the reconstruction ôf given h s→f .</p>
<p>We use the CLIP text encoder to encode the text embeddings e s→f from l s→f .</p>
<p>Visual-textual fusion.Inspired by GLIP [51], we perform a multi-stage cross-attention to fuse information from different modalities.First, we use text-to-image and image-to-text attention: cross attn(v s , e s→f ) and cross attn(e s→f , v s ), to fuse the initial visual state and language.Then, v f queries this fused feature via cross attn(v s , v f ) to produce h s→f .This enables the model to condition on both the initial image and the language instruction while grounding the goal image.</p>
<p>Goal-image prediction head.The per-patch feature h s→f is passed to a lightweight MLP head Ψ p to generate the per-pixel RGB values for each patch in the predicted goal image ôf .We supervise the model using an L 2 loss between ôf and the ground-truth o f .</p>
<p>Robot action prediction</p>
<p>Following the previous stage, we fine-tune our model for downstream robot manipulation tasks with an additional lightweight head Ψ a that predicts the robot action a s→f :
a s→f = Ψ a (h s→f , o s ).(2)
At test time, the model does not have o f and õf .However, our pretext task is designed to learn from õf with a high mask ratio, and hence our model can still predict h s→f as in Eq. 1 while using fully masked õf .We therefore include ôf as additional input to the action head Ψ a and rewrite Eq. 2 as:
a s→f = Ψ a (h s→f , o s , ôf ).(3)
For tabletop manipulation tasks, we define the output action as a s→f = (T s , T f ), where T s , T f ∈ SE(2) denote the pick and place poses of the end-effector.The model predicts an affordance map as an intermediate representation following [1,8], from which T s and T f are extracted via a softmax operation.For the task of predicting joint angles on specific robot arms, we drop the decoder as [7,42] and represent each action as a 9-DoF vector a s→f ∈ R 9 , comprising seven joint angles and two indicators for grasp status.</p>
<p>Dataset</p>
<p>Num of classes</p>
<p>Num of instances</p>
<p>Inter-class variation</p>
<p>Intra-class variation</p>
<p>Real scanned objects
RLBench † [52] 100 100 ✔ ✘ ✔ LIBERO † [53] 100 100 ✔ ✘ ✔ Ravens [1] 52 52 ✔ ✘ ✔ VIMA [9] 20 1800 * ✔ ✔ ✘ OOPP (ours)
180 3200 ✔ ✔ ✔ † RLBench and LIBERO focus on complex manipulation actions rather than diverse object interactions.We approximate the object number by the number of tasks as there is little intra-class object variation.</p>
<ul>
<li>VIMA provides 90 different textures per object.We treat each textured variant as a distinct instance here.Data annotation.OOPP is built upon the previous benchmarks [8,9] in the Pybullet Gym environment [11].We adopt the objects from the OmniObject3D dataset [10], filtering out the objects unsuitable for tabletop manipulation by retaining only objects with diameters between 4cm and 40cm.During simulation, we allow the robot to perform automatic manipulation with a pre-defined placeholder for collecting action annotations.These placeholders are then replaced with real objects.</li>
</ul>
<p>To ensure spatial diversity and prevent overcrowding, we adopt a KDTree-based spatial partitioning strategy inspired by [1], subdividing scenes into feasible regions for object placement.Based on the rich language description for each object provided by the original OmniObjects3D dataset [10], we generate diverse language instructions that are beyond just using object names, offering more natural and varied descriptions of object appearances as in Fig. 3.</p>
<p>Evaluation.To evaluate both intra-and inter-class generalisation, we partition the dataset into three mutually exclusive subsets.We use 160 object classes for training.For intra-class generalisation, we hold out a subset of instances from 20 categories in the training set for testing.For inter-class generalisation, we reserve 20 object categories that are entirely unseen during training.Unseen classes are sampled from four high-level semantic groups-Food, Daily-use &amp; Tools, Entertainment, and Others-with each group contributing 3-8 classes.We define two manipulation tasks: 1) packingobjects-group, where identical objects are packed simultaneously, and 2) packing-objects-sequence, where different objects are packed sequentially.Together, these variants yield six different tasks.</p>
<p>Please refer to the supplementary material for more details.</p>
<p>Experiments</p>
<p>Setup</p>
<p>Pre-training datasets.For the pretext task, we train our model using a total of 120k samples from our synthetic OOPP dataset and real-world robot video episodes from Bridge [12] and DROID [13].For all robot video episodes, we extract only the first and last frames, paired with language instructions.The pre-training phase takes 24 hours with 3×A100 GPUs.</p>
<p>Baselines.We compare our method against two types of methods: 1) Foundation model-based methods such as CLIPort [1], which leverage web-scale vision-language models for pick-and-place Table 1: Results on the Ravens benchmark [1].We report the results of multi-task performance trained on 1,000 demonstrations.tasks; and 2) Self/weakly supervised methods, including Voltron [7] and MPI [42], which learns representations from human/robot manipulation episodes for grounding language-guided manipulation.Since MPI pre-training also relies on supervision from object bounding boxes, we remove the detection head and make the pre-training in a fully self-supervised manner, denoted as MPI ‡ .Unless specified, we re-train every baseline using our data for fair comparison.</p>
<p>Downstream tasks and benchmarks.We evaluate our model on various downstream tasks, including both simulation and real-world environments (see supplementary for details):</p>
<p>• Ravens [1]: This dataset provides ten different tabletop rearrangement tasks.For fair comparison, we re-train all pre-training methods and fine-tune them using the same set of demonstrations.• OOPP: We also evaluate our models on our proposed dataset with diverse object classes and instances for intra-and inter-class evaluation.The training procedure follows that of Ravens.• Referring expression grounding [7]: This benchmark evaluates target object localisation in cluttered scenes based on language instructions, serving as a prerequisite for visuomotor control.• Franka Kitchen [54]: This popular benchmark provides comparisons with other state-of-the-art methods on five visuomotor control tasks in a simulated kitchen environment.• Real-robot experiments: We further evaluated our model on ten different manipulation tasks using UR5 robot arms to evaluate real-world generalisation.We capture the current observation from the top-down view and generate the affordance map for action execution accordingly.</p>
<p>Evaluation</p>
<p>In Ravens and OOPP, the model outputs SE(2) tabletop actions.In Tab. 1, our method outperforms other pre-training-based models and even the method designed for pick-and-place.Our method shows strong capability in interpreting actions with real-scanned objects, as demonstrated in the seen and unseen categories in Tab. 1 and all intra-and inter-class generalisation tasks in Tab. 2. Our model shows limited performance in coloured geometries, possibly because they are less common in real-world scenarios in our pre-training data.</p>
<p>Table 3: Results on Referring Expression Grounding [7] and Franka Kitchen [54].Our method outperforms self-supervised methods and is comparable to leading supervised methods.The results of existing methods are from [42] and [7].Minimum, Medium, Maximum in (a) denote the scenarios categorised according to the level of clutter.</p>
<p>(a) Referring Expression Grounding Method Minimum Medium Maximum Total Supervised pre-training methods MPI [42] 0.94 0.98 0.95 0.96 SUGAR [43] 0.98 0.97 0.96 0.97 Self-supervised pre-training methods R-R3M [45] 0.64 0.68 0.55 0.63 MVP [40] 0  In Franka Kitchen, as in Tab. 3 (b), our method achieves state-of-the-art results even compared with supervised pre-training methods [42,43].Unlike Ravens and OOPP, the model in Franka Kitchen is required to predict the next timestep's joint angles of the robot arm.We hence employ the encoder and fusion module in our model as a frozen backbone and discard the decoder, consistent with other compared baselines.</p>
<p>Referring expression grounding results in Tab. 3 (a), demonstrate that our method achieves better or comparable results, even compared with supervised methods [42,43].This validates that the goal-image prediction pretext task can learn the association between the target object and the corresponding language instructions, resulting in accurate object localisation.</p>
<p>In real robot experiments, we evaluate our model on a real robot across ten distinct tasks, as shown in Fig. 4.Each task contains 5 different scenarios, and we calculate the overall success rates.The performance is consistent with the results observed in the simulation.Notably, it demonstrated strong generalisation to unseen colours, unseen objects, and even previously unseen tasks.In Fig. 4 we report both perceptual score and physical score, to account for the case where the model correctly outputs the affordance but fails during physical execution due to inaccurate control and object movements, showing the perception-to-physical gap in real-world scenarios (see Sec. 7 for details).We analyse the effect of pre-training strategies and masking ratios on the Franka Kitchen benchmark and Ravens benchmark, respectively.</p>
<p>Analysis</p>
<p>In Fig. 6 (a), we present a set of ablation studies on the Franka Kitchen benchmark to evaluate the contribution of several key components of our approach.We evaluate the effect of removing the fusion module, which injects the language embedding directly into the decoder via cross-attention.</p>
<p>The suboptimal results are consistent with other recent works [57,51], showing the importance of integrating visual and textual modalities before decoding.We also examine the impact of training with the proposed OOPP dataset, which introduces a wide variety of object classes.Results show that the downstream performance can be significantly boosted, indicating the model benefits from broader object priors in the OOPP dataset to learn generalisable visual-action representations.</p>
<p>Finally, we evaluate the effect of the asymmetric masking strategy and the optimal masking ratio.Without asymmetric masking, the model may not be able to fully capture the underlying causality in language-guided manipulation, leading to reduced accuracy in downstream tasks, as shown in Fig. 6 (a).In Fig. 6 (b), we further study the impact of different masking ratios by training models under four commonly used settings [33,36,58].A 95% masking ratio performs the best.Lower ratios leak too much information, making learning less effective, while a 100% ratio, where prediction relies entirely on input, leads to convergence issues due to the future state's ambiguity.</p>
<p>Conclusion</p>
<p>We presented a visual-action representation learning framework for robot manipulation that leverages goal-image prediction as a pretext task to capture the underlying causality in language-guided robot manipulation.To facilitate learning from diverse object priors, we also proposed the OOPP dataset, which provides a rich collection of tabletop manipulation episodes.Through extensive evaluations in both simulated and real-world settings, our method demonstrates superior performance over existing baselines, exhibiting strong generalisation and effective knowledge transfer across tasks.Future work includes scaling training to diverse video datasets to learn a robust and generalisable representation and extend our learned representations for more complex robotic tasks.</p>
<p>Limitations</p>
<p>Manipulating precision.Our model exhibits limited performance when dealing with small objects or those with fine-grained structures, such as in Towers-of-Hanoi and Stack-Block-Pyramid (Tab.1).This is attributed to the use of ViTs, which process images as patches and may overlook fine details that are important for accurate manipulation, as shown in Fig 7 .Recent methods that enhance the spatial resolution by learning to upsample ViT features, such as DPT [59] or FeatUp [60], could potentially mitigate this issue and are promising directions for future exploration.</p>
<p>Pick the yellow ring to the middle Pseudo affordance.On the benchmark of Ravens [1] and OOPP, our method predicts a pseudoaffordance map where the pixel with the highest value is treated as the manipulation point (See Fig. 7).While this approach simplifies fine-tuning, it can be problematic for objects with complex geometry.In such cases, the manipulation can fail due to subtle physical interactions even if the predicted pick-and-place poses are geometrically correct.This leads to a discrepancy between the perceptual score and the actual physical success rate, as illustrated in Fig. 4. Incorporating selfsupervised learning that accounts for human or robot interaction dynamics could help mitigate this issue by enabling the model to learn priors over common manipulable regions.</p>
<p>3D awareness.Our model lacks explicit 3D understanding and relies solely on 2D visual cues.As a result, it can struggle to distinguish between objects with similar textures or appearances but differing 3D shapes or structures.Incorporating self-supervised learning methods that promote geometric awareness may help the model learn 3D priors and improve robustness in such cases.</p>
<p>Articulated objects.In real-world scenarios, some objects are composed of multiple rigid parts and interactable.However, our visual pre-training primarily focuses on rigid object manipulation and does not explicitly model any dynamics or interactions.Including articulated objects with several new tasks in our OOPP dataset can potentially help capture these complex interactions.</p>
<p>Blurry prediction results.MAE-based methods are known to produce blurry reconstructions lacking high-frequency details [34,36,56].Our goal image prediction also suffers from the blurry issue, as shown in Fig 1 .However, our objective is not to generate high-quality images, but rather to use goal image prediction as a pretext task for learning semantic representations.In addition, blurriness reflects the missing of high-frequency components in the image, which often correspond to perceptual details rather than semantic contents.In fact, despite the perceptual blur, the preserved semantic structure in our predictions demonstrates that the model captures semantic information effectively.The lack of perceptual fidelity may limit interpretability for humans, and we consider this as future work.</p>
<p>Acknowledgement</p>
<p>We chose 8 language-conditioned tasks for the experiment, as shown in 8, including Packing seen or unseen Google objects sequence, Packing seen or unseen Google objects group, Put block in the bowl, Stack blocking pyramid, Towers of Hanoi, Packing boxes pairs, Assembling kits, and Separating piles.Details of each task, including the train and test split of objects, and the language instruction template, can be referred to [1].Note that we did not split the tasks according to seen or unseen colours, as all the methods have already "seen" all the colours in their pre-train models or the unsupervised pertaining phase.Therefore, we combine both "seen" and "unseen" splits of colours into a single task and the scores just reflect all model's perception ability on all the colours.Evaluation details.We evaluated the capability of the proposed methods on multi-task experiments in the benchmark.Specifically, we trained the model using 1,000 demonstrations drawn from all task categories and assessed performance on another 100 test demonstrations per task.Since prior pre-training methods [42,7] have not been evaluated on this benchmark, we reimplemented their models using the original codebases and pre-trained them on the same pre-training data as our approach.When adapting to the downstream tasks, these pre-training methods were also equipped with the affordance head described in Section 2. Subsequently, all baselines, including the Pickand-Place baselines, were fully fine-tuned on the same set of downstream demonstrations, following the evaluation protocol outlined in [1], to ensure a fair comparison.</p>
<p>Franka Kitchen</p>
<p>Overview.Franka Kitchen [54] is a well-established benchmark for evaluating the efficacy of visual representations in facilitating the learning of visuomotor control policies from limited demonstrations.This benchmark comprises five distinct visuomotor control tasks, as shown in the Fig. 9, each captured from two camera viewpoints.</p>
<p>Evaluation details.We use the action head described in Sec. 2 for predicting joint velocities.As prior works [7,42,45,40,5], which leverage supervised or unsupervised pre-training for robot manipulation, commonly adopt this benchmark, we directly report the results stated in their original papers and compare our method against these approaches.Following the evaluation protocols widely adopted in these works, we trained the action head with the backbone frozen using 25 demonstrations, and report average success rates across five tasks, two viewpoints, and three random seeds.era for our real-world experiments.We capture the top-down RGB observation which covers the workspace of 60 cm × 30 cm, and the image from the camera is resized to 320 × 160 pixels.</p>
<p>Task details.Here we show the language template, variable factors and the success condition for each real robot task in Tab. 4. Images for each task could be referred in the main paper.</p>
<p>Additional details</p>
<p>Goal-image prediction.We provide more qualitative examples for the goal-image prediction and the results during the training of masked auto-encoders, as shown in Fig. 12 and Fig. 13.In Fig. 12 we show examples of the same input image but with different language instructions.The results show that our model can effectively predict different goal states given different language-based instructions and the initial observation.Namely, these results show that our model can interpret the input instructions, factorize different objects that need to be manipulated, and further understand the spatial location or direction in the scene.This indicates the learned visual-action representations after the self-supervised learning with the pretext task effectively associate visual states with action.In Fig. 13, we present example results in our pre-training phase.Our predicted goal images are blurry as in other MAE-based methods [56,36,34].These results demonstrate that our approach successfully learns to predict the goal image.</p>
<p>Dataset details.We build our Omni-Object Pick-and-Place (OOPP) dataset upon the previous benchmarsk [1,9] in the PyBullet Gym enviornment [11].We manually selected 180 real-scanned object classes from the OmniObject3D dataset [10], focusing on those suitable for robotic manipulation, resulting in a total of 3,200 distinct instances.For each object, we reduced the mesh resolution to 20K faces to enable efficient rendering in simulation.Additionally, we filtered out objects that were either too large or too small i.e., with dimensions less than 4cm or greater than 40cm).Our dataset includes full robot manipulation episodes, comprising annotated robot actions, language instructions, and simulation-generated rewards.We use 160 object classes for training.For evaluating intra-class generalisation, we hold out a subset of instances from 20 categories included in the training set.For inter-class generalisation, we reserve 20 object categories that are entirely unseen during</p>
<p>Figure 2 :
2
Figure2: Network structure.We use a fixed backbone while adapting different output heads for pretext and downstream robot tasks.We use a Siamese ViT encoder with asymmetric masking applied to the goal image only.Visual features from the input are first fused with text features and then integrated with those extracted from the masked goal image in the decoder.During inference, the goal image is fully masked as it is unknown.Additional output heads can be incorporated to support a wider range of downstream tasks.KEY -CA: cross-attention.</p>
<p>Figure 3 :
3
Figure 3: We show (a) Quantitative statistics of the proposed OOPP dataset compared to several existing datasets.(b) Visualised examples of manipulation sequences in OOPP datasets.The first row shows objects seen by the models and the second row shows unseen objects.</p>
<p>Figure 4 :
4
Figure 4: Real-robot experiments.We report the perceptual score and physical score, which indicate the success rates in affordance detection and physical robotic tasks, respectively.The tasks of Opening drawer and Pushing piles are not included in the training demonstrations.</p>
<p>Figure 5 :
5
Figure 5: Qualitative examples from real-robot manipulation.We show qualitative examples in our real-robot environment.Our goal-image predictions are blurry as in other MAE-based methods [56, 36, 34], but still show plausible object placement and the results are aligned with the input text instructions, which leads to accurate manipulation.(Only affordances for translation are shown)</p>
<p>Figure 6 :
6
Figure6: Ablation analysis.We analyse the effect of pre-training strategies and masking ratios on the Franka Kitchen benchmark and Ravens benchmark, respectively.</p>
<p>Figure 7 :
7
Figure 7: Visualised affordance of Towers-of-Hanoi task from the top-down view in Ravens.</p>
<p>Figure 8 :
8
Figure 8: Examples of eight robot manipulation tasks in the simulator.The language instructions are on the top of each image and the final states are shown in the green box.</p>
<p>Figure 11 :
11
Figure 11: Real robot experiment.The figure shows our physical robot setup along with both seen and unseen objects.Seen and unseen colored blocks are also included.</p>
<p>Figure 14 :
14
Figure 14: Examples of objects from different classes.</p>
<p>Figure 15 :
15
Figure 15: Examples of intra-class variance.</p>
<p>indicates that the model was tested using checkpoints provided by the authors, while others are trained on the same downstream data.MPI ‡ is our reimplementation of MPI, trained in a self-supervised manner.KEY -obj: objects, seq: sequence, grp: group.
Seen objectsUnseen objectsColoured geometriesTasksPackingPackingPackingPackingPut blockStack blockTowersPackingAssemblingSeparatingAverageobj-seqobj-grpobj-seqobj-grpin bowlpyramidof hanoiboxes pairskitspilesPick-and-place methodsTransporter [8]0.490.590.250.450.230.020.10.430.200.440.32CLIP only [5]0.570.720.490.630.430.110.20.50.360.470.45RN50-BERT [1] 0.460.640.410.590.450.020.210.470.270.50.40CLIPort  *  [1]0.800.890.550.740.590.380.690.800.600.710.68CLIPort [1]0.780.830.570.770.840.690.820.830.540.580.73Pre-training methodsNo pre-training0.590.720.400.560.330.220.180.640.550.460.47Voltron [7]0.580.70.430.570.800.360.240.700.600.430.54MPI  ‡ [42]0.560.750.410.660.670.220.420.420.460.460.50Ours0.830.840.770.830.980.570.670.940.750.930.81
*</p>
<p>Table 2 :
2
Results on the OOPP dataset.We report the results trained on 1,000 demonstrations and tested on 100 demonstrations in the test set.
MethodPacking obj-seqPacking obj-intra-seqPacking obj-inter-seqPacking obj-grpPacking obj-intra-grpPacking obj-inter-grpAverageCLIPort [1]0.620.600.480.650.570.6058.7Voltron [7]0.740.680.540.800.710.7269.8MPI  ‡ [42]0.690.590.580.730.650.6765.1Ours0.840.740.730.840.860.7779.6
MPI ‡ is our reimplementation of MPI, trained in a self-supervised manner.KEY -obj: objects, seq: sequence, intra: intra-class, inter: inter-class, grp: group.</p>
<p>Table 4 :
4
Real robot tasks settings.We present the language template, variable factors and the success condition for each real robot task Stacking blocks Stack the {color} block on the {color} blocks Block color and position (pick/place) Correct block stacked on target block Folding cloth Fold the cloth from {direction} to the {direction} Cloth color, initial orientation Grasp and fold directions match the template Packing objects Pick the {object} into the {object} Object type, distractor objects Target object placed into correct container Opening drawer Pull out the drawer Drawer position Drawer fully opened Press button Press the {color} button Button color, button location Correct button touched Aligning rope Align the rope from {direction} to the {direction} Rope position, direction variation Rope aligned from start to target direction Packing blocks Put the {color} block into the {color} bowl Block color, bowl color Correct block placed into matching bowl Pushing piles Push the pile of {objects} to the {color} area Object color, target area color Pile reaches target area within five attempts
Task nameLanguage templateVariable factorsSuccess condition6-DoFRobot armRGB-DCameraGraspReal robot setupSeen objects and distractorsUnseen objects</p>
<p>Table 5 :
5
Semantic group division of seen and unseen classesWe divided all objects into four semantic groups.We selected unseen classes from each group in proportion to the number of object classes it contains, ensuring an even distribution of unseen categories.corn, strawberry, anise, pizza, longan, loquat, chocolate, brussels sprout, haw thorn, green bean cake, cucumber, litchi, cake, dumpling, mooncake, rice cake, puff, water chestnut, mushroom, egg, broccoli, pastry, egg tart, kiwifruit, fig, cheese, chili, tomato, lemon, oyster, steamed bun, carrot, mangosteen, bread, ginger, waffle, bun, peach, apple, pear, potato, zongzi, pomegranate, sweet potato, onion, banana, chicken leg, sausage, coconut, broccolini, hami melon, durian, asparagus, walnut, mango, loquat, bucket noodle Daily-use thimble, beauty blender, battery, candle, calculator, plug, watch, nipple, power strip, bottle, medicine bottle, tissue, belt, dish, flash light, canned beverage, fork, cup glasses case, bowl, tape measure, speaker, laundry detergent, teapot glasses, wallet, insole, bumbag, fan, knife, umbrella, kettle, light, picnic basket, hammer, shoe, hat, laptop, vase, ornaments, spanner, book, soap, mouse, scissors, teapot, shampoo, toothpaste Entertainment toy boat, toy plant, toy car, toy bus, toy plane, timer, whistle, table tennis bat, toy motorcycle, drum, remote control, toy animals, garage kit, china, chess, Chinese chess, rubik cube, dinosaur, doll toy bus, teddy bear, toy animals
Semantic Group Seen Classes
hairpin, lotus root, house (model), plant, dumbbell, package bamboo shoots, brush, flute, ornaments, conch, magnet, box flower pot, red wine glass</p>
<p>The Omni-Object Pick-and-Place dataset Overview. We introduce the OOPP dataset, a tabletop simulation benchmark consisting of 3,200 unique real-scanned objects across 180 distinct categories. As shown in Fig.3(a), existing tabletop simulation datasets such as Ravens[1] and VIMA[9] typically exhibit limited object diversity
This work was supported by the Royal Society Research Grant RGS/R2/242051, and utilised the Sulis Tier 2 HPC under the UK EPSRC Grant EP/T022108/1 and the HPC Midlands+ consortium.Appendix 1 IntroductionWe provide additional material that supports our paper.• We invite readers to watch the demo video about the proposed pretext task (goal-image prediction), Omni-Object Pick-and-Place (OOPP) dataset, and real and simulated robot manipulation.• In Sec. 2, we provide more details about our method, including our backbone and different output heads utilised in different downstream tasks.• In Sec. 3, we describe more details of each downstream task, including the real-robot experiment.• In Sec. 4, we provide additional examples of the proposed pretext task, goal-image prediction, and our OOPP dataset.Method detailsBackbone architecture.We adopt ViT-Base[61]with a patch size of 16 as our backbone.The original Vit-Base architecture consists of 12 attention blocks with an embedding dimension of 768, and its the decoder typically comprises 8 attention blocks with an embedding dimension of 512.For a fair comparison with other methods using ViT-Base as a backbone, we include 6 attention blocks and 6 bi-directional attention blocks in our encoder.In the pre-training stage, our goal-image prediction head is a one-layer fully connected network.Affordance head.Our affordance head for generating SE(2) robot actions (the 2D location and 1D rotation) for tabletop manipulation tasks (Ravens, OOPP, and our real robot experiments) consists of 4 convolutional layers with skip connection to convert the output from the transformer backbone to the affordance map.To generate a rotation angle, we expand a single affordance map into 36 instances, where each instance represents a 10-degree step.We apply the softmax function to the expanded affordance map, which identifies the position and corresponding rotation angle.Since other pre-training methods have not been previously evaluated on this benchmark, we reimplemented them and used a unified action head across all pre-training baselines.For other Pick-and-Place methods, we retain their original action heads which are structurally similar but typically deeper due to their use of ResNet backbones.In addition, since our pre-training methods are designed to predict the goal image, we found that concatenating the goal image into the convolutional layers facilitates improved affordance generation, as the two tasks are closely correlated.Action head and detection head.Following the baseline established by Voltron[7]and MPI[42], we adopt the same shallow MLP policy network to predict joint velocities of R 9 (7 degree of freedom and two for grasp status) for robot actions in the Franka Kitchen benchmark, and to regress bounding boxes in the Referring Expression Grounding benchmark.We use features after the fusion module for two main reasons: First, for fair comparison: previous methods, including R3M[45], MVP[40]and Voltron[7], only evaluate frozen representations dropping the decoder part.Since we directly report their results on these benchmarks, we adopt a consistent setting.Second, based on task requirements, both benchmarks rely on understanding the current state or the next state, while features after the decoder in our model represent the goal image and correspond to the final state.Therefore, using features before the decoder is more suitable for these two benchmarks.3 Benchmark detailsRavensOverview.Ravens[8]is a simulated benchmark to evaluate tabletop pick-and-place robot manipulation tasks.We use PyBullet OpenAI Gym[11]based on the configuration described in CLIPort[1].Open the microwave Turn on the light Slide the right door Turn the stove top knob Open the left door Figure9: Tasks in the Franka Kitchen benchmark.Referring Expression GroundingOverview.The goal of this task is to predict a bounding box of an object in a cluttered scene based on the nature language expression.This task offers the evaluation of language-conditioned scene understanding and object recognising ability, which serves as an important prerequisite for language-based robot manipulation.The benchmark is based on OCID-Ref Dataset[62], which provides representatives scenes in robotises settings.The benchmark also provides splits based on the clutter level.Evaluation details.We use a shallow MLP as detection head to regress the bounding box directly.We use the evaluation codebase provided by[7].Similar to Franka Kitche, we report the results stated in the paper for each baseline[7,42,45,40,43].The evaluation metrics are the average precision at 0.25 IoU under each clutter level.Real robot experimentsOverview.We validate the applicability of our method in real-world scenarios.We validate our model on 10 manipulation tasks: Stacking blocks, Folding cloth, Packing objects, Opening drawer, Pressing button, Aligning rope, Packing blocks, and Pushing piles.Each task contains 5 different scenarios that differ in either objects or locations.We manually collected 200 training demos that contain robot image pairs, language descriptions, and annotations for real-world fine-tuning.Five colored blocks and five unseen objects were excluded from the training demonstrations, and no demonstrations from the Opening drawer or Pushing piles tasks were included, although similar tasks are present in the images used in the self-supervised pre-training phase.This design aims to evaluate whether the model can effectively generalised from the self-supervised pre-training, rather than relying on downstream demonstrations.Evaluation details.Fig.11shows our real-robot environment and objects.We trained both our model and CLIPort[1]on our manually collected training demos.We utilise a 6-degree-of-freedom (6-DoF) UR5 robotic arm, Robotiq 2F-85 two-finger gripper, and an Intel RealSense RGB-D cam-The timer on the rear left of the pudding_boxThe blue shampoo on the rear left of the cylinder food_canThe cuboid red cereal_box on the rear left of the sphere red apple.5.
Cliport: What and where pathways for robotic manipulation. M Shridhar, L Manuelli, D Fox, Proceedings of the Conference on Robot Learning. the Conference on Robot Learning2022</p>
<p>Programmatically grounded, compositionally generalizable robotic manipulation. R Wang, J Mao, J Hsu, H Zhao, J Wu, Y Gao, Proceedings of the International Conference on Learning Representations. the International Conference on Learning Representations2023</p>
<p>Perceiver-actor: A multi-task transformer for robotic manipulation. M Shridhar, L Manuelli, D Fox, Proceedings of the Conference on Robot Learning. the Conference on Robot Learning2023</p>
<p>Open-vocabulary pick and place via patch-level semantic maps. M Jia, H Huang, Z Zhang, C Wang, L Zhao, D Wang, J X Liu, R Walters, R Platt, S Tellex, IEEE Robotics and Automation Letters. 2024</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, G Krueger, I Sutskever, Proceedings of the International Conference on Machine Learning. the International Conference on Machine Learning2021</p>
<p>Video diffusion models. J Ho, T Salimans, A Gritsenko, W Chan, M Norouzi, D J Fleet, Proceedings of the Advances in Neural Information Processing Systems. the Advances in Neural Information Processing Systems2022</p>
<p>Language-driven representation learning for robotics. S Karamcheti, S Nair, A Chen, T Kollar, C Finn, D Sadigh, P Liang, Proceedings of Robotics: Science and Systems. Robotics: Science and Systems2023</p>
<p>Transporter networks: Rearranging the visual world for robotic manipulation. A Zeng, P Florence, J Tompson, S Welker, J Chien, M Attarian, T Armstrong, I Krasin, D Duong, V Sindhwani, J Lee, Proceedings of the Conference on Robot Learning. the Conference on Robot Learning2021</p>
<p>VIMA: General robot manipulation with multimodal prompts. Y Jiang, A Gupta, Z Zhang, G Wang, Y Dou, Y Chen, L Fei-Fei, A Anandkumar, Y Zhu, L Fan, Proceedings of the International Conference on Machine Learning. the International Conference on Machine Learning2023</p>
<p>OmniObject3D: Large-vocabulary 3D object dataset for realistic perception, reconstruction and generation. T Wu, J Zhang, X Fu, Y Wang, L P Jiawei Ren, W Wu, L Yang, J Wang, C Qian, D Lin, Z Liu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Pybullet, a python module for physics simulation for games, robotics and machine learning. E Coumans, Y Bai, </p>
<p>BridgeData V2: A dataset for robot learning at scale. H Walke, K Black, A Lee, M J Kim, M Du, C Zheng, T Zhao, P Hansen-Estruch, Q Vuong, A He, V Myers, K Fang, C Finn, S Levine, Proceedings of the Conference on Robot Learning. the Conference on Robot Learning2023</p>
<p>DROID: A large-scale in-the-wild robot manipulation dataset. A Khazatsky, K Pertsch, S Nair, A Balakrishna, S Dasari, S Karamcheti, S Nasiriany, M K Srirama, L Y Chen, K Ellis, Proceedings of Robotics: Science and Systems. Robotics: Science and Systems2024</p>
<p>PoseCNN: A convolutional neural network for 6d object pose estimation in cluttered scenes. Y Xiang, T Schmidt, V Narayanan, D Fox, Proceedings of Robotics: Science and Systems. Robotics: Science and Systems2017</p>
<p>Single image 3d object detection and pose estimation for grasping. M Zhu, K G Derpanis, Y Yang, S Brahmbhatt, M Zhang, C Phillips, M Lecce, K Daniilidis, Proceedings of the IEEE International Conference on Robotics and Automation. the IEEE International Conference on Robotics and Automation2014</p>
<p>Multi-view selfsupervised deep learning for 6d pose estimation in the amazon picking challenge. A Zeng, K.-T Yu, S Song, D Suo, E Walker, A Rodriguez, J Xiao, Proceedings of the IEEE International Conference on Robotics and Automation. the IEEE International Conference on Robotics and Automation2017</p>
<p>Self-supervised 6d object pose estimation for robot manipulation. X Deng, Y Xiang, A Mousavian, C Eppner, T Bretl, D Fox, Proceedings of the IEEE International Conference on Robotics and Automation. the IEEE International Conference on Robotics and Automation2020</p>
<p>Vision-based robotic grasping from object localization, object pose estimation to grasp estimation for parallel grippers: a review. G Du, K Wang, S Lian, K Zhao, Artificial Intelligence Review. 5432021</p>
<p>Form2fit: Learning shape priors for generalizable assembly from disassembly. K Zakka, A Zeng, J Lee, S Song, Proceedings of the IEEE International Conference on Robotics and Automation. the IEEE International Conference on Robotics and Automation2020</p>
<p>Grasping in the wild: Learning 6dof closedloop grasping from low-cost demonstrations. S Song, A Zeng, J Lee, T Funkhouser, IEEE Robotics and Automation Letters. 532020</p>
<p>Learning to manipulate deformable objects without demonstrations. Y Wu, W Yan, T Kurutach, L Pinto, P Abbeel, Proceedings of Robotics: Science and Systems. Robotics: Science and Systems2020</p>
<p>Transporters with visual foresight for solving unseen rearrangement tasks. H Wu, J Ye, X Meng, C Paxton, G S Chirikjian, Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems. the IEEE/RSJ International Conference on Intelligent Robots and Systems2022</p>
<p>A3vlm: Actionable articulation-aware vision language model. S Huang, H Chang, Y Liu, Y Zhu, H Dong, P Gao, A Boularias, H Li, Proceedings of the Conference on Robot Learning. the Conference on Robot Learning2024</p>
<p>Omnimanip: Towards general robotic manipulation via object-centric interaction primitives as spatial constraints. M Pan, J Zhang, T Wu, Y Zhao, W Gao, H Dong, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2025</p>
<p>Manipllm: Embodied multimodal large language model for object-centric robotic manipulation. X Li, M Zhang, Y Geng, H Geng, Y Long, Y Shen, R Zhang, J Liu, H Dong, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Unsupervised visual representation learning by context prediction. C Doersch, A Gupta, A A Efros, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2015</p>
<p>Colorful image colorization. R Zhang, P Isola, A A Efros, Proceedings of the European Conference on Computer Vision. the European Conference on Computer Vision2016</p>
<p>Unsupervised representation learning by predicting image rotations. S Gidaris, P Singh, N Komodakis, Proceedings of the International Conference on Learning Representations. the International Conference on Learning Representations2018</p>
<p>Context encoders: Feature learning by inpainting. D Pathak, P Krahenbuhl, J Donahue, T Darrell, A A Efros, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2016</p>
<p>Momentum contrast for unsupervised visual representation learning. K He, H Fan, Y Wu, S Xie, R Girshick, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2020</p>
<p>Exploring simple siamese representation learning. X Chen, K He, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2021</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. J D , M.-W C Kenton, L K Toutanova, Proceedings of the 2019 conference of the North American chapter of the Association for Computational Linguistics: human language technologies. the 2019 conference of the North American chapter of the Association for Computational Linguistics: human language technologies2019</p>
<p>Masked autoencoders are scalable vision learners. K He, X Chen, S Xie, Y Li, P Dollár, R Girshick, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Siamese masked autoencoders. A Gupta, J Wu, J Deng, F.-F Li, Proceedings of the Advances in Neural Information Processing Systems. the Advances in Neural Information Processing Systems2023</p>
<p>T Kim, D Han, B Heo, J Park, S Yun, arXiv:2507.06543Token bottleneck: One token to remember dynamics. 2025arXiv preprint</p>
<p>Croco: Self-supervised pre-training for 3d vision tasks by crossview completion. P Weinzaepfel, V Leroy, T Lucas, R Brégier, Y Cabon, V Arora, L Antsfeld, B Chidlovskii, G Csurka, J Revaud, Proceedings of the Advances in Neural Information Processing Systems. the Advances in Neural Information Processing Systems2022</p>
<p>Learning to see before learning to act: Visual pre-training for manipulation. L Yen-Chen, A Zeng, S Song, P Isola, T.-Y Lin, Proceedings of the IEEE International Conference on Robotics and Automation. the IEEE International Conference on Robotics and Automation2020</p>
<p>Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. I Kostrikov, D Yarats, R Fergus, Proceedings of the International Conference on Learning Representations. the International Conference on Learning Representations2020</p>
<p>Reinforcement learning with augmented data. M Laskin, K Lee, A Stooke, L Pinto, P Abbeel, A Srinivas, Proceedings of the Advances in Neural Information Processing Systems. the Advances in Neural Information Processing Systems2020</p>
<p>Real-world robot learning with masked visual pre-training. I Radosavovic, T Xiao, S James, P Abbeel, J Malik, T Darrell, Proceedings of the Conference on Robot Learning. the Conference on Robot Learning2023</p>
<p>Masked visual pre-training for motor control. T Xiao, I Radosavovic, T Darrell, J Malik, arXiv:2203.061732022arXiv preprint</p>
<p>Learning manipulation by predicting interaction. J Zeng, Q Bu, B Wang, W Xia, L Chen, H Dong, H Song, D Wang, D Hu, P Luo, Proceedings of Robotics: Science and Systems. Robotics: Science and Systems2024</p>
<p>SUGAR: Pre-training 3d visual representations for robotics. S Chen, R Garcia, I Laptev, C Schmid, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Vip: Towards universal visual reward and representation via value-implicit pre-training. Y J Ma, S Sodhani, D Jayaraman, O Bastani, V Kumar, A Zhang, Proceedings of the International Conference on Learning Representations. the International Conference on Learning Representations2022</p>
<p>R3m: A universal visual representation for robot manipulation. S Nair, A Rajeswaran, V Kumar, C Finn, A Gupta, Proceedings of the Conference on Robot Learning. the Conference on Robot Learning2023</p>
<p>Robots pre-train robots: Manipulationcentric robotic representation from large-scale robot datasets. G Jiang, Y Sun, T Huang, H Li, Y Liang, H Xu, Proceedings of the International Conference on Learning Representations. the International Conference on Learning Representations2025</p>
<p>Theia: Distilling diverse vision foundation models for robot learning. J Shang, K Schmeckpeper, B B May, M V Minniti, T Kelestemur, D Watkins, L Herlant, Proceedings of the Conference on Robot Learning. the Conference on Robot Learning2025</p>
<p>Zero-shot robotic manipulation with pre-trained image-editing diffusion models. K Black, M Nakamoto, P Atreya, H R Walke, C Finn, A Kumar, S Levine, Proceedings of the International Conference on Learning Representations. the International Conference on Learning Representations2024</p>
<p>3d-mvp: 3d multiview pretraining for robotic manipulation. S Qian, K Mo, V Blukis, D F Fouhey, D Fox, A Goyal, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2025</p>
<p>Masked video distillation: Rethinking masked feature modeling for self-supervised video representation learning. R Wang, D Chen, Z Wu, Y Chen, X Dai, M Liu, L Yuan, Y.-G Jiang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Grounded language-image pre-training. L H Li, P Zhang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>RLBench: The robot learning benchmark &amp; learning environment. S James, Z Ma, D R Arrojo, A J Davison, IEEE Robotics and Automation Letters. 522020</p>
<p>Libero: Benchmarking knowledge transfer for lifelong robot learning. B Liu, Y Zhu, C Gao, Y Feng, Q Liu, Y Zhu, P Stone, Proceedings of the Advances in Neural Information Processing Systems. the Advances in Neural Information Processing Systems2023</p>
<p>Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning. A Gupta, V Kumar, C Lynch, S Levine, K Hausman, Proceedings of the Conference on Robot Learning. the Conference on Robot Learning2020</p>
<p>Deep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2016</p>
<p>VLMAE: Vision-language masked autoencoder. S He, T Guo, T Dai, R Qiao, C Wu, X Shu, B Ren, arXiv:2208.093742022arXiv preprint</p>
<p>Grounding dino: Marrying dino with grounded pre-training for open-set object detection. S Liu, Z Zeng, T Ren, F Li, H Zhang, J Yang, Q Jiang, C Li, J Yang, H Su, Proceedings of the European Conference on Computer Vision. the European Conference on Computer Vision2024</p>
<p>VideoMAE: Masked autoencoders are data-efficient learners for self-supervised video pre-training. Z Tong, Y Song, J Wang, L Wang, Proceedings of the Advances in Neural Information Processing Systems. the Advances in Neural Information Processing Systems2022</p>
<p>Vision transformers for dense prediction. R Ranftl, A Bochkovskiy, V Koltun, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2021</p>
<p>Featup: A modelagnostic framework for features at any resolution. S Fu, M Hamilton, L Brandt, A Feldman, Z Zhang, W T Freeman, Proceedings of the International Conference on Learning Representations. the International Conference on Learning Representations2024</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. A Dosovitskiy, L Beyer, A Kolesnikov, D Weissenborn, X Zhai, T Unterthiner, M Dehghani, M Minderer, G Heigold, S Gelly, Proceedings of the International Conference on Learning Representations. the International Conference on Learning Representations2021</p>
<p>Ocid-ref: A 3d robotic dataset with embodied language for clutter scene grounding. K.-J Wang, Y.-H Liu, H.-T Su, J.-W Wang, Y.-S Wang, W Hsu, W.-C Chen, Association for Computational Linguistics2021</p>            </div>
        </div>

    </div>
</body>
</html>
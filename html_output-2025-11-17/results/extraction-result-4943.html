<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4943 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4943</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4943</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-105.html">extraction-schema-105</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <p><strong>Paper ID:</strong> paper-5437e8adab596d7294124c0e798708e050e25321</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5437e8adab596d7294124c0e798708e050e25321" target="_blank">Least-to-Most Prompting Enables Complex Reasoning in Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> Experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts.</p>
                <p><strong>Paper Abstract:</strong> Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99% using just 14 exemplars, compared to only 16% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4943.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4943.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>L2M_vs_CoT_last-letter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Least-to-most prompting versus Chain-of-Thought prompting on last-letter-concatenation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of least-to-most (decomposition + sequential subproblem solving) against chain-of-thought (single-step stepwise rationale) for the last-letter-concatenation task using GPT-3 models; measures length generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002 (GPT-3 / Codex family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary large language model from the Codex/GPT-3 family used as the base LM in experiments (model name as reported in paper); exact parameter count/size not specified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Least-to-Most prompting (decomposition + sequential solving)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Two-stage prompting: (1) prompt the model to decompose a long list into sequential sublists (subproblems); (2) sequentially prompt the model to solve each subproblem, appending prior subproblem answers so later steps build on earlier outputs. Decomposition produces dependent subquestions and a multi-step solution chain.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Last-letter-concatenation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a list of words, output the concatenation of the last letters of each word; experiment measures generalization to lists longer than exemplars in prompts (lengths L=4,6,8,10,12).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Least-to-Most accuracy (code-davinci-002): L=4: 94.0%, L=6: 88.4%, L=8: 83.0%, L=10: 76.4%, L=12: 74.0%</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Chain-of-Thought accuracy (code-davinci-002, same base examples): L=4: 84.2%, L=6: 69.2%, L=8: 50.2%, L=10: 39.8%, L=12: 31.8% (improved somewhat with more independent exemplars but still substantially lower than Least-to-Most for long lists: e.g., 8-shot CoT L=12 = 38.4%)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Least-to-most prompting substantially improves length generalization on this symbolic/algorithmic task versus chain-of-thought prompting; L2M maintains much higher accuracy as list length increases, demonstrating better easy-to-hard generalization and data-efficiency for iterative/recursive structure.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Chain-of-thought prompting benefits from additional and independent exemplars (performance increases with more shots), narrowing the gap but not closing it; some L2M failures are due to concatenation/copy errors rather than decomposition or last-letter extraction, so L2M is not perfect even when decomposition is trivial.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Least-to-Most Prompting Enables Complex Reasoning in Large Language Models', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4943.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4943.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>L2M_vs_CoT_SCAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Least-to-most prompting versus Chain-of-Thought prompting on SCAN (compositional generalization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of least-to-most prompting (explicit decomposition of commands + mapping) against chain-of-thought and standard few-shot prompting on the SCAN benchmark; evaluates compositional/length generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002 (GPT-3 / Codex family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary large language model from the Codex/GPT-3 family used as the base LM in experiments; exact size not specified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Least-to-Most prompting (decomposition + mapping)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Decompose a long natural-language command into a list of short commands (decomposition prompt) then sequentially map each short command to action sequences; mapping prompt uses exemplars covering SCAN semantics and uses prior solved subquestions as context for subsequent mappings.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SCAN (length split)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmark mapping natural language navigation-like commands to action sequences; length split requires generalization to longer action sequences at test time than in training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Least-to-Most accuracy (code-davinci-002) on SCAN length split: 99.7%</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Chain-of-Thought (code-davinci-002): 16.2%; Standard few-shot (code-davinci-002): 16.7%; text-davinci-002 and code-davinci-001 also reported and underperform relative to code-davinci-002 with L2M (e.g., text-davinci-002 L2M 76.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Least-to-most prompting enables near-perfect compositional generalization on SCAN using only a handful of exemplars and no finetuning, vastly outperforming chain-of-thought and standard few-shot prompting; L2M's explicit decomposition aligns with SCAN's compositional structure, enabling strong length generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Few (13) failures remain for L2M in code-davinci-002 tests, mainly due to misinterpretations of specific modifiers ('around' with 'twice'/'thrice' or 'after' vs 'and'), indicating decomposition + mapping is very effective but not flawless.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Least-to-Most Prompting Enables Complex Reasoning in Large Language Models', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4943.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4943.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>L2M_vs_CoT_GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Least-to-most prompting versus Chain-of-Thought prompting on GSM8K (math word problems)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of least-to-most prompting (explicit decomposition + sequential solving) against chain-of-thought prompting on GSM8K math problems; analyzes step-count stratified performance to test easy-to-hard generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002 (GPT-3 / Codex family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary large language model from the Codex/GPT-3 family used as the base LM in experiments; exact architecture/size not reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Least-to-Most prompting (single-pass or two-stage decomposition + subproblem solving)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Prompt demonstrates how to break a problem into numbered subquestions and then solves subquestions sequentially (examples show decomposition and the solving steps). In the reported setup L2M sometimes combined decomposition and solving in a single pass for GSM8K.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Grade-school math word problem dataset requiring multi-step arithmetic reasoning, problems vary in number of reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Least-to-Most overall accuracy (code-davinci-002): 62.39% (reported in Table 11). Step-stratified: All: 62.39%; 2 steps: 74.53%; 3 steps: 68.91%; 4 steps: 59.73%; >=5 steps: 45.23%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Chain-of-Thought (code-davinci-002) overall: 60.87%. Step-stratified: All: 60.87%; 2 steps: 76.68%; 3 steps: 67.29%; 4 steps: 59.39%; >=5 steps: 39.07%.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Least-to-most provides a modest overall improvement over chain-of-thought on GSM8K (≈+1.5 percentage points), with the largest relative gains on the hardest problems (>=5 steps: L2M 45.23% vs CoT 39.07%), indicating L2M helps easy-to-hard generalization for deeper multi-step problems. For simpler 2-step problems CoT slightly outperforms L2M (76.68% vs 74.53%).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Chain-of-thought outperforms L2M on 2-step problems; overall improvement of L2M is small and limited to problems requiring many steps. Authors note many GSM8K failures by L2M can be solved if a correct manual decomposition is provided, highlighting that automatic decomposition quality is a limiting factor.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Least-to-Most Prompting Enables Complex Reasoning in Large Language Models', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4943.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4943.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>L2M_vs_CoT_DROP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Least-to-most prompting versus Chain-of-Thought prompting on DROP (numerical reading comprehension subset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of least-to-most prompting (decomposition + solving) against chain-of-thought and other baselines on numerical reasoning questions in the DROP benchmark (split into 'non-football' and 'football' subsets).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002 (GPT-3 / Codex family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary large language model from the Codex/GPT-3 family used as the base LM in experiments; paper does not report parameter counts.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Least-to-Most prompting (decomposition + sequential solving)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Decompose paragraph-level numerical questions into simpler subproblems (e.g., identify quantities, compute sub-results) and sequentially compute answers using previous subresults, implemented with few-shot prompting (two-stage decomposition and solving prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>DROP (numerical subset: Non-football and Football)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Reading comprehension benchmark requiring discrete numerical reasoning over paragraphs; this experiment focuses on numerical problems subset and a football-specific subset.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Least-to-Most accuracy (code-davinci-002): Non-football subset: 82.45%; Football subset: 73.42%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Chain-of-Thought (code-davinci-002): Non-football: 74.77%; Football: 59.56%. Zero-shot and standard prompting also reported (lower).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Least-to-most substantially outperforms chain-of-thought on DROP numerical questions, particularly on the domain (football) subset where decomposition of quantities and event sequences is helpful; many DROP problems are trivially decomposable and thus benefit strongly from explicit decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Authors report that decomposition prompts do not generalize across domains; a decomposition prompt crafted for math problems is not effective for some common-sense reasoning tasks. Also, L2M can fail if decomposition is incorrect — error analysis includes wrong decomposition, wrong solving, and incorrect labels.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Least-to-Most Prompting Enables Complex Reasoning in Large Language Models', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Compositional generalization via neural-symbolic stack machines <em>(Rating: 2)</em></li>
                <li>Neural Logic Machines <em>(Rating: 1)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4943",
    "paper_id": "paper-5437e8adab596d7294124c0e798708e050e25321",
    "extraction_schema_id": "extraction-schema-105",
    "extracted_data": [
        {
            "name_short": "L2M_vs_CoT_last-letter",
            "name_full": "Least-to-most prompting versus Chain-of-Thought prompting on last-letter-concatenation",
            "brief_description": "Comparison of least-to-most (decomposition + sequential subproblem solving) against chain-of-thought (single-step stepwise rationale) for the last-letter-concatenation task using GPT-3 models; measures length generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "code-davinci-002 (GPT-3 / Codex family)",
            "model_description": "Proprietary large language model from the Codex/GPT-3 family used as the base LM in experiments (model name as reported in paper); exact parameter count/size not specified in the paper.",
            "reasoning_method_name": "Least-to-Most prompting (decomposition + sequential solving)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Two-stage prompting: (1) prompt the model to decompose a long list into sequential sublists (subproblems); (2) sequentially prompt the model to solve each subproblem, appending prior subproblem answers so later steps build on earlier outputs. Decomposition produces dependent subquestions and a multi-step solution chain.",
            "task_name": "Last-letter-concatenation",
            "task_description": "Given a list of words, output the concatenation of the last letters of each word; experiment measures generalization to lists longer than exemplars in prompts (lengths L=4,6,8,10,12).",
            "performance": "Least-to-Most accuracy (code-davinci-002): L=4: 94.0%, L=6: 88.4%, L=8: 83.0%, L=10: 76.4%, L=12: 74.0%",
            "comparison_with_other_method": true,
            "performance_other_method": "Chain-of-Thought accuracy (code-davinci-002, same base examples): L=4: 84.2%, L=6: 69.2%, L=8: 50.2%, L=10: 39.8%, L=12: 31.8% (improved somewhat with more independent exemplars but still substantially lower than Least-to-Most for long lists: e.g., 8-shot CoT L=12 = 38.4%)",
            "key_findings": "Least-to-most prompting substantially improves length generalization on this symbolic/algorithmic task versus chain-of-thought prompting; L2M maintains much higher accuracy as list length increases, demonstrating better easy-to-hard generalization and data-efficiency for iterative/recursive structure.",
            "counter_examples_or_negative_results": "Chain-of-thought prompting benefits from additional and independent exemplars (performance increases with more shots), narrowing the gap but not closing it; some L2M failures are due to concatenation/copy errors rather than decomposition or last-letter extraction, so L2M is not perfect even when decomposition is trivial.",
            "uuid": "e4943.0",
            "source_info": {
                "paper_title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "L2M_vs_CoT_SCAN",
            "name_full": "Least-to-most prompting versus Chain-of-Thought prompting on SCAN (compositional generalization)",
            "brief_description": "Comparison of least-to-most prompting (explicit decomposition of commands + mapping) against chain-of-thought and standard few-shot prompting on the SCAN benchmark; evaluates compositional/length generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "code-davinci-002 (GPT-3 / Codex family)",
            "model_description": "Proprietary large language model from the Codex/GPT-3 family used as the base LM in experiments; exact size not specified in the paper.",
            "reasoning_method_name": "Least-to-Most prompting (decomposition + mapping)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Decompose a long natural-language command into a list of short commands (decomposition prompt) then sequentially map each short command to action sequences; mapping prompt uses exemplars covering SCAN semantics and uses prior solved subquestions as context for subsequent mappings.",
            "task_name": "SCAN (length split)",
            "task_description": "Benchmark mapping natural language navigation-like commands to action sequences; length split requires generalization to longer action sequences at test time than in training.",
            "performance": "Least-to-Most accuracy (code-davinci-002) on SCAN length split: 99.7%",
            "comparison_with_other_method": true,
            "performance_other_method": "Chain-of-Thought (code-davinci-002): 16.2%; Standard few-shot (code-davinci-002): 16.7%; text-davinci-002 and code-davinci-001 also reported and underperform relative to code-davinci-002 with L2M (e.g., text-davinci-002 L2M 76.0%).",
            "key_findings": "Least-to-most prompting enables near-perfect compositional generalization on SCAN using only a handful of exemplars and no finetuning, vastly outperforming chain-of-thought and standard few-shot prompting; L2M's explicit decomposition aligns with SCAN's compositional structure, enabling strong length generalization.",
            "counter_examples_or_negative_results": "Few (13) failures remain for L2M in code-davinci-002 tests, mainly due to misinterpretations of specific modifiers ('around' with 'twice'/'thrice' or 'after' vs 'and'), indicating decomposition + mapping is very effective but not flawless.",
            "uuid": "e4943.1",
            "source_info": {
                "paper_title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "L2M_vs_CoT_GSM8K",
            "name_full": "Least-to-most prompting versus Chain-of-Thought prompting on GSM8K (math word problems)",
            "brief_description": "Comparison of least-to-most prompting (explicit decomposition + sequential solving) against chain-of-thought prompting on GSM8K math problems; analyzes step-count stratified performance to test easy-to-hard generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "code-davinci-002 (GPT-3 / Codex family)",
            "model_description": "Proprietary large language model from the Codex/GPT-3 family used as the base LM in experiments; exact architecture/size not reported in the paper.",
            "reasoning_method_name": "Least-to-Most prompting (single-pass or two-stage decomposition + subproblem solving)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Prompt demonstrates how to break a problem into numbered subquestions and then solves subquestions sequentially (examples show decomposition and the solving steps). In the reported setup L2M sometimes combined decomposition and solving in a single pass for GSM8K.",
            "task_name": "GSM8K",
            "task_description": "Grade-school math word problem dataset requiring multi-step arithmetic reasoning, problems vary in number of reasoning steps.",
            "performance": "Least-to-Most overall accuracy (code-davinci-002): 62.39% (reported in Table 11). Step-stratified: All: 62.39%; 2 steps: 74.53%; 3 steps: 68.91%; 4 steps: 59.73%; &gt;=5 steps: 45.23%.",
            "comparison_with_other_method": true,
            "performance_other_method": "Chain-of-Thought (code-davinci-002) overall: 60.87%. Step-stratified: All: 60.87%; 2 steps: 76.68%; 3 steps: 67.29%; 4 steps: 59.39%; &gt;=5 steps: 39.07%.",
            "key_findings": "Least-to-most provides a modest overall improvement over chain-of-thought on GSM8K (≈+1.5 percentage points), with the largest relative gains on the hardest problems (&gt;=5 steps: L2M 45.23% vs CoT 39.07%), indicating L2M helps easy-to-hard generalization for deeper multi-step problems. For simpler 2-step problems CoT slightly outperforms L2M (76.68% vs 74.53%).",
            "counter_examples_or_negative_results": "Chain-of-thought outperforms L2M on 2-step problems; overall improvement of L2M is small and limited to problems requiring many steps. Authors note many GSM8K failures by L2M can be solved if a correct manual decomposition is provided, highlighting that automatic decomposition quality is a limiting factor.",
            "uuid": "e4943.2",
            "source_info": {
                "paper_title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "L2M_vs_CoT_DROP",
            "name_full": "Least-to-most prompting versus Chain-of-Thought prompting on DROP (numerical reading comprehension subset)",
            "brief_description": "Comparison of least-to-most prompting (decomposition + solving) against chain-of-thought and other baselines on numerical reasoning questions in the DROP benchmark (split into 'non-football' and 'football' subsets).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "code-davinci-002 (GPT-3 / Codex family)",
            "model_description": "Proprietary large language model from the Codex/GPT-3 family used as the base LM in experiments; paper does not report parameter counts.",
            "reasoning_method_name": "Least-to-Most prompting (decomposition + sequential solving)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Decompose paragraph-level numerical questions into simpler subproblems (e.g., identify quantities, compute sub-results) and sequentially compute answers using previous subresults, implemented with few-shot prompting (two-stage decomposition and solving prompts).",
            "task_name": "DROP (numerical subset: Non-football and Football)",
            "task_description": "Reading comprehension benchmark requiring discrete numerical reasoning over paragraphs; this experiment focuses on numerical problems subset and a football-specific subset.",
            "performance": "Least-to-Most accuracy (code-davinci-002): Non-football subset: 82.45%; Football subset: 73.42%.",
            "comparison_with_other_method": true,
            "performance_other_method": "Chain-of-Thought (code-davinci-002): Non-football: 74.77%; Football: 59.56%. Zero-shot and standard prompting also reported (lower).",
            "key_findings": "Least-to-most substantially outperforms chain-of-thought on DROP numerical questions, particularly on the domain (football) subset where decomposition of quantities and event sequences is helpful; many DROP problems are trivially decomposable and thus benefit strongly from explicit decomposition.",
            "counter_examples_or_negative_results": "Authors report that decomposition prompts do not generalize across domains; a decomposition prompt crafted for math problems is not effective for some common-sense reasoning tasks. Also, L2M can fail if decomposition is incorrect — error analysis includes wrong decomposition, wrong solving, and incorrect labels.",
            "uuid": "e4943.3",
            "source_info": {
                "paper_title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
                "publication_date_yy_mm": "2022-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Compositional generalization via neural-symbolic stack machines",
            "rating": 2
        },
        {
            "paper_title": "Neural Logic Machines",
            "rating": 1
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 1
        }
    ],
    "cost": 0.014197749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LeAST-TO-MOST PROMPTING EnAbleS COMPLEX REASONING IN LARGE LANGUAGE MODELS</h1>
<p>Denny Zhou ${ }^{\dagger * *}$ Nathanael Schärli ${ }^{\dagger}$ Le Hou ${ }^{\dagger}$ Jason Wei ${ }^{\dagger}$ Nathan Scales ${ }^{\dagger}$ Xuezhi Wang ${ }^{\dagger}$ Dale Schuurmans ${ }^{\dagger}$ Claire Cui ${ }^{\dagger}$ Olivier Bousquet ${ }^{\dagger}$ Quoc Le ${ }^{\dagger}$ Ed Chi ${ }^{\dagger}$<br>${ }^{\dagger}$ Google Research, Brain Team</p>
<h4>Abstract</h4>
<p>Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least $99 \%$ using just 14 exemplars, compared to only $16 \%$ accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.</p>
<h2>1 INTRODUCTION</h2>
<p>Despite the great success of deep learning in the past decade, there still remain huge differences between human intelligence and machine learning: (1) Given a new task, humans usually can learn to accomplish it from only a few demonstration examples, while machine learning requires a large amount of labeled data for model training; (2) Humans can clearly explain the underlying rationale for their predictions or decisions, while machine learning is essentially a black box; (3) Humans can solve problems more difficult than any they have seen before, while for machine learning, examples in training and testing are typically at the same level of difficulty.
The recently proposed chain-of-thought prompting approach (Wei et al., 2022; Chowdhery et al., 2022) has taken a significant step for narrowing the gap between human intelligence and machine intelligence. It combines the idea of natural language rationales (Ling et al., 2017; Cobbe et al., 2021) with few-shot prompting (Brown et al., 2020). When further integrated with self-consistency decoding (Wang et al., 2022b) rather than using the typical greedy decoding, few-shot chain-of-thought prompting largely outperforms the state-of-the-art results in the literature on many challenging natural language processing tasks obtained from specially designed neural models trained with hundreds of times more annotated examples, while being fully interpretable.</p>
<p>However, chain-of-thought prompting has a key limitation-it often performs poorly on tasks that require generalization of solving problems harder than the demonstration examples, such as compositional generalization (Lake \&amp; Baroni, 2018; Keysers et al., 2020). To tackle such easy-to-hard generalization issues, we propose least-to-most prompting. It consists of two stages: first decomposing a complex problem into a list of easier subproblems, and then sequentially solving these subproblems, whereby solving a given subproblem is facilitated by the answers to previously solved</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>subproblems. Both stages are implemented by few-shot prompting, so that there is no training or finetuning in either stage. An example usage of least-to-most prompting is illustrated in Figure 1.</p>
<p>The term least-to-most prompting is borrowed from educational psychology (Libby et al., 2008), where it is used to denote the technique of using a progressive sequence of prompts to help a student to learn a new skill. Here we apply this technique for teaching humans to teach language models. Empirical results on symbolic manipulation, compositional generalization, and math reasoning show that least-to-most prompting can indeed generalize to problems harder than those demonstrated.</p>
<h1>Stage 1: Decompose Question into Subquestions</h1>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Least-to-most prompting solving a math word problem in two stages: (1) query the language model to decompose the problem into subproblems; (2) query the language model to sequentially solve the subproblems. The answer to the second subproblem is built on the answer to the first subproblem. The demonstration examples for each stage's prompt are omitted in this illustration.</p>
<h2>2 LeAST-TO-MOST PROMPTING</h2>
<p>Least-to-most prompting teaches language models how to solve a complex problem by decomposing it to a series of simpler subproblems. It consists of two sequential stages:</p>
<ol>
<li>Decomposition. The prompt in this stage contains constant examples that demonstrate the decomposition, followed by the specific question to be decomposed.</li>
<li>Subproblem solving. The prompt in this stage consists of three parts: (1) constant examples demonstrating how subproblems are solved; (2) a potentially empty list of previously answered subquestions and generated solutions, and (3) the question to be answered next.</li>
</ol>
<p>In the example shown in Figure 1, the language model is first asked to decompose the original problem into subproblems. The prompt that is passed to the model consists of examples that illustrate how to decompose complex problems (which are not shown in the figure), followed by the specific problem to be decomposed (as shown in the figure). The language model figures out that the original problem can be solved via solving an intermediate problem "How long does each trip take?".</p>
<p>In the next phase, we ask the language model to sequentially solve the subproblems from the problem decomposition stage. The original problem is appended as the final subproblem. The solving starts from passing to the language model a prompt that consists of examples that illustrate how problems are solved (not shown in the figure), followed by the first subproblem "How long does each trip take?". We then take the answer generated by the language model ("... each trip takes 5 minutes.") and construct the next prompt by appending the generated answer to the previous prompt, followed by the next subproblem, which happens to be the original problem in this example. The new prompt is then passed back to the language model, which returns the final answer.</p>
<p>Least-to-most prompting can be combined with other prompting techniques like chain-of-thought (Wei et al., 2022) and self-consistency (Wang et al., 2022b), but does not need to be. Also, for some tasks, the two stages in least-to-most prompting can be merged to form a single-pass prompt.</p>
<h1>3 ReSults</h1>
<p>We present least-to-most prompting results for symbolic manipulation, compositional generalization, and math reasoning tasks, and compare it with chain-of-thought prompting.</p>
<h3>3.1 Symbolic Manipulation</h3>
<p>We take the last-letter-concatenation task (Wei et al., 2022). In this task, each input is a list of words, and the corresponding output is the concatenation of the last letters of the words in the list. For example, "thinking, machine" outputs "ge", since the last letter of "thinking" is " g " and the last letter of "machine" is " e ". Chain-of-thought prompting does a perfect job when the testing lists have the same length as the lists in the prompt exemplars. However, it performs poorly when the testing lists are much longer than the lists in the prompt exemplars. We show that least-to-most prompting overcomes this limitation and significantly outperforms chain-of-thought prompting on length generalization.</p>
<p>Q: "think, machine, learning"
A: "think", "think, machine", "think, machine, learning"</p>
<p>Table 1: Least-to-most prompt context (decomposition) for the last-letter-concatenation task. It can decompose arbitrary long lists into sequential subsists with an accuracy of $100 \%$.</p>
<p>Q: "think, machine"
A: The last letter of "think" is " k ". The last letter of "machine" is " e ". Concatenating " k ", " e " leads to "ke". So, "think, machine" outputs "ke".</p>
<p>Q: "think, machine, learning"
A: "think, machine" outputs "ke". The last letter of "learning" is " g ". Concatenating "ke", " g " leads to "keg". So, "think, machine, learning" outputs "keg".</p>
<p>Table 2: Least-to-most prompt context (solution) for the last-letter-concatenation task. The two exemplars in this prompt actually demonstrate a base case and a recursive step.</p>
<p>Least-to-most prompting. The least-to-most prompt contexts for the last-letter-concatenation task are shown in Tables 1 and 2. The exemplar in Table 1 demonstrates how to decompose a list into a sequence of sublists. The exemplar in Table 2 demonstrates how to map an input to the desired output. Given a new list, we first append it to the exemplar in Table 1 to construct the decomposition prompt, which is sent to the language model to obtain the list's decomposition. Then, we construct for each sublist $S$ a solution prompt, which consists of the exemplars in Table 2, followed by the previous sublist/response pairs (if any), followed by $S$. We sequentially issue these prompts to the language model and use the last response as the final solution.</p>
<p>It is worth a closer look at the exemplars in Table 2. Essentially, they teach language models how to build answers to new problems using the answers to previously solved problems: (1) the list in the</p>
<p>second exemplar ("think, machine, learning") is an extension of the list in the first exemplar ("think, machine") rather than an entirely independent one; (2) the response to "think, machine, learning" is built on the output of "think, machine" by starting with a sentence saying that "think, machine" outputs "ke". The two exemplars together illustrate a base case and a recursive step.
Chain-of-thought prompting. The chain-of-thought prompt context for the last-letterconcatenation task is listed in Table 3. It uses the same lists as the least-to-most prompt in Table 2. The only difference is that, in the chain-of-thought prompt, the response to the second list ("think, machine, learning") is built from scratch, instead of using the output of the first list ("think, machine").</p>
<p>Q: "think, machine"
A: The last letter of "think" is " k ". The last letter of "machine" is " e ". Concatenating " k ", " e " leads to "ke". So, "think, machine" outputs "ke".</p>
<p>Q: "think, machine, learning"
A: The last letter of "think" is " k ". The last letter of "machine" is " e ". The last letter of "learning" is " g ". Concatenating " k ", " e ", " g " leads to "keg". So, "think, machine, learning" outputs "keg".</p>
<p>Table 3: Chain-of-thought prompt context for the last-letter-concatenation task. Unlike the least-to-most prompt in Table 2, the exemplars in the chain-of-thought prompt are independent of each other.</p>
<p>We compare least-to-most prompting (Table $1 \&amp; 2$ ) with chain-of-thought prompting (Table 3) and the standard few-shot prompting. The prompt for the standard few-shot prompting is constructed by removing the intermediate explanations in the chain-of-thought prompt. That is, it just consists of these two exemplars: (1) "think, machine" outputs "ke"; and (2) "think, machine, learning" outputs "keg". We do not consider a training or finetuning baseline because a machine learning model based on two examples would generalize very poorly.
Results. We randomly sample words in Wiktionary ${ }^{1}$ to construct testing lists with lengths varying from 4 to 12 . For each given length, 500 lists are constructed. The accuracies of different methods with code-davinci-002 in GPT-3 are shown in Table 4. Standard prompting completely fails all test cases with an accuracy of 0 . Chain-of-thought prompting significantly boosts the performance over standard prompting, but it still falls well behind least-to-most prompting, particularly when the lists are long. Moreover, the performance of chain-of-thought prompting drops much faster than least-to-most prompting as the length increases.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">$L=4$</th>
<th style="text-align: center;">$L=6$</th>
<th style="text-align: center;">$L=8$</th>
<th style="text-align: center;">$L=10$</th>
<th style="text-align: center;">$L=12$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Standard prompting</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">Chain-of-Thought</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">39.8</td>
<td style="text-align: center;">31.8</td>
</tr>
<tr>
<td style="text-align: left;">Least-to-Most</td>
<td style="text-align: center;">$\mathbf{9 4 . 0}$</td>
<td style="text-align: center;">$\mathbf{8 8 . 4}$</td>
<td style="text-align: center;">$\mathbf{8 3 . 0}$</td>
<td style="text-align: center;">$\mathbf{7 6 . 4}$</td>
<td style="text-align: center;">$\mathbf{7 4 . 0}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Accuracies of different prompting methods on the last-letter-concatenation task. The length of testing lists increases from 4 to 12 .</p>
<p>In Appendices 7.2 and 7.3, we present additional experiments with different chain-of-thought prompts and different language models. Note that in contrast to least-to-most prompting, the exemplars in a chain-of-thought prompt can be independent of each other. For the last-letter concatenation task, this means that we do not need to present exemplars that are sublists of other exemplars. In fact, a chain-of-thought prompt with independent lists tends to outperform one with dependent lists, as the former conveys more information. Furthermore, we can enhance chain-of-thought prompting by incorporating additional exemplars. This seems to be fair, as the least-to-most prompt contains more words due to its extra decomposition. As shown in Table 13 (Appendix 7.3), for lists with length 12, chain-of-thought prompting achieves an accuracy of $37.4 \%$ with 4 independent exemplars (Appendix 7.2.2), and $38.4 \%$ with 8 independent exemplars (Appendix 7.2.3). Although there</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>have been notable advancements compared to an accuracy of $31.8 \%$ by the original prompt in Table 3 , chain-of-thought prompting still lags behind least-to-most prompting, which boasts an accuracy of $74.0 \%$.</p>
<p>Error analysis. While least-to-most prompting significantly outperforms chain-of-thought prompting, it is still far from achieving $100 \%$ accuracy for long lists. In Appendix 7.4, we present a detailed error analysis. We find that only very few of them are due to incorrect last letters, while most of them are concatenation errors (dropping or adding a letter). For example, given the list "gratified, contract, fortitude, blew", the model drops the last letter in the concatenation of "dte" and "w", and thus predicts the outcome to be "dte" instead of "dtew". In another example "hollow, supplies, function, gorgeous", the model somehow duplicates the last letter " s " in the concatenation of "wsn" and "s", and thus the prediction becomes "wsnss" instead of "wsns".</p>
<h1>3.2 COMPOSITIONAL GENERALIZATION</h1>
<p>SCAN (Lake \&amp; Baroni, 2018) is probably the most popular benchmark for evaluating compositional generalization. It requires mapping natural language commands to action sequences (Table 5). Sequence-to-sequence models perform poorly under length split where the action sequences in the training set (about $80 \%$ of the full set with over 20,000 examples) are shorter than the action sequences in the testing set. Many specialized neural-symbolic models have been proposed to solve SCAN (Chen et al., 2020; Liu et al., 2020; Nye et al., 2020; Shaw et al., 2021; Kim, 2021). We show that large language models with least-to-most prompting can solve SCAN using only a few demonstration examples. No training or finetuning is needed.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Command</th>
<th style="text-align: left;">Action Sequence</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">"look thrice after jump"</td>
<td style="text-align: left;">JUMP LOOK LOOK LOOK</td>
</tr>
<tr>
<td style="text-align: left;">"run left and walk"</td>
<td style="text-align: left;">TURN_LEFT RUN WALK</td>
</tr>
<tr>
<td style="text-align: left;">"look opposite right"</td>
<td style="text-align: left;">TURN_RIGHT TURN_RIGHT LOOK</td>
</tr>
</tbody>
</table>
<p>Table 5: Example commands in SCAN and their corresponding action sequences. An agent successfully executes a natural language command by performing its corresponding action sequence.</p>
<p>Least-to-most prompting. Like the last-letter-concatenation task in Section 3.1, least-to-most prompting for SCAN is based on two kinds of prompts: (1) a command decomposition prompt containing 8 exemplars to demonstrate how to decompose a long command to a list of short commands (see Table 6 for some of these exemplars); and (2) a command mapping prompt containing 14 exemplars to demonstrate how to map natural language commands to action sequences (see Table 7 for some of these exemplars). The full prompt contexts for both decomposition and mapping are listed in Appendix 8. The exemplars used in the command-mapping prompt are intended to fully cover the semantics of the SCAN commands.</p>
<div class="codehilite"><pre><span></span><code>Q: &quot;look opposite right thrice after walk&quot;
A: &quot;look opposite right thrice&quot; can be solved by: &quot;look opposite right&quot;, &quot;look opposite right
thrice&quot;. &quot;walk&quot; can be solved by &quot;walk&quot;. So, &quot;look opposite right thrice after walk&quot; can be
solved by: &quot;look opposite right&quot;, &quot;look opposite right thrice&quot;, &quot;walk&quot;.
Q: &quot;look around right thrice and walk&quot;
A: &quot;look around right thrice&quot; can be solved by: &quot;look right&quot;, &quot;look around right&quot;, &quot;look around
right thrice&quot;. &quot;walk&quot; can be solved by &quot;walk&quot;. So, &quot;look around right thrice and walk&quot; can be
solved by: &quot;look right&quot;, &quot;look around right&quot;, &quot;look around right thrice&quot;, &quot;walk&quot;.
</code></pre></div>

<p>Table 6: Two exemplars taken from the least-to-most prompt for decomposing a long command to a sequence of short commands.</p>
<p>We use Python notation to make our prompts in least-to-most prompting and the baselines (stand few-shot prompting and chain-of-thought prompting) concise and meet the input size limit of language models (usually up to 2048 tokens). For example, in our prompt design, we map "look twice"</p>
<p>to "LOOK" * 2 rather than "LOOK LOOK". We may have to point out that these Python expressions are just intermediate representations. After obtaining responses from language models, we run a postprocessing script to expand the Python expressions to generate the final results. However, it should not be surprising that we can also further teach language models to correctly process expressions. In Appendix 8.4, we show that with several demonstration exemplars, language models are able to expand Python expressions with a near-perfect accuracy of $99.7 \%$.</p>
<div class="codehilite"><pre><span></span><code>Q: &quot;jump left&quot;
A: The output of &quot;jump left&quot; concatenates: the output of &quot;turn left&quot;, the output of &quot;jump&quot;. &quot;turn
left&quot; outputs &quot;TURN_LEFT&quot;. &quot;jump&quot; outputs &quot;JUMP&quot;. So concatenating the output of &quot;turn
left&quot; and the output of &quot;jump&quot; leads to &quot;TURN_LEFT&quot; + &quot;JUMP&quot;. So the output of &quot;jump left&quot;
is &quot;TURN_LEFT&quot; + &quot;JUMP&quot;.
Q: &quot;run and look twice&quot;
A: The output of &quot;run and look twice&quot; concatenates: the output of &quot;run&quot;, the output of &quot;look
twice&quot;. &quot;run&quot; outputs &quot;RUN&quot;. &quot;look twice&quot; outputs &quot;LOOK&quot; <span class="gs">* 2. So concatenating the output of</span>
<span class="gs">&quot;run&quot; and the output of &quot;look twice&quot; leads to &quot;RUN&quot; + &quot;LOOK&quot; *</span> 2. So the output of &quot;run and
look twice&quot; is &quot;RUN&quot; + &quot;LOOK&quot; * 2.
</code></pre></div>

<p>Table 7: Two exemplars taken from the least-to-most / chain-of-thought prompt for mapping commands to action sequences. Python expressions are used as intermediate representations.</p>
<p>Chain-of-thought prompting. The chain-of-thought prompt for SCAN uses the same commandmapping context as least-to-most prompting (see Table 7) but it does not use command decomposition, which is exclusively used for least-to-most prompting.</p>
<p>Results. We compare least-to-most prompting with chain-of-thought prompting and standard fewshot prompting. The exemplars for standard few-shot prompting are derived from the chain-ofthought prompt by removing the intermediate explanations. The accuracies of different prompting methods with different language models are presented in Table 8. Example outputs can be found in Appendix 8.3. Using code-davinci-002, least-to-most prompting achieves an accuracy of $99.7 \%$ under length split. We also test least-to-most prompting on all other splits and even the full SCAN dataset. We find that its solving rate remains the same. In addition, it may be interesting to note that code-davinci-002 consistently outperforms text-davinci-002, regardless of the prompting method.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Standard prompting</th>
<th style="text-align: center;">Chain-of-Thought</th>
<th style="text-align: center;">Least-to-Most</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">code-davinci-002</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">16.2</td>
<td style="text-align: center;">$\mathbf{9 9 . 7}$</td>
</tr>
<tr>
<td style="text-align: left;">text-davinci-002</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">$\mathbf{7 6 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">code-davinci-001</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">$\mathbf{6 0 . 7}$</td>
</tr>
</tbody>
</table>
<p>Table 8: Accuracies (\%) of different prompting methods on the test set of SCAN under length split. The results of text-davinci-002 are based on a random subset of 100 commands.</p>
<p>Error analysis. In the test set of the length split, there are 13 failures in total from least-to-most prompting: 6 of them incorrectly interpret "twice" and "thrice" following "around", and the rest incorrectly interpret "after" as "and". Let us show a failed example for each category. In the example "walk opposite right twice after run around right thrice", code-davinci-002 correctly translates the expression "run around right" to ("TURN_RIGHT" + "RUN") * 4. Then it makes a mistake when applying "thrice" to this expression and produces ("TURN_RIGHT" + "RUN") * 9 instead of ("TURN_RIGHT" + "RUN") * 4 * 3 or ("TURN_RIGHT" + "RUN") * 12. In the example "run opposite left thrice after run around left twice", code-davinci-002 produces the correct translations for both sub-expressions that are connected by "after" but it combines them as if they were connected by "and". This means that the model produces ("TURN_LEFT" * 2 + "RUN") * 3 + ("TURN_LEFT" + "RUN") * 4 * 2 instead of ("TURN_LEFT" + "RUN") * 4 * 2 + ("TURN_LEFT" * 2 + "RUN") * 3. A detailed error analysis can be found in Appendix 8.2.</p>
<h1>3.3 Math ReAsoning</h1>
<p>In this section, we apply least-to-most prompting to solve math word problems in GSM8K (Cobbe et al., 2021) and DROP (Dua et al., 2019). We are particularly interested at seeing if large language models combined with least-to-most prompting can solve problems more difficult than those seen in the prompts. Here, we simply measure the difficulty by the number of solving steps.</p>
<p>Q: Elsa has 5 apples. Anna has 2 more apples than Elsa. How many apples do they have together? A: Let's break down this problem: 1. How many apples does Anna have? 2. How many apples do they have together?</p>
<ol>
<li>Anna has 2 more apples than Elsa. So Anna has $2+5=7$ apples.</li>
<li>Elsa and Anna have $5+7=12$ apples together.</li>
</ol>
<p>The answer is: 12 .
Table 9: The least-to-most prompt for solving GSM8K. The demonstration problem is solved in only two steps, but the prompt can handle problems that require multiple steps to be solved.</p>
<p>Q: Elsa has 5 apples. Anna has 2 more apples than Elsa. How many apples do they have together? A: Anna has 2 more apples than Elsa. So Anna has $2+5=7$ apples. So Elsa and Anna have $5+7$ $=12$ apples together.</p>
<p>The answer is: 12 .
Table 10: The chain-of-thought prompt for solving GSM8K. It is derived from the least-to-most prompt in Table 9 by removing the decomposition part.</p>
<p>The prompt that we design to solve GSM8K is shown in Table 9. The demonstration exemplar consists of two parts. The first part (starting from "Let's break down this problem ...") shows how the original problem can be decomposed into simpler subproblems, and the the second part shows how the subproblems are solved in sequence. Note that this prompt combines decomposition and subproblem solving into a single pass. One may instead design two different prompts respectively for decomposition and subproblem solving, as the least-to-most prompts in the previous sections, to further improve performance. Here, we focus on investigating how this simple least-to-most prompt generalizes from a simple 2-step problem to more complex multi-step problems.
We also construct a chain-of-thought prompt (Table 10) as our baseline. It is derived from the least-to-most prompt (Table 9) by removing the decomposition part. The results are shown in Table 11. Overall, least-to-most prompting only slightly improves chain-of-thought prompting: from $60.97 \%$ to $62.39 \%$. However, least-to-most prompting essentially improves chain-of-thought prompting in solving problems which need at least 5 steps to be solved: from $39.07 \%$ to $45.23 \%$ (Table 12). We find that almost every problem in GSM8K that least-to-most prompting fails to solve can be eventually solved by using a manually crafted decomposition. This should not be surprising. For our humans, as long as we know how to decompose a complex problem into simpler subproblems, we actually have solved it. For the DROP benchmark, least-to-most prompting outperforms chain-of-thought prompting by a large margin (Table 11). That is probably because most problems in DROP can be trivially decomposed.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Non-football (DROP)</th>
<th style="text-align: center;">Football (DROP)</th>
<th style="text-align: center;">GSM8K</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Zero-Shot</td>
<td style="text-align: center;">43.86</td>
<td style="text-align: center;">51.77</td>
<td style="text-align: center;">16.38</td>
</tr>
<tr>
<td style="text-align: center;">Standard prompting</td>
<td style="text-align: center;">58.78</td>
<td style="text-align: center;">62.73</td>
<td style="text-align: center;">17.06</td>
</tr>
<tr>
<td style="text-align: center;">Chain-of-Thought</td>
<td style="text-align: center;">74.77</td>
<td style="text-align: center;">59.56</td>
<td style="text-align: center;">60.87</td>
</tr>
<tr>
<td style="text-align: center;">Least-to-Most</td>
<td style="text-align: center;">$\mathbf{8 2 . 4 5}$</td>
<td style="text-align: center;">$\mathbf{7 3 . 4 2}$</td>
<td style="text-align: center;">$\mathbf{6 2 . 3 9}$</td>
</tr>
</tbody>
</table>
<p>Table 11: Accuracies (\%) of different prompting methods on GSM8K and DROP (only the subset containing numerical problems). The base language model is code-davinci-002.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Accuracy by Steps (GSM8K)</th>
<th style="text-align: center;">All</th>
<th style="text-align: center;">2 Steps</th>
<th style="text-align: center;">3 Steps</th>
<th style="text-align: center;">4 steps</th>
<th style="text-align: center;">$\geq 5$ steps</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Least-to-Most</td>
<td style="text-align: center;">$\mathbf{6 2 . 3 9}$</td>
<td style="text-align: center;">74.53</td>
<td style="text-align: center;">$\mathbf{6 8 . 9 1}$</td>
<td style="text-align: center;">$\mathbf{5 9 . 7 3}$</td>
<td style="text-align: center;">$\mathbf{4 5 . 2 3}$</td>
</tr>
<tr>
<td style="text-align: center;">Chain-of-Thought</td>
<td style="text-align: center;">60.87</td>
<td style="text-align: center;">$\mathbf{7 6 . 6 8}$</td>
<td style="text-align: center;">67.29</td>
<td style="text-align: center;">59.39</td>
<td style="text-align: center;">39.07</td>
</tr>
</tbody>
</table>
<p>Table 12: Accuracies (\%) of least-to-most prompting and chain-of-thought prompting, broken down by the number of reasoning steps required in the expected solution.</p>
<h1>4 Related Work</h1>
<p>Compositional generalization. SCAN (Lake \&amp; Baroni, 2018) is a widely used benchmark to evaluate compositional generalization. Among all of its splits, the most challenging is the length split, which requires a model to generalize to test sequences longer than training ones. Prior work with good performance on SCAN mostly proposed neural-symbolic architectures (Chen et al., 2020; Liu et al., 2020) and grammar induction techniques (Nye et al., 2020; Shaw et al., 2021; Kim, 2021). Chen et al. (2020) proposed the neural-symbolic stack machine, which contains a neural network as the controller to generate an execution trace for a given input, and a symbolic stack machine to execute the trace and produce the output. The execution trace consists of domain-specific primitives for sequence manipulation, which allows the machine to break down the input sentence into different components, translate them separately, and compose them together. Liu et al. (2020) proposed a framework that cooperatively learns two neural modules, a composer and a solver, to jointly learn the input structure and the symbolic grammar rules. Both Nye et al. (2020) and Shaw et al. (2021) inferred the symbolic grammar rules of SCAN, while Kim (2021) proposed to learn a latent neural grammar. While approaches with symbolic components are able to achieve $100 \%$ accuracy on SCAN (Chen et al., 2020; Liu et al., 2020; Nye et al., 2020; Shaw et al., 2021), they require complicated model training and grammar inference algorithms to search in a large grammar space. Another line of work on SCAN designs data augmentation schemes (Andreas, 2020; Akyürek et al., 2021; Lake, 2019). Both Andreas (2020) and Akyürek et al. (2021) construct synthetic training samples by recombining fragments occurring in different training samples, and Akyürek et al. (2021) further designs a sampling scheme that encourages the recombination model to produce rare samples. On the other hand, Lake (2019) proposed a meta training algorithm, which requires a meta-grammar space to construct training data, and the format of sampled grammars is similar to the SCAN grammar. While these data augmentation techniques improve the performance on several compositional generalization benchmarks, they fail to solve the length split of SCAN. Other prior works propose neural network architectures to improve compositional generalization, where they encourage the model to learn the word and span mapping (Russin et al., 2019; Li et al., 2019), the alignment of input and output as span trees (Herzig \&amp; Berant, 2021), and the permutation equivariance of input and output words (Gordon et al., 2020). Still, these end-to-end neural networks without symbolic components do not generalize to longer test inputs. Unlike the existing work, we demonstrate that without model architectures and symbolic components specially designed to improve compositional generalization, least-to-most prompting achieves $99.7 \%$ accuracy on any split (including length split) with only a handful of demonstration examples, and it does not require any training or finetuning.
Easy-to-hard generalization. In addition to compositional generalization, there are many other tasks where the test cases require more reasoning steps to solve than the training examples, for example, the last-letter-concatenation task where the test lists are longer than the demonstration examples. Dong et al. (2019) propose Neural Logic Machines (NLMs) for both inductive learning and logic reasoning. NLMs trained on small-scale tasks (such as small size block worlds) can perfectly generalize to large-scale tasks (such as larger size block worlds). Schwarzschild et al. (2021) show that recurrent networks trained to solve simple problems with few recurrent steps (such as small size mazes or chess puzzles) can solve more complex problems (such as larger size mazes or chess puzzles) by performing additional recurrences during inference. In our method, we achieve easy-to-hard generalization by decomposing a complex problem into a series of easier problems.
Task decomposition. Perez et al. (2020) decompose a multi-hop question into a number of independent single-hop subquestions, which are answered by an off-the-shelf question answering (QA) model. Then those answers are aggregated to form the final answer. Both question decomposition and answer aggregation are implemented by trained models. Wang et al. (2022a) conducts multi-hop QA by modeling prompts as continuous virtual tokens and progressively eliciting relevant knowl-</p>
<p>edge from language models via iterative prompting. Unlike these methods, our approach does not involve any training or finetuning. Moreover, the subquestions generated in least-to-most prompting are usually dependent and have to be sequentially solved in a specific order so that answers to some subquestions can be used as building blocks to solve other subquestions. Yang et al. (2022) translate natural language questions to SQL queries by decomposing a question into a sequence of slot-filling natural language prompts corresponding to SQL clauses via a rule-based system. Wu et al. (2022) propose chaining large language model steps such that the output of one step becomes the input for the next and develop an interactive system for users to construct and modify chains. Least-to-most prompting chains the processes of problem decomposition and subproblem solving.</p>
<h1>5 LIMITATIONS</h1>
<p>Decomposition prompts typically don't generalize well across different domains. For instance, a prompt that demonstrates decomposing math word problems (as seen in Table 9) isn't effective for teaching large language models to break down common sense reasoning problems, such as "Did Aristotle use a laptop?" (Geva et al., 2021). A new prompt must be designed to demonstrate decomposition for these types of problems in order to achieve optimal performance.</p>
<p>Generalizing decomposition can even be difficult within the same domain. We've observed that nearly all problems in GSM8K can be accurately solved if the large language models are provided with the correct decomposition of those challenging problems. This finding isn't surprising and aligns with our experiences in solving math problems. Whenever we successfully break down a math problem into simpler subproblems we can solve, we've essentially solved the original problem. Exceptional results are achieved on the last-letter-concatenation task and the SCAN benchmark because decomposition in these tasks is relatively straightforward.</p>
<h2>6 CONCLUSION AND DISCUSSION</h2>
<p>We introduced least-to-most prompting to enable language models to solve problems that are harder than those in the prompt. This approach entails a two-fold process: a top-down decomposition of the problem and a bottom-up resolution generation. Our empirical findings, which encompass symbolic manipulation, compositional generalization, and mathematical reasoning, reveal that least-to-most prompting significantly surpasses standard prompting and chain-of-thought prompting.
In general, prompting might not be the optimal method for teaching reasoning skills to large language models. Prompting can be viewed as a unidirectional communication form in which we instruct a language model without considering its feedback. A natural progression would be to evolve prompting into fully bidirectional conversations, enabling immediate feedback to language models, thereby facilitating more efficient and effective learning. The least-to-most prompting technique represents a stride towards instructing language models through such bidirectional interactions.</p>
<h2>ACKNOWLEDGEMENT</h2>
<p>We sincerely thank Xinyun Chen, Xinying Song, Jeff Dean, Zoubin Ghahramani, Fernando Pereira, Jacob Devlin, and Pete Shaw for sharing their valuable knowledge and advice during our discussions. Their expertise greatly improved the quality of our work. Additionally, we are grateful to the anonymous reviewers for their careful review and helpful suggestions, which helped shape our manuscript into its final form.</p>
<h2>REFERENCES</h2>
<p>Ekin Akyürek, Afra Feyza Akyürek, and Jacob Andreas. Learning to recombine and resample data for compositional generalization. In International Conference on Learning Representations, 2021.</p>
<p>Jacob Andreas. Good-enough compositional data augmentation. In Annual Meeting of the Association for Computational Linguistics, 2020.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Xinyun Chen, Chen Liang, Adams Wei Yu, Dawn Song, and Denny Zhou. Compositional generalization via neural-symbolic stack machines. Advances in Neural Information Processing Systems, 33:1690-1701, 2020.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Honghua Dong, Jiayuan Mao, Tian Lin, Chong Wang, Lihong Li, and Denny Zhou. Neural logic machines. In International Conference on Learning Representations, 2019.</p>
<p>Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. arXiv preprint arXiv:1903.00161, 2019.</p>
<p>Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies. Transactions of the Association for Computational Linguistics (TACL), 2021.</p>
<p>Jonathan Gordon, David Lopez-Paz, Marco Baroni, and Diane Bouchacourt. Permutation equivariant models for compositional generalization in language. In International Conference on Learning Representations, 2020.</p>
<p>Jonathan Herzig and Jonathan Berant. Span-based semantic parsing for compositional generalization. In Annual Meeting of the Association for Computational Linguistics, 2021.</p>
<p>Daniel Keysers, Nathanael Schärli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz Stafiniak, Tibor Tihon, et al. Measuring compositional generalization: A comprehensive method on realistic data. International Conference on Learning Representations, 2020.</p>
<p>Yoon Kim. Sequence-to-sequence learning with latent neural grammars. Advances in Neural Information Processing Systems, 34, 2021.</p>
<p>Brenden Lake and Marco Baroni. Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. In International conference on machine learning, pp. 2873-2882. PMLR, 2018.</p>
<p>Brenden M Lake. Compositional generalization through meta sequence-to-sequence learning. Advances in neural information processing systems, 32, 2019.</p>
<p>Yuanpeng Li, Liang Zhao, Jianyu Wang, and Joel Hestness. Compositional generalization for primitive substitutions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 4284-4293, 2019.</p>
<p>Myrna E Libby, Julie S Weiss, Stacie Bancroft, and William H Ahearn. A comparison of most-toleast and least-to-most prompting on the acquisition of solitary play skills. Behavior analysis in practice, 1(1):37-43, 2008.</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2017.</p>
<p>Qian Liu, Shengnan An, Jian-Guang Lou, Bei Chen, Zeqi Lin, Yan Gao, Bin Zhou, Nanning Zheng, and Dongmei Zhang. Compositional generalization by learning analytical expressions. Advances in Neural Information Processing Systems, 33:11416-11427, 2020.</p>
<p>Maxwell Nye, Armando Solar-Lezama, Josh Tenenbaum, and Brenden M Lake. Learning compositional rules via neural program synthesis. Advances in Neural Information Processing Systems, 33:10832-10842, 2020.</p>
<p>Ethan Perez, Patrick Lewis, Wen-tau Yih, Kyunghyun Cho, and Douwe Kiela. Unsupervised question decomposition for question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 8864-8880, 2020.</p>
<p>Jake Russin, Jason Jo, Randall C O'Reilly, and Yoshua Bengio. Compositional generalization in a deep seq2seq model by separating syntax and semantics. arXiv preprint arXiv:1904.09708, 2019.</p>
<p>Avi Schwarzschild, Eitan Borgnia, Arjun Gupta, Furong Huang, Uzi Vishkin, Micah Goldblum, and Tom Goldstein. Can you learn an algorithm? generalizing from easy to hard problems with recurrent networks. Advances in Neural Information Processing Systems, 34, 2021.</p>
<p>Peter Shaw, Ming-Wei Chang, Panupong Pasupat, and Kristina Toutanova. Compositional generalization and natural language variation: Can a semantic parsing approach handle both? In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 922-938, 2021.</p>
<p>Boshi Wang, Xiang Deng, and Huan Sun. Shepherd pre-trained language models to develop a train of thought: An iterative prompting approach. arXiv preprint arXiv:2203.08383, 2022a.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022b.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Brian Ichter, Fei Xia, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35, 2022.</p>
<p>Tongshuang Wu, Michael Terry, and Carrie Jun Cai. AI chains: Transparent and controllable humanAI interaction by chaining large language model prompts. In CHI Conference on Human Factors in Computing Systems, pp. 1-22, 2022.</p>
<p>Jingfeng Yang, Haoming Jiang, Qingyu Yin, Danqing Zhang, Bing Yin, and Diyi Yang. Seqzero: Few-shot compositional semantic parsing with sequential prompts and zero-shot models. arXiv preprint arXiv:2205.07381, 2022.</p>
<h1>Appendix</h1>
<h2>Table of Contents</h2>
<p>7 Last-letter-concatenation ..... 14
7.1 Prompt context for decomposing a word list into subproblems ..... 14
7.2 Prompt contexts with more and different examples ..... 14
7.2.1 Standard prompting, 4 -shot ..... 14
7.2.2 Chain-of-thought prompting, 4 -shot ..... 14
7.2.3 Chain-of-thought prompting, 8 -shot ..... 15
7.2.4 Chain-of-thought prompting, 2-shot, same examples as for least-to-most ..... 15
7.2.5 Least-to-most prompting, 4-shot ..... 15
7.3 Data Generation and additional results ..... 16
7.4 Error analysis: Least-to-most prompting ..... 17
7.5 Example outputs from code-davinci-002 ..... 18
7.5.1 Standard prompting: Failure ..... 18
7.5.2 Chain-of-thought prompting: Success ..... 19
7.5.3 Chain-of-thought prompting: Failure ..... 21
7.5.4 Least-to-most prompting: Success ..... 22
7.5.5 Least-to-most prompting: Failure ..... 25
8 SCAN ..... 28
8.1 Prompt contexts ..... 28
8.1.1 Standard prompting ..... 29
8.1.2 Least-to-most prompting ..... 29
8.1.3 Chain-of-thought prompting ..... 31
8.2 Error analysis: Least-to-most prompting ..... 31
8.3 Example outputs from code-davinci-002 ..... 33
8.3.1 Chain-of-thought prompting: Success ..... 33
8.3.2 Chain-of-thought prompting: Failure ..... 35
8.3.3 Least-to-most prompting: Success ..... 37
8.3.4 Least-to-most prompting: Failure ..... 40
8.4 Expanding Python expressions using prompting ..... 45
9 DROP ..... 46
9.1 Results with text-davinci-002 and LM-540B ..... 46
9.2 Non-football Subset ..... 46
9.2.1 Zero-shot prompting ..... 46
9.2.2 Standard prompting with 3 examples ..... 47
9.2.3 Chain-of-thought prompting with 3 examples ..... 47
9.2.4 Least-to-most prompting I: problem decomposition (5 examples) ..... 48
9.2.5 Least-to-most prompting II: problem solving (3 examples) ..... 48
9.3 Football subset ..... 49
9.3.1 Zero-shot prompting ..... 49
9.3.2 Standard prompting with 3 examples ..... 49
9.3.3 Chain-of-thought prompting with 3 examples ..... 49
9.3.4 Least-to-most prompting I: problem decomposition (6 examples) ..... 50
9.3.5 Least-to-most prompting II: problem solving (3 examples) ..... 51
9.4 Examples where least-to-most succeeded but chain-of-thought failed ..... 52
9.4.1 Case 1 ..... 52
9.4.2 Case 2 ..... 52
9.4.3 Case 3 ..... 53</p>
<p>9.4.4 Case 4 ..... 54
9.4.5 Case 5 ..... 54
9.5 Error analysis: Least-to-most prompting ..... 54
9.5.1 Example of wrong problem decomposition ..... 55
9.5.2 Example of wrong problem solving ..... 55
9.5.3 Example of wrong given label ..... 55
10 GSM8K ..... 56
10.1 Experiment results: One-shot prompts ..... 56
10.2 Experiment results: Engineered prompts ..... 56
10.3 Prompt contexts: One-shot prompts ..... 57
10.3.1 Chain-of-Thought (1-shot) ..... 58
10.3.2 Least-to-Most (1-shot) ..... 58
10.4 Prompt contexts: Engineered prompts ..... 58
10.4.1 Zero-Shot ..... 58
10.4.2 Standard prompting: 4 examples ..... 58
10.4.3 Chain-of-Thought (best): 4 examples ..... 59
10.4.4 Least-to-Most (best) I - problem decomposition: 7 examples ..... 59
10.4.5 Least-to-Most (best) II - problem solving: 4 examples ..... 60</p>
<h1>7 LAST-LETTER-CONCATENATION</h1>
<h3>7.1 PROMPT CONTEXT FOR DECOMPOSING A WORD LIST INTO SUBPROBLEMS</h3>
<p>In Section 3.1 we mentioned that language model prompting can be used to decompose a word list such as "think, machine, learning, reasoning" into a sequence of subproblems "think, machine", "think, machine, learning", and "think, machine, learning, reasoning".</p>
<p>The following prompt context achieves $100 \%$ accuracy on this task when using the text-davinci-002 model. Note that it achieves perfect accuracy on lists up to size 12 (which is the maximum that we tested for our experiment) even though it only contains one exemplar each for lists of sizes 2 and 3 .</p>
<p>Q: "machine, learning"
A: creating sequential sublists of the list "machine, learning":
"machine"
"machine, learning"
Q: "machine, learning, artificial"
A: creating sequential sublists of the list "machine, learning, artificial":
"machine"
"machine, learning"
"machine, learning, artificial"</p>
<h3>7.2 PROMPT CONTEXTS WITH MORE AND DIFFERENT EXAMPLES</h3>
<p>The last-letter-concatenation experiments presented in Section 3.1 are based on prompt contexts that consists of 2 demonstration examples. To make sure that the accuracy gain achieved by least-to-most prompting is not caused by the slight increase in example length when compared to chain-of-thought, we also performed experiments with more context examples so that we can compare least-to-most vs. chain-of-thought for different prompt sizes. Also, we perform experiments where we use for chain-of-thought prompting the same prompt examples that we use for least-to-most prompting (unlike the situation in Table ?? where we use different examples). All these prompts are shown below, and we present and discuss the corresponding accuracies in Section 7.3.</p>
<h3>7.2.1 StANDARD PROMPTING, 4-SHOT</h3>
<p>Q: "think, machine"
A: "ke"
Q: "learning, reasoning, generalization"
A: "ggn"
Q: "artificial, intelligence"
A: "le"
Q: "transformer, language, vision"
A: "ren"</p>
<h3>7.2.2 CHAIN-OF-THOUGHT PROMPTING, 4-SHOT</h3>
<p>Q: "think, machine"
A: The last letter of "think" is " $k$ ". The last letter of "machine" is "e". Concatenating " $k$ ", "e" leads to "ke". So, "think, machine" outputs "ke".</p>
<p>Q: "learning, reasoning, generalization"
A: The last letter of "learning" is "g". The last letter of "reasoning" is "g". The last letter of "generalization" is " $n$ ". Concatenating " $g$ ", " $g$ ", " $n$ " leads to "ggn". So, "learning, reasoning, generalization" outputs "ggn".</p>
<p>Q: "artificial, intelligence"
A: The last letter of "artificial" is "l". The last letter of "intelligence" is "e". Concatenating "l", "e" leads to "le". So, "artificial, intelligence" outputs "le".</p>
<p>Q: "transformer, language, vision"
A: The last letter of "transformer" is "r". The last letter of "language" is "e". The last letter of "vision" is " $n$ ". Concatenating " $r$ ", " $e$ ", " $n$ " leads to "ren". So, "transformer, language, vision" outputs "ren".</p>
<h1>7.2.3 CHAIN-OF-THOUGHT PROMPTING, 8-SHOT</h1>
<p>Q: "think, machine"
A: The last letter of "think" is " $k$ ". The last letter of "machine" is "e". Concatenating " $k$ ", "e" leads to "ke". So, "think, machine" outputs "ke".</p>
<p>Q: "learning, reasoning, generalization"
A: The last letter of "learning" is " $g$ ". The last letter of "reasoning" is " $g$ ". The last letter of "generalization" is " $n$ ". Concatenating " $g$ ", " $g$ ", " $n$ " leads to " $g g n$ ". So, "learning, reasoning, generalization" outputs "ggn".</p>
<p>Q: "artificial, intelligence"
A: The last letter of "artificial" is "l". The last letter of "intelligence" is "e". Concatenating "l", "e" leads to "le". So, "artificial, intelligence" outputs "le".</p>
<p>Q: "transformer, language, vision"
A: The last letter of "transformer" is "r". The last letter of "language" is "e". The last letter of "vision" is " $n$ ". Concatenating " $r$ ", "e", " $n$ " leads to "ren". So, "transformer, language, vision" outputs "ren".</p>
<p>Q: "school, teacher"
A: The last letter of "school" is "l". The last letter of "teacher" is "r". Concatenating "l", "r" leads to "lr". So, "school, teacher" outputs "lr".</p>
<p>Q: "art, science, department"
A: The last letter of "art" is "t". The last letter of "science" is "e". The last letter of "department" is "t". Concatenating "t", "e", "t" leads to "tet". So, "art, science, department" outputs "tet".</p>
<p>Q: "pen, laptop"
A: The last letter of "pen" is " $n$ ". The last letter of "laptop" is " $p$ ". Concatenating: " $n$ ", " $p$ " leads to " $n p$ ". So, "pen, laptop" outputs "np".</p>
<p>Q: "book, campus, university"
A: The last letter of "book" is " $k$ ". The last letter of "campus" is " $s$ ". The last letter of "university" is " $y$ ". Concatenating: " $k$ ", " $s$ ", " $y$ " leads to "ksy". So, "book, campus, university" outputs "ksy".</p>
<h3>7.2.4 CHAIN-OF-THOUGHT PROMPTING, 2-SHOT, SAME EXAMPLES AS FOR LEAST-TO-MOST</h3>
<p>Q: "think, machine"
A: The last letter of "think" is " $k$ ". The last letter of "machine" is "e". Concatenating " $k$ ", "e" leads to "ke". So, "think, machine" outputs "ke".</p>
<p>Q: "think, machine, learning" A: The last letter of "think" is " $k$ ". The last letter of "machine" is "e". The last letter of "learning" is " $g$ ". Concatenating " $k$ ", "e", " $g$ " leads to "keg". So, "think, machine, learning" outputs "keg".</p>
<h3>7.2.5 LeAST-TO-MOST PROMPTING, 4-SHOT</h3>
<p>Q: "think, machine"
A: The last letter of "think" is " $k$ ". The last letter of "machine" is "e". Concatenating " $k$ ", "e" leads to</p>
<p>"ke". So, "think, machine" outputs "ke".
Q: "think, machine, learning"
A: "think, machine" outputs "ke". The last letter of "learning" is "g". Concatenating "ke", "g" leads to "keg". So, "think, machine, learning" outputs "keg".</p>
<p>Q: "transformer, language"
A: The last letter of "transformer" is "r". The last letter of "language" is "e". Concatenating: "r", "e" leads to "re". So, "transformer, language" outputs "re".</p>
<p>Q: "transformer, language, vision"
A: "transformer, language" outputs "re". The last letter of "vision" is "n". Concatenating: "re", "n" leads to "ren". So, "transformer, language, vision" outputs "ren".</p>
<h1>7.3 Data Generation and additional ReSults</h1>
<p>Data generation. The last-letter-concatenation dataset is based on a list of the 10k most common English words (including proper nouns) used in books that are part of project Gutenberg, as collected in Wiktionary ${ }^{2}$. After eliminating profane words, we ended up with a list of 9694 words (all lowercase). For each of the desired list sizes $2,4,6,8,10,12$, we then generated 500 examples, each of which consists of a random sequence of these words (input) and the corresponding sequence of last letters (output). We will release the full dataset upon publication of this paper. Below are 10 random examples of list size 6:</p>
<ul>
<li>IN: "narrative, celebrate, neighbouring, indebted, stove, calling" OUT: "eegdeg"</li>
<li>IN: "barley, silk, thankful, kiss, logs, silent" OUT: "yklsst"</li>
<li>IN: "knitting, conveyance, receives, represent, cow, shut" OUT: "gestwt"</li>
<li>IN: "olive, dark, limitation, airy, pocket, wondered" OUT: "eknytd"</li>
<li>IN: "apprehensive, exclamation, perspiration, trusting, destiny, tactics" OUT: "enngys"</li>
<li>IN: "qualified, envoy, disciple, exert, witnesses, plane" OUT: "dyetse"</li>
<li>IN: "decidedly, dome, france, chris, knowing, peaceful" OUT: "yeesgl"</li>
<li>IN: "deceit, refinement, tips, cord, princes, discovery" OUT: "ttsdsy"</li>
<li>IN: "drops, paste, defective, bohemia, requested, convenient" OUT: "seeadt"</li>
<li>IN: "diverse, christopher, homely, agreeable, fright, suspended" OUT: "eryetd"</li>
</ul>
<p>Complete results. Table 13 summarizes all the experiments we performed for the last-letterconcatenation task. In addition to the experiments where prompt contexts contain 2 demonstration examples presented in Section 3.1, this includes experiments where the prompts contain 4 and 8 demonstration examples (see above).
While more prompt examples have no effect for standard prompting (the accuracy remains at 0 ), they increase the accuracy across the board for chain-of-thought and least-to-most prompting. However, least-to-most prompting consistently outperforms chain-of-thought prompting. In fact, even if we compare 2-shot least-to-most (prompt size 123 GPT3 tokens) to 8 -shot chain-of-thought (prompt size 573 GPT3 tokens), the accuracy for least-to-most prompting is much higher than for chain-of-thought prompting. The difference is especially pronounced for long sequences (e.g., for $\mathrm{L}=$ 12, we have least-to-most at $74.0 \%$ vs. chain-of-thought at $38.4 \%$ ). This shows that least-to-most prompting is much more data-efficient than chain-of-thought prompting for this problem.
Comparing the first two rows for chain-of-thought prompting shows that chain-of-thought achieves higher accuracy if we use two independent examples (see prompt in Table ??) instead of the two dependent examples that we use for least-to-most prompting. This demonstrates that the accuracy advantage of least-to-most prompting over chain-of-thought prompting remains even if the use the same examples for both of them.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Prompting method</th>
<th style="text-align: center;"># Examples</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">$\mathbf{L}=\mathbf{4}$</th>
<th style="text-align: center;">$\mathbf{L}=\mathbf{6}$</th>
<th style="text-align: center;">$\mathbf{L}=\mathbf{8}$</th>
<th style="text-align: center;">$\mathbf{L}=\mathbf{1 0}$</th>
<th style="text-align: center;">$\mathbf{L}=\mathbf{1 2}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Standard</td>
<td style="text-align: center;">Any</td>
<td style="text-align: center;">Any</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">Chain-of-Thought</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">code-002</td>
<td style="text-align: center;">89.4</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">51.8</td>
<td style="text-align: center;">39.8</td>
<td style="text-align: center;">33.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2 (L2M)</td>
<td style="text-align: center;">code-002</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">39.8</td>
<td style="text-align: center;">31.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">code-002</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">53.4</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">37.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">code-002</td>
<td style="text-align: center;">91.0</td>
<td style="text-align: center;">79.8</td>
<td style="text-align: center;">56.8</td>
<td style="text-align: center;">46.8</td>
<td style="text-align: center;">38.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">text-002*</td>
<td style="text-align: center;">87.0</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">46.0</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">14.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">code-001</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">Least-to-Most</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">code-002</td>
<td style="text-align: center;">94.0</td>
<td style="text-align: center;">88.4</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">74.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">code-002</td>
<td style="text-align: center;">$\mathbf{9 6 . 0}$</td>
<td style="text-align: center;">$\mathbf{9 2 . 0}$</td>
<td style="text-align: center;">$\mathbf{8 4 . 6}$</td>
<td style="text-align: center;">$\mathbf{8 0 . 2}$</td>
<td style="text-align: center;">$\mathbf{7 6 . 6}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">text-002*</td>
<td style="text-align: center;">94.0</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">84.0</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">66.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">code-001</td>
<td style="text-align: center;">19.6</td>
<td style="text-align: center;">8.4</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.1</td>
</tr>
</tbody>
</table>
<p>Table 13: Accuracy of different prompting methods, prompt sizes, and GPT3 models on the last-letter-concatenation task with the length of lists increasing from 4 to 12 . We use code-002 to denote the model code-davinci-002, text-002 to denote the model text-davinci-002, and code-001 to denote the model code-davinci-001. The results in the second row for chain-of-thought prompting correspond to the experiment where we use for chain-of-thought the same prompt examples that we use for least-to-most. The results of text-davinci-002 are based on a subset of 100 random examples (rather than the full set of 500 exammples).</p>
<p>The table also contains the results from running against two additional GPT-3 models: text-davinci-002 and codex-davinci-001. While text-davinci-002 shows similar accuracy to code-davinci-002 on small list sizes, the accuracy drops off much faster when moving to larger list sizes, both for chain-of-thought prompting as well as for least-to-most prompting. This indicates that the code-davinci-002 model has an advantage when it comes to dealing with iteration and recursion.</p>
<p>The code-davinci-001 model performs much worse than code-davinci-002 across all dimensions. Even for the shortest list size $(\mathrm{L}=4)$, the accuracy for least-to-most prompting is only $19.6 \%$ compared to $96 \%$ for code-davinci-002. This indicates that there is a large potential for improvement when using the exact same configuration with new model generations.</p>
<h1>7.4 ERROR ANALYSIS: LEAST-TO-MOST PROMPTING</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">Error type</th>
<th style="text-align: center;">2 examples</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">4 examples</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$\mathbf{L}=\mathbf{4}$</td>
<td style="text-align: center;">$\mathbf{L}=\mathbf{1 2}$</td>
<td style="text-align: center;">$\mathbf{L}=\mathbf{4}$</td>
<td style="text-align: center;">$\mathbf{L}=\mathbf{1 2}$</td>
</tr>
<tr>
<td style="text-align: left;">Concatenation error</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: left;">- Dropping a letter</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">15</td>
</tr>
<tr>
<td style="text-align: left;">- Adding a letter</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: left;">- Wrong order</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">Wrong template</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">Incorrect last letter</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">Copy error</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>Table 14: Least-to-most prompting error analysis of 20 random failures of the code-davinci-002 model on list lengths 4 and 12 for prompt contexts consisting of 2 and 4 examples. Note that for some examples, the model made more than one type of error (e.g., dropping and adding a letter during concatenation).</p>
<p>For least-to-most prompting, we analyzed 20 random failures of the code-davinci-002 model on list lengths 4 and 12 for prompt contexts consisting of 2 and 4 examples. The results are shown in Table 14. Concatenation errors may either be due to dropping a letter, adding a letter or outputting the letters in the wrong order. Wrong template means that the model used the extension template instead of the base template to concatenate the last letter of the first two words of the list. Incorrect last letter means that the model got the last letter of a word wrong, and copy error means that the error was due to making a mistake when copying an intermediate result.</p>
<p>We observe that for the prompt consisting of 2 examples, the fraction of concatenation errors increases as we go from length 4 to length 12 while the fraction of wrong template errors go down. This makes sense because the number of concatenations grows with the length of the list, while the number of times the model needs to use the base template stays constant. Note that the template errors disappear when we move to the double prompt, which means that adding two more examples helps the model recognize which template to use. As a consequence, the double prompt has a similar distribution of errors for both list lengths.</p>
<p>Examples of concatenation errors. In the example "gratified, contract, fortitude, blew", the model drops the last letter in the concatenation of "dte" and "w", which means that it predicts the last letter sequence to be "dte" instead of "dtew".</p>
<p>In the example "hollow, supplies, function, gorgeous", the model duplicates the last letter "s" in the concatenation of "wsn" and "s", which means that it predicts the last letter sequence "wsnss" instead of "wsns".</p>
<p>In the example "madly, vengeance, cowardice, monk", the model drops the last letter " k " in the concatenation of "yee" and "k" and instead adds the letter "g". Consequently, the model predicts "yeeg" instead of "yeek".</p>
<p>In the example "slender, lash, throng, scheme", the model breaks the order of the letters "h" and "g" in the concatenation of "rh" and "g", which means that it predicts the last letter sequence "rghe" instead of "rhge".</p>
<p>Example of incorrect last letter. In the example "modification, introducing, schools, lunch", the model determines the last letter of the word "modification" to be "g". Consequently, the predicted last letter sequence is "ggsh" instead of "ngsh".</p>
<p>Example of wrong template application. In the example "upper, unexpectedly, specifically, connection", the model uses the extension template to determine the output of the first two words "upper, unexpectedly". I.e., it produces:</p>
<ul>
<li>"upper" outputs "er". The last letter of "unexpectedly" is "y". Concatenating "er", "y" leads to "ery". So, "upper, unexpectedly" outputs "ery".
when it should have produced:</li>
<li>The last letter of "upper" is "r". The last letter of "unexpectedly" is "y". Concatenating "r", "y" leads to "ry".</li>
</ul>
<p>As a consequence, the model predicts the sequence "eryyn" instead of "ryyn".
Example of copy error. In the example "displeased, hawk, healing, anchor", the model correctly computes the final concatenation "dkgr" but then makes a mistake when copying the result to the final output and drops the final "r", which leads to the prediction "dkg" instead of "dkgr":</p>
<ul>
<li>"displeased, hawk, healing" outputs "dkg". The last letter of "anchor" is "r". Concatenating: "dkg", "r" leads to "dkgr". So, "displeased, hawk, healing, anchor" outputs "dkg".</li>
</ul>
<h1>7.5 EXAMPLE OUTPUTS FROM CODE-DAVINCI-002</h1>
<p>In this section, we present 5 examples (one example for lengths $4,6,8,10,12$ ) of successes and failures of the code-davinci-002 model for standard prompting, chain-of-thought prompting, and least-to-most prompting. They are all based on the prompt contexts containing 4 demonstration examples.</p>
<h3>7.5.1 Standard Prompting: Failure</h3>
<p>"witnesses, arkansas, remaining, privileged"</p>
<h2>Golden:</h2>
<p>"ssgd"</p>
<h1>Solution:</h1>
<p>Q: "witnesses, arkansas, remaining, privileged"
A: " $n$ "
"clan, noun, roy, nicholas, relish, liking"
Golden:
"nnyshg"</p>
<h2>Solution:</h2>
<p>Q: "clan, noun, roy, nicholas, relish, liking"
A: "nicholas"
"franklin, temporarily, high, origin, rapidly, diversion, firmament, promptly"</p>
<h2>Golden:</h2>
<p>"nyhnynty"</p>
<h2>Solution:</h2>
<p>Q: "franklin, temporarily, high, origin, rapidly, diversion, firmament, promptly"
A: " $n$ "
"conquered, owed, enclosure, indeed, shattered, beads, increased, provided, weak, grind"
Golden:
"ddeddsddkd"</p>
<h2>Solution:</h2>
<p>Q: "conquered, owed, enclosure, indeed, shattered, beads, increased, provided, weak, grind"
A: "d"
"hope, alison, mirrors, benefits, altered, curve, keen, portuguese, distance, interfaces, proof, seamus"</p>
<h2>Golden:</h2>
<p>"enssdeneesfs"</p>
<h2>Solution:</h2>
<p>Q: "hope, alison, mirrors, benefits, altered, curve, keen, portuguese, distance, interfaces, proof, seamus"
A: "hope"</p>
<h3>7.5.2 CHAIN-OF-THOUGHT PROMPTING: SUCCESS</h3>
<p>"geese, rogers, benefactor, version"</p>
<h2>Golden:</h2>
<p>"esrn"</p>
<h2>Solution:</h2>
<p>Q: "geese, rogers, benefactor, version"
A: The last letter of "geese" is "e". The last letter of "rogers" is "s". The last letter of "benefactor" is "r". The last letter of "version" is " $n$ ". Concatenating "e", "s", "r", "n" leads to "esrn". So, "geese, rogers,</p>
<p>benefactor, version" outputs "esrn".</p>
<h1>"amply, bestow, maryland, edited, blowing, prosperous"</h1>
<h2>Golden:</h2>
<p>"ywddgs"</p>
<h2>Solution:</h2>
<p>Q: "amply, bestow, maryland, edited, blowing, prosperous"
A: The last letter of "amply" is "y". The last letter of "bestow" is "w". The last letter of "maryland" is "d". The last letter of "edited" is "d". The last letter of "blowing" is "g". The last letter of "prosperous" is "s". Concatenating "y", "w", "d", "d", "g", "s" leads to "ywddgs". So, "amply, bestow, maryland, edited, blowing, prosperous" outputs "ywddgs".</p>
<h2>"needless, courts, reflected, curly, glowing, asp, chronicles, anthony"</h2>
<h2>Golden:</h2>
<p>"ssdygpsy"</p>
<h2>Solution:</h2>
<p>Q: "needless, courts, reflected, curly, glowing, asp, chronicles, anthony"
A: The last letter of "needless" is "s". The last letter of "courts" is "s". The last letter of "reflected" is "d". The last letter of "curly" is "y". The last letter of "glowing" is "g". The last letter of "asp" is "p". The last letter of "chronicles" is "s". The last letter of "anthony" is "y". Concatenating "s", "s", "d", "y", "g", "p", "s", "y" leads to "ssdygpsy". So, "needless, courts, reflected, curly, glowing, asp, chronicles, anthony" outputs "ssdygpsy".</p>
<h2>"nigel, alaska, rent, hospital, newspaper, medical, nation, dave, associated, rumour"</h2>
<h2>Golden:</h2>
<p>"latlrlnedr"</p>
<h2>Solution:</h2>
<p>Q: "nigel, alaska, rent, hospital, newspaper, medical, nation, dave, associated, rumour"
A: The last letter of "nigel" is "l". The last letter of "alaska" is "a". The last letter of "rent" is "t". The last letter of "hospital" is "l". The last letter of "newspaper" is "r". The last letter of "medical" is "l". The last letter of "nation" is " $n$ ". The last letter of "dave" is " $e$ ". The last letter of "associated" is " $d$ ". The last letter of "rumour" is "r". Concatenating "l", "a", "t", "l", "r", "l", "n", "e", "d", "r" leads to "latlrlnedr". So, "nigel, alaska, rent, hospital, newspaper, medical, nation, dave, associated, rumour" outputs "latlrlnedr".</p>
<h2>"brightness, preparatory, intense, accumulated, reluctantly, nets, inquiries, grip, inquisition, alley, innocent, herbert"</h2>
<h2>Golden:</h2>
<p>"syedysspnytt"</p>
<h2>Solution:</h2>
<p>Q: "brightness, preparatory, intense, accumulated, reluctantly, nets, inquiries, grip, inquisition, alley, innocent, herbert"
A: The last letter of "brightness" is "s". The last letter of "preparatory" is "y". The last letter of "intense" is "e". The last letter of "accumulated" is "d". The last letter of "reluctantly" is "y". The last letter of "nets" is "s". The last letter of "inquiries" is "s". The last letter of "grip" is "p". The last letter of "inquisition" is " $n$ ". The last letter of "alley" is " $y$ ". The last letter of "innocent" is " $t$ ". The last letter of</p>
<p>"herbert" is "t". Concatenating "s", "y", "e", "d", "y", "s", "s", "p", "n", "y", "t", "t" leads to "syedysspnytt". So, "brightness, preparatory, intense, accumulated, reluctantly, nets, inquiries, grip, inquisition, alley, innocent, herbert" outputs "syedysspnytt".</p>
<h1>7.5.3 Chain-of-thought prompting: Failure</h1>
<p>"pattern, vote, afterwards, solemn"</p>
<h2>Golden:</h2>
<p>"nesn"</p>
<h2>Solution:</h2>
<p>Q: "pattern, vote, afterwards, solemn"
A: The last letter of "pattern" is " $n$ ". The last letter of "vote" is "e". The last letter of "afterwards" is "s". The last letter of "solemn" is " $n$ ". Concatenating " $n$ ", " $e$ ", " $s$ ", " $n$ " leads to "ness". So, "pattern, vote, afterwards, solemn" outputs "ness".</p>
<h2>"disappointment, troublesome, respectfully, monstrous, worry, politics"</h2>
<h2>Golden:</h2>
<p>"teysys"</p>
<h2>Solution:</h2>
<p>Q: "disappointment, troublesome, respectfully, monstrous, worry, politics"
A: The last letter of "disappointment" is " $t$ ". The last letter of "troublesome" is " $e$ ". The last letter of "respectfully" is " $y$ ". The last letter of "monstrous" is " $s$ ". The last letter of "worry" is " $y$ ". The last letter of "politics" is "s". Concatenating "t", "e", "y", "s", "y", "s" leads to "teyssy". So, "disappointment, troublesome, respectfully, monstrous, worry, politics" outputs "teyssy".</p>
<h2>"drawing, changes, purified, basic, others, send, chimney, vice"</h2>
<h2>Golden:</h2>
<p>"gsdcsdye"</p>
<h2>Solution:</h2>
<p>Q: "drawing, changes, purified, basic, others, send, chimney, vice"
A: The last letter of "drawing" is " $g$ ". The last letter of "changes" is " $s$ ". The last letter of "purified" is "d". The last letter of "basic" is "c". The last letter of "others" is "s". The last letter of "send" is "d". The last letter of "chimney" is " $y$ ". The last letter of "vice" is "e". Concatenating "g", "s", "d", "c", "s", "d", "y", "e" leads to "gsdcsyde". So, "drawing, changes, purified, basic, others, send, chimney, vice" outputs "gsdcsyde".</p>
<h2>"sensitive, pray, wreath, betwixt, golden, regions, loyalty, interval, hunting, ranch"</h2>
<h2>Golden:</h2>
<p>"eyhtnsylgh"</p>
<h2>Solution:</h2>
<p>Q: "sensitive, pray, wreath, betwixt, golden, regions, loyalty, interval, hunting, ranch"
A: The last letter of "sensitive" is "e". The last letter of "pray" is "y". The last letter of "wreath" is "h". The last letter of "betwixt" is "t". The last letter of "golden" is " $n$ ". The last letter of "regions" is "s". The last letter of "loyalty" is " $y$ ". The last letter of "interval" is " 1 ". The last letter of "hunting" is " $g$ ". The last letter of "ranch" is " $h$ ". Concatenating "e", " $y$ ", " $h$ ", " $t$ ", " $n$ ", " $s$ ", " $y$ ", " $l$ ", " $g$ ", " $h$ " leads to "eyhtsynslgh". So, "sensitive, pray, wreath, betwixt, golden, regions, loyalty, interval, hunting, ranch" outputs "eyhtsynslgh".</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/PG/2006/04/ $1-10000$&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
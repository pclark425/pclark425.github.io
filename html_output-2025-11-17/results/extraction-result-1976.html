<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1976 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1976</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1976</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-40.html">extraction-schema-40</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <p><strong>Paper ID:</strong> paper-280391224</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2507.22791v1.pdf" target="_blank">Modality-Aware Feature Matching: A Comprehensive Review of Single- and Cross-Modality Techniques</a></p>
                <p><strong>Paper Abstract:</strong> Feature matching is a cornerstone task in computer vision, essential for applications such as image retrieval, stereo matching, 3D reconstruction, and SLAM. This survey comprehensively reviews modality-based feature matching, exploring traditional handcrafted methods and emphasizing contemporary deep learning approaches across various modalities, including RGB images, depth images, 3D point clouds, LiDAR scans, medical images, and vision-language interactions. Traditional methods, leveraging detectors like Harris corners and descriptors such as SIFT and ORB, demonstrate robustness under moderate intra-modality variations but struggle with significant modality gaps. Contemporary deep learning-based methods, exemplified by detector-free strategies like CNN-based SuperPoint and transformer-based LoFTR, substantially improve robustness and adaptability across modalities. We highlight modality-aware advancements, such as geometric and depth-specific descriptors for depth images, sparse and dense learning methods for 3D point clouds, attention-enhanced neural networks for LiDAR scans, and specialized solutions like the MIND descriptor for complex medical image matching. Cross-modal applications, particularly in medical image registration and vision-language tasks, underscore the evolution of feature matching to handle increasingly diverse data interactions.</p>
                <p><strong>Cost:</strong> 0.031</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1976.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1976.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIP (Contrastive Language–Image Pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A contrastively trained dual-encoder that learns a shared image–text embedding space from web-scale image-caption pairs, enabling open-vocabulary retrieval and zero-shot recognition.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning transferable visual models from natural language supervision</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dual-encoder contrastive model: an image encoder and a text encoder trained to place paired images and captions nearby in a shared embedding space; grounding is performed by comparing cosine similarity between image and text embeddings (late fusion).</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>CLIP image encoder (dual-encoder architecture; implementation-specific backbones like ResNet or ViT in original CLIP variants)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Contrastive pretraining on ~400M image-text pairs from web-scale alt-text corpora (as described in the survey)</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Shared embedding space + cosine-similarity; contrastive alignment between global image embeddings and text embeddings (no explicit per-region cross-attention in canonical CLIP).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Global image features (global image embedding); can be adapted to region/pixel level via additional mechanisms but canonical CLIP is global.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>None / implicit only (no explicit bounding-box or pixel-level spatial tokens in canonical dual-encoder CLIP)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Potentially used for embodied tasks (paper mentions foundation models like CLIP as relevant to vision-language matching in general); not evaluated specifically in embodied tasks within this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Web-scale natural images (internet alt-text data)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td>Survey notes large-scale contrastive pretraining (CLIP, ALIGN) yields strong zero-shot retrieval and classification transfer; ALIGN (1.8B pairs) and CLIP (400M) are cited to show pretraining scale matters, but no embodied-task numbers are given.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Survey highlights general perception issues relevant to using foundation models in embodied settings: partial observability, novel objects missing from pretraining data, compositionality/robustness failures, and sim-to-real domain gaps that can limit direct transfer of CLIP-style models into navigation/manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Paper-level analysis: foundation models (including CLIP) still suffer from compositional reasoning failures, robustness/bias issues, and limited spatial precision for tasks requiring fine-grained localization (these are listed as ongoing challenges; no dataset-level failure frequencies reported).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Survey discusses approaches broadly (fine-tuning, prompt/prompt-tuning, locking text encoder (LiT-style) or adapting via small-shot prompt learning), but reports that domain shift (sim-to-real, robot egocentric imagery) remains an open challenge.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>Survey mentions strategies (e.g., LiT) that lock text encoder and adapt image encoder; specific frozen vs fine-tuned numbers for embodied tasks are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Survey emphasizes positive scale effects: larger web-scale pretraining (e.g., ALIGN vs CLIP) improves open-vocabulary retrieval/zero-shot transfer, suggesting improved grounding generality with scale, but no quantitative embodied-task scaling law is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Late fusion / similarity-based (cosine similarity of global embeddings).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Contrastive dual-encoder models (CLIP) provide strong open-vocabulary grounding at the global image level and enable zero-shot retrieval/classification, but they lack explicit spatial grounding and thus have limited direct applicability for fine-grained embodied tasks (navigation/manipulation) without additional region-level or cross-attention mechanisms; scale helps generalization but sim-to-real and compositionality remain bottlenecks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1976.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1976.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GLIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GLIP (Grounded Language-Image Pre-training)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale pretraining approach that unifies object detection and phrase grounding by converting detection training examples into phrase-grounding supervision and learning region-text alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Grounded Language-Image Pre-training</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GLIP</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretraining framework that treats object detection labels as phrase grounding signals and trains a detector to align region proposals with text phrases, resulting in a model that performs detection and phrase grounding in a unified fashion (region-level alignment).</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Region-proposal / detector-based visual backbone (survey describes GLIP as unifying detection and grounding; specific backbone details not provided in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Pretrained on large-scale aligned vision–language datasets and detection/phrase datasets (survey-level description; exact dataset sizes not enumerated).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Region-to-phrase alignment (phrase grounding): learn to map bounding-box / region features to textual phrases during pretraining (region-level supervision).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Region-level / object-centric representations (explicit bounding boxes / region features).</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Bounding boxes / region proposals (explicit region spatial tokens used for grounding).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Mentioned as a vision-language grounding architecture; survey suggests such region-level grounding is relevant to embodied tasks but does not report direct embodied evaluations for GLIP.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Natural images and detection datasets (web-scale + detection corpora)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Survey does not provide per-method failure statistics for GLIP; it notes open problems like spatial precision and compositional reasoning remain challenging for vision-language models broadly.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Survey notes large-scale pretraining on diverse data helps generalization; however, specific GLIP sim-to-real or embodied domain-adaptation methods are not described.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Implicit: GLIP leverages large-scale pretraining to improve open-vocabulary detection/grounding; survey frames this within the broader finding that scale and diverse data help grounding generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Region-text alignment via supervised grounding objective (region-text pairing) — effectively region-level cross-modal matching.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Region-level pretraining that treats detection as phrase grounding (GLIP) yields unified detection+grounding capabilities and is promising for tasks requiring object-centric grounding, but the survey emphasizes that further work is needed to translate region-level grounding into robust embodied behaviors under partial observability and sim-to-real shifts.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1976.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1976.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MDETR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MDETR (Modulated Detection for End-to-End Multi-Modal Understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end transformer-based detector that conditions detection on text by modulating the transformer decoder with language, enabling direct phrase grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MDETR: Modulated Detection for End-to-End Multi-Modal Understanding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MDETR</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Extends DETR-style object detection by injecting text encodings into the transformer decoder alongside visual features so that the detector directly predicts boxes grounded to textual phrases via cross-modal attention.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>DETR-style detector (CNN backbone + transformer encoder/decoder where text is provided to decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Trained on aligned image–text and detection/phrase datasets (survey-level mention; exact sizes not given).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Cross-attention between language tokens and visual transformer decoder queries; language-modulated decoding directly produces phrase-grounded object boxes.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Region-level / object-centric (direct bounding-box outputs conditioned on text).</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Bounding boxes predicted by the decoder (explicit spatial outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Visual grounding / referring expression comprehension; paper notes methods like MDETR have improved precise alignment between language and specific image regions — relevant to embodied tasks that require referring resolution, but no direct embodied evaluations reported in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Natural images with phrase grounding/detection annotations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Survey notes general grounding challenges such as compositionality and spatial precision; MDETR addresses precise region–phrase alignment but survey does not provide quantitative failure-mode breakdowns.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Not specified beyond general observation that transformer-based grounding benefits from large aligned datasets; sim-to-real issues persist for embodied use.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Early cross-modal fusion in decoder via cross-attention (language tokens interact with visual decoding queries).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>End-to-end transformer detectors that inject language into the detection decoder (MDETR) provide accurate phrase–region grounding, addressing spatial alignment needs important for embodied instruction following; survey underscores that this family is promising but that adapting such region-level grounding to partially observed, sequential embodied tasks requires further work.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1976.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1976.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TransVG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TransVG</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based, end-to-end visual grounding model that fuses vision and language with a pure transformer encoder-decoder to directly predict grounding outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>TransVG: End-to-end visual grounding with transformers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TransVG</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses a pure transformer encoder-decoder to fuse visual token embeddings with language embeddings and predict grounding outputs (boxes/masks) in a single end-to-end architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Transformer-based vision encoder (visual tokens fed into transformer alongside language tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Pretraining on multimodal grounding datasets is referenced in survey; explicit pretraining dataset sizes are not listed.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Joint transformer encoding of visual and language tokens with cross-attention to produce direct grounding predictions (early/mid-level fusion).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Region-level / token-level (image patches / visual tokens attended together with language tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Bounding boxes / token positions (implicit spatial via patch/token indices; explicit box outputs predicted).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Visual grounding / referring; survey positions such methods as relevant components for embodied tasks (e.g., resolving object references during navigation/manipulation) but no embodiment-specific metrics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Natural images / grounding datasets</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Joint encoding (early fusion) via transformer attention between image patch tokens and language tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Transformer-based end-to-end grounding (TransVG) enables tight visual–language fusion and direct grounding outputs, offering a promising path to precise region-level grounding needed in embodied tasks, but the survey notes adaptation to sequential/egocentric embodied signals remains an open area.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1976.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1976.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReferringTransformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Referring Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer architecture specialized for referring-expression grounding that performs one-step multi-task visual grounding by attending jointly to language and vision.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Referring Transformer: A one-step approach to multi-task visual grounding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Referring Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applies transformer attention to fuse vision and language in a single-stage model to directly predict grounded regions for referring expressions; designed for multi-task grounding setups.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Transformer-based fusion of visual features and language tokens (paper-level description in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Pretrained on grounding/referring expression datasets as discussed in survey; explicit dataset sizes not given.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Cross-attention across language tokens and visual features within a transformer to directly output grounded regions (one-step grounding).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Region-level / token-level (direct region prediction conditioned on language).</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Bounding boxes / region outputs (explicit grounding outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Referring expression grounding (relevant to embodied object search/identification tasks), survey notes such architectures as applicable but gives no embodiment-specific evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Natural images with referring-expression annotations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Cross-attention within transformer enabling direct language-to-region mapping (joint multimodal attention).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>One-step transformer models for referring grounding simplify pipeline complexity and improve direct alignment between phrases and image regions; survey suggests these are useful building blocks for embodied agents that need immediate phrase-to-region grounding, but concrete embodied outcomes are not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1976.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1976.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Speaker-Follower</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Speaker-Follower Models for Vision-and-Language Navigation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLN approach that augments training data with synthetic instruction–path pairs by using a learned 'speaker' to generate instructions for sampled paths, improving generalization of instruction-following agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Speaker-follower models for vision-and-language navigation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Speaker-Follower</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Sequence-to-sequence navigation architecture: instruction encoder (LSTM) and action decoder (LSTM) with attention, augmented by a 'speaker' model that generates synthetic natural-language instructions for paths to increase training diversity and reduce overfitting to training environments.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Panoramic / CNN-based visual features fed into LSTM-based seq2seq navigation model (survey-level description of early VLN pipelines).</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Not specified in survey for this model; early VLN models use ImageNet-pretrained CNNs or panorama feature extractors.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Attention-based alignment between language instruction tokens and visual features at each step of the LSTM decoder (seq2seq attention grounding).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>View-level / step-level (agent-attended visual features per time step; not pixel-precise grounding).</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Implicit via available panoramic view features and action space (discrete navigable viewpoints); no explicit 3D coordinates reported in survey for this method.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Vision-Language Navigation (VLN) — instruction following in simulated environments.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Room-to-Room (R2R) and related VLN benchmarks (survey references Speaker-Follower applied on R2R)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Photorealistic simulation (Matterport3D panoramas / indoor navigation simulators)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Standard VLN metrics such as Success Rate (SR) and SPL (survey describes these metrics for R2R).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Survey states speaker-based augmentation improved generalization vs naive supervised training on limited data, but no numeric delta is provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Survey highlights VLN-specific perception bottlenecks: partial observability (agent only sees limited FoV), compounding errors over steps, and limited robustness to unseen environments; these limit grounding reliability for Speaker-Follower-style agents.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Survey-level failure modes: overfitting to training environments, brittle generalization to unseen houses, error compounding from single missed instruction step causing large trajectory drift (no per-method frequencies given).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Mitigation via synthetic instruction augmentation (speaker) and RL fine-tuning is discussed; sim-to-real remains a broader challenge.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Attention-based fusion between instruction embeddings and visual features at each decoding step (seq2seq attention).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Survey notes augmentation via a speaker improves effective training data and generalization, but does not provide strict sample counts.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Data augmentation via a learned speaker helps VLN agents better ground instructions to visual trajectories and reduces overfitting, but fundamental perception challenges in egocentric/partial-observability settings (occlusions, unseen environments) remain primary failure drivers for embodied instruction-following.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1976.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1976.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HAMT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HAMT (History-Aware Multimodal Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based VLN model that attends over the agent's history and multimodal inputs to improve navigation performance and grounding of instructions to observations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>History aware multimodal transformer for vision-and-language navigation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>HAMT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer architecture that records and attends to trajectory history, combining language and sequential visual observations via multimodal transformer layers to better ground instruction segments to past and present observations.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Transformer-based multimodal encoder combining visual tokens (panoramic/view features) and language tokens; survey describes HAMT as transformer-based with history attention.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Not specified in survey beyond multimodal pretraining / fine-tuning on VLN datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Cross-attention between language tokens and a multimodal history of visual tokens; explicit attention over past observations to resolve references and subgoals.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Step-level / sequence-level (attends to historical visual observations and language together).</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Implicit via sequence of navigable viewpoints and their visual features; explicit 3D coordinates not detailed in survey for HAMT.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Vision-Language Navigation (VLN), designed to improve long-term grounding across trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>R2R and other VLN benchmarks (survey references HAMT in context of history-aware models)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Photorealistic simulation (egocentric views in Matterport3D-style environments)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>VLN metrics (SR, SPL) referenced as community standards; survey does not record HAMT numeric performance here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Survey notes HAMT addresses temporal grounding and partial-observability by attending to history, countering some failure modes like missed subgoals; nonetheless, perception (occlusions, unseen objects) and sim-to-real remain bottlenecks.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Survey-level observations: history-aware attention reduces some trajectory drift and ambiguity, but error compounding and failures on long instructions persist; no per-case frequencies reported.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>HAMT improves within-sim generalization via history modeling; survey does not claim it solves sim-to-real transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Multimodal transformer cross-attention combining language and a history of visual tokens (early/mid fusion in transformer layers).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>In VLN, transformer models that attend to history (HAMT) improve grounding across long instructions by providing temporal context, reducing certain classes of failure (ambiguous references, missed subgoals), but do not eliminate perception-driven failures from partial observability or unseen visual domains.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1976.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1976.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReinforcedCrossModalMatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid approach for VLN combining reinforcement learning with a cross-modal matching objective and self-supervised imitation to improve robustness to deviation from reference trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Reinforced Cross-Modal Matching (RCM) + Self-Supervised Imitation</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses reinforcement learning to expose the agent to its own mistakes and a cross-modal matching objective to align visual and language modalities; self-supervised imitation augments learning by imitating rollouts and correcting drift.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>VLN-style visual encoders (CNN or panoramic feature extractors) combined with language encoders; survey describes the approach at a conceptual level.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Not detailed in the survey for this specific approach; typical VLN setups use ImageNet-pretrained CNN backbones.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Cross-modal matching objective (encourages visual observations and instruction segments to be mutually informative) combined with RL to train policies that ground instructions into sequential actions robustly.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Step-level grounding across trajectories; aligns segments of instructions with episodic visual observations.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Implicit via viewpoint/action space; explicit spatial coordinates not described in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Vision-Language Navigation (VLN)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>R2R and related navigation tasks (survey cites this family of approaches in the VLN section)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Photorealistic simulation (Matterport3D-style indoor environments)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>VLN metrics (SR, SPL) referenced; survey does not include numeric performance here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Survey states RL + cross-modal matching improves robustness to compounding errors compared to pure supervised learning on reference trajectories; no numeric delta provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Survey highlights that RL-based exposure to off-reference states reduces compound errors but perception failures (occlusions, unseen object appearance, novel scenes) still dominate many failures in embodied instruction-following.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Survey-level failure modes: agents still fail due to partial observability, long-horizon planning errors, and perception mismatches; RL mitigates some but not all grounding failures.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Robustness improved via RL and self-supervised imitation, but sim-to-real and domain shift remain open problems per survey.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Cross-modal matching objective + policy learning (RL) that fuses language and visual features during action prediction; mechanism is hybrid (representation alignment + policy optimization).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Combining cross-modal matching with RL and self-supervised imitation exposes agents to realistic offreference states and improves robustness to trajectory drift, but fundamental perception bottlenecks (partial observability, novel objects, sim-to-real) continue to limit embodied grounding performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1976.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1976.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ALFRED</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ALFRED (Action Learning From Realistic Environments and Directives)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark for embodied instruction following that requires agents to perform multi-step household tasks involving navigation and object manipulation guided by natural-language instructions in simulated indoor environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ALFRED: A benchmark for interpreting grounded instructions for everyday tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ALFRED (benchmark/task)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Task suite requiring integrated perception, language understanding, long-horizon planning and manipulation: agents must navigate and manipulate objects according to natural-language directives in AI2-THOR simulated environments.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Multi-level (navigation-level, object/manipulation-level grounding required across steps).</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Requires object-centric/3D interaction (simulated 3D environment coordinates and object state), but the survey describes ALFRED mainly as a task rather than a single model design.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Navigation + Interactive Manipulation (instruction following with object interactions).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>ALFRED (AI2-THOR simulated interactive tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Photorealistic simulation with interactive objects (AI2-THOR)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Task completion metrics combining navigation success and correct manipulation/goal state; survey cites ALFRED as an example but does not list numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Survey explicitly notes ALFRED-style tasks highlight embodied challenges: partial observability, need for long-term planning, interaction affordance perception, and error compounding across multi-step manipulations; sim-to-real differences further complicate transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Survey-level failures for ALFRED-like tasks include compounding errors across steps, difficulty recognizing small or occluded objects for manipulation, and brittleness to visual variation; no per-method frequencies are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Survey notes sim-to-real remains a major open problem; approaches include domain adaptation and fine-tuning but no consensus remedy is given.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>ALFRED illustrates requirements for fine-grained spatial/object-centric grounding plus sequential action planning; survey emphasizes that existing vision-language grounding models need extensions (object affordance perception, persistent memory, robust spatial grounding) to succeed on interactive manipulation benchmarks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1976.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e1976.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REVERIE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>REVERIE (Remote Embodied Visual Referring Expression)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-and-language navigation benchmark that combines navigation with referring expression resolution: agents must navigate to a remote location and identify a referred object described by language.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>REVERIE: Remote embodied visual referring expression in real indoor environments</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>REVERIE (benchmark/task)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two-stage embodied task combining language-guided navigation to a region and fine-grained referring-object grounding once in the target region; evaluates both navigation and grounding accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Requires combined navigation-level grounding and region/object-level referring grounding (survey describes task structure).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Multi-level (navigation-level to reach region, then object-level referring grounding).</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Implicit via navigation viewpoints and object bounding regions within target panorama; explicit bounding-box level grounding required for object identification.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Vision-Language Navigation + Referring Expression Grounding</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>REVERIE</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Photorealistic simulation (Matterport3D indoor panoramas)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Navigation success (SR) and grounding success (correct object identification); survey mentions both metrics as part of REVERIE evaluation protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Survey notes REVERIE highlights the need for persistent memory and multi-step grounding; failures often arise from inability to resolve referents from partial views and from navigation errors that prevent reaching the referent.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Survey-level failure modes: navigation errors leading to wrong region; inability to disambiguate similar objects from panoramic views; no per-case statistics reported in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Not detailed; survey reiterates sim-to-real as nontrivial for such composed tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Combination of navigation policy conditioned on language + region/object grounding module (survey-level description).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Tasks that combine navigation and referring (REVERIE) expose both coarse navigation grounding and fine-grained object grounding failures; success requires multi-level grounding solutions (trajectory memory + region/object detectors) rather than solely global image–text matching.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning transferable visual models from natural language supervision <em>(Rating: 2)</em></li>
                <li>Grounded Language-Image Pre-training <em>(Rating: 2)</em></li>
                <li>MDETR: Modulated Detection for End-to-End Multi-Modal Understanding <em>(Rating: 2)</em></li>
                <li>TransVG: End-to-end visual grounding with transformers <em>(Rating: 1)</em></li>
                <li>Speaker-follower models for vision-and-language navigation <em>(Rating: 2)</em></li>
                <li>History aware multimodal transformer for vision-and-language navigation <em>(Rating: 2)</em></li>
                <li>Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation <em>(Rating: 2)</em></li>
                <li>ALFRED: A benchmark for interpreting grounded instructions for everyday tasks <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1976",
    "paper_id": "paper-280391224",
    "extraction_schema_id": "extraction-schema-40",
    "extracted_data": [
        {
            "name_short": "CLIP",
            "name_full": "CLIP (Contrastive Language–Image Pretraining)",
            "brief_description": "A contrastively trained dual-encoder that learns a shared image–text embedding space from web-scale image-caption pairs, enabling open-vocabulary retrieval and zero-shot recognition.",
            "citation_title": "Learning transferable visual models from natural language supervision",
            "mention_or_use": "mention",
            "model_name": "CLIP",
            "model_description": "Dual-encoder contrastive model: an image encoder and a text encoder trained to place paired images and captions nearby in a shared embedding space; grounding is performed by comparing cosine similarity between image and text embeddings (late fusion).",
            "visual_encoder_type": "CLIP image encoder (dual-encoder architecture; implementation-specific backbones like ResNet or ViT in original CLIP variants)",
            "visual_encoder_pretraining": "Contrastive pretraining on ~400M image-text pairs from web-scale alt-text corpora (as described in the survey)",
            "grounding_mechanism": "Shared embedding space + cosine-similarity; contrastive alignment between global image embeddings and text embeddings (no explicit per-region cross-attention in canonical CLIP).",
            "representation_level": "Global image features (global image embedding); can be adapted to region/pixel level via additional mechanisms but canonical CLIP is global.",
            "spatial_representation": "None / implicit only (no explicit bounding-box or pixel-level spatial tokens in canonical dual-encoder CLIP)",
            "embodied_task_type": "Potentially used for embodied tasks (paper mentions foundation models like CLIP as relevant to vision-language matching in general); not evaluated specifically in embodied tasks within this survey.",
            "embodied_task_name": null,
            "visual_domain": "Web-scale natural images (internet alt-text data)",
            "performance_metric": null,
            "performance_value": null,
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": null,
            "encoder_comparison_results": "Survey notes large-scale contrastive pretraining (CLIP, ALIGN) yields strong zero-shot retrieval and classification transfer; ALIGN (1.8B pairs) and CLIP (400M) are cited to show pretraining scale matters, but no embodied-task numbers are given.",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Survey highlights general perception issues relevant to using foundation models in embodied settings: partial observability, novel objects missing from pretraining data, compositionality/robustness failures, and sim-to-real domain gaps that can limit direct transfer of CLIP-style models into navigation/manipulation.",
            "failure_mode_analysis": "Paper-level analysis: foundation models (including CLIP) still suffer from compositional reasoning failures, robustness/bias issues, and limited spatial precision for tasks requiring fine-grained localization (these are listed as ongoing challenges; no dataset-level failure frequencies reported).",
            "domain_shift_handling": "Survey discusses approaches broadly (fine-tuning, prompt/prompt-tuning, locking text encoder (LiT-style) or adapting via small-shot prompt learning), but reports that domain shift (sim-to-real, robot egocentric imagery) remains an open challenge.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": "Survey mentions strategies (e.g., LiT) that lock text encoder and adapt image encoder; specific frozen vs fine-tuned numbers for embodied tasks are not provided.",
            "pretraining_scale_effect": "Survey emphasizes positive scale effects: larger web-scale pretraining (e.g., ALIGN vs CLIP) improves open-vocabulary retrieval/zero-shot transfer, suggesting improved grounding generality with scale, but no quantitative embodied-task scaling law is provided.",
            "fusion_mechanism": "Late fusion / similarity-based (cosine similarity of global embeddings).",
            "sample_efficiency": null,
            "key_findings_grounding": "Contrastive dual-encoder models (CLIP) provide strong open-vocabulary grounding at the global image level and enable zero-shot retrieval/classification, but they lack explicit spatial grounding and thus have limited direct applicability for fine-grained embodied tasks (navigation/manipulation) without additional region-level or cross-attention mechanisms; scale helps generalization but sim-to-real and compositionality remain bottlenecks.",
            "uuid": "e1976.0"
        },
        {
            "name_short": "GLIP",
            "name_full": "GLIP (Grounded Language-Image Pre-training)",
            "brief_description": "A large-scale pretraining approach that unifies object detection and phrase grounding by converting detection training examples into phrase-grounding supervision and learning region-text alignment.",
            "citation_title": "Grounded Language-Image Pre-training",
            "mention_or_use": "mention",
            "model_name": "GLIP",
            "model_description": "Pretraining framework that treats object detection labels as phrase grounding signals and trains a detector to align region proposals with text phrases, resulting in a model that performs detection and phrase grounding in a unified fashion (region-level alignment).",
            "visual_encoder_type": "Region-proposal / detector-based visual backbone (survey describes GLIP as unifying detection and grounding; specific backbone details not provided in survey).",
            "visual_encoder_pretraining": "Pretrained on large-scale aligned vision–language datasets and detection/phrase datasets (survey-level description; exact dataset sizes not enumerated).",
            "grounding_mechanism": "Region-to-phrase alignment (phrase grounding): learn to map bounding-box / region features to textual phrases during pretraining (region-level supervision).",
            "representation_level": "Region-level / object-centric representations (explicit bounding boxes / region features).",
            "spatial_representation": "Bounding boxes / region proposals (explicit region spatial tokens used for grounding).",
            "embodied_task_type": "Mentioned as a vision-language grounding architecture; survey suggests such region-level grounding is relevant to embodied tasks but does not report direct embodied evaluations for GLIP.",
            "embodied_task_name": null,
            "visual_domain": "Natural images and detection datasets (web-scale + detection corpora)",
            "performance_metric": null,
            "performance_value": null,
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": null,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": null,
            "perception_bottleneck_details": null,
            "failure_mode_analysis": "Survey does not provide per-method failure statistics for GLIP; it notes open problems like spatial precision and compositional reasoning remain challenging for vision-language models broadly.",
            "domain_shift_handling": "Survey notes large-scale pretraining on diverse data helps generalization; however, specific GLIP sim-to-real or embodied domain-adaptation methods are not described.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": "Implicit: GLIP leverages large-scale pretraining to improve open-vocabulary detection/grounding; survey frames this within the broader finding that scale and diverse data help grounding generalization.",
            "fusion_mechanism": "Region-text alignment via supervised grounding objective (region-text pairing) — effectively region-level cross-modal matching.",
            "sample_efficiency": null,
            "key_findings_grounding": "Region-level pretraining that treats detection as phrase grounding (GLIP) yields unified detection+grounding capabilities and is promising for tasks requiring object-centric grounding, but the survey emphasizes that further work is needed to translate region-level grounding into robust embodied behaviors under partial observability and sim-to-real shifts.",
            "uuid": "e1976.1"
        },
        {
            "name_short": "MDETR",
            "name_full": "MDETR (Modulated Detection for End-to-End Multi-Modal Understanding)",
            "brief_description": "An end-to-end transformer-based detector that conditions detection on text by modulating the transformer decoder with language, enabling direct phrase grounding.",
            "citation_title": "MDETR: Modulated Detection for End-to-End Multi-Modal Understanding",
            "mention_or_use": "mention",
            "model_name": "MDETR",
            "model_description": "Extends DETR-style object detection by injecting text encodings into the transformer decoder alongside visual features so that the detector directly predicts boxes grounded to textual phrases via cross-modal attention.",
            "visual_encoder_type": "DETR-style detector (CNN backbone + transformer encoder/decoder where text is provided to decoder)",
            "visual_encoder_pretraining": "Trained on aligned image–text and detection/phrase datasets (survey-level mention; exact sizes not given).",
            "grounding_mechanism": "Cross-attention between language tokens and visual transformer decoder queries; language-modulated decoding directly produces phrase-grounded object boxes.",
            "representation_level": "Region-level / object-centric (direct bounding-box outputs conditioned on text).",
            "spatial_representation": "Bounding boxes predicted by the decoder (explicit spatial outputs).",
            "embodied_task_type": "Visual grounding / referring expression comprehension; paper notes methods like MDETR have improved precise alignment between language and specific image regions — relevant to embodied tasks that require referring resolution, but no direct embodied evaluations reported in survey.",
            "embodied_task_name": null,
            "visual_domain": "Natural images with phrase grounding/detection annotations",
            "performance_metric": null,
            "performance_value": null,
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": null,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": null,
            "perception_bottleneck_details": null,
            "failure_mode_analysis": "Survey notes general grounding challenges such as compositionality and spatial precision; MDETR addresses precise region–phrase alignment but survey does not provide quantitative failure-mode breakdowns.",
            "domain_shift_handling": "Not specified beyond general observation that transformer-based grounding benefits from large aligned datasets; sim-to-real issues persist for embodied use.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Early cross-modal fusion in decoder via cross-attention (language tokens interact with visual decoding queries).",
            "sample_efficiency": null,
            "key_findings_grounding": "End-to-end transformer detectors that inject language into the detection decoder (MDETR) provide accurate phrase–region grounding, addressing spatial alignment needs important for embodied instruction following; survey underscores that this family is promising but that adapting such region-level grounding to partially observed, sequential embodied tasks requires further work.",
            "uuid": "e1976.2"
        },
        {
            "name_short": "TransVG",
            "name_full": "TransVG",
            "brief_description": "A transformer-based, end-to-end visual grounding model that fuses vision and language with a pure transformer encoder-decoder to directly predict grounding outputs.",
            "citation_title": "TransVG: End-to-end visual grounding with transformers",
            "mention_or_use": "mention",
            "model_name": "TransVG",
            "model_description": "Uses a pure transformer encoder-decoder to fuse visual token embeddings with language embeddings and predict grounding outputs (boxes/masks) in a single end-to-end architecture.",
            "visual_encoder_type": "Transformer-based vision encoder (visual tokens fed into transformer alongside language tokens)",
            "visual_encoder_pretraining": "Pretraining on multimodal grounding datasets is referenced in survey; explicit pretraining dataset sizes are not listed.",
            "grounding_mechanism": "Joint transformer encoding of visual and language tokens with cross-attention to produce direct grounding predictions (early/mid-level fusion).",
            "representation_level": "Region-level / token-level (image patches / visual tokens attended together with language tokens).",
            "spatial_representation": "Bounding boxes / token positions (implicit spatial via patch/token indices; explicit box outputs predicted).",
            "embodied_task_type": "Visual grounding / referring; survey positions such methods as relevant components for embodied tasks (e.g., resolving object references during navigation/manipulation) but no embodiment-specific metrics provided.",
            "embodied_task_name": null,
            "visual_domain": "Natural images / grounding datasets",
            "performance_metric": null,
            "performance_value": null,
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": null,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": null,
            "perception_bottleneck_details": null,
            "failure_mode_analysis": null,
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Joint encoding (early fusion) via transformer attention between image patch tokens and language tokens.",
            "sample_efficiency": null,
            "key_findings_grounding": "Transformer-based end-to-end grounding (TransVG) enables tight visual–language fusion and direct grounding outputs, offering a promising path to precise region-level grounding needed in embodied tasks, but the survey notes adaptation to sequential/egocentric embodied signals remains an open area.",
            "uuid": "e1976.3"
        },
        {
            "name_short": "ReferringTransformer",
            "name_full": "Referring Transformer",
            "brief_description": "A transformer architecture specialized for referring-expression grounding that performs one-step multi-task visual grounding by attending jointly to language and vision.",
            "citation_title": "Referring Transformer: A one-step approach to multi-task visual grounding",
            "mention_or_use": "mention",
            "model_name": "Referring Transformer",
            "model_description": "Applies transformer attention to fuse vision and language in a single-stage model to directly predict grounded regions for referring expressions; designed for multi-task grounding setups.",
            "visual_encoder_type": "Transformer-based fusion of visual features and language tokens (paper-level description in survey).",
            "visual_encoder_pretraining": "Pretrained on grounding/referring expression datasets as discussed in survey; explicit dataset sizes not given.",
            "grounding_mechanism": "Cross-attention across language tokens and visual features within a transformer to directly output grounded regions (one-step grounding).",
            "representation_level": "Region-level / token-level (direct region prediction conditioned on language).",
            "spatial_representation": "Bounding boxes / region outputs (explicit grounding outputs).",
            "embodied_task_type": "Referring expression grounding (relevant to embodied object search/identification tasks), survey notes such architectures as applicable but gives no embodiment-specific evaluations.",
            "embodied_task_name": null,
            "visual_domain": "Natural images with referring-expression annotations",
            "performance_metric": null,
            "performance_value": null,
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": null,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": null,
            "perception_bottleneck_details": null,
            "failure_mode_analysis": null,
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Cross-attention within transformer enabling direct language-to-region mapping (joint multimodal attention).",
            "sample_efficiency": null,
            "key_findings_grounding": "One-step transformer models for referring grounding simplify pipeline complexity and improve direct alignment between phrases and image regions; survey suggests these are useful building blocks for embodied agents that need immediate phrase-to-region grounding, but concrete embodied outcomes are not reported.",
            "uuid": "e1976.4"
        },
        {
            "name_short": "Speaker-Follower",
            "name_full": "Speaker-Follower Models for Vision-and-Language Navigation",
            "brief_description": "A VLN approach that augments training data with synthetic instruction–path pairs by using a learned 'speaker' to generate instructions for sampled paths, improving generalization of instruction-following agents.",
            "citation_title": "Speaker-follower models for vision-and-language navigation",
            "mention_or_use": "mention",
            "model_name": "Speaker-Follower",
            "model_description": "Sequence-to-sequence navigation architecture: instruction encoder (LSTM) and action decoder (LSTM) with attention, augmented by a 'speaker' model that generates synthetic natural-language instructions for paths to increase training diversity and reduce overfitting to training environments.",
            "visual_encoder_type": "Panoramic / CNN-based visual features fed into LSTM-based seq2seq navigation model (survey-level description of early VLN pipelines).",
            "visual_encoder_pretraining": "Not specified in survey for this model; early VLN models use ImageNet-pretrained CNNs or panorama feature extractors.",
            "grounding_mechanism": "Attention-based alignment between language instruction tokens and visual features at each step of the LSTM decoder (seq2seq attention grounding).",
            "representation_level": "View-level / step-level (agent-attended visual features per time step; not pixel-precise grounding).",
            "spatial_representation": "Implicit via available panoramic view features and action space (discrete navigable viewpoints); no explicit 3D coordinates reported in survey for this method.",
            "embodied_task_type": "Vision-Language Navigation (VLN) — instruction following in simulated environments.",
            "embodied_task_name": "Room-to-Room (R2R) and related VLN benchmarks (survey references Speaker-Follower applied on R2R)",
            "visual_domain": "Photorealistic simulation (Matterport3D panoramas / indoor navigation simulators)",
            "performance_metric": "Standard VLN metrics such as Success Rate (SR) and SPL (survey describes these metrics for R2R).",
            "performance_value": null,
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": "Survey states speaker-based augmentation improved generalization vs naive supervised training on limited data, but no numeric delta is provided in this review.",
            "has_encoder_comparison": null,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Survey highlights VLN-specific perception bottlenecks: partial observability (agent only sees limited FoV), compounding errors over steps, and limited robustness to unseen environments; these limit grounding reliability for Speaker-Follower-style agents.",
            "failure_mode_analysis": "Survey-level failure modes: overfitting to training environments, brittle generalization to unseen houses, error compounding from single missed instruction step causing large trajectory drift (no per-method frequencies given).",
            "domain_shift_handling": "Mitigation via synthetic instruction augmentation (speaker) and RL fine-tuning is discussed; sim-to-real remains a broader challenge.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Attention-based fusion between instruction embeddings and visual features at each decoding step (seq2seq attention).",
            "sample_efficiency": "Survey notes augmentation via a speaker improves effective training data and generalization, but does not provide strict sample counts.",
            "key_findings_grounding": "Data augmentation via a learned speaker helps VLN agents better ground instructions to visual trajectories and reduces overfitting, but fundamental perception challenges in egocentric/partial-observability settings (occlusions, unseen environments) remain primary failure drivers for embodied instruction-following.",
            "uuid": "e1976.5"
        },
        {
            "name_short": "HAMT",
            "name_full": "HAMT (History-Aware Multimodal Transformer)",
            "brief_description": "A transformer-based VLN model that attends over the agent's history and multimodal inputs to improve navigation performance and grounding of instructions to observations.",
            "citation_title": "History aware multimodal transformer for vision-and-language navigation",
            "mention_or_use": "mention",
            "model_name": "HAMT",
            "model_description": "Transformer architecture that records and attends to trajectory history, combining language and sequential visual observations via multimodal transformer layers to better ground instruction segments to past and present observations.",
            "visual_encoder_type": "Transformer-based multimodal encoder combining visual tokens (panoramic/view features) and language tokens; survey describes HAMT as transformer-based with history attention.",
            "visual_encoder_pretraining": "Not specified in survey beyond multimodal pretraining / fine-tuning on VLN datasets.",
            "grounding_mechanism": "Cross-attention between language tokens and a multimodal history of visual tokens; explicit attention over past observations to resolve references and subgoals.",
            "representation_level": "Step-level / sequence-level (attends to historical visual observations and language together).",
            "spatial_representation": "Implicit via sequence of navigable viewpoints and their visual features; explicit 3D coordinates not detailed in survey for HAMT.",
            "embodied_task_type": "Vision-Language Navigation (VLN), designed to improve long-term grounding across trajectories.",
            "embodied_task_name": "R2R and other VLN benchmarks (survey references HAMT in context of history-aware models)",
            "visual_domain": "Photorealistic simulation (egocentric views in Matterport3D-style environments)",
            "performance_metric": "VLN metrics (SR, SPL) referenced as community standards; survey does not record HAMT numeric performance here.",
            "performance_value": null,
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": null,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Survey notes HAMT addresses temporal grounding and partial-observability by attending to history, countering some failure modes like missed subgoals; nonetheless, perception (occlusions, unseen objects) and sim-to-real remain bottlenecks.",
            "failure_mode_analysis": "Survey-level observations: history-aware attention reduces some trajectory drift and ambiguity, but error compounding and failures on long instructions persist; no per-case frequencies reported.",
            "domain_shift_handling": "HAMT improves within-sim generalization via history modeling; survey does not claim it solves sim-to-real transfer.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Multimodal transformer cross-attention combining language and a history of visual tokens (early/mid fusion in transformer layers).",
            "sample_efficiency": null,
            "key_findings_grounding": "In VLN, transformer models that attend to history (HAMT) improve grounding across long instructions by providing temporal context, reducing certain classes of failure (ambiguous references, missed subgoals), but do not eliminate perception-driven failures from partial observability or unseen visual domains.",
            "uuid": "e1976.6"
        },
        {
            "name_short": "ReinforcedCrossModalMatch",
            "name_full": "Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning",
            "brief_description": "A hybrid approach for VLN combining reinforcement learning with a cross-modal matching objective and self-supervised imitation to improve robustness to deviation from reference trajectories.",
            "citation_title": "Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation",
            "mention_or_use": "mention",
            "model_name": "Reinforced Cross-Modal Matching (RCM) + Self-Supervised Imitation",
            "model_description": "Uses reinforcement learning to expose the agent to its own mistakes and a cross-modal matching objective to align visual and language modalities; self-supervised imitation augments learning by imitating rollouts and correcting drift.",
            "visual_encoder_type": "VLN-style visual encoders (CNN or panoramic feature extractors) combined with language encoders; survey describes the approach at a conceptual level.",
            "visual_encoder_pretraining": "Not detailed in the survey for this specific approach; typical VLN setups use ImageNet-pretrained CNN backbones.",
            "grounding_mechanism": "Cross-modal matching objective (encourages visual observations and instruction segments to be mutually informative) combined with RL to train policies that ground instructions into sequential actions robustly.",
            "representation_level": "Step-level grounding across trajectories; aligns segments of instructions with episodic visual observations.",
            "spatial_representation": "Implicit via viewpoint/action space; explicit spatial coordinates not described in survey.",
            "embodied_task_type": "Vision-Language Navigation (VLN)",
            "embodied_task_name": "R2R and related navigation tasks (survey cites this family of approaches in the VLN section)",
            "visual_domain": "Photorealistic simulation (Matterport3D-style indoor environments)",
            "performance_metric": "VLN metrics (SR, SPL) referenced; survey does not include numeric performance here.",
            "performance_value": null,
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": "Survey states RL + cross-modal matching improves robustness to compounding errors compared to pure supervised learning on reference trajectories; no numeric delta provided in the survey.",
            "has_encoder_comparison": null,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Survey highlights that RL-based exposure to off-reference states reduces compound errors but perception failures (occlusions, unseen object appearance, novel scenes) still dominate many failures in embodied instruction-following.",
            "failure_mode_analysis": "Survey-level failure modes: agents still fail due to partial observability, long-horizon planning errors, and perception mismatches; RL mitigates some but not all grounding failures.",
            "domain_shift_handling": "Robustness improved via RL and self-supervised imitation, but sim-to-real and domain shift remain open problems per survey.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Cross-modal matching objective + policy learning (RL) that fuses language and visual features during action prediction; mechanism is hybrid (representation alignment + policy optimization).",
            "sample_efficiency": null,
            "key_findings_grounding": "Combining cross-modal matching with RL and self-supervised imitation exposes agents to realistic offreference states and improves robustness to trajectory drift, but fundamental perception bottlenecks (partial observability, novel objects, sim-to-real) continue to limit embodied grounding performance.",
            "uuid": "e1976.7"
        },
        {
            "name_short": "ALFRED",
            "name_full": "ALFRED (Action Learning From Realistic Environments and Directives)",
            "brief_description": "A benchmark for embodied instruction following that requires agents to perform multi-step household tasks involving navigation and object manipulation guided by natural-language instructions in simulated indoor environments.",
            "citation_title": "ALFRED: A benchmark for interpreting grounded instructions for everyday tasks",
            "mention_or_use": "mention",
            "model_name": "ALFRED (benchmark/task)",
            "model_description": "Task suite requiring integrated perception, language understanding, long-horizon planning and manipulation: agents must navigate and manipulate objects according to natural-language directives in AI2-THOR simulated environments.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": null,
            "representation_level": "Multi-level (navigation-level, object/manipulation-level grounding required across steps).",
            "spatial_representation": "Requires object-centric/3D interaction (simulated 3D environment coordinates and object state), but the survey describes ALFRED mainly as a task rather than a single model design.",
            "embodied_task_type": "Navigation + Interactive Manipulation (instruction following with object interactions).",
            "embodied_task_name": "ALFRED (AI2-THOR simulated interactive tasks)",
            "visual_domain": "Photorealistic simulation with interactive objects (AI2-THOR)",
            "performance_metric": "Task completion metrics combining navigation success and correct manipulation/goal state; survey cites ALFRED as an example but does not list numbers.",
            "performance_value": null,
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": null,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Survey explicitly notes ALFRED-style tasks highlight embodied challenges: partial observability, need for long-term planning, interaction affordance perception, and error compounding across multi-step manipulations; sim-to-real differences further complicate transfer.",
            "failure_mode_analysis": "Survey-level failures for ALFRED-like tasks include compounding errors across steps, difficulty recognizing small or occluded objects for manipulation, and brittleness to visual variation; no per-method frequencies are provided.",
            "domain_shift_handling": "Survey notes sim-to-real remains a major open problem; approaches include domain adaptation and fine-tuning but no consensus remedy is given.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": null,
            "sample_efficiency": null,
            "key_findings_grounding": "ALFRED illustrates requirements for fine-grained spatial/object-centric grounding plus sequential action planning; survey emphasizes that existing vision-language grounding models need extensions (object affordance perception, persistent memory, robust spatial grounding) to succeed on interactive manipulation benchmarks.",
            "uuid": "e1976.8"
        },
        {
            "name_short": "REVERIE",
            "name_full": "REVERIE (Remote Embodied Visual Referring Expression)",
            "brief_description": "A vision-and-language navigation benchmark that combines navigation with referring expression resolution: agents must navigate to a remote location and identify a referred object described by language.",
            "citation_title": "REVERIE: Remote embodied visual referring expression in real indoor environments",
            "mention_or_use": "mention",
            "model_name": "REVERIE (benchmark/task)",
            "model_description": "Two-stage embodied task combining language-guided navigation to a region and fine-grained referring-object grounding once in the target region; evaluates both navigation and grounding accuracy.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Requires combined navigation-level grounding and region/object-level referring grounding (survey describes task structure).",
            "representation_level": "Multi-level (navigation-level to reach region, then object-level referring grounding).",
            "spatial_representation": "Implicit via navigation viewpoints and object bounding regions within target panorama; explicit bounding-box level grounding required for object identification.",
            "embodied_task_type": "Vision-Language Navigation + Referring Expression Grounding",
            "embodied_task_name": "REVERIE",
            "visual_domain": "Photorealistic simulation (Matterport3D indoor panoramas)",
            "performance_metric": "Navigation success (SR) and grounding success (correct object identification); survey mentions both metrics as part of REVERIE evaluation protocol.",
            "performance_value": null,
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": null,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Survey notes REVERIE highlights the need for persistent memory and multi-step grounding; failures often arise from inability to resolve referents from partial views and from navigation errors that prevent reaching the referent.",
            "failure_mode_analysis": "Survey-level failure modes: navigation errors leading to wrong region; inability to disambiguate similar objects from panoramic views; no per-case statistics reported in survey.",
            "domain_shift_handling": "Not detailed; survey reiterates sim-to-real as nontrivial for such composed tasks.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Combination of navigation policy conditioned on language + region/object grounding module (survey-level description).",
            "sample_efficiency": null,
            "key_findings_grounding": "Tasks that combine navigation and referring (REVERIE) expose both coarse navigation grounding and fine-grained object grounding failures; success requires multi-level grounding solutions (trajectory memory + region/object detectors) rather than solely global image–text matching.",
            "uuid": "e1976.9"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning transferable visual models from natural language supervision",
            "rating": 2
        },
        {
            "paper_title": "Grounded Language-Image Pre-training",
            "rating": 2
        },
        {
            "paper_title": "MDETR: Modulated Detection for End-to-End Multi-Modal Understanding",
            "rating": 2
        },
        {
            "paper_title": "TransVG: End-to-end visual grounding with transformers",
            "rating": 1
        },
        {
            "paper_title": "Speaker-follower models for vision-and-language navigation",
            "rating": 2
        },
        {
            "paper_title": "History aware multimodal transformer for vision-and-language navigation",
            "rating": 2
        },
        {
            "paper_title": "Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation",
            "rating": 2
        },
        {
            "paper_title": "ALFRED: A benchmark for interpreting grounded instructions for everyday tasks",
            "rating": 2
        }
    ],
    "cost": 0.0312755,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Modality-Aware Feature Matching: A Comprehensive Review of Single-and Cross-Modality Techniques
30 Jul 2025</p>
<p>Weide Liu weide001@e.ntu.edu.sg 
Wei Zhou 
Ping Hu 
Jun Cheng cheng_jun@i2r.a-star.edu.sg 
Jungong Han jungonghan77@gmail.com 
Weisi Lin wslin@ntu.edu.sg </p>
<p>College of Computing and Data Science
Nanyang Technological University
Singapore</p>
<p>School of Computer Science and Informatics
Cardiff University
UK</p>
<p>School of Computing and Communications
JUN LIU
Lancaster University
UK</p>
<p>School of Computer Science and Engineering
University of Electronic Science and Technology of China
China</p>
<p>Agency for Science, Technology and Research (A*STAR)
Singapore</p>
<p>School of Computer Science
University of Sheffield
UK</p>
<p>College of Computing and Data Science
Nanyang Technological University
Singapore</p>
<p>College of Computing and Data Science
Nanyang Technological University
Singapore, Wei ZhouSingapore</p>
<p>School of Computer Science and Informatics
Cardiff University
Cardiff, Jun LiuUK</p>
<p>School of Computing and Communications
Lancaster University
LancasterUK</p>
<p>School of Computer Science and Engineering
University of Electronic Science and Technology of China
Sichuan, Jun ChengChina</p>
<p>Agency for Science, Technology and Research (A*STAR)
Singapore, Jungong HanSingapore</p>
<p>School of Computer Science
University of Sheffield
Sheffield, Weisi LinUK</p>
<p>College of Computing and Data Science
Nanyang Technological University
SingaporeSingapore</p>
<p>Modality-Aware Feature Matching: A Comprehensive Review of Single-and Cross-Modality Techniques
30 Jul 2025644AF370F0D75EC3354D7C0BD61E359FarXiv:2507.22791v1[cs.CV]Manuscript submitted to ACM Manuscript submitted to ACMCCS Concepts:Computing methodologies → Artificial intelligenceComputer visionMatching Feature-Matching, Single-Modality, Cross-Modality, Vision-Language, Medical Image
Feature matching is a cornerstone task in computer vision, essential for applications such as image retrieval, stereo matching, 3D reconstruction, and SLAM.This survey comprehensively reviews modality-based feature matching, exploring traditional handcrafted methods and emphasizing contemporary deep learning approaches across various modalities, including RGB images, depth images, 3D point clouds, LiDAR scans, medical images, and vision-language interactions.Traditional methods, leveraging detectors like Harris corners and descriptors such as SIFT and ORB, demonstrate robustness under moderate intra-modality variations but struggle with significant modality gaps.Contemporary deep learning-based methods, exemplified by detector-free strategies like CNN-based SuperPoint and transformer-based LoFTR, substantially improve robustness and adaptability across modalities.We highlight modalityaware advancements, such as geometric and depth-specific descriptors for depth images, sparse and dense learning methods for 3D point clouds, attention-enhanced neural networks for LiDAR scans, and specialized solutions like the MIND descriptor for complex medical image matching.Cross-modal applications, particularly in medical image registration and vision-language tasks, underscore the evolution of feature matching to handle increasingly diverse data interactions.</p>
<p>Introduction</p>
<p>Feature matching is a fundamental task in computer vision, essential for various critical applications, including image retrieval, stereo matching, 3D reconstruction, and simultaneous localization and mapping (SLAM).To provide a systematic and comprehensive analysis of this domain, this survey organizes feature matching methods according to specific data modalities, covering both single-modality (RGB images, 3D data, medical images) and cross-modality (medical imaging and vision-language) scenarios.</p>
<p>We first introduce single-modality feature matching methods for RGB images and 3D data, highlighting the progression from early handcrafted detectors and descriptors to modern deep-learning-based approaches.Subsequently, we dive into medical imaging, a domain uniquely positioned at the intersection of single-and cross-modality matching, discussing the specialized approaches required for effective image registration across different medical modalities.We then address vision-language feature matching, emphasizing cross-modal alignment techniques that bridge visual and textual data.Finally, we outline potential future research directions, reflecting emerging trends and promising avenues in multi-modal and generalized feature matching.</p>
<p>In the context of RGB image matching, techniques have evolved significantly from early handcrafted methods, including corner detectors such as the Harris operator [74], robust local descriptors like SIFT [120] and SURF [15], to efficient binary descriptors exemplified by ORB [153].While these model-driven approaches proved effective for intramodality matching under moderate viewpoint and illumination changes, they often struggle with the larger domain gaps and sensing differences encountered across modalities.Recent learning-based methods have therefore proposed to overcome these limitations: for example, SuperPoint s [47] employs a self-supervised CNN detector-descriptor trained on synthetic data to achieve robust feature correspondence.LoFTR [176] advances the field further with its transformer-based matching architecture, eliminating the need for explicit keypoint detection.</p>
<p>In the domain of 3D data, which includes RGB-D, LiDAR point clouds, 3D meshes, and multi-view 2D to 3D point sets, the feature matching techniques initially relied on geometric descriptors, such as Spin Images [88] and Fast Point Feature Histograms (FPFH) [154], specifically designed to address rigid transformations and sparse data structures.</p>
<p>Recent developments have increasingly used deep learning approaches, with methods such as 3DMatch [216], FCGF [38], D3Feat [8], and transformer-based architectures such as Predator [82], significantly improving matching accuracy and robustness.</p>
<p>In medical imaging, feature matching often requires specialized strategies due to inherent intensity variations and anatomical deformations across different imaging modalities (e.g., MRI, CT, PET, ultrasound).Traditional methods such as mutual information (MI) [190] and normalized mutual information (NMI) [174] laid the groundwork for multi-modal registration.Recent deep learning-driven approaches, including VoxelMorph [9] and DiffuseMorph [96], integrate powerful unsupervised learning strategies, effectively handling deformable and intensity-based matching challenges.</p>
<p>For Vision-language feature matching, which integrates visual and textual information, driving tasks such as image captioning [188], visual question answering [4], and cross-modal retrieval [93].Key developments include contrastively trained dual-encoder models like CLIP [148] and ALIGN [84], which enable scalable open-vocabulary retrieval and classification.Visual grounding methods, exemplified by transformer-based models such as MDETR [92] and GLIP [111], have improved precise alignment between language and specific image regions.Additionally, open-vocabulary approaches have extended classification, detection, and segmentation beyond training labels, leveraging semantic embeddings from large-scale pre-training [63,106,223].Ongoing challenges include compositional reasoning, robustness, Manuscript submitted to ACM bias mitigation, and evaluation scalability [7,89,220], guiding future research toward more interactive, embodied, and continuously learning vision-language systems.Compared to existing surveys, such as Xu et al. 's work [199] (Information Fusion, 2024) examining feature matching from detector-based and detector-free perspectives, Huang et al. 's analysis [81] (IET Image Processing, 2024) distinguishing between traditional and deep learning-based detection, description, and matching methods, and Ma's comparative survey [128] (IJCV, 2021) focusing on classical versus deep learning-based image matching approaches, our survey provides unique contributions as follows:</p>
<p>(1) While previous surveys predominantly categorize methods based on algorithmic frameworks (handcrafted vs. learned, detector-based vs. detector-free), this survey systematically organizes feature matching techniques according to distinct data modalities.Specifically, we cover RGB images, depth data, LiDAR scans, 3D point clouds, medical imaging modalities (e.g., X-ray, CT, MRI), and vision-language applications.</p>
<p>(2) None of the prior surveys adequately address vision-language matching, a rapidly developing domain essential for applications such as image captioning, visual question answering, and cross-modal retrieval.Our survey explicitly highlights advancements, challenges, and methodologies pertinent to this critical area.</p>
<p>(3) We provide in-depth comparative evaluations within and across different modalities, emphasizing the evolutionary shift from traditional handcrafted techniques to sophisticated deep learning solutions.</p>
<p>In this survey, our discussion addresses unique challenges and methodologies related to single-modal feature matching (e.g., RGB, depth, medical imaging) and cross-modal scenarios (e.g., medical image registration, visionlanguage integration).We delineate the progression from classical detector-based pipelines towards contemporary detector-free solutions.Figure 1 illustrates the overall pipeline of the survey, clearly mapping the evolution of feature matching methodologies across various data modalities.Furthermore, Figure 2 offers representative examples of the results of the matching of modality-awarecharacteristics.</p>
<p>RGB Images</p>
<p>Feature matching in RGB images traditionally follows a pipeline of feature detection, feature description, and feature matching.Over the decades, a lot of methods have been proposed for each stage of this pipeline, from handcrafted approaches to more recent deep learning-based techniques.</p>
<p>Handcrafted Feature Detectors and Descriptors</p>
<p>Early work on feature matching was dominated by handcrafted methods that rely on explicit formulas and heuristics for detection and description.Notable corner detectors include the Harris detector [74], which finds points with significant intensity variation in two orthogonal directions by analyzing the autocorrelation matrix, and the Shi-Tomasi "Good Features to Track" criterion [161] that selects corners based on the smaller eigenvalue of that matrix for improved tracking stability.These corner detectors assume a fixed scale; to achieve scale or affine invariance, researchers introduced multi-scale or affine adaptation.For instance, the Harris-Laplace and Hessian-Laplace detectors [136] measure over a Laplacian-of-Gaussian (LoG) scale space to detect scale-invariant points.Another influential detector is the Maximally Stable Extremal Region (MSER) detector [133], which finds blob-like regions by thresholding intensity and selecting extremal regions that are stable across thresholds; MSER yields affine-covariant regions and was widely used in wide-baseline matching.</p>
<p>Manuscript submitted to ACM</p>
<p>RGB Image</p>
<p>3D Data</p>
<p>From RGB-D, LiDAR point cloud, 3D Mesh, Multi-view 2D to Raw 3D Point Sets</p>
<p>Medical Image</p>
<p>From MRI, CT, PET to Ultrasound</p>
<p>Vision-Language</p>
<p>Single Modality</p>
<p>Cross Modality Later, Lowe [120] proposed the Scale-Invariant Feature Transform (SIFT) for robust scale-invariant features.SIFT's detector identifies blob-like keypoints in scale-space as local maxima of Difference-of-Gaussians. Numerous subsequent methods sought improvements in speed or invariance.For example, Speeded-Up Robust Features (SURF) [15] employs an integer approximation of LoG using box filters for fast scale-space detection, and a descriptor based on Haar wavelet responses, reducing computation while maintaining robustness.Another family of fast detectors is those based on binary intensity tests, such as the FAST corner detector [152].FAST classifies a pixel as a corner by examining a small circle of neighboring pixels.</p>
<p>Handcrafted descriptors have likewise evolved.The SIFT descriptor remained dominant for its robustness, but alternatives were proposed to reduce dimensionality or computation.PCA-SIFT [94] showed that applying Principal Component Analysis to normalized gradient patches can compress SIFT's descriptor to a shorter vector while preserving discrimination.Similarly, the Gradient Location-Orientation Histogram (GLOH) descriptor [135] extended SIFT by using a log-polar location grid and PCA compression to increase distinctiveness.Later, there was a shift toward binary descriptors for efficiency in matching.BRIEF [25] was an early binary descriptor: instead of histograms, it compares pixel intensities in a smoothed patch according to a predefined random pattern of point pairs, producing a bit-string descriptor.BRIEF is very fast to compute and match, but it is not rotation-invariant.ORB [153] builds on FAST detection and BRIEF description, adding an orientation assignment and learning a rotated binary test pattern for robustness to rotation.ORB became popular as a free alternative to SIFT, offering near real-time performance while retaining some invariance [153].Other notable binary descriptors include BRISK [105], which uses patterns of point pairs at multiple   [30] on an XCAT-to-CT alignment task, showing fixed, moving, and registered images.The bottom right visualizes vision-language alignment using CLIP [148], which embeds text and image inputs into a shared representation space for contrastive learning.</p>
<p>scales and short/long pairings to improve rotation and scale invariance, and FREAK [1], which uses a retinal sampling pattern to achieve robustness with very few bits.</p>
<p>Beyond corners and blobs, researchers also developed region detectors capturing larger structures.For example, edge-based and intensity-based region detectors [186] identify invariant regions tied to image boundaries or intensity patterns.Kadir and Brady's salient region detector [91] selects regions that maximize entropy across scales, which are informative regions for matching.Another distinctive approach is the SUSAN detector [168], which defines corners via the "Univalue Segment Assimilating Nucleus" principle, avoiding explicit gradient computation and instead counting contiguous areas of similar brightness around a pixel.</p>
<p>To further improve reliability, robust estimation methods like RANSAC [57] are applied to the set of putative matches to filter out outliers by fitting a geometric model.Numerous variants of RANSAC have been proposed to improve its speed or accuracy.Notable examples include: PROSAC [39], which leverages an assumed ranking of matches by quality to prioritize sampling high-quality correspondences first, yielding faster convergence; LO-RANSAC [40] adds a local optimization step to polish the model estimate after RANSAC finds a consensus, thereby increasing accuracy;</p>
<p>GC-RANSAC and MAGSAC [13] introduce statistically robust model scoring (M-estimator sampling consensus) and Graph-Cut optimization to handle varying inlier scales and improve stability; and other specialized versions like GroupSAC [141] for situations with grouped structures, or EVSAC [58] models matching scores with extreme value theory to guide hypothesis generation.In addition, matching algorithms that enforce global consistency have been explored.For example, Leordeanu and Hebert's spectral matching [104] casts feature matching as a graph correspondence problem and uses eigen-decomposition to find an assignment that maximizes pairwise alignment consistency.Similarly, Manuscript submitted to ACM reweighted random walk matching [36] uses random walk probabilities on a correspondence graph to find a coherent set of matches.</p>
<p>Overall, handcrafted methods have been extensively evaluated in literature.Classic benchmarks by Schmid et al. [158] compared early corner detectors, introducing the repeatability metric (the percentage of true scene points re-detected under varying imaging conditions) as a standard for detector performance.Similarly, Mikolajczyk and Schmid's influential study evaluated descriptors (SIFT, PCA-SIFT, shape context, steerable filters, etc.) under rotations, scale changes, and affine transformations, using recall-precision curves to measure distinctiveness [135].Their findings confirmed SIFT's superiority among early descriptors and spurred interest in developing new ones.The combination of multi-scale detectors with SIFT or SURF descriptors and robust matching via RANSAC became the dominant paradigm for feature matching in RGB images.Open-source implementations (e.g., OpenCV [21]) further popularized these methods.However, limitations remained: handcrafted features can fail under extreme appearance changes, repetitive patterns, or a lack of semantic robustness.This opened the door for learning-based approaches to further improve the resilience and specificity of feature matching.</p>
<p>Deep Learning-Based Local Features</p>
<p>The resurgence of neural networks in vision prompted researchers to replace or augment each stage of the feature matching pipeline with learned components.Early attempts focused on learning better descriptors for patches extracted by traditional detectors.For example, instead of using SIFT's analytic descriptor, CNN-based patch descriptors were trained to discriminate between matching and non-matching image patches.One of the first such efforts compared features from convolutional neural networks pre-trained for ImageNet classification and found they could outperform SIFT on patch matching benchmarks [56].stable interest points under severe lighting changes.Another landmark was LIFT [208], which presented the first fully learned pipeline: a deep network was trained end-to-end to perform detection, orientation assignment, and description.</p>
<p>Shortly after, LF-Net [143] improved on this by formulating the detector and descriptor as a unified differentiable architecture and using self-supervision to reduce the need for 3D ground truth.LF-Net's detector was explicitly trained for repeatability and reliability, marking a step toward truly learnable detectors.</p>
<p>Another direction is joint learning of detection and description.D2-Net [49] took a bold approach: dispense with a separate detector entirely and use a deep CNN to generate a dense feature map from the image, then simply detect keypoints as local peaks in the feature maps.D2-Net showed strong results, especially in difficult conditions, and highlighted the advantage of dense, high-level features for locating useful keypoints beyond classical corner definitions.</p>
<p>However, it also produced many keypoints on textured areas and had no built-in mechanism to suppress less reliable points.This was addressed by R2D2 [150], which learned to predict not only dense descriptors but also a "reliability" score for each location.with the local descriptor using a learned fusion [125].This improved matching in ambiguous regions by mimicking how humans consider their surroundings when matching points.The trend of using context culminated in SuperGlue [156],</p>
<p>which is not a detector or descriptor but a learned matching network.SuperGlue takes as input two sets of keypoints and their descriptors (e.g., from SIFT or SuperPoint) and uses a Graph Neural Network with self-and cross-attention to directly predict an optimal matching between the two sets [156].By treating the problem as graph matching, SuperGlue can enforce one-to-one match constraints and leverage context from neighboring keypoints to resolve ambiguous matches.It significantly outperformed naive nearest-neighbor matching, essentially learning to be an "intelligent matcher" that performs mutual compatibility reasoning.SuperGlue set a new state-of-the-art in feature matching and has been used in downstream tasks like SLAM and SfM, effectively achieving an end-to-end learned matching pipeline when combined with a learned detector like SuperPoint [47].A lighter successor, LightGlue [115], further optimized this approach to run faster with minimal loss of accuracy, making learned matching more practical.</p>
<p>Another major advance was the rise of transformer-based detector-free approaches.Transformers, with their strong ability to capture long-range dependencies via self-attention, have proven very effective in matching two images without requiring explicit keypoint detection.The pioneering work in this vein is LoFTR by Sun et al. [176], which establishes correspondences on a grid of image features by alternating self-and cross-attention between the two images' feature maps.LoFTR produces dense or semi-dense matches and can handle low-texture areas by effectively "hallucinating" matches guided by global context [176].It first computes coarse matches on a downsampled image feature map and then refines matches at finer resolution, yielding highly accurate correspondences even without a traditional keypoint detector.LoFTR and subsequent transformer matchers bypass the need to detect keypoints beforehand, instead letting the network attend to all pixels and pick out matches.Variants and improvements soon followed: COTR (Correspondence Transformer) [86] uses a similar idea but with a different query mechanism for correspondence prediction; QuadTree Attention [177] and MatchFormer [192] introduced hierarchical attention to improve efficiency for high-resolution images, reducing the quadratic cost of global attention while preserving accuracy.These detector-free methods, though computationally heavier, have achieved impressive results on difficult wide-baseline problems, finding matches in scenes Manuscript submitted to ACM with weak texture or repetitive patterns where sparse detectors struggle.Dense matching networks have also been adapted for feature matching; for example, NCNet [151] performs differentiable nearest-neighbor matching followed by a neighborhood consensus network to find coherent matches across images.Such methods treat matching as a learning problem globally, rather than focusing on local descriptor invariance alone.</p>
<p>In summary, deep learning-based methods have greatly advanced the state of RGB image matching.They have produced more repeatable detectors (learning where to detect points that survive viewpoint/illumination changes), more discriminative descriptors (learning how to describe patches for higher distinctiveness), and even entirely new paradigms for matching (learning to match with attention mechanisms, and end-to-end trainable pipelines).Table 1 highlights some of the influential methods from both the handcrafted and deep learning eras.These modern techniques, when evaluated on challenging benchmarks, significantly outperform classic methods in terms of matching precision and robustness, although often at a higher computational cost.Nonetheless, due to their task-specific optimizations and ability to learn from data, deep feature matching methods have become the go-to choice in applications requiring high reliability under difficult conditions (e.g., long-term visual localization, as evidenced by SuperGlue and LoFTR's superior results in day-night matching challenges).Active research continues into making these networks faster, more generalizable, and easier to train, bridging the gap between handcrafted efficiency and deep learning performance.</p>
<p>Method Brief Description Key Advantage</p>
<p>Harris Corner [74] Detects corner points via the autocorrelation matrix; finds points with high intensity variation in orthogonal directions.</p>
<p>Very fast; high repeatability under small transformations; foundation for many later detectors.</p>
<p>SIFT [120] Scale-Invariant Feature Transform: DoG blob detector plus 128-dimensional gradienthistogram descriptor.</p>
<p>Invariant to scale and rotation; robust to viewpoint and lighting changes; highly distinctive descriptors.SURF [15] Blob detector/descriptor using Haar wavelets and integral images (approximate DoG).</p>
<p>Faster than SIFT; some affine invariance; good speed/robustness balance.FAST [152] Corner detector using machine-learned pixelintensity tests on a circle around each candidate pixel.</p>
<p>Extremely fast (real-time); widely used in SLAM and ORB pipelines.</p>
<p>ORB [153] Oriented FAST + Rotated BRIEF; binary descriptor on multi-scale FAST keypoints.</p>
<p>Very fast matching; patent-free; ideal for mobile/AR real-time use.LIFT [208] End-to-end CNN detector, orientation estimator, and descriptor trained with SfM supervision.</p>
<p>First fully learned local feature pipeline; better under challenging conditions than SIFT.</p>
<p>SuperPoint [47] Self-supervised CNN detector/descriptor.Single forward pass for thousands of keypoints; no manual labels; excellent speed/quality tradeoff.D2-Net [49] Dense CNN features: keypoints are peaks in feature maps, descriptors from the same network.</p>
<p>Joint detection/description; reliable in lowtexture areas; strong invariance from deep features.SuperGlue [156] Graph neural-network matcher refining Su-perPoint (or other) features with attention.</p>
<p>Learns globally consistent correspondences; greatly improves precision/recall.LoFTR [176] Detector-free transformer matcher producing coarse-to-fine dense matches.</p>
<p>Handles extreme viewpoint changes; dense matches without explicit keypoints.DKM [50] Dense matching framework with kernelized global matcher, stacked depthwise warp refinement, and depth-based certainty estimation.</p>
<p>Robustly handles large viewpoint variations and challenging illumination conditions with accurate dense correspondences.</p>
<p>Benchmark Datasets and Evaluation Protocols</p>
<p>The development of feature matching methods has been guided by standardized benchmark datasets and evaluation metrics.Early works utilized the Oxford affine covariant regions dataset ("Oxford benchmark"), introduced by Mikolajczyk et al. [136].This dataset provides several image sequences of the same scene with known homographies simulating viewpoint or illumination changes (e.g., Graffiti, Wall, Bark, Boat sequences).It became a classic for evaluating detectors and descriptors: repeatability of detectors is measured as the percentage of keypoints that map correctly (under groundtruth homography) to a corresponding keypoint in the transformed image, and descriptor performance is measured by the recall at a fixed false-positive rate or by plotting precision-recall of matching a set of initial correspondences.Table 2 highlights some selected influential datasets and evaluation protocols, while Table 3 summarizes method performance on the HPatches dataset [10].Repeatability and matching score for keypoint detectors / descriptors under viewpoint and lighting change.UBC PhotoTour [23] Patch correspondence (3-D scenes)</p>
<p>∼2.5 M 64×64 patch pairs from three SfM scenes (Liberty, Notre Dame, Yosemite) with ground-truth labels.</p>
<p>Training / testing local descriptors via ROC curves, FPR@TPR, etc.</p>
<p>Wide-Baseline Stereo [182] Wide-baseline stereo pairs 47 pairs from two short sequences plus additional wide-baseline images with depth maps.</p>
<p>Dense descriptor and stereo-matching evaluation under large baselines.HPatches [10] Planar homography sequences 116 six-image sequences with known homographies (59 viewpoints, 57 illuminations).</p>
<p>Standard benchmark for detection / description; homography-estimation accuracy.KITTI 2012/2015 [62] Driving stereo (outdoor) Urban stereo pairs (≈200 training + 200 test) with LiDAR ground-truth disparity / optical flow.</p>
<p>Matching accuracy and end-point error in dynamic scenes.</p>
<p>Strecha MVS [173] Multi-view; wide baseline</p>
<p>Outdoor multi-view stereo (e.g., Fountain-P11, Herz-Jesu-P8) with calibrated cameras and dense 3-D ground truth.</p>
<p>3-D reprojection error for SfM / MVS robustness to strong viewpoint change.</p>
<p>ETH3D [159] Multi-view (indoor / outdoor)</p>
<p>High-resolution images with ground-truth poses and point clouds; 20 stereo pairs + several multi-view sets.</p>
<p>Two-view matching and full SfM / MVS evaluation with precise geometry.</p>
<p>ScanNet [41] Indoor RGB-D video 1613 scans (≈2.5 M frames) with depth, ground-truth trajectories, and surface reconstructions.</p>
<p>SLAM / odometry tests: relocalization accuracy, frame-to-frame matching.</p>
<p>YFCC100M (landmarks) [179] Internet photo collections (SfM) 100 M Flickr images; landmark subsets (e.g., 4 000 pairs) with SfM poses and depth.</p>
<p>Large-scale relative-pose evaluation on crowdsourced imagery.Aachen Day-Night [157] Day vs. night localization 4 328 daytime references + 922 queries (824 day, 98 night) of urban scenes.</p>
<p>Pose recall / inlier counts across extreme illumination change.PhotoTourism (IMC 2020) [87] Crowdsourced landmark SfM Large landmark collections used in the CVPR 2020 Image Matching Challenge; COLMAP poses and depth.</p>
<p>Pose and reconstruction accuracy in widebaseline landmark imagery.ZEB (Zero-Shot Eval.) [160] Cross-domain matching Benchmark of 46 k pairs from 12 datasets (8 real, 4 synthetic) spanning resolutions, scenes, and overlaps (10-50 %).</p>
<p>Generalization test across domains; reported via pose and inlier metrics.</p>
<p>3 3D Data: From RGB-D, LiDAR point cloud, 3D Mesh to Multi-view 2D</p>
<p>3D images (such as those from RGB-D sensors or LiDAR range scans) provide geometric information that enables feature matching based on 3D shape cues rather than appearance.A rich body of work has been devoted to detecting keypoints and computing local descriptors directly on 3D data for tasks like point cloud registration, 3D object recognition,</p>
<p>Manuscript submitted to ACM Table 3. Estimation accuracy on the HPatches dataset [10].We report the Area Under the Curve (AUC) of the corner projection error at thresholds 3px, 5px, and 10px (higher is better).</p>
<p>Method AUC@3px AUC@5px AUC@10px SIFT [120] 24.0 40.0 57.0 ORB [153] 20.5 35.7 50.2 SuperPoint [47] + SuperGlue [156] 53.9 and SLAM.This section reviews both classical handcrafted approaches and deep learning-based methods for feature matching using 3D data.We also summarize common benchmark datasets and evaluation protocols, and highlight representative applications of 3D image feature matching.</p>
<p>Handcrafted Feature Detectors and Descriptors</p>
<p>Early research on depth images adapted 2D feature concepts to 2.5D range data and 3D surfaces.One seminal work is the spin image descriptor [88], which projects a 3D point's neighborhood onto a 2D histogram using radial and elevation distances to form a rotation-invariant surface descriptor.Spin images demonstrated robust object recognition in cluttered scenes using only geometric shape information [88].Another influential descriptor is the 3D Shape Context (3DSC) [60], which extends 2D shape context to 3D by histogramming the spatial distribution of neighboring points in a spherical grid around the keypoint.</p>
<p>Subsequent approaches focused on capturing local surface geometry with greater invariance and efficiency.Rusu et al. proposed the Point Feature Histogram (PFH) [155], which encodes the pairwise geometric relations (angles and distances) between points in a neighborhood.Its accelerated variant, Fast Point Feature Histogram (FPFH) [154], reduces computation by considering only a keypoint's direct neighbors for the histogram accumulation.To detect repeatable keypoints in range data, several techniques have adapted corner detectors to 3D surfaces.For example, the Harris 3D detector [167] extends the Harris corner criterion to 3D mesh geometry, and the Intrinsic Shape Signature (ISS) keypoint [222] selects points with salient local variation based on eigenanalysis of the neighborhood's covariance matrix (avoiding flat or linear regions).Researchers also developed descriptors tailored specifically for organized range images ("2.5D" data).Lo [119] introduced 2.5D SIFT, which applies SIFT-style filtering on depth images to obtain invariant keypoints and descriptors.Bayramoglu and Alatan proposed Shape Index SIFT (SI-SIFT), combining shape index maps of the depth image with SIFT to handle rotational and scale changes in range data [16].These methods leveraged the structured nature of depth images (rows and columns corresponding to sensor view) to directly extend 2D feature detectors into the depth modality.that is more rotation-robust than point-only input [45].An unsupervised variant, PPF-FoldNet [44], uses a folding-based autoencoder to learn descriptors without manual correspondences, achieving rotation invariance through on-the-fly data augmentation.</p>
<p>Another direction leverages fully-convolutional networks on sparse 3D data.Choy et al. proposed FCGF (Fully Convolutional Geometric Features) [38], which uses a 3D sparse CNN (with Minkowski convolution) to compute a dense grid of descriptors over the space, rather than extracting per-patch descriptors.Despite its efficiency, it achieves state-of-the-art accuracy on indoor (3DMatch) and outdoor (KITTI) benchmarks.Bai et al. extend this idea in D3Feat [8], which not only computes dense descriptors via a U-Net style sparse CNN but also learns to predict which points are good keypoints.By jointly training a detection and description head (with self-supervision for the detector using mutual consistency of matches), D3Feat finds repeatable keypoints and descriptors that yield improved registration results compared to using fixed detectors like ISS or random sampling [8].</p>
<p>Rotation invariance is a crucial issue for 3D features -many early learned models were not inherently rotationinvariant and relied on data augmentation.SpinNet maps the local 3D neighborhood onto a cylindrical coordinate space aligned with the keypoint's principal axes, and then applies 3D convolutions [5].This architecture produces descriptors invariant to  (2) rotations around the spin axis while preserving rich geometric detail, enabling robust alignment even under varying rotations.Another state-of-the-art approach, Predator by Huang et al. [82], incorporates an attention-based mechanism to handle the case of partially overlapping point clouds.It introduces an overlap attention module that helps the network focus on points likely to have correspondences in the other cloud.</p>
<p>Other notable deep learning contributions include CGF (Compact Geometric Features) [95], which used a random forest to pre-align patches and then a simple CNN, producing a 32-dimensional descriptor that was among the first learning-based methods to rival handcrafted features on standard datasets.3DFeat-Net [207] was a pioneering attempt to learn both detector and descriptor in a weakly-supervised manner (using GPS/INS for relative pose instead of exact Manuscript submitted to ACM Table 4.The notable works on 3D data feature matching.</p>
<p>Method Brief Description Key Advantage</p>
<p>Spin Images [88] Introduced a 2-D histogram descriptor for 3-D surfaces (spin images).</p>
<p>Rotation-invariant; robust to clutter; enabled early 3-D object recognition from depth-only data.3-D Shape Context [60] Extended shape context to 3-D point clouds for range-image matching.</p>
<p>Captures global point distribution in a local patch; effective on early range datasets.FPFH [154] Fast Point Feature Histograms for surface registration.</p>
<p>Efficient computation (lower complexity than PFH); robust on noisy scans; implemented in the PCL library.SHOT [184] Signature of Histograms of Orientations descriptor with a local reference frame.</p>
<p>Highly distinctive yet fast, rotation-invariant through an accurate LRF; widely used baseline.2.5-D SIFT [119] Adapted SIFT keypoints and descriptors to depth (range) images.</p>
<p>Reuses mature 2-D SIFT pipeline; invariant to inplane transformations on depth maps.3DMatch [216] First deep-learned local descriptor for RGB-D scan alignment.</p>
<p>Data-driven; markedly improves match recall over hand-crafted features.CGF [95] Learned Compact Geometric Features (32-D) on 3-D point clouds.</p>
<p>Very low-dimensional; efficient matching with competitive accuracy-early learning-based success.FCGF [38] Fully Convolutional Geometric Features using 3-D sparse convolutions.</p>
<p>End-to-end dense extraction; single fast forward pass; high accuracy on indoor &amp; outdoor data.D3Feat [8] Jointly learned 3-D keypoint detector and descriptor (dense CNN).</p>
<p>Simultaneously optimises detection and description; high recall; task-specific keypoints improve registration.PREDATOR [82] Overlap-aware transformer for low-overlap pointcloud registration.</p>
<p>Focuses on overlapping regions; state-of-the-art registration recall on challenging cases.Coupled Laplacian [14] Locally-aware rigid point cloud matching using graph Laplacian eigen maps with Coupled Laplacian operator.</p>
<p>Robustly captures fine local geometrical differences and handles eigen maps alignment without landmark dependency.correspondences).It targeted outdoor LiDAR point clouds and introduced an alignment loss to encourage networkpredicted keypoints to match between frames.Similarly, USIP [107] proposed an unsupervised way to learn 3D keypoint detection by maximizing repeatability.While these methods did not always outperform handcrafted detectors in all cases, they represent important steps toward fully learning-based feature matching pipelines.</p>
<p>In summary, deep learning-based approaches have significantly advanced the state-of-the-art in depth image feature matching.They achieve higher matching accuracy and robustness by learning from large data corpora of 3D scenes, capturing complex geometric patterns that handcrafted descriptors struggle to encode.However, learned descriptors typically require careful training on data similar to the target domain and may generalize poorly if the sensor or environment differs (a challenge that recent works explicitly address through rotation-equivariant designs or training on diverse datasets).The trend in current research is toward end-to-end frameworks that jointly optimize keypoint detection, local description, and even matching or pose estimation.This integration is moving the field beyond treating detection/description as independent problems, towards holistic depth-based correspondence networks that maximize overall registration or recognition metrics.Table 4 summarizes some of the most significant works in depth image feature matching, while Table 6 summarizes methods' performance on the 3DMatch dataset [216].Early milestones like spin images, 3D shape contexts, and FPFH introduced foundational concepts for describing 3D local geometry.Later, the advent of learned descriptors (3DMatch, CGF) and advanced deep architectures (FCGF, D3Feat, Predator) greatly boosted matching performance and robustness, even in very challenging scenarios.</p>
<p>Manuscript submitted to ACM Table 5.The benchmark datasets for depth-image feature matching.Each dataset contains depth (range) data plus ground truth for assessing correspondence accuracy; common evaluation protocols are listed.</p>
<p>Dataset</p>
<p>Use-case Description Typical Evaluation Protocol TUM RGB-D [175] RGB-D SLAM / odometry</p>
<p>Indoor handheld RGB-D sequences (living-room, office, etc.) with groundtruth camera trajectories.</p>
<p>Absolute / relative pose error (ATE, RPE); keypoint repeatability across frames; relocalization success rate.ICL-NUIM [73] Synthetic indoor SLAM Rendered RGB-D images of synthetic apartments with exact pose and map ground truth.</p>
<p>Same trajectory-error metrics as TUM; enables controlled, noise-free evaluation.</p>
<p>7-Scenes [163] Camera relocalization</p>
<p>Seven small indoor scenes were recorded with RGB-D; each frame has a known pose.</p>
<p>Percentage of queries within 5 cm / 5 degrees of ground truth; precision/recall of feature matches.Redwood [37] 3-D local patch matching Pairs of point-cloud fragments from indoor scenes with known relative transforms.</p>
<p>Registration recall (fraction correctly aligned using RANSAC on matches); match-recall at fixed precision.ETH Laser Reg.[145] General 3-D registration Laser-scan datasets (office, apartment, stairwell, gazebo, forest) with groundtruth poses.</p>
<p>RMS alignment error, registration success rate (converging below threshold); inlier ratio after ICP / feature matching.KITTI Odometry [62] Outdoor LiDAR odometry HDL-64 LiDAR scans of driving scenes, plus GPS/INS ground-truth poses; stereo depth for some sequences.</p>
<p>Trajectory drift per sequence; outlier ratio of feature matches, pairwise registration recall.3DLoMatch [82] Low-overlap registration Subset of 3DMatch with fragment pairs having &lt; 30% overlap.</p>
<p>Registration recall under low-overlap; inlier precision/recall; runtime for partial-overlap matching.</p>
<p>Benchmark Datasets and Evaluation Protocols</p>
<p>Research in 3d data feature matching has been accelerated by the availability of benchmark datasets and standardized evaluation protocols.We summarize some of the most widely used datasets for assessing 3D local features in Table 5.</p>
<p>These benchmarks span indoor RGB-D scans, LiDAR point cloud data, and synthetic scenes, providing a comprehensive testbed for feature matching algorithms.</p>
<p>Table 6.Feature-Match Recall (FMR) on the 3DMatch benchmark [216].(higher is better).</p>
<p>Method</p>
<p>Feature-Match Recall (%)</p>
<p>FPFH [154] 35.9 SHOT [183] 23.8 3DMatch [216] 59.6 PerfectMatch [64] 94.7 FCGF [38] 95.2 D3Feat [8] 95.8 PREDATOR [82] 96.7 SpinNet [5] 97.6 CoFiNet [210] 98.1</p>
<p>Medical Images</p>
<p>Medical image registration involves aligning multiple images into a unified spatial coordinate system, crucial for clinical and research applications such as longitudinal analysis, multi-atlas segmentation, and motion correction.This</p>
<p>Manuscript submitted to ACM alignment spans both single-modality (e.g., MRI-to-MRI) and cross-modality (e.g., MRI-to-CT, PET-to-MRI) scenarios, each presenting unique challenges due to differences in imaging characteristics and intensity distributions.</p>
<p>Handcrafted Methods</p>
<p>Traditional medical registration methods primarily fall into intensity-based and feature-based approaches.Intensitybased methods directly optimize voxel-wise similarity measures, initially employing simple metrics like sum-of-squared differences or cross-correlation for same-modality alignment [22,78].A significant advancement was Mutual Information (MI), introduced independently by Collignon et al. and Viola &amp; Wells [129,190], becoming the standard for crossmodality registration due to its robustness against varying intensity distributions between modalities.Variants like Normalized Mutual Information (NMI) further enhanced performance [174].</p>
<p>Feature-based methods rely on identifying and matching salient anatomical landmarks, edges, or surfaces.Landmarkbased registration methods, such as thin-plate splines (TPS), precisely align identified points [20], whereas surface-based methods like the Iterative Closest Point (ICP) algorithm are effective for rigid alignments [18].Modality-independent descriptors, notably the Modality Independent Neighbourhood Descriptor (MIND) [76], significantly advanced crossmodality feature matching by encoding local image structure robustly across diverse modalities.</p>
<p>To ensure physically plausible deformations, diffeomorphic methods like Large Deformation Diffeomorphic Metric Mapping (LDDMM) [17] and Symmetric Normalization (SyN) [6] were developed.Widely adopted software toolkits,</p>
<p>including Elastix [98] and ITK [213], encapsulate these robust registration methodologies, facilitating broader clinical and research adoption.</p>
<p>Deep Learning-Based Methods</p>
<p>The emergence of deep learning has profoundly impacted medical image registration by automating feature extraction and transformation prediction.Initial supervised deep methods required labeled deformation fields [170,204], but unsupervised learning paradigms, notably exemplified by VoxelMorph [9], allowed for training without explicit ground truth, optimizing similarity metrics directly via differentiable spatial transformations.</p>
<p>Later, deep architectures evolved, introducing multi-stage cascade networks for handling large deformations effectively [52,85,221].Crucially, enforcing diffeomorphism within deep networks, using frameworks like Laplacian Pyramid Network (LapIRN) [140] and VoxelMorph-diffeomorphic [42], integrated theoretical robustness with computational efficiency.Neural ordinary differential equations (ODEs) and implicit representations further advanced these frameworks by ensuring smooth, invertible transformations [71,196].</p>
<p>Adversarial training provided another avenue for enhancing registration accuracy by leveraging discriminators to critique alignments and implicitly learn robust similarity metrics [51,55,130].Transformers introduced powerful mechanisms to capture long-range spatial dependencies effectively, as illustrated by ViT-V-Net [31], TransMorph [30],</p>
<p>and XMorpher [162], demonstrating improved global registration performance.</p>
<p>Cross-modality deep learning approaches often utilize modality translation as an intermediate step via generative adversarial networks (GANs) and cycle-consistency frameworks, transforming the multi-modal registration problem into a mono-modal scenario [26,131,202].Moreover, modality-invariant embeddings and cross-modal attention mechanisms directly facilitate accurate multi-modal alignment without explicit modality conversion [166,172,191].</p>
<p>Emerging trends include employing diffusion models, such as DiffuseMorph [96], to inherently model deformation uncertainties, and exploring implicit neural representations for continuous spatial transformations [71].These methods Manuscript submitted to ACM Table 7.The influential methods on medical image registration.Each paper introduced a key concept or method; advantages are noted in context.</p>
<p>Method</p>
<p>Brief Description Key Advantage MI [190] Introduced mutual information (MI) for multi-modal alignment; maximization of MI to find the best transform.Demonstrated registration of CT/MRI with no prior segmentation.</p>
<p>Landmark method enabling fully automatic multi-modal registration; invariant to intensity scaling; became the foundation for most intensity-based multi-modal registrations.NMI [174] Proposed Normalized Mutual Information (NMI) to improve robustness of MI by compensating for overlap changes.Applied to 3D brain registration.</p>
<p>NMI handled cases with limited overlap and was less biased by areas outside common volume; became standard in applications like brain MRI-PET alignment.Maes et al. [129] Simultaneous, independent introduction of MI-based registration (with normalized MI variants) and validation on brain CT/MR and PET/MR datasets.</p>
<p>Reinforced MI as a modality-agnostic similarity.Their software became widely adopted; it showed sub-voxel accuracy comparable to experts.SyN [6] Developed SyN (Symmetric Normalization) diffeomorphic registration.Used cross-correlation as similarity (robust for multimodal MRI).Showed excellent results on inter-subject brain MRI (evaluated on cross-modality too).</p>
<p>SyN provided very accurate deformable registration with theoretical invertibility.The approach (implemented in ANTs) became top-ranked in many benchmarks and is applicable to multi-modal (with CC or MI).MIND [76] Introduced MIND descriptor for multi-modal deformable registration.Demonstrated MIND-based alignment on CT/MR chest and brain, outperforming MI on challenging cases.</p>
<p>Provided a lightweight, modality-independent feature that could plug into any registration algorithm; improved robustness to intensity distortions and outliers (e.g., pathology).Simonovsky et al. [166] Learned a CNN-based metric for multi-modal (MR-CT) patch similarity.Integrated this learned metric into a registration framework (replacing MI).</p>
<p>Pioneered the use of deep learning to directly improve the similarity measure.Achieved higher accuracy than handcrafted metrics, opening the door to learning-based similarity in registration.VoxelMorph [9] VoxelMorph unsupervised learning framework for deformable registration.Trained on MRI (later extended to CT, etc.) using spatial transformer networks and a reconstruction loss.</p>
<p>Extremely fast at test time (CNN predicts deformation in one pass); demonstrated learning-based registration can approach the quality of iterative methods while being orders of magnitude faster.Fan et al. [54] Adversarial learning for multi-modal (and mono-modal) registration.Used a generator network for deformable registration and a discriminator to judge alignment realism between MR and CT.</p>
<p>One of the first to apply GANs to registration.Improved alignment in cross-modality by learning implicit common representations; showed potential of adversarial loss to capture complex appearance differences.</p>
<p>highlight potential advantages in uncertainty modeling and diverse deformation sampling, though further investigation is needed.</p>
<p>The integration of classical rigor and deep learning's flexibility is reshaping medical image registration.While handcrafted methods remain valuable for their interpretability and theoretical guarantees, deep learning methods offer unprecedented speed and adaptability to complex scenarios.Table 7 summarizes some landmark methods in the development of medical image registration, spanning from the introduction of mutual information to recent deep learning innovations.Each has significantly influenced this field.Table 9 summarizes methods performance on the Learn2Reg dataset [219].</p>
<p>Benchmark Datasets and Evaluation Protocols</p>
<p>Research in medical image registration has benefited from several public datasets and standardized evaluation protocols, which are particularly important for multi-modal methods (where validation is tricky due to lack of ground-truth "known" transformations).Early on, the Vanderbilt Retrospective Image Registration Evaluation (RIRE) project [195] provided a framework for comparing algorithms on real patient data.The RIRE dataset includes brain images of patients with implanted fiducial markers scanned with multiple modalities (CT, MR, PET).The gold-standard transformation aligning each modality pair was obtained via the fiducials, allowing authors to quantitatively evaluate the target registration error (TRE) of their methods in millimeters [195].</p>
<p>Another influential resource is the IXI dataset, which consists of roughly 600 healthy subject brain scans with multiple MRI sequences (T1, T2, PD).IXI has been widely used for cross-modality experiments (e.g., affinely register T2</p>
<p>Manuscript submitted to ACM to T1, or synthesize one from the other) since all scans are already in a common space per subject.Similarly, the BraTS challenge datasets [134] for brain tumor segmentation provide multi-modal MRI (T1, post-contrast T1, T2, FLAIR) for each patient, which, while already roughly aligned, have been used to evaluate deformable registration algorithms that refine alignment between MRI sequences (for example, to account for brain shift).</p>
<p>For evaluation of deformable multi-modal registration, a common approach (when ground-truth alignment is unavailable) is to use surrogate ground truth via anatomical labels.That is, one can apply the candidate registration to propagate a segmentation from one image to another and then measure overlap (Dice coefficient) with a manual segmentation in the target image.This protocol is used in many benchmarks.For instance, the MM-WHS 2017 challenge (Multi-Modality Whole Heart Segmentation) [225] provided 3D cardiac MRI and CT images from multiple patients with ground-truth labels for heart substructures; although the primary goal was segmentation, many participants used registration-based approaches or evaluated how well MRI-CT registration could transfer labels.</p>
<p>The advent of learning-based methods led to community challenges to establish common benchmarks.The Learn2Reg challenges [77]  [48] demonstrated a cross-modality multi-atlas segmentation where deep learning registration was used to warp CT atlases to MR images for cardiac structure labeling, yielding accuracy close to using same-modality atlases.In radiology workflows, this could translate to, e.g., using a CT atlas to segment a corresponding MRI automatically.</p>
<p>In summary, cross-modality registration techniques have become deeply embedded in clinical systems: from neuronavigation, where MR-CT or MR-US reg is used to compensate brain shift, to radiotherapy, where PET-CT and MR-CT fusion inform treatment, and even to augmented reality (overlaying pre-op 3D imaging onto live endoscopic or laparoscopic video in the correct pose -essentially video-to-CT registration).72.43 ± 2.94 deedsBCV [200] 69.77 ± 2.74 TransMorph [30] 75.9 ± 3.19</p>
<p>5 Cross-Modality Vision-Language Feature Matching: From Traditional Methods to Large-Scale</p>
<p>Foundation Models</p>
<p>Vision-language feature matching refers to the broad set of techniques that enable joint understanding of visual content and natural language.In recent years, a variety of tasks have emerged at the intersection of computer vision and natural language processing, requiring models to align and integrate visual and textual information [148].Such tasks include image captioning [188], visual question answering (VQA) [4], cross-modal retrieval [93], among others.</p>
<p>Progress in these areas has been propelled by advances in deep learning, the development of large-scale multimodal datasets, and the introduction of powerful vision-language pre-training paradigms.This section provides a comprehensive review of the field, covering seven major areas: (  7) benchmark datasets and evaluation protocols.In addition, we identify ten particularly pivotal works across all topics and summarize them in Table 11, and we provide an overview of important datasets in Table 10.</p>
<p>Cross-Modal Retrieval and Search</p>
<p>A breakthrough in cross-modal retrieval came with contrastively trained dual-encoder models on web-scale data.CLIP used 400 million image-text pairs for contrastive learning, producing an image encoder and a text encoder whose embeddings are directly comparable via cosine similarity [148].CLIP achieved remarkable zero-shot retrieval and zero-shot classification performance, far surpassing previous supervised models when scaled.Concurrently, ALIGN trained on 1.8 billion noisy image-alt-text pairs and similarly found that simple dual-encoder architectures can be extremely effective given enough data [84].These models revolutionized cross-modal retrieval by enabling open-domain searches: e.g., using natural language queries to find images in an unseen collection.Recent research in cross-modal retrieval has explored hybrid approaches that combine the efficiency of dual encoders with the precision of interaction models.Examples include leveraging CLIP features with lightweight cross-attention re-ranking to refine results [103],</p>
<p>or learning modality-aware experts and fusion at retrieval time [223].Nonetheless, the dominant trend is using large pre-trained vision-language models and fine-tuning or prompt-tuning them for retrieval tasks.</p>
<p>Visual Grounding and Referring Expressions</p>
<p>Early work on referring expressions built upon object detection and language models.Initial approaches often used a two-stage pipeline: generate region proposals, then rank them by how well they match the query expression.For instance, Mao et al. [132] presented a CNN-LSTM model that could both generate and comprehend referring expressions.</p>
<p>Furthermore, they introduced a large-scale RefCOCO/RefCOCO+ dataset based on MSCOCO images, and evaluated comprehension accuracy by whether the model's selected region matched the ground truth.</p>
<p>Subsequent methods improved upon visual and linguistic feature fusion.Neglecting other objects [80] proposed to incorporate context such as surrounding objects' features to disambiguate references.Speaker-Listener-Reinforcer models [212] jointly trained a generator (speaker) and a comprehension model (listener) with reinforcement learning to ensure that generated expressions are discriminative and comprehensible.This approach improved grounding accuracy by using the listener's feedback as a reward for the speaker, effectively pushing the descriptions (and the comprehension)</p>
<p>to focus on uniquely identifying details.Later, MAttNet (Modular Attention Network) achieved state-of-the-art on RefCOCO by decomposing the expression into subject, location, and relationship components, each attended by a Manuscript submitted to ACM separate module that guided the visual feature processing [211].MAttNet explicitly handled attributes (e.g., color or size), relations (e.g., "next to the table"), and the target object itself, combining the evidence to score proposals.This modular design significantly improved grounding, especially for complex expressions.</p>
<p>One-Stage Grounding methods removed the need for an explicit proposal stage by directly predicting the box coordinates from language and image features [203].They formulated grounding as a regression problem conditioned on text, using techniques from one-stage detectors to output the referred object location in a single forward pass.At the same time, large multi-task vision-language models started to incorporate grounding capabilities.For example, the pre-trained VilBERT [122] and UNITER [35] models could be fine-tuned for referring expression tasks by adding an output layer that selects the region corresponding to the expression [35,122].These models leverage cross-modal attention to directly align words with image regions represented by object detection features (such as Faster R-CNN region features).</p>
<p>The introduction of transformer-based detection models further revolutionized visual grounding.MDETR is a notable example that unified object detection and grounding in one end-to-end transformer model [92].MDETR extended the DETR object detector by feeding the text encoding into the transformer decoder alongside visual features.Following MDETR, other works have improved grounded detection.Such as TransVG also used a pure transformer encoderdecoder for visual grounding, with careful feature fusion between a vision transformer and language embeddings [46].</p>
<p>Referring Transformer likewise applied a transformer architecture specialized for grounding tasks [112].These models benefit from the global context and multi-head attention of transformers to resolve ambiguous language by looking at all objects simultaneously and attending to relevant parts of the image for each phrase in the query.</p>
<p>Another line of advancement is large-scale grounded pre-training.GLIP (Grounded Language-Image Pre-training) treated every object detection training example as a phrase grounding example by converting class labels to words, and trained a model to align region proposals with those words [111].By doing so on millions of examples, GLIP learned a unified representation for detection and grounding.</p>
<p>Open-Vocabulary Recognition, Detection, and Segmentation</p>
<p>A significant challenge in computer vision is recognizing and localizing visual concepts that were not seen during training.Traditionally, vision models were limited to closed vocabularies (fixed sets of classes or labels).However, by leveraging semantic information from language, models can extend their knowledge to an open vocabulary.Following CLIP, various improvements and adaptations emerged: ALIGN [84] similarly learned dual encoders on an even larger dataset; LiT explored locking the text encoder while fine-tuning the image encoder for better transfer [217];</p>
<p>FILIP introduced finer-grained alignment at the token level, attempting to match words to image patches for improved zero-shot recognition [206].Another extension is prompt engineering and prompt learning: instead of using a simple prompt like "a photo of a [class], " methods like CoOp learn continuous prompt vectors to condition CLIP for a given downstream classification task, yielding better performance, especially in the few-shot regime [224].These approaches demonstrate the flexibility of open-vocabulary classifiers: since the image encoder is fixed, one can adapt the textual side to different tasks or domains with minimal effort.methods by incorporating class semantic embeddings from Word2Vec [137] or GloVe [144] into the network and formulating a loss that encourages detection boxes to predict those embeddings for unseen classes [12].They often relied on attribute predictions or careful handling of background regions to avoid confusion.</p>
<p>However, like classification, open-vocabulary detection saw major progress with the advent of powerful visionlanguage models.One line of work leveraged the region classification head of a detector.For example, Kim et al.</p>
<p>introduced Open-Vocabulary Region CNN, which is trained on paired image-caption data to detect objects described in captions, thereby learning to localize a wider variety of concepts than the fixed label set of detection datasets [97].</p>
<p>Another important work is ViLD [66], which took a standard detector (like Faster R-CNN) and replaced or augmented Another family of approaches trains detectors on the fly with language.For example, GLIP [111], as mentioned, unified detection and grounding: it can take arbitrary text queries and highlight those objects in the image.GLIP's training formulated detection as phrase grounding; thus it naturally handles open vocabulary by taking the object's category name as a phrase.OWL-ViT [138] similarly built on a Vision Transformer to detect objects described by text prompts, enabling flexible queries like "a decorated cake" and returning matching boxes.</p>
<p>5.3.3</p>
<p>Open-Vocabulary Semantic Segmentation.Earlier zero-shot segmentation methods often extended zero-shot classification by using word embeddings of class names and relating them to pixel-level features.Bucher et al. [24] proposed a zero-shot segmentation approach that projected image features and class embedding vectors into a common space and computed segmentation masks for unseen classes via similarity.Recently, open-vocabulary segmentation has benefited from vision-language models and multi-modal training.One notable approach is LSeg [106].LSeg introduced a transformer-based model that takes an arbitrary text label and produces segmentation masks.It uses a contrastive training objective to align pixel embeddings with text embeddings for known classes, which encourages pixels of unseen classes to naturally align with semantically similar text descriptions.Another approach is GroupViT, which combined vision transformers with CLIP pretraining to perform segmentation via a grouping process [197].OpenSeg</p>
<p>explored training a segmentation model on the union of many segmentation datasets plus image-caption data, using a text encoder to represent class labels and even long descriptions for each mask [63].Similarly, the trend of using CLIP's Manuscript submitted to ACM semantic space has influenced segmentation: for example, CLIPSeg fine-tuned CLIP to produce segmentation masks given a text prompt, essentially performing text-conditioned segmentation in a zero-shot manner [124].</p>
<p>In conclusion, open-vocabulary recognition (classification, detection, segmentation) is a clear beneficiary of visionlanguage feature alignment.By training on diverse data or explicitly coupling vision models with language embeddings, we can achieve recognition of a virtually unlimited set of concepts.Table 10 lists key datasets, typically with modified evaluation protocols to test generalization to unseen classes.</p>
<p>Multimodal Question Answering and Captioning</p>
<p>Two of the most prominent vision-language matching tasks are Visual Question Answering (VQA) and Image Captioning.</p>
<p>Both tasks require a deep understanding of the image and the ability to relate it to textual content.Early models for VQA followed an encoder-decoder paradigm: encode the image and the question, then combine these features to predict an answer.The simplest approach was to concatenate image and question feature vectors and use an MLP to classify the answer [4].Stacked Attention Networks (SAN) applied one or multiple rounds of attention on image features guided by the question embedding, allowing the model to focus on regions relevant to the question [205].For example, for "What is the man holding?", the attention would ideally highlight the man's hands and the object there.Following SAN, co-attention models were introduced [123], which not only attend to image regions but also attend to question words.</p>
<p>Another major development was the better fusion of multimodal features.Multimodal Compact Bilinear pooling combined image and text feature vectors through an outer product in a high-dimensional space (approximated by sketching) [61].Later, BAN introduced a Bilinear Attention Network that simultaneously performed attention and bi-linear fusion, yielding state-of-the-art results [67].</p>
<p>In 2017, the VQA v2 [65] dataset was released to address biases in v1 [4].It balanced the answer distribution such that each question had complementary pairs of images.This made it much harder for models to guess based on priors (e.g., in v1, many questions "What color is the grass?" had answer "green").The field responded by focusing more on visual understanding.Anderson et al. introduced Bottom-Up and Top-Down Attention [2].They provided "bottom-up" region features: instead of CNN grid features, they used Faster R-CNN to propose object regions, each with a feature vector.</p>
<p>Besides standard VQA, related tasks include Visual Dialog [43], where a series of questions are asked about an image and the model must maintain context of the dialog, and Visual Commonsense Reasoning (VCR) [215], where the task is to answer a question that requires commonsense or inference beyond the image.fed that as the initial hidden state of an LSTM, which generated the caption word by word [189].However, "Show and</p>
<p>Tell" sometimes makes obvious mistakes by focusing only on the most salient object or failing to mention important details.Xu et al. addressed this with Show, Attend and Tell, introducing attention to caption generation [198].Their model learned to attend to different parts of the image at each word-generating step.</p>
<p>Manuscript submitted to ACM Over the next few years, improvements in captioning came from various directions.Better vision features: using ResNet or EfficientNet features, and later the bottom-up region features [2].Better training losses: traditional training optimized next-word prediction, which often led to generic captions.CIDEr optimization via reinforcement learning (Self-Critical Sequence Training, SCST) was a game changer [149].SCST used the evaluation metric (CIDEr) as a reward, adjusting the LSTM's generation policy to directly maximize that reward.The use of explicit object tags: OSCAR proposed adding detected object tags as input tokens to the transformer model during pre-training and fine-tuning [113].For captioning, this provided the model with a set of salient visual words (like "dog", "grass") that helped it ground the generated caption.OSCAR and its improved version VinVL [218] achieved new state-of-the-art results on MSCOCO captioning.</p>
<p>The latest trend is unifying captioning with other tasks using powerful generative models.SimVLM treated captioning as a language modeling problem with an image prefix [194].It was pre-trained on vast image-text data end-to-end and achieved excellent captioning results, even surpassing human performance on some metrics.Google's PaLI further scaled such an approach to billions of parameters and multilingual data, enabling captioning in multiple languages [34].</p>
<p>And BLIP [109] utilizes similar models to connect vision encoders with large language models, which can be prompted to produce very fluent and contextually rich captions or answers about an image.These large models can integrate world knowledge (from the language model) with visual content, resulting in captions that are more informative or contextual.</p>
<p>Embodied AI and Vision-Language Navigation</p>
<p>Embodied AI refers to AI agents that interact with environments, often in a physical or simulated 3D space.One of the flagship tasks in this category is Vision-Language Navigation (VLN), where an agent is placed in an environment and must follow a natural language instruction (e.g., "Go down the hall and turn left into the kitchen, then stop by the refrigerator") to reach a goal location.The VLN task was popularized by Anderson et al., who introduced the Room-to-Room (R2R) dataset [3].R2R provided 7,189 paths in simulated houses (Matterport3D environments [27])</p>
<p>with corresponding English instructions.The agent's objective is to navigate from a start to an end location purely by following the instruction.</p>
<p>Initial approaches to VLN used a sequence-to-sequence model with attention: an LSTM to encode the instruction, another LSTM to decode actions, attending to the instruction context at each step [3].One key difficulty in VLN is the mismatch between how instructions are given and how agents are trained.Early models struggled with generalization: they overfit to training environments, partly due to limited data.Research addressed this with data augmentation and better learning methods: Speaker-Follower Models introduced an idea of synthetic instruction-path pairs [59].RCM added an auxiliary progress monitor and used rewards for moving closer to the goal described by the instruction [193].</p>
<p>RL helps because it exposes the agent to its own mistakes during training and teaches it to correct course, whereas pure supervised learning can be brittle if anything goes off the reference path.Ma et al.added a self-monitoring component where the agent predicts how much of the instruction has been completed at each step [127].More recently, HAMT [32] uses transformers and attention over history, effectively remembering where the agent has been and what was seen, similar to memory in navigation.</p>
<p>Beyond R2R, many variants of the task were created: Room-Across-Room (RxR) provided multilingual instructions (English, Hindi, Telugu) and longer instructions per path, increasing diversity and requiring models to handle multiple languages [100].Touchdown addressed outdoor navigation in Google Street View with very long instructions and a final "touchdown" location to pinpoint [29].Vision-and-Dialog Navigation (CVDN) combined navigation with</p>
<p>Manuscript submitted to ACM Accuracy on open-ended answers.Also provides consistency and validity metrics (consistency: answer similar questions similarly; validity: answer is in a plausible range).Usually evaluated on a balanced test split with ∼3M questions.CLEVR [90] Diagnostic VQA 100K synthetic images of 3D shapes and 1M generated questions testing compositional reasoning (counting, comparing attributes, logic).</p>
<p>Accuracy (exact match) on each question type.Because the dataset is balanced and free of bias, overall accuracy reflects reasoning ability.Human performance is nearly 100%; model performance indicates specific reasoning failures.Visual Dialog [43] Imagegrounded Dialogue 123k dialogs (QA sequences) on COCO images.Each dialog has 10 question-answer pairs, where each question is asked based on the image and conversation history.</p>
<p>Two modes: retrieval (choose the correct answer from 100 candidates at each turn, evaluated by mean reciprocal rank, Recall@1) and generative (free-form answer generation, evaluated by BLEU or human judgment).Also, track dialog consistency and use of image evidence qualitatively.Roomto-Room (R2R) [3] Vision-Language Navigation 7.2k navigation instructions for 662 paths in Matterport3D houses.Each instruction averages 29 words describing a route through connected panoramic views.Success Rate (SR): fraction of trajectories where the agent's end location is within 3m of the goal.SPL: SR normalized by path length (penalizes longer-thannecessary routes).Also, Oracle's success (if any visited location was the goal).Evaluated on unseen houses in the val/test.RxR [100] Multilingual VLN 126k instructions (in English, Hindi, Telugu) for 12k paths in Matterport3D.Richer, more diverse instructions, some with multiple sentences.</p>
<p>Same metrics as R2R (SR, SPL), reported per-language as well.Tests cross-lingual generalization (training often on English and testing on other languages).</p>
<p>REVERIE [147]</p>
<p>VLN + Referring Uses Matterport3D scenes: 10k navigation instructions that refer to remote objects (not visible from start).Combining navigation to a room and then identifying a referred object by a short phrase.</p>
<p>Two metrics: Navigation success and Grounding success."Success on Remote Vision-and-Dialog" is measured by both navigation and identification being correct.</p>
<p>LVIS [70] Large-Vocabulary Detection/Segmentation 164k images (subset of COCO) with 1203 object categories (long-tail distribution).Each image has segmentation masks for present objects.</p>
<p>Detection/Seg: Average Precision (AP) computed for all classes, as well as AP on frequent, common, and rare splits of classes.Open-vocabulary detection methods often train on base classes and evaluate on rare (unseen) classes of LVIS to measure generalization.</p>
<p>Table 11.The vision-language methods across different topics, with a brief description and their key advantages.</p>
<p>Paper Description Key Advantage</p>
<p>Karpathy et al. [93] Introduced a deep neural model aligning image regions and words for caption generation and retrieval.</p>
<p>Pioneered joint vision-language embeddings; enabled bidirectional imagecaption retrieval on Flickr/MSCOCO.Show, Attend and Tell [198] Applied attention mechanisms to image captioning (CNN + RNN) to focus on relevant image parts.</p>
<p>First use of visual attention in captioning; improved descriptive detail and interpretability of image descriptions.VQA v1 [4] Created the first large Visual Question Answering dataset and baseline model combining CNN and LSTM features.</p>
<p>Established VQA as a benchmark task; spurred extensive research into visionlanguage reasoning.Bottom-Up and Top-Down Attention [2] Used object detection features and two-stage attention for captioning/VQA.</p>
<p>Significantly improved VQA and captioning performance by leveraging detected objects as visual tokens.Room-to-Room (R2R) [3] Introduced vision-language navigation with the agent following natural-language instructions in real scenes.</p>
<p>Launched embodied vision-language navigation research; provided a benchmark for grounded instruction following.ViLBERT [122] Proposed a two-stream transformer pre-trained on image-text data for downstream VQA, captioning, etc.</p>
<p>One of the first vision-language BERT models; demonstrated the power of large-scale multimodal pre-training.UNITER [35] Unified single-stream transformer for vision-language tasks with extensive pre-training on image-text pairs.</p>
<p>Achieved state-of-the-art on multiple tasks (VQA, retrieval, grounding) by joint encoding of images and text in one transformer.CLIP [148] Learned visual and textual encoders jointly via contrastive learning on 400M image-text pairs (web data).</p>
<p>Produced a powerful generic representation; enabled open-vocabulary image recognition and retrieval with zero-shot transfer.MDETR [92] Proposed a transformer that modulates DETR with text to directly ground expressions in images (object detection + language).</p>
<p>Unified referring expression comprehension with object detection; achieved strong phrase grounding by end-to-end training on aligned data.GLIP [111] Unified object detection and phrase grounding in a pretraining framework on aligned visual-language data.</p>
<p>Enabled open-vocabulary detection; excelled in detecting and localizing novel objects described by text without taskspecific training.DeepSeek-VL [121] Open-source vision-language model featuring a hybrid vision encoder, modality-balanced pretraining, and realworld-aligned supervised fine-tuning.</p>
<p>Demonstrates robust multimodal understanding and superior performance in real-world scenarios and across diverse vision-language benchmarks.a dialog agent, where the agent could ask questions when confused [178].This involves a "guide" and a "follower" having a dialog, bringing language interaction into the loop.REVERIE was a task that combined VLN with object grounding: the instruction refers to an object that is not explicitly named by category [147].The agent must navigate to the correct room and also identify the referent object.This bridges navigation with referring expression resolution.</p>
<p>ALFRED took embodiment further by adding interaction: ALFRED is an indoor instructional task where the agent has to not just navigate but also manipulate objects to complete a task (like "put a potato in a microwave and turn it on") [164].This requires vision, language, and action planning, including picking up objects, using receptacles, etc., in a simulated environment (AI2-THOR simulator [99]).</p>
<p>Manuscript submitted to ACM Embodied vision-language tasks have revealed some unique challenges not present in static tasks: The need to handle partial observability (the agent only sees what's in front of it; it must remember or explore for unseen info).</p>
<p>Longer-term planning: instructions can be long, and subgoals might need to be inferred.Error compounding: A single missed instruction step can lead the agent far off course.Also, sim-to-real gap: ultimately, we want agents that can do this in the real world.Issues like different visuals, continuous motion, and safety come into play.</p>
<p>Despite these challenges, embodied AI with language grounding is making strides.The combination of computer vision for perception, NLP for understanding and generation, and reinforcement learning for decision making makes it one of the most interdisciplinary and rich areas of AI.It is likely to benefit further from the trend of foundation models: e.g., a large language model could be used to better interpret instructions or to handle dialogues (by providing common-sense reasoning), while a vision model like SAM could help with object detection and manipulation.Integrating those into a single agent architecture is an open research question.</p>
<p>Benchmark Datasets and Evaluation Protocols</p>
<p>Benchmarks have played a crucial role in driving progress in vision-language research, while Table 12 summarizes method performance on the MSCOCO dataset [114] for the Image-text retrieval task.Table 10 summarizes some of the most important datasets across the tasks discussed, along with their typical use-cases and evaluation metrics.Here we provide additional context on these datasets and how evaluation is conducted, as shown in Table 10.</p>
<p>Table 12.Image-text retrieval results on MSCOCO (5K test set) [114].We report Recall@1 for image-to-text (I2T) and text-to-image (T2I) retrieval (higher is better).This survey provides extensive coverage, clearly differentiating handcrafted from learned methods and systematically comparing detector-based versus detector-free strategies.Additionally, it offers detailed insights into relevant datasets, evaluation protocols, and practical applications.Nevertheless, it acknowledges potential limitations, including possible gaps in benchmarks and challenges in keeping pace with the rapidly evolving landscape of deep learning methodologies.</p>
<p>Method</p>
<p>Robust feature matching remains fundamental to various computer vision tasks, including 3D reconstruction, simultaneous localization and mapping (SLAM), and object recognition.Despite deep learning-driven improvements, persistent challenges include limited generalization across diverse domains, computational inefficiencies restricting real-time applicability, and inherent complexities of cross-modal feature matching.</p>
<p>Looking ahead, the field is at an exciting juncture with promising opportunities to bridge traditionally isolated modalities within cohesive frameworks.Future research should prioritize developing adaptable, multi-modal pipelines that integrate RGB images, depth data, LiDAR scans, 3D point clouds, medical imaging, and vision-language modalities into unified systems.</p>
<p>Key future directions include, but are not limited to:</p>
<p>• Modality-Agnostic Representations: Developing robust, generalized representations that effectively bridge diverse sensor types and imaging conditions.Leveraging advances in self-supervised learning, foundation models, and transformer architectures will be crucial for achieving robust representations and reducing modality-aware engineering overhead.• Computational Efficiency: Emphasizing the design of lightweight and resource-efficient models [116][117][118] that adapt dynamically to context and available modalities.Optimizing network structures and integrating hardware acceleration will significantly enhance real-time and mobile application deployment.</p>
<p>• Comprehensive Benchmarking: Establishing realistic and extensive benchmarking protocols tailored specifically to multi-modal and real-world scenarios.These benchmarks will provide critical guidance to align research efforts with practical application needs and to promote the development of integrated, robust, and universally applicable feature matching solutions.</p>
<p>• Foundation and Generative Models for Multi-Modal Matching: The rise of large foundation models offers a path toward versatile cross-modal feature matching.Models like CLIP and BLIP already learn alignments between vision and language.Extending this paradigm, future work can develop foundation models that jointly encode RGB images, 3D scans, medical images, and more into a shared representational space.Generative models like diffusion networks also hold potential: their cross-modal capabilities (e.g., text-to-image generation) and rich internal representations can be repurposed for feature matching tasks.For instance, the attention maps of a text-to-image diffusion model have been used to align semantic regions across modalities.By leveraging large-scale pretraining and billion-parameter models, these approaches aim for high generalizability: a single model could be tuned for diverse matching tasks with minimal modality-aware engineering.</p>
<p>• Unified Multi-Modal and Multi-Task Frameworks: An ambitious yet increasingly tangible goal is a unified architecture capable of handling multiple modalities and tasks within one system.Instead of maintaining separate pipelines for RGB vs. depth vs. text, or for retrieval vs. registration vs. captioning, a single framework could flexibly accommodate all.Such a unified model can simultaneously perform image-text retrieval, 3D scene alignment, medical image registration, or even captioning and question-answering, by simply switching inputs and prompts.The benefits of unification include efficiency and seamless modality interplay.The trend toward Manuscript submitted to ACM "all-in-one" models is growing, and we anticipate systems that can ingest anything from natural images to point clouds to radiology scans, and produce whichever output is required, all under a cohesive framework.</p>
<p>• Lifelong Learning and Continual Adaptation: Finally, future feature matching systems are expected to learn continuously, adapting to new modalities and tasks over time without forgetting past knowledge.Lifelong learning would enable a deployed matching system to self-improve as it encounters novel conditions.For example, a robot's matching module could acquire new skills when a new sensor (thermal camera, ultrasound, etc.) is added, or a medical image registration network could continually update itself as it sees new types of scans.</p>
<p>Research may exploit strategies like experience replay, modular expansion, or meta-learning to allow models to evolve.Few-shot learning will be crucial while the aim is to incorporate drastically different modalities or tasks using only a small number of new examples.</p>
<p>Fig. 2 .
2
Fig. 2. Illustration of modality-aware feature matching.The top row shows examples of single-modality matching.The RGB image pair demonstrates keypoint matching using SIFT [120].The 3D data shows local feature correspondences extracted by D3Feat [8].The bottom row shows examples of cross-modality matching.The left presents medical image registration results from TransMorph[30] on an XCAT-to-CT alignment task, showing fixed, moving, and registered images.The bottom right visualizes vision-language alignment using CLIP[148], which embeds text and image inputs into a shared representation space for contrastive learning.</p>
<ol>
<li>
<p>3 . 1
31
Open-Vocabulary Image Recognition.Early work in zero-shot image classification predated the deep learning wave and often relied on human-defined attributes or semantic word embeddings.Lampert et al.[101] introduced an attribute-based classification approach: models were trained to predict intermediate attributes (like "has stripes" or "has fur") on seen classes and then infer unseen classes by their attribute signature.This idea of between-class attribute transfer allowed recognition of new categories (e.g., "zebra") by reasoning about attributes (striped, four-legged, etc.) even if no zebra images were in the training set.Later, Socher et al. proposed mapping image features and class name embeddings (obtained from text corpora) into a common space, enabling zero-shot recognition by selecting the class whose embedding is closest to the image embedding[169].Norouzi et al. extended this by introducing ConSE, which averaged the embeddings of predicted seen classes to synthesize an embedding for the image and compared it to unseen class embeddings[142].</p>
</li>
<li>
<p>3 . 2
32
Open-Vocabulary Object Detection.Extending open-vocabulary recognition to object detection is more complex, as it requires unseen classes in images.Early zero-shot detection approaches adapted zero-shot classification techniques to the detection pipeline.For instance, Bansal et al. proposed one of the first zero-shot object detection</p>
</li>
</ol>
<p>its classifier with CLIP's image-text similarity: the region-of-interest features from the detector were matched to text embeddings of class names.Essentially, they transferred the classifier's knowledge to an open vocabulary by using CLIP's text encoder as the classification layer.Similarly, RegionCLIP went a step further by also fine-tuning the image encoder at the region level: it generated region proposals and paired them with caption segments during training to better align region features with text[223].By iteratively refining region-text alignment, RegionCLIP improved open-vocabulary detection performance, especially on smaller or harder-to-recognize objects.</p>
<ol>
<li>
<p>4 . 1
41
Visual Question Answering (VQA).VQA was introduced as a grand challenge for vision and language by Antol et al., who released the VQA v1 dataset containing open-ended questions about MSCOCO images[4].Each question in VQA comes with a free-form answer.</p>
</li>
<li>
<p>4 . 2
42
Image Captioning.Image captioning is the task of generating a natural language description of an image.Show and Tell by Vinyals et al. (2015) was a landmark work: it used a CNN to encode the image into a feature vector, then</p>
</li>
</ol>
<p>6</p>
<p>Conclusion and Future DirectionsThis survey comprehensively synthesized feature matching methods across diverse modalities, including RGB images, depth (RGB-D), LiDAR, 3D point clouds, medical imaging, and vision-language tasks.A prominent trend observed is the shift from traditional handcrafted techniques such as SIFT and SURF towards advanced deep learning approaches, which offer enhanced accuracy and adaptability.Additionally, detector-free architectures, exemplified by integrated solutions like SuperGlue and LoFTR, signify an important evolution towards robust, unified matching pipelines.Feature matching methods have effectively adapted to the unique challenges posed by each modality.For instance, techniques designed for depth and LiDAR data emphasize geometric invariance, whereas solutions for medical imaging primarily address intensity variations.Despite these advancements, cross-modal matching continues to present significant challenges due to substantial representational disparities among different modalities.Addressing these requires either specialized cross-modal strategies or the development of truly modality-agnostic representations.Manuscript submitted to ACM</p>
<p>Table 1 .
1
The local feature matching methods for RGB images.</p>
<p>Table 2 .
2
The public academic datasets commonly used to evaluate single-modality RGB feature matching.
DatasetUse-caseDescriptionTypical Evaluation ProtocolOxford Affine [135]Planar scenes; viewpoint40 image pairs (8 scenes, 5 transformations/ illumination changeeach) with known homographies.</p>
<p>[201]hieve rotation invariance in 3D, many handcrafted descriptors compute a local reference frame (LRF) at each keypoint.The Unique Shape Context (USC) of Tombari et al. is an LRF-aligned extension of 3D shape context that avoids multiple descriptors per point by consistently orienting the neighborhood[183].Tombari et al. also developed the SHOT descriptor (Signature of Histograms of Orientations)[184], which builds an LRF and then aggregates point counts weighted by normals into a set of angular histograms.Other notable descriptors include Spin Images variations Manuscript submitted to ACM (e.g., Tri-Spin Images integrating multiple spin image projections)[69], 3D SURF adaptations using integral images on depth data, and RoPS (Rotational Projection Statistics) by Guo et al., which projects the 3D neighborhood onto multiple planes and computes statistical moments to form a descriptor[68].RoPS, coupled with a robust LRF, was shown to be highly distinctive for object retrieval and recognition in 3D scenes, outperforming many earlier features under clutter and occlusions[68].Other notable descriptors include TOLDI[201], which generates a Triple Orthogonal Local Depth Image descriptor by projecting the local surface onto three orthogonal planes.However, these methods can be sensitive to noise, point density variations, and occlusions.Moreover, they often produce high-dimensional descriptors and require fine-tuned parameters for each dataset.Despite these limitations, handcrafted features laid the groundwork for depth image matching and are still employed in real-time systems and as benchmarks.They enable correspondence matching in applications like coarse object alignment and SLAM loop closure detection, where no training data are available.</p>
<p>[45]]eep Learning-Based MethodsOne challenge in learning 3D features is how to represent the input.Different strategies have been explored: 3DMatch used voxel grids with truncated distance function values as input[216].Other works project local 3D surfaces into multi-view images: for example, Bai et al. in D3Feat mention that early attempts like Zeng's volumetric CNN and multi-view approaches were later complemented by fully point-based networks.Point-based methods, inspired by PointNet[146], feed raw point coordinates (or point pairs) into a network.PPFNet[45]is one such example: it augments PointNet with point-pair features (distances and angles) to incorporate geometric context, yielding a learned descriptor</p>
<p>Table 8 .
8
The public datasets for medical image registration, with modality details, use cases, and evaluation notes.
DatasetModalityUse-CaseDescription &amp; Evaluation ProtocolRIRE [195]CT, MR, PETRigid reg., algo-Retrospective Image Registration Evaluation: Brain images with implanted(Brain)rithm compari-markers. Gold-standard rigid transforms known via fiducials. Error measuredsonas TRE (mm) at marked anatomical targets; enabled first objective rankingof multi-modal reg methods.IXI [19]MRI (T1, T2,DeformableLarge collection of normal brain MRI in multiple sequences for each sub-PD) (Brain)reg., synthesisject. Used for cross-sequence registration and learning modality mappings.Evaluation by mutual information or downstream task (e.g., segmentationconsistency).BraTS [134]MRI (T1, T1c,DeformableBrain Tumor Segmentation Challenge data. Multi-parametric MR per patientT2, FLAIR)reg.,tumor(already roughly aligned). Registration methods tested by aligning sequences(Brain)analysisor longitudinal scans; eval via overlap of tumor/structure labels or visualconsistency of tumor boundaries.MM-WHS [225] MRI,CTCross-modalityMulti-Modality Whole Heart Segmentation Challenge: 60 cardiac CT and 60(Heart)segmentationMRI from different patients with labeled structures. Not paired, but used to(indirect reg)test reg-by-seg approaches and domain adaptation. Reg algorithms can mapan atlas from CT to MRI (or vice versa); evaluated by Dice of propagatedlabels and surface distances.Learn2Reg [77] CT/MRMultiple (chal-(abdomen),lenge tasks)MR/US(brain), oth-ers
Challenge with multiple multi-modal registration tasks: e.g., intra-patient liver CT-MRI, neurosurgical MR-US.Provided training and test splits.Evaluation via hidden ground truth or expert landmark analysis; metrics included TRE on hidden landmarks, Dice of organ masks, and fold count in deformations.Established a common benchmark for learning-based methods.</p>
<p>[171]duced multiple tasks, including cross-modality ones such as deformable CT-MRI abdomen registration (intra-patient scans from CT and MRI of the same subject's torso) and MRI-ultrasound brain registration (from the Cross-modality registration is a cornerstone of many medical imaging workflows, enabling synergistic use of different imaging technologies.A primary application is in diagnostic imaging fusion.For example, in neuroimaging, MRI provides high anatomical detail and PET provides metabolic information; registering MRI and PET of the brain allows neurologists and neurosurgeons to accurately localize PET findings (e.g., epileptic foci or tumor activity) on the patient's MRI[171].This MRI-PET fusion, often performed with rigid MI-based registration in commercial software, has become routine in brain tumor and dementia evaluations.Likewise, combining CT and MRI is valuable in head and neck cancers and pelvic cancers, where MRI delineates soft-tissue tumor extent and CT provides electron density for treatment planning.Rigid or deformable CT-MRI registration is used to bring MRI into the CT space for radiation therapy planning; this is challenging around organs that deform or where MRI and CT have different contrast (e.g., brachytherapy for cervical cancer requires MR-CT alignment of pelvic organs).Multi-modal deformable algorithms (often using mutual information or contour-based landmark alignment) are employed, with accuracy directly impacting treatment margins.
4.4 ApplicationsCuRIOUS 2018 challenge). Participants trained algorithms (often deep learning-based) and were evaluated on held-outtest cases via metrics like TRE on landmarks, Dice on propagated labels, and deformation field statistics. These challengesnot only yielded open datasets but also established modern evaluation protocols: e.g., evaluating inverse-consistencyerror, Jacobian determinant maps to check for non-physical folds in the deformation, and run-time/memory usagecomparisons.Manuscript submitted to ACM
Another major application area is image-guided surgery and intervention.Here pre-operative images (MRI or CT) must be aligned to intra-operative imaging (ultrasound, fluoroscopy, or CT) to guide the surgeon.A prominent example is MRI-to-ultrasound registration during neurosurgery for brain tumor resection.Pre-op MRI shows the tumor and critical structures, while intra-op ultrasound is used to visualize brain shift after skull opening.By deformably registering MRI to the ultrasound, updated neuro-navigation can be achieved, improving surgical accuracy.Successful implementations of MR-US registration (using techniques like MIND descriptors or learned features) have been reported, and databases like RESECT and BITE have fostered research in this domain.Similarly, in prostate cancer biopsies, transrectal ultrasound (TRUS) is used in real time, but suspicious lesions are identified on MRI beforehand; MR-TRUS registration (rigid + deformable) allows the MRI lesions to be mapped onto the ultrasound for targeted needle guidance.Another application is multi-modal atlas construction and segmentation.By registering a set of delineated atlases in one modality to a target image in another modality, one can propagate labels for organs or pathologies.Ding et al.</p>
<p>Table 9 .
9
[219] MRI inter-patient registration on Learn2Reg[219].Performance is measured by mean Dice similarity (%) on anatomical structures (higher is better).
MethodDice (%)uniGradICON [180] 73.69 ± 4.12VoxelMorph [9]71.86 ± 3.40SynthMorph [79]</p>
<p>Table 10 .
10
The vision-language datasets with their typical use-cases, content description, and evaluation protocols.These benchmarks have driven the development of methods discussed in this survey.
DatasetUse-caseDescriptionEvaluation ProtocolMSCOCOImage Caption-123,000 images (COCO) each with 5 human-Caption: Automatic metrics (BLEU, METEOR, ROUGE,Captionsing, Retrievalwritten captions. Diverse everyday scenes withCIDEr); human evaluation for fluency/accuracy. Re-[33]people, objects, and activities.trieval: Recall@1,5,10 on 1K test images (e.g., COCOKarpathy split).Flickr30kCaptioning,31,000 web images with 5 captions each. OftenRetrieval: Train/test sets with standard 1K test images,[209]Image-Textsingle-object or simple scenes (people and ani-measure Recall@K. Captions: similar automatic metricsRetrievalmals in various activities).as COCO (though COCO is primarily for captioning).RefCOCOReferringImages from COCO with 50k+ referring expres-Accuracy of selecting the correct region given an ex-[132, 212]Expressionsions for 20k objects. RefCOCO/RefCOCO+pression. Evaluated on val/test splits, sometimes splitComprehen-focus on multiple objects, short phrases; Ref-by whether multiple instances of the object class aresionCOCOg has longer, more complex expressions.present.VQA [65] Visual Q&amp;A∼204,000 images (COCO) with 1.1M questions,Accuracy computed by agreement with human answers:each with 10 free-response answers (to reducean answer is correct if at least 3 of 10 annotators gaveguess bias). Questions across categories: "Whatthat answer. Report overall accuracy and per-question-is. . . ?", "How many. . . ?", etc.type accuracy on test set.GQA [83] Compositional113K images (VG) with 22M synthesized ques-VQAtions that require multi-step reasoning (bal-anced to reduce language bias). Annotationsinclude structured scene graphs for the images.
Manuscript submitted to ACM</p>
<p>Freak: Fast retina keypoint. Alexandre Alahi, Raphael Ortiz, Pierre Vandergheynst, 2012 IEEE conference on computer vision and pattern recognition. Ieee2012</p>
<p>Bottom-up and top-down attention for image captioning and visual question answering. Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, Lei Zhang, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, Anton Van Den, Hengel, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>Vqa: Visual question answering. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, Devi Parikh, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision2015</p>
<p>Spinnet: Learning a general surface descriptor for 3d point cloud registration. Sheng Ao, Qingyong Hu, Bo Yang, Andrew Markham, Yulan Guo, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2021</p>
<p>Symmetric diffeomorphic image registration with cross-correlation: evaluating automated labeling of elderly and neurodegenerative brain. Charles L Brian B Avants, Murray Epstein, James C Grossman, Gee, Medical image analysis. 122008. 2008</p>
<p>Dzmitry Bahdanau, Timothy J Harm De Vries, Shikhar O'donnell, Philippe Murty, Yoshua Beaudoin, Aaron Bengio, Courville, arXiv:1912.05783Closure: Assessing systematic generalization of clevr models. 2019. 2019arXiv preprint</p>
<p>D3feat: Joint learning of dense detection and description of 3d local features. Xuyang Bai, Zixin Luo, Lei Zhou, Hongbo Fu, Long Quan, Chiew-Lan Tai, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020</p>
<p>Voxelmorph: a learning framework for deformable medical image registration. Guha Balakrishnan, Amy Zhao, John Mert R Sabuncu, Adrian V Guttag, Dalca, IEEE transactions on medical imaging. 382019. 2019</p>
<p>HPatches: A benchmark and evaluation of handcrafted and learned local descriptors. Vassileios Balntas, Karel Lenc, Andrea Vedaldi, Krystian Mikolajczyk, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>Learning local feature descriptors with triplets and shallow convolutional neural networks. Vassileios Balntas, Edgar Riba, Daniel Ponsa, Krystian Mikolajczyk, Bmvc. 20161</p>
<p>Zero-shot object detection. Ankan Bansal, Karan Sikka, Gaurav Sharma, Rama Chellappa, Ajay Divakaran, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)2018</p>
<p>Graph-cut RANSAC. Daniel Barath, Jiří Matas, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>Coupled Laplacian Eigenmaps for Locally-Aware 3D Rigid Point Cloud Matching. Matteo Bastico, Etienne Decencière, Laurent Corté, Yannick Tillier, David Ryckelynck, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Surf: Speeded up robust features. Herbert Bay, Tinne Tuytelaars, Luc Van Gool, Computer Vision-ECCV 2006: 9th European Conference on Computer Vision. Graz, AustriaSpringer2006. May 7-13, 2006Proceedings, Part I 9</p>
<p>Shape index SIFT: Range image recognition using local features. Neslihan Bayramoglu, Aydin Alatan, 2010 20th International Conference on Pattern Recognition. IEEE2010</p>
<p>Computing large deformation metric mappings via geodesic flows of diffeomorphisms. Michael I Beg, Alain Miller, Laurent Trouvé, Younes, International journal of computer vision. 612005. 2005</p>
<p>Method for registration of 3-D shapes. Sensor fusion IV: control paradigms and data structures 1611. J Paul, Neil D Besl, Mckay, 1992</p>
<p>. Biomedical Image Analysis Group. Imperial College London. 2025. IXI Dataset</p>
<p>Principal warps: Thin-plate splines and the decomposition of deformations. Fred L Bookstein, IEEE Transactions on pattern analysis and machine intelligence. 111989. 1989Manuscript submitted to ACM</p>
<p>. Gary Bradski, Adrian Kaehler, OpenCV. Dr. Dobb's journal of software tools. 32000. 2000</p>
<p>A survey of image registration techniques. Lisa Gottesfeld Brown, ACM computing surveys (CSUR). 241992. 1992</p>
<p>Discriminative Learning of Local Image Descriptors. Matthew Brown, Gang Hua, Simon Winder, European Conference on Computer Vision (ECCV) (LNCS. Springer20106313</p>
<p>Zero-shot semantic segmentation. Maxime Bucher, Tuan-Hung Vu, Matthieu Cord, Patrick Pérez, Advances in Neural Information Processing Systems. 322019. 2019</p>
<p>Brief: Binary robust independent elementary features. Michael Calonder, Vincent Lepetit, Christoph Strecha, Pascal Fua, Computer Vision-ECCV 2010: 11th European Conference on Computer Vision. Heraklion, Crete, GreeceSpringer2010. September 5-11, 2010Proceedings, Part IV 11</p>
<p>Dual-core steered non-rigid registration for multi-modal images via bi-directional image synthesis. Xiaohuan Cao, Jianhua Yang, Yaozong Gao, Yanrong Guo, Guorong Wu, Dinggang Shen, Medical image analysis. 412017. 2017</p>
<p>Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, Yinda Zhang, arXiv:1709.06158Matterport3d: Learning from rgb-d data in indoor environments. 2017arXiv preprint</p>
<p>Aspanformer: Detector-free image matching with adaptive span transformer. Hongkai Chen, Zixin Luo, Lei Zhou, Yurun Tian, Mingmin Zhen, Tian Fang, David Mckinnon, Yanghai Tsin, Long Quan, European Conference on Computer Vision. Springer2022</p>
<p>Touchdown: Natural language navigation and spatial reasoning in visual street environments. Howard Chen, Alane Suhr, Dipendra Misra, Noah Snavely, Yoav Artzi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2019</p>
<p>Transmorph: Transformer for unsupervised medical image registration. Junyu Chen, Eric C Frey, Yufan He, Ye William P Segars, Yong Li, Du, Medical image analysis. 821026152022. 2022</p>
<p>Vit-v-net: Vision transformer for unsupervised volumetric medical image registration. Junyu Chen, Yufan He, Eric C Frey, Ye Li, Yong Du, arXiv:2104.064682021arXiv preprint</p>
<p>History aware multimodal transformer for vision-and-language navigation. Shizhe Chen, Pierre-Louis Guhur, Cordelia Schmid, Ivan Laptev, Advances in neural information processing systems. 342021. 2021</p>
<p>Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, Lawrence Zitnick, arXiv:1504.00325Microsoft coco captions: Data collection and evaluation server. 2015. 2015arXiv preprint</p>
<p>Xi Chen, Xiao Wang, Soravit Changpinyo, Piotr Piergiovanni, Daniel Padlewski, Sebastian Salz, Adam Goodman, Basil Grycner, Lucas Mustafa, Beyer, arXiv:2209.06794Pali: A jointly-scaled multilingual language-image model. 2022. 2022arXiv preprint</p>
<p>Uniter: Universal image-text representation learning. Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, Jingjing Liu, European conference on computer vision. Springer2020</p>
<p>Reweighted random walks for graph matching. Minsu Cho, Jungmin Lee, Kyoung Mu, Lee , Computer Vision-ECCV 2010: 11th European Conference on Computer Vision. Heraklion, Crete, GreeceSpringer2010. September 5-11, 2010Proceedings, Part V 11</p>
<p>Robust reconstruction of indoor scenes. Sungjoon Choi, Qian-Yi Zhou, Vladlen Koltun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2015</p>
<p>Fully convolutional geometric features. Christopher Choy, Jaesik Park, Vladlen Koltun, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2019</p>
<p>Matching with PROSAC-progressive sample consensus. Ondrej Chum, Jiri Matas, 2005 IEEE computer society conference on computer vision and pattern recognition (CVPR'05). IEEE20051</p>
<p>Locally optimized RANSAC. Ondřej Chum, Jiří Matas, Josef Kittler, Joint pattern recognition symposium. Springer2003</p>
<p>Scannet: Richly-annotated 3d reconstructions of indoor scenes. Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, Matthias Nießner, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>Unsupervised learning of probabilistic diffeomorphic registration for images and surfaces. Adrian V Dalca, Guha Balakrishnan, John Guttag, Mert R Sabuncu, Medical image analysis. 572019. 2019</p>
<p>Visual dialog. Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, M F José, Devi Moura, Dhruv Parikh, Batra, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>Ppf-foldnet: Unsupervised learning of rotation invariant 3d local descriptors. Haowen Deng, Tolga Birdal, Slobodan Ilic, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)2018</p>
<p>Ppfnet: Global context aware local features for robust 3d point matching. Haowen Deng, Tolga Birdal, Slobodan Ilic, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>Transvg: End-to-end visual grounding with transformers. Jiajun Deng, Zhengyuan Yang, Tianlang Chen, Wengang Zhou, Houqiang Li, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021</p>
<p>Superpoint: Self-supervised interest point detection and description. Daniel Detone, Tomasz Malisiewicz, Andrew Rabinovich, Proceedings of the IEEE conference on computer vision and pattern recognition workshops. the IEEE conference on computer vision and pattern recognition workshops2018</p>
<p>Cross-modality multi-atlas segmentation via deep registration and label fusion. Wangbin Ding, Lei Li, Xiahai Zhuang, Liqin Huang, IEEE Journal of Biomedical and Health Informatics. 262022. 2022</p>
<p>D2-net: A trainable cnn for joint description and detection of local features. Mihai Dusmanu, Ignacio Rocco, Tomas Pajdla, Marc Pollefeys, Josef Sivic, Akihiko Torii, Torsten Sattler, Proceedings of the ieee/cvf conference on computer vision and pattern recognition. the ieee/cvf conference on computer vision and pattern recognition2019</p>
<p>DKM: Dense kernelized feature matching for geometry estimation. Johan Edstedt, Ioannis Athanasiadis, Mårten Wadenbäck, Michael Felsberg, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Adversarial optimization for joint registration and segmentation in prostate CT radiotherapy. Mohamed S Elmahdy, M Jelmer, Hessam Wolterink, Ivana Sokooti, Marius Išgum, Staring, Medical Image Computing and Computer Assisted Intervention-MICCAI 2019: 22nd International Conference. Shenzhen, ChinaSpringer2019. October 13-17, 2019Proceedings, Part VI 22</p>
<p>Progressively trained convolutional neural networks for deformable image registration. Maxime W Koen Aj Eppenhof, Mitko Lafarge, Josien Pw Veta, Pluim, IEEE transactions on medical imaging. 392019. 2019</p>
<p>Vse++: Improving visual-semantic embeddings with hard negatives. Fartash Faghri, David J Fleet, Jamie Ryan Kiros, Sanja Fidler, arXiv:1707.056122017. 2017arXiv preprint</p>
<p>Adversarial learning for mono-or multi-modal registration. Jingfan Fan, Xiaohuan Cao, Qian Wang, Medical image analysis. 581015452019. 2019Pew-Thian Yap, and Dinggang Shen</p>
<p>Adversarial similarity network for evaluating image alignment in deep learning based registration. Jingfan Fan, Xiaohuan Cao, Zhong Xue, Medical Image Computing and Computer Assisted Intervention-MICCAI 2018: 21st International Conference. Granada, SpainSpringer2018. September 16-20. 2018Proceedings, Part I</p>
<p>Philipp Fischer, Alexey Dosovitskiy, Thomas Brox, arXiv:1405.5769Descriptor matching with convolutional neural networks: a comparison to sift. 2014. 2014arXiv preprint</p>
<p>Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. A Martin, Robert C Fischler, Bolles, Commun. ACM. 241981. 1981</p>
<p>EVSAC: accelerating hypotheses generation by modeling matching scores with extreme value theory. Victor Fragoso, Pradeep Sen, Sergio Rodriguez, Matthew Turk, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision2013</p>
<p>Speaker-follower models for vision-and-language navigation. Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, Trevor Darrell, Advances in neural information processing systems. 312018. 2018</p>
<p>Recognizing objects in range data using regional point descriptors. Andrea Frome, Daniel Huber, Ravi Kolluri, Thomas Bülow, Jitendra Malik, Computer Vision-ECCV 2004: 8th European Conference on Computer Vision. Prague, Czech RepublicSpringer2004. May 11-14, 2004Proceedings, Part III 8</p>
<p>Multimodal compact bilinear pooling for visual question answering and visual grounding. Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, Marcus Rohrbach, arXiv:1606.018472016. 2016arXiv preprint</p>
<p>Vision meets robotics: The kitti dataset. Andreas Geiger, Philip Lenz, Christoph Stiller, Raquel Urtasun, The international journal of robotics research. 32112013. 2013</p>
<p>Scaling open-vocabulary image segmentation with image-level labels. Golnaz Ghiasi, Xiuye Gu, Yin Cui, Tsung-Yi Lin, European conference on computer vision. Springer2022</p>
<p>The perfect match: 3d point cloud matching with smoothed densities. Zan Gojcic, Caifa Zhou, Jan D Wegner, Andreas Wieser, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Making the v in vqa matter: Elevating the role of image understanding in visual question answering. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>Open-vocabulary object detection via vision and language knowledge distillation. Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, Yin Cui, arXiv:2104.139212021. 2021arXiv preprint</p>
<p>Bilinear graph networks for visual question answering. Dalu Guo, Chang Xu, Dacheng Tao, IEEE Transactions on neural networks and learning systems. 342021. 2021</p>
<p>Rotational projection statistics for 3D local surface description and object recognition. Yulan Guo, Ferdous Sohel, Mohammed Bennamoun, Min Lu, Jianwei Wan, International journal of computer vision. 1052013. 2013</p>
<p>A novel local surface feature for 3D object recognition under clutter and occlusion. Yulan Guo, Ferdous Sohel, Mohammed Bennamoun, Jianwei Wan, Min Lu, Information Sciences. 2932015. 2015</p>
<p>Lvis: A dataset for large vocabulary instance segmentation. Agrim Gupta, Piotr Dollar, Ross Girshick, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Diffeomorphic image registration with neural velocity field. Kun Han, Shanlin Sun, Xiangyi Yan, Chenyu You, Hao Tang, Junayed Naushad, Haoyu Ma, Deying Kong, Xiaohui Xie, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer Vision2023</p>
<p>Matchnet: Unifying feature and metric learning for patch-based matching. Xufeng Han, Thomas Leung, Yangqing Jia, Rahul Sukthankar, Alexander C Berg, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2015</p>
<p>A benchmark for RGB-D visual odometry, 3D reconstruction and SLAM. Ankur Handa, Thomas Whelan, John Mcdonald, Andrew J Davison, IEEE international conference on Robotics and automation (ICRA). 2014. 2014IEEE</p>
<p>A combined corner and edge detector. Chris Harris, Mike Stephens, Alvey vision conference. Citeseer198815</p>
<p>Local descriptors optimized for average precision. Kun He, Yan Lu, Stan Sclaroff, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018Manuscript submitted to ACM</p>
<p>MIND: Modality independent neighbourhood descriptor for multi-modal deformable registration. Mattias P Heinrich, Mark Jenkinson, Manav Bhushan, Tahreema Matin, Fergus V Gleeson, Michael Brady, Julia A Schnabel, Medical image analysis. 162012. 2012</p>
<p>Learn2Reg: comprehensive multi-task medical image registration challenge, dataset and evaluation in the era of deep learning. Alessa Hering, Lasse Hansen, Tony Cw Mok, Albert Cs Chung, Hanna Siebert, Stephanie Häger, Annkristin Lange, Sven Kuckertz, Stefan Heldmann, Wei Shao, IEEE Transactions on Medical Imaging. 422022. 2022</p>
<p>Medical image registration. Derek Lg Hill, G Philipp, Mark Batchelor, David J Holden, Hawkes, Physics in medicine &amp; biology. 46R12001. 2001</p>
<p>SynthMorph: learning contrast-invariant registration without acquired images. Malte Hoffmann, Benjamin Billot, Juan Eugenio Douglas N Greve, Bruce Iglesias, Adrian V Fischl, Dalca, IEEE transactions on medical imaging. 412021. 2021</p>
<p>Segmentation from natural language expressions. Ronghang Hu, Marcus Rohrbach, Trevor Darrell, Computer Vision-ECCV 2016: 14th European Conference. Amsterdam, The NetherlandsSpringer2016. October 11-14, 2016Proceedings, Part I 14</p>
<p>A survey of feature matching methods. Qian Huang, Xiaotong Guo, Yiming Wang, Huashan Sun, Lijie Yang, IET Image Processing. 182024. 2024</p>
<p>Predator: Registration of 3d point clouds with low overlap. Shengyu Huang, Zan Gojcic, Mikhail Usvyatsov, Andreas Wieser, Konrad Schindler, Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition. the IEEE/CVF Conference on computer vision and pattern recognition2021</p>
<p>Gqa: A new dataset for real-world visual reasoning and compositional question answering. A Drew, Christopher D Hudson, Manning, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Scaling Up Visual and Vision-Language Representation Learning with Noisy Text Supervision. Chao-Yuan, Yin Jia, Yinfei Yang, Yi Xia, Zhuyun Chen, Hieu Parekh, Quoc Pham, Yuning Le, Tobias Li, Duerig, Proceedings of the International Conference on Machine Learning (ICML). the International Conference on Machine Learning (ICML)2021</p>
<p>One shot PACS: Patient specific Anatomic Context and Shape prior aware recurrent registrationsegmentation of longitudinal thoracic cone beam CTs. Jue Jiang, Harini Veeraraghavan, IEEE transactions on medical imaging. 412022. 2022</p>
<p>Cotr: Correspondence transformer for matching across images. Wei Jiang, Eduard Trulls, Jan Hosang, Andrea Tagliasacchi, Kwang Moo, Yi , Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2021</p>
<p>Image matching across wide baselines: From paper to practice. Yuhe Jin, Dmytro Mishkin, Anastasiia Mishchuk, Jiri Matas, Pascal Fua, Kwang Moo Yi, Eduard Trulls, International Journal of Computer Vision. 1292021. 2021</p>
<p>Using spin images for efficient object recognition in cluttered 3D scenes. Andrew E , Johnson , Martial Hebert, IEEE Transactions. 211999. 1999</p>
<p>Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, Ross Girshick, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, Ross Girshick, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>Saliency, scale and image description. Timor Kadir, Michael Brady, International Journal of Computer Vision. 452001. 2001</p>
<p>Ishan Misra, and Nicolas Carion. 2021. Mdetr-modulated detection for end-to-end multi-modal understanding. Aishwarya Kamath, Mannat Singh, Yann Lecun, Gabriel Synnaeve, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision</p>
<p>Deep visual-semantic alignments for generating image descriptions. Andrej Karpathy, Li Fei-Fei, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2015</p>
<p>PCA-SIFT: A more distinctive representation for local image descriptors. Yan Ke, Rahul Sukthankar, Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. the 2004 IEEE Computer Society Conference on Computer Vision and Pattern RecognitionIEEE2004. 2004. 20042II-II</p>
<p>Learning compact geometric features. Marc Khoury, Qian-Yi Zhou, Vladlen Koltun, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision2017</p>
<p>Diffusemorph: Unsupervised deformable image registration using diffusion model. Boah Kim, Inhwa Han, Jong Chul, Ye , European conference on computer vision. Springer2022</p>
<p>Region-aware pretraining for open-vocabulary object detection with vision transformers. Dahun Kim, Anelia Angelova, Weicheng Kuo, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2023</p>
<p>Elastix: a toolbox for intensity-based medical image registration. Stefan Klein, Marius Staring, Keelin Murphy, Max A Viergever, Josien Pw Pluim, IEEE transactions on medical imaging. 292009. 2009</p>
<p>Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli Vanderbilt, Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu, arXiv:1712.05474Ai2-thor: An interactive 3d environment for visual ai. 2017. 2017arXiv preprint</p>
<p>Room-across-room: Multilingual vision-and-language navigation with dense spatiotemporal grounding. Alexander Ku, Peter Anderson, Roma Patel, Eugene Ie, Jason Baldridge, arXiv:2010.079542020. 2020arXiv preprint</p>
<p>Learning to detect unseen object classes by between-class attribute transfer. Hannes Christoph H Lampert, Stefan Nickisch, Harmeling, 2009 IEEE conference on computer vision and pattern recognition. IEEE2009</p>
<p>Stacked cross attention for image-text matching. Kuang-Huei Lee, Xi Chen, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)2018Gang Hua, Houdong Hu, and Xiaodong He</p>
<p>Less is more: Clipbert for video-and-language learning via sparse sampling. Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg, Mohit Bansal, Jingjing Liu, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2021</p>
<p>A spectral technique for correspondence problems using pairwise constraints. Marius Leordeanu, Martial Hebert, Tenth IEEE International Conference on Computer Vision (ICCV'05). IEEE20051</p>
<p>BRISK: Binary robust invariant scalable keypoints. Stefan Leutenegger, Margarita Chli, Roland Y Siegwart, 2011 International conference on computer vision. Ieee2011</p>
<p>Language-driven semantic segmentation. Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen Koltun, René Ranftl, arXiv:2201.035462022. 2022arXiv preprint</p>
<p>Usip: Unsupervised stable interest point detection from 3d point clouds. Jiaxin Li, Gim Hee, Lee , Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2019</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, 2023. 2023</p>
<p>Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi, International conference on machine learning. PMLR2022</p>
<p>Visual semantic reasoning for image-text matching. Kunpeng Li, Yulun Zhang, Kai Li, Yuanyuan Li, Yun Fu, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2019</p>
<p>Grounded language-image pre-training. Liunian Harold, Li , Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022</p>
<p>Referring transformer: A one-step approach to multi-task visual grounding. Muchen Li, Leonid Sigal, Advances in neural information processing systems. 342021</p>
<p>Oscar: Object-semantics aligned pre-training for vision-language tasks. Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringer2020. August 23-28, 2020Proceedings, Part XXX 16</p>
<p>Microsoft COCO: Common Objects in Context. Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, C Lawrence Zitnick, European Conference on Computer Vision (ECCV). 2014</p>
<p>Lightglue: Local feature matching at light speed. Philipp Lindenberger, Paul-Edouard Sarlin, Marc Pollefeys, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Harmonizing base and novel classes: A class-contrastive approach for generalized few-shot segmentation. Weide Liu, Zhonghua Wu, Yang Zhao, International Journal of Computer Vision. 132Jun Cheng, and Guosheng Lin. 2024. 2024Yuming Fang, Chuan-Sheng Foo</p>
<p>Few-shot segmentation with optimal transport matching and message flow. Weide Liu, Chi Zhang, Henghui Ding, Tzu-Yi Hung, Guosheng Lin, IEEE Transactions on Multimedia. 252022. 2022</p>
<p>Crnet: Cross-reference networks for few-shot segmentation. Weide Liu, Chi Zhang, Guosheng Lin, Fayao Liu, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020</p>
<p>Local feature extraction and matching on range images: 2.5 D SIFT. Tsz-Wai , Rachel Lo, J Paul, Siebert , Computer Vision and Image Understanding. 1132009. 2009</p>
<p>Distinctive image features from scale-invariant keypoints. David G Lowe, International journal of computer vision. 602004. 2004</p>
<p>Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, arXiv:2403.05525Deepseek-vl: towards real-world vision-language understanding. 2024. 2024arXiv preprint</p>
<p>Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee, Advances in neural information processing systems. 201932</p>
<p>Hierarchical question-image co-attention for visual question answering. Jiasen Lu, Jianwei Yang, Dhruv Batra, Devi Parikh, Advances in neural information processing systems. 292016</p>
<p>Image segmentation using text and image prompts. Timo Lüddecke, Alexander Ecker, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022</p>
<p>Contextdesc: Local descriptor augmentation with cross-modality context. Zixin Luo, Tianwei Shen, Lei Zhou, Jiahui Zhang, Yao Yao, Shiwei Li, Tian Fang, Long Quan, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Geodesc: Learning local descriptors by integrating geometry constraints. Zixin Luo, Tianwei Shen, Lei Zhou, Siyu Zhu, Runze Zhang, Yao Yao, Tian Fang, Long Quan, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)2018</p>
<p>The regretful agent: Heuristic-aided navigation through progress estimation. Chih-Yao Ma, Zuxuan Wu, Ghassan Alregib, Caiming Xiong, Zsolt Kira, Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition. the IEEE/CVF conference on Computer Vision and Pattern Recognition2019</p>
<p>Image matching from handcrafted to deep features: A survey. Jiayi Ma, Xingyu Jiang, Aoxiang Fan, Junjun Jiang, Junchi Yan, International Journal of Computer Vision. 1292021. 2021Manuscript submitted to ACM</p>
<p>Multimodality image registration by maximization of mutual information. Frederik Maes, Andre Collignon, Dirk Vandermeulen, Guy Marchal, Paul Suetens, IEEE transactions on Medical Imaging. 162002. 2002</p>
<p>Training data independent image registration using generative adversarial networks and domain adaptation. Dwarikanath Mahapatra, Zongyuan Ge, Pattern Recognition. 1001071092020. 2020</p>
<p>Joint registration and segmentation of xray images using generative adversarial networks. Dwarikanath Mahapatra, Zongyuan Ge, Suman Sedai, Rajib Chakravorty, Machine Learning in Medical Imaging: 9th International Workshop, MLMI 2018, Held in Conjunction with MICCAI 2018. Proceedings. Granada, SpainSpringer2018. September 16. 20189</p>
<p>Generation and comprehension of unambiguous object descriptions. Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, Kevin Murphy, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016</p>
<p>Robust wide-baseline stereo from maximally stable extremal regions. Jiri Matas, Ondrej Chum, Martin Urban, Tomás Pajdla, Image and vision computing. 222004. 2004</p>
<p>H Bjoern, Andras Menze, Stefan Jakab, Jayashree Bauer, Keyvan Kalpathy-Cramer, Justin Farahani, Yuliya Kirby, Nicole Burren, Johannes Porz, Roland Slotboom, Levente Wiest, Elizabeth Lanczi, Marc-Andre Gerstner, Tal Weber, Brian B Arbel, Nicholas Avants, Patricia Ayache, D Louis Buendia, Nicolas Collins, Jason J Cordier, Antonio Corso, Tilak Criminisi, Herve Das, Cagatay Delingette, Christopher R Demiralp, Michel Durst, Senan Dojat, Joana Doyle, Florence Festa, Ezequiel Forbes, Ben Geremia, Polina Glocker, Xiaotao Golland, Andac Guo, Hamamci, M Khan, Raj Iftekharuddin, Nigel M Jena, Ender John, Danial Konukoglu, Jose Lashkari, Raphael Antonio Mariz, Sergio Meier, Doina Pereira, Stephen J Precup, Tammy Riklin Price, Raviv, M S Syed, Michael Reza, Duygu Ryan, Lawrence Sarikaya, Hoo-Chang Schwartz, Jamie Shin, Carlos A Shotton, Nuno Silva, Nagesh K Sousa, Gabor Subbanna, Thomas J Szekely, Owen M Taylor, Nicholas J Thomas, Gozde Tustison, Flor Unal, Max Vasseur, Dong Wintermark, Liang Hye Ye, Binsheng Zhao, Darko Zhao, Marcel Zikic, Mauricio Prastawa, Koen Reyes, Van Leemput, The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS). 2015. 201534</p>
<p>A performance evaluation of local descriptors. Krystian Mikolajczyk, Cordelia Schmid, 2005. 200527</p>
<p>A comparison of affine region detectors. Krystian Mikolajczyk, Tinne Tuytelaars, Cordelia Schmid, Andrew Zisserman, Jiri Matas, Frederik Schaffalitzky, Timor Kadir, Van Gool, International journal of computer vision. 652005. 2005</p>
<p>Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean, arXiv:1301.3781Efficient estimation of word representations in vector space. 2013. 2013arXiv preprint</p>
<p>Simple open-vocabulary object detection. Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, European conference on computer vision. Springer2022</p>
<p>Working hard to know your neighbor's margins: Local descriptor learning loss. Anastasiia Mishchuk, Dmytro Mishkin, Filip Radenovic, Jiri Matas, 2017. 201730Advances in neural information processing systems</p>
<p>Large deformation diffeomorphic image registration with laplacian pyramid networks. C W Tony, Albert Cs Mok, Chung, Medical Image Computing and Computer Assisted Intervention-MICCAI 2020: 23rd International Conference. Lima, PeruSpringer2020. October 4-8, 2020Proceedings, Part III 23</p>
<p>GroupSAC: Efficient consensus in the presence of groupings. Kai Ni, Jin Hailin, Frank Dellaert, 2009 IEEE 12th International Conference on Computer Vision. IEEE2009</p>
<p>Zero-shot learning by convex combination of semantic embeddings. Mohammad Norouzi, Tomas Mikolov, Samy Bengio, Yoram Singer, Jonathon Shlens, Andrea Frome, Greg S Corrado, Jeffrey Dean, arXiv:1312.56502013. 2013arXiv preprint</p>
<p>LF-Net: Learning local features from images. Yuki Ono, Eduard Trulls, Pascal Fua, Kwang Moo, Yi , Advances in neural information processing systems. 2018. 201831</p>
<p>Glove: Global vectors for word representation. Jeffrey Pennington, Richard Socher, Christopher D Manning, Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). the 2014 conference on empirical methods in natural language processing (EMNLP)2014</p>
<p>Challenging data sets for point cloud registration algorithms. François Pomerleau, Ming Liu, Francis Colas, Roland Siegwart, The International Journal of Robotics Research. 312012. 2012</p>
<p>Pointnet: Deep learning on point sets for 3d classification and segmentation. Hao Charles R Qi, Kaichun Su, Leonidas J Mo, Guibas, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>Reverie: Remote embodied visual referring expression in real indoor environments. Yuankai Qi, Qi Wu, Peter Anderson, Xin Wang, William Yang Wang, Chunhua Shen, Anton Van Den, Hengel, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2020</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. PmLR2021</p>
<p>Self-critical sequence training for image captioning. Etienne Steven J Rennie, Youssef Marcheret, Jerret Mroueh, Vaibhava Ross, Goel, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>R2d2: Reliable and repeatable detector and descriptor. Jerome Revaud, Cesar De Souza, Martin Humenberger, Philippe Weinzaepfel, Advances in neural information processing systems. 322019. 2019</p>
<p>Neighbourhood consensus networks. Ignacio Rocco, Mircea Cimpoi, Relja Arandjelović, Akihiko Torii, Tomas Pajdla, Josef Sivic, Advances in neural information processing systems. 2018. 201831</p>
<p>Machine learning for high-speed corner detection. Edward Rosten, Tom Drummond, Computer Vision-ECCV 2006: 9th European Conference on Computer Vision. Graz, AustriaSpringer2006. May 7-13, 2006Proceedings, Part I 9</p>
<p>ORB: An efficient alternative to SIFT or SURF. Ethan Rublee, Vincent Rabaud, Kurt Konolige, Gary Bradski, 2011 International conference on computer vision. Ieee2011</p>
<p>Fast point feature histograms (FPFH) for 3D registration. Bogdan Radu, Nico Rusu, Michael Blodow, Beetz, IEEE international conference on robotics and automation. 2009. 2009IEEE</p>
<p>Aligning point cloud views using persistent feature histograms. Bogdan Radu, Nico Rusu, Zoltan Blodow, Michael Csaba Marton, Beetz, IEEE. 2008. 2008IEEE</p>
<p>Superglue: Learning feature matching with graph neural networks. Paul-Edouard Sarlin, Daniel Detone, Tomasz Malisiewicz, Andrew Rabinovich, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020</p>
<p>Benchmarking 6dof outdoor visual localization in changing conditions. Torsten Sattler, Will Maddern, Carl Toft, Akihiko Torii, Lars Hammarstrand, Erik Stenborg, Daniel Safari, Masatoshi Okutomi, Marc Pollefeys, Josef Sivic, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>Evaluation of interest point detectors. Cordelia Schmid, Roger Mohr, Christian Bauckhage, International Journal of computer vision. 372000. 2000</p>
<p>A multi-view stereo benchmark with high-resolution images and multi-camera videos. Thomas Schops, Johannes L Schonberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Pollefeys, Andreas Geiger, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>Xuelun Shen, Zhipeng Cai, Wei Yin, Matthias Müller, Zijun Li, Kaixuan Wang, Xiaozhi Chen, Cheng Wang, arXiv:2402.11095Gim: Learning generalizable image matcher from internet videos. 2024. 2024arXiv preprint</p>
<p>Good features to track. Jianbo Shi, 1994. 1994IEEE</p>
<p>Xmorpher: Full transformer for deformable medical image registration via cross attention. Jiacheng Shi, Yuting He, Youyong Kong, Jean-Louis Coatrieux, Huazhong Shu, Guanyu Yang, Shuo Li, International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer2022</p>
<p>Scene coordinate regression forests for camera relocalization in RGB-D images. Jamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, Andrew Fitzgibbon, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2013</p>
<p>Alfred: A benchmark for interpreting grounded instructions for everyday tasks. Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, Dieter Fox, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020</p>
<p>Discriminative learning of deep convolutional feature point descriptors. Edgar Simo-Serra, Eduard Trulls, Luis Ferraz, Iasonas Kokkinos, Pascal Fua, Francesc Moreno-Noguer, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision2015</p>
<p>A deep metric for multimodal registration. Martin Simonovsky, Benjamín Gutiérrez-Becker, Diana Mateus, Nassir Navab, Nikos Komodakis, Medical Image Computing and Computer-Assisted Intervention-MICCAI 2016: 19th International Conference. Athens, GreeceSpringer2016. October 17-21, 2016Proceedings, Part III 19</p>
<p>Harris 3D: a robust extension of the Harris operator for interest point detection on 3D meshes. Ivan Sipiran, Benjamin Bustos, The Visual Computer. 272011. 2011</p>
<p>SUSAN-a new approach to low level image processing. M Stephen, J Smith, Brady Michael, International journal of computer vision. 231997. 1997</p>
<p>Zero-shot learning through cross-modal transfer. Richard Socher, Milind Ganjoo, Christopher D Manning, Andrew Ng, Advances in neural information processing systems. 262013. 2013</p>
<p>Nonrigid image registration using multi-scale 3D convolutional neural networks. Hessam Sokooti, Bob De Vos, Floris Berendsen, P F Boudewijn, Ivana Lelieveldt, Marius Išgum, Staring, Medical Image Computing and Computer Assisted Intervention-MICCAI 2017: 20th International Conference. Quebec City, QC, CanadaSpringer2017. September 11-13, 2017Proceedings, Part I 20</p>
<p>PET-MR image fusion in soft tissue sarcoma: accuracy, reliability and practicality of interactive point-based and automated mutual information techniques. J Edward, Somer, Nigel A Paul K Marsden, Joanne Benatar, Goodey, J O' Michael, Michael A Doherty, Smith, European journal of nuclear medicine and molecular imaging. 302003. 2003</p>
<p>Cross-modal attention for multi-modal image registration. Xinrui Song, Hanqing Chao, Xuanang Xu, Hengtao Guo, Sheng Xu, Baris Turkbey, Bradford J Wood, Thomas Sanford, Ge Wang, Pingkun Yan, Medical Image Analysis. 821026122022. 2022</p>
<p>On benchmarking camera calibration and multi-view stereo for high resolution imagery. Christoph Strecha, Wolfgang Von Hansen, Luc Van Gool, Pascal Fua, Ulrich Thoennessen, 2008 IEEE conference on computer vision and pattern recognition. Ieee2008</p>
<p>An overlap invariant entropy measure of 3D medical image alignment. Colin Studholme, Derek Lg Hill, David J Hawkes, Pattern recognition. 321999. 1999</p>
<p>A benchmark for the evaluation of RGB-D SLAM systems. Jürgen Sturm, Nikolas Engelhard, Felix Endres, Wolfram Burgard, Daniel Cremers, IEEE. 2012. 2012IEEE. Manuscript submitted to ACM</p>
<p>LoFTR: Detector-free local feature matching with transformers. Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, Xiaowei Zhou, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2021</p>
<p>Quadtree attention for vision transformers. Shitao Tang, Jiahui Zhang, Siyu Zhu, Ping Tan, arXiv:2201.027672022. 2022arXiv preprint</p>
<p>Vision-and-dialog navigation. Jesse Thomason, Michael Murray, Maya Cakmak, Luke Zettlemoyer, Conference on Robot Learning. PMLR. 2020</p>
<p>Yfcc100m: The new data in multimedia research. Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, Li-Jia Li, Commun. ACM. 592016. 2016</p>
<p>unigradicon: A foundation model for medical image registration. Lin Tian, Hastings Greer, Roland Kwitt, François-Xavier Vialard, Raúl San, José Estépar, Sylvain Bouix, Richard Rushmore, Marc Niethammer, International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer2024</p>
<p>L2-net: Deep learning of discriminative patch descriptor in euclidean space. Yurun Tian, Bin Fan, Fuchao Wu, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>Daisy: An efficient dense descriptor applied to wide-baseline stereo. Engin Tola, Vincent Lepetit, Pascal Fua, 2009. 200932</p>
<p>Unique shape context for 3D data description. Federico Tombari, Samuele Salti, Luigi Di, Stefano , Proceedings of the ACM workshop on 3D object retrieval. the ACM workshop on 3D object retrieval2010</p>
<p>Unique signatures of histograms for local surface description. Federico Tombari, Samuele Salti, Luigi Di, Stefano , Computer Vision-ECCV 2010: 11th European Conference on Computer Vision. Heraklion, Crete, GreeceSpringer2010. September 5-11, 2010Proceedings, Part III 11</p>
<p>Learning image descriptors with boosting. Tomasz Trzcinski, Mario Christoudias, Vincent Lepetit, 2014. 201437</p>
<p>Matching widely separated views based on affine invariant regions. Tinne Tuytelaars, Luc Van Gool, International journal of computer vision. 592004. 2004</p>
<p>Tilde: A temporally invariant learned detector. Yannick Verdie, Kwang Yi, Pascal Fua, Vincent Lepetit, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2015</p>
<p>Show and tell: A neural image caption generator. Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2015</p>
<p>Show and tell: Lessons learned from the 2015 mscoco image captioning challenge. Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan, 2016. 201639</p>
<p>Alignment by maximization of mutual information. Paul Viola, William M Wells, Iii , International journal of computer vision. 241997. 1997</p>
<p>Fast cross-staining alignment of gigapixel whole slide images with application to prostate cancer and breast cancer analysis. Ching-Wei Wang, Yu-Ching Lee, Muhammad-Adil Khalil, Kuan-Yu Lin, Cheng-Ping Yu, Huang-Chun Lien, Scientific Reports. 12116232022. 2022</p>
<p>Matchformer: Interleaving attention in transformers for feature matching. Qing Wang, Jiaming Zhang, Kailun Yang, Kunyu Peng, Rainer Stiefelhagen, Proceedings of the Asian Conference on Computer Vision. the Asian Conference on Computer Vision2022</p>
<p>Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation. Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang, Wang , Lei Zhang, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, Yuan Cao, arXiv:2108.10904Simvlm: Simple visual language model pretraining with weak supervision. 2021. 2021arXiv preprint</p>
<p>Comparison and evaluation of retrospective intermodality brain image registration techniques. Jay West, Michael Fitzpatrick, Matthew Y Wang, Calvin R Benoit M Dawant, Robert M MaurerJr, Robert J Kessler, Christian Maciunas, Didier Barillot, Andre Lemoine, Collignon, Journal of computer assisted tomography. 211997. 1997</p>
<p>Multi-scale neural odes for 3d medical image registration. Junshen Xu, Eric Z Chen, Xiao Chen, Terrence Chen, Shanhui Sun, Medical Image Computing and Computer Assisted Intervention-MICCAI 2021: 24th International Conference. Strasbourg, FranceSpringer2021. September 27-October 1, 2021Proceedings, Part IV 24</p>
<p>Groupvit: Semantic segmentation emerges from text supervision. Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionJan Kautz, and Xiaolong Wang. 2022</p>
<p>Show, attend and tell: Neural image caption generation with visual attention. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, Yoshua Bengio, International conference on machine learning. PMLR2015</p>
<p>Local feature matching using deep learning: A survey. Shibiao Xu, Shunpeng Chen, Rongtao Xu, Changwei Wang, Peng Lu, Li Guo, Information Fusion. 1071023442024. 2024</p>
<p>Evaluation of six registration methods for the human abdomen on clinically acquired CT. Zhoubing Xu, Mattias P Christopher P Lee, Marc Heinrich, Daniel Modat, Sebastien Rueckert, Ourselin, Bennett A Richard G Abramson, Landman, IEEE Transactions on Biomedical Engineering. 632016. 2016</p>
<p>TOLDI: An effective and robust approach for 3D local shape description. Jiaqi Yang, Qian Zhang, Yang Xiao, Zhiguo Cao, Pattern Recognition. 652017. 2017</p>
<p>Cross-modality image registration using a training-time privileged third modality. Qianye Yang, David Atkinson, Yunguan Fu, Tom Syer, Wen Yan, Shonit Punwani, Matthew J Clarkson, Dean C Barratt, Tom Vercauteren, Yipeng Hu, IEEE Transactions on Medical Imaging. 412022. 2022</p>
<p>Dynamic graph attention for referring expression comprehension. Sibei Yang, Guanbin Li, Yizhou Yu, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2019</p>
<p>Quicksilver: Fast predictive image registration-a deep learning approach. Xiao Yang, Roland Kwitt, Martin Styner, Marc Niethammer, NeuroImage. 1582017. 2017</p>
<p>Stacked attention networks for image question answering. Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Smola, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016</p>
<p>Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, Chunjing Xu, arXiv:2111.07783Filip: Fine-grained interactive language-image pre-training. 2021. 2021arXiv preprint</p>
<p>3dfeat-net: Weakly supervised local 3d features for point cloud registration. Jian Zi, Gim Yew, Lee Hee, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)2018</p>
<p>LIFT: Learned Invariant Feature Transform. Kwang Moo, Yi , Edouard Trulls, Vincent Lepetit, Pascal Fua, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)20169910</p>
<p>From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Peter Young, Alice Lai, Micah Hodosh, Julia Hockenmaier, Transactions of the association for computational linguistics. 22014. 2014</p>
<p>Cofinet: Reliable coarse-to-fine correspondences for robust pointcloud registration. Hao Yu, Fu Li, Mahdi Saleh, Benjamin Busam, Slobodan Ilic, Advances in Neural Information Processing Systems. 342021. 2021</p>
<p>Mattnet: Modular attention network for referring expression comprehension. Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, Tamara L Berg, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>A joint speaker-listener-reinforcer model for referring expressions. Licheng Yu, Hao Tan, Mohit Bansal, Tamara L Berg, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>ITK-SNAP: An interactive tool for semi-automatic segmentation of multi-modality biomedical images. Yang Paul A Yushkevich, Guido Gao, Gerig, 2016 38th annual international conference of the IEEE engineering in medicine and biology society (EMBC). IEEE2016</p>
<p>Learning to compare image patches via convolutional neural networks. Sergey Zagoruyko, Nikos Komodakis, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2015</p>
<p>From recognition to cognition: Visual commonsense reasoning. Rowan Zellers, Yonatan Bisk, Ali Farhadi, Yejin Choi, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>3dmatch: Learning local geometric descriptors from rgb-d reconstructions. Andy Zeng, Shuran Song, Matthias Nießner, Matthew Fisher, Jianxiong Xiao, Thomas Funkhouser, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>Lit: Zero-shot transfer with locked-image text tuning. Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, Lucas Beyer, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022</p>
<p>Vinvl: Revisiting visual representations in vision-language models. Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, Jianfeng Gao, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2021</p>
<p>Large Scale Unsupervised Brain MRI Image Registration Solution for Learn2Reg. Yuxi Zhang, Xiang Chen, Jiazheng Wang, Min Liu, Yaonan Wang, Dongdong Liu, Renjiu Hu, Hang Zhang, arXiv:2409.009172024. 2024. 2024arXiv preprint</p>
<p>Understanding and evaluating racial biases in image captioning. Dora Zhao, Angelina Wang, Olga Russakovsky, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2021</p>
<p>Unsupervised 3D end-to-end medical image registration with volume tweening network. Shengyu Zhao, Tingfung Lau, Ji Luo, I-Chao Eric, Yan Chang, Xu, IEEE journal of biomedical and health informatics. 242019. 2019</p>
<p>Intrinsic shape signatures: A shape descriptor for 3D object recognition. Yu Zhong, 2009 IEEE 12th international conference on computer vision workshops, ICCV Workshops. IEEE2009</p>
<p>Regionclip: Region-based language-image pretraining. Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022</p>
<p>Learning to prompt for vision-language models. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu, International Journal of Computer Vision. 1302022. 2022</p>
<p>Evaluation of algorithms for multi-modality whole heart segmentation: an open-access grand challenge. Xiahai Zhuang, Lei Li, Christian Payer, Darko Štern, Martin Urschler, Mattias P Heinrich, Julien Oster, Chunliang Wang, Örjan Smedby, Cheng Bian, Medical image analysis. 581015372019. 2019</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1942 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1942</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1942</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-39.html">extraction-schema-39</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <p><strong>Paper ID:</strong> paper-280711581</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2508.17449v2.pdf" target="_blank">Robotic Manipulation via Imitation Learning: Taxonomy, Evolution, Benchmark, and Challenges</a></p>
                <p><strong>Paper Abstract:</strong> Robotic Manipulation (RM) is central to the advancement of autonomous robots, enabling them to interact with and manipulate objects in real-world environments. This survey focuses on RM methodologies that leverage imitation learning, a powerful technique that allows robots to learn complex manipulation skills by mimicking human demonstrations. We identify and analyze the most influential studies in this domain, selected based on community impact and intrinsic quality. For each paper, we provide a structured summary, covering the research purpose, technical implementation, hierarchical classification, input formats, key priors, strengths and limitations, and citation metrics. Additionally, we trace the chronological development of imitation learning techniques within RM policy (RMP), offering a timeline of key technological advancements. Where available, we report benchmark results and perform quantitative evaluations to compare existing methods. By synthesizing these insights, this review provides a comprehensive resource for researchers and practitioners, highlighting both the state of the art and the challenges that lie ahead in the field of robotic manipulation through imitation learning.</p>
                <p><strong>Cost:</strong> 0.032</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1942.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1942.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Robotics Transformer 1 (RT-1)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language-action (VLA) model trained on large-scale real-world teleoperation data that tokenizes inputs/outputs to generate discrete robot actions directly from raw observations for hundreds of household primitives and real deployments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Rt-1: Robotics transformer for real-world control at scale</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RT-1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>End-to-end Transformer-based VLA that tokenizes RGB (sometimes multi-view) observations and natural language commands and generates discrete action tokens; trained on large-scale real teleoperation demonstrations (Open-X style).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>multimodal pretraining on large-scale robotic action datasets + vision-language conditioning</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Pretraining on large-scale real teleoperation demonstrations with paired language commands (natural-language-labeled tasks), covering many household primitives; data contains object descriptions and language-conditioned task labels, limited action-free web video content.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Real-world robotic manipulation (household primitives)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Hundreds of short household primitive tasks (pick/place, open/close, move items), discrete action token space used in real robots; evaluated on real-world deployment scenarios and simulated leaderboards.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Survey reports explicit alignment: pretraining commands are natural-language labels closely matching target tasks (high overlap in objects and verbs for household primitives).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reported real-world deployment readiness and strong multi-task performance; on SimplerEnv-Google-Robot tasks RT-1 achieved Visual Matching average success ≈53.4% (per Table VII) and Variant Aggregation average ≈39.6% (per Table VII).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>No direct numeric baseline without language pretraining reported in the survey for RT-1; survey notes RT-1 established VLA concept compared to training from scratch.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Survey notes RT-1 benefits from large-scale teleoperation pretraining to obtain broad generalization; no explicit numeric sample-efficiency comparison provided in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No detailed attention-visualization analysis reported in the survey for RT-1 (survey lists it as engineering recipe and deployment-ready but lacks fine-grained attention studies).</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No explicit embedding-space clustering or representational analysis for RT-1 reported in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Survey attributes RT-1's success to transferring language-conditioned priors from teleoperation data into action token generation, but does not present mechanistic grounding analyses linking verbs to affordances.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>No layered feature analysis reported; survey characterizes RT-1 as end-to-end with discrete action tokens rather than hierarchical decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Per survey, RT-1 transfers well to household primitives when training data covers similar objects and language commands; transfer is reduced under domain shifts (different camera setups, large perceptual changes).</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Survey describes RT-1 generalizes to seen/nearby household objects through pretraining but does not present explicit numeric split comparing familiar vs novel object performance.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>RT-1 shows strong zero-/few-shot generalization in practice per survey claims (motivating later RT-2 work), but no specific zero-shot numeric breakdown provided in the survey for RT-1 itself.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No ablation/layer-freezing analysis reported in the survey for RT-1.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Survey does not report concrete negative-transfer measurements for RT-1, only notes limitations in closed-loop feedback and discrete-action resolution.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Survey contrasts VLA approach (RT-1) with vision-only policies qualitatively, claiming language conditioning helps semantic generalization; no direct quantitative vision-only vs VLA numbers given for RT-1 in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No temporal progression (early vs late training phase) analysis for RT-1 reported in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No dimensionality or intrinsic-dimension analysis for RT-1 reported in the survey.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1942.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1942.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT-2 (RT-X series continuation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A follow-up VLA that discretizes actions into tokens and jointly pretrains on large-scale vision-language (VQA-style) data together with real robot trajectories to transfer web-scale semantic and reasoning capabilities to robotic control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large Transformer VLA that integrates web-scale vision-language pretraining (VQA-style) with robot trajectory data using discrete action tokenization to improve zero-/few-shot and cross-scene generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>joint multimodal pretraining on vision-language (VQA-style) data + in-domain robot trajectory data</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Mixture of out-of-domain web VQA image-text pairs (rich in object descriptions and spatial relations) and in-domain robot demonstrations with action tokens; includes semantic verbs and object labels.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Multi-scene, multi-task robotic manipulation (cross-scene transfer)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Multi-task robotic control across scenes and robots using discrete action tokens; evaluated on cross-scene zero-/few-shot tasks and real robot deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Survey explicitly notes RT-2 transfers web-scale semantic/reasoning capabilities into robot control — high semantic overlap due to VQA pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Survey states RT-2-style joint pretraining yields strong zero-/few-shot generalization and semantic reasoning performance; no single numeric success rate provided in survey text for RT-2.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Survey does not report direct numeric baseline for RT-2 without VQA pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Survey claims joint pretraining increases sample efficiency (better zero/few-shot), but provides no numeric multipliers for RT-2 specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No detailed attention or visualization analyses reported for RT-2 within survey summary.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No explicit embedding-space analysis for RT-2 reported in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Survey reports RT-2 demonstrates semantic reasoning transferred from VLM to action tokens, but no mechanistic grounding experiments detailed in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Survey does not report hierarchical feature analyses for RT-2.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Works best when VQA-style web semantics align with robot task vocabulary and objects; survey notes transfer depends on overlap between pretraining and robot task domains.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Survey mentions RT-2 improves zero/few-shot on novel tasks but does not provide explicit per-object split numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Survey attributes strong zero-/few-shot generalization to RT-2-style pretraining; no precise counts of shots reported.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No layer/component ablations for RT-2 discussed in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Survey does not report quantified negative transfer cases for RT-2.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Survey implies RT-2 (vision+language) outperforms vision-only baselines qualitatively but does not provide explicit numbers in the summarized text.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No temporal dynamics analysis reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No dimensionality analysis reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1942.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1942.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM-E</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM-E (Embodied Multimodal Language Model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An embodied multimodal model that injects visual and state embeddings into a large language model, enabling perception, multi-step reasoning (CoT), and control within a single architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PaLM-E: An embodied multimodal language model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-E</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large language model conditioned on visual and proprioceptive/state embeddings (multimodal), enabling chain-of-thought (CoT) style reasoning combined with action prediction; leverages LLM reasoning fused with perception.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>language-model pretraining augmented with multimodal finetuning on multimodal/embodied datasets</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Large text corpora for base LLM plus multimodal alignment data that links images/states to language reasoning steps; contains stepwise instructions and object/verb semantics but limited large-scale action-labeled robot data.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Embodied perception and manipulation with multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Perception, planning and control tasks requiring multi-step reasoning in robotics; evaluated on multimodal reasoning and embodied tasks (sim/real); action interface can be either high-level plans or lower-level actions depending on head.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Survey reports explicit design to align language reasoning with visual/state inputs; semantic overlap is high for planning/description-style tasks but may be lower for fine motor control actions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Survey states PaLM-E shows positive transfer and strong reasoning-capable performance across perception and control tasks; no single aggregate numeric success reported in survey summary.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Survey does not report explicit numeric baselines comparing to non-language-pretrained variants.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Survey highlights joint multimodal training yields positive transfer (improved sample efficiency qualitatively) but no quantitative numbers presented.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Survey does not report attention visualization results for PaLM-E specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No embedding-space clustering analysis for PaLM-E reported in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>PaLM-E leverages LLM reasoning to produce action-aligned outputs; survey cites 'positive transfer' but does not present mechanistic grounding (mapping verbs to affordances) experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Survey notes PaLM-E supports multi-step CoT decomposition (high-level planning) but lacks detailed hierarchical representation analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Per survey, works best when tasks require semantic planning and the pretraining contains related instruction/semantic content; less explicit benefit for high-frequency low-level motor control without action-specific finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Survey does not report explicit comparisons of novel vs familiar object performance for PaLM-E.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Survey indicates PaLM-E exhibits generalist and few-shot planning/semantic capabilities but lacks quantified few-shot robot action numbers in this summary.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No layer-wise ablation analysis reported for PaLM-E in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Survey does not describe quantified negative transfer for PaLM-E.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Survey characterizes PaLM-E as providing additional reasoning benefits beyond vision-only models; no explicit numeric comparison in survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No temporal training-phase analyses reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No dimensionality analysis reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1942.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1942.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenVLA (Open-source Vision-Language-Action model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source VLA pretrained on nearly a million real robot demonstrations, aiming for fast, parameter-efficient fine-tuning for new robots and tasks and providing reproducible baselines for generalist policies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Openvla: An open-source vision-language-action model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based VLA pretrained on ~1M real-robot demonstrations with vision-language conditioning; includes unified architecture for perception, reasoning and action with tools for fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>multimodal pretraining on large-scale in-domain robot action datasets (real demonstrations) with language annotations</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Nearly a million real-world robot demonstrations paired with language instructions, covering many household/robotic primitives, object descriptions, and action sequences (rich in action labels).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Multi-task, cross-scene robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Language-conditioned multi-task manipulation across scenes and embodiments; discrete and continuous action decodings supported for real-robot tasks and benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>High: pretraining data are in-domain robot demonstrations with paired language commands, providing direct overlap between pretraining semantics and target tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Survey reports OpenVLA outperforms RT-2-X by 16.5% across 29 tasks (Table II) and shows high LIBERO/CALVIN performance (Table V indicates OpenVLA variants achieving top scores in LIBERO: e.g., OpenVLAOFT with high success rates).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Survey does not provide a direct numeric baseline within the OpenVLA entry contrasting training from scratch; however many VLA baselines are lower in reported leaderboards.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Survey emphasizes OpenVLA enables parameter-efficient fine-tuning on new robots/tasks and better few-shot adaptation; no concrete numeric sample-efficiency multiplier provided in summary.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Survey does not detail attention maps for OpenVLA specifically, though the model family emphasizes unified perception-reasoning architectures (no explicit attention visualization reported).</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No explicit embedding-space analyses reported in survey for OpenVLA beyond qualitative transfer claims.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Survey reports OpenVLA demonstrates improved semantic grounding enabling zero-/few-shot adaptation; evidence is through improved task success rates and cross-scene transfer, but no fine-grained verb-to-affordance mapping experiments are presented in the survey summary.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Survey notes OpenVLA supports both perception and reasoning heads but does not provide explicit hierarchical feature analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Transfer strongest when target robot/task shares object and instruction vocabulary with pretraining dataset; OpenVLA emphasizes cross-robot adaptability via large diverse dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Survey reports strong zero-/few-shot performance on novel objects in LIBERO/CALVIN leaderboards for OpenVLA variants (see Table V), but specific per-object deltas are not enumerated in the summary.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>OpenVLA exhibits strong few-shot and zero-shot generalization in reported benchmarks; Table V and other leaderboards show top-tier few-shot numbers (OpenVLAOFT and OpenVLA variants listed with high LIBERO scores).</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Survey mentions OpenVLA supports parameter-efficient fine-tuning (implying adapter-style studies) but does not list detailed layer-freezing ablations in the summary.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Survey does not highlight significant negative transfer for OpenVLA; limitations are listed as low inference throughput and robustness in some contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Survey reports OpenVLA (VLA) outperforms RT-2-X and likely vision-only baselines on multi-task benchmarks but exact vision-only comparison numbers are not provided alongside the OpenVLA claim.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No explicit training-time representation dynamics reported for OpenVLA in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No dimensionality/intrinsic-dimension analysis reported for OpenVLA.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1942.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1942.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Octo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Octo (Open-source generalist robot policy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generalist VLA trained on 800k diverse demonstrations to support multi-modal commands and multi-embodiment control via transformer-based diffusion decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Octo: An open-source generalist robot policy</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Octo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based diffusion decoder VLA trained at scale (~800k demos) supporting multiple command modalities and multiple robot embodiments; combines large diverse action data with diffusion-based action generation.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>large-scale in-domain action pretraining on diverse robot demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>~800k diverse robot demonstrations spanning multiple tasks and embodiments; contains paired language commands, object manipulations, and multi-modal annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Generalist multi-embodiment robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Multi-task control across hundreds of robots and tasks, evaluated for cross-embodiment transfer, bench/test suites and simulated to real scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>High alignment: pretraining data are diverse robot demos annotated with language and task labels matching target tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Survey lists Octo among strong generalist baselines; Table V shows Octo with LIBERO-style numbers (e.g., Octo row in Table V: values like 78.9, 85.7, 84.6 across splits with average 75.1 as reported), demonstrating competitive transfer after large pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Survey does not provide direct numbers for an Octo variant without language pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Survey suggests large-scale action pretraining enables Octo to transfer broadly; no explicit numeric sample-efficiency ratios provided for Octo in survey summary.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No attention/interpretability analyses reported in survey for Octo.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No embedding-space analyses reported for Octo in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Survey attributes Octo's generalization to its large, diverse pretraining enabling semantic-action coupling; no explicit probing of verb-to-affordance grounding provided in summary.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>No explicit hierarchical features analysis reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Per survey, Octo transfers well across embodiments when pretraining includes multi-embodiment coverage; domain shifts reduce performance if new embodiments differ substantially from pretraining set.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Survey does not quantify novel vs familiar object performance for Octo specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Octo demonstrates cross-embodiment and zero-shot abilities qualitatively per survey; benchmark numbers (LIBERO/CALVIN) indicate effective generalization, but explicit shot counts not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No layer importance or ablation analysis reported in survey for Octo.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Survey notes Octo can be unstable in long-horizon tasks despite strong generalization baseline — an implicit limitation rather than measured negative transfer magnitude.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Survey positions Octo (action-pretrained VLA) as stronger than smaller/no VLA baselines but does not give direct vision-only comparison numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No explicit temporal training dynamics presented for Octo in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No dimensionality analysis reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1942.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1942.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>π0 (pi-zero)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>π0: A vision-language-action flow model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLA that integrates pretrained vision-language knowledge with flow-matching continuous action generation to preserve high control rates and action smoothness for real-world manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>π 0 : A vision-language-action flow model for general robot control</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>π0</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Combines pretrained VLMs for semantic reasoning with a flow-matching continuous-action head to produce high-frequency smooth control suitable for contact-rich real-world tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>pretrained VLM (vision-language) backbone + flow-matching action-head pretraining on multi-embodiment action datasets</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Mixture of vision-language pretrained models (internet image-text semantics) plus large multi-embodiment action datasets enabling alignment of semantics to continuous motor commands; contains object descriptions and task language.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Real-world continuous high-frequency robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Contact-rich and high-frequency tasks (laundry folding, table cleaning, box assembly) executed on real robots with continuous action spaces driven by flow-matching models; emphasis on smoothness and responsiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Survey reports explicit effort to align VLM semantic priors with continuous action outputs; alignment quality improves robustness in semantically guided tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Survey reports π0 achieves strong generalization across household skills and is cited among high-performing generalist models; no single numeric success rate given in the survey summary, but π0 reported in LIBERO/CALVIN listings with strong results (e.g., Table V entries).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>No direct numeric baseline without language pretraining presented in the survey for π0.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Survey mentions π0 benefits from action pretraining and VLM priors for faster adaptation; no explicit quantitative sample-efficiency numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No attention visualization analyses reported in survey for π0.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No embedding-space analyses presented for π0 in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Survey states π0 preserves semantic guidance (from VLM) while producing continuous motor outputs; this is supported by task successes and reported sim-to-real transfer claims, but no fine-grained grounding studies shown in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>No hierarchical feature-level analyses reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Per survey π0 transfers best when semantic task vocabulary overlaps with pretraining and when action spaces align with flow-matching outputs; sensitive to scene variation.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Survey does not provide explicit numeric comparisons for novel vs familiar objects for π0.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Survey claims π0 exhibits zero-shot generalization on some household tasks; numeric few-shot counts are not given in the summary.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No layer-wise ablations reported in survey for π0.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Survey notes low sampling efficiency and sensitivity to scene as limitations for π0 but does not quantify negative transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Survey claims that combining VLMs with flow-matching improves over vision-only baselines qualitatively; no numeric head-to-head data in the summary.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No explicit temporal dynamics analysis provided.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No dimensionality analysis reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1942.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1942.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PerAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Perceiver-Actor (PerAct)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A voxel/grid-based language-conditioned policy that predicts discrete 3D actions from RGB-D voxelized scene representations and language goals, notable for sample efficiency within a single scene.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Perceiver-actor: A multi-task transformer for robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PerAct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Voxel-grid (3D) representation fed into transformer/Perceiver-style architecture conditioned on language goals; outputs discrete action heatmaps/voxel classifications executed via motion planners.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>pretraining not central; uses 3D voxel representations and supervised imitation learning from demonstrations (some works may use pretrained 2D features lifted to 3D)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Uses robot-specific observation data (RGB-D/point clouds) with action labels; not primarily pre-trained on image-text corpora in the original PerAct formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Language-conditioned 3D robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Tabletop and voxelized workspace tasks with discrete action outputs (pick/place/press/rotate) executed via a motion planner; evaluated in simulation benchmarks like RLBench and real robot variants.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>PerAct conditions on language goals directly; alignment between language and 3D voxel semantics is core to design, enabling sample-efficient learning in single-scene multi-task regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Survey lists PerAct as sample-efficient and reports competitive RLBench numbers (PerAct appears in Table IV with an average success shown in survey; Table IV lists PerAct among higher-performing methods with ~62.9% on RLBench 18-task average as presented).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>No explicit numbers for PerAct without language conditioning provided in the survey; baseline comparisons typically use ablated language input studies in original works (not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Survey explicitly notes PerAct emphasizes extreme sample efficiency within single scenes; specific quantity (e.g., #demonstrations) not provided in the survey summary.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No detailed attention maps reported in the survey for PerAct, though PerAct uses Perceiver-style attention mechanisms internally.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Survey does not summarize explicit embedding-space analyses for PerAct.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>PerAct grounds language goals into 3D action heatmaps (explicit affordance-like outputs), providing operational grounding from language to executable 3D actions; survey highlights this as a core design advantage.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>PerAct uses voxelified geometric features (mid-level spatial features) for action prediction; survey does not provide a layered feature-benefit breakdown comparing low/high-level features.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>PerAct performs well when 3D reconstruction / voxel fidelity is good and camera/depth sensors align with training; suffers under large viewpoint changes without additional augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Survey does not present a numerical novel-vs-familiar object split for PerAct here, but notes PerAct generalizes across many task variants in a single scene.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>PerAct shows few-shot adaptability in single-scene multitask regimes per survey, but no exact shot counts are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Survey does not report layer ablations for PerAct.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Survey notes PerAct's slow inference speed and discretized actions as limitations; no quantified negative transfer reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>PerAct relies on 3D voxel inputs rather than solely 2D vision; survey claims this improves sample efficiency compared to purely 2D methods in many scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No explicit temporal dynamics analysis reported for PerAct in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No dimensionality/intrinsic-dimension analysis reported for PerAct.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1942.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1942.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIPort</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIPort</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLA-style pixel-level pick-and-place policy that conditions on CLIP-extracted semantic features and predicts transport/affordance maps for sample-efficient single-task learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLIPort</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pixel-wise transport model that leverages CLIP image-text features to condition affordance/transport predictions; outputs pixel action locations for pick/place tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language pretraining via CLIP (image-text pairs) used to extract features; policy trained via imitation on pick-and-place demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>CLIP pretraining on large-scale image-text pairs (internet-scale) providing object semantics and attribute information; policy training data are task-specific demonstrations with pick/place labels.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Language-conditioned pick-and-place / tabletop manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Tabletop pick-and-place and transport tasks conditioned on language attributes (e.g., 'place the red block'), discrete pixel-action outputs executed in simulation and real robot setups.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>High semantic alignment because CLIP features encode object attributes and text prompts used in tasks; survey emphasizes benefit of CLIP features for semantic grounding in affordance predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Survey states CLIPort enables sample-efficient single-task learning and reports strong single-task performance on pick-and-place benchmarks (no single aggregate numeric provided in the survey summary here).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>No explicit numeric baseline in survey comparing CLIP features vs non-CLIP features for CLIPort (original works did such ablations, but survey summary does not provide numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Survey highlights CLIPort's sample efficiency for single-task regimes due to CLIP features; no explicit numeric fewer-demo factor provided in summary.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Survey does not report attention visualizations for CLIPort specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Survey notes CLIP feature semantic richness helps affordance prediction but does not summarize internal embedding clustering results.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>CLIP features are used to ground text attributes to pixel affordances (e.g., identify the target object and produce pick/place heatmaps), representing operational evidence of language-perception-action grounding in CLIPort.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Survey does not present multi-level feature analysis for CLIPort beyond noting the use of pretrained CLIP features (high-level semantics) together with pixel-level policy heads (low-level control).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Works well when CLIP semantics cover task vocabulary; less effective when target objects or phrasing diverge from CLIP pretraining distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Survey mentions generalization across instance appearance and positions for CLIPort but does not provide numeric novel vs familiar splits here.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>CLIPort enables few-shot adaptation via CLIP semantics; survey does not quantify shot counts in summary.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Survey does not report layer freezing/ablation details for CLIPort.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Survey mentions brittleness on transparent or deformable items and viewpoint shifts, but not quantified negative transfer due to CLIP features.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Survey implies CLIP-based conditioning improves over vision-only pixel features in semantic tasks, but does not provide explicit numbers in summary.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No temporal dynamics analysis reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No dimensionality analysis reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1942.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1942.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DiVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DiVLA (Diffusion VLA / Diffusion-VLA family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Models that unify vision-language reasoning with diffusion-based action heads, translating high-level instructions into low-level continuous actions for adaptable, instruction-conditioned control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Diffusion-vla: Scaling robot foundation models via unified diffusion and autoregression</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DiVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Combines pretrained VLM reasoning (language/vision) with a diffusion-based controller head to produce continuous actions; sometimes decouples perception/reasoning and action generation for modularity.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language pretraining for the reasoning backbone + action pretraining/finetuning for diffusion action head (mix of actionful and action-free datasets across works)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Backbone pretrained on image-text corpora; action head trained on robot demonstrations or generated pseudo-demonstrations; mixed content includes object descriptions, spatial relations and some task action labels depending on variant.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Instruction-conditioned continuous-action robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Tasks requiring continuous, precise action trajectories (e.g., contact-rich and dexterous tasks) in sim and real settings; diffusion head generates high-dimensional continuous actions, closed-loop in some variants.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Survey reports strong conceptual alignment: VLM provides semantic reasoning and diffusion head maps semantics to actions; degree of overlap depends on action-head training data.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Survey cites DiVLA variants as adaptable to novel instructions and environments; no single aggregate numeric success rate provided in survey summary, though DiVLA family appears in leaderboards and qualitative comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>No direct numbers reported comparing DiVLA to a version without language pretraining in the survey summary.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Survey notes diffusion+VLM yields strong generalization but diffusion sampling can be computationally heavy; sample-efficiency numbers not provided explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Survey does not report detailed attention studies; some DiVLA variants decouple perception/reasoning allowing interpretability but not quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No explicit embedding-space analyses summarized for DiVLA in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Survey claims DiVLA unifies semantic reasoning with precise action generation and reports improved semantic-to-action transfer qualitatively, but no fine-grained grounding metrics are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Some DiVLA variants separate cognitive (reasoning) and action modules, implying hierarchical processing; survey does not include layerwise empirical evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Survey indicates transfer depends on action-head training coverage and semantic overlap between pretraining and target instructions; diffusion sampling cost and closed-loop requirements also affect transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Survey does not present explicit numeric comparisons for novel vs familiar objects for DiVLA.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Survey suggests DiVLA variants support zero-shot instruction following in some settings (qualitative claims), but no precise counts provided.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No layer/component ablations summarized in survey for DiVLA.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Survey notes diffusion inference cost and sampling inefficiency are limitations; no quantified negative transfer evidence reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Survey presents DiVLA as improving over vision-only diffusion policies by adding language reasoning, but no head-to-head numbers provided in survey summary.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No explicit temporal learning-dynamics analysis reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No dimensionality analysis reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1942.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e1942.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraspVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraspVLA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A grasping foundation model pretrained on billion-scale synthetic action data that integrates autoregressive perception tasks with flow-matching-based action generation to achieve sim-to-real transfer and zero-shot generalization in grasping.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graspvla: a grasping foundation model pretrained on billion-scale synthetic action data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GraspVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Grasp-focused VLA pretrained on massive synthetic datasets; integrates autoregressive perception and flow-matching action generation, emphasizing grasp affordance prediction and sim-to-real transfer for rigid object grasping.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>large-scale synthetic action pretraining (billion-scale synthetic demonstrations) focused on grasping</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Synthetic grasping demonstrations with grasp affordance labels and object geometry; includes parametric variations to support generalization (object shapes, poses); language content limited for some variants but combined with perception tasks in model.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Open-set tabletop grasping and grasp transfer</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Open-set grasping of novel rigid objects with continuous or discrete grasp action outputs; tested on sim-to-real transfer, few-shot and zero-shot adaptation to new objects.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Pretraining provides grasp affordance and object-centric priors that align well with grasping target tasks; language alignment is weaker (more focus on perception-action pairs) unless combined with web semantics in joint training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Survey reports GraspVLA demonstrates direct sim-to-real transfer and strong zero-shot generalization; Table V shows GraspVLA few-shot numbers on LIBERO (e.g., entries showing high success rates: 94.1%/91.2%/82.0% in certain splits as reported in survey table excerpts).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Survey does not list a direct numeric baseline without the large synthetic pretraining; implied large improvement relative to smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Survey claims strong few-shot adaptability and direct sim-to-real transfer suggesting substantial sample-efficiency benefits, but does not quantify exact factors in the summary.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Survey does not summarize attention visualizations for GraspVLA.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No embedding-space clustering analyses reported specifically for GraspVLA in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>GraspVLA's pretraining on action-labeled synthetic data provides direct evidence of mapping between perception and grasp actions through affordance priors and successful sim-to-real transfer; the survey reports these transfer results as evidence of grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Survey does not report multi-level feature decomposition analyses, but GraspVLA's architecture couples perception and action modules supporting hierarchical processing in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Works best for rigid object grasping where synthetic data statistics match real objects; limitations for deformables or non-rigid objects are noted in broader survey context.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Survey reports strong zero-shot performance on novel objects for GraspVLA (as evidenced by sim-to-real and zero-shot claims in tables), with high success rates in reported leaderboard excerpts.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Survey indicates strong zero-shot generalization and few-shot adaptability; LIBERO/other table entries show high few-shot numbers for GraspVLA (see Table V excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No explicit ablation/layer-freezing studies reported for GraspVLA in survey summary.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Survey does not report explicit negative transfer for GraspVLA; limitations discussed are focused scope (rigid objects) rather than negative transfer magnitudes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Survey implies that action-pretraining on synthetic demonstration data outperforms vision-only pretraining for grasp tasks; exact numeric comparisons are not detailed in the summary.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No temporal dynamics analysis reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No dimensionality analysis reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1942.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e1942.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>3D-VLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>3D-VLA (3D Vision-Language-Action model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generative world-model VLA integrating 3D scene representations (object/location tokens) with language and action generation to improve spatial grounding and multi-step manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>3d-vla: A 3d vision-language-action generative world model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>3D-VLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>VLA architecture that incorporates 3D-aware world modeling (object/location tokens, 3D representations) into a vision-language-action generative framework for better spatial reasoning and action generation.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>multimodal pretraining incorporating 3D scene data plus action demonstrations (mix of in-domain action data and 3D scene representations)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Pretraining data include 3D scenes (point clouds/RGB-D), paired language descriptions and demonstrations; captures object-centric spatial relations and physical placements.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>3D-aware language-conditioned manipulation and multi-step planning</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Tasks requiring spatial reasoning in 3D (object localization, placement, bimanual coordination) in simulation and real settings; targets discrete/continuous actions depending on downstream head.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>High semantic alignment for spatial/object-centric tasks due to explicit 3D tokens and object-location representations used during pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Survey claims 3D-VLA improves spatial grounding and generalization in multi-step manipulation; no single numeric aggregate provided in the survey summary, but it appears in leaderboard and comparative tables as competitive.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Survey does not provide a direct numeric comparison without language pretraining for 3D-VLA.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Survey suggests 3D representations improve data efficiency for spatial tasks; however, explicit numeric sample-efficiency comparisons are not provided in the summary.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Survey does not report attention/interpretability analyses specifically for 3D-VLA.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No embedding-space analyses provided in survey summary for 3D-VLA.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>3D-VLA's object/location tokenization and joint world/action modeling provide architectural grounding hypothesized to improve mapping from language to 3D actions; survey reports qualitative improvements but no fine-grained grounding metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>3D-VLA explicitly separates 3D world modeling from action decoding, implying hierarchical representations; no empirical layerwise evidence provided in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Transfer benefits when target tasks require explicit 3D spatial reasoning and when depth/point-cloud sensors are available; limited by closed-loop feedback in some variants per survey.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Survey does not report explicit numeric novel vs familiar object comparisons for 3D-VLA.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Survey describes 3D-VLA improving zero-shot/transfer in spatial tasks qualitatively; no precise shot counts provided.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No layer/component ablations summarized for 3D-VLA in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Survey notes limited closed-loop feedback as a drawback in some 3D-VLA variants; no explicit negative transfer numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Survey implies 3D-VLA (3D-aware) outperforms 2D-only VLAs on spatial tasks qualitatively, but explicit numeric comparisons are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No temporal dynamics analyses reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No dimensionality analysis reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1942.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e1942.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT-VLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CoT-VLA (Chain-of-Thought Vision-Language-Action)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLA that integrates an explicit visual chain-of-thought by autoregressively predicting future image frames or intermediate visual goals before generating action sequences, improving temporal planning and interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cot-vla: Visual chain-of-thought reasoning for vision-language-action models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CoT-VLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive VLA augmented with chain-of-thought (CoT) style intermediate visual prediction steps (future images/waypoints) that serve as visual subgoals and improve subsequent action prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language pretraining plus CoT-style multimodal finetuning on sequences with intermediate visual goals; mixture of actionful and action-free video data used for visual prediction pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Training data includes sequences where intermediate frames/future visuals and language instructions are paired with actions; contains temporal structure, object motions, and affordance cues.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Long-horizon, multi-step language-conditioned manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Tasks with long horizons requiring intermediate visual goals and temporal consistency (e.g., multi-step kitchen routines); action outputs are short sequences conditioned on predicted visual subgoals.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>High alignment for tasks requiring stepwise decomposition: language instructions map to visual subgoals that align well with task semantics and spatial relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Survey reports CoT-VLA outperforms OpenVLA by 17% in real-world tasks (explicit claim in Table II summary) and shows large improvements in robustness and interpretability on benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Survey does not give direct numeric baselines of CoT-VLA without CoT or without language pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Survey notes CoT improves robustness and sample efficiency qualitatively by structuring predictions, but no explicit numeric sample counts are provided in summary.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Survey highlights improved interpretability via visual CoT predictions (intermediate images/waypoints), serving as an implicit attention/causal trace, though explicit attention-map analyses are not enumerated.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No detailed embedding-space clustering analyses summarized for CoT-VLA, although its intermediate visual goals provide a form of structured latent trajectory.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>CoT-VLA provides stronger behavioral evidence for grounding by explicitly predicting visual subgoals that mediate between language and actions; survey frames this as evidence of improved language-perception-action grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>CoT-VLA implements an explicit hierarchical decomposition (visual goal prediction then short action sequences), and survey reports this benefits long-horizon tasks — supporting hierarchical feature benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Works well when intermediate visual states are predictive of action sequences and when sensor fidelity supports reliable future-frame prediction; fails when visual prediction error compounds.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Survey does not report exact numerical comparisons for novel vs familiar object performance for CoT-VLA, but claims improved generalization qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>CoT-VLA demonstrates improved zero-/few-shot robustness in benchmarks per survey, though precise shot counts not given.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Survey does not document detailed layer ablation studies for CoT-VLA within summary.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Survey notes increased computational and memory overhead as a limitation; no quantified negative transfer effects reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Survey indicates CoT-VLA outperforms baseline VLAs (e.g., OpenVLA) that lack CoT on real-world tasks by reported margins (17% improvement claimed), implying benefit over simpler vision-language models.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Survey emphasizes CoT-VLA's explicit modeling of temporal intermediate goals to improve long-horizon consistency; no fine-grained temporal learning curves provided.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No dimensionality/intrinsic-dimension measurements reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Rt-1: Robotics transformer for real-world control at scale <em>(Rating: 2)</em></li>
                <li>RT-2: Vision-language-action models transfer web knowledge to robotic control <em>(Rating: 2)</em></li>
                <li>PaLM-E: An embodied multimodal language model <em>(Rating: 2)</em></li>
                <li>Openvla: An open-source vision-language-action model <em>(Rating: 2)</em></li>
                <li>Perceiver-actor: A multi-task transformer for robotic manipulation <em>(Rating: 2)</em></li>
                <li>CLIPort: What and where pathways for robotic manipulation <em>(Rating: 2)</em></li>
                <li>Graspvla: a grasping foundation model pretrained on billion-scale synthetic action data <em>(Rating: 2)</em></li>
                <li>3d-vla: A 3d vision-language-action generative world model <em>(Rating: 2)</em></li>
                <li>Cot-vla: Visual chain-of-thought reasoning for vision-language-action models <em>(Rating: 2)</em></li>
                <li>Octo: An open-source generalist robot policy <em>(Rating: 1)</em></li>
                <li>π 0 : A vision-language-action flow model for general robot control <em>(Rating: 1)</em></li>
                <li>Diffusion-vla: Scaling robot foundation models via unified diffusion and autoregression <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1942",
    "paper_id": "paper-280711581",
    "extraction_schema_id": "extraction-schema-39",
    "extracted_data": [
        {
            "name_short": "RT-1",
            "name_full": "Robotics Transformer 1 (RT-1)",
            "brief_description": "A vision-language-action (VLA) model trained on large-scale real-world teleoperation data that tokenizes inputs/outputs to generate discrete robot actions directly from raw observations for hundreds of household primitives and real deployments.",
            "citation_title": "Rt-1: Robotics transformer for real-world control at scale",
            "mention_or_use": "use",
            "model_name": "RT-1",
            "model_description": "End-to-end Transformer-based VLA that tokenizes RGB (sometimes multi-view) observations and natural language commands and generates discrete action tokens; trained on large-scale real teleoperation demonstrations (Open-X style).",
            "pretraining_type": "multimodal pretraining on large-scale robotic action datasets + vision-language conditioning",
            "pretraining_data_description": "Pretraining on large-scale real teleoperation demonstrations with paired language commands (natural-language-labeled tasks), covering many household primitives; data contains object descriptions and language-conditioned task labels, limited action-free web video content.",
            "target_task_name": "Real-world robotic manipulation (household primitives)",
            "target_task_description": "Hundreds of short household primitive tasks (pick/place, open/close, move items), discrete action token space used in real robots; evaluated on real-world deployment scenarios and simulated leaderboards.",
            "semantic_alignment": "Survey reports explicit alignment: pretraining commands are natural-language labels closely matching target tasks (high overlap in objects and verbs for household primitives).",
            "performance_with_language_pretraining": "Reported real-world deployment readiness and strong multi-task performance; on SimplerEnv-Google-Robot tasks RT-1 achieved Visual Matching average success ≈53.4% (per Table VII) and Variant Aggregation average ≈39.6% (per Table VII).",
            "performance_without_language_pretraining": "No direct numeric baseline without language pretraining reported in the survey for RT-1; survey notes RT-1 established VLA concept compared to training from scratch.",
            "sample_efficiency_comparison": "Survey notes RT-1 benefits from large-scale teleoperation pretraining to obtain broad generalization; no explicit numeric sample-efficiency comparison provided in survey.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No detailed attention-visualization analysis reported in the survey for RT-1 (survey lists it as engineering recipe and deployment-ready but lacks fine-grained attention studies).",
            "embedding_space_analysis": "No explicit embedding-space clustering or representational analysis for RT-1 reported in the survey.",
            "action_grounding_evidence": "Survey attributes RT-1's success to transferring language-conditioned priors from teleoperation data into action token generation, but does not present mechanistic grounding analyses linking verbs to affordances.",
            "hierarchical_features_evidence": "No layered feature analysis reported; survey characterizes RT-1 as end-to-end with discrete action tokens rather than hierarchical decomposition.",
            "transfer_conditions": "Per survey, RT-1 transfers well to household primitives when training data covers similar objects and language commands; transfer is reduced under domain shifts (different camera setups, large perceptual changes).",
            "novel_vs_familiar_objects": "Survey describes RT-1 generalizes to seen/nearby household objects through pretraining but does not present explicit numeric split comparing familiar vs novel object performance.",
            "zero_shot_or_few_shot": "RT-1 shows strong zero-/few-shot generalization in practice per survey claims (motivating later RT-2 work), but no specific zero-shot numeric breakdown provided in the survey for RT-1 itself.",
            "layer_analysis": "No ablation/layer-freezing analysis reported in the survey for RT-1.",
            "negative_transfer_evidence": "Survey does not report concrete negative-transfer measurements for RT-1, only notes limitations in closed-loop feedback and discrete-action resolution.",
            "comparison_to_vision_only": "Survey contrasts VLA approach (RT-1) with vision-only policies qualitatively, claiming language conditioning helps semantic generalization; no direct quantitative vision-only vs VLA numbers given for RT-1 in survey.",
            "temporal_dynamics": "No temporal progression (early vs late training phase) analysis for RT-1 reported in survey.",
            "dimensionality_analysis": "No dimensionality or intrinsic-dimension analysis for RT-1 reported in the survey.",
            "uuid": "e1942.0"
        },
        {
            "name_short": "RT-2",
            "name_full": "RT-2 (RT-X series continuation)",
            "brief_description": "A follow-up VLA that discretizes actions into tokens and jointly pretrains on large-scale vision-language (VQA-style) data together with real robot trajectories to transfer web-scale semantic and reasoning capabilities to robotic control.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "RT-2",
            "model_description": "Large Transformer VLA that integrates web-scale vision-language pretraining (VQA-style) with robot trajectory data using discrete action tokenization to improve zero-/few-shot and cross-scene generalization.",
            "pretraining_type": "joint multimodal pretraining on vision-language (VQA-style) data + in-domain robot trajectory data",
            "pretraining_data_description": "Mixture of out-of-domain web VQA image-text pairs (rich in object descriptions and spatial relations) and in-domain robot demonstrations with action tokens; includes semantic verbs and object labels.",
            "target_task_name": "Multi-scene, multi-task robotic manipulation (cross-scene transfer)",
            "target_task_description": "Multi-task robotic control across scenes and robots using discrete action tokens; evaluated on cross-scene zero-/few-shot tasks and real robot deployment.",
            "semantic_alignment": "Survey explicitly notes RT-2 transfers web-scale semantic/reasoning capabilities into robot control — high semantic overlap due to VQA pretraining.",
            "performance_with_language_pretraining": "Survey states RT-2-style joint pretraining yields strong zero-/few-shot generalization and semantic reasoning performance; no single numeric success rate provided in survey text for RT-2.",
            "performance_without_language_pretraining": "Survey does not report direct numeric baseline for RT-2 without VQA pretraining.",
            "sample_efficiency_comparison": "Survey claims joint pretraining increases sample efficiency (better zero/few-shot), but provides no numeric multipliers for RT-2 specifically.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No detailed attention or visualization analyses reported for RT-2 within survey summary.",
            "embedding_space_analysis": "No explicit embedding-space analysis for RT-2 reported in survey.",
            "action_grounding_evidence": "Survey reports RT-2 demonstrates semantic reasoning transferred from VLM to action tokens, but no mechanistic grounding experiments detailed in survey.",
            "hierarchical_features_evidence": "Survey does not report hierarchical feature analyses for RT-2.",
            "transfer_conditions": "Works best when VQA-style web semantics align with robot task vocabulary and objects; survey notes transfer depends on overlap between pretraining and robot task domains.",
            "novel_vs_familiar_objects": "Survey mentions RT-2 improves zero/few-shot on novel tasks but does not provide explicit per-object split numbers.",
            "zero_shot_or_few_shot": "Survey attributes strong zero-/few-shot generalization to RT-2-style pretraining; no precise counts of shots reported.",
            "layer_analysis": "No layer/component ablations for RT-2 discussed in survey.",
            "negative_transfer_evidence": "Survey does not report quantified negative transfer cases for RT-2.",
            "comparison_to_vision_only": "Survey implies RT-2 (vision+language) outperforms vision-only baselines qualitatively but does not provide explicit numbers in the summarized text.",
            "temporal_dynamics": "No temporal dynamics analysis reported.",
            "dimensionality_analysis": "No dimensionality analysis reported.",
            "uuid": "e1942.1"
        },
        {
            "name_short": "PaLM-E",
            "name_full": "PaLM-E (Embodied Multimodal Language Model)",
            "brief_description": "An embodied multimodal model that injects visual and state embeddings into a large language model, enabling perception, multi-step reasoning (CoT), and control within a single architecture.",
            "citation_title": "PaLM-E: An embodied multimodal language model",
            "mention_or_use": "mention",
            "model_name": "PaLM-E",
            "model_description": "Large language model conditioned on visual and proprioceptive/state embeddings (multimodal), enabling chain-of-thought (CoT) style reasoning combined with action prediction; leverages LLM reasoning fused with perception.",
            "pretraining_type": "language-model pretraining augmented with multimodal finetuning on multimodal/embodied datasets",
            "pretraining_data_description": "Large text corpora for base LLM plus multimodal alignment data that links images/states to language reasoning steps; contains stepwise instructions and object/verb semantics but limited large-scale action-labeled robot data.",
            "target_task_name": "Embodied perception and manipulation with multi-step reasoning",
            "target_task_description": "Perception, planning and control tasks requiring multi-step reasoning in robotics; evaluated on multimodal reasoning and embodied tasks (sim/real); action interface can be either high-level plans or lower-level actions depending on head.",
            "semantic_alignment": "Survey reports explicit design to align language reasoning with visual/state inputs; semantic overlap is high for planning/description-style tasks but may be lower for fine motor control actions.",
            "performance_with_language_pretraining": "Survey states PaLM-E shows positive transfer and strong reasoning-capable performance across perception and control tasks; no single aggregate numeric success reported in survey summary.",
            "performance_without_language_pretraining": "Survey does not report explicit numeric baselines comparing to non-language-pretrained variants.",
            "sample_efficiency_comparison": "Survey highlights joint multimodal training yields positive transfer (improved sample efficiency qualitatively) but no quantitative numbers presented.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Survey does not report attention visualization results for PaLM-E specifically.",
            "embedding_space_analysis": "No embedding-space clustering analysis for PaLM-E reported in the survey.",
            "action_grounding_evidence": "PaLM-E leverages LLM reasoning to produce action-aligned outputs; survey cites 'positive transfer' but does not present mechanistic grounding (mapping verbs to affordances) experiments.",
            "hierarchical_features_evidence": "Survey notes PaLM-E supports multi-step CoT decomposition (high-level planning) but lacks detailed hierarchical representation analyses.",
            "transfer_conditions": "Per survey, works best when tasks require semantic planning and the pretraining contains related instruction/semantic content; less explicit benefit for high-frequency low-level motor control without action-specific finetuning.",
            "novel_vs_familiar_objects": "Survey does not report explicit comparisons of novel vs familiar object performance for PaLM-E.",
            "zero_shot_or_few_shot": "Survey indicates PaLM-E exhibits generalist and few-shot planning/semantic capabilities but lacks quantified few-shot robot action numbers in this summary.",
            "layer_analysis": "No layer-wise ablation analysis reported for PaLM-E in the survey.",
            "negative_transfer_evidence": "Survey does not describe quantified negative transfer for PaLM-E.",
            "comparison_to_vision_only": "Survey characterizes PaLM-E as providing additional reasoning benefits beyond vision-only models; no explicit numeric comparison in survey text.",
            "temporal_dynamics": "No temporal training-phase analyses reported.",
            "dimensionality_analysis": "No dimensionality analysis reported.",
            "uuid": "e1942.2"
        },
        {
            "name_short": "OpenVLA",
            "name_full": "OpenVLA (Open-source Vision-Language-Action model)",
            "brief_description": "An open-source VLA pretrained on nearly a million real robot demonstrations, aiming for fast, parameter-efficient fine-tuning for new robots and tasks and providing reproducible baselines for generalist policies.",
            "citation_title": "Openvla: An open-source vision-language-action model",
            "mention_or_use": "use",
            "model_name": "OpenVLA",
            "model_description": "Transformer-based VLA pretrained on ~1M real-robot demonstrations with vision-language conditioning; includes unified architecture for perception, reasoning and action with tools for fine-tuning.",
            "pretraining_type": "multimodal pretraining on large-scale in-domain robot action datasets (real demonstrations) with language annotations",
            "pretraining_data_description": "Nearly a million real-world robot demonstrations paired with language instructions, covering many household/robotic primitives, object descriptions, and action sequences (rich in action labels).",
            "target_task_name": "Multi-task, cross-scene robotic manipulation",
            "target_task_description": "Language-conditioned multi-task manipulation across scenes and embodiments; discrete and continuous action decodings supported for real-robot tasks and benchmarks.",
            "semantic_alignment": "High: pretraining data are in-domain robot demonstrations with paired language commands, providing direct overlap between pretraining semantics and target tasks.",
            "performance_with_language_pretraining": "Survey reports OpenVLA outperforms RT-2-X by 16.5% across 29 tasks (Table II) and shows high LIBERO/CALVIN performance (Table V indicates OpenVLA variants achieving top scores in LIBERO: e.g., OpenVLAOFT with high success rates).",
            "performance_without_language_pretraining": "Survey does not provide a direct numeric baseline within the OpenVLA entry contrasting training from scratch; however many VLA baselines are lower in reported leaderboards.",
            "sample_efficiency_comparison": "Survey emphasizes OpenVLA enables parameter-efficient fine-tuning on new robots/tasks and better few-shot adaptation; no concrete numeric sample-efficiency multiplier provided in summary.",
            "has_sample_efficiency_data": true,
            "attention_analysis": "Survey does not detail attention maps for OpenVLA specifically, though the model family emphasizes unified perception-reasoning architectures (no explicit attention visualization reported).",
            "embedding_space_analysis": "No explicit embedding-space analyses reported in survey for OpenVLA beyond qualitative transfer claims.",
            "action_grounding_evidence": "Survey reports OpenVLA demonstrates improved semantic grounding enabling zero-/few-shot adaptation; evidence is through improved task success rates and cross-scene transfer, but no fine-grained verb-to-affordance mapping experiments are presented in the survey summary.",
            "hierarchical_features_evidence": "Survey notes OpenVLA supports both perception and reasoning heads but does not provide explicit hierarchical feature analyses.",
            "transfer_conditions": "Transfer strongest when target robot/task shares object and instruction vocabulary with pretraining dataset; OpenVLA emphasizes cross-robot adaptability via large diverse dataset.",
            "novel_vs_familiar_objects": "Survey reports strong zero-/few-shot performance on novel objects in LIBERO/CALVIN leaderboards for OpenVLA variants (see Table V), but specific per-object deltas are not enumerated in the summary.",
            "zero_shot_or_few_shot": "OpenVLA exhibits strong few-shot and zero-shot generalization in reported benchmarks; Table V and other leaderboards show top-tier few-shot numbers (OpenVLAOFT and OpenVLA variants listed with high LIBERO scores).",
            "layer_analysis": "Survey mentions OpenVLA supports parameter-efficient fine-tuning (implying adapter-style studies) but does not list detailed layer-freezing ablations in the summary.",
            "negative_transfer_evidence": "Survey does not highlight significant negative transfer for OpenVLA; limitations are listed as low inference throughput and robustness in some contexts.",
            "comparison_to_vision_only": "Survey reports OpenVLA (VLA) outperforms RT-2-X and likely vision-only baselines on multi-task benchmarks but exact vision-only comparison numbers are not provided alongside the OpenVLA claim.",
            "temporal_dynamics": "No explicit training-time representation dynamics reported for OpenVLA in the survey.",
            "dimensionality_analysis": "No dimensionality/intrinsic-dimension analysis reported for OpenVLA.",
            "uuid": "e1942.3"
        },
        {
            "name_short": "Octo",
            "name_full": "Octo (Open-source generalist robot policy)",
            "brief_description": "A generalist VLA trained on 800k diverse demonstrations to support multi-modal commands and multi-embodiment control via transformer-based diffusion decoding.",
            "citation_title": "Octo: An open-source generalist robot policy",
            "mention_or_use": "use",
            "model_name": "Octo",
            "model_description": "Transformer-based diffusion decoder VLA trained at scale (~800k demos) supporting multiple command modalities and multiple robot embodiments; combines large diverse action data with diffusion-based action generation.",
            "pretraining_type": "large-scale in-domain action pretraining on diverse robot demonstrations",
            "pretraining_data_description": "~800k diverse robot demonstrations spanning multiple tasks and embodiments; contains paired language commands, object manipulations, and multi-modal annotations.",
            "target_task_name": "Generalist multi-embodiment robotic manipulation",
            "target_task_description": "Multi-task control across hundreds of robots and tasks, evaluated for cross-embodiment transfer, bench/test suites and simulated to real scenarios.",
            "semantic_alignment": "High alignment: pretraining data are diverse robot demos annotated with language and task labels matching target tasks.",
            "performance_with_language_pretraining": "Survey lists Octo among strong generalist baselines; Table V shows Octo with LIBERO-style numbers (e.g., Octo row in Table V: values like 78.9, 85.7, 84.6 across splits with average 75.1 as reported), demonstrating competitive transfer after large pretraining.",
            "performance_without_language_pretraining": "Survey does not provide direct numbers for an Octo variant without language pretraining.",
            "sample_efficiency_comparison": "Survey suggests large-scale action pretraining enables Octo to transfer broadly; no explicit numeric sample-efficiency ratios provided for Octo in survey summary.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No attention/interpretability analyses reported in survey for Octo.",
            "embedding_space_analysis": "No embedding-space analyses reported for Octo in survey.",
            "action_grounding_evidence": "Survey attributes Octo's generalization to its large, diverse pretraining enabling semantic-action coupling; no explicit probing of verb-to-affordance grounding provided in summary.",
            "hierarchical_features_evidence": "No explicit hierarchical features analysis reported.",
            "transfer_conditions": "Per survey, Octo transfers well across embodiments when pretraining includes multi-embodiment coverage; domain shifts reduce performance if new embodiments differ substantially from pretraining set.",
            "novel_vs_familiar_objects": "Survey does not quantify novel vs familiar object performance for Octo specifically.",
            "zero_shot_or_few_shot": "Octo demonstrates cross-embodiment and zero-shot abilities qualitatively per survey; benchmark numbers (LIBERO/CALVIN) indicate effective generalization, but explicit shot counts not provided.",
            "layer_analysis": "No layer importance or ablation analysis reported in survey for Octo.",
            "negative_transfer_evidence": "Survey notes Octo can be unstable in long-horizon tasks despite strong generalization baseline — an implicit limitation rather than measured negative transfer magnitude.",
            "comparison_to_vision_only": "Survey positions Octo (action-pretrained VLA) as stronger than smaller/no VLA baselines but does not give direct vision-only comparison numbers.",
            "temporal_dynamics": "No explicit temporal training dynamics presented for Octo in survey.",
            "dimensionality_analysis": "No dimensionality analysis reported.",
            "uuid": "e1942.4"
        },
        {
            "name_short": "π0 (pi-zero)",
            "name_full": "π0: A vision-language-action flow model",
            "brief_description": "A VLA that integrates pretrained vision-language knowledge with flow-matching continuous action generation to preserve high control rates and action smoothness for real-world manipulation.",
            "citation_title": "π 0 : A vision-language-action flow model for general robot control",
            "mention_or_use": "mention",
            "model_name": "π0",
            "model_description": "Combines pretrained VLMs for semantic reasoning with a flow-matching continuous-action head to produce high-frequency smooth control suitable for contact-rich real-world tasks.",
            "pretraining_type": "pretrained VLM (vision-language) backbone + flow-matching action-head pretraining on multi-embodiment action datasets",
            "pretraining_data_description": "Mixture of vision-language pretrained models (internet image-text semantics) plus large multi-embodiment action datasets enabling alignment of semantics to continuous motor commands; contains object descriptions and task language.",
            "target_task_name": "Real-world continuous high-frequency robotic manipulation",
            "target_task_description": "Contact-rich and high-frequency tasks (laundry folding, table cleaning, box assembly) executed on real robots with continuous action spaces driven by flow-matching models; emphasis on smoothness and responsiveness.",
            "semantic_alignment": "Survey reports explicit effort to align VLM semantic priors with continuous action outputs; alignment quality improves robustness in semantically guided tasks.",
            "performance_with_language_pretraining": "Survey reports π0 achieves strong generalization across household skills and is cited among high-performing generalist models; no single numeric success rate given in the survey summary, but π0 reported in LIBERO/CALVIN listings with strong results (e.g., Table V entries).",
            "performance_without_language_pretraining": "No direct numeric baseline without language pretraining presented in the survey for π0.",
            "sample_efficiency_comparison": "Survey mentions π0 benefits from action pretraining and VLM priors for faster adaptation; no explicit quantitative sample-efficiency numbers provided.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No attention visualization analyses reported in survey for π0.",
            "embedding_space_analysis": "No embedding-space analyses presented for π0 in survey.",
            "action_grounding_evidence": "Survey states π0 preserves semantic guidance (from VLM) while producing continuous motor outputs; this is supported by task successes and reported sim-to-real transfer claims, but no fine-grained grounding studies shown in the survey.",
            "hierarchical_features_evidence": "No hierarchical feature-level analyses reported.",
            "transfer_conditions": "Per survey π0 transfers best when semantic task vocabulary overlaps with pretraining and when action spaces align with flow-matching outputs; sensitive to scene variation.",
            "novel_vs_familiar_objects": "Survey does not provide explicit numeric comparisons for novel vs familiar objects for π0.",
            "zero_shot_or_few_shot": "Survey claims π0 exhibits zero-shot generalization on some household tasks; numeric few-shot counts are not given in the summary.",
            "layer_analysis": "No layer-wise ablations reported in survey for π0.",
            "negative_transfer_evidence": "Survey notes low sampling efficiency and sensitivity to scene as limitations for π0 but does not quantify negative transfer.",
            "comparison_to_vision_only": "Survey claims that combining VLMs with flow-matching improves over vision-only baselines qualitatively; no numeric head-to-head data in the summary.",
            "temporal_dynamics": "No explicit temporal dynamics analysis provided.",
            "dimensionality_analysis": "No dimensionality analysis reported.",
            "uuid": "e1942.5"
        },
        {
            "name_short": "PerAct",
            "name_full": "Perceiver-Actor (PerAct)",
            "brief_description": "A voxel/grid-based language-conditioned policy that predicts discrete 3D actions from RGB-D voxelized scene representations and language goals, notable for sample efficiency within a single scene.",
            "citation_title": "Perceiver-actor: A multi-task transformer for robotic manipulation",
            "mention_or_use": "use",
            "model_name": "PerAct",
            "model_description": "Voxel-grid (3D) representation fed into transformer/Perceiver-style architecture conditioned on language goals; outputs discrete action heatmaps/voxel classifications executed via motion planners.",
            "pretraining_type": "pretraining not central; uses 3D voxel representations and supervised imitation learning from demonstrations (some works may use pretrained 2D features lifted to 3D)",
            "pretraining_data_description": "Uses robot-specific observation data (RGB-D/point clouds) with action labels; not primarily pre-trained on image-text corpora in the original PerAct formulation.",
            "target_task_name": "Language-conditioned 3D robotic manipulation",
            "target_task_description": "Tabletop and voxelized workspace tasks with discrete action outputs (pick/place/press/rotate) executed via a motion planner; evaluated in simulation benchmarks like RLBench and real robot variants.",
            "semantic_alignment": "PerAct conditions on language goals directly; alignment between language and 3D voxel semantics is core to design, enabling sample-efficient learning in single-scene multi-task regimes.",
            "performance_with_language_pretraining": "Survey lists PerAct as sample-efficient and reports competitive RLBench numbers (PerAct appears in Table IV with an average success shown in survey; Table IV lists PerAct among higher-performing methods with ~62.9% on RLBench 18-task average as presented).",
            "performance_without_language_pretraining": "No explicit numbers for PerAct without language conditioning provided in the survey; baseline comparisons typically use ablated language input studies in original works (not detailed here).",
            "sample_efficiency_comparison": "Survey explicitly notes PerAct emphasizes extreme sample efficiency within single scenes; specific quantity (e.g., #demonstrations) not provided in the survey summary.",
            "has_sample_efficiency_data": true,
            "attention_analysis": "No detailed attention maps reported in the survey for PerAct, though PerAct uses Perceiver-style attention mechanisms internally.",
            "embedding_space_analysis": "Survey does not summarize explicit embedding-space analyses for PerAct.",
            "action_grounding_evidence": "PerAct grounds language goals into 3D action heatmaps (explicit affordance-like outputs), providing operational grounding from language to executable 3D actions; survey highlights this as a core design advantage.",
            "hierarchical_features_evidence": "PerAct uses voxelified geometric features (mid-level spatial features) for action prediction; survey does not provide a layered feature-benefit breakdown comparing low/high-level features.",
            "transfer_conditions": "PerAct performs well when 3D reconstruction / voxel fidelity is good and camera/depth sensors align with training; suffers under large viewpoint changes without additional augmentation.",
            "novel_vs_familiar_objects": "Survey does not present a numerical novel-vs-familiar object split for PerAct here, but notes PerAct generalizes across many task variants in a single scene.",
            "zero_shot_or_few_shot": "PerAct shows few-shot adaptability in single-scene multitask regimes per survey, but no exact shot counts are provided.",
            "layer_analysis": "Survey does not report layer ablations for PerAct.",
            "negative_transfer_evidence": "Survey notes PerAct's slow inference speed and discretized actions as limitations; no quantified negative transfer reported.",
            "comparison_to_vision_only": "PerAct relies on 3D voxel inputs rather than solely 2D vision; survey claims this improves sample efficiency compared to purely 2D methods in many scenarios.",
            "temporal_dynamics": "No explicit temporal dynamics analysis reported for PerAct in survey.",
            "dimensionality_analysis": "No dimensionality/intrinsic-dimension analysis reported for PerAct.",
            "uuid": "e1942.6"
        },
        {
            "name_short": "CLIPort",
            "name_full": "CLIPort",
            "brief_description": "A VLA-style pixel-level pick-and-place policy that conditions on CLIP-extracted semantic features and predicts transport/affordance maps for sample-efficient single-task learning.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "CLIPort",
            "model_description": "Pixel-wise transport model that leverages CLIP image-text features to condition affordance/transport predictions; outputs pixel action locations for pick/place tasks.",
            "pretraining_type": "vision-language pretraining via CLIP (image-text pairs) used to extract features; policy trained via imitation on pick-and-place demonstrations",
            "pretraining_data_description": "CLIP pretraining on large-scale image-text pairs (internet-scale) providing object semantics and attribute information; policy training data are task-specific demonstrations with pick/place labels.",
            "target_task_name": "Language-conditioned pick-and-place / tabletop manipulation",
            "target_task_description": "Tabletop pick-and-place and transport tasks conditioned on language attributes (e.g., 'place the red block'), discrete pixel-action outputs executed in simulation and real robot setups.",
            "semantic_alignment": "High semantic alignment because CLIP features encode object attributes and text prompts used in tasks; survey emphasizes benefit of CLIP features for semantic grounding in affordance predictions.",
            "performance_with_language_pretraining": "Survey states CLIPort enables sample-efficient single-task learning and reports strong single-task performance on pick-and-place benchmarks (no single aggregate numeric provided in the survey summary here).",
            "performance_without_language_pretraining": "No explicit numeric baseline in survey comparing CLIP features vs non-CLIP features for CLIPort (original works did such ablations, but survey summary does not provide numbers).",
            "sample_efficiency_comparison": "Survey highlights CLIPort's sample efficiency for single-task regimes due to CLIP features; no explicit numeric fewer-demo factor provided in summary.",
            "has_sample_efficiency_data": true,
            "attention_analysis": "Survey does not report attention visualizations for CLIPort specifically.",
            "embedding_space_analysis": "Survey notes CLIP feature semantic richness helps affordance prediction but does not summarize internal embedding clustering results.",
            "action_grounding_evidence": "CLIP features are used to ground text attributes to pixel affordances (e.g., identify the target object and produce pick/place heatmaps), representing operational evidence of language-perception-action grounding in CLIPort.",
            "hierarchical_features_evidence": "Survey does not present multi-level feature analysis for CLIPort beyond noting the use of pretrained CLIP features (high-level semantics) together with pixel-level policy heads (low-level control).",
            "transfer_conditions": "Works well when CLIP semantics cover task vocabulary; less effective when target objects or phrasing diverge from CLIP pretraining distribution.",
            "novel_vs_familiar_objects": "Survey mentions generalization across instance appearance and positions for CLIPort but does not provide numeric novel vs familiar splits here.",
            "zero_shot_or_few_shot": "CLIPort enables few-shot adaptation via CLIP semantics; survey does not quantify shot counts in summary.",
            "layer_analysis": "Survey does not report layer freezing/ablation details for CLIPort.",
            "negative_transfer_evidence": "Survey mentions brittleness on transparent or deformable items and viewpoint shifts, but not quantified negative transfer due to CLIP features.",
            "comparison_to_vision_only": "Survey implies CLIP-based conditioning improves over vision-only pixel features in semantic tasks, but does not provide explicit numbers in summary.",
            "temporal_dynamics": "No temporal dynamics analysis reported.",
            "dimensionality_analysis": "No dimensionality analysis reported.",
            "uuid": "e1942.7"
        },
        {
            "name_short": "DiVLA",
            "name_full": "DiVLA (Diffusion VLA / Diffusion-VLA family)",
            "brief_description": "Models that unify vision-language reasoning with diffusion-based action heads, translating high-level instructions into low-level continuous actions for adaptable, instruction-conditioned control.",
            "citation_title": "Diffusion-vla: Scaling robot foundation models via unified diffusion and autoregression",
            "mention_or_use": "mention",
            "model_name": "DiVLA",
            "model_description": "Combines pretrained VLM reasoning (language/vision) with a diffusion-based controller head to produce continuous actions; sometimes decouples perception/reasoning and action generation for modularity.",
            "pretraining_type": "vision-language pretraining for the reasoning backbone + action pretraining/finetuning for diffusion action head (mix of actionful and action-free datasets across works)",
            "pretraining_data_description": "Backbone pretrained on image-text corpora; action head trained on robot demonstrations or generated pseudo-demonstrations; mixed content includes object descriptions, spatial relations and some task action labels depending on variant.",
            "target_task_name": "Instruction-conditioned continuous-action robotic manipulation",
            "target_task_description": "Tasks requiring continuous, precise action trajectories (e.g., contact-rich and dexterous tasks) in sim and real settings; diffusion head generates high-dimensional continuous actions, closed-loop in some variants.",
            "semantic_alignment": "Survey reports strong conceptual alignment: VLM provides semantic reasoning and diffusion head maps semantics to actions; degree of overlap depends on action-head training data.",
            "performance_with_language_pretraining": "Survey cites DiVLA variants as adaptable to novel instructions and environments; no single aggregate numeric success rate provided in survey summary, though DiVLA family appears in leaderboards and qualitative comparisons.",
            "performance_without_language_pretraining": "No direct numbers reported comparing DiVLA to a version without language pretraining in the survey summary.",
            "sample_efficiency_comparison": "Survey notes diffusion+VLM yields strong generalization but diffusion sampling can be computationally heavy; sample-efficiency numbers not provided explicitly.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Survey does not report detailed attention studies; some DiVLA variants decouple perception/reasoning allowing interpretability but not quantified here.",
            "embedding_space_analysis": "No explicit embedding-space analyses summarized for DiVLA in the survey.",
            "action_grounding_evidence": "Survey claims DiVLA unifies semantic reasoning with precise action generation and reports improved semantic-to-action transfer qualitatively, but no fine-grained grounding metrics are provided.",
            "hierarchical_features_evidence": "Some DiVLA variants separate cognitive (reasoning) and action modules, implying hierarchical processing; survey does not include layerwise empirical evidence.",
            "transfer_conditions": "Survey indicates transfer depends on action-head training coverage and semantic overlap between pretraining and target instructions; diffusion sampling cost and closed-loop requirements also affect transfer.",
            "novel_vs_familiar_objects": "Survey does not present explicit numeric comparisons for novel vs familiar objects for DiVLA.",
            "zero_shot_or_few_shot": "Survey suggests DiVLA variants support zero-shot instruction following in some settings (qualitative claims), but no precise counts provided.",
            "layer_analysis": "No layer/component ablations summarized in survey for DiVLA.",
            "negative_transfer_evidence": "Survey notes diffusion inference cost and sampling inefficiency are limitations; no quantified negative transfer evidence reported.",
            "comparison_to_vision_only": "Survey presents DiVLA as improving over vision-only diffusion policies by adding language reasoning, but no head-to-head numbers provided in survey summary.",
            "temporal_dynamics": "No explicit temporal learning-dynamics analysis reported.",
            "dimensionality_analysis": "No dimensionality analysis reported.",
            "uuid": "e1942.8"
        },
        {
            "name_short": "GraspVLA",
            "name_full": "GraspVLA",
            "brief_description": "A grasping foundation model pretrained on billion-scale synthetic action data that integrates autoregressive perception tasks with flow-matching-based action generation to achieve sim-to-real transfer and zero-shot generalization in grasping.",
            "citation_title": "Graspvla: a grasping foundation model pretrained on billion-scale synthetic action data",
            "mention_or_use": "mention",
            "model_name": "GraspVLA",
            "model_description": "Grasp-focused VLA pretrained on massive synthetic datasets; integrates autoregressive perception and flow-matching action generation, emphasizing grasp affordance prediction and sim-to-real transfer for rigid object grasping.",
            "pretraining_type": "large-scale synthetic action pretraining (billion-scale synthetic demonstrations) focused on grasping",
            "pretraining_data_description": "Synthetic grasping demonstrations with grasp affordance labels and object geometry; includes parametric variations to support generalization (object shapes, poses); language content limited for some variants but combined with perception tasks in model.",
            "target_task_name": "Open-set tabletop grasping and grasp transfer",
            "target_task_description": "Open-set grasping of novel rigid objects with continuous or discrete grasp action outputs; tested on sim-to-real transfer, few-shot and zero-shot adaptation to new objects.",
            "semantic_alignment": "Pretraining provides grasp affordance and object-centric priors that align well with grasping target tasks; language alignment is weaker (more focus on perception-action pairs) unless combined with web semantics in joint training.",
            "performance_with_language_pretraining": "Survey reports GraspVLA demonstrates direct sim-to-real transfer and strong zero-shot generalization; Table V shows GraspVLA few-shot numbers on LIBERO (e.g., entries showing high success rates: 94.1%/91.2%/82.0% in certain splits as reported in survey table excerpts).",
            "performance_without_language_pretraining": "Survey does not list a direct numeric baseline without the large synthetic pretraining; implied large improvement relative to smaller models.",
            "sample_efficiency_comparison": "Survey claims strong few-shot adaptability and direct sim-to-real transfer suggesting substantial sample-efficiency benefits, but does not quantify exact factors in the summary.",
            "has_sample_efficiency_data": true,
            "attention_analysis": "Survey does not summarize attention visualizations for GraspVLA.",
            "embedding_space_analysis": "No embedding-space clustering analyses reported specifically for GraspVLA in survey.",
            "action_grounding_evidence": "GraspVLA's pretraining on action-labeled synthetic data provides direct evidence of mapping between perception and grasp actions through affordance priors and successful sim-to-real transfer; the survey reports these transfer results as evidence of grounding.",
            "hierarchical_features_evidence": "Survey does not report multi-level feature decomposition analyses, but GraspVLA's architecture couples perception and action modules supporting hierarchical processing in practice.",
            "transfer_conditions": "Works best for rigid object grasping where synthetic data statistics match real objects; limitations for deformables or non-rigid objects are noted in broader survey context.",
            "novel_vs_familiar_objects": "Survey reports strong zero-shot performance on novel objects for GraspVLA (as evidenced by sim-to-real and zero-shot claims in tables), with high success rates in reported leaderboard excerpts.",
            "zero_shot_or_few_shot": "Survey indicates strong zero-shot generalization and few-shot adaptability; LIBERO/other table entries show high few-shot numbers for GraspVLA (see Table V excerpt).",
            "layer_analysis": "No explicit ablation/layer-freezing studies reported for GraspVLA in survey summary.",
            "negative_transfer_evidence": "Survey does not report explicit negative transfer for GraspVLA; limitations discussed are focused scope (rigid objects) rather than negative transfer magnitudes.",
            "comparison_to_vision_only": "Survey implies that action-pretraining on synthetic demonstration data outperforms vision-only pretraining for grasp tasks; exact numeric comparisons are not detailed in the summary.",
            "temporal_dynamics": "No temporal dynamics analysis reported.",
            "dimensionality_analysis": "No dimensionality analysis reported.",
            "uuid": "e1942.9"
        },
        {
            "name_short": "3D-VLA",
            "name_full": "3D-VLA (3D Vision-Language-Action model)",
            "brief_description": "A generative world-model VLA integrating 3D scene representations (object/location tokens) with language and action generation to improve spatial grounding and multi-step manipulation.",
            "citation_title": "3d-vla: A 3d vision-language-action generative world model",
            "mention_or_use": "mention",
            "model_name": "3D-VLA",
            "model_description": "VLA architecture that incorporates 3D-aware world modeling (object/location tokens, 3D representations) into a vision-language-action generative framework for better spatial reasoning and action generation.",
            "pretraining_type": "multimodal pretraining incorporating 3D scene data plus action demonstrations (mix of in-domain action data and 3D scene representations)",
            "pretraining_data_description": "Pretraining data include 3D scenes (point clouds/RGB-D), paired language descriptions and demonstrations; captures object-centric spatial relations and physical placements.",
            "target_task_name": "3D-aware language-conditioned manipulation and multi-step planning",
            "target_task_description": "Tasks requiring spatial reasoning in 3D (object localization, placement, bimanual coordination) in simulation and real settings; targets discrete/continuous actions depending on downstream head.",
            "semantic_alignment": "High semantic alignment for spatial/object-centric tasks due to explicit 3D tokens and object-location representations used during pretraining.",
            "performance_with_language_pretraining": "Survey claims 3D-VLA improves spatial grounding and generalization in multi-step manipulation; no single numeric aggregate provided in the survey summary, but it appears in leaderboard and comparative tables as competitive.",
            "performance_without_language_pretraining": "Survey does not provide a direct numeric comparison without language pretraining for 3D-VLA.",
            "sample_efficiency_comparison": "Survey suggests 3D representations improve data efficiency for spatial tasks; however, explicit numeric sample-efficiency comparisons are not provided in the summary.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Survey does not report attention/interpretability analyses specifically for 3D-VLA.",
            "embedding_space_analysis": "No embedding-space analyses provided in survey summary for 3D-VLA.",
            "action_grounding_evidence": "3D-VLA's object/location tokenization and joint world/action modeling provide architectural grounding hypothesized to improve mapping from language to 3D actions; survey reports qualitative improvements but no fine-grained grounding metrics.",
            "hierarchical_features_evidence": "3D-VLA explicitly separates 3D world modeling from action decoding, implying hierarchical representations; no empirical layerwise evidence provided in survey.",
            "transfer_conditions": "Transfer benefits when target tasks require explicit 3D spatial reasoning and when depth/point-cloud sensors are available; limited by closed-loop feedback in some variants per survey.",
            "novel_vs_familiar_objects": "Survey does not report explicit numeric novel vs familiar object comparisons for 3D-VLA.",
            "zero_shot_or_few_shot": "Survey describes 3D-VLA improving zero-shot/transfer in spatial tasks qualitatively; no precise shot counts provided.",
            "layer_analysis": "No layer/component ablations summarized for 3D-VLA in survey.",
            "negative_transfer_evidence": "Survey notes limited closed-loop feedback as a drawback in some 3D-VLA variants; no explicit negative transfer numbers.",
            "comparison_to_vision_only": "Survey implies 3D-VLA (3D-aware) outperforms 2D-only VLAs on spatial tasks qualitatively, but explicit numeric comparisons are not provided.",
            "temporal_dynamics": "No temporal dynamics analyses reported.",
            "dimensionality_analysis": "No dimensionality analysis reported.",
            "uuid": "e1942.10"
        },
        {
            "name_short": "CoT-VLA",
            "name_full": "CoT-VLA (Chain-of-Thought Vision-Language-Action)",
            "brief_description": "A VLA that integrates an explicit visual chain-of-thought by autoregressively predicting future image frames or intermediate visual goals before generating action sequences, improving temporal planning and interpretability.",
            "citation_title": "Cot-vla: Visual chain-of-thought reasoning for vision-language-action models",
            "mention_or_use": "use",
            "model_name": "CoT-VLA",
            "model_description": "Autoregressive VLA augmented with chain-of-thought (CoT) style intermediate visual prediction steps (future images/waypoints) that serve as visual subgoals and improve subsequent action prediction.",
            "pretraining_type": "vision-language pretraining plus CoT-style multimodal finetuning on sequences with intermediate visual goals; mixture of actionful and action-free video data used for visual prediction pretraining.",
            "pretraining_data_description": "Training data includes sequences where intermediate frames/future visuals and language instructions are paired with actions; contains temporal structure, object motions, and affordance cues.",
            "target_task_name": "Long-horizon, multi-step language-conditioned manipulation",
            "target_task_description": "Tasks with long horizons requiring intermediate visual goals and temporal consistency (e.g., multi-step kitchen routines); action outputs are short sequences conditioned on predicted visual subgoals.",
            "semantic_alignment": "High alignment for tasks requiring stepwise decomposition: language instructions map to visual subgoals that align well with task semantics and spatial relationships.",
            "performance_with_language_pretraining": "Survey reports CoT-VLA outperforms OpenVLA by 17% in real-world tasks (explicit claim in Table II summary) and shows large improvements in robustness and interpretability on benchmarks.",
            "performance_without_language_pretraining": "Survey does not give direct numeric baselines of CoT-VLA without CoT or without language pretraining.",
            "sample_efficiency_comparison": "Survey notes CoT improves robustness and sample efficiency qualitatively by structuring predictions, but no explicit numeric sample counts are provided in summary.",
            "has_sample_efficiency_data": true,
            "attention_analysis": "Survey highlights improved interpretability via visual CoT predictions (intermediate images/waypoints), serving as an implicit attention/causal trace, though explicit attention-map analyses are not enumerated.",
            "embedding_space_analysis": "No detailed embedding-space clustering analyses summarized for CoT-VLA, although its intermediate visual goals provide a form of structured latent trajectory.",
            "action_grounding_evidence": "CoT-VLA provides stronger behavioral evidence for grounding by explicitly predicting visual subgoals that mediate between language and actions; survey frames this as evidence of improved language-perception-action grounding.",
            "hierarchical_features_evidence": "CoT-VLA implements an explicit hierarchical decomposition (visual goal prediction then short action sequences), and survey reports this benefits long-horizon tasks — supporting hierarchical feature benefits.",
            "transfer_conditions": "Works well when intermediate visual states are predictive of action sequences and when sensor fidelity supports reliable future-frame prediction; fails when visual prediction error compounds.",
            "novel_vs_familiar_objects": "Survey does not report exact numerical comparisons for novel vs familiar object performance for CoT-VLA, but claims improved generalization qualitatively.",
            "zero_shot_or_few_shot": "CoT-VLA demonstrates improved zero-/few-shot robustness in benchmarks per survey, though precise shot counts not given.",
            "layer_analysis": "Survey does not document detailed layer ablation studies for CoT-VLA within summary.",
            "negative_transfer_evidence": "Survey notes increased computational and memory overhead as a limitation; no quantified negative transfer effects reported.",
            "comparison_to_vision_only": "Survey indicates CoT-VLA outperforms baseline VLAs (e.g., OpenVLA) that lack CoT on real-world tasks by reported margins (17% improvement claimed), implying benefit over simpler vision-language models.",
            "temporal_dynamics": "Survey emphasizes CoT-VLA's explicit modeling of temporal intermediate goals to improve long-horizon consistency; no fine-grained temporal learning curves provided.",
            "dimensionality_analysis": "No dimensionality/intrinsic-dimension measurements reported.",
            "uuid": "e1942.11"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Rt-1: Robotics transformer for real-world control at scale",
            "rating": 2
        },
        {
            "paper_title": "RT-2: Vision-language-action models transfer web knowledge to robotic control",
            "rating": 2
        },
        {
            "paper_title": "PaLM-E: An embodied multimodal language model",
            "rating": 2
        },
        {
            "paper_title": "Openvla: An open-source vision-language-action model",
            "rating": 2
        },
        {
            "paper_title": "Perceiver-actor: A multi-task transformer for robotic manipulation",
            "rating": 2
        },
        {
            "paper_title": "CLIPort: What and where pathways for robotic manipulation",
            "rating": 2
        },
        {
            "paper_title": "Graspvla: a grasping foundation model pretrained on billion-scale synthetic action data",
            "rating": 2
        },
        {
            "paper_title": "3d-vla: A 3d vision-language-action generative world model",
            "rating": 2
        },
        {
            "paper_title": "Cot-vla: Visual chain-of-thought reasoning for vision-language-action models",
            "rating": 2
        },
        {
            "paper_title": "Octo: An open-source generalist robot policy",
            "rating": 1
        },
        {
            "paper_title": "π 0 : A vision-language-action flow model for general robot control",
            "rating": 1
        },
        {
            "paper_title": "Diffusion-vla: Scaling robot foundation models via unified diffusion and autoregression",
            "rating": 1
        }
    ],
    "cost": 0.032383,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Robotic Manipulation via Imitation Learning: Taxonomy, Evolution, Benchmark, and Challenges
4 Sep 2025</p>
<p>Zezeng Li 
Ecole Centrale de Lyon
CNRS
Universite
Claude Bernard Lyon 1</p>
<p>UMR5205
INSA Lyon
Université Lumière Lyon 2
LIRIS
69130EcullyFrance</p>
<p>Alexandre Chapin 
Ecole Centrale de Lyon
CNRS
Universite
Claude Bernard Lyon 1</p>
<p>UMR5205
INSA Lyon
Université Lumière Lyon 2
LIRIS
69130EcullyFrance</p>
<p>Enda Xiang 
Beihang University
BeijingChina</p>
<p>Rui Yang 
Ecole Centrale de Lyon
CNRS
Universite
Claude Bernard Lyon 1</p>
<p>UMR5205
INSA Lyon
Université Lumière Lyon 2
LIRIS
69130EcullyFrance</p>
<p>Bruno Machado 
Ecole Centrale de Lyon
CNRS
Universite
Claude Bernard Lyon 1</p>
<p>UMR5205
INSA Lyon
Université Lumière Lyon 2
LIRIS
69130EcullyFrance</p>
<p>Na Lei 
Dalian University of Technology
DalianChina</p>
<p>Emmanuel Dellandrea 
Ecole Centrale de Lyon
CNRS
Universite
Claude Bernard Lyon 1</p>
<p>UMR5205
INSA Lyon
Université Lumière Lyon 2
LIRIS
69130EcullyFrance</p>
<p>Di Huang 
Beihang University
BeijingChina</p>
<p>Liming Chen 
Ecole Centrale de Lyon
CNRS
Universite
Claude Bernard Lyon 1</p>
<p>UMR5205
INSA Lyon
Université Lumière Lyon 2
LIRIS
69130EcullyFrance</p>
<p>Institut Universitaire de France (IUF)</p>
<p>Robotic Manipulation via Imitation Learning: Taxonomy, Evolution, Benchmark, and Challenges
4 Sep 2025355354CB89B512D2281EC1421CE287ABarXiv:2508.17449v2[cs.RO]Learning and Adaptive SystemsAutonomous AgentsRobot LearningImitation Learning
Robotic manipulation (RM) is central to enabling autonomous robots to interact with and alter their environments in real-world scenarios.Among the learning paradigms, imitation learning has emerged as a powerful approach, allowing robots to rapidly acquire complex manipulation skills from human demonstrations.This survey provides the first systematic review dedicated to imitation learning for robotic manipulation.We identify and analyze a large set of representative studies selected for their scientific quality and community impact.For each, we provide a structured summary covering purpose, technical implementation, taxonomy, input formats, priors, strengths, limitations, and citation metrics.Beyond cataloging, we trace the chronological evolution of imitation learning techniques within robotic manipulation policies (RMPs), highlighting key methodological shifts-from diffusion and flow matching to autoregressive and affordance-driven policies.Where available, we compile benchmark results and conduct quantitative comparisons, enabling an integrated view of performance across tasks and environments.Finally, we outline open challenges such as generalization, embodiment diversity, data efficiency, and benchmark standardization, and we discuss promising directions toward scalable and general-purpose RMPs.By synthesizing methods, benchmarks, and challenges, this survey aims to serve both as an entry point for newcomers and a reference for active researchers seeking to advance imitation learning in RMPs.</p>
<p>I. INTRODUCTION</p>
<p>A. Motivation</p>
<p>Robotic manipulation (RM) refers to the ability of robots to physically interact with and transform their surroundings by grasping, moving, assembling, or otherwise altering objects.It is a core capability for deploying autonomous systems in the real world.The importance of manipulation has long been recognized: already Aristotle described the human hand as "the tool of tools" [1], while Anaxagoras argued that "man is the most intelligent of the animals because he has hands" [2].RM is crucial because it enables automation of tasks that are too dangerous, precise, repetitive, or labor-intensive for humans, spanning domains such as manufacturing, healthcare, logistics, and household assistance.By extending human reach into hazardous or delicate settings, RM enhances safety, efficiency, and productivity.Achieving robust RM, however, requires the development of robust control policies that allow robots to act adaptively in dynamic and unstructured environments-an enduring challenge at the core of modern robotics.</p>
<p>A robotic manipulation policy (RMP) specifies how a robot selects and executes actions based on its sensory observations to achieve a manipulation goal, i.e., π : O × G → A, a t = π(o t , g).Here O is the observation space, G is the goal space (e.g., language instructions, object states), and A is the action space.However, the space of possible object states, contact dynamics, and task variations is combinatorially large, making it impossible to encode effective policies through explicit rules or heuristics.Consequently, current state-ofthe-art approaches are predominantly data-driven, leveraging deep learning to learn representations and control policies from large-scale datasets.Yet, even these approaches face significant challenges, since robots must operate in dynamic and unstructured environments, where not only object properties and task goals but also environmental conditionssuch as lighting, clutter, occlusions, or background changescan vary unpredictably.To address these difficulties, imitation learning (IL) has emerged as a powerful paradigm that learns a policy from expert demonstrations D = {{(o i t , a i t )} Ti t=1 } N i=1 , with the objective π θ ≈ π E , where π E is the expert policy.Recent progress amplified by advances in computer vision and large language models (LLMs) has further enhanced robots' ability to perceive, reason, and plan actions.This survey focuses on IL-based RMPs, providing a comprehensive analysis of methodologies, benchmarks, applications, and outlining future directions toward scalable and general-purpose RMPs.</p>
<p>In light of the fast-paced developments in RMP, this review primarily considers research conducted between 2021 and 2025.Our motivation is to provide newcomers to this field with a rapid and comprehensive understanding of the current research landscape, while also supporting active researchers by helping them efficiently locate related works of interest.By presenting the hierarchical taxonomy, purpose, input, and pre-training strategies, strengths, limitations, and citations of each paper, we aim to distill the core information and enable researchers to quickly assess which approaches align with their specific needs.Furthermore, we present quantitative comparisons across different benchmarks and summarize highly sought-after application areas and key existing challenges in the field, intending to mobilize collective efforts within the community to accelerate progress.</p>
<p>B. Related Surveys</p>
<p>In recent years, several survey papers on embodied intelligence and robotic manipulation have been published.In this Fig.1: Robotic manipulation classification over four perspectives: their purpose, pretraining strategy, input types, and finally their control strategy (DM, FM, GM, NR, AR, NC, and AF denote diffusion model, flow matching, Gaussian mixture, naive regression, autoregressive, naive classification, and affordance) section, we discuss the most relevant ones [3]- [10].</p>
<p>In 2021, Kroemer et al. [3] surveyed a representative subset of publications prior to that year that applied machine learning to RM.They proposed a unified framework by formulating and categorizing existing RM methods based on representation learning, policy learning, and skill transfer.Liu et al. [4] and Dong et al. [6] reviewed advances in deep reinforcement learning for robotic manipulation control.Markku et al. [5] focus on in-contact tasks that robots can handle and how these tasks are controlled and represented.Newbury et al. [7] reviewed research on six-degree-of-freedom (6-DOF) grasp synthesis.Sapkota et al. [10] surveyed over 80 Vision-Language-Action (VLA) models developed in the past 3 years, highlighting key advancements in this area.Ma et al. [11], Zhong et al. [12], and Din et al. [13] also focus on VLA models.</p>
<p>Zheng et al. [8] categorized related work into three main areas: embodied perceptual learning, embodied policy learning, and embodied task-oriented learning.Their survey is the most closely related to ours.However, our work significantly differs in both methodology and information presentation.1) We identified 82 of the most representative papers, selected based on community attention and intrinsic quality.2) For each, we provide a structured summary covering purpose, technical implementation and hierarchical classification, input formats, key priors, strengths and limitations, and citation metrics.This format allows researchers to grasp essential information and locate relevant studies quickly.3) We compiled in Fig. 2 the chronological development of key technologies, offering a timeline-based view for understanding the evolution of the field.4) Where available, we also report benchmark results and provide quantitative evaluations and comparative analyses of existing methods.</p>
<p>C. Paper Selection Criteria and Content Extraction</p>
<p>We began by collecting an initial pool of approximately 150 papers through continuous monitoring of the literature and targeted keyword searches.From this pool, we selected a subset of works that were judged to be classic, distinctive, or particularly influential.To quantify influence, we considered both the average monthly citation count and indicators of community attention such as social media engagement.The selected papers were then organized chronologically within each methodological category, as shown in Tab.I and Tab.II.Owing to space constraints, we typically retained only one representative paper from each series; for instance, within the RT family of models, we report only RT-1 [14].</p>
<p>In Tab.I, we summarized the technique type, target scenarios, challenges, pre-training data, and input data types for each method.The following sections largely expand on the content recorded in Tab.I. To help readers quickly locate the information of interest, we have organized works of the same category into the same section, separated by horizontal lines.Articles within the same technical area are sorted chronologically.Additionally, the "Purpose" section highlights the motivation behind each paper, suggesting potential desirable properties of these methods.Tab.II outlines the core tools used by each method, their pros and cons, and average monthly citations, providing a macro-level evaluation of these approaches.</p>
<p>The remaining sections of this paper are organized according to Fig. 1 and Tab.I. First, we give a hierarchical taxonomy of RMP from the perspective of control strategy and introduce the technological evolution for each technique (Section II).Then, we summarize the target challenge and task scenario of each paper (Section III), summarize pretraining strategies (Section IV), present relevant benchmarks, evaluation metrics, and conduct performance comparisons and analyses (Section V).Finally, we discuss future challenges and research directions, and conclude the survey (Section VII).</p>
<p>II. CLASSIFICATION BASED ON CONTROL STRATEGY</p>
<p>In this section, we present a hierarchical taxonomy of RMP from the perspective of control strategy (see Tab.I for details).We begin by categorizing existing RMP into two main groups: action generation and task planner.Task planners focus on predicting high-level information, such as key poses and affordance maps, and enabling manipulation with the aid of motion planning algorithms.In this taxonomy, we highlight how leading RMP approaches effectively integrate generative models, such as diffusion models [15]- [18], flow matching [19], and autoregressive models [20], [21], alongside
✓ ✓ ✓ ✓ SuSIE [23] ✓ ✓ ✓ ✓ ✓ ChaDiffuser [24] ✓ ✓ ✓ ✓ ✓ ✓ ✓ VPDD [25] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ DP3 [26] ✓ ✓ ✓ ✓ ✓ ✓ Diffuser Actor [27] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Equidiff [28] ✓ ✓ ✓ ✓ ✓ EquiBot [29] ✓ ✓ ✓ ✓ ✓ ✓ MDT [30] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ SDP [31] ✓ ✓ ✓ ✓ ✓ ✓ PSEC [32] ✓ ✓ ✓ ✓ ✓ ✓ AdaManip [33] ✓ ✓ ✓ ✓ ✓ ✓ AffordDP [34] ✓ ✓ ✓ ✓ ✓ ✓ KStar [35] ✓ ✓ ✓ ✓ ✓ BRS [36] ✓ ✓ ✓ ✓ ✓ ✓ CotPolicy [37] ✓ ✓ ✓ ✓ ✓ Octo [38] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ DiVLA [39] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Cogact [40] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ChatVLA [41] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ RDT-1B [42] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ GO-1 [43] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ GR00TN1 [44] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ DreamGen [45] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ HybridVLA [46] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ FMP [47] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ RDP [48] ✓ ✓ ✓ ✓ ✓ ActionFlow [49] ✓ ✓ ✓ ✓ ✓ ✓ π0 [50] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ GraspVLA [51] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ HiRobot [52] ✓ ✓ ✓ ✓ ✓ ✓ ✓ SmolVLA [53] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ MVP [54] ✓ ✓ ✓ ✓ ✓ ✓ GR-1 [55] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ RoboUniView [56] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Lift3D [57] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ SAM2Act [58] ✓ ✓ ✓ ✓ ✓ ✓ OpenVLAOFT [59] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Gato [60] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ VIMA [61] ✓ ✓ ✓ ✓ ✓ RT-1 [14] ✓ ✓ ✓ ✓ ✓ ✓ ✓ PaLM-E [62] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ACT [63] ✓ ✓ ✓ ✓ ✓ ✓ RoboFlamingo [64] ✓ ✓ ✓ ✓ ✓ ✓ ✓ 3D-VLA [65] ✓ ✓ ✓ ✓ ✓ ✓ ✓ VQ-BeT [66] ✓ ✓ ✓ ✓ ✓ OpenVLA [67] ✓ ✓ ✓ ✓ ✓ ✓ ✓ QueST [68] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ARP [69] ✓ ✓ ✓ ✓ ✓ CARP [70] ✓ ✓ ✓ ✓ ✓ TraceVLA [71] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ RoboVLM [72] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Fast [73] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ SpatialVLA [74] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ VLACache [75] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Hamster [76] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Magma [77] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ CoT-VLA [78] ✓ ✓ ✓ ✓ ✓ ✓ ✓ UniVLA [79] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ LAPA [80] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ECoT [81] ✓ ✓ ✓ ✓ ✓ ✓ ✓ WorldVLA [82] ✓ ✓ ✓ ✓ ✓ ✓ ✓ LOTUS [83] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ HULC [84] ✓ ✓ ✓ ✓ ✓ ✓ ✓ BridgeVLA [85] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ SEDF [86] ✓ ✓ ✓ ✓ ✓ A0 [87] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ FlowMS [88] ✓ ✓ ✓ ✓ ✓ ✓ PolarNet [89] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ HiveFm [90] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ PerAct [91] ✓ ✓ ✓ ✓ ✓ ✓ ✓ RVT [92] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Act3D [93] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ SAM-E [94] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ EquAct [95] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Cliport [96] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ MOKA [97] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ RAM [98] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ReKep [99] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ 3D-LOTUS [100] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ OmniManip [101] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ GeminiRob [102] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓
naive regression and classification with multi-layer perceptron (MLP).We also discuss the evolution of each technique.Further elaborations are provided in the subsections that follow.</p>
<p>A. Policy for Action Generation</p>
<p>For action generation methods, we further classify them based on the type of action produced: continuous or discrete.</p>
<p>1) Continuous Action: For continuous action generation, we categorize the methods into several types: diffusion modelbased, flow matching-based, and naive regression-based.</p>
<p>a) Action generation via diffusion models: The progression of this area has evolved from the initial diffusion policy to 3D diffusion policy, followed by the introduction of equivariant diffusion policy.This was further enhanced with the combination of diffusion and autoregressive policies, leading to the emergence of the test-time diffusion policy.</p>
<p>Diffusion Policy (DP) [22] pioneered this idea by formulating control as a conditional denoising process, iteratively refining actions from noise given current observations.Building on this, SuSIE [23] used a pre-trained image-editing diffusion model for visual subgoal generation, paired with a controller to execute these goals, enabling zero-shot generalization.ChaDiffuser [24] and VPDD [25] incorporated autoregressive planning and predictive video modeling to extend diffusion into longer-horizon control.Spatial reasoning was enhanced in 3D Diffusion Policy (DP3) [26] and 3D Diffuser Actor [27] by integrating point clouds and 3D scene features, improving robustness to viewpoint and object variation.To exploit structural priors, EquiDiff [28] and EquiBot [29] embedded SO (2) or SE(3) equivariance into diffusion, yielding higher data efficiency and invariance to rotations and translations.</p>
<p>Recent work has also targeted continual learning, adaptability, and domain transfer.SDP [31] accelerated sampling with reduced denoising steps; PSEC [32] represented skills in parameter space via LoRA-style modules, allowing incremental expansion and composition without retraining the whole model.AffordDP [34] guided sampling with transferable 3D affordances to enable cross-category generalization, while KStar [35] enforced kinematics-aware planning for collisionfree dual-arm motions.AdaManip [33] adapted to articulated objects with configuration-dependent control.These advances have moved diffusion policies from lab demonstrations to real-world deployments, such as BRS [36] for household tasks and CoTPolicy [37], which integrates chain-of-thought planning with diffusion to tackle complex multi-instruction, long-horizon tasks.Collectively, these works expand diffusion policies into versatile and semantically aware controllers.</p>
<p>In parallel, foundation models have combined diffusion with large-scale vision-language learning to enable generalist robot behavior.Octo [38] trained on 800k diverse demonstrations, supporting multi-modal commands and multi-embodiment control via transformer-based diffusion decoding.DiVLA [39] unified vision-language model (VLM) reasoning with a diffusion control head, translating high-level instructions into lowlevel actions.CogACT [40] decoupled perception/reasoning from a learned diffusion transformer for improved adaptability, while ChatVLA [41] staged training between a diffusion con-troller and a pre-trained VLM, producing agents that both understand instructions and execute precise actions.Scaling has amplified capabilities.RDT-1B [42], with 1.2B parameters and over a million demonstrations, achieved state-of-the-art performance on complex bimanual skills.GO-1 [43] demonstrated cross-embodiment generalization across hundreds of robots and tasks by learning a unified latent action planner.GR00T-N1 [44] paired a frozen VLM for semantic reasoning with a learned diffusion executor for reactive control, improving interpretability and task decomposition.Data augmentation has also been explored: DreamGen [45] synthesized novel demonstrations via generative world and action models to enhance robustness.Finally, HybridVLA [46] combined autoregressive reasoning for long-horizon planning with diffusion for precise action generation, outperforming single-paradigm baselines.</p>
<p>b) Action generation via flow-matching: Flowmatching, as a novel generative model, has garnered significant attention due to its ability to generate high-quality results more quickly compared to diffusion models.Many researchers have explored its potential in combination with robotic action generation, leading to the emergence of several outstanding works in this area.Initially, FMP [47] gave the first attempt to ground vision-language model (VLM) affordance with flow matching for robot manipulation, enabling more stable training and faster inference.Reactive diffusion policy (RDP) [48], a novel slow-fast visual-tactile imitation learning algorithm, allowing robots to adjust their policies in real-time for contact-rich manipulation tasks.ActionFlow [49] introduces an SE(3) invariant Transformer, which enables informed spatial reasoning based on the relative SE(3) poses between observations and actions.Later, π 0 [50] integrates pre-trained VLM with flow matching to inherit Internet-scale semantic knowledge, enhancing the robot's ability to efficiently learn and execute a wide variety of tasks, such as laundry folding, table cleaning, and assembling boxes.GraspVLA [51] integrates autoregressive perception tasks and flow-matching-based action generation into a unified chain-of-thought process, enabling joint training on synthetic action data and Internet semantics data, exhibiting direct simto-real transfer and strong zero-shot generalization, as well as few-shot adaptability to specialized scenarios and human preferences.This was further advanced by HiRobot [52], which utilizes VLMs for both high-level reasoning and low-level task execution, enabling the processing of much more complex prompts.Most recently, SmolVLA [53] focused on making these techniques more efficient in resource-constrained environments, incorporating lightweight visual learning systems while maintaining the robustness of flow matching.Overall, these developments illustrate the continuous refinement of flow-matching techniques, expanding their applicability from basic manipulation to more complex, adaptable, and resource-efficient systems.c) Action generation via naive regression: Some researchers, particularly those working with the VLA model, argue that the visual and language encoders are responsible for the core task of information extraction, a function that is universal across tasks.In this view, the action policy head merely needs to predict the actions based on the extracted features, with different scenarios potentially requiring fine-tuning of the action policy head.As a result, they suggest that the action generation module does not require a complex architecture.Instead, they opt for a simple MLP and use straightforward regression techniques to learn action generation.</p>
<p>The evolution of action generation via naive regression has progressed through several key stages.Initially, MVP [54] introduced a masked vision-policy framework, which regressed action commands directly from visual features.This approach demonstrated that self-supervised visual pre-training can be effective for motor control.GR-1 [55] generative video models to predict future frames and behaviors, and shows that such pretraining benefits downstream robot manipulation when finetuned.RoboUniView [56] further advanced the model by integrating multi-view vision systems, addressing the challenge of being sensitive to camera specifications and positions.Lift3D [57] then extended this approach by incorporating 3D depth information, improving accuracy in spatially complex environments.SAM2Act [58] equips the agent with spatial memory awareness, allowing it to solve spatial memory-based tasks.Most recently, OpenVLAOFT [59] explored crucial design choices for adapting VLAs, including various action decoding schemes, action representations, and learning objectives for fine-tuning.Using OpenVLA as the base model, this work provides valuable insights on how to fine-tune VLAs for novel setups.Together, these advancements illustrate a shift from simple regression to more adaptable and robust approaches in robotic action generation.</p>
<p>2) Discrete Action: Discrete action generation is a crucial component of RMP, where the goal is to predict and execute a finite set of distinct actions based on sensory inputs.Its advantage lies in reducing the solution space and simplifying the problem-solving process.But it cannot provide fine-grained, continuous manipulation actions.Discrete action generation methods are generally categorized into two main approaches: autoregressive-based methods and naive classification-based methods.Autoregressive models generate actions sequentially, conditioning actions on the previous, while naive classification models treat the problem as a classification task, directly mapping inputs to discrete actions.</p>
<p>One of the key advantages of discrete action generation is its efficiency.Since actions are categorized into distinct classes, the learning process benefits from the vast array of classification techniques available in deep learning, enabling faster and more stable training.These methods can also leverage well-established classification models and frameworks, making them particularly well-suited for tasks with clear, finite action spaces.However, a notable disadvantage is the lack of precision.Since discrete action generation typically relies on predefined action categories, it may struggle with tasks that require fine-grained, nuanced control.The discrete nature of these models limits their ability to handle complex or highly variable tasks that require more detailed action specifications.</p>
<p>This section explores both autoregressive-based and naive classification-based approaches, emphasizing their unique characteristics, challenges, and key advancements in the field.</p>
<p>a) Discrete action generation via autoregressive model: Given the success of Transformer-based networks in LLMs and VLMs, an increasing number of studies have integrated the language modality into visuomotor policies, establishing a paradigm of autoregressive action generation conditioned on multimodal inputs.Gato [60] firstly frames a single large Transformer as a generalist embodied agent able to produce multiple output modalities from diverse inputs, which demonstrates the feasibility of weight-sharing across tasks and embodiments while highlighting limits in scaling and task-specific performance.VIMA [61] extends the prompting paradigm to robotics by using multimodal prompts to specify tasks; it introduces a desktop-task suite and shows that a Transformer trained with multimodal prompts generalizes well in few-shot settings.RT-1 [14] presents an engineering recipe for tokenizing all inputs outputs and training a large end-toend Transformer to generate actions directly from raw observations.It's worth noting that although the RT-1 model is small, it establishes the VLA concept; RT-2 [103] later explicitly defines and popularizes it, and this has had a profound impact on subsequent work.PaLM-E [62] injects visual and state embeddings into a large language model to form a single embodied multimodal model, showing that joint multimodal training yields positive transfer: the model can perform per-ception, multi-step reasoning, and control-related tasks within the same architecture.RoboFlamingo [64] demonstrates that large vision-language foundation models can be repurposed as effective imitation policies: by adding lightweight policy heads and small amounts of tuning, VLMs can interpret multimodal demonstrations and drive robot behavior with minimal modification.OpenVLA [67] provides an open-source VLA model trained on large-scale vision-language-action datasets, with a unified architecture for perception, reasoning, and control.It aims to democratize research in generalist policies and offers tools for fine-tuning and evaluation.RoboVLM [72] provides a systematic empirical study on the choices that most affect VLA model performance, covering dataset composition, architecture design, and training strategies.It offers guidelines for building more capable generalist robot policies.</p>
<p>Using autoregressive models in single-task or singlescenario settings is another widely studied approach.ACT [63] package high-frequency, low-level controls into discrete "action chunks," greatly shortening autoregressive sequence lengths and improving sample efficiency in low-demo regimes.VQ-BeT [66] proposes representing actions in a compact latent space learned from demonstrations, which allows for smooth, temporally consistent behavior generation.This reduces the complexity of action prediction and improves generalization across different manipulation tasks.QueST [68] introduces a method for learning skill abstractions in a self-supervised manner, where low-level control policies are organized into reusable high-level skills.ARP [69] introduces variable-step or mixed-token prediction mechanisms that let a single architecture emit tokens at different frequencies and granularities, reducing the number of autoregressive steps while increasing adaptability.CARP [70] extends autoregressive policy learning with a coarse-to-fine prediction scheme, where a high-level action plan is refined into fine-grained motor commands.This structure improves efficiency and accuracy in manipulation.</p>
<p>Subsequent works often improve the quality of action generation from different directions; one approach is to enhance the model's spatial perception capabilities.3D-VLA [65] introduces a generative world model that jointly learns 3D visual perception, language understanding, and action generation by integrating 3D scene representations into a vision-languageaction architecture.Based on OpenVLA, TraceVLA [71] introduces visual trace prompting, which augments inputs with visual trajectory cues from past frames to improve spatialtemporal grounding.This boosts generalization and performance of VLA models on multi-step manipulation tasks.SpatialVLA [74] introduces spatially aware representations include Ego3D position encoding and adaptive action grids, to inject 3D robot-agnostic spatial structure into VLA models, showing improved zero-shot and transfer performance after pretraining on millions of real robot episodes.Hamster [76] argues for a hierarchical VLA design where a high-level VLM predicts coarse 2D trajectories and a low-level 3D controller executes motions.This separation improves domain transfer and yields large gains over monolithic VLA baselines.</p>
<p>Meanwhile, because robot demonstration data are scarce, several works have begun exploring how to learn useful action representations from large-scale actionless videos.LAPA [80] proposes unsupervised latent-action pretraining from actionless videos by learning discrete latent actions and then finetuning to map latents to robot actions, enabling large-scale pretraining without costly robot action labels.UniVLA [79] learns task-centric latent action representations from heterogeneous, cross-embodiment videos so a single policy can be decoded onto different robots, enabling scalable, computeefficient pretraining that transfers across embodiments and environments.WorldVLA [82] unify a discrete autoregressive world model and an action model in one VLA framework, so images and actions are jointly modeled.The coupled model improves video prediction and action performance, and introduces masking attention strategies to reduce autoregressive action error accumulation.Some works aim to improve model encoding and inference efficiency, since the generation frequency is also critical for manipulation tasks.Fast [73] proposes a compression-based action tokenization that transforms short action chunks into frequency space using the discrete cosine transform, then compresses those coefficients with byte-pair encoding, producing compact tokens that better represent high-frequency.VLA-Cache [75] proposes an adaptive token-caching mechanism that detects unchanged visual tokens across timesteps and reuses their key-value computations, reducing VLA inference cost with minimal loss in success rate.</p>
<p>Other works also introduce the concept of chain-ofthought (CoT) reasoning from LLMs, achieving promising results as well.ECoT [81] trains VLAs to perform embodied, multi-step reasoning like plans, sub-tasks and visually grounded waypoints before predicting actions, showing large improvements in robustness and interpretability without extra robot data.CoT-VLA [78] integrates explicit visual CoT by autoregressively predicting future image frames as intermediate visual goals before generating short action sequences.This approach improves temporal planning and yields substantial gains on real and simulated manipulation benchmarks.</p>
<p>b) Discrete action generation via naive classification:</p>
<p>The evolution of discrete action generation via naive classification can be observed across the following works: HULC [84] was one of the first to apply naive classification for discrete action generation.It simplifies the action generation process by treating it as a classification task, enhancing the efficiency for robotic manipulation.BridgeVLA [85] expanded on this approach by integrating VLAs.It unifies visual and language representations to better contextualize actions within dynamic environments.To leverage the structural priors of the 3D input, BridgeVLA is trained to predict 2D heatmaps, which facilitate more accurate translational action prediction.VPDD [25] further advances the field by introducing a discrete diffusion model to combine generative pre-training on human videos and discrete diffusion policy fine-tuning on a small number of action-labeled robot videos.VPDD emphasizes learning to predict not only actions but also the conditions under which specific actions should be executed, improving the model's generalization ability to new tasks.</p>
<p>B. Policy for Task Planner</p>
<p>For methods leveraging task planners, we further classify them based on method's type used to produce the targets: generative models, naive classification or affordance prediction.</p>
<p>a) Task planner via generative models: There are two primary generative models which has been integrated into task planner-based robotic manipulation, i.e., diffusion models and flow-matching models.</p>
<p>SEDF [86] was one of the pioneering works that combined diffusion models with robotic manipulation.It decouples the grasp pose selection and the motion planning, resolves the grasp and motion planning problem by iteratively improving the trajectory to jointly minimize the object-grasp diffusion cost and the task-related costs.A0 [87] further advanced this approach by introducing a hierarchical, affordance-aware diffusion model.This model decomposes the manipulation task into high-level spatial affordance understanding and lowlevel action execution, significantly enhancing generalization capabilities and offering an embodiment-agnostic design, making it adaptable to a wide range of robotic systems.</p>
<p>Building on flow matching, FlowMS [88] introduces an imitation learning architecture designed for multi-support manipulation tasks, enabling a multi-contact whole-body controller.Leveraging flow matching, it enhances the efficiency of generating feasible manipulation trajectories.</p>
<p>b) Task planner via naive classification: PerAct [91] is one of the earliest motion-planning-based robotic manipulation policies.It takes a language goal and a voxel grid reconstructed from RGB-D sensors as input, and uses MLPs as the policy head to predict discrete actions, which are then executed using a motion planner.Given the computational burden introduced by voxel-based 3D representations, RVT [92] optimized Per-Act's network architecture by proposing a multi-view Transformer, reducing complexity while maintaining performance.Act3D [93] extends this by lifting 2D pre-trained features to 3D using sensed depth, and predicts the 3D location of the end effector through classification of 3D points in the robot's workspace.SAM-E [94] incorporated SAM [104] as the foundation model for extracting task-relevant features, then framed action prediction as a classification problem, guided by heatmaps, to improve generalization in few-shot adaptations to new tasks.Most recently, EquAct [95] leveraged SE(3) equivariance as a key structural property shared by both policy and language, enhancing generalization to novel 3D scene configurations.This progression reflects a shift from simpler action representations to increasingly sophisticated frameworks that are more generalizable and efficient.c) Task planner via affordance prediction: In robotic manipulation, affordance refers to the potential actions or interactions that an object allows a robot to perform.It represents the relationship between an object and the robot, essentially describing how a robot interacts with objects in its environment.Affordance can be expressed in various forms, such as keypoint [97]- [99], segmentation masks [58], [94], [100], and affordance maps [34], [96], [102].By recognizing and utilizing affordances, robots can make more informed decisions about how to manipulate objects, even in dynamic or unstructured environments.This approach reduces the need for exhaustive task-specific programming, allowing robots to generalize across different scenarios, handle a wider range of objects, and perform more complex manipulation tasks.</p>
<p>The evolution of manipulation via motion planning with affordance techniques progresses from Cliport [96], which formulates tabletop rearrangement as a series of languageconditioned affordance predictions, and thus benefits from the strengths of data-driven scale and generalization.RAM [98] refined this approach by introducing a retrieve-and-transfer framework for zero-shot robotic manipulation.It hierarchically retrieves the most similar demonstration from the affordance memory and transfers such out-of-domain 2D affordance to in-domain 3D executable affordance in a zeroshot and embodiment-agnostic manner.MOKA [97] advanced affordance-driven manipulation by incorporating a compact point-based representation of affordance, bridging the VLM's predictions on observed images and the robot's actions in the physical world.By representing a manipulation task as a sequence of Relational Keypoint Constraints, ReKep [99] introduced a hierarchical optimization procedure to solve for robot actions.3D-LOTUS [100] focused on improving generalization by integrating foundation VLMs for task planning.It decomposes tasks into step-by-step plans and grounds objects by localizing them with segmentation masks.Once the objects are grounded and the primitive actions are identified, 3D-LOTUS serves as a motion controller to generate action trajectories.GeminiRob [102] utilizes Gemini 2.0 for open-world affordance prediction, enhancing its ability to generalize to unseen environments.This progression reflects a shift toward a more adaptive and generalist agent capable of handling dynamic, complex real-world environments.</p>
<p>III. CLASSIFICATION BASED ON PURPOSE</p>
<p>This section outlines the motivation behind each paper, highlighting potential desirable properties of the methods.We have primarily classified the works from two views: scenarios and challenges.We aim to help readers quickly find relevant works based on specific scenarios or challenges.</p>
<p>A. Policy for Single Task</p>
<p>Early advances in robotic manipulation policy often targeted single-task settings, where the objective was to master a specific manipulation skill efficiently and reliably.CLIPort [48] uses CLIP-extracted semantic visual features to condition pixel-level pick-and-place/transport policies, combined with transport/affordance prediction for sample-efficient single-task learning.ACT [48] packs high-frequency, low-level controls into action chunking and performs autoregression at the chunk level, substantially shortening sequence length and improving stability and success rates for fine-grained single tasks with few demonstrations.Diffusion Policy [48] uses conditional diffusion models to generate continuous actions or trajectories, excelling at modeling multi-modal target distributions and producing high-quality, natural action sequences.In single-task settings it often yields robust and diverse solutions.Equivariant Diffusion Policy [28] Incorporates SO(2) equivariance into diffusion policies, leveraging task symmetries to improve sample efficiency, generalization, and robustness to observation transforms for single task.Reactive Diffusion Policy [48] places diffusion generation inside a closed-loop control loop, repeatedly resampling and adapting actions based on the latest tactile/force inputs.It performed impressively on a single contact-rich task.Collectively, these studies systematically explore optimal architectures, training regimes, and inference pipelines for single-task control, thereby providing a robust foundation for the development of generalist policies that can scale to multiple tasks and diverse environments.</p>
<p>B. Policy for Multi-Task Under Single Scenario</p>
<p>While model advanced, new iterations created models trained on several tasks but limited to a specific scenario.Some works leverage stronger spatial perception and flexible conditional generation to cover different tasks in same environment.MDT [30] and 3D Diffusion policy [26] emphasize high-quality, long-horizon generation conditioned on multimodal and 3D inputs.PerAct [91] emphasizes extreme sample efficiency for many task variants within one scene.RVT [92] uses multi-view aggregation and view synthesis within a Transformer architecture to build scalable, high-fidelity 3D perception, achieving fast training and inference while maintaining precision.Other works centers on optimization-driven approaches, using visual relational keypoint constraints to structure tasks so that action sequences can be produced by optimization-yielding interpretable behavior and strong zeroshot adaptability.VoxPoser [105] grounds LLM reasoning and constraint into spatial value functions and then uses planning to produce trajectories instead of training policies to directly output actions.ReKep [99] models tasks using relational keypoint constraints, framing task execution as a set of optimizable constraints.Overall, these works, whether because of limitations in problem formulation and nongeneral experimental settings, or because they are constrained by model capacity and the diversity of training data, have not demonstrated task completion across different tasks and scenes.</p>
<p>C. Policy for Multi Scenario</p>
<p>A few years back, Gato [60] initiated the idea of creating a multi-purpose "generalist agent", capable of solving a set of tasks, even tasks it has never seen.A main issue to solve this problem is access to reliable data.Recently, the scale of robotic data has increased by a large margin thanks to community initiatives such as Open-X [106].This permitted new models to emerge by getting inspiration from other fields such as natural language processing and computer vision for pretraining (see Section IV).Leveraging the reasoning capacities of pretrained Visual Language Models, a set of new methods was introduced to obtain agents capable of understanding and reasoning over scenes before acting, leading to multi-scenario policies capable of generalizing.One of the first iteration of such model is RT-1 [103] which pretrains a visual model with text conditioning on a large-scale dataset of tasks.Following work leverages pretrained models on out-ofdomain data for their strong reasoning capabilities to learn an action model, which leads to the set of VLAs.</p>
<p>RT-2 [103] Discretizes actions into tokens and jointly pretrains/fine-tunes on large-scale vision-language (VQAstyle) data together with real trajectory data, thereby transferring web-scale semantic and reasoning capabilities into multitask, cross-scene robot control and achieving strong zero-/few-shot generalization and semantic reasoning performance.OpenVLA [67] Provides an open-source VLA pretrained on nearly a million real robot demonstrations, emphasizing fast, parameter-efficient fine-tuning for new robots and tasks; it champions large-data-driven generality, reproducibility, and adaptability in cross-scene, multi-task settings.HiRobot [52] uses a hierarchical inference-execution pipeline that separates high-level language reasoning/task decomposition from lowlevel real-time control and supports integration of human feedback and online corrections during execution, improving adaptability and robustness on open-ended, multi-step, interactive tasks.CoT-VLA [78] introduces a visual chain-of-thought by autoregressively predicting future image frames as intermediate visual goals before generating short action sequences, enhancing temporal reasoning, stepwise planning, long-horizon consistency, and model interpretability across multi-step and multi-scene tasks.SwitchVLA [107] proposes an executionaware task-switching framework that treats task switches as behavior-modulation problems conditioned on execution state and instruction context; by segmenting demonstrations into contact phases and training multi-behavior conditional policies, it improves smoothness and robustness when switching tasks in dynamic and interactive environments.</p>
<p>D. Policy for Real-world Scenario</p>
<p>Currently, while most works incorporate real-world validation, the following efforts specifically optimize for realworld scenarios, making targeted improvements in data scale, training strategies, and data standardization, and demonstrating remarkable performance on real-world manipulation tasks.RT-2 [103] treats actions as discrete tokens and jointly trains and fine-tunes on VQA-style visual-language data together with real-world trajectory data, successfully transferring the capabilities of large VLMs to real-world robotic tasks.π 0 [50] focuses on combining pretrained vision-language knowledge with continuous, high-frequency action generation: it uses flow-matching-based continuous action modeling to preserve high control rates and action smoothness, yielding better consistency and frequency responsiveness in semantically guided real-world manipulation.HiRobot [52] proposes a hierarchical inference-execution pipeline that separates high-level language reasoning and task planning from low-level realtime control; this design improves adaptability to open-ended, multi-step, and interactive real-world tasks and facilitates handling dynamic scenes and online instruction corrections.Geminiobot [102], built on the large multimodal foundation model Gemini, applies powerful language understanding, spatial reasoning, and generative capabilities directly to the robot's closed-loop perception-planning-execution stack, exploiting large models' strengths in complex reasoning (e.g., multistep fine manipulations and tool use).BRS [36] focuses on whole-body household scenarios and develops corresponding hardware and data-collection pipelines suited for long-horizon tasks, dual-arm coordination, and constrained-space manipulation.It emphasizes standardization and systematization of large-scale real demonstrations, demonstrating the robustness and reproducibility of end-to-end policies on household tasks.</p>
<p>E. Policy for Continual Learning</p>
<p>Across recent robot manipulation work, Continual Learning is pursued via complementary policy designs.LOTUS [83] discovers reusable visuomotor skills from raw demos and composes them with a meta-controller, yielding a continually expanding skill library for lifelong imitation.SDP [31] embeds sparsity into diffusion policies via MoE, activating only a few experts per task to curb interference, retain past skills as tasks accrue.PSEC [32] treats skills as plug-and-play LoRA modules in parameter space, enabling iterative expansion and direct skill composition with lightweight routing.ChatVLA [41] tackles forgetting and task interference when unifying perception-language understanding with control, using phased alignment and MoE to co-train a generalist, extensible VLA policy.All four decouple what to reuse (skills/experts/adapters) from how to compose (router/meta-controller), updating only lightweight modules instead of rewriting the whole controller to reduce cross-task interference.</p>
<p>F. Policy for Generalization</p>
<p>Currently, robot manipulation learning based on imitation learning suffers from evident overfitting, and the generalization of the policies remains a widespread and unresolved issue.Generalization itself is a broad problem, and researchers have attempted to enhance it from various perspectives, which we categorize into the following two main areas.</p>
<p>• Intra-task generalization demands that a learned policy remain robust under a wide range of contextual shifts, such as changes in object appearance, shape, pose, lighting, distractors, background, or embodiment.• Inter-task generalization requires robots not just to recall previously learned behaviors, but to systematically accumulate, organize, and recombine those skills to perform entirely new tasks.For example, intra-task generalization implies that a model trained for the task "pick up the mug and place it on the shelf" should be able to handle different instances of mugs (tall vs. short, ceramic vs. plastic, with or without handles), varying poses (mug upright, on its side, or partially occluded), different environments (e.g., office, home), and potential distractors (e.g., cups, bowls nearby).In contrast, inter-task generalization means a model trained on tasks such as: a) picking up the mug and placing it on the shelf, and b) pushing the red block to the left pad, should be able to generalize to a novel task: d) stacking the blue bowl on top of the green plate.In this case, the verb "stack" was not seen during training, the new object pair (bowl, plate) introduces unique object interactions and stability constraints, and the goal condition (achieving vertical alignment for stacking) differs both semantically and physically from the previous tasks.</p>
<p>For intra-task generalization, Cliport [96], GR-1 [55], and RoboFlamingo [64] can generalize across instances with varying colors and positions.VIMA [61] and PaLM-E [62] demonstrate ability to generalize to new combinations of objects.MVP [54] disentangles shape and color, enabling it to handle different object geometries and colors.SuSIE [23] introduces a goal-conditioned policy based on future observation prediction.All works tackle intra-task generalization from the perspective of objects, while others focus on addressing generalization from the perspective of the scene.RAM [98] generalizes across various objects and environments using a retrievalbased transfer paradigm.3D-LOTUS [100] and Lift3D [57] enhance generalization across different instances, background scenes, and lighting conditions.RoboUniView [56] maintains high performance under unseen camera parameters by utilizing a unified view representation.Diffuser Actor [27] leverages 3D point clouds to boost generalization across camera viewpoints.SAM2Act [58] improves generalization to diverse scene perturbations by incorporating a memory bank.</p>
<p>For embodiment generalization, Cogact [40], UniVLA [79], and SpatialVLA [74] provide examples of good generalization to unseen embodiments and tasks.Hamster [76], TraceVLA [71], and OpenVLA [67] exhibit strong generalization to different embodiments and scenes, accommodating varying lighting and distractors.Despite these initial approaches, generalization across different embodiments is still limited to similar morphologies and application scenarios.Currently, the generalization of manipulation policies between embodiments with different configurations, particularly those with varying degrees of freedom, has not yet been achieved.</p>
<p>For inter-task generalization, Gato [60], PaLM-E [62], GeminiRob [102], and 3D-VLA [65] present generalist models that go beyond manipulation tasks.RT-1 [14], Octo [38], pi 0 [50], RDT-1B [42], GO-1 [43], and GraspVLA [51] improve task generalization by applying scaling laws: more data and model parameters.HiveFm [90] and SAM-E [94] achieve generalization for new RLBench [108] tasks.ReKep [99] and RoboVLM [72] show generalization to unseen objects and tasks.DiVLA [39] and HybridVLA [46] combine the continuous nature of diffusion-based actions with the reasoning capabilities of autoregressive generation, leading to strong generalization.AffordDP [34] enhances generalization by incorporating affordance prediction from VLMs, a widely adopted approach for improving generalization.DreamGen [45] improves generalization by generating robot data using video world models.LAPA [80], VPDD [25], and GR00T N1 [44] were pretrained with massive action-less data, further enhancing generalization across tasks.Magma [77] develops spatialtemporal intelligence to achieve generalization in long-horizon tasks.VIDEOSAUR [109] reveal that object-centric representations benefit generalization ability.QueST [68] demonstrates generalization to new skills.ADPro [110] proposes a testtime adaptive policy via manifold and initial noise constraints.HiRobot [52], ECoT [81], and CoT-VLA [78] utilize the planning and reasoning capabilities of LLM-based CoT to generalize across new tasks and long-horizon challenges.</p>
<p>G. Policy for Data Efficiency</p>
<p>In imitation learning, acquiring expert data with action labels is both time-consuming and labor-intensive, making it impractical to prepare expert data for every task or scenario.Given these real-world challenges, research on achieving effective model learning with fewer labeled data-i.e., improving data efficiency-holds significant value.</p>
<p>Several approaches tackle this issue by reducing reliance on labeled data.For instance, PerAct [91], PolarNet [89], DP3 [26], and BridgeVLA [85] utilize 3D data representations, reducing the need for 2D data from multiple viewpoints.MDT [30], LAPA [80], and Hamster [76] are trained using actionless data, minimizing the need for labor-intensive labeling.Equidiff [28], EquiBot [29], ActionFlow [49], and EquAct [95] design networks that ensure model predictions adhere to equivariance properties, such as rotation, translation, and scaling, which decreases the demand for extensive labeled data.Additionally, other methods like LDuS [111] and ADPro [110] incorporate prior guidance during test time as constraints, avoiding dependence on target task labels.</p>
<p>H. Policy for Sampling Efficiency</p>
<p>Sampling efficiency is critical for high-frequency control and minimizing latency, allowing robots to complete tasks fast and smoothly.This is especially important in dynamic scenes, where rapid reactions are essential for task success.</p>
<p>Early works such as Act3D [93] and RVT [92] improved action sampling efficiency by replacing information-dense multi-view 2D images with sparse 3D point clouds, reducing computational overhead.Later, methods like ActionFlow [49] and FlowMS [88] boosted generation efficiency by replacing SDE-based diffusion models with ODE-based flow matching models, which require fewer iterative steps.FMP [47] applies flow matching to transform random waypoints into desired action trajectories, while π 0 [50] and OpenVLAOFT [59] use flow matching to generate action tokens for faster decoding.SmolVLA [53] employs a 10-step flow matching expert to produce action trunks deployable on consumer-grade GPUs or even CPUs.CotPolicy [37] further reduces inference cost by using conditional optimal transport to enforce straight solutions in the flow ODE for action generation.</p>
<p>Other works improve efficiency by replacing single-action generation with action trunk, such as ACT [63], VQ-BeT [66], and ARP [69].Transformer-based methods also benefit from more efficient action token usage: Fast [73] introduces a frequency-space action sequence tokenization that greatly compresses token count, while VLACache [75] selects and reuses tokens with minimal changes across steps.CARP [70] adopts a coarse-to-fine autoregressive policy, first learning multi-scale action representations and then refining them via a GPT-style transformer.DiVLA [39] and RDP [48] enhance efficiency by generating shared action tokens in latent space.</p>
<p>We should view the concepts of generalization and data efficiency discussed in each paper from a developmental perspective, as their meanings evolve.Generalization progresses from basic forms to more advanced ones that better align with real-world requirements.Similarly, the expectations for data efficiency evolve with time as technology advances.</p>
<p>IV. PRETRAINING METHODS</p>
<p>Large-scale pretraining of models has emerged in recent years as a powerful paradigm in natural language processing and computer vision, enabling remarkable advancements.Leveraging vast text corpora, LLMs have transformed text generation and inspired similar breakthroughs in vision through multimodal training.Building on this momentum, models trained on paired image-text data have been introduced to connect visual and linguistic modalities, leading to the development of VLMs.Encouraged by these successes, robotics researchers began to explore whether large-scale models could be extended to embodied domains, where reasoning must be grounded in action and interaction with the physical world.</p>
<p>In this section, we separate two pretraining methods: pretraining with available action data, and action-free datasets.Each method can include pretraining on in-domain (robotic data containing examples of tasks to be achieved) and/or on out-of-domain data (non-robotic data such as egocentric videos from different tasks and environments).</p>
<p>A. Pretraining with action data</p>
<p>Reproducing such breakthroughs in robotics, however, has been challenging for a long time due to the scarcity of largescale action datasets and the inherent difficulty of collecting diverse demonstrations across tasks, environments and embodiments.Early attempts such as Gato [60] and VIMA [61] demonstrated the feasibility of scaling transformers to multitask, multi-embodiment data.RT-1 [14] and later iterations in the RT-X series were the first to showcase how large-scale real-world robot data could produce scalable, generalizable policies.Other works such as PaLM-E [62], ACT [63] and RoboFlamingo [64] began to bridge multimodal reasoning with action execution, highlighting the synergy between language, perception and control.PaLM-E is one of the first VLAs, e.g., using a VLM to predict robotic actions.</p>
<p>A major step forward came with the Open-X Embodiment dataset [106], which pooled a large-scale set of demonstrations across multiple labs, robots and institutions.This large-scale collaborative effort not only expanded the quantity of available trajectories but also introduced unprecedented diversity in embodiments, environments, and task types.The Open-X initiative laid the foundation for models such as OpenVLA [67] and 3D-VLA [65], which demonstrated how pre-training on diverse action data can unlock generalization, key ingredients for building general-purpose agents.</p>
<p>Following the introduction of Open-X, several works have demonstrated that pretraining on large-scale action data is a key enabler of generalization and efficiency in robotics.GR00T-N1 [44] showed that pretraining a VLA model on a heterogeneous mix of real robot demonstrations, human videos and synthetic data equips humanoid robots with transferable sensorimotor priors, yielding robust zero-shot manipulation and faster adaptation compared to training from scratch.Similarly, Physical Intelligence's π 0 [50] explicitly validated the benefits of action pretraining by layering a flow-matching action head onto a pretrained VLM, allowing the model to leverage broad multi-embodiment datasets for robust generalization.Their follow-up, FAST [73], introduced an efficient action tokenization method that preserves the benefits of large-scale pretraining while reducing compute overhead, demonstrating that pretrained action representations can rival diffusion-based policies with greater efficiency.Open-source initiatives have echoed this trend: Octo [38] highlighted how large-scale action pretraining allows a single model to transfer across diverse observation and action spaces; while SmolVLA [53] pushed in the opposite direction, showing that even lightweight models pretrained on community-collected action datasets can match the performance of larger VLAs.Collectively, these efforts illustrate that pretraining on diverse action datasets is not just beneficial but essential, serving as the robotics analogue of LLM pretraining, laying the groundwork for scalable, generalpurpose embodied intelligence.</p>
<p>B. Pretraining with action-free data</p>
<p>Despite the advances in robotic pretraining, the scale of publicly available robotic action data remains orders of magnitude smaller than text or image corpora used in large language or vision models.This limitation has spurred the development of emerging approaches that explore pretraining strategies leveraging passive observation, video prediction, or world models to learn sensorimotor priors.</p>
<p>Notable examples include GR-1 [55], which employs largescale video generative pretraining to learn multi-task visual robot manipulation without requiring extensive action-labeled data; LAPA [80], which introduces an unsupervised method for pretraining VLA models from internet-scale videos and UniVLA [79], which derives task-centric latent actions from videos, enabling cross-embodiment policy learning without action annotations.Here, leveraging out-of-domain video data can provide large-scale coverage for generalizable priors, while in-domain video data improves task-specific adaptation.These approaches demonstrate that leveraging large-scale, unlabeled video data can effectively pretrain models for robotic tasks, reducing the dependency on action-labeled datasets.</p>
<p>Another complementary direction focuses on learning compact action representations through discrete latent variable models.Methods such as QueST [68] map continuous action sequences to discrete skill tokens using an autoencoder architecture that captures temporal correlations, which are then used to train policies via next-token prediction.VQ-BeT [66] similarly encodes action sequences into discrete latent vectors, learning shared tokens over low-level skills without explicitly modeling temporal correlations.These discrete latent representations enable robust transfer across tasks and embodiments while reducing reliance on large-scale action datasets.</p>
<p>These action-free pretraining approaches suggest robots can acquire transferable skills in domains with limited actionlabeled data, advancing generalist robotic systems.</p>
<p>V. EVALUATIONS</p>
<p>For the evaluation of each method, we first reviewed each paper, summarizing the core tools or priors, strengths, and limitations, and providing a subjective qualitative assessment.Then, we calculated the average monthly citation count for each method, which reflects, to some extent, the impact of the work.The statistical results are shown in Tab.II.</p>
<p>Furthermore, we conducted a comprehensive search and organized qualitative comparison results across multiple benchmarks, providing an objective qualitative evaluation.Below, we will present the datasets, evaluation metrics, and quantitative comparison results related to these qualitative analyses.</p>
<p>A. Datasets</p>
<p>As shown in Tab.I, RMP commonly consume 3 observation modalities: i) 1D states (proprioception, joint/EE pose, and compact object states), ii) 2D visual streams (RGB from one or multiple views), and iii) 3D observations (RGB-D or point clouds).To study manipulation and close the sim-to-real gap, the community relies on high-fidelity simulators/environments together with real-robot datasets that mirror these setups.Representative resources include panda-gym [112], Robo-Verse [113], and MuJoCo Playground [114]for light-weight experiments in physics simulators, and large-scale, real-world datasets such as Jacquard [115], RH20T [116], and Open-X Embodiment [106] that provide demonstrations aligned with common robot hardware and scenes.</p>
<p>The CALVIN [117] benchmark is built on top of the PyBullet [118] simulator and involves a Franka Panda Robot arm that manipulates the scene.CALVIN consists of 34 tasks and 4 different environments.All environments are equipped with a desk, a sliding door, a drawer, a button that turns on/off an LED, a switch that controls a lightbulb and three different colored blocks (red, blue and pink).These environments differ from each other in the texture of the desk and positions of the objects.CALVIN provides 24 hours of tele-operated unstructured play data, 35% of which are annotated with language descriptions.Each instruction chain includes five language instructions that need to be executed sequentially.</p>
<p>RLBench [108] is built atop the CoppelaSim [119] simulator, where a Franka Panda Robot is used to manipulate the scene.There are four RGB-D cameras available, front, wrist, left shoulder and right shoulder of the robot.</p>
<p>LBERO [120] consists of over 130 language-conditioned manipulation tasks divided into 5 different task suites: LIBERO-Spatial, LIBERO-Goal, LIBERO-Object, LIBERO-90, and LIBERO-Long.Each task suite except for LIBERO-90 consists of 10 different tasks with 50 demonstrations.Each task suite focuses on different challenges of imitation learning: LIBERO-Goal tests on tasks with similar object categories but different goals.LIBERO-Spatial requires policies to adapt to changing spatial arrangements of the same objects.In contrast, LIBERO-Object maintains he layout while changing the objects.LIBERO-90 consists of 90 different tasks in several environments and tasks with various spatial layouts.</p>
<p>Meta-World [121] is a MuJoCo-based [114] benchmark featuring a Sawyer robot and 50 diverse manipulation tasks (e.g., pick-place, door open, drawer open).It provides standard splits for multi-task and meta-RL: MT10/MT50 for multi-task learning and ML10/ML45 for meta-learning, with disjoint train/test task sets to measure cross-task generalization.Observations can be state vectors or RGB; success is typically measured by binary task completion and goal distance thresholds.</p>
<p>RoboSuite [122] builds on MuJoCo and supports Sawyer, Franka, and other arms with standardized tasks such as Lift, Stack, Door, NutAssembly, Wipe.Multiple camera views are provided (front/side/agent/wrist) alongside low-dimensional states.The robomimic [123] datasets supply large-scale human teleoperation demonstrations at varying qualities (expert/medium/mixed), enabling imitation and offline RL benchmarks with consistent metrics and baselines.</p>
<p>ManiSkill [124] offers GPU-accelerated simulation for dozens of tabletop and articulated-object tasks (e.g., PickCube, Stack, OpenCabinetDoor/Drawer, Plug/Unplug).It includes large demonstration sets generated via motion planning and teleoperation, along with language/task descriptions for some suites.Standard metrics include success rate, completion time, and pose/placement accuracy; they emphasize contact-rich tasks and generalization across object instances.</p>
<p>RT-1 / Open-X Embodiment [14], [106] aggregates largescale real-world teleoperation data from fleets of mobile manipulators performing hundreds of natural-language-labeled tasks (pick/place, open/close, tidy up).RGB observations (sometimes multi-view) and language commands train transformer policies for broad generalization.Benchmarks evaluate success rates across seen and novel tasks, cross-robot transfer, and robustness in unconstrained homes and offices.</p>
<p>SimplerEnv [125] is a simulation suite designed to mirror real Google-Robot tabletop tasks so that policy rankings in sim correlate with rankings on the real robot.Concretely, it reproduces tasks such as Pick Coke Can (PCC), Move Near (MN), and Open/Close Drawer (OCD) and evaluates policies under two regimes: Visual Matching (renderings closely match real scenes) and Variant Aggregation (averaging success over many appearance/layout variations).</p>
<p>The COLOSSEUM [126] is a generalization benchmark (built on RLBench/CoppeliaSim) that selects 20 manipulation tasks and systematically perturbs the environment along 14 axes (e.g., lighting, colors/textures, distractors, object sizes, physical properties, camera pose).The principal metric is the average decrease in performance from the base setting to perturbed settings (lower is better).</p>
<p>B. Metrics</p>
<p>Success rate denotes the proportion of attempts that successfully complete the task, which measures how often the robot can perform a given task correctly.</p>
<p>Task completion time is the time taken for the robot to finish the task from start to end.Faster completion indicates higher efficiency, which is important in practical scenarios.</p>
<p>Average length(Avg.Len) in CALVIN's long-horizon evaluation, each episode is a chain of K atomic instructions.For rollout i, let L i be the length of the longest correct prefix-the number of consecutive sub-tasks completed in order before the first error.The metric is Avg.Len = 1 M M i=1 L i computed over M test episodes.It captures partial progress on instruction chains (larger is better) and complements binary success.</p>
<p>Success weighted by Path Length(SPL) couples whether an agent succeeds with how directly it moves and is bounded TABLE II: Core Priors or Tools, strengths, and limitations of all selected robotic manipulation methods.AMC denotes average monthly citations.</p>
<p>Article</p>
<p>Core Priors / Tools Strengths Limitations AMC DP [22] time-series diffusion multimodal and high-dimensional output low inference efficiency 54.2 SuSIE [23] pretrained generative model zero-shot generalization to new objects image generation and policy trained separately 7.2 ChaDiffuser [24] keypose prediction improved long-horizon task performance keypose prediction errors propagate 18.12 VPDD [25] discrete diffusion model actionless pretraining from video lack language support and fine-grained control 0.5 DP3 [26] compact 3D representation generalize across position, instance, appearance lack language input support 10.2 DiffuserActor [27] 3D scene representations robust to scene changes requires camera calibration, low efficiency 8.5 Equidiff [28] SO(2)-equivariant sample-efficient learning lack language support; low sampling efficiency 2.66 EquiBot [29] SIM(3)-equivariant sample-efficient learning lack language support; low sampling efficiency 3.23 MDT [30] goal-image conditioned excellent performance on CALVIN and LIBERO high computational and memory overhead 5.16 SDP [31] task-specific routers skill reuse across tasks needs task IDs; MoE routing complexity 1.75 PSEC [32] compositional policies continual policy shift and dynamic shift settings redundant skill expansion 0.5 AdaManip [33] adaptive data collection focus on articulated object manipulation limited to 9 object categories and 5 mechanisms 1.25 AffordDP [34] transferable affordance generalize to unseen instances and categories heavily rely on affordance's accuracy 0.83 KStar [35] kinematics constraint reliable and kinematics-aware action no significant improvement in scores 0.67 BRS [36] rich user feedback combine arms, base, torso for real-world manipulation limited generalizability 2.25 CotPolicy [37] optimal transport (OT) competitive success rates with high efficiency requires rich and aligned demonstrations for OT 0.0 Octo [38] scaling law a versatile policy initialization across platforms unstable in long-horizon tasks 26.69 DiVLA [39] contextual reasoning adaptability to novel instructions and environments heavy computational cost 3.79 Cogact [40] separates cognitive and action study design of action modules and scaling behaviors limited generalizability 6.53 ChatVLA [41] mixture of experts perform well on zero-shot and few-shot tasks heavy dependence on VLM/LLM accuracy 4.0 RDT-1B [42] unified action space advancements in dexterous bimanual manipulation heavy computational cost 13.86GO-1 [43] latent action model + DP generalization and dexterity improve with dataset size low inference efficiency 4.61 GR00TN1 [44] latent actions open-source, cross-embodiment foundation model focus on short-horizon tabletop manipulation tasks 13.94 DreamGen [45] pseudo-action extraction learn from actionless videos, broader generalization heavy computation; limited dexterous behaviors 0.63 HybridVLA [46] autoregressive+diffusion robust to unseen objects, layouts, and lighting low inference efficiency 5.15 FMP [47] affordance prediction stable training; faster inference than DP not robust to scene and camera view 1.56 RDP [48] real-time tactile response improved contact-aware behavior limited scalability to long-horizon tasks 2.73 π0 [50] VLMs; scaling law generalist robot policies low sampling efficiency, sensitivity to scene 26.43 GraspVLA [51] 2D bounding boxes actionless pretraining from video only for rigid object grasping 4.3 HiRobot [52] intermediate text command evaluate on single-arm, dual-arm, and mobile robots high-level policy not aware of low-level execution 7.12 SmolVLA [53] Asynchronous inference matches 10× larger VLAs in performance only for simple and short-horizon tasks 1.1 MVP [54] masked visual pre-training without relying on labels or expert demonstrations policy head must be re-optimized for each task 6.86 RT-1 [14] pre-trained FiLM-EfficientNet real-world deployment ready lacks closed-loop feedback; discrete action 41.92 RoboUniView [56] vision-language alignment generalization across various camera parameters rely on precise camera calibration 0.83 Lift3D [57] task-related affordance mask enhanced 3D spatial awareness no language control 2.38 SAM2Act [58] memory bank; SAM+LoRA a strong understanding of spatial memory maintaining memory in new scenes is challengings 1.04 OpenVLAOFT [59] parallel decode+action chunk enhanced efficiency, policy performance and flexibility struggle to model multimodal action distributions 7.67 Gato [60] autoregressive Transformers a multi-task multi-embodiment generalist policy limited task performance 31.31VIMA [61] latent action model object-centric tokens enable scalable learning high computational and memory overhead 8.4 PaLM-E [62] chain-of-thought reasoning scalable and general-purpose heavy resource requirements 133.3 ACT [63] chunking with Transformer low-cost hardware enables high-precision operation chunk length sensitivity; limited generalization 25.76 GR-1 [55] pretrained generative model foundation VLA model limited generalization, discrete actions 6.0 RoboFlamingo [64] pre-trained VLMs enhanced performance and generalization low computational efficiency 9.25 3D-VLA [65] object and location tokens 3D-aware world modeling limited closed-loop feedback 7.17 VQ-BeT [66] common latent action tokens sampling efficiency; multimodal action prediction lack language input support 5.1 OpenVLA [67] VLM + diverse dataset outperform RT-2-X by 16.5% across 29 tasks low inference throughput and robustness 47.95 QueST [68] common latent skill tokens superior long-horizon modeling capabilities no scene/object awareness in skill tokens 1.27 ARP [69] causal chunked Transformer robustness across various control frequencies need multiple high-quality expert demonstrations 1.16 CARP [70] coarse-to-fine refinement competitive success rates with high efficiency lack language support and losed-loop feedback 0.95 TraceVLA [71] visual trace prompting robust generalization across embodiments and scenarios high computational and memory overhead 4.26 RoboVLM [72] VLMs+history fusion detailed guidebook for the design of VLAs limited long-horizon, complex task ability 3.85 Fast [73] discrete cosine transform good at dexterous and high-frequency tasks low inference efficiency 10.00 SpatialVLA [74] adaptive action grids strong zero-shot performance, high-frequency control action discretization limit precision 5.34 VLACache [75] reuse unchanged tokens achieve 1.7× faster with comparable performance cache effectiveness depends on visual stability 1.49 Hamster [76] 2D path guidance robust spatially-aware action generation lack spatial 3D understanding 3.25 Magma [77] set-of-mark; trace-of-mark enhanced the spatial-temporal intelligence high computational and memory overhead 5.28 CoT-VLA [78] visual chain-of-thought outperforming OpenVLA by 17% in real-world tasks high computational and memory overhead 9.12 UniVLA [79] latent action learning unified, embodiment-agnostic action space action discretization limit precision 1.0 LAPA [80] latent action via visual change unsupervised pretraining from actionless videos frame-level focus limits long-horizon reasoning 5.41 ECoT [81] chain-of-thought reasoning generalization to novel tasks data and prompt engineering sensitivity 7.28 WorldVLA [82] attention mask strategy closed-loop action and observation prediction action discretization limit precision 0.0 LOTUS [83] continual skill discovery unsupervised skills from raw, unsegmented demos replay buffer grows with tasks 1.49 HULC [84] visual-language alignment systematic analysis of impact of various factors low success rate 4.45 BridgeVLA [85] 2D heatmaps real-robot success with high sample efficiency rely on precise camera calibration 0.0 SEDF [86] SE(3)-equivariance sample-efficient learning only object-level grasping 18.1 A0 [87] affordance representation easy to deploy across different robotic platforms rely on depth and camera calibration accuracy 2.5 FlowMS [88] smooth action vector field whole-body movements on a full-size humanoid robot success rate and diversity lower than DPs.1.27 ActionFlow [49] SE(3)-equivariance high efficiency and low latency assumes smooth transitions 0.11 PolarNet [89] 3D PC representation outperforms SOTA in single-and multi-task settings limited generalization to new scenes, objects, tasks 2.1 HiveFm [90] history-aware Transformer outperform SOTA baselines on 74 RLBench tasks quadratic computational cost with sequence length 3.78 PerAct [91] voxel-based formulation data efficiency slow inference speed, discretized actions 16.87 RVT [92] multi-view fusion enhanced geometric reasoning sensitive to camera configuration 6.5 Act3D [93] relative 3D cross-attentions generalize well to novel camera placements rely on precise camera calibration 3.45 SAM-E [94] SAM+LoRA improve generalization to new tasks performance is sensitive to segmentation quality 1.09 EquAct [95] SE(3)-equivariance generalize better across object poses and orientations low sampling efficiency 0.0 Cliport [96] affordance prediction with spatial reasoning discrete actions; just for tabletop tasks 13.09 MOKA [97] affordance in-context learning enable zero/few-shot open-world manipulation high latency; limited affordance accuracy 4.2 RAM [98] retrieval-based affordance generalizable zero-shot robotic manipulation struggle with long-horizon and complex actions 2.6 ReKep [99] keypoint constraints consider spatio-temporal dependencies rely on accurate point tracking 12.11 3D-LOTUS [100] pretrained VLMs strong few-shot performance over-reliance on VLM for planning and grounding 1.12 GeminiRob [102] Gemini 2.0; vast dataset generalist model capable of directly controlling robots struggle with grounding spatial relationships 7.0 GeminiRob [102] object-centric interaction robust control without requiring VLM fine-tuning reliance on high-quality object meshes 2.32 in [0, 1].For episode i, let S i ∈ {0, 1} indicate success, ℓ * i be the shortest-path length (e.g., geodesic) from start to goal, and ℓ i the agent's executed path length.The dataset-level score is
SPL = 1 M M i=1 S i ℓ * i max(ℓ i , ℓ * i )(1)
For long-horizon manipulation (e.g., Franka Kitchen [127]), papers report sub-goals achieved (microwave open, burner on, door open, etc.) in addition to binary success.</p>
<p>Generalization quantifies how performance changes under controlled perturbations.The COLOSSEUM [126] perturbs lighting, textures, distractors, camera, object properties, etc., and reports the average performance decrease from the base setting across perturbation axes (↓ is better).SimplerEnv [125] reports MMRV (Mean Maximum Rank Violation, ↓) and Pearson (↓) to measure sim-to-real rank correlation, alongside success rates under "visual matching" and "variant aggregation."These metrics explicitly target robustness and sim-to-real alignment rather than raw success alone</p>
<p>C. Qualitative evaluations</p>
<p>To provide a fair and comprehensive evaluation of existing RMPs, we first searched through publicly available results and got the leaderboards of CALVIN and RLBench.On the other hand, we reviewed the qualitative comparison results presented in the papers and recorded the outcomes obtained under the same benchmarks and experimental setups.Here, we present the results from benchmarks with six or more comparison methods, specifically focusing on LIBERO, Meta-World, SimplerEnv-Google Robot, and COLOSSEUM.These qualitative comparisons evaluate existing RMPs from different perspectives, using various task setups and metrics.Table VIII reports the results on the COLOSSEUM benchmark, which extends RLBench.All models are trained using data from the original RLBench but evaluated in environments that span 12 axes of perturbations.These perturbations include variations in object texture, color, size, and changes in background, lighting, distractors, and camera poses.In total, COLOSSEUM generates 20,371 unique task perturbation instances to evaluate model's generalization capabilities.Specifically, methods are evaluated by performing each task across 25 trials per perturbation, and the average decrease across all perturbations is computed to assess the generalization.</p>
<p>VI. APPLICATIONS</p>
<p>Real-world manipulation methods span heterogeneous demands, and we group them into: (i) Primitive Manipulation Tasks-short, one-shot skills where perception-tocontrol dominates; (ii) Contact-Rich Assembly Tasks-tighttolerance insertions, creasing, and continuous-force operations; (iii) Kitchen Assistant-long-horizon domestic routines under language or goal guidance; (iv) Tool-Mediated Manipulation-skills that require exploiting external affordances; and (v) Garbage Cleaning-perception-guided pickup and wiping for litter removal and tabletop cleaning</p>
<p>A. Primitive Manipulation Tasks</p>
<p>These are short, one-shot manipulations (grasp-place, press, slide, open/close) whose core challenge is precise perceptionto-control under noise and contact.Representative methods: GraspVLA [51]: open-set tabletop grasping of novel objects; PerAct [91]: RLBench primitives like pick/place blocks, rotate knobs, press switches via language; Act3D [93]: multi-view 3D grasping, peg/slot insertion, button pressing; RDP [48]: contact-rich peg-in-hole / connector insertion; CLIPort [96]: pick-and-place and sorting by attributes; RT-1 [14]: hundreds of household primitives like opening doors, tossing trash, moving items to bins.Strong reliability in controlled setups and good breadth with data scaling; still brittle for transparent/deformable items, odd viewpoints, and sim-to-real-needs better tactile fusion and uncertainty-aware control.</p>
<p>B. Contact-rich Assembly Tasks</p>
<p>This category targets operations that require sustained contact, precise alignment, and deformation control beyond simple pick-and-place.Manual2Skill [144] parses human manuals into step-wise skills to execute long-horizon furniture assembly with reliable multi-contact sequences.SRSA [145] focuses on industrial parts assembly, emphasizing stable contact formation and alignment under tight tolerances.The generalist policy π 0 [50] demonstrates cardboard-box folding, a deformable, contact-dominant procedure involving creasing, pressing, and closure.Complementing assembly, RDP [48] provides reactive control for peeling and wiping, covering surface-preparation and post-assembly finishing where continuous contact and friction management are critical.Garment-Pile [146] extends contact-rich manipulation to deformable textiles in clutter, learning point-level affordances to retrieve and reorganize piled garments as a precursor to folding or sorting.Together, these methods illustrate progress from isolated grasps toward crease/bend, insert/fit, and surface-treatment behaviors executed robustly on real hardware.</p>
<p>C. Kitchen Assistant</p>
<p>Robotic policies act as versatile household helpers that fetch items, open/close storage, place utensils/food, and perform light prep/cleanup under language guidance.Representative systems include SayCan [147] and RT-1 [14].Recent generalist VLA models extend this further: π 0 [50] demonstrates broad kitchen skills (e.g., coffee making, dish handling, table setting) as part of its scaled household repertoire, while a liquid-mixing system [148] targets beverage preparation with bimanual pouring, quantity control, and step-wise recipe execution.Together, these works show reliable item retrieval, placement, container operations, and simple food/beverage preparation in real kitchens.</p>
<p>D. Tool-Mediated Manipulation</p>
<p>The goal is to acquire and stabilize a tool and exploit its affordances, demanding functional understanding and contact modeling.Representative methods: MVP [54]: simulated levering with a bar and hook-and-pull actions; GR-1/GR-2 [55]: evaluations on cutting with a knife and scooping with a spoon/cup after video pretraining; HULC [84]: everyday wiping/cleaning with a cloth as tool-like behavior; Magma [77]: demonstrations of using kitchen utensils (e.g., spoon scooping, spatula flipping); MOKA [97]: real-robot evaluations used everyday tools and appliances ToolBench / MuJoCo-Manipulus: benchmark tasks like pouring, scooping, scraping, hammering used by general VLA policies.Clear progress on canonical cases, but robust compositional generalization to unseen tools/targets, liquids and deformables, and varying friction/compliance remains limited; richer contact sensing, dynamics priors, and tool-centric data are needed.</p>
<p>E. Garbage Cleaning</p>
<p>Garbage cleaning spans aquatic, tabletop, and outdoor settings.Haldorai et al. [149] present a vision-guided watersurface robot that detects and collects floating trash in real aquatic environments.GScbam-Net [150] offers a lightweight attention-based garbage image classifier to support waste sorting and downstream robotic cleaning.Lv et al. [151] develop ML-based garbage detection and 3D localization to enable autonomous trash pickup in the field.</p>
<p>VII. DISCUSSION AND CONCLUSION</p>
<p>Data-driven robot learning faces a fundamental bottleneck: the scarcity of high-quality and diverse in-domain data.Unlike fields such as natural language processing or computer vision, where massive datasets are readily available, robotic data is costly to collect and often narrow in scope, limiting the generalization ability of learned policies.This calls for smarter learning strategies that embed inductive biases into robotic models, reducing dependence on exhaustive data coverage.As argued in our recent position paper [152], a promising path forward is to design bio-inspired foundation models for robotics, where principles drawn from human and animal perception, motor control, and adaptation can provide the structural priors necessary to scale beyond current limitations.</p>
<p>A. Commonly Used Prior</p>
<p>In RMP, several forms of prior knowledge are commonly utilized to improve performance.Chain-of-thought (CoT) is one of the most widely used priors [62], [78], [81].CoT, derived from LLMs, enables robots to break down complex or long-horizon tasks into manageable subtasks, facilitating more effective decision-making during manipulation.Affordance knowledge [34], [47], [97] allows robots to recognize and predict how objects can be interacted with, guiding their actions based on the physical properties and potential uses of objects.It is often employed to enhance a model's generalization ability.Waypoints [47], [153] are a specific form of affordance, defining intermediate positions or goals in a manipulation task to help robots plan and execute more precise and efficient movements.Kinematic constraints [35] ensure that robots respect physical limitations during movement, optimizing their trajectories and minimizing errors.Additionally, equivariance constraints [28], [29], [49], [95] can be applied to improve the efficiency of policy learning by reducing the required data size.These constraints help the model learn invariant behaviors under specific transformations, thus reducing the demand for data volume.Moreover, other priors, such as manifold constraints [110] derived from observation, are used to fully leverage the information from test-time data.</p>
<p>B. Summary of open challenges</p>
<p>a) Insufficient Generalization: RMP often struggles with generalization, especially when applied to real-world scenarios.These agents are highly sensitive to the specific scenes or tasks that they are trained on.Their performance deteriorates significantly when confronted with variations in the environment, object types, or task conditions.In particular, the ability to generalize to real-world tasks remains limited, preventing robots from independently handling everyday tasks in dynamic and unstructured settings.</p>
<p>b) Diverse Robot Configurations: The wide variety of robot designs and configurations leads to significant differences in the input and output spaces for learning manipulation policies.These discrepancies make it difficult for existing models to generalize across different robotic platforms.As a result, a unified manipulation policy that can work across various robot types is challenging to achieve.To address this, having a standardized robot configuration is essential for building a versatile and practical foundational VLA model.This would allow for more effective and transferable learning across different robotic systems and tasks.</p>
<p>c) Lack of Unified Benchmarks: The lack of a common, user-friendly benchmark for evaluating different methods is another major challenge in RM.Researchers often assess their approaches within their own setups, which makes it difficult to conduct comprehensive and consistent comparisons across techniques.While some benchmarks do exist, they are either difficult to deploy on headless servers or are limited in the range of tasks they cover.This absence of standardized benchmarks impedes progress, as there is no reliable way to measure improvements or draw definitive conclusions across various studies.Additionally, existing datasets vary significantly in terms of observation modalities, action dimensions, and task settings, making them incompatible with one another.d) Dependence on Expert Data: Current robotic manipulation methods remain heavily reliant on expert data, such as labeled datasets and demonstrations.Often, systems require fine-tuning with domain-specific data before they can perform well on real-world tasks.Moreover, these systems typically lack sufficient adaptability during testing, with limited abilities for self-adaptation or continuous learning.This dependence on pre-collected data and the inability to continuously learn from new experiences restricts the scalability and flexibility of robotic manipulation techniques in real-world applications.</p>
<p>e) Collaborative and Dexterous Manipulation: Dualarm collaborative manipulation and high-degree-of-freedom manipulation are still in their early stages, representing an emerging area of research.These advancements are essential for enabling robots to coordinate and perform tasks with natural dexterity in real-life scenarios, as well as for the progression of humanoid robots.</p>
<p>C. Conclusion</p>
<p>In this survey, we provide a systematic and comprehensive review of RMP methodologies, highlighting key techniques, application scopes, learning objectives, data types, targeted challenges, strengths, and limitations.We analyzed 120 research papers to extract detailed data, focusing on: 1) the challenges addressed, 2) the core priors or tools, strengths, and limitations, 3) the type of input data and pretraining strategies, and 4) potential future research directions.The papers were categorized in a hierarchical manner based on their techniques.While significant progress has been made in RMP in recent years, we also identify ongoing challenges and open questions that set the stage for future investigation in this field.</p>
<p>This survey offers a comprehensive overview and valuable research resources for scholars in the field of RMP.However, there are some limitations of our review.Specifically, our focus was on RMP, and methods based on reinforcement learning were not covered.Additionally, since grasping is just one aspect of manipulation, we have excluded literature that focuses solely on grasp pose generation.Consequently, several valuable papers were not included.Despite our efforts to apply a systematic literature review methodology and conduct manual searches to ensure thorough inclusion, it is possible that some relevant papers were overlooked during the initial selection due to the vast volume of available literature.</p>
<p>Fig. 2 :
2
Fig. 2: Timeline of the models explored in this survey.Each model is classified by its control strategy as presented in Tab.I.</p>
<p>TABLE I :
I
[22]nomy of RMPs.DH, PN, OS, and TX represent depth, proprioception, object state, and text.DM, FM, GM, NR, AR, NC, AF denote diffusion model, flow matching, Gaussian mixture, naive regression, autoregressive, naive classification, affordance.ID, OD, ST, SS, MS, RW, CL, GL, DE, SE represent in-domain, out-of-domain, single-task, single-scenario, multi-scenario, real-world, continual learning, generalization, data efficiency, sampling efficiency.OS TX DM FM GM NR AR NC DM FM NR NC AF ID OD TX ID OD ST SS MS RW CL GL DE SE DP[22]
InputControl strategyPretrainingPurposeArticleaction generationtask plannerimagesactionsscenarioschallengesRGB DH PN</p>
<p>TABLE III :
III
Zero-shot (Train A, B, C →Test D) long-horizon evaluation on CALVIN.Avg.Len means average length of the correct trajectory.
Method1Task completed in a row 2 3 45Avg. Len ↑MCIL [128]30.41.30.20.00.00.31HULC [84]41.8 16.55.71.91.10.67ChaDiffuser [24]49.9 21.18.03.51.50.84SPIL [129]74.2 46.3 27.6 14.78.01.71RoboFlamingo [64]82.4 61.9 46.6 33.1 23.52.48SIE [23]87.0 69.0 49.0 38.0 26.02.69DeeR [23]86.2 70.1 51.8 41.5 30.42.82GR-1 [55]85.4 71.2 59.6 49.7 40.13.06DP3 [26]53.9 44.7 38.0 34.3 29.02.00Diffuser [27]93.8 80.3 66.2 53.3 41.23.35CLOVER [130]96.0 83.5 70.8 57.5 45.43.53DTP [131]94.5 82.5 72.8 61.3 50.03.61RoboUniView [56]94.2 84.2 73.4 62.2 50.73.64ADPro [110]94.7 83.0 73.6 61.4 51.13.64GHIL-Glue [132]95.2 88.5 73.2 62.5 49.83.69UniVLA [79]95.5 85.8 75.4 66.9 56.53.80MoDE [133]96.2 88.9 81.1 71.8 63.54.01GR-MG [134]96.8 89.3 81.5 72.7 64.44.04RoboVLM [72]98.0 93.6 85.4 77.8 70.44.25SeeR-Large [135]96.3 91.6 86.1 80.3 74.04.28</p>
<p>Table III presents the results from the publicly available leaderboard on CALVIN, which includes 34 tasks across 4 different environments (A, B, C, and D).All methods are evaluated under a zero-shot generalization setup, where models are trained in environments A, B, and C, and tested in environment D. The evaluation metric used is the success rate.</p>
<p>TABLE IV :
IV
Average success rate on RLBench (18 tasks, 100 demo/task).Table IV reports the results from the RLBench leaderboard, which includes 18 manipulation test tasks, each with 2 to 60 variations.All methods are trained to predict the next endeffector keypose, and the evaluation metric is task success rate, representing the proportion of execution trajectories that meet the goal conditions specified in language instructions.
EquAct [95]BridgeVLA [85] SAM2Act [58]ARP [69]89.488.286.884.9ADPro [110]3D-LOTUS [100] RVT-2 [136] Diffuser Actor [27]83.983.181.481.3Mi-Diffuser [137]SAM-E [94]Act3D [93]RVT [92]77.670.665.062.9PerAct [91]PolarNet [89]HiveFm [90]C2F [138]49.446.445.320.1</p>
<p>TABLE V :
V
Success rate on LIBERO with four settings (spatial, object, goal, long-horizon).VLA Pt refers pretraining on robotics data.The first part refers to fine-tuning on the entire training set.The second part involves few-shot learning using a small subset of data.The last row uses zero-shot setting.Table V presents the results on LIBERO, as reported in their official paper.Methods are tested on all tasks with 20 rollouts each, and the success rate is averaged over 3 different seeds.
MethodVLA Pt Spatial Object Goal Long Avg.SmolVLA [53] WorldVLA [82] MDT [30] DP [22]✗ ✗ ✗ ✗93 87.6 78.5 78.394 96.2 83.4 60.0 81.8 91 77 88.75 87.5 73.5 64.8 76.1 92.5 68.3 50.5 72.4OpenVLAOFT [59]✓96.998.1 95.5 91.1 95.4UniVLA [79]✓96.596.8 95.6 92.0 95.2π 0 [50] MaIL [139]✓ ✓90 74.386 90.1 81.8 78.6 83.5 95 73 86.0DiT Policy [131]✓84.296.3 85.4 63.8 82.4CoT-VLA [78]✓87.591.6 87.6 69.0 81.13OpenVLA [67]✓84.788.4 79.2 53.7 76.5Octo [38]✓78.985.7 84.6 51.1 75.1VLACache [75]✓83.885.8 76.4 52.8 74.7LAPA [80]✓73.874.6 58.8 55.4 65.7Magma <a href="10 demos">77</a>✓264929--OpenVLA <a href="10 demos">67</a>✓82410--GraspVLA [51]✓-94.1 91.2 82.0-</p>
<p>TABLE VI :
VI
Average success rate on MetaWorld different difficulty levels.Table VI reports the average success rate of six RMPs on MetaWorld, a tabletop environment featuring a Sawyer arm with a gripper.The test tasks are divided into four difficulty levels and are captured from two corner camera perspectives.The task categories are as: easy tasks include button-press, drawer-open, reach, handle-pull, peg-unplug-side, lever-pull, and dial-turn; medium tasks include hammer, sweep-into, binpicking, push-wall, and box-close; hard and very hard tasks include assembly, hand-insert, and shelf-place.TableVIIpresents results on SimplerEnv with Google Robot setups, which offer diverse manipulation scenarios under varying lighting, color, texture, and robot camera pose
MethodEasy Medium Hard VeryHard Avg.Lift3D [57]93.182.488.028.084.5SmolVLA [53] 87.14 51.82706468.24DP3 [26]85.749.657.018.065.3π 0 [50]80.440.936.744.050.5TinyVLA [140] 77.621.511.415.831.6DP [22]23.110.71.96.110.5</p>
<p>TABLE VII :
VII
Success rate on SimplerEnv-Google Robot.
MethodVisual Matching PCC MN OCD Avg. PCC MN OCD Avg. Variant AggregationSoFar [141]92.3 91.7 40.3 74.9 90.7 74.0 29.7 67.6Cogact [40]91.3 85.0 71.8 74.8 89.6 80.8 28.3 61.3VLACache [75] 92.0 83.3 70.5 74.4 91.7 79.3 32.5 62.3SpatialVLA [74] 81.0 69.6 59.3 71.9 89.5 71.7 36.2 68.8π 0 [50]88.0 80.3 56.0 70.1----RoboVLM [72] 72.7 66.3 26.8 56.3 68.3 56.0 8.5 46.3RT-1 [14]56.7 31.7 59.7 53.4 49.0 32.3 29.4 39.6TraceVLA [71] 28.0 53.7 57.0 42.0 60.0 56.4 31.0 45.0OpenVLA [67] 16.3 46.2 35.6 27.7 54.5 47.7 17.7 39.8Octo [38]17.0 4.2 22.7 16.8 0.6 3.1 1.1 1.1conditions. In Tab. VII, PCC, MN, and OCD denote the tasksPick Coke Can, Move Near, and Open/Close Drawer.</p>
<p>TABLE VIII :
VIII
Robot Manipulation Generalization on The COLOSSEUM.The metric is the average decrease across all perturbations.
SAM2Act [58]RVT [92]Diffuser [27] MVP [54] PerAct [91]-4.3-14.5-15.6-16.3-17.3RVT-2 [136] GENIMA [142] BridgeVLA [85] R3M [143] ACT [63]-19.5-41.6-45.3-49.9-61.8
VIII. ACKNOWLEDGEMENTS This work was in part supported by the French Research Agency ANR, l'Agence Nationale de Recherche, through the projects Chiron (ANR-20-IADJ-0001-01), Aristotle (ANR-21-FAI1-0009-01), and Astérix (ANR-23-EDIA-0002), the French national investment prioritary program through the PSPC FAIR WASTE project, and the Franco-Chinese Research Center for Carbon Neutrality (Le Centre de Neutralité Carbone franco-chinois-CNC) through collaborative Artemis project.
Parts of Animals, ser. Loeb Classical Library. Aristotle , 1937book IV323Cambridge, MA</p>
<p>Berlin: Weidmann, 1951, fragment of Anaxagoras B21a, reporting that man is the most intelligent of animals because he has hands. H Diels, W Kranz, Die Fragmente der Vorsokratiker. 6th ed</p>
<p>A review of robot learning for manipulation: Challenges, representations, and algorithms. O Kroemer, S Niekum, G Konidaris, J. Mach. Learn. Res. 22302021</p>
<p>Deep reinforcement learning for the control of robotic manipulation: a focussed mini-review. R Liu, F Nageotte, P Zanne, M De Mathelin, B Dresp-Langley, Robotics. 101222021</p>
<p>. M Suomalainen, Y Karayiannidis, V Kyrki, Rob. Auton. Syst. 1561042242022Rob. auton. syst</p>
<p>A survey on deep reinforcement learning algorithms for robotic manipulation. D Han, B Mulyana, V Stankovic, S Cheng, Sensors. 23737622023</p>
<p>Deep learning approaches to grasp synthesis: A review. R Newbury, M Gu, L Chumbley, A Mousavian, C Eppner, J Leitner, J Bohg, A Morales, T Asfour, D Kragic, IEEE Transactions on Robotics. 3952023</p>
<p>A survey of embodied learning for object-centric robotic manipulation. Y Zheng, L Yao, Y Su, Y Zhang, Y Wang, S Zhao, Y Zhang, L.-P Chau, arXiv:2408.115372024</p>
<p>A review of embodied grasping. J Sun, P Mao, L Kong, J Wang, Sensors. 2538522025</p>
<p>Visionlanguage-action models: Concepts, progress, applications and challenges. R Sapkota, Y Cao, K I Roumeliotis, M Karkee, arXiv:2505.047692025</p>
<p>A survey on visionlanguage-action models for embodied ai. Y Ma, Z Song, Y Zhuang, J Hao, I King, arXiv:2405.140932024</p>
<p>A survey on vision-language-action models: An action tokenization perspective. Y Zhong, F Bai, S Cai, X Huang, Z Chen, X Zhang, Y Wang, S Guo, T Guan, K N Lui, arXiv:2507.019252025</p>
<p>Vision language action models in robotic manipulation: A systematic review. M U Din, W Akram, L S Saoud, J Rosell, I Hussain, arXiv:2507.106722025</p>
<p>Rt-1: Robotics transformer for real-world control at scale. A Brohan, N Brown, J Carbajal, Y Chebotar, J Dabis, C Finn, K Gopalakrishnan, arXiv:2212.068172022</p>
<p>Denoising diffusion probabilistic models. J Ho, A Jain, P Abbeel, NeurIPS. 332020</p>
<p>Denoising diffusion implicit models. J Song, C Meng, S Ermon, ICLR. 2021</p>
<p>Score-based generative modeling through stochastic differential equations. Y Song, J Sohl-Dickstein, D P Kingma, A Kumar, S Ermon, B Poole, ICLR2021</p>
<p>Dpm-ot: A new diffusion probabilistic model based on optimal transport. Z Li, S Li, Z Wang, N Lei, Z Luo, X Gu, ICCV. 2023633</p>
<p>Flow matching for generative modeling. Y Lipman, R T Chen, H Ben-Hamu, M Nickel, M Le, ICLR2023</p>
<p>Vector autoregressive models. H Lütkepohl, Handbook of research methods and applications in empirical macroeconomics. Edward Elgar Publishing2013</p>
<p>Transformation autoregressive networks. J Oliva, A Dubey, M Zaheer, B Poczos, R Salakhutdinov, E Xing, J Schneider, ICML. PMLR2018</p>
<p>Diffusion policy: Visuomotor policy learning via action diffusion. C Chi, Z Xu, S Feng, E Cousineau, Y Du, B Burchfiel, R Tedrake, S Song, The International Journal of Robotics Research. 027836492412736682023</p>
<p>Zero-shot robotic manipulation with pre-trained imageediting diffusion models. K Black, M Nakamoto, P Atreya, H R Walke, C Finn, A Kumar, S Levine, The Twelfth ICLR. 2024</p>
<p>Chaineddiffuser: Unifying trajectory diffusion and keypose prediction for robotic manipulation. Z Xian, N Gkanatsios, T Gervet, T.-W Ke, K Fragkiadaki, CoRL. PMLR2023</p>
<p>Learning an actionable discrete diffusion policy via large-scale actionless video pretraining. H He, C Bai, L Pan, W Zhang, B Zhao, X Li, NeurIPS. 372025</p>
<p>3d diffusion policy: Generalizable visuomotor policy learning via simple 3d representations. Y Ze, G Zhang, K Zhang, C Hu, M Wang, H Xu, ICRA 2024 Workshop. 2024</p>
<p>3d diffuser actor: Policy diffusion with 3d scene representations. T.-W Ke, N Gkanatsios, K Fragkiadaki, 8th Annual CoRL. 2024</p>
<p>Equivariant diffusion policy. D Wang, S Hart, D Surovik, T Kelestemur, H Huang, H Zhao, M Yeatman, J Wang, R Walters, R Platt, 20248th Annual CoRL</p>
<p>Equibot: Sim (3)-equivariant diffusion policy for generalizable and data efficient learning. J Yang, Z Cao, C Deng, R Antonova, S Song, J Bohg, 2024in 8th Annual CoRL</p>
<p>Multimodal diffusion transformer: Learning versatile behavior from multimodal goals. M Reuss, Ö E Yagmurlu, F Wenzel, R Lioutikov, arXiv:2407.059962024</p>
<p>Sparse diffusion policy: A sparse, reusable, and flexible policy for robot learning. Y Wang, Y Zhang, M Huo, T Tian, X Zhang, Y Xie, C Xu, P Ji, W Zhan, M Ding, 2024in 8th Annual CoRL</p>
<p>Skill expansion and composition in parameter space. T Liu, J Li, Y Zheng, H Niu, Y Lan, X Xu, X Zhan, ICLR2025</p>
<p>Adamanip: Adaptive articulated object manipulation environments and policy learning. Y Wang, X Zhang, R Wu, Y Li, Y Shen, M Wu, Z He, Y Wang, H Dong, The Thirteenth ICLR. 2025</p>
<p>Afforddp: Generalizable diffusion policy with transferable affordance. S Wu, Y Zhu, Y Huang, K Zhu, J Gu, J Yu, Y Shi, J Wang, CVPR. 2025</p>
<p>Spatial-temporal graph diffusion policy with kinematic modeling for bimanual robotic manipulation. Q Lv, H Li, X Deng, R Shao, Y Li, J Hao, L Gao, M Y Wang, L Nie, CVPR. 2025</p>
<p>Behavior robot suite: Streamlining real-world whole-body manipulation for everyday household activities. Y Jiang, R Zhang, J Wong, C Wang, Y Ze, H Yin, C Gokmen, S Song, J Wu, L Fei-Fei, RSS 2025 Workshop. 2025</p>
<p>Fast flow-based visuomotor policies via conditional optimal transport couplings. A Sochopoulos, N Malkin, N Tsagkas, J Moura, M Gienger, S Vijayakumar, arXiv:2505.011792025</p>
<p>Octo: An open-source generalist robot policy. O M Team, D Ghosh, H Walke, K Pertsch, K Black, O Mees, S Dasari, J Hejna, T Kreiman, C Xu, arXiv:2405.122132024</p>
<p>Diffusion-vla: Scaling robot foundation models via unified diffusion and autoregression. J Wen, M Zhu, Y Zhu, Z Tang, J Li, Z Zhou, C Li, X Liu, Y Peng, C Shen, arXiv:2412.032932024</p>
<p>Cogact: A foundational vision-language-action model for synergizing cognition and action in robotic manipulation. Q Li, Y Liang, Z Wang, L Luo, X Chen, M Liao, F Wei, arXiv:2411.196502024</p>
<p>Chatvla: Unified multimodal understanding and robot control with vision-language-action model. Z Zhou, Y Zhu, M Zhu, J Wen, N Liu, Z Xu, W Meng, R Cheng, Y Peng, C Shen, arXiv:2502.144202025</p>
<p>Rdt-1b: a diffusion foundation model for bimanual manipulation. S Liu, L Wu, B Li, H Tan, H Chen, Z Wang, K Xu, H Su, J Zhu, arXiv:2410.078642024</p>
<p>Agibot world colosseo: A large-scale manipulation platform for scalable and intelligent embodied systems. Q Bu, J Cai, L Chen, X Cui, Y Ding, S Feng, S Gao, arXiv:2503.066692025</p>
<p>J Bjorck, F Castañeda, N Cherniadev, X Da, R Ding, L Fan, Y Fang, D Fox, F Hu, S Huang, arXiv:2503.14734Gr00t n1: An open foundation model for generalist humanoid robots. 2025</p>
<p>Dreamgen: Unlocking generalization in robot learning through neural trajectories. J Jang, S Ye, Z Lin, J Xiang, J Bjorck, Y Fang, F Hu, S Huang, K Kundalia, Y.-C Lin, arXiv:2505.127052025</p>
<p>Hybridvla: Collaborative diffusion and autoregression in a unified vision-language-action model. J Liu, H Chen, P An, Z Liu, R Zhang, C Gu, X Li, Z Guo, S Chen, M Liu, arXiv:2503.106312025</p>
<p>Affordance-based robot manipulation with flow matching. F Zhang, M Gienger, arXiv:2409.010832024</p>
<p>Reactive diffusion policy: Slow-fast visual-tactile policy learning for contact-rich manipulation. H Xue, J Ren, W Chen, G Zhang, F Yuan, G Gu, H Xu, C Lu, ICRA 2025 Workshop. 2025</p>
<p>Actionflow: Efficient, accurate, and fast policies with spatially symmetric flow matching. N Funk, J Urain, J Carvalho, V Prasad, G Chalvatzaki, J Peters, R: SS workshop: Structural Priors as Inductive Biases for Learning Robot Dynamics. 2024</p>
<p>π 0 : A vision-language-action flow model for general robot control. K Black, N Brown, D Driess, A Esmail, M Equi, C Finn, N Fusai, L Groom, arXiv:2410.24164202425</p>
<p>Graspvla: a grasping foundation model pretrained on billion-scale synthetic action data. S Deng, M Yan, S Wei, H Ma, Y Yang, J Chen, Z Zhang, T Yang, X Zhang, H Cui, arXiv:2505.032332025</p>
<p>Hi robot: Open-ended instruction following with hierarchical vision-language-action models. L X Shi, B Ichter, M Equi, L Ke, K Pertsch, Q Vuong, J Tanner, A Walling, H Wang, N Fusai, arXiv:2502.194172025</p>
<p>M Shukor, D Aubakirova, F Capuano, P Kooijmans, S Palma, A Zouitine, M Aractingi, C Pascal, M Russi, A Marafioti, arXiv:2506.01844Smolvla: A vision-language-action model for affordable and efficient robotics. 2025</p>
<p>Masked visual pretraining for motor control. T Xiao, I Radosavovic, T Darrell, J Malik, arXiv:2203.061732022</p>
<p>Unleashing large-scale video generative pre-training for visual robot manipulation. H Wu, Y Jing, C Cheang, G Chen, J Xu, X Li, M Liu, H Li, T Kong, The Twelfth ICLR. 2024</p>
<p>Robouniview: Visual-language model with unified view representation for robotic manipulation. F Liu, F Yan, L Zheng, C Feng, Y Huang, L Ma, arXiv:2406.189772024</p>
<p>Lift3d foundation policy: Lifting 2d large-scale pretrained models for robust 3d robotic manipulation. Y Jia, J Liu, S Chen, C Gu, Z Wang, L Luo, L Lee, P Wang, Z Wang, CVPR. 2025</p>
<p>Sam2act: Integrating visual foundation model with a memory architecture for robotic manipulation. H Fang, M Grotz, W Pumacay, Y R Wang, D Fox, R Krishna, J Duan, arXiv:2501.185642025</p>
<p>Fine-tuning vision-language-action models: Optimizing speed and success. M J Kim, C Finn, P Liang, arXiv:2502.196452025</p>
<p>. S Reed, K Zolna, E Parisotto, S G Colmenarejo, A Novikov, G Barth-Maron, M Gimenez, Y Sulsky, J Kay, J T Springenberg, arXiv:2205.06175A generalist agent. 2022</p>
<p>Vima: General robot manipulation with multimodal prompts. Y Jiang, A Gupta, Z Zhang, G Wang, Y Dou, Y Chen, L Fei-Fei, A Anandkumar, Y Zhu, L Fan, NeurIPS 2022 Foundation Models for Decision Making Workshop. 2022</p>
<p>Palm-e: An embodied multimodal language model. D Driess, F Xia, M S Sajjadi, C Lynch, A Chowdhery, B Ichter, A Wahid, J Tompson, Q Vuong, T Yu, ICML. PMLR2023</p>
<p>Learning fine-grained bimanual manipulation with low-cost hardware. T Z Zhao, V Kumar, S Levine, C Finn, ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems. 2023</p>
<p>Vision-language foundation models as effective robot imitators. X Li, M Liu, H Zhang, C Yu, J Xu, H Wu, C Cheang, Y Jing, W Zhang, H Liu, The Twelfth ICLR. 2024</p>
<p>3d-vla: A 3d vision-language-action generative world model. H Zhen, X Qiu, P Chen, J Yang, X Yan, Y Du, Y Hong, C Gan, ICML. PMLR2024245</p>
<p>Behavior generation with latent actions. S Lee, Y Wang, H Etukuru, H J Kim, N M M Shafiullah, L Pinto, ICML. PMLR2024268</p>
<p>Openvla: An open-source vision-language-action model. M J Kim, K Pertsch, S Karamcheti, T Xiao, A Balakrishna, S Nair, R Rafailov, E P Foster, P R Sanketi, Q Vuong, 2024in 8th Annual CoRL</p>
<p>Quest: Selfsupervised skill abstractions for learning continuous control. A Mete, H Xue, A Wilcox, Y Chen, A Garg, NeurIPS. 372025</p>
<p>Autoregressive action sequence learning for robotic manipulation. X Zhang, Y Liu, H Chang, L Schramm, A Boularias, Rob. Autom. Lett. 2025</p>
<p>Carp: Visuomotor policy learning via coarse-to-fine autoregressive prediction. Z Gong, P Ding, S Lyu, S Huang, M Sun, W Zhao, Z Fan, D Wang, arXiv:2412.067822024</p>
<p>Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies. R Zheng, Y Liang, S Huang, J Gao, H Daumé, Iii , A Kolobov, F Huang, J Yang, arXiv:2412.103452024</p>
<p>Towards generalist robot policies: What matters in building vision-language-action models. X Li, P Li, M Liu, D Wang, J Liu, B Kang, X Ma, T Kong, H Zhang, H Liu, arXiv:2412.140582024</p>
<p>Fast: Efficient action tokenization for vision-language-action models. K Pertsch, K Stachowicz, B Ichter, D Driess, S Nair, Q Vuong, O Mees, C Finn, S Levine, arXiv:2501.097472025</p>
<p>D Qu, H Song, Q Chen, Y Yao, X Ye, Y Ding, Z Wang, J Gu, B Zhao, D Wang, arXiv:2501.15830Spatialvla: Exploring spatial representations for visual-language-action model. 2025</p>
<p>Vla-cache: Towards efficient vision-language-action model via adaptive token caching in robotic manipulation. S Xu, Y Wang, C Xia, D Zhu, T Huang, C Xu, arXiv:2502.021752025</p>
<p>Hamster: Hierarchical action models for open-world robot manipulation. Y Li, Y Deng, J Zhang, J Jang, M Memmel, R Yu, C R Garrett, F Ramos, D Fox, A Li, arXiv:2502.054852025</p>
<p>Magma: A foundation model for multimodal ai agents. J Yang, R Tan, Q Wu, R Zheng, B Peng, Y Liang, Y Gu, M Cai, S Ye, J Jang, CVPR. 202514214</p>
<p>Cot-vla: Visual chain-of-thought reasoning for vision-language-action models. Q Zhao, Y Lu, M J Kim, Z Fu, Z Zhang, Y Wu, Z Li, Q Ma, S Han, C Finn, CVPR. 2025</p>
<p>Univla: Learning to act anywhere with task-centric latent actions. Q Bu, Y Yang, J Cai, S Gao, G Ren, M Yao, P Luo, H Li, arXiv:2505.061112025</p>
<p>Latent action pretraining from videos. S Ye, J Jang, B Jeon, S J Joo, J Yang, B Peng, A Mandlekar, R Tan, Y.-W Chao, B Y Lin, 2024in CoRL 2024 Workshop</p>
<p>Robotic control via embodied chain-of-thought reasoning. M Zawalski, W Chen, K Pertsch, O Mees, C Finn, S Levine, 8th Annual CoRL. 2024</p>
<p>J Cen, C Yu, H Yuan, Y Jiang, S Huang, J Guo, X Li, Y Song, H Luo, F Wang, D Zhao, H Chen, arXiv:2506.21539Worldvla: Towards autoregressive action world model. 2025</p>
<p>Lotus: Continual imitation learning for robot manipulation through unsupervised skill discovery. W Wan, Y Zhu, R Shah, Y Zhu, ICRA. 2024</p>
<p>What matters in language conditioned robotic imitation learning over unstructured data. O Mees, L Hermann, W Burgard, Rob. Autom. Lett. 742122022</p>
<p>Bridgevla: Input-output alignment for efficient 3d manipulation learning with vision-language models. P Li, Y Chen, H Wu, X Ma, X Wu, Y Huang, L Wang, T Kong, T Tan, arXiv:2506.079612025</p>
<p>Se (3)-diffusionfields: Learning smooth cost functions for joint grasp and motion optimization through diffusion. J Urain, N Funk, J Peters, G Chalvatzaki, ICRA. IEEE2023</p>
<p>A0: An affordance-aware hierarchical model for general robotic manipulation. R Xu, J Zhang, M Guo, Y Wen, H Yang, M Lin, J Huang, Z Li, K Zhang, L Wang, arXiv:2504.126362025</p>
<p>Flow matching imitation learning for multi-support manipulation. Q Rouxel, A Ferrari, S Ivaldi, J.-B Mouret, Humanoids. 2024IEEE</p>
<p>Polarnet: 3d point clouds for language-guided robotic manipulation. S Chen, R G Pinel, C Schmid, I Laptev, CoRL. PMLR2023</p>
<p>Instruction-driven history-aware policies for robotic manipulations. P.-L Guhur, S Chen, R G Pinel, M Tapaswi, I Laptev, C Schmid, CoRL. PMLR2023</p>
<p>Perceiver-actor: A multi-task transformer for robotic manipulation. M Shridhar, L Manuelli, D Fox, CoRL2023</p>
<p>Rvt: Robotic view transformer for 3d object manipulation. A Goyal, J Xu, Y Guo, V Blukis, Y.-W Chao, D Fox, CoRL. PMLR2023</p>
<p>Act3d: 3d feature field transformers for multi-task robotic manipulation. T Gervet, Z Xian, N Gkanatsios, K Fragkiadaki, CoRL. PMLR2023</p>
<p>Same: Leveraging visual foundation model with sequence imitation for embodied manipulation. J Zhang, C Bai, H He, Z Wang, B Zhao, X Li, X Li, ICML. PMLR202458598</p>
<p>Equact: An se (3)-equivariant multi-task transformer for open-loop robotic manipulation. X Zhu, Y Qi, Y Zhu, R Walters, R Platt, arXiv:2505.213512025</p>
<p>Cliport: What and where pathways for robotic manipulation. M Shridhar, L Manuelli, D Fox, CoRL2022</p>
<p>Moka: Open-vocabulary robotic manipulation through mark-based visual prompting. F Liu, K Fang, P Abbeel, S Levine, ICRA Workshop. 2024</p>
<p>Ram: Retrieval-based affordance transfer for generalizable zero-shot robotic manipulation. Y Kuang, J Ye, H Geng, J Mao, C Deng, L Guibas, H Wang, Y Wang, 2024in 8th Annual CoRL</p>
<p>Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation. W Huang, C Wang, Y Li, R Zhang, L Fei-Fei, 2024in 2nd CoRL Workshop</p>
<p>Towards generalizable visionlanguage robotic manipulation: A benchmark and llm-guided 3d policy. R Garcia, S Chen, C Schmid, arXiv:2410.013452024</p>
<p>Omnimanip: Towards general robotic manipulation via object-centric interaction primitives as spatial constraints. M Pan, J Zhang, T Wu, Y Zhao, W Gao, H Dong, CVPR. 202517369</p>
<p>G R Team, S Abeyruwan, J Ainslie, J.-B Alayrac, M G Arenas, T Armstrong, A Balakrishna, R Baruch, arXiv:2503.20020Gemini robotics: Bringing ai into the physical world. 2025</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. A Brohan, N Brown, J Carbajal, Y Chebotar, X Chen, K Choromanski, T Ding, D Driess, arXiv:2307.158182023</p>
<p>Segment anything. A Kirillov, E Mintun, N Ravi, H Mao, C Rolland, L Gustafson, T Xiao, S Whitehead, A C Berg, W.-Y Lo, ICCV. 2023</p>
<p>Voxposer: Composable 3d value maps for robotic manipulation with language models. W Huang, C Wang, R Zhang, Y Li, J Wu, L Fei-Fei, arXiv:2307.059732023</p>
<p>Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0. A O'neill, A Rehman, A Maddukuri, A Gupta, A Padalkar, A Lee, ICRA. 2024</p>
<p>M Li, Z Zhao, Z Che, F Liao, K Wu, Z Xu, P Ren, Z Jin, N Liu, J Tang, arXiv:2506.03574Switchvla: Execution-aware task switching for visionlanguage-action models. 2025</p>
<p>Rlbench: The robot learning benchmark &amp; learning environment. S James, Z Ma, D R Arrojo, A J Davison, Rob. Autom. Lett. 522020</p>
<p>Object-centric representations improve policy generalization in robot manipulation. A Chapin, B Machado, E Dellandrea, L Chen, arXiv:2505.115632025</p>
<p>Adpro: a test-time adaptive diffusion policy for robot manipulation via manifold and initial noise constraints. Z Li, R Yang, R Chen, Z Luo, L Chen, arXiv:2508.062662025</p>
<p>Llm-based skill diffusion for zero-shot policy adaptation. W K Kim, Y Lee, J Kim, H Woo, NeurIPS. 372024</p>
<p>panda-gym: Open-source goal-conditioned environments for robotic learning. Q Gallouédec, N Cazin, E Dellandréa, L Chen, 4th NeurIPS Workshop. 2021</p>
<p>Roboverse: Towards a unified platform, dataset and benchmark for scalable and generalizable robot learning. H Geng, F Wang, S Wei, Y Li, B Wang, B An, C T Cheng, arXiv:2504.189042025</p>
<p>K Zakka, B Tabanpour, Q Liao, M Haiderbhai, S Holt, J Y Luo, A Allshire, E Frey, K Sreenath, L A Kahrs, arXiv:2502.08844Mujoco playground. 2025</p>
<p>Jacquard: A large scale dataset for robotic grasp detection. A Depierre, E Dellandréa, L Chen, in IROS. 2018</p>
<p>Rh20t: A robotic dataset for learning diverse skills in one-shot. H.-S Fang, H Fang, Z Tang, J Liu, J Wang, H Zhu, C Lu, RSS 2023 Workshop on Learning for Task and Motion Planning. 2023</p>
<p>Calvin: A benchmark for languageconditioned policy learning for long-horizon robot manipulation tasks. O Mees, L Hermann, Rob. Autom. Lett. 732022</p>
<p>Pybullet, a python module for physics simulation for games, robotics and machine learning. E Coumans, Y Bai, 2016</p>
<p>V-rep: A versatile and scalable robot simulation framework. E Rohmer, S P Singh, M Freese, IROS. IEEE2013</p>
<p>Libero: Benchmarking knowledge transfer for lifelong robot learning. B Liu, Y Zhu, C Gao, Y Feng, Q Liu, Y Zhu, P Stone, NeurIPS. 362023</p>
<p>Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. T Yu, D Quillen, Z He, R Julian, K Hausman, C Finn, S Levine, CoRL. PMLR2020</p>
<p>robosuite: A modular simulation framework and benchmark for robot learning. Y Zhu, J Wong, A Mandlekar, R Martín-Martín, A Joshi, S Nasiriany, Y Zhu, arXiv:2009.122932020</p>
<p>What matters in learning from offline human demonstrations for robot manipulation. A Mandlekar, D Xu, J Wong, S Nasiriany, C Wang, R Kulkarni, L Fei-Fei, 2021in 5th Annual CoRL</p>
<p>Maniskill2: A unified benchmark for generalizable manipulation skills. J Gu, F Xiang, X Li, Z Ling, X Liu, T Mu, Y Tang, S Tao, X Wei, Y Yao, X Yuan, P Xie, ICLR. 2023</p>
<p>Evaluating real-world robot manipulation policies in simulation. X Li, K Hsu, J Gu, K Pertsch, O Mees, H R Walke, C Fu, I Lunawat, I Sieh, S Kirmani, arXiv:2405.059412024</p>
<p>The colosseum: A benchmark for evaluating generalization for robotic manipulation. W Pumacay, I Singh, J Duan, R Krishna, J Thomason, D Fox, RSS Workshop. 2024</p>
<p>D4rl: Datasets for deep data-driven reinforcement learning. J Fu, A Kumar, O Nachum, G Tucker, S Levine, 2020</p>
<p>Language conditioned imitation learning over unstructured data. C Lynch, P Sermanet, arXiv:2005.076482020</p>
<p>Language-conditioned imitation learning with base skill priors under unstructured data. H Zhou, Z Bing, X Yao, X Su, C Yang, K Huang, A Knoll, Rob. Autom. Lett. 2024</p>
<p>Closed-loop visuomotor control with generative expectation for robotic manipulation. Y Yang, The Conference on Neural Information Processing Systems. 2024</p>
<p>Z Hou, T Zhang, Y Xiong, H Pu, C Zhao, R Tong, Y Qiao, J Dai, Y Chen, arXiv:2410.15959Diffusion transformer policy. 2024</p>
<p>Ghil-glue: Hierarchical control with filtered subgoal images. K B Hatch, A Balakrishna, O Mees, S Nair, S Park, B Wulfe, M Itkina, B Eysenbach, S Levine, arXiv:2410.200182024</p>
<p>Efficient diffusion transformer policies with mixture of expert denoisers for multitask learning. M Reuss, J Pari, P Agrawal, R Lioutikov, arXiv:2412.129532024</p>
<p>Gr-mg: Leveraging partially-annotated data via multi-modal goal-conditioned policy. P Li, H Wu, Y Huang, C Cheang, L Wang, T Kong, Rob. Autom. Lett. 2025</p>
<p>Predictive inverse dynamics models are scalable learners for robotic manipulation. Y Tian, S Yang, J Zeng, P Wang, D Lin, H Dong, J Pang, The Thirteenth ICLR. 2025</p>
<p>Rvt-2: Learning precise manipulation from few demonstrations. A Goyal, V Blukis, J Xu, Y Guo, Y.-W Chao, D Fox, RSS Workshop. 2024</p>
<p>Train a multi-task diffusion policy on rlbench-18 in one day with one gpu. Y Hu, P S , K Wen, R Detry, arXiv:2505.094302025</p>
<p>Coarse-tofine q-attention: Efficient learning for visual robotic manipulation via discretisation. S James, K Wada, T Laidlow, A J Davison, CVPR. 2022748</p>
<p>Mail: Improving imitation learning with selective state space models. X Jia, Q Wang, A Donat, B Xing, G Li, H Zhou, O Celik, D Blessing, R Lioutikov, G Neumann, CoRL2024</p>
<p>Tinyllava: A framework of small-scale large multimodal models. B Zhou, Y Hu, X Weng, J Jia, J Luo, X Liu, J Wu, L Huang, arXiv:2402.142892024</p>
<p>Sofar: Language-grounded orientation bridges spatial reasoning and object manipulation. Z Qi, W Zhang, Y Ding, R Dong, X Yu, J Li, L Xu, B Li, X He, G Fan, arXiv:2502.131432025</p>
<p>M Shridhar, Y L Lo, S James, arXiv:2407.07875Generative image as action models. 2024</p>
<p>R3m: A universal visual representation for robot manipulation. S Nair, A Rajeswaran, V Kumar, C Finn, A Gupta, CoRL. PMLR2023</p>
<p>Manual2skill: Learning to read manuals and acquire robotic skills for furniture assembly using vision-language models. C Tie, S Sun, J Zhu, Y Liu, J Guo, Y Hu, H Chen, J Chen, R Wu, L Shao, arXiv:2502.100902025</p>
<p>Srsa: Skill retrieval and adaptation for robotic assembly tasks. Y Guo, B Tang, I Akinola, D Fox, A Gupta, Y Narang, The Fifteenth ICLR. 2025</p>
<p>Point-level visual affordance guided retrieval and adaptation for cluttered garments manipulation. R Wu, Z Zhu, Y Wang, Y Chen, J Wang, H Dong, CVPR. 2025</p>
<p>M Ahn, A Brohan, N Brown, Y Chebotar, O Cortes, B David, C Finn, C Fu, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022</p>
<p>Shake-vla: Vision-language-action model-based system for bimanual robotic manipulations and liquid mixing. M H Khan, S Asfaw, D Iarchuk, M A Cabrera, L Moreno, arXiv:2501.069192025</p>
<p>An improved single short detection method for smart vision-based water garbage cleaning robot. A Haldorai, M Suriya, M Balakrishnan, Cognitive Robotics. 42024</p>
<p>Learning fusion feature representation for garbage image classification model in human-robot interaction. X Li, T Li, S Li, B Tian, J Ju, Infrared Physics &amp; Technology. 1281044572023</p>
<p>Machine learning-based garbage detection and 3d spatial localization for intelligent robotic grasp. Z Lv, T Chen, Z Cai, Z Chen, Applied Sciences. 1318100182023</p>
<p>Foundational Models for Robotics need to be made Bio-Inspired. L Chen, S M Nguyen, ARSO. Jul. 2025</p>
<p>Waypoint-based reinforcement learning for robot manipulation tasks. S Mehta, S Habibian, D Losey, IROS. 2024</p>
<p>Rui Yang received a B.S. degree from Wuhan University in 2017 and an Engineering degree from École Centrale de Lyon (ECL) in 2020. He is currently pursuing a Ph.D. degree at LIRIS, École Centrale de Lyon. His research interests continual learning and robotic manipulation. Bruno Machado received an M.S. degree from the École Nationale Supérieure de l' Électronique et ses Applications (ENSEA) in 2023. He is currently pursuing a Ph.D. at the École Centrale de Lyon (ECL). She mainly focuses on computational conformal geometry, computer mathematics, and its applications in computer vision, geometric modeling, and embodied intelligence. Emmanuel Dellandrea is Associate Professor at Ecole Centrale de Lyon, France, since 2004. He was awarded an M.S. and Engineering. France; Beijing, China; France; France; Paris 6, France2005 and 2008. 2011. 1984. 1986. 1989Zezeng Li received a B.S. degree from Beijing University of Technology (BJUT) in 2015 and a Ph.D. degree from Dalian University of Technology (DUT ; Institut National des Sciences Appliquées de Rennes (Insa Rennes ; D. degree at School of Computer Science and Engineering, Beihang University ; Computer Science from the Université de Tours ; from Beihang University ; Beihang University,as a Faculty Member, where he is currently a Professor. His research interests include computer vision, pattern recognition, and representation learning. He is a Senior Member of the IEEE. Liming Chen was awarded his B.Sc. degree in joint mathematics-computer science from the University of Nantes. and his M.S. and Ph.D. degrees from the University. He first served as an Associate Professor with the Universite de Technologie de Compi'egne, before joining the Ecole Centrale de Lyon as a Professor in 1998, where he leads an Advanced Research Team in multimedia computing and pattern recognition. His current research interests include computer vision and multimedia, and in particular face analysis, image and video categorization, affective computing, and robotic manipulation. He is a Senior Member of the IEEE</p>            </div>
        </div>

    </div>
</body>
</html>
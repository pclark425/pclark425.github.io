<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5035 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5035</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5035</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-106.html">extraction-schema-106</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-91408ef7aa1c5278e01c2deaf681f2ee7e9343ff</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/91408ef7aa1c5278e01c2deaf681f2ee7e9343ff" target="_blank">InstanceRefer: Cooperative Holistic Understanding for Visual Grounding on Point Clouds through Instance Multi-level Contextual Referring</a></p>
                <p><strong>Paper Venue:</strong> IEEE International Conference on Computer Vision</p>
                <p><strong>Paper TL;DR:</strong> The non-trivial 3D visual grounding task has been effectively re-formulated as a simplified instance-matching problem, considering that instance-level candidates are more rational than the redundant 3D object proposals.</p>
                <p><strong>Paper Abstract:</strong> Compared with the visual grounding on 2D images, the natural-language-guided 3D object localization on point clouds is more challenging. In this paper, we propose a new model, named InstanceRefer1, to achieve a superior 3D visual grounding through the grounding-by-matching strategy. In practice, our model first predicts the target category from the language descriptions using a simple language classification model. Then, based on the category, our model sifts out a small number of instance candidates (usually less than 20) from the panoptic segmentation on point clouds. Thus, the non-trivial 3D visual grounding task has been effectively re-formulated as a simplified instance-matching problem, considering that instance-level candidates are more rational than the redundant 3D object proposals. Subsequently, for each candidate, we perform the multi-level contextual inference, i.e., referring from instance attribute perception, instance-to-instance relation perception, and instance-to-background global localization perception, respectively. Eventually, the most relevant candidate is selected and localized by ranking confidence scores, which are obtained by the cooperative holistic visual-language feature matching. Experiments confirm that our method outperforms previous state-of-the-arts on ScanRefer online benchmark and Nr3D/Sr3D datasets.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5035",
    "paper_id": "paper-91408ef7aa1c5278e01c2deaf681f2ee7e9343ff",
    "extraction_schema_id": "extraction-schema-106",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0042225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>InstanceRefer: Cooperative Holistic Understanding for Visual Grounding on Point Clouds through Instance Multi-level Contextual Referring</h1>
<p>Zhihao Yuan ${ }^{1, \dagger}$, Xu Yan ${ }^{1, \dagger}$, Yinghong Liao ${ }^{1}$, Ruimao Zhang ${ }^{1}$, Sheng Wang ${ }^{2}$, Zhen Li ${ }^{1, *}$, Shuguang Cui ${ }^{1}$<br>${ }^{1}$ The Chinese University of Hong Kong (Shenzhen), Shenzhen Research Institute of Big Data<br>${ }^{2}$ CryoEM Center, Southern University of Science and Technology<br>{zhihaoyuan@link., xuyan1@link., lizhen@}cuhk.edu.cn</p>
<h4>Abstract</h4>
<p>Compared with the visual grounding on 2D images, the natural-language-guided 3D object localization on point clouds is more challenging. In this paper, we propose a new model, named InstanceRefer ${ }^{1}$, to achieve a superior 3D visual grounding through the grounding-by-matching strategy. In practice, our model first predicts the target category from the language descriptions using a simple language classification model. Then based on the category, our model sifts out a small number of instance candidates (usually less than 20) from the panoptic segmentation on point clouds. Thus, the non-trivial 3D visual grounding task has been effectively re-formulated as a simplified instance-matching problem, considering that instancelevel candidates are more rational than the redundant 3D object proposals. Subsequently, for each candidate, we perform the multi-level contextual inference, i.e., referring from instance attribute perception, instance-to-instance relation perception, and instance-to-background global localization perception, respectively. Eventually, the most relevant candidate is selected and localized by ranking confidence scores, which are obtained by the cooperative holistic visual-language feature matching. Experiments confirm that our method outperforms previous state-of-the-arts on ScanRefer online benchmark and Nr3D/Sr3D datasets.</p>
<h2>1. Introduction</h2>
<p>Visual grounding (VG), which aims at localizing the desired objects or areas in an image or a video based on an object-related linguistic query, has achieved great progress in the 2D computer vision community [12, 18, 29, 17, 19]. With the rapid development of 3D sensor and 3D representation, the VG task has gradually merged more informative</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Multi-level contextual referring. For each instancelevel candidate, we match it with linguistic query from attribute, local relation and global localization. The attribute, relation and localization descriptions are in orange, blue and green boxes.</p>
<p>3D data. Unlike 2D images with regular and well-organized pixels, 3D data mostly comes in the form of point clouds, which is sparse, irregular, and unordered. Therefore, previous 2D-based schemes are usually deficient for real 3D scenarios.</p>
<p>Chen et al. [2] is the pioneer for visual grounding on point clouds. They propose the first dataset ScanRefer and solve the problem by extending the 2D grounding-bydetection pipeline to 3D. Specifically, it first uses a 3D object detector [24] to generate hundreds of proposals. Then the feature of each proposal is merged with a global representation of the linguistic query to predict a matching score. The proposal with the maximal score is considered as the</p>
<p>object we are looking for. However, it suffers from several issues when transferring the 2D method to 3D VG as follows: 1) The object proposals in the large 3D scene are usually redundant. Compared with the actual instances, the number of proposals is large and the inter-proposal relationship is complex, which inevitably introduces noise and ambiguity. 2) The appearance and attribute information is not sufficiently captured. Due to the noise and occlusion, the obtained point clouds are usually sparse and incomplete, leading to missing geometric details in object-aware proposals. Conventional point cloud-based methods fail to effectively extract the attribute information, e.g., red, gray, and wooden, which might ignore some vital linguistic cues for referring. 3) The relations among proposals and the ones between proposals and background are not fully studied.</p>
<p>To address the above issues, this paper investigates a novel framework, namely InstanceRefer, to achieve a superior visual grounding on point clouds with grounding-by-matching strategy. Specifically, via the global panoptic segmentation, our proposed model extracts several instance point clouds from the original scene. These instances are subsequently filtered by the predicted category from the natural language descriptions, obtaining the candidates set. Compared with the object-proposal based candidates [2], these filtered instance point clouds contain more original geometric and attribute details (i.e., color, texture, etc.) while maintaining a smaller number. We notice that the recent work TGNN [10] also employs instance segmentation to reduce the difficulty of referring. However, they directly exploit the learned semantic scores from the segmentation backbone as the instance features, which suffer from the lossy geometric and attribute information. By comparison, our InstanceRefer applied filtered candidates and their original information for further referring. Thus, it can not only reduce the number of the candidates, but also maintain each candidate's original information. Besides, to fully comprehend the whole scene, multi-level contextual learning modules are further proposed, i.e., explicitly capturing the context of each candidate from instance attributes, instance-to-instance relationships, and instance-to-background global localization, respectively. Eventually, with the well-designed matching module and contrastive strategy, InstanceRefer can efficiently and effectively select and localize the target. In consequence, our model outperforms previous methods by a large margin regardless of any settings, i.e., exploiting any segmentation backbone.</p>
<p>In summary, the key contributions of this paper are as follows: 1) We propose a new framework InstanceRefer for visual grounding on point clouds, which exploits panoptic segmentation and language cues to select the instance point clouds as candidates and re-formulates the task in a grounding-by-matching manner. 2) Three novel components are proposed to select the most relevant instance can-
didate from attributes, local relations, and global localization aspects jointly. 3) Experimental results on ScanRefer and Sr3D/Nr3D datasets confirm the superiority of InstanceRefer, which achieves state-of-the-arts on ScanRefer benchmark and Nr3D/Sr3D dataset.</p>
<h2>2. Related Work</h2>
<p>Visual Grounding on 2D Images. The task of visual grounding on images is to localize a specific area of the image described by a natural language query. Depending on the type of language query, it can be further divided into phrase localization [12, 23, 29] and referring expression comprehension [20, 16, 30]. Most approaches conduct localization in the bounding box level and a two-stage manner. The first stage is to generate candidate proposals with either unsupervised methods or a pre-trained object detection network. In the second stage, the best matching proposal is selected according to the language query. Such methods mainly focus on improving the ranking accuracy of the second stage. MAttNet [35] proposes a modular attention network to decompose the language query to different components (i.e., subject appearance, location, and relationship to other objects) and process them in different modular networks separately. Inspired but different from MAttNet, our work delves specifically into the characteristics of 3D point clouds, and each proposed module differs greatly from those in MAttNet.
Visual Grounding on 3D Point Clouds. Chen et al. [2] releases the first 3D VG dataset ScanRefer, in which the object bounding boxes are referred by their corresponding language queries in an indoor scene. ReferIt3D [1] also proposes two datasets for 3D VG, Sr3D (labeled by machine) and Nr3D (labeled by human). Different from ScanRefer, ReferIt3D assumes that all objects are wellsegmented, thus localization is not required. Very recently, TGNN [10] proposes a similar task called referring 3D instance segmentation, which aims to segment out the target instance. It first extracts per-point features and predicts offsets for object clustering. Then a Text-Guided Graph Neural Network is applied to achieve more accurate referring. However, TGNN fails to capture the attribute of instances and instance-to-background relations. Goyal et al. [6] also presents a dataset named Rel3D for only grounding object spatial relations. In this paper, we focus on the task of visual grounding on raw point clouds, which is extended from ScanRefer and ReferIt3D.
3D Representation Learning on Point Clouds. Unlike 2D images with regular grids, point clouds are irregular and often sparsely-scattered. Recently, point-based models leverage the permutation invariant nature of raw point cloud for enhanced 3D processing [25, 26]. Specifically, most pointbased models first sample sub-points from the initial point clouds, then apply a feature aggregation function on each</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. The pipeline of InstanceRefer. It firstly uses the panoptic segmentation model to extract all the instance point clouds in the large 3D scene. Under the guidance of target prediction from language description, the instances belonging to the target category are filtered out to form the initial candidates $\hat{P}^{I}$. In parallel, the summarized language encoding $\hat{E}$ is achieved through attention pooling. Subsequently, a visual-language matching module outputs the similarity score Q through comparing multi-level visual perceptions (<em>i.e.</em>, $\hat{F}^{A}$, $\hat{F}^{R}$, and $\hat{F}^{G}$) against $\hat{E}$. Eventually, 3D bonding-box of the instance with the highest score is regarded as the final grounding result.</p>
<p>sub-point in a local point cloud cluster after the grouping. The representatives of the point-based methods are graph-based learning [31, 38, 15, 14] and convolution-like operations [37, 28, 32, 8, 33]. With the development of the representation learning on point clouds, various downstream tasks related to visual grounding on point clouds have been explored rapidly, <em>e.g.</em>, 3D object detection [24] and instance segmentation [11].</p>
<h1>3. Method</h1>
<p>InstanceRefer is a novel framework for point cloud VG, which conducts the multi-level contextual referring to select the most relevant instance-level object. As shown in Figure 2, by exploiting point cloud panoptic segmentation, InstanceRefer first extracts instances with their semantic labels from the whole scene (Sec. 3.1). Following that, the sentences are embedded into a high-dimensional feature space, and a text classification is conducted as the linguistic guidance (Sec. 3.2). Finally, after filtering out the candidates from all instances, the three-level progressive referring modules, <em>i.e.</em>, attribute perception (AP) module, relation perception (RP) module, and global localization perception (GLP) module (Sec. 3.3), are employed to select the optimal candidate.</p>
<h2>3.1. Instance Set Generation</h2>
<p>Unlike ScanRefer [2] that selects the candidates from all object proposals, our framework first extracts all foreground instances from the input point cloud to generate a set of instances. Then, we re-formulate the 3D visual grounding problem as an instance-matching problem.</p>
<p>For this purpose, panoptic segmentation [13] is adopted in our model, which aims to tackle the semantic segmentation and the instance segmentation jointly. Taking a point cloud $P \in \mathbb{R}^{N \times 3}$ and its features $F \in \mathbb{R}^{N \times D}$ as input, InstanceRefer returns two prediction sets, semantics $S \in \mathbb{R}^{N \times 1}$ and instance masks $I \in \mathbb{R}^{N \times 1}$, recording the semantic class and the instance index of each point, respectively. Through the instance masks, InstanceRefer extracts instance point clouds from the original scene point clouds. All instance point clouds in the foreground are represented as $\boldsymbol{P}^{I} = {P_{i}^{I}}<em i="i">{i=0}^{M}$, where $P</em>$, respectively.}^{I}$ means the points of the $i$-th instance within the total $M$ instances. Similarly, features and semantics of all instances are denoted as $\boldsymbol{F}^{I}$ and $\boldsymbol{S}^{I</p>
<h2>3.2. Description Encoding</h2>
<p>Each token of the language description is first mapped into the 300d-vector via the pre-trained GloVE word embedding [22]. Then the whole sequence is fed into Bidirectional GRU layers [3] to extract the contextual word features $E = {e_i} \in \mathbb{R}^{N_w \times D}$, where $N_w$ is the query length and $D$ is the feature dimension. The final language encoding is achieved through attention pooling. In practice, the attention pooling updates each word feature and aggregate them to a global representation by</p>
<p>$$
\hat{e}_i = \text{AvgPool}({\text{Rel}(e_i, e_j) \odot e_j, \forall e_j \in E}), \tag{1}
$$</p>
<p>$$
\hat{E} = \text{MaxPool}({\hat{e}<em i="1">i}</em>
$$}^{N_w}), \tag{2</p>
<p>where the aggregation function $\text{AvgPool}(\cdot)$ and $\text{MaxPool}(\cdot)$ are set as average pooling and max pooling and a pairwise relationship function $\text{Rel}(\cdot)$ is the normalized dot-product similarity between features of two tokens, and the sign $\odot$ represents the element-wise multiplication. The feature of each token $e_i \in E$ is first updated to $\hat{e}_i$ via the aggregation of all token features weighted by relation.</p>
<p>Then, the global representation of the query is obtained, i.e., $\hat{E}\in\mathbb{R}^{1\times D}$. Furthermore, by appending an additional GRU layer and linear layer, InstanceRefer predicts the target category of the query by language features. This output aids the model in subsequently filtering out the candidates from all instances.</p>
<h3>3.3 Multi-Level Visual Context</h3>
<p>Before feeding the instances into the following modules, InstanceRefer first uses the predicted target category from the language encoder to filter candidates. For example, as shown in Figure 2, regarding all instances extracted from the original point cloud, we only keep the remaining instances belonging to the target category ‘chair’. Subsequently, the corresponding point clouds and features of candidates $\hat{\boldsymbol{P}}^{I}$ and $\hat{\boldsymbol{F}}^{I}$ are attained. Note that the target classification accuracy of language query is over 97%. Hence, the exploited filtering operation will not introduce obvious noise for further referring, while it can greatly boost the grounding performance for the unique instance candidate scenario. Then, the filtered instances are compared with the following multi-level visual context modules.</p>
<p>AP Module. Considering there are many adjectives in a sentence (e.g., “a long bookshelf” for scale, “a brown chair” for color, “a square table” for shape, etc.), the attribute perception (AP) module is designed to explicitly capture such information from attribute phrases. Concretely, the AP module takes the information of the $i$-th candidate, i.e., point cloud $\hat{P}<em i="i">{i}^{I}$ and its attribute features $\hat{F}</em>$ is obtained.}^{I}$ as input and generates a global representation vector $F_{i}^{A}$ of the candidate. As shown in Figure 3 (a), our model constructs a four-layer Sparse Convolution (SparseConv) [7] as the feature extractor. The extractor firstly voxelizes the point cloud into 3D voxels and then conducts the convolution operation only on non-empty voxels in a more efficient way. Subsequently, through an average pooling, the feature representation $\hat{F}^{A}\in\mathbb{R}^{1\times D</p>
<p>RP Module. Since there are many descriptions about the relations between different instances, e.g., “The desk is between a black chair and bed”, using only the attributerelated manner fails to capture such information. Therefore, a relation perception (RP) module is proposed for the relation encoding between the candidate and its surrounding instances. Figure 3 (b) illustrates the design of the RP module. Given the $i$-th candidate point cloud $\hat{P}<em i="i">{i}^{I}$, the RP module first searches $K$ instance-level neighborhoods that have the closest Euclidean distance to the center of instance $\hat{P}</em>$ between the $i$-th candidate and the $k$-th neighboring instance, the RP module adopts the DGCNN [31]:}^{I}$. Following that, a graph-based aggregation is employed to fuse the features of local neighborhoods. To define the edge-relation $r_{ij</p>
<p>$r_{ik}=\operatorname{MLP}([\mathcal{C}(\hat{P}<em k="k">{i}^{I})-\mathcal{C}(P</em>]),$ (3)
}^{I});S_{i}^{I} ;S_{k}^{I<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. The inner structure of AP and RP modules. Part (a) illustrates the attribute perception (AP) module, which uses a four-layer SparseConv to extract the global features of each candidate. In part (b), the relation perception (RP) module aggregates the information of the candidate with its neighboring instances.</p>
<p>where $\mathcal{C}(\cdot)$ is to choose center coordinates of instances, $S_{i}^{I}$ and $S_{k}^{I}$ represent the semantic instance masks for the $i$-th and $k$-th instances, respectively. $\operatorname{MLP}(\cdot)$ denotes the multilayer perceptrons. The sign $[\cdot ;\cdot]$ means the channel-wise concatenation. Through Eq. (3), the RP module considers not only the relative positions but also the semantic relationships between the candidate and its neighbors. Eventually, the enhanced feature $\hat{F}_{i}^{R}\in\mathbb{R}^{1\times D}$ for the $i$-th candidate can be obtained by</p>
<p>$$
\begin{aligned}
h_{ik} &amp; =\operatorname{MLP}\left(\left[P_{k}^{I} ; S_{k}^{I}\right]\right), \forall P_{k}^{I} \in \mathcal{N}\left(\hat{P}<em i="i">{i}^{I}, K\right) \
\hat{F}</em>\right)
\end{aligned}
$$}^{R} &amp; =\operatorname{MaxPool}\left(\left{r_{ik} \odot h_{ik}\right}_{k=1}^{K</p>
<p>where $\mathcal{N}\left(\hat{P}<em i="i">{i}^{I}, K\right)$ denotes the $K$ nearest neighboring instances for $\hat{P}</em>(\cdot)$ have the same output dimensions with that in Eq. (3).}^{I}$, and $h_{ik}$ is the unified representation of the coordinate and the semantic instance mask for the $k$-th instance. Note that the above $\operatorname{MLP</p>
<p>GLP Module. The global localization perception (GLP) module aims to supplement the background information neglected by the two aforementioned modules. There are other descriptions about global localization information, e.g., “in the corner” and “next to the wall”, but such information cannot be included in the AP and RP modules. As shown in Figure 4, the GLP module takes the entire point cloud as the input. By employing another SparseConv encoder, the module first extracts the point-wise features of the entire scene. An average pooling in the height axis is then performed to generate the bird-eyes-view (BEV) features. Note that each input point cloud is divided into $3 \times 3$ areas in the BEV plane. By repeatedly concatenating the language features $\hat{E}$ and then flowing through the MLPs,</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Illustration of GLP module. GLP module firstly predicts the localization of the target in a 3×3 bird-eyes-view (BEV) plane. It then uses the interpolated features and candidate features to generate the aggregated feature $\hat{F}^{G}_{i}$.</p>
<p>The GLP module predicts the object candidate's location in one of the nine areas. Moreover, the probability of each area is interpolated into point clouds of the i-th candidate $\hat{P}^{I}_{i}$ by</p>
<p>$$
\hat{f}<em j="1">{i,k} = \sum</em>}^{n} ||\hat{p<em j="j">{i,k}^{I} - a</em>||}^{e<em j="j">{2}^{-1} \cdot a</em>
$$}^{p}, \tag{6</p>
<p>where $\hat{p}<em i="i">{i,k}^{I} \in \hat{P}^{I}</em>}$ and $\hat{f<em j="j">{i,k}$ are the coordinates and interpolated features of the k-th point in the i-th candidate. n = 9 represents the total number of areas, $a</em>$.}^{p}$ and $a_{j}^{e}$ are the probabilities of localization and center coordinates of j-th area in the total nine areas, respectively. Finally, after concatenating interpolated features with candidate features, another MLPs with max-pooling aggregates the features of the i-th candidate to a global representation $\hat{F}^{G}_{i</p>
<h3>3.4. Visual-Language Matching</h3>
<p>With multi-level visual features of the candidates (<em>i.e.</em>, $\hat{\mathbf{F}}^{A}, \hat{\mathbf{F}}^{B}, \hat{\mathbf{F}}^{G}$) and language features $\hat{E}$, we perform a matching operation to obtain the confidence score for each instance. Considering the obtained multiple features, a simple scheme of pinpointing the referred target is to find the most relevant visual features to the linguistic ones through their similarities. However, this approach ignores the varying proportions for three perception modules. To tackle this issue, we utilize the modular co-attention from MCAN [36] to perform adaptive visual-language matching. For the i-th instance, we concatenate three visual features to the merged features $\hat{F}_{i} \in \mathbb{R}^{1 \times (D \times 3)}$. Then, we further employ three co-attention layers to aggregate the language features to update the instance feature. Finally, the sigmoid activation function is exploited to output the instance score.</p>
<h3>3.5. Contrastive Objective</h3>
<p>For the objective function, we adopt a contrastive manner to train our network. Here, we define an instance as a positive example for a query if its IoU with the GT object bounding box exceeds the threshold Γ, otherwise a negative example. If there is no positive example for a query, we do not compute its loss. Intuitively, the matching score of positive pairs should be higher than negative pairs. Thus we derive our matching loss from [27] with the consideration of multiple positive examples as follows:</p>
<p>$$
L_{\text{mat}} = -\log \frac{\sum_{i=1}^{L} \exp(Q_{i}^{+})}{\sum_{i=1}^{L} \exp(Q_{i}^{+}) + \sum_{i=L+1}^{M} \exp(Q_{i}^{-})}, \tag{7}
$$</p>
<p>where $Q^+$ and $Q^-$ denote the scores of positive and negative pairs, $L$ and $M$ are the numbers of positive candidates and total candidates in a scene, respectively. All candidates are introduced to the optimization process.</p>
<p>The language classification loss $L_{\text{cls}}$ and the BEV localization loss $L_{\text{bev}}$ are also included for the joint target categorization and localization prediction. The final loss is a weighted sum of matching loss, including the object classification loss on the linguistic queries and the localization loss, $L = L_{\text{mat}} + \lambda_1 L_{\text{cls}} + \lambda_2 L_{\text{bev}}$, where $\lambda_1 = \lambda_2 = 0.1$ are the weights to adjust the ratios of each loss, respectively. The IoU threshold Γ is set as 0.3.</p>
<h2>4. Experiments</h2>
<p>In this section, we present the experimental procedures and analysis in detail to demonstrate the effectiveness of our InstanceRefer in 3D visual grounding.</p>
<h3>4.1. Implementation</h3>
<p>In our experiment, we adopt the official pre-trained PointGroup [11] as the backbone to perform the panoptic segmentation. For language encoding, we employ the same GloVE and BiGRU used in ScanRefer [2] to generate the word features in channel D = 256. The output of self-attention preserves the identical 256 channels. The AP module consists of four 3D sparse convolution blocks, each of which has two 3D sparse convolutions inside. Going deeper, we gradually increase the number of channels (<em>i.e.</em>, 32, 64, 128, 256). The GLP module applies the same structure of the sparse convolution block but with fewer blocks (<em>i.e.</em>, 3 blocks with channel 32, 128, 256). In the RP module, the kNN instance number K is 8, and the channel numbers of two MLPs are (256, 256) and (256), respectively.</p>
<p>We train the network for 30 epochs by using the Adam optimizer with a batch size of 32. The learning rate of the network is initialized as 0.0005 with the decay as 0.9 for every 10 epochs. All experiments are implemented on PyTorch and a single NVIDIA 1080Ti GPU. We will release our code and pre-trained model for future research.</p>
<p>Table 1: Comparison of localization results. TGNN replaces the original GRU layers with pre-trained BERT to extract language features. Our method follows TGNN’s strategy of only taking coordinates (Geo) and color information (RGB) as input, while results of ScanRefer on benchmark are obtained by using additional normals (Nor) and multi-view features from a pre-trained 2D feature extractor. Scores for the test set are obtained from the online evaluation. Only the published methods are compared. Accessed on March 18, 2021.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Input</th>
<th>Unique</th>
<th></th>
<th>Multiple</th>
<th></th>
<th>Overall</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Acc@0.25</td>
<td>Acc@0.5</td>
<td>Acc@0.25</td>
<td>Acc@0.5</td>
<td>Acc@0.25</td>
<td>Acc@0.5</td>
</tr>
<tr>
<td>Validation results</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>SCRC <em>[9]</em></td>
<td>RGB image</td>
<td>24.03</td>
<td>9.22</td>
<td>17.77</td>
<td>5.97</td>
<td>18.70</td>
<td>6.45</td>
</tr>
<tr>
<td>One-stage <em>[34]</em></td>
<td>RGB image</td>
<td>29.32</td>
<td>22.82</td>
<td>18.72</td>
<td>6.49</td>
<td>20.38</td>
<td>9.04</td>
</tr>
<tr>
<td>ScanRefer <em>[2]</em></td>
<td>Geo + RGB</td>
<td>65.00</td>
<td>43.31</td>
<td>30.63</td>
<td>19.75</td>
<td>37.30</td>
<td>24.32</td>
</tr>
<tr>
<td>TGNN <em>[10]</em></td>
<td>Geo + RGB</td>
<td>64.50</td>
<td>53.01</td>
<td>27.01</td>
<td>21.88</td>
<td>34.29</td>
<td>27.92</td>
</tr>
<tr>
<td>TGNN<em>[10]</em>+BERT <em>[5]</em></td>
<td>Geo + RGB</td>
<td>68.61</td>
<td>56.80</td>
<td>29.84</td>
<td>23.18</td>
<td>37.37</td>
<td>29.70</td>
</tr>
<tr>
<td>IntanceRefer (Ours)</td>
<td>Geo + RGB</td>
<td>77.45</td>
<td>66.83</td>
<td>31.27</td>
<td>24.77</td>
<td>40.23</td>
<td>32.93</td>
</tr>
<tr>
<td>Test results (ScanRefer benchmark)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>ScanRefer <em>[2]</em></td>
<td>Geo+Nor+Multiview</td>
<td>68.59</td>
<td>43.53</td>
<td>34.88</td>
<td>20.97</td>
<td>42.44</td>
<td>26.03</td>
</tr>
<tr>
<td>TGNN <em>[10]</em></td>
<td>Geo + RGB</td>
<td>62.40</td>
<td>53.30</td>
<td>28.20</td>
<td>21.30</td>
<td>35.90</td>
<td>28.50</td>
</tr>
<tr>
<td>TGNN <em>[10]</em>+BERT <em>[5]</em></td>
<td>Geo + RGB</td>
<td>68.34</td>
<td>58.94</td>
<td>33.12</td>
<td>25.26</td>
<td>41.02</td>
<td>32.81</td>
</tr>
<tr>
<td>IntanceRefer (Ours)</td>
<td>Geo + RGB</td>
<td>77.82</td>
<td>66.69</td>
<td>34.57</td>
<td>26.88</td>
<td>44.27</td>
<td>35.80</td>
</tr>
</tbody>
</table>
<h3>4.2 Dataset and Metrics</h3>
<p>ScanRefer. The ScanRefer dataset is a newly proposed 3D scene visual grounding dataset, to the best of our knowledge, which consists of 51,538 descriptions for ScanNet scenes <em>[4]</em>. The dataset is split into 36,655 samples for training, 9,508 samples for validation, and 5,410 samples for testing, respectively.</p>
<p>For the evaluation metrics, it calculates the 3D intersection over union (IoU) between the predicted bounding box and ground truth. The Acc@$m$IoU is adopted as the evaluation metric, where $m\in{0.25,0.5}$. Accuracy is reported in “unique” and “multiple” categories, respectively. If only a single object of its class exists in the scene, we regard it as “unique”, otherwise “multiple”. Moreover, to fully evaluate our model, we conduct a fair comparison on both the validation set and test set. ScanRefer benchmark conducts online testing and every method is allowed to submit results for only twice.</p>
<p>Nr3D and Sr3D. ReferIt3D <em>[1]</em> dataset uses the same train/valid split with ScanRefer on ScanNet but exploits manually extracted instances as input, i.e., object masks for each scene are provided, and aims to choose the solely referred object. Specifically, it contains two datasets, where Sr3D (Spatial Reference in 3D) has 83.5K synthetic expressions generated by templates and Nr3D (Natural Reference in 3D) consists of 41.5K human expressions collected in a similar manner as ReferItGame <em>[12]</em>. Since ReferIt3D directly uses point clouds of instances as input, it can be seen as the instance-matching stage of our InstanceRefer without interaction with the environment, i.e., wall and floor.</p>
<p>We empirically validate AP and RP modules on ReferIt3D to verify the effectiveness of our proposed modules. We use the same evaluation strategies and metrics with their paper.</p>
<h3>4.3 Quantitative Comparisons</h3>
<p>We first compare IntanceRefer with the state-of-the-art methods on the ScanRefer dataset, where the results are displayed in Table 1. Among these methods, SCRC <em>[9]</em> and One-stage <em>[34]</em> are image-based methods with the RGB images as input. Specifically, they select the 2D bounding box with the highest confidence score and project it to the 3D space using the depth map of that frame. ScanRefer <em>[2]</em> and TGNN <em>[10]</em> are point cloud based methods that take coordinates and other features of point clouds as input. In this paper, we follow the input modality of TGNN <em>[10]</em>, which only exploits geometric coordinates (XYZ) and color information (RGB) as input.</p>
<p>As shown in Table 1, our model gains the highest scores on both validation set and online benchmark. Note that the image-based methods (i.e., SCRC and One-stage) fail to achieve satisfactory results since they are limited by the view of a single frame. Though TGNN also applies PointGroup to perform instance segmentation, our method outperforms it by large margins, especially in the “unique” case, which mainly benefits from the rational strategy of filtering candidates. Furthermore, the improvements of our model are apparent, reporting 11.8% in “unique” and 6.7% “multiple” in Acc@0.5 when employing both the GloVE and GRU as the language encoder. Our improvements originate from the well-designed pipeline and three novel models, whereas the improvements of TGNN are largely based on the pre-trained BERT embeddings. More impor-</p>
<p>Table 2: Comparison of referring object identification on Nr3D and Sr3D datasets. Here ‘easy’ and ‘hard’ determined by whether there are more than two instances of the same object class in the scene. ‘view-dependent’ and ‘view-independent’ determined by whether the referring expression depending on camera view.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Method</th>
<th>Easy</th>
<th>Hard</th>
<th>View-dep.</th>
<th>View-indep.</th>
<th>Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td>Nr3D</td>
<td>ReferIt3DNet<em>[1]</em></td>
<td>$43.6\%\pm 0.8\%$</td>
<td>$27.9\%\pm 0.7\%$</td>
<td>$32.5\%\pm 0.7\%$</td>
<td>$37.1\%\pm 0.8\%$</td>
<td>$35.6\%\pm 0.7\%$</td>
</tr>
<tr>
<td></td>
<td>TGNN<em>[10]</em></td>
<td>$44.2\%\pm 0.4\%$</td>
<td>$30.6\%\pm 0.2\%$</td>
<td>$\mathbf{35.8\%\pm 0.2\%}$</td>
<td>$38.0\%\pm 0.3\%$</td>
<td>$37.3\%\pm 0.3\%$</td>
</tr>
<tr>
<td></td>
<td>IntanceRefer (Ours)</td>
<td>$\mathbf{46.0\%\pm 0.5\%}$</td>
<td>$\mathbf{31.8\%\pm 0.4\%}$</td>
<td>$34.5\%\pm 0.6\%$</td>
<td>$\mathbf{41.9\%\pm 0.4\%}$</td>
<td>$\mathbf{38.8\%\pm 0.4\%}$</td>
</tr>
<tr>
<td>Sr3D</td>
<td>ReferIt3DNet<em>[1]</em></td>
<td>$44.7\%\pm 0.1\%$</td>
<td>$31.5\%\pm 0.4\%$</td>
<td>$39.2\%\pm 1.0\%$</td>
<td>$40.8\%\pm 0.1\%$</td>
<td>$40.8\%\pm 0.2\%$</td>
</tr>
<tr>
<td></td>
<td>TGNN<em>[10]</em></td>
<td>$48.5\%\pm 0.2\%$</td>
<td>$36.9\%\pm 0.5\%$</td>
<td>$\mathbf{45.8\%\pm 1.1\%}$</td>
<td>$45.0\%\pm 0.2\%$</td>
<td>$45.0\%\pm 0.2\%$</td>
</tr>
<tr>
<td></td>
<td>IntanceRefer (Ours)</td>
<td>$\mathbf{51.1\%\pm 0.2\%}$</td>
<td>$\mathbf{40.5\%\pm 0.3\%}$</td>
<td>$45.4\%\pm 0.9\%$</td>
<td>$\mathbf{48.1\%\pm 0.3\%}$</td>
<td>$\mathbf{48.0\%\pm 0.3\%}$</td>
</tr>
</tbody>
</table>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Qualitative results from ScanRefer and our InstanceRefer. Predicted boxes are marked green if they have an IoU score higher than 0.5, otherwise they are marked red. The ground truth boxes are displayed in yellow.</p>
<p>tantly, even if ScanRefer utilizes additional multiview features from the pre-trained ENet<em>[21]</em> on the benchmark, the overall result of ours still achieves $\sim$10% improvement on Acc@0.5. Also, since we employ a pre-trained panoptic segmentation model and stores the segmentation results offline, our method has a much shorter training time and lower memory consumption than others.</p>
<p>The results in Table 2 illustrate the instance-matching accuracy on the Nr3D and Sr3D datasets. Our proposed InstanceRefer achieves top-ranked results on both the Nr3D and Sr3D datasets. The experiments prove that our proposed perception modules are effective components for accurate grounding on the scenes of point clouds and boost the pure instance-matching performances significantly. As a result, our InstanceRefer manifests a stronger capacity than ScanRefer and TGNN on the 3D point cloud VG task.</p>
<p>Visualization results produced by ScanRefer and by our method are displayed in Figure 5. Predicted boxes are marked green if they have an IoU score higher than 0.5, otherwise, they are marked red. Failure cases of ScanRefer illustrate that its architecture cannot distinguish ambiguous objects according to their spatial relations. On the contrary, InstanceRefer can accurately localize the described objects even in the complex scenarios with the long textual descriptions, e.g., the results in the first and second columns show our accurate instance selection and the fifth columns illus-</p>
<p>Table 3: The ablated results for different network architecture on ScanRefer validation set, where Acc@0.5 is used as metrics. Here MAT. means matching module.</p>
<table>
<thead>
<tr>
<th>AP</th>
<th>RP</th>
<th>GLP</th>
<th>MAT.</th>
<th>Unique</th>
<th>Multiple</th>
<th>Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td>✓</td>
<td></td>
<td></td>
<td></td>
<td>66.43</td>
<td>20.32</td>
<td>29.46</td>
</tr>
<tr>
<td></td>
<td>✓</td>
<td></td>
<td></td>
<td>62.66</td>
<td>19.67</td>
<td>28.01</td>
</tr>
<tr>
<td></td>
<td></td>
<td>✓</td>
<td></td>
<td>62.85</td>
<td>16.21</td>
<td>25.29</td>
</tr>
<tr>
<td>✓</td>
<td>✓</td>
<td></td>
<td></td>
<td>66.59</td>
<td>21.56</td>
<td>30.49</td>
</tr>
<tr>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td></td>
<td>66.80</td>
<td>22.18</td>
<td>31.04</td>
</tr>
<tr>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>66.83</td>
<td>24.77</td>
<td>32.93</td>
</tr>
</tbody>
</table>
<p>trates our method can generate bounding box more finely.</p>
<h3>4.4 Comprehensive Analysis</h3>
<p>Ablation Studies. Table 3 presents the effectiveness of different modules. On the one hand, if only a single perception module is employed, the AP module can achieve the best results. On the other hand, when the additional RP and GLP modules are utilized, a remarkable improvement can be seen over the AP module’s result. Specifically, the gain of the RP module is slightly larger than that of the GLP module. The reason is that the descriptions of the correlation among instances in the scene are more commonly compared with the localization. Furthermore, the visual-language matching method we apply is better than simple ranking by cosine similarity.</p>
<p>Instance-matching with Same Backbone. To further illustrate the effectiveness of the proposed modules, we compare instance-matching results when using instances extracted by the same panoptic segmentation backbone or ground truth instances. For the ScanRefer [2], we use PointNet++ [26] to extract features of each instance and replace its proposal features. Also, we evaluate ScanRefer using points in ground truths bounding boxes as input. For the ReferIt3DNet [1], since its original framework is applied on manually segmented instances, we use its original model directly. Both of them are trained using the same strategies of our model for the fair comparison. Since TGNN [10] originally used PointGroup to conduct instance segmentation, we will not discuss it in this section.</p>
<p>The experimental results are summarized in Table 4. From the upper part of Table 4, we can find that panoptic segmentation by PointGroup [11] can boost the performances of ScanRefer, especially on the “unique” case. Besides, ReferIt3DNet achieves similar performances with ScanRefer when using the extracted instances by PointGroup. Note that InstanceRefer still surpasses them by a significant improvement on Acc@0.5, achieving 6.6% in for “unique” and 3.1% for “multiple”. It confirms that our gains not only come from the filtering strategy based on panoptic segmentation but also the well-designed multilevel perception modules. Besides, the improvement using</p>
<p>Table 4: The instance-matching results with same panoptic segmentation backbone on ScanRefer validation set, where Acc@0.5 is used as metrics. ^{∗} denotes using GT language classification.</p>
<table>
<thead>
<tr>
<th>Backbone &amp; Method</th>
<th>Unique</th>
<th>Multiple</th>
<th>Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td>ScanRefer</td>
<td>43.31</td>
<td>19.75</td>
<td>24.32</td>
</tr>
<tr>
<td>PG [11] + ScanRefer</td>
<td>60.05</td>
<td>21.61</td>
<td>29.07</td>
</tr>
<tr>
<td>PG [11] + ReferIt3DNet</td>
<td>60.22</td>
<td>21.41</td>
<td>28.94</td>
</tr>
<tr>
<td>InstanceRefer</td>
<td>66.83</td>
<td>24.77</td>
<td>32.93</td>
</tr>
<tr>
<td>InstanceRefer^{∗}</td>
<td>68.78</td>
<td>24.82</td>
<td>33.35</td>
</tr>
<tr>
<td>GT Box + ScanRefer</td>
<td>73.55</td>
<td>32.00</td>
<td>40.06</td>
</tr>
<tr>
<td>GT Inst + ScanRefer</td>
<td>79.35</td>
<td>36.08</td>
<td>44.48</td>
</tr>
<tr>
<td>GT Inst + ReferIt3DNet</td>
<td>79.04</td>
<td>37.19</td>
<td>45.38</td>
</tr>
<tr>
<td>GT Inst + InstanceRefer</td>
<td>90.24</td>
<td>39.32</td>
<td>49.20</td>
</tr>
</tbody>
</table>
<p>ground-truth target is limited since the language classification is high enough (over 97%). From the bottom part of Table 4, we compare different methods using ground truth instances as input. Experimental results reveal that using GT instance point clouds is better than using points in GT bounding boxes, which partly owes to the interference of occlusion in 3D bounding boxes. Also, InstanceRefer achieves state-of-the-art performances over ReferIt3DNet and ScanRefer, which credits to the instance-matching strategy. In summary, our proposed framework can effectively boost the performances.</p>
<h2>5 Conclusion</h2>
<p>In this paper, we proposed a novel framework, named InstanceRefer, for 3D visual grounding. Our model performs more accurate localization via unifying instance attribute, relation, and localization perceptions. Specifically, InstanceRefer innovatively predicts the target category from linguistic queries and filters out a small number of candidates by panoptic segmentation. Moreover, we propose the concept of cooperative holistic scene-language understanding for each candidate, i.e., multi-level contextual referring to instance attribute, instance-to-instance relation, and instance-to-background global localization. Experimental results demonstrate that InstanceRefer outperforms the previous methods by a large margin. We believe our work formulates a new strategy for 3D visual grounding.</p>
<h2>Acknowledgment</h2>
<p>This work was supported in part by NSFC-Youth 61902335, by Key Area R&amp;D Program of Guangdong Province with grant No.2018B030338001, by the National Key R&amp;D Program of China with grant No.2018YFB1800800, by Shenzhen Outstanding Talents Training Fund, by Guangdong Research Project No.2017ZT07X152, by Guangdong Regional Joint Fund-Key Projects 2019B1515120039, by the NSFC 61931024&amp;81922046, by helixon biotechnology company Fund and CCF-Tencent Open Fund.</p>
<h2>References</h2>
<p>[1] Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed Elhoseiny, and Leonidas Guibas. Referit3d: Neural listeners for fine-grained 3d object identification in real-world scenes. In European Conference on Computer Vision, pages 422-440. Springer, 2020. 2, $6,7,8$
[2] Dave Zhenyu Chen, Angel X Chang, and Matthias Nießner. Scanrefer: 3d object localization in rgb-d scans using natural language. 16th European Conference on Computer Vision (ECCV), 2020. 1, 2, 3, 5, 6, 8
[3] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014. 3
[4] Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In IEEE Conf. Comput. Vis. Pattern Recog., pages 5828-5839, 2017. 6
[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding, 2018. 6
[6] Ankit Goyal, Kaiyu Yang, Dawei Yang, and Jia Deng. Rel3d: A minimally contrastive benchmark for grounding spatial relations in 3d. Advances in Neural Information Processing Systems, 33, 2020. 2
[7] Benjamin Graham, Martin Engelcke, and Laurens Van Der Maaten. 3d semantic segmentation with submanifold sparse convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 9224-9232, 2018. 4
[8] Qingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Yulan Guo, Zhihua Wang, Niki Trigoni, and Andrew Markham. RandLA-Net: Efficient semantic segmentation of large-scale point clouds. IEEE Conf. Comput. Vis. Pattern Recog., pages 11108-11117, 2020. 3
[9] Ronghang Hu, Huazhe Xu, Marcus Rohrbach, Jiashi Feng, Kate Saenko, and Trevor Darrell. Natural language object retrieval. In IEEE Conf. Comput. Vis. Pattern Recog., pages 4555-4564, 2016. 6
[10] Pin-Hao Huang, Han-Hung Lee, Hwann-Tzong Chen, and Tyng-Luh Liu. Text-guided graph neural networks for referring 3d instance segmentation. 35th AAAI Conference on Artificial Intelligence, 2021. 2, 6, 7, 8
[11] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. In IEEE Conf. Comput. Vis. Pattern Recog., pages 4867-4876, 2020. 3, 5, 8
[12] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 787-798, 2014. $1,2,6$
[13] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Dollár. Panoptic segmentation. In IEEE Conf. Comput. Vis. Pattern Recog., pages 9404-9413, 2019. 3
[14] Loic Landrieu and Mohamed Boussaha. Point cloud oversegmentation with graph-structured deep metric learning. In IEEE Conf. Comput. Vis. Pattern Recog., pages 7440-7449, 2019. 3
[15] Loic Landrieu and Martin Simonovsky. Large-scale point cloud semantic segmentation with superpoint graphs. In IEEE Conf. Comput. Vis. Pattern Recog., pages 4558-4567, 2018. 3
[16] Jianan Li, Yunchao Wei, Xiaodan Liang, Fang Zhao, Jianshu Li, Tingfa Xu, and Jiashi Feng. Deep attributepreserving metric learning for natural language object retrieval. In ACM Int. Conf. Multimedia, pages 181189, 2017. 2
[17] Xihui Liu, Zihao Wang, Jing Shao, Xiaogang Wang, and Hongsheng Li. Improving referring expression grounding with cross-modal attention-guided erasing. In IEEE Conf. Comput. Vis. Pattern Recog., pages 1950-1959, 2019. 1
[18] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In IEEE Conf. Comput. Vis. Pattern Recog., pages 11-20, 2016. 1
[19] Aditya Mogadala, Marimuthu Kalimuthu, and Dietrich Klakow. Trends in integration of vision and language research: A survey of tasks, datasets, and methods. arXiv preprint arXiv:1907.09358, 2019. 1
[20] Varun K Nagaraja, Vlad I Morariu, and Larry S Davis. Modeling context between objects for referring expression understanding. In Eur. Conf. Comput. Vis., pages 792-807. Springer, 2016. 2
[21] Adam Paszke, Abhishek Chaurasia, Sangpil Kim, and Eugenio Culurciello. Enet: A deep neural network architecture for real-time semantic segmentation. arXiv preprint arXiv:1606.02147, 2016. 7
[22] Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference</p>
<p>on empirical methods in natural language processing (EMNLP), pages 1532-1543, 2014. 3
[23] Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-tophrase correspondences for richer image-to-sentence models. In Int. Conf. Comput. Vis., pages 2641-2649, 2015. 2
[24] Charles R Qi, Or Litany, Kaiming He, and Leonidas J Guibas. Deep hough voting for 3d object detection in point clouds. In Int. Conf. Comput. Vis., pages 92779286, 2019. 1, 3
[25] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In IEEE Conf. Comput. Vis. Pattern Recog., pages 652-660, 2017. 2
[26] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In Adv. Neural Inform. Process. Syst., pages 5099-5108, 2017. 2, 8
[27] Kihyuk Sohn. Improved deep metric learning with multi-class n-pair loss objective. In Proceedings of the 30th International Conference on Neural Information Processing Systems, pages 1857-1865, 2016. 5
[28] Hugues Thomas, Charles R. Qi, Jean-Emmanuel Deschaud, Beatriz Marcotegui, Francois Goulette, and Leonidas J. Guibas. Kpconv: Flexible and deformable convolution for point clouds. In Int. Conf. Comput. Vis., October 2019. 3
[29] Liwei Wang, Yin Li, Jing Huang, and Svetlana Lazebnik. Learning two-branch neural networks for imagetext matching tasks. IEEE Trans. Pattern Anal. Mach. Intell., 41(2):394-407, 2018. 1, 2
[30] Peng Wang, Qi Wu, Jiewei Cao, Chunhua Shen, Lianli Gao, and Anton van den Hengel. Neighbourhood watch: Referring expression comprehension via language-guided graph attention networks. In IEEE Conf. Comput. Vis. Pattern Recog., pages 1960-1968, 2019. 2
[31] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M Solomon. Dynamic graph cnn for learning on point clouds. ACM Trans. Graph., 38(5):1-12, 2019. 3, 4
[32] Wenxuan Wu, Zhongang Qi, and Li Fuxin. Pointconv: Deep convolutional networks on 3d point clouds. In IEEE Conf. Comput. Vis. Pattern Recog., pages 96219630, 2019. 3
[33] Xu Yan, Chaoda Zheng, Zhen Li, Sheng Wang, and Shuguang Cui. Pointasnl: Robust point clouds processing using nonlocal neural networks with adaptive sampling. 2020. 3
[34] Zhengyuan Yang, Boqing Gong, Liwei Wang, Wenbing Huang, Dong Yu, and Jiebo Luo. A fast and accurate one-stage approach to visual grounding. In Int. Conf. Comput. Vis., pages 4683-4693, 2019. 6
[35] Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, and Tamara L Berg. Mattnet: Modular attention network for referring expression comprehension. In IEEE Conf. Comput. Vis. Pattern Recog., pages 1307-1315, 2018. 2
[36] Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian. Deep modular co-attention networks for visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 6281-6290, 2019. 5
[37] Hengshuang Zhao, Li Jiang, Chi-Wing Fu, and Jiaya Jia. Pointweb: Enhancing local neighborhood features for point cloud processing. In IEEE Conf. Comput. Vis. Pattern Recog., pages 5565-5573, 2019. 3
[38] Kang Zhiheng and Li Ning. Pyramnet: Point cloud pyramid attention network and graph embedding module for classification and segmentation. arXiv preprint arXiv:1906.03299, 2019. 3</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ul>
<li>Corresponding author: Zhen Li. ${ }^{\dagger}$ Equal first authorship.
${ }^{\dagger}$ https://github.com/CurryYuan/InstanceRefer</li>
</ul>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
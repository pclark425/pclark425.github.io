<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6066 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6066</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6066</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-120.html">extraction-schema-120</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-0606bb9a541ce7e57bd78ac680a7df0225ece30c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0606bb9a541ce7e57bd78ac680a7df0225ece30c" target="_blank">Can large language models build causal graphs?</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work evaluates if large language models can be a useful tool in complementing causal graph development and shows that they could be.</p>
                <p><strong>Paper Abstract:</strong> Building causal graphs can be a laborious process. To ensure all relevant causal pathways have been captured, researchers often have to discuss with clinicians and experts while also reviewing extensive relevant medical literature. By encoding common and medical knowledge, large language models (LLMs) represent an opportunity to ease this process by automatically scoring edges (i.e., connections between two variables) in potential graphs. LLMs however have been shown to be brittle to the choice of probing words, context, and prompts that the user employs. In this work, we evaluate if LLMs can be a useful tool in complementing causal graph development.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6066.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6066.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large pre-trained autoregressive language model (Brown et al., 2020) used in this paper to probe corpus-encoded medical/common knowledge by scoring natural-language statements about directed relationships between variable pairs to infer edges in causal DAGs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can Large Language Models Build Causal Graphs?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-3 edge-scoring for DAG construction</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A prompt-based probing method that constructs two natural-language statements per ordered variable pair (one asserting a directed causal edge, one asserting its absence), submits them to GPT-3, and treats the model's statement score/preference as a signal for the presence or absence of that directed edge; experiments vary prompt phrasing, implied authority, linking verbs, and variable specificity.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-3 (Brown et al., 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Four small, hand-specified ground-truth medical DAGs (Alcohol, Cancer, Diabetes, Obesity). For each DAG the method enumerates every ordered variable pair (2 * C(N,2) arrows) and for each pair queries two short natural-language statements (presence vs absence of edge). The paper does not process corpora of scholarly papers directly; inputs are variable names and short statements describing relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Knowledge-probing / binary edge-classification via prompt engineering: for each ordered pair the model scores two natural-language assertions and the higher-scored assertion determines the predicted presence/absence of a directed edge. Variations include prefacing statements with an authority ('According to medical doctors'), changing the linking verb (e.g., 'causes', 'increases risk'), and varying specificity of variable phrasing.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Per-ordered-pair binary decision (edge present vs absent) plus per-DAG aggregated accuracy metrics; tables of accuracy per DAG under different prompt/linking-verb/specificity conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Comparison against four ground-truth DAGs: prediction considered correct if GPT-3's score for the correct statement (presence or absence) exceeded the incorrect one. Primary metric: classification accuracy (proportion of correct ordered-pair predictions). Experiments report accuracy stratified by prompt authority, linking verb, and variable specificity.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>GPT-3 achieved substantially better-than-chance accuracy for at least one prompting setting per DAG. Reported accuracies (selected highlights): Alcohol best 0.83 (prompt 'According to medical doctors'), Cancer best 1.00 (prompts 'medical doctors' or 'medical studies'), Diabetes best 0.67 (baseline/no authority), Obesity best 0.75 (baseline/medical doctors/medical studies). 'Increases risk' linking verb often produced the highest accuracy for 3 of 4 DAGs. Overall finding: performance is sensitive to prompt wording, implied authority, linking verb, and specificity.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Custom evaluation suite of four ground-truth medical DAGs (Alcohol, Cancer, Diabetes, Obesity) created for the experiments; GPT-3 itself is trained on a large internet text corpus (not controlled by the authors). No large scholarly-paper corpora (e.g., PubMed) were ingested or processed in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Model brittleness to prompt wording, linking verb, and variable specificity; dependence on the coverage and timeliness of GPT-3's training corpus (lagging behind new medical literature); potential mismatch because online/casual text uses causal language differently than academic literature; assumption that causal connections of interest are present and well-established in the model's training data; small-scale evaluation (only four DAGs); acyclicity control and full graph-construction not addressed; authors recommend expert verification.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>No head-to-head experimental comparisons to other LLM-based synthesis systems in this paper. The paper cites prior work noting that domain-specific models (e.g., BioBERT) can outperform GPT-3 on medical NLP tasks and references Willig et al. (2022) who compared three query LLMs for causal graph prediction in a general context, but this work does not perform direct benchmarking versus those systems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can large language models build causal graphs?', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6066.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6066.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioBERT (biomedical BERT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-specific pre-trained biomedical language representation model (Lee et al., 2020) mentioned as a promising alternative to generic LLMs for medical NLP tasks and as a candidate for future work to signal edges using medical terminology.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can Large Language Models Build Causal Graphs?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BioBERT (suggested)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A biomedical domain-specific language model suggested by the authors for future experiments to improve edge-signaling for DAG construction by leveraging PubMed/biomedical text rather than the general internet corpus used by GPT-3.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>BioBERT (Lee et al., 2020) — mentioned as a target model for future work, not used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Not used; authors propose pairing a biomedical LLM with medical literature (e.g., PubMed) to better capture medical terminology and evidence — no concrete pipeline, paper count, or corpus size provided.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Proposed (not implemented): use a domain-specific LLM to signal presence/absence of edges in DAGs likely via the same prompt-based edge-scoring approach or via medical-terminology-aware prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Proposed binary edge signals or edge-confidence scores per variable pair (analogous to the GPT-3 setup), not demonstrated.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Not specified in the paper for BioBERT; the authors suggest future evaluation similar to current experiments (accuracy vs ground-truth DAGs) or domain-appropriate benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>No experimental results reported in this paper for BioBERT; paper cites literature stating BioBERT outperforms GPT-3 on some medical-domain NLP tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Authors suggest PubMed as an appropriate medical literature corpus to pair with a domain-specific model like BioBERT; no dataset was actually used here.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Not empirically evaluated in this work; potential limitations implied include need for domain-specific pretraining/fine-tuning and ensuring up-to-date literature coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Paper references prior work indicating that domain-specific LLMs (e.g., BioBERT) can outperform general-purpose GPT-3 on medical NLP tasks, but provides no direct comparison in the DAG experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can large language models build causal graphs?', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6066.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6066.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>web-GPT + PubMed (proposed)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>web-GPT with PubMed-context (proposed)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proposed domain-specific LLM configuration that would condition on or be trained/fine-tuned with PubMed literature to better extract medical causal knowledge for DAG building; mentioned as intended future work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can Large Language Models Build Causal Graphs?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>web-GPT with PubMed (proposed)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Authors propose using web-GPT (or similarly adapted LLMs) restricted to or fine-tuned on PubMed/medical literature to more faithfully signal edges in medical DAGs, leveraging more authoritative and domain-specific text than the general internet.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>web-GPT (conceptual) paired with PubMed — mentioned only as future direction, not implemented.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Not implemented; suggested input would be PubMed-scale biomedical articles rather than free internet text; authors do not specify the number of papers or concrete ingestion pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Proposed application: probe a PubMed-conditioned LLM with edge-assertion statements (or aggregate evidence extraction) to produce signals about causal links between variables; details unspecified.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Hypothesized edge-presence/absence signals or confidence estimates for DAG edges; no outputs demonstrated.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Not specified; authors imply similar evaluation (accuracy vs ground-truth DAGs) and expert verification.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>None reported (proposal only).</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>PubMed is suggested as the corpus to pair with web-GPT; no experiments performed.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Not empirically studied in this work; general concerns from paper apply (training-data timeliness, need for expert verification, prompt sensitivity).</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Suggested as potentially preferable to general-purpose GPT-3 for medical tasks; no experimental comparisons provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can large language models build causal graphs?', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6066.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6066.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Willig et al. (2022)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Can foundation models talk causality? (Willig et al., 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced study that compared the performance of three query LLMs on causal graph prediction in a general (non-medical-specific) context, cited as related work indicating growing interest in using LLMs for causal graph tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can Large Language Models Build Causal Graphs?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Study comparing query LLMs for causal graph prediction</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Reported comparative evaluation of multiple foundation/query LLMs for predicting causal graph structure in general contexts; cited to place current work in context but not detailed or used experimentally here.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Three query LLMs (unspecified in this paper's text) — the Willig et al. paper is cited but detailed model names and settings are not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Not described in this paper; Willig et al. is cited as performing general-context causal graph predictions, likely using their own evaluation setups.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Not detailed here; presumably LLM-based probing for causal relations or structure prediction, but specifics deferred to the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Not specified here; Willig et al. is referenced as related work on LLM causal-graph prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Not reported here — only cited.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Not discussed here beyond general statement that few studies have explored LLM utility for causal diagram development.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>The citation itself is a comparative study; this paper does not reproduce or detail its comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can large language models build causal graphs?', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6066.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6066.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Feder et al. (2021)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Causal inference in natural language processing: Estimation, prediction, interpretation and beyond</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited consolidated exploration (preprint) of causal inference methods situated in NLP, referenced to connect causal inference literature and LLM/NLP research rather than as an implemented system in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can Large Language Models Build Causal Graphs?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Causal inference in NLP (survey/summary)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A literature consolidation and overview discussing causal inference concepts and their interplay with NLP/LLMs; cited as background but not used experimentally.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not applicable (survey paper).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Survey/analysis of causal inference approaches relevant to NLP; not an implemented distillation system for scholarly corpora in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Survey insights and synthesis (not used here).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Not applicable to this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Not applicable in this paper; referenced for context.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Referenced work discusses wider challenges in applying causal inference in NLP; this paper notes that the literature is expanding but gives no experimental details.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Survey compares conceptual approaches in the cited preprint; this paper only cites it as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can large language models build causal graphs?', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6066.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6066.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vig et al. (2020)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Investigating gender bias in language models using causal mediation analysis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited study applying causal inference (causal mediation analysis) to probe and understand bias in LLMs, mentioned as an example of causal methods being applied in the LLM context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can Large Language Models Build Causal Graphs?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Causal mediation analysis of LLMs (example)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An example work applying causal-inference tools to study properties of language models (gender bias), cited to show related lines of research linking causal inference and LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Applied to LLMs in the cited work (not detailed in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Not applicable; cited as causal analysis of model behavior rather than literature distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Not applicable to this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Not described here; referenced as an example of causal-analysis work on LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Cited as part of literature connecting causal inference and LLM research; no details given.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can large language models build causal graphs?', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Biobert: a pre-trained biomedical language representation model for biomedical text mining <em>(Rating: 2)</em></li>
                <li>Can foundation models talk causality? <em>(Rating: 2)</em></li>
                <li>Causal inference in natural language processing: Estimation, prediction, interpretation and beyond <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 1)</em></li>
                <li>Gpt-3 models are poor few-shot learners in the biomedical domain <em>(Rating: 1)</em></li>
                <li>Can large language models reason about medical questions? <em>(Rating: 1)</em></li>
                <li>Investigating gender bias in language models using causal mediation analysis <em>(Rating: 1)</em></li>
                <li>A medical question answering system using large language models and knowledge graphs <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6066",
    "paper_id": "paper-0606bb9a541ce7e57bd78ac680a7df0225ece30c",
    "extraction_schema_id": "extraction-schema-120",
    "extracted_data": [
        {
            "name_short": "GPT-3",
            "name_full": "Generative Pre-trained Transformer 3",
            "brief_description": "A large pre-trained autoregressive language model (Brown et al., 2020) used in this paper to probe corpus-encoded medical/common knowledge by scoring natural-language statements about directed relationships between variable pairs to infer edges in causal DAGs.",
            "citation_title": "Can Large Language Models Build Causal Graphs?",
            "mention_or_use": "use",
            "system_name": "GPT-3 edge-scoring for DAG construction",
            "system_description": "A prompt-based probing method that constructs two natural-language statements per ordered variable pair (one asserting a directed causal edge, one asserting its absence), submits them to GPT-3, and treats the model's statement score/preference as a signal for the presence or absence of that directed edge; experiments vary prompt phrasing, implied authority, linking verbs, and variable specificity.",
            "llm_model_used": "GPT-3 (Brown et al., 2020)",
            "input_type_and_size": "Four small, hand-specified ground-truth medical DAGs (Alcohol, Cancer, Diabetes, Obesity). For each DAG the method enumerates every ordered variable pair (2 * C(N,2) arrows) and for each pair queries two short natural-language statements (presence vs absence of edge). The paper does not process corpora of scholarly papers directly; inputs are variable names and short statements describing relationships.",
            "distillation_approach": "Knowledge-probing / binary edge-classification via prompt engineering: for each ordered pair the model scores two natural-language assertions and the higher-scored assertion determines the predicted presence/absence of a directed edge. Variations include prefacing statements with an authority ('According to medical doctors'), changing the linking verb (e.g., 'causes', 'increases risk'), and varying specificity of variable phrasing.",
            "output_type": "Per-ordered-pair binary decision (edge present vs absent) plus per-DAG aggregated accuracy metrics; tables of accuracy per DAG under different prompt/linking-verb/specificity conditions.",
            "evaluation_methods": "Comparison against four ground-truth DAGs: prediction considered correct if GPT-3's score for the correct statement (presence or absence) exceeded the incorrect one. Primary metric: classification accuracy (proportion of correct ordered-pair predictions). Experiments report accuracy stratified by prompt authority, linking verb, and variable specificity.",
            "results": "GPT-3 achieved substantially better-than-chance accuracy for at least one prompting setting per DAG. Reported accuracies (selected highlights): Alcohol best 0.83 (prompt 'According to medical doctors'), Cancer best 1.00 (prompts 'medical doctors' or 'medical studies'), Diabetes best 0.67 (baseline/no authority), Obesity best 0.75 (baseline/medical doctors/medical studies). 'Increases risk' linking verb often produced the highest accuracy for 3 of 4 DAGs. Overall finding: performance is sensitive to prompt wording, implied authority, linking verb, and specificity.",
            "datasets_or_benchmarks": "Custom evaluation suite of four ground-truth medical DAGs (Alcohol, Cancer, Diabetes, Obesity) created for the experiments; GPT-3 itself is trained on a large internet text corpus (not controlled by the authors). No large scholarly-paper corpora (e.g., PubMed) were ingested or processed in these experiments.",
            "challenges_or_limitations": "Model brittleness to prompt wording, linking verb, and variable specificity; dependence on the coverage and timeliness of GPT-3's training corpus (lagging behind new medical literature); potential mismatch because online/casual text uses causal language differently than academic literature; assumption that causal connections of interest are present and well-established in the model's training data; small-scale evaluation (only four DAGs); acyclicity control and full graph-construction not addressed; authors recommend expert verification.",
            "comparisons_to_other_methods": "No head-to-head experimental comparisons to other LLM-based synthesis systems in this paper. The paper cites prior work noting that domain-specific models (e.g., BioBERT) can outperform GPT-3 on medical NLP tasks and references Willig et al. (2022) who compared three query LLMs for causal graph prediction in a general context, but this work does not perform direct benchmarking versus those systems.",
            "uuid": "e6066.0",
            "source_info": {
                "paper_title": "Can large language models build causal graphs?",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "BioBERT",
            "name_full": "BioBERT (biomedical BERT)",
            "brief_description": "A domain-specific pre-trained biomedical language representation model (Lee et al., 2020) mentioned as a promising alternative to generic LLMs for medical NLP tasks and as a candidate for future work to signal edges using medical terminology.",
            "citation_title": "Can Large Language Models Build Causal Graphs?",
            "mention_or_use": "mention",
            "system_name": "BioBERT (suggested)",
            "system_description": "A biomedical domain-specific language model suggested by the authors for future experiments to improve edge-signaling for DAG construction by leveraging PubMed/biomedical text rather than the general internet corpus used by GPT-3.",
            "llm_model_used": "BioBERT (Lee et al., 2020) — mentioned as a target model for future work, not used in experiments.",
            "input_type_and_size": "Not used; authors propose pairing a biomedical LLM with medical literature (e.g., PubMed) to better capture medical terminology and evidence — no concrete pipeline, paper count, or corpus size provided.",
            "distillation_approach": "Proposed (not implemented): use a domain-specific LLM to signal presence/absence of edges in DAGs likely via the same prompt-based edge-scoring approach or via medical-terminology-aware prompts.",
            "output_type": "Proposed binary edge signals or edge-confidence scores per variable pair (analogous to the GPT-3 setup), not demonstrated.",
            "evaluation_methods": "Not specified in the paper for BioBERT; the authors suggest future evaluation similar to current experiments (accuracy vs ground-truth DAGs) or domain-appropriate benchmarks.",
            "results": "No experimental results reported in this paper for BioBERT; paper cites literature stating BioBERT outperforms GPT-3 on some medical-domain NLP tasks.",
            "datasets_or_benchmarks": "Authors suggest PubMed as an appropriate medical literature corpus to pair with a domain-specific model like BioBERT; no dataset was actually used here.",
            "challenges_or_limitations": "Not empirically evaluated in this work; potential limitations implied include need for domain-specific pretraining/fine-tuning and ensuring up-to-date literature coverage.",
            "comparisons_to_other_methods": "Paper references prior work indicating that domain-specific LLMs (e.g., BioBERT) can outperform general-purpose GPT-3 on medical NLP tasks, but provides no direct comparison in the DAG experiments.",
            "uuid": "e6066.1",
            "source_info": {
                "paper_title": "Can large language models build causal graphs?",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "web-GPT + PubMed (proposed)",
            "name_full": "web-GPT with PubMed-context (proposed)",
            "brief_description": "A proposed domain-specific LLM configuration that would condition on or be trained/fine-tuned with PubMed literature to better extract medical causal knowledge for DAG building; mentioned as intended future work.",
            "citation_title": "Can Large Language Models Build Causal Graphs?",
            "mention_or_use": "mention",
            "system_name": "web-GPT with PubMed (proposed)",
            "system_description": "Authors propose using web-GPT (or similarly adapted LLMs) restricted to or fine-tuned on PubMed/medical literature to more faithfully signal edges in medical DAGs, leveraging more authoritative and domain-specific text than the general internet.",
            "llm_model_used": "web-GPT (conceptual) paired with PubMed — mentioned only as future direction, not implemented.",
            "input_type_and_size": "Not implemented; suggested input would be PubMed-scale biomedical articles rather than free internet text; authors do not specify the number of papers or concrete ingestion pipeline.",
            "distillation_approach": "Proposed application: probe a PubMed-conditioned LLM with edge-assertion statements (or aggregate evidence extraction) to produce signals about causal links between variables; details unspecified.",
            "output_type": "Hypothesized edge-presence/absence signals or confidence estimates for DAG edges; no outputs demonstrated.",
            "evaluation_methods": "Not specified; authors imply similar evaluation (accuracy vs ground-truth DAGs) and expert verification.",
            "results": "None reported (proposal only).",
            "datasets_or_benchmarks": "PubMed is suggested as the corpus to pair with web-GPT; no experiments performed.",
            "challenges_or_limitations": "Not empirically studied in this work; general concerns from paper apply (training-data timeliness, need for expert verification, prompt sensitivity).",
            "comparisons_to_other_methods": "Suggested as potentially preferable to general-purpose GPT-3 for medical tasks; no experimental comparisons provided.",
            "uuid": "e6066.2",
            "source_info": {
                "paper_title": "Can large language models build causal graphs?",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Willig et al. (2022)",
            "name_full": "Can foundation models talk causality? (Willig et al., 2022)",
            "brief_description": "A referenced study that compared the performance of three query LLMs on causal graph prediction in a general (non-medical-specific) context, cited as related work indicating growing interest in using LLMs for causal graph tasks.",
            "citation_title": "Can Large Language Models Build Causal Graphs?",
            "mention_or_use": "mention",
            "system_name": "Study comparing query LLMs for causal graph prediction",
            "system_description": "Reported comparative evaluation of multiple foundation/query LLMs for predicting causal graph structure in general contexts; cited to place current work in context but not detailed or used experimentally here.",
            "llm_model_used": "Three query LLMs (unspecified in this paper's text) — the Willig et al. paper is cited but detailed model names and settings are not provided here.",
            "input_type_and_size": "Not described in this paper; Willig et al. is cited as performing general-context causal graph predictions, likely using their own evaluation setups.",
            "distillation_approach": "Not detailed here; presumably LLM-based probing for causal relations or structure prediction, but specifics deferred to the cited work.",
            "output_type": "Not specified in this paper.",
            "evaluation_methods": "Not specified here; Willig et al. is referenced as related work on LLM causal-graph prediction.",
            "results": "Not reported here — only cited.",
            "datasets_or_benchmarks": "Not specified in this paper.",
            "challenges_or_limitations": "Not discussed here beyond general statement that few studies have explored LLM utility for causal diagram development.",
            "comparisons_to_other_methods": "The citation itself is a comparative study; this paper does not reproduce or detail its comparisons.",
            "uuid": "e6066.3",
            "source_info": {
                "paper_title": "Can large language models build causal graphs?",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Feder et al. (2021)",
            "name_full": "Causal inference in natural language processing: Estimation, prediction, interpretation and beyond",
            "brief_description": "A cited consolidated exploration (preprint) of causal inference methods situated in NLP, referenced to connect causal inference literature and LLM/NLP research rather than as an implemented system in this paper.",
            "citation_title": "Can Large Language Models Build Causal Graphs?",
            "mention_or_use": "mention",
            "system_name": "Causal inference in NLP (survey/summary)",
            "system_description": "A literature consolidation and overview discussing causal inference concepts and their interplay with NLP/LLMs; cited as background but not used experimentally.",
            "llm_model_used": "Not applicable (survey paper).",
            "input_type_and_size": "Not applicable.",
            "distillation_approach": "Survey/analysis of causal inference approaches relevant to NLP; not an implemented distillation system for scholarly corpora in this paper.",
            "output_type": "Survey insights and synthesis (not used here).",
            "evaluation_methods": "Not applicable to this paper's experiments.",
            "results": "Not applicable in this paper; referenced for context.",
            "datasets_or_benchmarks": "Not applicable.",
            "challenges_or_limitations": "Referenced work discusses wider challenges in applying causal inference in NLP; this paper notes that the literature is expanding but gives no experimental details.",
            "comparisons_to_other_methods": "Survey compares conceptual approaches in the cited preprint; this paper only cites it as related work.",
            "uuid": "e6066.4",
            "source_info": {
                "paper_title": "Can large language models build causal graphs?",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Vig et al. (2020)",
            "name_full": "Investigating gender bias in language models using causal mediation analysis",
            "brief_description": "A cited study applying causal inference (causal mediation analysis) to probe and understand bias in LLMs, mentioned as an example of causal methods being applied in the LLM context.",
            "citation_title": "Can Large Language Models Build Causal Graphs?",
            "mention_or_use": "mention",
            "system_name": "Causal mediation analysis of LLMs (example)",
            "system_description": "An example work applying causal-inference tools to study properties of language models (gender bias), cited to show related lines of research linking causal inference and LLMs.",
            "llm_model_used": "Applied to LLMs in the cited work (not detailed in this paper).",
            "input_type_and_size": "Not specified here.",
            "distillation_approach": "Not applicable; cited as causal analysis of model behavior rather than literature distillation.",
            "output_type": "Not applicable to this paper's experiments.",
            "evaluation_methods": "Not described here; referenced as an example of causal-analysis work on LLMs.",
            "results": "Not reported in this paper.",
            "datasets_or_benchmarks": "Not specified here.",
            "challenges_or_limitations": "Cited as part of literature connecting causal inference and LLM research; no details given.",
            "comparisons_to_other_methods": "Not discussed here.",
            "uuid": "e6066.5",
            "source_info": {
                "paper_title": "Can large language models build causal graphs?",
                "publication_date_yy_mm": "2023-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2
        },
        {
            "paper_title": "Biobert: a pre-trained biomedical language representation model for biomedical text mining",
            "rating": 2
        },
        {
            "paper_title": "Can foundation models talk causality?",
            "rating": 2
        },
        {
            "paper_title": "Causal inference in natural language processing: Estimation, prediction, interpretation and beyond",
            "rating": 2
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 1
        },
        {
            "paper_title": "Gpt-3 models are poor few-shot learners in the biomedical domain",
            "rating": 1
        },
        {
            "paper_title": "Can large language models reason about medical questions?",
            "rating": 1
        },
        {
            "paper_title": "Investigating gender bias in language models using causal mediation analysis",
            "rating": 1
        },
        {
            "paper_title": "A medical question answering system using large language models and knowledge graphs",
            "rating": 1
        }
    ],
    "cost": 0.01300625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Can Large Language Models Build Causal Graphs?</h1>
<p>Stephanie Long*<br>Dept. of Family Medicine, McGill University</p>
<p>Tibor Schuster<br>Dept. of Family Medicine, McGill University</p>
<p>Alexandre Piché<br>Mila, Université de Montréal<br>ServiceNow Research</p>
<h4>Abstract</h4>
<p>Building causal graphs can be a laborious process. To ensure all relevant causal pathways have been captured, researchers often have to discuss with clinicians and experts while also reviewing extensive relevant medical literature. By encoding common and medical knowledge, large language models (LLMs) represent an opportunity to ease this process by automatically scoring edges (i.e., connections between two variables) in potential graphs. LLMs however have been shown to be brittle to the choice of probing words, context, and prompts that the user employs. In this work, we evaluate if LLMs can be a useful tool in complementing causal graph development.</p>
<h2>1 Introduction</h2>
<p>Advances in causal inference have important implications in empirical research as most research questions asked in the health and medical context are not associational, but causal in nature. Examples of such research questions include: What is the efficacy of a given drug in a given population? What is the expected effect of a given intervention on a specific outcome? Common amongst these research questions is the desire to uncover the cause-and-effect relationships amongst a set of variables i.e., treatments, interventions, and outcomes. Such causal questions cannot be answered from (observed) data alone or from the distributions that govern said data [Pearl, 2009]. In addition, external knowledge is needed to understand the underlying data-generating mechanisms to enable the setup of an appropriate 'inference engine'.
Causal diagrams play a central role in causal inference because they encode contextual knowledge of the observable and unobservable variables, and their causal dependencies. Causal inference pioneer Judea Pearl refers to the nodes in a causal diagram as a "society of listening variables" [Pearl, 2017]. The term "listening" stresses the defining property of directed and acyclic relationships between the variables, i.e., listening being asymmetrical, variable A listening to variable B, does not imply variable B listening to variable A, motivating the commonly adapted nomenclature of Directed Acyclic Graphs (DAGs) [Greenland et al., 1999, Greenland and Pearl, 2006].
The first step when aiming to address causal questions using data is to draw a causal diagram e.g., a causal DAG. However, with the growing complexity and depth of health and medical knowledge being generated and increasing availability of new research articles daily, research databases are reaching dimensions that limit the possibility of parsing through the enormity of evidence needed to craft comprehensive DAGs [Raghupathi, 2014]. Though expert opinion is the most valuable tool for drawing DAGs, experts do not always generate perfect DAGs, sometimes missing important confounding pathways [Oates et al., 2017]. Additionally, obtaining the opinions of numerous experts is costly both in time and resources. Thus, the ongoing developments of Large Language Models (LLM) may offer promise to help overcome some of these challenges by leveraging existing text data that may express causal sentiments (e.g., "X causes Y").
This research aims to answer the question, "Can large language models help researchers build causal diagrams in the medical context using existing text data?" Here we will conduct experiments to</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of the evaluation. To predict the structure of a given causal graph, for every ordered variable pair, we scored two statements using GPT-3, where the first statement implied the presence of an arrow and the second implied the absence of an arrow. GTP-3 was accurate if the correct statement had a higher accuracy score than the incorrect statement. For example, GPT-3 would be accurate if the statement implying the presence (or absence) of an arrow had a higher accuracy than the incorrect statement and the arrow was present (or absent) in the true DAG.
determine under what conditions (e.g., prompt engineering, use of alternative language) GPT-3 [Brown et al., 2020] is able to provide accurate answers regarding the relationship between variables in a medical context and what are its limitations in doing so.</p>
<p>The main contributions of this paper are:</p>
<ul>
<li>Determining whether GPT-3 can signal the presence or absence of an edge between two variables in a directed acyclic graph from the medical context.</li>
<li>Evaluating whether the use of certain language in prompts or linking verbs improves the classification accuracy of GPT-3.</li>
<li>Exploring the limitations of GPT-3 in understanding the causal relationships between variables in the medical context.</li>
</ul>
<h1>2 Background</h1>
<h3>2.1 Large language models</h3>
<p>Large language models capture non-trivial relationships and knowledge about the datasets they have been trained upon. This knowledge has the possibility to unlock numerous applications in healthcare such as summarizing research papers, assessing patient risks from subjective symptoms, and diagnosing patients from clinical notes.
Although LLMs perform well on general natural language processing (NLP) tasks, its performance has been shown to be sensitive to its prompt [Moradi et al., 2021, Gutiérrez et al., 2022]. The advent of prompt-based learning introduced a possible solution to context sensitive text, by querying LLMs with a prompt that uses in-domain examples or task descriptions [Liu et al., 2021]. For example, chain-of-thought prompts such as "Let's take this step by step" have been shown to trigger multi-step reasoning in solving arithmetic problems [Kojima et al., 2022]. Such prompts have also been shown to significantly improve performance in reasoning about medical questions [Liévin et al., 2022].
Large language models are also sensitive to the type of text data they are trained on. For instance, GPT-3 [Brown et al., 2020] was trained on the corpus of text information on the internet. As one can imagine, the entirety of the internet would include a range of text data from lay and casual use of language on social media to more formal language in news articles. These differences in writing styles may influence the frequency of the use of causal language describing non-causal relationships. For instance, an individual writing a social media post may use the word 'cause' more lightly than medical researchers in medical journals.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Ground Truth DAGs. Four DAGs illustrating well-known exposure-outcome effects in the medical literature. DAG (A) represents the simplest DAG evaluated by GPT-3. DAGs (B-D) represent more complex structures involving a collider variable (node with two arrows pointing into it e.g., 'respiratory disease', 'body weight', and 'heart failure') with a common cause with the outcome.</p>
<h1>2.2 Causal diagram overview</h1>
<p>Causal models are typically accompanied by graphical representations i.e., Directed Acyclic Graphs (DAGs) which are acyclic graphs that succinctly illustrate the qualitative assumptions made by the models, not captured by conventional statistical models or machine learning algorithms [Greenland and Brumback, 2002, Greenland et al., 1999].
In epidemiological research, DAGs have a variety of purposes including: (1) representing the causal relationships amongst variables [Greenland and Brumback, 2002, Greenland and Pearl, 2006, Pearl, 1995]; (2) identifying the potential confounding variables which need to be controlled for in order to estimate causal effects [Greenland and Pearl, 2006, Pearl, 1995, Robins, 2001, Hernán et al., 2002]; and more recently (3) as a means of classifying the types of causal relationships that may give rise to selection bias [Hernán et al., 2004].
A DAG is composed of variables (nodes), both measured and unmeasured, and their connections are displayed via line segments (directed edges) [Greenland et al., 1999, Hernán et al., 2004]. The absence of an arrow between variables indicates the lack of a direct relationship between the variables. If the edge has an arrowhead, the variable at the tail is the parent node and the variable at the arrowhead is the child node [Greenland and Pearl, 2006]. An edge or arc is any line (with an arrowhead or not) that connects two variables [Greenland and Brumback, 2002]. The main characteristics of DAGs are that they are: (1) directed i.e., the edge has a defined direction (arrowhead), and (2) acyclic i.e., lack of cycles or loops within the graph.
A DAG is causal if: (1) the arrows between variables can be interpreted as direct causal effects, and (2) all common causes of any pair of variables are present [Hernán et al., 2004]. The causal effects are 'direct' relative to certain degrees of abstraction in that the DAG does not include any variables that may mediate the effect [Greenland and Pearl, 2006]. As the name suggests, DAGs are acyclic because a variable cannot be the cause of itself, either directly or indirectly through another variable i.e., there are no feedback loops; as illustrated by each DAG in Figure 2 [Hernán et al., 2004]. Additionally, in DAGs, causal pathways are represented with directed paths from the starting variable to the final variable; thus, a variable is the cause of its descendants and an effect of its ancestors [Greenland and Pearl, 2006].</p>
<h1>3 Experiments</h1>
<h3>3.1 Experimental details</h3>
<p>To empirically assess the potential effectiveness of LLMs in building DAGs, we used four DAGs representing well-known exposure-outcome relationships in the medical literature (Figure 2) as the Ground Truth. These DAGs are varied in complexity, amount of variables, and reflect different medical contexts. For a DAG of $N$ variables, there are $\binom{N}{2}$ possible edges between two variables, and there are twice this amount of possible arrows since the arrows are directed. For example, a DAG of 4 variables has $2 \times\binom{4}{2}=12$ possible arrows.
For each DAG, we looped through every ordered variable pair, and asked GPT-3 to score two statements per pair: (1) one implying the presence of a directed edge from variable 1 to variable 2 , and (2) one implying absence of a directed edge from variable 1 to variable 2 . The presence or absence of an edge between two variables is a binary decision (Yes / No), thus, we defined the prediction as accurate, if GPT-3 scored the correct statement higher than the incorrect one. We reported the accuracy or the proportion of correct predictions of our model.</p>
<h3>3.2 Results</h3>
<p>Q1: Does using prompt engineering lead to more accurate answers? We investigated if the prediction accuracy of GPT-3 could be improved by prompting the statements with a reference to a medical authority. For example,
"According to X, var1 increases the risk of developing var2",
instead of
"Var1 increases the risk of developing var2" (baseline),
where X is an individual or entity with medical authority or expertise, e.g., medical doctors, medical studies, or "Big Pharma". These prompts were chosen as they vary in their credibility with the public. We found that in 2 cases (Diabetes and Obesity DAGs) prompt engineering did not help and baseline (no prompting individual or authority) outperformed all other prompts. While in 2 other cases, the "According to medical doctors," prompting significantly improved the accuracy of GPT-3. Interestingly, conditioning on "According to Big Pharma," decreases the accuracy of 3 of the 4 DAGs compared to the baseline. Furthermore, prompting the model on medical studies or medical doctors resulted in different results for half the DAGs. See Table 1 for all results.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">DAG name</th>
<th style="text-align: left;">Prompt</th>
<th style="text-align: center;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Alcohol</td>
<td style="text-align: left;">Baseline</td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Big Pharma</td>
<td style="text-align: center;">0.50</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Medical doctors</td>
<td style="text-align: center;">$\mathbf{0 . 8 3}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Medical studies</td>
<td style="text-align: center;">0.67</td>
</tr>
<tr>
<td style="text-align: left;">Cancer</td>
<td style="text-align: left;">Baseline</td>
<td style="text-align: center;">0.75</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Big Pharma</td>
<td style="text-align: center;">0.58</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Medical doctors</td>
<td style="text-align: center;">$\mathbf{1 . 0 0}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Medical studies</td>
<td style="text-align: center;">$\mathbf{1 . 0 0}$</td>
</tr>
<tr>
<td style="text-align: left;">Diabetes</td>
<td style="text-align: left;">Baseline</td>
<td style="text-align: center;">$\mathbf{0 . 6 7}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Big Pharma</td>
<td style="text-align: center;">0.50</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Medical doctors</td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Medical studies</td>
<td style="text-align: center;">0.42</td>
</tr>
<tr>
<td style="text-align: left;">Obesity</td>
<td style="text-align: left;">Baseline</td>
<td style="text-align: center;">$\mathbf{0 . 7 5}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Big Pharma</td>
<td style="text-align: center;">0.58</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Medical doctors</td>
<td style="text-align: center;">$\mathbf{0 . 7 5}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Medical studies</td>
<td style="text-align: center;">$\mathbf{0 . 7 5}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Prompt engineering: The medical authority used to prompt the statement.</p>
<p>Q2: Does the verb used to denote the relationship between the variables have an impact on accuracy? For instance, "Variable 1 X Variable 2" where $X$ represents the verb (or phrase) that denotes the relationship between the variables, e.g., "causes" or "increases the risk".
Our results demonstrated that while no verb consistently improved classification accuracy, the choice of verb linking the two variables of interest influenced accuracy. 'Increases risk' had the highest accuracy for three of the four DAGs. Though it did not achieve the highest accuracy in the Alcohol DAG. Overall, the use of 'cause' yielded decent results for all DAGs. Results are reported in Table 2.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">DAG name</th>
<th style="text-align: left;">Linking Verb</th>
<th style="text-align: center;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Alcohol</td>
<td style="text-align: left;">Cause</td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Increases likelihood</td>
<td style="text-align: center;">$\mathbf{0 . 5 0}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Increases risk</td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: left;">Cancer</td>
<td style="text-align: left;">Cause</td>
<td style="text-align: center;">0.58</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Increases likelihood</td>
<td style="text-align: center;">0.58</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Increases risk</td>
<td style="text-align: center;">$\mathbf{0 . 7 5}$</td>
</tr>
<tr>
<td style="text-align: left;">Diabetes</td>
<td style="text-align: left;">Cause</td>
<td style="text-align: center;">0.58</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Increases likelihood</td>
<td style="text-align: center;">0.42</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Increases risk</td>
<td style="text-align: center;">$\mathbf{0 . 6 7}$</td>
</tr>
<tr>
<td style="text-align: left;">Obesity</td>
<td style="text-align: left;">Cause</td>
<td style="text-align: center;">0.58</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Increases likelihood</td>
<td style="text-align: center;">0.42</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Increases risk</td>
<td style="text-align: center;">$\mathbf{0 . 7 5}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Linking verb: The verb or phrase used to link the two variables of interest.</p>
<p>Q3: Does specificity in language improve accuracy? We investigated if making our statements more specific or descriptive improved GPT-3's accuracy.
Unsurprisingly, rephrasing the "alcohol" variable to "excessive alcohol consumption" increased the accuracy of GPT-3 on the Alcohol DAG. However, being more specific about the number of cigarettes being smoked and using a clinical term to qualify obesity resulted in worse accuracy for the Cancer and Obesity DAGs. Overall, In this analysis, more specific statements did not increase the accuracy and often resulted in worse accuracy for different linking verbs. Results are reported in Table 3.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">DAG name</th>
<th style="text-align: left;">Variable Name</th>
<th style="text-align: left;">Linking Verb</th>
<th style="text-align: center;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Alcohol</td>
<td style="text-align: left;">Alcohol</td>
<td style="text-align: left;">Cause</td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">Increases risk</td>
<td style="text-align: center;">0.50</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Excessive alcohol consumption</td>
<td style="text-align: left;">Cause</td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">Increases risk</td>
<td style="text-align: center;">$\mathbf{0 . 6 7}$</td>
</tr>
<tr>
<td style="text-align: left;">Cancer</td>
<td style="text-align: left;">Cigarette smoking</td>
<td style="text-align: left;">Cause</td>
<td style="text-align: center;">0.58</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">Increases risk</td>
<td style="text-align: center;">$\mathbf{0 . 6 7}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Smoking 100 cigarettes a day</td>
<td style="text-align: left;">Cause</td>
<td style="text-align: center;">0.50</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">Increases risk</td>
<td style="text-align: center;">0.58</td>
</tr>
<tr>
<td style="text-align: left;">Obesity</td>
<td style="text-align: left;">Obesity</td>
<td style="text-align: left;">Cause</td>
<td style="text-align: center;">0.58</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">Increases risk</td>
<td style="text-align: center;">$\mathbf{0 . 6 7}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Excessive fat accumulation</td>
<td style="text-align: left;">Cause</td>
<td style="text-align: center;">0.58</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">Increases Risk</td>
<td style="text-align: center;">0.58</td>
</tr>
</tbody>
</table>
<p>Table 3: Specificity: More extensive descriptions of variables/concepts.</p>
<h1>4 Discussion</h1>
<p>In this work, we explored if LLMs could be used to complement and speed up the workflow of researchers by automatically scoring edges in potential DAGs. For the relatively simple and wellstudied DAGs that we tested GPT-3 on, the results were overall encouraging as the performance</p>
<p>reached much higher than $50 \%$ accuracy (random guessing) on all DAGs for at least one of the tested settings (e.g., prompt or linking verb). In this analysis, we found that GPT-3's accuracy performance was influenced by different prompts and linking verbs between variables of interest.</p>
<p>To the best of our knowledge, this is the first study to examine using LLM for causal diagram development in the medical context. Though there is growing interest, to date, there are few studies exploring the utility of LLM in causal diagram development. A recent study by Willig et al. [2022] compared the performance of three query LLMs in making causal graph predictions in a general context. There also has been some interesting works applying causal inference in the LLM context. For instance, Vig et al. [2020] investigated gender bias present in LLM using causal mediation analysis. Feder et al. [2021] released a preprint of a consolidated exploration of causal inference situated in NLP. These works suggest more focus is being devoted to researching how causal inference can be applied to LLMs and NLP.</p>
<p>Furthermore, there has been some research investigating LLM's ability to answer and reason with medical text data. Several recent studies [Liévin et al., 2022, Guo et al., 2022] showed promising results on LLMs ability to answer medical exam questions. Others [Moradi et al., 2021, Gutiérrez et al., 2022] have shown that context-specific LLMs such as BioBert are able to outperform GPT-3 in medical domain NLP tasks.</p>
<p>Limitations This study has some limitations. First, it must be acknowledged that the updating of LLMs, themselves as well as the data they are trained upon, lags behind the availability of new medical literature, and, thus may not be useful for informing the building of DAGs for novel diseases. Additionally, GPT-3 was trained upon the corpus of text data uploaded to the internet. The language used on the broader internet is likely more casual with the use of causal language than the medical academic literature [Haber et al., 2022]. Lastly, the way in which we probed GPT-3's ability to draw an edge between variables assumes that the causal connections between variables would be well-established in the corpus of text data.</p>
<p>Future work Future work aims to use a medical language context-specific LLM such as web-GPT with PubMed or BioBert [Lee et al., 2020] to signal the presence or absence of edges in DAGs using medical terminology. Additionally, since our preliminary evaluations only examined the presence/absence of arrows and their direction, upcoming projects will be focused on controlling for acyclicity amongst variables, another important characteristic of DAGs.</p>
<h1>5 Conclusion</h1>
<p>Our results illustrate that GPT-3's level of accuracy in confirming an edge connecting two variables in a DAG depends on the language used to describe the relationship. Presently, expert opinion is the most valuable tool for constructing DAGs; however, like LLMs, experts are not exempt from making errors resulting in imperfect or erroneous DAGs via omission of important confounder variables [Oates et al., 2017]. These imperfections highlight that the use of LLMs to build DAGs should be, at present, only conducted with expert verification. We see LLMs providing utility in extracting common knowledge from medical text which when paired with expert knowledge may present a more efficient means to generate comprehensive DAGs.</p>
<p>Large Language Models represent an exciting opportunity to extract common knowledge from the medical literature to complement and speed up DAG creation, but further research must be done to address the limitations reported above.</p>
<h1>References</h1>
<p>T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
A. Feder, K. A. Keith, E. Manzoor, R. Pryzant, D. Sridhar, Z. Wood-Doughty, J. Eisenstein, J. Grimmer, R. Reichart, M. E. Roberts, et al. Causal inference in natural language processing: Estimation, prediction, interpretation and beyond. arXiv preprint arXiv:2109.00725, 2021.
S. Greenland and B. Brumback. An overview of relations among causal modelling methods. International journal of epidemiology, 31(5):1030-1037, 2002.
S. Greenland and J. Pearl. Causal diagrams. Encyclopedia of Epidemiology, 2006.
S. Greenland, J. Pearl, and J. M. Robins. Causal diagrams for epidemiologic research. Epidemiology, pages $37-48,1999$.
Q. Guo, S. Cao, and Z. Yi. A medical question answering system using large language models and knowledge graphs. International Journal of Intelligent Systems, 37(11):8548-8564, 2022. doi: https://doi.org/10.1002/int. 22955.
B. J. Gutiérrez, N. McNeal, C. Washington, Y. Chen, L. Li, H. Sun, and Y. Su. Thinking about gpt-3 in-context learning for biomedical ie? think again. arXiv preprint arXiv:2203.08410, 2022.
N. Haber, S. Wieten, J. Rohrer, O. Arah, P. Tennant, E. Stuart, E. Murray, S. Pilleron, S. Lam, E. Riederer, et al. Causal and associational language in observational health research: a systematic evaluation. American Journal of Epidemiology, 2022.
M. A. Hernán, S. Hernández-Díaz, M. M. Werler, and A. A. Mitchell. Causal knowledge as a prerequisite for confounding evaluation: an application to birth defects epidemiology. American journal of epidemiology, 155(2):176-184, 2002.
M. A. Hernán, S. Hernández-Díaz, and J. M. Robins. A structural approach to selection bias. Epidemiology, pages 615-625, 2004.
T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.
J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang. Biobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4):1234-1240, 2020.
V. Liévin, C. E. Hother, and O. Winther. Can large language models reason about medical questions? arXiv preprint arXiv:2207.08143, 2022.
P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. arXiv preprint arXiv:2107.13586, 2021.
M. Moradi, K. Blagec, F. Haberl, and M. Samwald. Gpt-3 models are poor few-shot learners in the biomedical domain. arXiv preprint arXiv:2109.02555, 2021.
C. Oates, J. Kasza, J. Simpson, and A. Forbes. Repair of partly misspecified causal diagrams. Epidemiology, 28, 2017.
J. Pearl. Causal diagrams for empirical research. Biometrika, 82(4):669-688, 1995.
J. Pearl. Causal inference in statistics: An overview. Statistics Surveys, 2009.
J. Pearl. The eight pillars of causal wisdom. UCLA, 2017.
V. Raghupathi, W.; Raghupathi. Big data analytics in healthcare- promise and potential. Health Information Science and Systems, 2, 2014.
J. M. Robins. Data, design, and background knowledge in etiologic inference. Epidemiology, pages $313-320,2001$.</p>
<p>J. Vig, S. Gehrmann, Y. Belinkov, S. Qian, D. Nevo, Y. Singer, and S. Shieber. Investigating gender bias in language models using causal mediation analysis. Advances in Neural Information Processing Systems, 33:12388-12401, 2020.
M. Willig, M. Zečević, D. S. Dhami, and K. Kersting. Can foundation models talk causality? arXiv preprint arXiv:2206.10591, 2022.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*stephanie.long@mail.mcgill.ca&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
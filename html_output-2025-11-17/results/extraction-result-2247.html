<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2247 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2247</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2247</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-62.html">extraction-schema-62</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <p><strong>Paper ID:</strong> paper-276937863</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.08751v2.pdf" target="_blank">Disentangled World Models: Learning to Transfer Semantic Knowledge from Distracting Videos for Reinforcement Learning</a></p>
                <p><strong>Paper Abstract:</strong> Training visual reinforcement learning (RL) in practical scenarios presents a significant challenge, $\textit{i.e.,}$ RL agents suffer from low sample efficiency in environments with variations. While various approaches have attempted to alleviate this issue by disentangled representation learning, these methods usually start learning from scratch without prior knowledge of the world. This paper, in contrast, tries to learn and understand underlying semantic variations from distracting videos via offline-to-online latent distillation and flexible disentanglement constraints. To enable effective cross-domain semantic knowledge transfer, we introduce an interpretable model-based RL framework, dubbed Disentangled World Models (DisWM). Specifically, we pretrain the action-free video prediction model offline with disentanglement regularization to extract semantic knowledge from distracting videos. The disentanglement capability of the pretrained model is then transferred to the world model through latent distillation. For finetuning in the online environment, we exploit the knowledge from the pretrained model and introduce a disentanglement constraint to the world model. During the adaptation phase, the incorporation of actions and rewards from online environment interactions enriches the diversity of the data, which in turn strengthens the disentangled representation learning. Experimental results validate the superiority of our approach on various benchmarks.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2247.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2247.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DisWM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Disentangled World Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based visual RL framework that pretrains an action-free β-VAE video predictor on distracting videos to learn disentangled semantic latents, then transfers that disentanglement to an action-conditioned world model via offline-to-online latent distillation and disentanglement constraints during online finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Disentangled World Models (DisWM)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>latent-level semantic / disentangled factors (learned low-dimensional latent space with generative reconstruction)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>implicit via disentanglement regularization (β-VAE KL to isotropic Gaussian, β-weighted KL terms) and KL-based latent distillation from a pretrained disentangled encoder; no explicit attention or masking</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Continuous-control visual RL (DeepMind Control Suite (DMC) tasks, MuJoCo Pusher manipulation, DrawerWorld texture transfer)</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>Yes — explicit visual distractors used in pretraining and finetuning: color perturbations, texture variations, and other 'distracting videos' assembled from varied domains (e.g., DMC frames as distractors for MuJoCo downstream).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative/aggregate: reports improved sample efficiency and higher episode return curves compared to baselines (DreamerV2, APV, DV2 Finetune, TED, CURL); specific numeric returns or percentages are not reported in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Reported: each run uses ~5 GB VRAM and takes ~16 hours on a single NVIDIA RTX 3090 for training to reported evaluation point; training up to 1e6 environment steps for DMC tasks; pretrained distracting-video dataset built with ~1M frames; latent dimension for both pretrained z_disen and downstream z_task = 20; distillation hyperparameter η annealed from 0.1 to 0.01.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Qualitative: DisWM outperforms TED, CURL, APV and DreamerV2 baselines on sample-efficiency and final return in the reported benchmarks; DV2 Finetune is second-best overall but suffers in large domain-shift cases (e.g., DMC→MuJoCo) where DisWM degrades less. No exact numeric gaps provided in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Evaluated cross-domain transfer from action-free distracting videos to downstream tasks; latent distillation transfers disentanglement capability and improves downstream sample efficiency versus direct finetuning (DV2 Finetune) and APV-style approaches. Quantitative transfer metrics (e.g., relative % improvement) are not provided in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Evaluated across multiple tasks (DMC: Walker Walk, Cheetah Run, Hopper Stand, Finger Spin, Cartpole Swingup; MuJoCo Pusher; DrawerWorld textures). Uses a shared pretrained disentangled encoder and then task-specific world-model finetuning; results reported as per-task learning curves showing consistent improvements, but no per-task numeric table in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Authors report limitations: disentangled representation learning can struggle in more complex or non-stationary environments (e.g., time-varying background videos); large domain mismatches can cause overwriting of pretrained features if distillation is not controlled.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Ablations performed on presence/absence of latent distillation and disentanglement constraints showing both components contribute positively to performance; quantitative ablation numbers are referenced but not enumerated in the main text (details likely in supplementary).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Claim: improves sample efficiency relative to baselines (learning faster within 1e6 environment steps on DMC); no explicit numeric samples-to-threshold reported in main body.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Cross-domain generalization evaluated (e.g., pretraining on DMC distracting videos and transferring to MuJoCo), showing robustness to visual and domain variations; t-SNE visualization indicates online interaction latents are more diverse than offline pretraining latents. No numeric OOD generalization metrics given.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Model includes image reconstruction losses (negative log-likelihood term) as part of training objective, but no PSNR/SSIM/MSE values are reported in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Paper motivates separation of task-relevant vs task-irrelevant features via disentanglement: pretrained z_disen captures semantic factors so that changes (e.g., color) only affect a small subset of latents; qualitative latent traversals (Fig.5/6) demonstrate interpretable factors but no formal task-relevance attribution scores are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>Partially — the adaptation process anneals the distillation weight η (0.1 → 0.01) to progressively reduce reliance on pretrained latents, enabling dynamic adjustment of contributed prior knowledge during finetuning; no automatic switching between abstraction levels beyond this annealing is described.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Discussed qualitatively: incorporation of actions and rewards during online finetuning enriches sample diversity and strengthens disentanglement, implying benefits during exploration (more diverse imagined/seen states); no explicit separate experiments comparing abstraction levels in exploration vs exploitation phases.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>Uses KL divergence terms (β-VAE KL to isotropic Gaussian for disentanglement; KL between pretrained and task latent distributions for distillation). No explicit mutual-information estimates, compression bounds, or formal rate–distortion curves are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td>The model retains pixel-reconstruction objective (image log-likelihood) during both pretraining and adaptation. The paper argues that reconstruction is necessary to train the generative latent model but does not provide experiments isolating when pixel fidelity specifically benefits downstream task performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2247.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2247.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>β-VAE pretrain</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>β-Variational Autoencoder action-free video prediction (pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An action-free video prediction model using a β-VAE encoder/decoder and a latent prior trained on distracting videos to learn disentangled latent factors (z_disen) via β-weighted KL regularization and an action-free KL prior on predicted latents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>beta-vae: Learning basic visual concepts with a constrained variational framework</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>β-VAE-based action-free video predictor</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>latent-level disentangled semantic factors (trained to reconstruct pixels but regularized for factorization)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>Disentanglement enforced via β2 KL term encouraging posterior q(z|o) to match isotropic Gaussian and an action-free KL between posterior and prior from historical latents; thus selection is implicit via regularization rather than supervised relevance signals.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Pretraining domain: distracting videos sampled from DMC interactions with color distractors (1M frames dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>Yes — trained specifically on 'distracting videos' containing visual variations (color perturbations, textures) to encourage robust disentanglement of semantic factors from irrelevant appearance changes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No numeric reconstruction or disentanglement metrics (e.g., MIG, DCI) reported in main text; qualitative latent traversals shown (Fig.5) to indicate successful disentanglement.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Trained offline on 1M-frame distracting-video dataset; latent dimension reported as 20; no FLOPs or parameter counts provided specifically for the β-VAE component.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Used as pretrained prior in DisWM; authors argue it yields more interpretable orthogonal latent spaces than training from scratch, leading to improved downstream sample efficiency compared to baselines that lack this pretraining. No direct numerical head-to-head on pretraining alone provided.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Pretrained disentangled latents are distilled into downstream world model via KL minimization, claimed to transfer disentangling capability cross-domain; empirical gains shown in downstream learning curves but no numeric transfer ratios reported.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Pretrained encoder reused across multiple downstream tasks; qualitative improvements in multiple DMC tasks and MuJoCo Pusher reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Pretraining β-VAE may be insufficient when environment complexity increases beyond what distractor videos capture; disentanglement quality depends on latent dimension (sensitivity analysis in supplement indicates too small latent dims hurt disentanglement).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Sensitivity analysis on latent dimension reported in supplement: too-small z_dim hurts disentanglement and downstream performance; no numeric scores in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Contributes to improved downstream sample efficiency when distilled, but absolute numbers not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Pretraining on diverse distractors supports cross-domain transfer (DMC→MuJoCo experiments) qualitatively; no quantitative OOD metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Optimizes image log-likelihood reconstruction term in loss (Eq.2) but specific metrics (MSE/SSIM) are not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Latent traversals show individual latent dimensions correspond to interpretable attributes (e.g., color, position), supporting separation of task-irrelevant (color/background) from task-relevant features; analysis is qualitative.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>Pretraining itself is static (learned latents); dynamic adjustment occurs later during world-model finetuning via annealed distillation weight, not in this pretraining stage.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Not explicitly discussed for the pretraining stage; pretraining supplies priors that later influence exploration during online finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>Uses β-weighted KL regularizer to encourage posterior compression and orthogonality; no formal MI measurements reported.</td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td>Reconstruction loss retained to train generative mapping between pixels and latents; authors do not isolate scenarios where pixel fidelity is essential, but latent interpretability is emphasized as primary downstream benefit.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2247.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2247.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LatentDistill</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Offline-to-Online Latent Distillation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A KL-based distillation procedure that transfers the disentanglement capability from the pretrained action-free latent z_disen to the downstream world-model latent z_task by minimizing KL(z_disen || z_task) during online finetuning, with an annealed weight η.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Offline-to-Online Latent Distillation (KL-based)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>latent-to-latent semantic alignment (transfers factorized latent semantics rather than pixel mappings)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>Encourages downstream latent dimensions to match the pretrained disentangled posterior via KL minimization; this implicitly biases which latent factors are preserved (task-irrelevant factors that were disentangled in pretraining are penalized if not matched).</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Applied when finetuning a world model on downstream continuous-control visual RL tasks (DMC, MuJoCo Pusher, DrawerWorld).</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>Designed specifically to exploit pretraining on distracting videos; target tasks include visual distractors and domain shifts (color/textures, different dynamics).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Authors report improved finetuning performance and robustness to domain shifts when using distillation versus naive checkpoint initialization, but numeric magnitudes are not provided in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Adds KL computation between two latent distributions per batch and requires storing pretrained encoder checkpoints; no additional FLOPs/parameter counts provided. Distillation weight η annealed from 0.1 to 0.01 during adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Latent distillation offers better robustness than direct finetuning of pretrained DreamerV2 checkpoints (DV2 Finetune), particularly under large distribution shift (e.g., DMC→MuJoCo) where direct finetuning can overwrite disentangled structure.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Enables cross-domain transfer of disentanglement leading to improved downstream sample efficiency relative to baselines that either (a) do no pretraining, or (b) pretrain but do not distill latents; specific numeric transfer metrics not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Applied across multiple downstream tasks as the general transfer mechanism; learned distillation benefits observed across the evaluated tasks qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>If domain discrepancy is extremely large, distillation must be weighted carefully — otherwise the pretrained latent prior can conflict with task-specific dynamics; authors address by annealing η but report potential mismatch if not tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Ablation removing distillation degrades downstream performance; exact numbers are in supplementary material but not extracted in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Claimed to improve sample efficiency during finetuning, enabling faster attainment of higher returns; no absolute sample counts provided beyond overall training budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Distillation helps when pretraining domain differs from target domain visually and dynamically, demonstrated qualitatively (DMC→MuJoCo experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Distillation acts on latents; reconstruction loss is still used in world model training but distillation itself does not target pixel outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Distillation enforces that downstream latents preserve disentangled structure, indirectly separating task-relevant from task-irrelevant features by preserving factors that were identified as independent in pretraining; analysis is descriptive rather than quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>Distillation weight η is annealed during training, providing a mechanism to gradually reduce reliance on pretraining priors and thus adjust effective abstraction contribution over time.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Authors argue distillation plus action/reward incorporation during finetuning enriches data diversity (helpful for exploration), but there is no explicit experimental partitioning of exploration vs exploitation phases.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>Distillation uses KL divergence between latent distributions as the objective; no deeper theoretical bounds or MI estimates provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2247.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2247.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>M_phi</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Disentangled World Model M_ϕ (action-conditioned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The action-conditioned world model used during online adaptation that combines a recurrent transition, β-VAE encoder/posterior/prior, reconstruction, reward and discount prediction heads, trained with KL regularizers and distillation loss to enforce disentanglement and transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Disentangled World Model (M_ϕ)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>latent-level dynamics model with generative pixel reconstruction and additional reward/discount predictors (hybrid: latent dynamics + pixel reconstruction)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>Disentanglement enforced via βKL regularization (βKL between q(z|o) and p(z)) and distillation loss from pretrained z_disen; no explicit task-relevance objective beyond reward-prediction head which couples latents to task-relevant signals.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Finetuning domain: downstream RL tasks requiring action-conditioned predictions (DMC tasks, MuJoCo Pusher, DrawerWorld).</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>Trained online with data containing distractors (color/texture variations); adaptation aims to be robust to these task-irrelevant factors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>World model's training objectives include log-likelihood for reconstruction, reward and discount prediction; reported downstream policy performance improves when M_ϕ is trained with distillation and disentanglement constraints, but no separate numeric world-model predictive metrics (e.g., next-frame error) are reported in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>M_ϕ is trained on replay buffer samples; no explicit parameter count given. In aggregate the whole DisWM runs within ~5GB VRAM and ~16 hours on RTX 3090 for experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>M_ϕ with distillation + disentanglement outperforms standard DreamerV2 world-model finetuning and APV stacked-predictor approaches on the reported benchmarks; exact numeric deltas not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>When initialized with distilled/pretrained latent structure, M_ϕ learns downstream tasks faster and is more robust to visual variations than without distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Same M_ϕ architecture is used across multiple downstream tasks; shared representation learning (via distillation) benefits multiple tasks, with per-task finetuning yielding improved returns (plots shown but numbers not tabulated in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Performance depends on balancing reconstruction and disentanglement (β hyperparameter) and on distillation weight annealing; incorrect settings can reduce performance or overwrite pretrained structure.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Removing KL/disentanglement or distillation terms reduces downstream performance (ablation study cited), but detailed numeric ablation results are relegated to supplementary.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Using M_ϕ with distillation yields higher sample-efficiency during online finetuning versus baselines; no numerical sample counts provided beyond total training budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>M_ϕ trained with distilled disentangled latents shows improved robustness to domain shifts and distractors in cross-domain experiments (qualitative descriptions and t-SNE visualizations provided).</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Includes image reconstruction objective in loss (Eq.5) but exact reconstruction quality metrics are not reported in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Reward-prediction head couples latent dimensions to task signals, but no explicit per-latent task-relevance scoring is provided; qualitative traversals indicate preservation of semantic attributes.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>Adaptation schedule (annealed η) provides gradual shift from pretrained abstraction to task-specific latents; no per-step adaptive abstraction selection mechanism beyond this schedule.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>World model is used with Dreamer-like imagination for behavior learning; authors argue improved latent disentanglement yields better imagined rollouts for policy learning but do not provide explicit exploration/exploitation experiments across abstraction levels.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>KL terms present in loss (α and β coefficients) and explicit distillation KL; no additional information-theoretic metrics reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2247.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2247.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DreamerV2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DreamerV2 — Dream to control: Learning behaviors by latent imagination</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based RL algorithm that trains a recurrent latent dynamics world model and uses latent imagination for policy learning; used by the authors both as a baseline and as the behavioral learning recipe for DisWM's actor-critic.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dream to control: Learning behaviors by latent imagination</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DreamerV2</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>latent dynamics + pixel reconstruction (latent imagination-planning for policy learning)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>No explicit disentanglement objective in original DreamerV2; feature usefulness is shaped indirectly by reconstruction and reward prediction losses.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Model used as baseline and behavior-learning backbone for continuous-control visual RL (DMC benchmarks).</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>Baseline DreamerV2 struggles more in environments with visual distractors according to the paper's comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported qualitatively in plots as outperformed by DisWM; specific numeric comparisons are not provided in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Not detailed in this paper beyond that DisWM's experiments are comparable in compute (DisWM uses ~5GB VRAM and ~16 hours on one RTX 3090).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>DreamerV2 is outperformed by DisWM which adds disentanglement and latent distillation; DreamerV2 finetuned directly on distractors (DV2 Finetune variant) is competitive but suffers in large domain shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>DreamerV2 finetuning from distractor-pretrained checkpoints (DV2 Finetune) provides transfer benefit but can suffer catastrophic overwriting under domain mismatch; quantitative numbers not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>DreamerV2 evaluated across same DMC tasks as baseline comparisons; tends to be less sample-efficient under distractors than DisWM.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Less robust to visual distractors and cross-domain transfer when compared to disentanglement-augmented approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not performed on DreamerV2 within this paper beyond baseline comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Baseline sample efficiency is lower than DisWM in authors' plots; exact sample-to-threshold quantities not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>DreamerV2 finetune variants show degraded performance when source and target domains differ substantially (e.g., visual/dynamics/reward differences), as described qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>DreamerV2 uses reconstruction in world model training; no metrics reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>No explicit separation of task-relevant vs irrelevant features in original DreamerV2; paper contrasts this with DisWM's disentanglement approach.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>DreamerV2 does not include explicit dynamic abstraction control; DisWM adds annealed distillation on top of Dreamer-style learning.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Not specifically analyzed here for DreamerV2 beyond general baseline performance in RL tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>Not provided for DreamerV2 in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2247.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2247.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>APV</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>APV (Action-free Pretraining of Video models) / stacked latent prediction for video pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior pretraining-finetuning approach that learns stacked latent prediction models on videos (action-free) and then fine-tunes with actions in downstream tasks; used as a comparative baseline in DisWM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>APV (video pretraining + finetune)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>stacked latent prediction (latent-level, pretraining on pixels then finetuning)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>No explicit disentanglement objective reported here; relies on predictive latent training to capture useful structure.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Video-based pretraining for downstream continuous-control visual RL tasks (same benchmarks as DisWM comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>APV pretraining on videos can include various visual variations but lacks explicit distractor-specific design per authors' critique.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>APV is reported to underperform DisWM on distractor tasks; no numeric metrics are provided in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>APV performs worse than DisWM in distractor-rich settings because it lacks disentanglement-specific designs; direct training without environment-specific designs may decrease downstream performance.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>APV provides transfer benefits compared to no pretraining but is less robust under distractors than DisWM; no numeric transfer metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Used as comparative baseline across multiple tasks; reported qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Without explicit disentanglement, APV may not isolate task-irrelevant visual variations, reducing downstream robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not provided in this paper for APV other than comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Reported qualitatively as worse than DisWM in presence of distractors; no specific sample counts.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Authors claim APV lacks environment-specific designs for visual distractors and may thus generalize less well in those scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>APV does not include mechanisms to explicitly analyze task-relevant vs irrelevant features.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>No dynamic abstraction mechanism described.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Not discussed for APV in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>Not reported for APV here.</td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td>Not explicitly evaluated in the APV comparison within this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2247.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2247.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DV2 Finetune</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DreamerV2 Finetune on distracting videos (DV2 Finetune)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that pretrains a DreamerV2 model on distracting videos and then finetunes it in downstream tasks; shows some transfer benefit but is sensitive to large domain shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DV2 Finetune (pretrain DreamerV2 then finetune)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>latent dynamics world model with pixel reconstruction (DreamerV2 style)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>No explicit disentanglement; selection emerges from joint reconstruction and reward-prediction objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Pretraining on distractor videos then finetune on downstream continuous-control tasks (as compared in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>Pretrain dataset contains distractors; finetuning domains may differ causing domain mismatch issues.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported as second-best overall in some settings, but suffers large declines under domain shift (e.g., DMC→MuJoCo); no numeric metrics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Not separately reported beyond general experimental compute; direct finetuning may require mapping action spaces when mismatched.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Per authors, DV2 Finetune is competitive but less robust in large cross-domain shifts compared to DisWM with latent distillation; specific performance differences not enumerated.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Shows transfer benefit when domain shift is mild; performance degrades when observation/dynamics/actions/rewards differ substantially between source and target.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Evaluated across same tasks as baselines; no detailed per-task numbers in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Prone to overwriting pretrained disentangled information and suffering significant declines when source and target domains are mismatched (visual/dynamic discrepancies).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>No ablation for DV2 Finetune provided in this paper beyond baseline comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Better than training from scratch in some settings but less sample-efficient than DisWM in distractor-rich and cross-domain scenarios per authors' plots.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Authors highlight sensitivity to domain shifts; qualitative comparisons show larger performance drop relative to DisWM.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>No explicit mechanism to isolate task-relevant features; potential overwriting implies poor preservation of semantic priors in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>Not described for this baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Not explicitly analyzed for DV2 Finetune in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2247.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2247.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TED</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TED — Temporal Entangled/Disentangled representations for RL (TED)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that uses a self-supervised auxiliary classification task to learn temporally disentangled representations for reinforcement learning; used as a baseline in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TED (temporally disentangled representation learning)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>temporal disentanglement of latent features (latent-level with temporal priors)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>Self-supervised auxiliary tasks that encourage temporal factorization; not a reward-weighted or explicit masking mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Visual RL tasks with distractors used in comparisons (DMC benchmark tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>Designed to improve robustness to environment changes including distractors; used in comparisons where distractors are present.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported to be outperformed by DisWM in presence of distractors; no exact numeric values provided.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>DisWM performs better than TED on distractor-rich tasks per authors' plots.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>TED is not primarily a cross-domain pretraining+distillation approach; comparisons show DisWM's pretraining+distillation is more effective for cross-domain transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>TED evaluated as one of baselines across tasks; no per-task numeric results provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>TED may be less effective than explicit disentangled pretraining with distillation when large cross-domain shifts exist (per qualitative comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not discussed for TED within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>DisWM achieves higher sample efficiency than TED in presented experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>TED's temporal disentanglement improves generalization to some temporal variations but was outperformed by DisWM in distractor and cross-domain scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Not applicable / not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>TED focuses on temporal disentanglement but does not provide explicit task-relevance attribution in this paper's discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>Not described here.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Not analyzed in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>Not discussed in this paper's comparison.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2247.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2247.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CURL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CURL — Contrastive Unsupervised Representations for RL</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-free RL baseline that uses contrastive learning on visual observations to produce useful representations for RL; included as a comparative baseline and reported to struggle on some tasks (e.g., Hopper Stand).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Curl: Contrastive unsupervised representations for reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CURL</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>contrastive latent representation (no generative pixel reconstruction)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>Contrastive learning objective encourages discriminative features; no explicit disentanglement or task-relevance masking.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Model-free visual RL on DMC-like continuous control tasks used as baseline in paper comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>Reported to struggle in presence of distractors on some tasks per authors (e.g., Hopper Stand).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Described qualitatively as underperforming DisWM and some other baselines on certain tasks; specific numerical results not provided in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Not provided in this paper beyond being one baseline among several.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>CURL underperforms DisWM in distractor-rich settings according to the paper; exact performance deltas are not listed.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>CURL is not a pretraining+distillation transfer approach; no cross-domain transfer results reported in this paper for CURL.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Included in main comparison curves; struggles particularly in Hopper Stand among evaluated tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Struggles to learn effective behavior policies in some tasks with distractors (Hopper Stand example).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not presented within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Lower sample efficiency than DisWM on evaluated distracted tasks (qualitative statement).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Not deeply analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Not applicable (no reconstruction objective).</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Contrastive objective provides discriminative features but does not explicitly analyze task-relevant vs irrelevant features within this paper's comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Not discussed for CURL here.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Dream to control: Learning behaviors by latent imagination <em>(Rating: 2)</em></li>
                <li>beta-vae: Learning basic visual concepts with a constrained variational framework <em>(Rating: 2)</em></li>
                <li>APV <em>(Rating: 2)</em></li>
                <li>Temporal disentanglement of representations for improved generalisation in reinforcement learning <em>(Rating: 2)</em></li>
                <li>Curl: Contrastive unsupervised representations for reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2247",
    "paper_id": "paper-276937863",
    "extraction_schema_id": "extraction-schema-62",
    "extracted_data": [
        {
            "name_short": "DisWM",
            "name_full": "Disentangled World Models",
            "brief_description": "A model-based visual RL framework that pretrains an action-free β-VAE video predictor on distracting videos to learn disentangled semantic latents, then transfers that disentanglement to an action-conditioned world model via offline-to-online latent distillation and disentanglement constraints during online finetuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Disentangled World Models (DisWM)",
            "abstraction_level": "latent-level semantic / disentangled factors (learned low-dimensional latent space with generative reconstruction)",
            "feature_selection_mechanism": "implicit via disentanglement regularization (β-VAE KL to isotropic Gaussian, β-weighted KL terms) and KL-based latent distillation from a pretrained disentangled encoder; no explicit attention or masking",
            "task_domain": "Continuous-control visual RL (DeepMind Control Suite (DMC) tasks, MuJoCo Pusher manipulation, DrawerWorld texture transfer)",
            "distractor_presence": "Yes — explicit visual distractors used in pretraining and finetuning: color perturbations, texture variations, and other 'distracting videos' assembled from varied domains (e.g., DMC frames as distractors for MuJoCo downstream).",
            "performance_metrics": "Qualitative/aggregate: reports improved sample efficiency and higher episode return curves compared to baselines (DreamerV2, APV, DV2 Finetune, TED, CURL); specific numeric returns or percentages are not reported in the main text.",
            "computational_cost_details": "Reported: each run uses ~5 GB VRAM and takes ~16 hours on a single NVIDIA RTX 3090 for training to reported evaluation point; training up to 1e6 environment steps for DMC tasks; pretrained distracting-video dataset built with ~1M frames; latent dimension for both pretrained z_disen and downstream z_task = 20; distillation hyperparameter η annealed from 0.1 to 0.01.",
            "comparison_to_baselines": "Qualitative: DisWM outperforms TED, CURL, APV and DreamerV2 baselines on sample-efficiency and final return in the reported benchmarks; DV2 Finetune is second-best overall but suffers in large domain-shift cases (e.g., DMC→MuJoCo) where DisWM degrades less. No exact numeric gaps provided in main text.",
            "transfer_learning_results": "Evaluated cross-domain transfer from action-free distracting videos to downstream tasks; latent distillation transfers disentanglement capability and improves downstream sample efficiency versus direct finetuning (DV2 Finetune) and APV-style approaches. Quantitative transfer metrics (e.g., relative % improvement) are not provided in the main text.",
            "multi_task_performance": "Evaluated across multiple tasks (DMC: Walker Walk, Cheetah Run, Hopper Stand, Finger Spin, Cartpole Swingup; MuJoCo Pusher; DrawerWorld textures). Uses a shared pretrained disentangled encoder and then task-specific world-model finetuning; results reported as per-task learning curves showing consistent improvements, but no per-task numeric table in main text.",
            "failure_modes": "Authors report limitations: disentangled representation learning can struggle in more complex or non-stationary environments (e.g., time-varying background videos); large domain mismatches can cause overwriting of pretrained features if distillation is not controlled.",
            "ablation_studies": "Ablations performed on presence/absence of latent distillation and disentanglement constraints showing both components contribute positively to performance; quantitative ablation numbers are referenced but not enumerated in the main text (details likely in supplementary).",
            "sample_efficiency": "Claim: improves sample efficiency relative to baselines (learning faster within 1e6 environment steps on DMC); no explicit numeric samples-to-threshold reported in main body.",
            "generalization_analysis": "Cross-domain generalization evaluated (e.g., pretraining on DMC distracting videos and transferring to MuJoCo), showing robustness to visual and domain variations; t-SNE visualization indicates online interaction latents are more diverse than offline pretraining latents. No numeric OOD generalization metrics given.",
            "reconstruction_quality": "Model includes image reconstruction losses (negative log-likelihood term) as part of training objective, but no PSNR/SSIM/MSE values are reported in the main text.",
            "task_relevance_analysis": "Paper motivates separation of task-relevant vs task-irrelevant features via disentanglement: pretrained z_disen captures semantic factors so that changes (e.g., color) only affect a small subset of latents; qualitative latent traversals (Fig.5/6) demonstrate interpretable factors but no formal task-relevance attribution scores are reported.",
            "dynamic_abstraction": "Partially — the adaptation process anneals the distillation weight η (0.1 → 0.01) to progressively reduce reliance on pretrained latents, enabling dynamic adjustment of contributed prior knowledge during finetuning; no automatic switching between abstraction levels beyond this annealing is described.",
            "exploration_vs_exploitation": "Discussed qualitatively: incorporation of actions and rewards during online finetuning enriches sample diversity and strengthens disentanglement, implying benefits during exploration (more diverse imagined/seen states); no explicit separate experiments comparing abstraction levels in exploration vs exploitation phases.",
            "information_theoretic_analysis": "Uses KL divergence terms (β-VAE KL to isotropic Gaussian for disentanglement; KL between pretrained and task latent distributions for distillation). No explicit mutual-information estimates, compression bounds, or formal rate–distortion curves are reported.",
            "pixel_fidelity_benefits": "The model retains pixel-reconstruction objective (image log-likelihood) during both pretraining and adaptation. The paper argues that reconstruction is necessary to train the generative latent model but does not provide experiments isolating when pixel fidelity specifically benefits downstream task performance.",
            "uuid": "e2247.0"
        },
        {
            "name_short": "β-VAE pretrain",
            "name_full": "β-Variational Autoencoder action-free video prediction (pretraining)",
            "brief_description": "An action-free video prediction model using a β-VAE encoder/decoder and a latent prior trained on distracting videos to learn disentangled latent factors (z_disen) via β-weighted KL regularization and an action-free KL prior on predicted latents.",
            "citation_title": "beta-vae: Learning basic visual concepts with a constrained variational framework",
            "mention_or_use": "use",
            "model_name": "β-VAE-based action-free video predictor",
            "abstraction_level": "latent-level disentangled semantic factors (trained to reconstruct pixels but regularized for factorization)",
            "feature_selection_mechanism": "Disentanglement enforced via β2 KL term encouraging posterior q(z|o) to match isotropic Gaussian and an action-free KL between posterior and prior from historical latents; thus selection is implicit via regularization rather than supervised relevance signals.",
            "task_domain": "Pretraining domain: distracting videos sampled from DMC interactions with color distractors (1M frames dataset).",
            "distractor_presence": "Yes — trained specifically on 'distracting videos' containing visual variations (color perturbations, textures) to encourage robust disentanglement of semantic factors from irrelevant appearance changes.",
            "performance_metrics": "No numeric reconstruction or disentanglement metrics (e.g., MIG, DCI) reported in main text; qualitative latent traversals shown (Fig.5) to indicate successful disentanglement.",
            "computational_cost_details": "Trained offline on 1M-frame distracting-video dataset; latent dimension reported as 20; no FLOPs or parameter counts provided specifically for the β-VAE component.",
            "comparison_to_baselines": "Used as pretrained prior in DisWM; authors argue it yields more interpretable orthogonal latent spaces than training from scratch, leading to improved downstream sample efficiency compared to baselines that lack this pretraining. No direct numerical head-to-head on pretraining alone provided.",
            "transfer_learning_results": "Pretrained disentangled latents are distilled into downstream world model via KL minimization, claimed to transfer disentangling capability cross-domain; empirical gains shown in downstream learning curves but no numeric transfer ratios reported.",
            "multi_task_performance": "Pretrained encoder reused across multiple downstream tasks; qualitative improvements in multiple DMC tasks and MuJoCo Pusher reported.",
            "failure_modes": "Pretraining β-VAE may be insufficient when environment complexity increases beyond what distractor videos capture; disentanglement quality depends on latent dimension (sensitivity analysis in supplement indicates too small latent dims hurt disentanglement).",
            "ablation_studies": "Sensitivity analysis on latent dimension reported in supplement: too-small z_dim hurts disentanglement and downstream performance; no numeric scores in main text.",
            "sample_efficiency": "Contributes to improved downstream sample efficiency when distilled, but absolute numbers not provided.",
            "generalization_analysis": "Pretraining on diverse distractors supports cross-domain transfer (DMC→MuJoCo experiments) qualitatively; no quantitative OOD metrics.",
            "reconstruction_quality": "Optimizes image log-likelihood reconstruction term in loss (Eq.2) but specific metrics (MSE/SSIM) are not reported.",
            "task_relevance_analysis": "Latent traversals show individual latent dimensions correspond to interpretable attributes (e.g., color, position), supporting separation of task-irrelevant (color/background) from task-relevant features; analysis is qualitative.",
            "dynamic_abstraction": "Pretraining itself is static (learned latents); dynamic adjustment occurs later during world-model finetuning via annealed distillation weight, not in this pretraining stage.",
            "exploration_vs_exploitation": "Not explicitly discussed for the pretraining stage; pretraining supplies priors that later influence exploration during online finetuning.",
            "information_theoretic_analysis": "Uses β-weighted KL regularizer to encourage posterior compression and orthogonality; no formal MI measurements reported.",
            "pixel_fidelity_benefits": "Reconstruction loss retained to train generative mapping between pixels and latents; authors do not isolate scenarios where pixel fidelity is essential, but latent interpretability is emphasized as primary downstream benefit.",
            "uuid": "e2247.1"
        },
        {
            "name_short": "LatentDistill",
            "name_full": "Offline-to-Online Latent Distillation",
            "brief_description": "A KL-based distillation procedure that transfers the disentanglement capability from the pretrained action-free latent z_disen to the downstream world-model latent z_task by minimizing KL(z_disen || z_task) during online finetuning, with an annealed weight η.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Offline-to-Online Latent Distillation (KL-based)",
            "abstraction_level": "latent-to-latent semantic alignment (transfers factorized latent semantics rather than pixel mappings)",
            "feature_selection_mechanism": "Encourages downstream latent dimensions to match the pretrained disentangled posterior via KL minimization; this implicitly biases which latent factors are preserved (task-irrelevant factors that were disentangled in pretraining are penalized if not matched).",
            "task_domain": "Applied when finetuning a world model on downstream continuous-control visual RL tasks (DMC, MuJoCo Pusher, DrawerWorld).",
            "distractor_presence": "Designed specifically to exploit pretraining on distracting videos; target tasks include visual distractors and domain shifts (color/textures, different dynamics).",
            "performance_metrics": "Authors report improved finetuning performance and robustness to domain shifts when using distillation versus naive checkpoint initialization, but numeric magnitudes are not provided in the main text.",
            "computational_cost_details": "Adds KL computation between two latent distributions per batch and requires storing pretrained encoder checkpoints; no additional FLOPs/parameter counts provided. Distillation weight η annealed from 0.1 to 0.01 during adaptation.",
            "comparison_to_baselines": "Latent distillation offers better robustness than direct finetuning of pretrained DreamerV2 checkpoints (DV2 Finetune), particularly under large distribution shift (e.g., DMC→MuJoCo) where direct finetuning can overwrite disentangled structure.",
            "transfer_learning_results": "Enables cross-domain transfer of disentanglement leading to improved downstream sample efficiency relative to baselines that either (a) do no pretraining, or (b) pretrain but do not distill latents; specific numeric transfer metrics not provided.",
            "multi_task_performance": "Applied across multiple downstream tasks as the general transfer mechanism; learned distillation benefits observed across the evaluated tasks qualitatively.",
            "failure_modes": "If domain discrepancy is extremely large, distillation must be weighted carefully — otherwise the pretrained latent prior can conflict with task-specific dynamics; authors address by annealing η but report potential mismatch if not tuned.",
            "ablation_studies": "Ablation removing distillation degrades downstream performance; exact numbers are in supplementary material but not extracted in main text.",
            "sample_efficiency": "Claimed to improve sample efficiency during finetuning, enabling faster attainment of higher returns; no absolute sample counts provided beyond overall training budgets.",
            "generalization_analysis": "Distillation helps when pretraining domain differs from target domain visually and dynamically, demonstrated qualitatively (DMC→MuJoCo experiment).",
            "reconstruction_quality": "Distillation acts on latents; reconstruction loss is still used in world model training but distillation itself does not target pixel outputs.",
            "task_relevance_analysis": "Distillation enforces that downstream latents preserve disentangled structure, indirectly separating task-relevant from task-irrelevant features by preserving factors that were identified as independent in pretraining; analysis is descriptive rather than quantified.",
            "dynamic_abstraction": "Distillation weight η is annealed during training, providing a mechanism to gradually reduce reliance on pretraining priors and thus adjust effective abstraction contribution over time.",
            "exploration_vs_exploitation": "Authors argue distillation plus action/reward incorporation during finetuning enriches data diversity (helpful for exploration), but there is no explicit experimental partitioning of exploration vs exploitation phases.",
            "information_theoretic_analysis": "Distillation uses KL divergence between latent distributions as the objective; no deeper theoretical bounds or MI estimates provided.",
            "uuid": "e2247.2"
        },
        {
            "name_short": "M_phi",
            "name_full": "Disentangled World Model M_ϕ (action-conditioned)",
            "brief_description": "The action-conditioned world model used during online adaptation that combines a recurrent transition, β-VAE encoder/posterior/prior, reconstruction, reward and discount prediction heads, trained with KL regularizers and distillation loss to enforce disentanglement and transfer.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Disentangled World Model (M_ϕ)",
            "abstraction_level": "latent-level dynamics model with generative pixel reconstruction and additional reward/discount predictors (hybrid: latent dynamics + pixel reconstruction)",
            "feature_selection_mechanism": "Disentanglement enforced via βKL regularization (βKL between q(z|o) and p(z)) and distillation loss from pretrained z_disen; no explicit task-relevance objective beyond reward-prediction head which couples latents to task-relevant signals.",
            "task_domain": "Finetuning domain: downstream RL tasks requiring action-conditioned predictions (DMC tasks, MuJoCo Pusher, DrawerWorld).",
            "distractor_presence": "Trained online with data containing distractors (color/texture variations); adaptation aims to be robust to these task-irrelevant factors.",
            "performance_metrics": "World model's training objectives include log-likelihood for reconstruction, reward and discount prediction; reported downstream policy performance improves when M_ϕ is trained with distillation and disentanglement constraints, but no separate numeric world-model predictive metrics (e.g., next-frame error) are reported in main text.",
            "computational_cost_details": "M_ϕ is trained on replay buffer samples; no explicit parameter count given. In aggregate the whole DisWM runs within ~5GB VRAM and ~16 hours on RTX 3090 for experiments reported.",
            "comparison_to_baselines": "M_ϕ with distillation + disentanglement outperforms standard DreamerV2 world-model finetuning and APV stacked-predictor approaches on the reported benchmarks; exact numeric deltas not provided.",
            "transfer_learning_results": "When initialized with distilled/pretrained latent structure, M_ϕ learns downstream tasks faster and is more robust to visual variations than without distillation.",
            "multi_task_performance": "Same M_ϕ architecture is used across multiple downstream tasks; shared representation learning (via distillation) benefits multiple tasks, with per-task finetuning yielding improved returns (plots shown but numbers not tabulated in main text).",
            "failure_modes": "Performance depends on balancing reconstruction and disentanglement (β hyperparameter) and on distillation weight annealing; incorrect settings can reduce performance or overwrite pretrained structure.",
            "ablation_studies": "Removing KL/disentanglement or distillation terms reduces downstream performance (ablation study cited), but detailed numeric ablation results are relegated to supplementary.",
            "sample_efficiency": "Using M_ϕ with distillation yields higher sample-efficiency during online finetuning versus baselines; no numerical sample counts provided beyond total training budgets.",
            "generalization_analysis": "M_ϕ trained with distilled disentangled latents shows improved robustness to domain shifts and distractors in cross-domain experiments (qualitative descriptions and t-SNE visualizations provided).",
            "reconstruction_quality": "Includes image reconstruction objective in loss (Eq.5) but exact reconstruction quality metrics are not reported in main text.",
            "task_relevance_analysis": "Reward-prediction head couples latent dimensions to task signals, but no explicit per-latent task-relevance scoring is provided; qualitative traversals indicate preservation of semantic attributes.",
            "dynamic_abstraction": "Adaptation schedule (annealed η) provides gradual shift from pretrained abstraction to task-specific latents; no per-step adaptive abstraction selection mechanism beyond this schedule.",
            "exploration_vs_exploitation": "World model is used with Dreamer-like imagination for behavior learning; authors argue improved latent disentanglement yields better imagined rollouts for policy learning but do not provide explicit exploration/exploitation experiments across abstraction levels.",
            "information_theoretic_analysis": "KL terms present in loss (α and β coefficients) and explicit distillation KL; no additional information-theoretic metrics reported.",
            "uuid": "e2247.3"
        },
        {
            "name_short": "DreamerV2",
            "name_full": "DreamerV2 — Dream to control: Learning behaviors by latent imagination",
            "brief_description": "A model-based RL algorithm that trains a recurrent latent dynamics world model and uses latent imagination for policy learning; used by the authors both as a baseline and as the behavioral learning recipe for DisWM's actor-critic.",
            "citation_title": "Dream to control: Learning behaviors by latent imagination",
            "mention_or_use": "use",
            "model_name": "DreamerV2",
            "abstraction_level": "latent dynamics + pixel reconstruction (latent imagination-planning for policy learning)",
            "feature_selection_mechanism": "No explicit disentanglement objective in original DreamerV2; feature usefulness is shaped indirectly by reconstruction and reward prediction losses.",
            "task_domain": "Model used as baseline and behavior-learning backbone for continuous-control visual RL (DMC benchmarks).",
            "distractor_presence": "Baseline DreamerV2 struggles more in environments with visual distractors according to the paper's comparisons.",
            "performance_metrics": "Reported qualitatively in plots as outperformed by DisWM; specific numeric comparisons are not provided in main text.",
            "computational_cost_details": "Not detailed in this paper beyond that DisWM's experiments are comparable in compute (DisWM uses ~5GB VRAM and ~16 hours on one RTX 3090).",
            "comparison_to_baselines": "DreamerV2 is outperformed by DisWM which adds disentanglement and latent distillation; DreamerV2 finetuned directly on distractors (DV2 Finetune variant) is competitive but suffers in large domain shifts.",
            "transfer_learning_results": "DreamerV2 finetuning from distractor-pretrained checkpoints (DV2 Finetune) provides transfer benefit but can suffer catastrophic overwriting under domain mismatch; quantitative numbers not provided.",
            "multi_task_performance": "DreamerV2 evaluated across same DMC tasks as baseline comparisons; tends to be less sample-efficient under distractors than DisWM.",
            "failure_modes": "Less robust to visual distractors and cross-domain transfer when compared to disentanglement-augmented approaches.",
            "ablation_studies": "Not performed on DreamerV2 within this paper beyond baseline comparison.",
            "sample_efficiency": "Baseline sample efficiency is lower than DisWM in authors' plots; exact sample-to-threshold quantities not provided.",
            "generalization_analysis": "DreamerV2 finetune variants show degraded performance when source and target domains differ substantially (e.g., visual/dynamics/reward differences), as described qualitatively.",
            "reconstruction_quality": "DreamerV2 uses reconstruction in world model training; no metrics reported here.",
            "task_relevance_analysis": "No explicit separation of task-relevant vs irrelevant features in original DreamerV2; paper contrasts this with DisWM's disentanglement approach.",
            "dynamic_abstraction": "DreamerV2 does not include explicit dynamic abstraction control; DisWM adds annealed distillation on top of Dreamer-style learning.",
            "exploration_vs_exploitation": "Not specifically analyzed here for DreamerV2 beyond general baseline performance in RL tasks.",
            "information_theoretic_analysis": "Not provided for DreamerV2 in this paper.",
            "uuid": "e2247.4"
        },
        {
            "name_short": "APV",
            "name_full": "APV (Action-free Pretraining of Video models) / stacked latent prediction for video pretraining",
            "brief_description": "A prior pretraining-finetuning approach that learns stacked latent prediction models on videos (action-free) and then fine-tunes with actions in downstream tasks; used as a comparative baseline in DisWM.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "APV (video pretraining + finetune)",
            "abstraction_level": "stacked latent prediction (latent-level, pretraining on pixels then finetuning)",
            "feature_selection_mechanism": "No explicit disentanglement objective reported here; relies on predictive latent training to capture useful structure.",
            "task_domain": "Video-based pretraining for downstream continuous-control visual RL tasks (same benchmarks as DisWM comparisons).",
            "distractor_presence": "APV pretraining on videos can include various visual variations but lacks explicit distractor-specific design per authors' critique.",
            "performance_metrics": "APV is reported to underperform DisWM on distractor tasks; no numeric metrics are provided in main text.",
            "computational_cost_details": "Not reported in this paper.",
            "comparison_to_baselines": "APV performs worse than DisWM in distractor-rich settings because it lacks disentanglement-specific designs; direct training without environment-specific designs may decrease downstream performance.",
            "transfer_learning_results": "APV provides transfer benefits compared to no pretraining but is less robust under distractors than DisWM; no numeric transfer metrics.",
            "multi_task_performance": "Used as comparative baseline across multiple tasks; reported qualitatively.",
            "failure_modes": "Without explicit disentanglement, APV may not isolate task-irrelevant visual variations, reducing downstream robustness.",
            "ablation_studies": "Not provided in this paper for APV other than comparisons.",
            "sample_efficiency": "Reported qualitatively as worse than DisWM in presence of distractors; no specific sample counts.",
            "generalization_analysis": "Authors claim APV lacks environment-specific designs for visual distractors and may thus generalize less well in those scenarios.",
            "reconstruction_quality": "Not reported here.",
            "task_relevance_analysis": "APV does not include mechanisms to explicitly analyze task-relevant vs irrelevant features.",
            "dynamic_abstraction": "No dynamic abstraction mechanism described.",
            "exploration_vs_exploitation": "Not discussed for APV in this paper.",
            "information_theoretic_analysis": "Not reported for APV here.",
            "pixel_fidelity_benefits": "Not explicitly evaluated in the APV comparison within this paper.",
            "uuid": "e2247.5"
        },
        {
            "name_short": "DV2 Finetune",
            "name_full": "DreamerV2 Finetune on distracting videos (DV2 Finetune)",
            "brief_description": "A baseline that pretrains a DreamerV2 model on distracting videos and then finetunes it in downstream tasks; shows some transfer benefit but is sensitive to large domain shifts.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "DV2 Finetune (pretrain DreamerV2 then finetune)",
            "abstraction_level": "latent dynamics world model with pixel reconstruction (DreamerV2 style)",
            "feature_selection_mechanism": "No explicit disentanglement; selection emerges from joint reconstruction and reward-prediction objectives.",
            "task_domain": "Pretraining on distractor videos then finetune on downstream continuous-control tasks (as compared in paper).",
            "distractor_presence": "Pretrain dataset contains distractors; finetuning domains may differ causing domain mismatch issues.",
            "performance_metrics": "Reported as second-best overall in some settings, but suffers large declines under domain shift (e.g., DMC→MuJoCo); no numeric metrics provided.",
            "computational_cost_details": "Not separately reported beyond general experimental compute; direct finetuning may require mapping action spaces when mismatched.",
            "comparison_to_baselines": "Per authors, DV2 Finetune is competitive but less robust in large cross-domain shifts compared to DisWM with latent distillation; specific performance differences not enumerated.",
            "transfer_learning_results": "Shows transfer benefit when domain shift is mild; performance degrades when observation/dynamics/actions/rewards differ substantially between source and target.",
            "multi_task_performance": "Evaluated across same tasks as baselines; no detailed per-task numbers in main text.",
            "failure_modes": "Prone to overwriting pretrained disentangled information and suffering significant declines when source and target domains are mismatched (visual/dynamic discrepancies).",
            "ablation_studies": "No ablation for DV2 Finetune provided in this paper beyond baseline comparisons.",
            "sample_efficiency": "Better than training from scratch in some settings but less sample-efficient than DisWM in distractor-rich and cross-domain scenarios per authors' plots.",
            "generalization_analysis": "Authors highlight sensitivity to domain shifts; qualitative comparisons show larger performance drop relative to DisWM.",
            "reconstruction_quality": "Not reported here.",
            "task_relevance_analysis": "No explicit mechanism to isolate task-relevant features; potential overwriting implies poor preservation of semantic priors in some cases.",
            "dynamic_abstraction": "Not described for this baseline.",
            "exploration_vs_exploitation": "Not explicitly analyzed for DV2 Finetune in this paper.",
            "information_theoretic_analysis": "Not reported.",
            "uuid": "e2247.6"
        },
        {
            "name_short": "TED",
            "name_full": "TED — Temporal Entangled/Disentangled representations for RL (TED)",
            "brief_description": "A method that uses a self-supervised auxiliary classification task to learn temporally disentangled representations for reinforcement learning; used as a baseline in comparisons.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "TED (temporally disentangled representation learning)",
            "abstraction_level": "temporal disentanglement of latent features (latent-level with temporal priors)",
            "feature_selection_mechanism": "Self-supervised auxiliary tasks that encourage temporal factorization; not a reward-weighted or explicit masking mechanism.",
            "task_domain": "Visual RL tasks with distractors used in comparisons (DMC benchmark tasks).",
            "distractor_presence": "Designed to improve robustness to environment changes including distractors; used in comparisons where distractors are present.",
            "performance_metrics": "Reported to be outperformed by DisWM in presence of distractors; no exact numeric values provided.",
            "computational_cost_details": "Not provided in this paper.",
            "comparison_to_baselines": "DisWM performs better than TED on distractor-rich tasks per authors' plots.",
            "transfer_learning_results": "TED is not primarily a cross-domain pretraining+distillation approach; comparisons show DisWM's pretraining+distillation is more effective for cross-domain transfer.",
            "multi_task_performance": "TED evaluated as one of baselines across tasks; no per-task numeric results provided here.",
            "failure_modes": "TED may be less effective than explicit disentangled pretraining with distillation when large cross-domain shifts exist (per qualitative comparisons).",
            "ablation_studies": "Not discussed for TED within this paper.",
            "sample_efficiency": "DisWM achieves higher sample efficiency than TED in presented experiments.",
            "generalization_analysis": "TED's temporal disentanglement improves generalization to some temporal variations but was outperformed by DisWM in distractor and cross-domain scenarios.",
            "reconstruction_quality": "Not applicable / not reported here.",
            "task_relevance_analysis": "TED focuses on temporal disentanglement but does not provide explicit task-relevance attribution in this paper's discussion.",
            "dynamic_abstraction": "Not described here.",
            "exploration_vs_exploitation": "Not analyzed in comparisons.",
            "information_theoretic_analysis": "Not discussed in this paper's comparison.",
            "uuid": "e2247.7"
        },
        {
            "name_short": "CURL",
            "name_full": "CURL — Contrastive Unsupervised Representations for RL",
            "brief_description": "A model-free RL baseline that uses contrastive learning on visual observations to produce useful representations for RL; included as a comparative baseline and reported to struggle on some tasks (e.g., Hopper Stand).",
            "citation_title": "Curl: Contrastive unsupervised representations for reinforcement learning",
            "mention_or_use": "mention",
            "model_name": "CURL",
            "abstraction_level": "contrastive latent representation (no generative pixel reconstruction)",
            "feature_selection_mechanism": "Contrastive learning objective encourages discriminative features; no explicit disentanglement or task-relevance masking.",
            "task_domain": "Model-free visual RL on DMC-like continuous control tasks used as baseline in paper comparisons.",
            "distractor_presence": "Reported to struggle in presence of distractors on some tasks per authors (e.g., Hopper Stand).",
            "performance_metrics": "Described qualitatively as underperforming DisWM and some other baselines on certain tasks; specific numerical results not provided in main text.",
            "computational_cost_details": "Not provided in this paper beyond being one baseline among several.",
            "comparison_to_baselines": "CURL underperforms DisWM in distractor-rich settings according to the paper; exact performance deltas are not listed.",
            "transfer_learning_results": "CURL is not a pretraining+distillation transfer approach; no cross-domain transfer results reported in this paper for CURL.",
            "multi_task_performance": "Included in main comparison curves; struggles particularly in Hopper Stand among evaluated tasks.",
            "failure_modes": "Struggles to learn effective behavior policies in some tasks with distractors (Hopper Stand example).",
            "ablation_studies": "Not presented within this paper.",
            "sample_efficiency": "Lower sample efficiency than DisWM on evaluated distracted tasks (qualitative statement).",
            "generalization_analysis": "Not deeply analyzed here.",
            "reconstruction_quality": "Not applicable (no reconstruction objective).",
            "task_relevance_analysis": "Contrastive objective provides discriminative features but does not explicitly analyze task-relevant vs irrelevant features within this paper's comparisons.",
            "dynamic_abstraction": "Not applicable.",
            "exploration_vs_exploitation": "Not discussed for CURL here.",
            "information_theoretic_analysis": "Not provided in this paper.",
            "uuid": "e2247.8"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Dream to control: Learning behaviors by latent imagination",
            "rating": 2
        },
        {
            "paper_title": "beta-vae: Learning basic visual concepts with a constrained variational framework",
            "rating": 2
        },
        {
            "paper_title": "APV",
            "rating": 2
        },
        {
            "paper_title": "Temporal disentanglement of representations for improved generalisation in reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Curl: Contrastive unsupervised representations for reinforcement learning",
            "rating": 1
        }
    ],
    "cost": 0.02065675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Disentangled World Models: Learning to Transfer Semantic Knowledge from Distracting Videos for Reinforcement Learning
28 Aug 2025</p>
<p>Qi Wang 
MoE Key Lab of Artificial Intelligence
AI Institute
Shanghai Jiao Tong University</p>
<p>Ningbo Institute of Digital Twin
Eastern Institute of Technology
NingboChina</p>
<p>Ningbo Key Laboratory of Spatial Intelligence and Digital Derivative
NingboChina</p>
<p>Zhipeng Zhang 
University of Chinese Academy of Sciences</p>
<p>Shenyang Institute of Computing Technology
Chinese Academy of Sciences</p>
<p>Baao Xie 
Ningbo Institute of Digital Twin
Eastern Institute of Technology
NingboChina</p>
<p>Ningbo Key Laboratory of Spatial Intelligence and Digital Derivative
NingboChina</p>
<p>Xin Jin <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#106;&#105;&#110;&#120;&#105;&#110;&#64;&#101;&#105;&#116;&#101;&#99;&#104;&#46;&#101;&#100;&#117;&#46;&#99;&#110;">&#106;&#105;&#110;&#120;&#105;&#110;&#64;&#101;&#105;&#116;&#101;&#99;&#104;&#46;&#101;&#100;&#117;&#46;&#99;&#110;</a>. 
Ningbo Institute of Digital Twin
Eastern Institute of Technology
NingboChina</p>
<p>Ningbo Key Laboratory of Spatial Intelligence and Digital Derivative
NingboChina</p>
<p>Yunbo Wang 
MoE Key Lab of Artificial Intelligence
AI Institute
Shanghai Jiao Tong University</p>
<p>Shiyu Wang 
Shenyang Institute of Computing Technology
Chinese Academy of Sciences</p>
<p>Shenyang CASNC Technology Co
Ltd</p>
<p>Liaomo Zheng 
Shenyang Institute of Computing Technology
Chinese Academy of Sciences</p>
<p>Shenyang CASNC Technology Co
Ltd</p>
<p>Xiaokang Yang 
MoE Key Lab of Artificial Intelligence
AI Institute
Shanghai Jiao Tong University</p>
<p>Wenjun Zeng 
Ningbo Institute of Digital Twin
Eastern Institute of Technology
NingboChina</p>
<p>Ningbo Key Laboratory of Spatial Intelligence and Digital Derivative
NingboChina</p>
<p>Disentangled World Models: Learning to Transfer Semantic Knowledge from Distracting Videos for Reinforcement Learning
28 Aug 202549359D09C3874A58E9BCC6EF0B005074arXiv:2503.08751v2[cs.CV]
Training visual reinforcement learning (RL) in practical scenarios presents a significant challenge, i.e., RL agents suffer from low sample efficiency in environments with variations.While various approaches have attempted to alleviate this issue by disentangled representation learning, these methods usually start learning from scratch without prior knowledge of the world.This paper, in contrast, tries to learn and understand underlying semantic variations from distracting videos via offline-to-online latent distillation and flexible disentanglement constraints.To enable effective cross-domain semantic knowledge transfer, we introduce an interpretable model-based RL framework, dubbed Disentangled World Models (DisWM).Specifically, we pretrain the action-free video prediction model offline with disentanglement regularization to extract semantic knowledge from distracting videos.The disentanglement capability of the pretrained model is then transferred to the world model through latent distillation.For finetuning in the online environment, we exploit the knowledge from the pretrained model and introduce a disentanglement constraint to the world model.During the adaptation phase, the incorporation of actions and rewards from online environment interactions enriches the diversity of the data, which in turn strengthens the disentangled representation learning.Experimental results validate the superiority of our approach on various benchmarks.</p>
<p>Offline distrac ng videos from different domains</p>
<p>Disentangled video prediciton model</p>
<p>Pretraining</p>
<p>Latent dis lla on
Finetuing Online environment Disentangled world model</p>
<p>Introduction</p>
<p>Visual reinforcement learning (VRL) presents a promising approach for training agents within complex environments [11,18,20,28,45].However, VRL frequently suffers from performance degradation in practical scenarios due to the complexity, volatility, and visual distractions in environments.Even minor environmental variations can result in significant pixel-level shifts, making the trained VRL policies ineffective or suboptimal [6,7].For instance, a slight change in lighting conditions can affect an object's appearance (e.g., color, shadow, or other visual attributes).Therefore, it is crucial to enhance models with interpretability, enabling them to perceive, learn, and understand the semantic environmental variations.Disentangled representation learning (DRL) presents a promising approach to addressing the interpretability challenges inherent in the "black-box" nature of deep learning algorithms.Fundamentally, DRL approaches mimic the cognitive processes of biological intelligence, where under- standing the world is facilitated by decomposing observations into distinct and independent factors [2,14,38,40,41].In this form, when a factor of variation is changed (e.g., color), only a small portion of features in the disentangled representation will be affected, enabling the agent to recover performance quickly.Several studies have explored the integration of DRL algorithms in the domain of VRL.For example, Higgins et al. [15] trained a β-VAE offline to obtain disentangled representations for reinforcement learning.TED [7] adopts a self-supervised auxiliary task to learn temporally disentangled representations for reinforcement learning.Additionally, Dunion et al. [6] introduced conditional mutual information to achieve a disentangled representation with correlated data.However, existing methods typically learn the representations from scratch, lacking any prior knowledge of the world.These approaches often require extensive interactions with the environment to acquire desired behaviors.Towards this challenge, we introduce a model-based interpretable VRL framework, dubbed Disentangled World Models (DisWM), which leverages prior knowledge extracted from distracting videos to facilitate the learning of unseen downstream tasks through latent distillation.It is crucial to note that distracting videos refer to videos with visual distractions, which are beneficial for learning disentangled representations.Specifically, as depicted in Figure 1, our framework consists of two phases: first, we pretrain a DRL encoder to learn disentangled latent representations from distracting videos.By doing so, the pretrained DRL encoder is "knowledgeable" in terms of representation disentanglement.Subsequently, we finetune an orthogonally designed world model with dual constraints of disentanglement and distillation, leveraging semantic knowledge transferred from the pretrained model via offline-to-online latent distillation.Another benefit of disentangled world model adaptation is that incorporating actions and rewards from online interactions with the environment enriches the diversity of the visual observations, which in turn strengthens the process of disentangled representation learning.It is worth mentioning that, as a cross-domain framework, DisWM does not require the pretraining videos to originate from the same domain as the downstream tasks.</p>
<p>Experimental results demonstrate the effectiveness of our proposed approach in improving the sample efficiency of VRL agents across our modified DeepMind Control and MuJoCo Pusher.The contributions of this work can be summarized as follows:</p>
<p>• We frame the problem of learning interpretable VRL agents as a domain transfer learning problem.The key idea is to extract semantic knowledge from distracting videos and transfer this disentanglement capability to downstream control tasks.• We present DisWM, an approach that follows the pretraining-finetuning paradigm using distracting videos, incorporating specific techniques of offline-to-online latent distillation and flexible disentanglement constraints.</p>
<p>Problem Setup</p>
<p>We formulate visual reinforcement learning as a partially observable Markov decision process (POMDP) that uses DMC and MuJoCo Pusher as the test bench.Specifically, we concentrate on scenarios where videos without actions and rewards are accessible, enabling world knowledge transfer.The goal is to maximize the cumulative reward of the target POMDP ⟨O, A, T , R, γ⟩ by transferring the shared world knowledge from the videos.These notations correspond to the visual observation space, the action space, the transition probabilities, the reward function, and the discount factor, respectively.</p>
<p>For instance, in one of the cross-domain experiments, we use MuJoCo as the downstream domain and the frames collected from DMC as the distracting video.Table 1 highlights the differences between the two domains in terms of visual appearances, physical dynamics, action spaces, and reward functions.</p>
<p>Method</p>
<p>Overview of DisWM</p>
<p>In this section, we present the details of DisWM, which involves three main stages (see Figure 2</p>
<p>Disentangled Representation Pretraining</p>
<p>To extract well-disentangled representations that can be transferred to the downstream world model, we first train a video prediction model on distracting videos without incorporating action information (Lines 4-8 of Alg. 1).This model comprises three key components: (i) the posterior learner that encodes the observation o t to latent state z t via β-VAE encoder 1 , which serves as a typical DRL framework to extract latent features z t from observations, (ii) the prior module that predicts future latent states based on historical states, without directly relying on the current observation o t , and (iii) the β-VAE-based decoder that reconstructs ôt from the latent state z t .Concretely, the model can be formulated as follows:
β-VAE encoder: z t = e ϕ ′ (o t )
Posterior state:
z t ∼ q ϕ ′ (z t | z t−1 , z t )
Prior state:
ẑt ∼ p ϕ ′ (ẑ t | z t−1 ) Reconstruction: ôt ∼ p ϕ ′ (ô t | z t )
Isotropic unit Gaussian: p(z) = N (0, I).</p>
<p>(
)1
where ϕ ′ denotes the parameters of the model.The β-VAEbased video prediction model is trained to minimize the fol-1 β-VAE [14] is a variant of autoencoder that introduces a hyperparameter β to balance the reconstruction quality and disentanglement capability.lowing loss function:
L(ϕ ′ ) = Eq ϕ ′ T t=1 − ln p ϕ ′ (ot | zt) image reconstruction + β1KL [q ϕ ′ (zt | zt−1, ot) ∥p ϕ ′ (ẑt | zt−1)] action-free KL loss +β2KL [q ϕ ′ (zt | ot) ∥ p(zt)] disentanglement loss . (2)
The variantional posterior distribution q ϕ ′ (z t | o t ) is encouraged to be close to the standard multivariate Gaussian distribution N (0, I) to strengthen the orthogonality and disentanglement of the latent space.The importance of the disentanglement loss term is governed by β 2 .</p>
<p>Offline-to-Online Latent Distillation</p>
<p>After the offline pretraining with the distracting videos, the model is fine-tuned online to adapt to the downstream task by integrating actions and rewards (Lines 13 of Alg. 1).A straightforward approach to transfer disentangled features involves initializing the action-conditioned world model with checkpoints obtained from the pretrained video prediction model.Nevertheless, it may experience a potential mismatch issue caused by the discrepancies between the two domains in visual appearances and physical dynamics.Directly applying the pretraining-finetuning paradigm for downstream tasks tends to overwrite the disentangled information encoded in the pretrained latent features, leading to decreased performance when there are large domain discrepancies between the source and the target domains.</p>
<p>Through comprehensive pretraining on distracting videos that contain diverse visual variations, the video prediction model thus builds an interpretable and orthogonality latent space.In this space, the latent variable z disen achieves a high degree of disentanglement.To exploit the prior semantic knowledge from the pretrained model and improve the sample efficiency of the downstream tasks, we introduce an offline-to-online latent distillation.This approach enables the disentangling capability of z disen from the pretrained model to be effectively transferred to the latent variable z task of the world model.Specifically, this is achieved by minimizing the Kullback-Leibler (KL) divergence between the latent distributions of the two domains.The corresponding distillation loss L distill can be formulated as follows:
L distill = KL (z disen ∥z task ) = z disen • log z disen z task (3)</p>
<p>Disentangled World Model Adaptation</p>
<p>By obtaining well-disentangled representations of z disen and employing the latent distillation for knowledge transfer, we then propose a DRL-based world model M ϕ , designed to harness these features to enhance interoperability and robustness against environmental variations (Lines 14-15 of Alg. 1).The components of M ϕ can be detailed as follows:</p>
<p>Recurrent transition:
h t = f ϕ (h t−1 , z t−1 , a t−1 )
β-VAE encoder:
z t ∼ e ϕ (o t )
Posterior state:
z t ∼ q ϕ (z t | h t , z t )
Prior state:
ẑt ∼ p ϕ (ẑ t | h t ) Reconstruction: ôt ∼ p ϕ (ô t | h t , z t ) Reward prediction: rt ∼ r ϕ (r t | h t , z t ) Discount factor: γt ∼ p ϕ (γ t | h t , z t ) Isotropic unit Gaussian: p(z) = N (0, I),(4)
where ϕ represents the combined parameters of the world model.We train M ϕ on the sampled data from the replay buffer B with the following loss function:
L(ϕ) = Eq ϕ T t=1 − ln p ϕ (ot | ht, zt) image reconstruction − ln r ϕ (rt | ht, zt) reward prediction − ln p ϕ (γt | ht, zt) discount prediction +α KL [q ϕ (zt | ht, ot) ∥ p ϕ (ẑt | ht)] KL divergence +βKL [q ϕ (zt | ot) ∥ p(zt)] disentanglement loss + ηLdistill distillation loss . (5)
where β is a hyperparameter used to balance reconstruction quality and disentanglement capability.In this adaptation stage, η serves as a hyperparameter that gradually decreases from 0.1 to 0.01.Intuitively, η controls the progressive adaptation of the world model with the shared world knowledge transfer from the pretrained video prediction model.Through the comprehensive training processes of this framework, we equip the DisWM with the capability to learn and understand the underlying semantic representations.This enhancement enables the model to be less sensitive to environmental variations, such as changes in object colors, positions, and backgrounds.Furthermore, by incorporating actions and rewards during the finetuning phase, the world model can generate data with more diverse representations, thereby improving the disentangled representation learning.For behavior learning, we utilize the actorcritic method that is in line with DreamerV2 [10] (Lines 16-18 of Alg. 1).For more details of behavior learning, please refer to Supplementary Material B.</p>
<p>Experiments</p>
<p>Experimental Setups</p>
<p>Benchmark We evaluate DisWM on DeepMind Control Suite (DMC) [34], MuJoCo Pusher [35], and Draw-erWorld [37].DMC is a widely adopted benchmark with comprehensive and flexible robotic-control tasks.For DMC benchmark, we use 5 tasks, i.e., Walker Walk, Cheetah Run, Hopper Stand, Finger Spin, Cartpole Swingup.In MuJoCo Pusher, a multi-jointed robotic arm is employed to manipulate a target cylinder (object).The goal is to move the object to a designated target position using the robot's end effector (fingertip).The agent receives the negative reward, which is a combination of three components: the distance between the fingertip and the target, the distance between the object and the target goal position, and a penalty for large actions.DrawerWorld is a modified Metaworld [43] benchmark designed to evaluate texture adaptability in manipulation tasks.It includes five additional textures from realistic photos and grid texture.During training, we initially employ the grid texture and then change to the wood texture midway, while adopting the metal texture exclusively for evaluation.The corresponding results on Draw-erWorld are reported in Supplementary Material C.2. Furthermore, the introduction of compared baselines is given in Supplementary Material A.</p>
<p>Implementation details.The visual observations of online finetuning stages are resized to 64 × 64 pixels.Inspired by APV [29], we build distracting video datasets with 1M frames using DreamerV2 [10] to interact with the environments with visual color distractors.This video datasets consist of the samples stored in the replay buffer throughout the Sample random minibatch {ot} T t=1 ∼ D.</p>
<p>6:</p>
<p>Obtain Gaussian prior zt from Isotropic unit Gaussian N (0, I).</p>
<p>7:</p>
<p>Pretrain the action-free video prediction model with disentanglement regularization by minimizing Eq. ( 2).8: end for 9: Train the random agent and collect a replay buffer B. 10: while not converged do Sample {(ot, at, rt)} T t=1 ∼ B.</p>
<p>13:</p>
<p>Distill the disentangled features to the world model using Eq. ( 3).▷ Offline-to-online latent distillation 14:</p>
<p>Obtain Gaussian prior zt from Isotropic unit Gaussian N (0, I).▷ Disentangled world model adaptation 15:</p>
<p>Train the world model M ϕ with latent distillation and disentanglement constraints using Eq. ( 5).Sample ât ∼ π ψ (ât | ẑt).</p>
<p>23:</p>
<p>rt, ot+1 ← env.step(ât).</p>
<p>24:</p>
<p>end for</p>
<p>25:</p>
<p>Append data to the replay buffer B. 26: end while training process until the agent achieves maximum score.For tasks in the DMC benchmark, the training steps of the agent are limited to 1 × 10 6 environment steps.Each run of DisWM requires roughly 5GB of VRAM and takes around 16 hours to train on a single RTX 3090 GPU.The dimensions of well-disentangled latent z disen and downstream task latent z task are both set to 20 in our approach.In Figure 3, we showcase the example observations of various tasks with color distractors.We train the agent using a fixed set of colors, where the RGB values are varied within a restricted range around the original values.Furthermore, at the midpoint of the training process, we change to a different color scheme for varying distractors.</p>
<p>Main Comparison</p>
<p>We evaluate the sample efficiency and task performance for all the methods with the training curves of episode return.Figure 4 shows the performance of DisWM and all the baselines.Remarkably, it achieves better performance than TED [7], a method on top of RAD [17] and tailored for environments with distractors.For the offline-to-online finetuning models, DV2 Finetune achieves the second-best performance by transferring knowledge from the distracting videos.However, we observe a significant decline in sample efficiency, particularly in scenarios with large data distribution shifts between the source and target domains (e.g., DMC → MuJoCo).These shifts can occur in various aspects, including visual observation, physical dynamics, reward definition, or the action space of the robots.Another crucial baseline is APV [29], which focuses on transferring knowledge obtained from videos with a stacked latent prediction model.Nevertheless, without environment-specific designs for visual distractors, directly training may eventually result in a decrease in performance in the downstream tasks.The CURL model struggles to learn effective behavior policies, especially in Hopper Stand.Additional results on the challenging DMC Humanoid Walk can be found in Supplementary Material C.1.</p>
<p>Additionally, we present qualitative results in Figure 5 and Figure 6. Figure 5 shows the traversals of β-VAE during the pretraining phase.In each row of traversals, a distinct attribute varies, while other attributes remain constant, indicating that the pretrained model has successfully disentangled and learned this attribute, thereby improving the sample efficiency of the RL agent.Figure 6 displays the fine-grained disentanglement results on MuJoCo Pusher during the finetuning phase, demonstrating that the world model can effectively disentangle the variations.</p>
<p>Model Analyses</p>
<p>Ablation studies.We conduct ablation studies to validate the effect of the latent distillation and disentanglement con-</p>
<p>Related Work</p>
<p>Visual MBRL.Visual RL learns control policies from raw pixels, which has achieved remarkable performance in various tasks [3,4,31,37], while prior RL studies focus on learning policies from low-dimensional states.Extant approaches can be divided into two main directions: model-free RL [18,19,24,32,42,44,47] and model-based RL [1, 8-10, 12, 20, 21, 23, 27, 30, 36, 46].The following methods specifically address the variations of environments in visual MBRL.Pan [28] et al. decompose visual dynamics into controllable and uncontrollable states through the optimization of inverse dynamics.SeeX [16] proposed a bilevel optimization framework that adopts a separated world model and maximizes the task-relevant uncertainty.Orthogonal to these studies, our approach employs a DRL-based world model to alleviate the issue of visual variations.</p>
<p>Transfer RL To facilitate the learning of unseen tasks, transfer RL [22,24,26,33,36,48] leverages the knowledge learned from past tasks.One promising way is to transfer world knowledge from accessible videos to improve the downstream control.APV [29] established a pretraining-finetuning framework with a stacked latent prediction model and video-based intrinsic bonus.IPV [39] introduced contextualized world models that pretrained on diverse in-the-wild videos.It incorporates a context encoder that works alongside the latent dynamics model into the image encoder to capture rich contextual information.PreLAR [45] pretrained the world model with the derived meaningful actions from the action-free video using an inverse dynamics encoder.Different from these approaches, we propose a new solution to transfer world knowledge from distracting videos to improve the learning efficiency of downstream tasks via offline-to-online latent distillation.</p>
<p>Conclusions and Limitations</p>
<p>In this paper, we present a transfer RL dubbed DisWM, which addresses the challenge of environment variations in practical scenarios.Our key insight is to leverage the accessible distracting videos to facilitate the sample efficiency of downstream tasks to offer flexible disentanglement constraints.Specifically, we introduce disentangled representation pretraining, offline-to-online latent distillation, and disentangled world model adaptation to improve the downstream control.DisWM demonstrates superior performance than existing visual RL baselines across various benchmarks.One limitation of our approach is that disentangled representation learning encounters challenges in complex environments.Exploring the non-stationary environments with more intricate variations, such as time-varying background video distractions, could further highlight the potential of our approach for practical scenarios.</p>
<p>Figure 1 .
1
Figure 1.Overview of our proposed framework.The key idea is to leverage distracting videos for semantic knowledge transfer, enabling the downstream agent to improve sample efficiency on unseen tasks.</p>
<p>): a) Disentangled representation pretraining: Pretrain a DRL-based video prediction model from distracting videos to extract disentangled features.b) Offline-to-online latent distillation: Transfer the semantic knowledge from the pretrained model to the world model via cross-domain latent distillation.c) Disentangled world model adaptation: Finetune the downstream agent with disentanglement constraints by incorporating the action and reward information.</p>
<p>Figure 2 .
2
Figure 2. Architecture of Disentangled World Models.(a) The action-free video prediction model with disentanglement constraints is pretrained on distracting videos offline for the well-disentangled latent variable zdisen, which extracts semantic knowledge from the visual observations.The disentangled capability of zdisen is then transferred to the world model through latent distillation.(b) The action-conditioned world model is finetuned through disentanglement regularization online, which encourages the factorized representation.Moreover, the incorporation of actions and rewards enriches the diversity of the data, which in turn strengthens the disentangled representation learning.</p>
<p>Algorithm 1 1 :
11
The training pipeline of DisWM.Hyperparameters: H: Horizon of latent imagination.2: Require: Distracting video dataset D. 3: Initialize: Parameters of the model {ϕ, ψ, ξ}.4: for training step t = 1, 2, . . ., K1 do ▷ Disentangled representation pretraining 5:</p>
<p>11 :
11
for training step t = 1, 2, . . ., K2 do 12:</p>
<p>Figure 3 .
3
Figure 3. Example image observations of our modified DMC and MuJoCo Pusher with color distractors.</p>
<p>Figure 4 .Figure 5 .Figure 6 .Figure 7 .Figure 8 .
45678
Figure 4. Comparison of DisWM against visual RL baselines, including DreamerV2 [10], APV [29], DV2 Finetune, TED [7], CURL [18].</p>
<p>Table 1 .
1
MuJoCo (downstream domain) vs. DMC (accessible distracting videos).
Video: DMCTarget: MuJoCoSimilarity / DifferenceTaskReacher EasyPusherRelevant robotic control tasksDynamicsTwo-link planarMulti-jointed robot armDifferentAction spaceBox(-1, 1, (2,), float32) Box(-2, 2, (7,), float32)DifferentReward range[0, 1][-4.49, 0]Different
Acknowledgements.This work was supported by Grants of NSFC 62302246 &amp; 62250062, ZJNSFC LQ23F010008, Ningbo 2023Z237 &amp; 2024Z284 &amp; 2024Z289 &amp; 2023CX050011 &amp; 2025Z038, the Smart Grid National Science and Technology Major Project (2024ZD0801200), the Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102), the Fundamental Research Funds for the Central Universities, and the IDT Foundation of Youth Doctoral Innovation (S203.2.01.32.002).Additional support was provided by the project of Supporting Program for Young and Middle-aged Scientific and Technological Innovation Talents in Shenyang (Grant RC210488), the project of Provincial Doctoral Research Initiation Fund Program (Grant 2023-BS-214), the High Performance Computing Center at Eastern Institute of Technology, Ningbo, and Ningbo Institute of Digital Twin.Disentangled World Models: Learning to Transfer Semantic Knowledge from Distracting Videos for Reinforcement LearningSupplementary MaterialA. Compared BaselinesWe compare DisWM with strong visual RL agents, including • DreamerV2[10]: A model-based RL (MBRL) approach that trains world model and learns by imagining future latent states.• APV[29]: It learns informational representations via action-free pretraining on videos and finetunes the agent with learned representations in the downstream tasks with action.• DV2 Finetune: It pretrains a DreamerV2 agent[10]on distracting videos and then finetunes the trained model in the downstream tasks.Note that some tasks have different action spaces, which makes it difficult to finetune directly.Therefore, the action space of two tasks is set as the maximum action space of both environments.• TED[7]: It adopts a classification task to learn temporally disentangled representations in visual RL.• CURL[18]: A model-free RL method that employs contrastive learning to improve its sample efficiency.B. Behavior LearningFor the behavior learning of DisWM, we adopt the actorcritic method following DreamerV2[10].Concretely, the actor and critic are both implemented as MLPs with ELU activations[5].Formally, the actor and critic are defined as below:The actor π ψ is optimized by maximizingWe train the critic v ξ by minimizingwhere sg is a stop gradient operator.The λ-target V t that involves a weighted average of reward information used in Eq. (7) and Eq. (8) is defined as:where H is the imagination horizon.Notably, the disentangled world model is not optimized during behavior learning.C. Additional ResultsC.1. Results on DMCWe compare the performance of DreamerV3[12], TD-MPC2[13], ContextWM[39], and our approach on DMC.As shownC.2. Results on DrawerWorldWe present results on DrawerWorld[37]C.3. Sensitivity of the Latent Space DimensionWe visualize sensitivity analyses on the latent space dimension in Figure I.We observe that when z dim for the β-VAE is too small, it impedes the learning of disentangled representations, leading to a decline in performance.C.4. Runtime ComparisonsWe provide the detailed runtime and parameter comparisons with baselines in TableC.Note that the inference time is computed for one episode.C.5. Sample Diversity VisualizationThe adaptation stage enriches the sample diversity, as shown in FigureJ, for Cheetah Run → Walker Walk, we sample 200 video clips of length 50 and visualize the corresponding latent features using t-SNE[25].We find that the latent features of the online interactions are more diverse than those of the offline dataset.D. HyperparametersThe final hyperparameters of DisWM are reported in Table D.Cheetah Run Walker Walk
Tim Pearce, and Franc ¸ois Fleuret. Diffusion for world modeling: Visual details matter in atari. Eloi Alonso, Adam Jelley, Vincent Micheli, Anssi Kanervisto, Amos Storkey, NeurIPS2024</p>
<p>Representation learning: A review and new perspectives. Yoshua Bengio, Aaron Courville, Pascal Vincent, TPAMI. 3582013</p>
<p>Environment agnostic representation for visual reinforcement learning. Hyesong Choi, Hunsang Lee, Seongwon Jeong, Dongbo Min, ICCV. 2023</p>
<p>Local-guided global: Paired similarity representation for visual reinforcement learning. Hyesong Choi, Hunsang Lee, Wonil Song, Sangryul Jeon, Kwanghoon Sohn, Dongbo Min, CVPR. 2023</p>
<p>Fast and accurate deep network learning by exponential linear units (elus). Djork-Arné Clevert, Thomas Unterthiner, Sepp Hochreiter, arXiv:1511.072892015arXiv preprint</p>
<p>Conditional mutual information for disentangled representations in reinforcement learning. Mhairi Dunion, Trevor Mcinroe, Kevin Sebastian Luck, Josiah Hanna, Stefano Albrecht, NeurIPS. 20231</p>
<p>Temporal disentanglement of representations for improved generalisation in reinforcement learning. Mhairi Dunion, Trevor Mcinroe, Kevin Sebastian Luck, Josiah P Hanna, Stefano V Albrecht, ICLR. 2023. 1, 2, 5, 6</p>
<p>Learning latent dynamics for planning from pixels. Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, James Davidson, ICML. 2019</p>
<p>Dream to control: Learning behaviors by latent imagination. Danijar Hafner, Timothy Lillicrap, Jimmy Ba, Mohammad Norouzi, ICLR2020</p>
<p>Mastering atari with discrete world models. Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, Jimmy Ba, ICLR, 2021. 4, 6. 81</p>
<p>Danijar Hafner, Kuang-Huei Lee, Ian Fischer, Pieter Abbeel, arXiv:2206.04114Deep hierarchical planning from pixels. 2022arXiv preprint</p>
<p>Mastering diverse domains through world models. Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, Timothy Lillicrap, Nature. 812025</p>
<p>Td-mpc2: Scalable, robust world models for continuous control. Nicklas Hansen, Hao Su, Xiaolong Wang, ICLR. 2024</p>
<p>beta-vae: Learning basic visual concepts with a constrained variational framework. Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, Alexander Lerchner, ICLR201723</p>
<p>Darla: Improving zero-shot transfer in reinforcement learning. Irina Higgins, Arka Pal, Andrei Rusu, Loic Matthey, Christopher Burgess, Alexander Pritzel, Matthew Botvinick, Charles Blundell, Alexander Lerchner, ICML. 2017</p>
<p>Leveraging separated world model for exploration in visually distracted environments. Kaichen Huang, Shenghua Wan, Minghao Shao, Hai-Hang Sun, Le Gan, Shuai Feng, De-Chuan Zhan, NeurIPS. 82024</p>
<p>Reinforcement learning with augmented data. Misha Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, Aravind Srinivas, NeurIPS2020</p>
<p>Curl: Contrastive unsupervised representations for reinforcement learning. Michael Laskin, Aravind Srinivas, Pieter Abbeel, ICML. 2020. 1, 6, 8</p>
<p>Generalizing consistency policy to visual rl with prioritized proximal experience regularization. Haoran Li, Zhennan Jiang, Yuhui Chen, Dongbin Zhao, NeurIPS. 2024</p>
<p>Open-world reinforcement learning over long short-term imagination. Jiajian Li, Qi Wang, Yunbo Wang, Xin Jin, Yang Li, Wenjun Zeng, Xiaokang Yang, ICLR. 20251</p>
<p>Learning to model the world with language. Jessy Lin, Yuqing Du, Olivia Watkins, Danijar Hafner, Pieter Abbeel, Dan Klein, Anca Dragan, ICML. 2024</p>
<p>Structured state space models for in-context reinforcement learning. Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh, Feryal Behbahani, NeurIPS. 82023</p>
<p>Harmonydream: Task harmonization inside world models. Haoyu Ma, Jialong Wu, Ningya Feng, Chenjun Xiao, Dong Li, Jianye Hao, Jianmin Wang, Mingsheng Long, ICML. 2024</p>
<p>Vip: Towards universal visual reward and representation via value-implicit pre-training. Jason Yecheng, Shagun Ma, Dinesh Sodhani, Osbert Jayaraman, Vikash Bastani, Amy Kumar, Zhang, ICLR, 2023. 8</p>
<p>Visualizing data using t-sne. Laurens Van Der Maaten, Geoffrey Hinton, JMLR. 92Nov. 2008</p>
<p>Choreographer: Learning and adapting skills in imagination. Pietro Mazzaglia, Tim Verbelen, Bart Dhoedt, Alexandre Lacoste, Sai Rajeswar, ICLR, 2023. 8</p>
<p>Genrl: Multimodalfoundation world models for generalization in embodied agents. Pietro Mazzaglia, Tim Verbelen, Bart Dhoedt, Aaron C Courville, Sai Rajeswar, Mudumba , NeurIPS. 82024</p>
<p>Iso-dream: Isolating and leveraging noncontrollable visual dynamics in world models. Minting Pan, Xiangming Zhu, Yunbo Wang, Xiaokang Yang, NeurIPS. 20221</p>
<p>Reinforcement learning with action-free pretraining from videos. Younggyo Seo, Kimin Lee, Stephen L James, Pieter Abbeel, ICML. 2022. 4, 5, 6, 8, 1</p>
<p>Multi-view masked world models for visual robotic manipulation. Younggyo Seo, Junsu Kim, Stephen James, Kimin Lee, Jinwoo Shin, Pieter Abbeel, ICML. 2023</p>
<p>A simple framework for generalization in visual rl under dynamic scene perturbations. Wonil Song, Hyesong Choi, Kwanghoon Sohn, Dongbo Min, NeurIPS. 3782024</p>
<p>Decoupling representation learning from reinforcement learning. Adam Stooke, Kimin Lee, Pieter Abbeel, Michael Laskin, ICML. 2021</p>
<p>Transfer rl across observation feature spaces via model-based regularization. Yanchao Sun, Ruijie Zheng, Xiyao Wang, Andrew Cohen, Furong Huang, ICLR, 2022. 8</p>
<p>Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego De Las, David Casas, Budden, arXiv:1801.00690Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. 2018arXiv preprint</p>
<p>Mujoco: A physics engine for model-based control. Emanuel Todorov, Tom Erez, Yuval Tassa, IROS. 2012</p>
<p>Making offline rl online: Collaborative world models for offline visual reinforcement learning. Qi Wang, Junming Yang, Yunbo Wang, Xin Jin, Wenjun Zeng, Xiaokang Yang, NeurIPS2024</p>
<p>Unsupervised visual attention and invariance for reinforcement learning. Xudong Wang, Long Lian, Stella X Yu, CVPR. 202141</p>
<p>Disentangled representation learning. Xin Wang, Hong Chen, Zihao Wu, Wenwu Zhu, TPAMI. 22024</p>
<p>Pre-training contextualized world models with in-the-wild videos for reinforcement learning. Jialong Wu, Haoyu Ma, Chaoyi Deng, Mingsheng Long, NeurIPS. 812023</p>
<p>Navinerf: Nerf-based 3d representation disentanglement by latent semantic navigation. Baao Xie, Bohan Li, Zequn Zhang, Junting Dong, Xin Jin, Jingyu Yang, Wenjun Zeng, ICCV. 2023</p>
<p>Graph-based unsupervised disentangled representation learning via multimodal large language models. Baao Xie, Qiuyu Chen, Yunnan Wang, Zequn Zhang, Xin Jin, Wenjun Zeng, NeurIPS2024</p>
<p>Mastering visual continuous control: Improved dataaugmented reinforcement learning. Denis Yarats, Rob Fergus, Alessandro Lazaric, Lerrel Pinto, ICLR. 2022</p>
<p>Metaworld: A benchmark and evaluation for multi-task and meta reinforcement learning. Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, Sergey Levine, CoRL2019</p>
<p>Learning invariant representations for reinforcement learning without reconstruction. Amy Zhang, Rowan Mcallister, Roberto Calandra, Yarin Gal, Sergey Levine, ICLR. 2021</p>
<p>Prelar: World model pre-training with learnable action representation. Lixuan Zhang, Meina Kan, Shiguang Shan, Xilin Chen, ECCV. 20241</p>
<p>Storm: Efficient stochastic transformer based world models for reinforcement learning. Weipu Zhang, Gang Wang, Jian Sun, Yetian Yuan, Gao Huang, NeurIPS2023</p>
<p>Taco: Temporal latent action-driven contrastive loss for visual reinforcement learning. Ruijie Zheng, Xiyao Wang, Yanchao Sun, Shuang Ma, Jieyu Zhao, Huazhe Xu, Hal Daumé, Iii , Furong Huang, NeurIPS2023</p>
<p>Transfer learning in deep reinforcement learning: A survey. Zhuangdi Zhu, Kaixiang Lin, Anil K Jain, Jiayu Zhou, TPAMI. 45112023</p>            </div>
        </div>

    </div>
</body>
</html>
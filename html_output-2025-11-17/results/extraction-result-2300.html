<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2300 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2300</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2300</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-64.html">extraction-schema-64</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <p><strong>Paper ID:</strong> paper-251444259</p>
                <p><strong>Paper Title:</strong> Computational Scientific Discovery in Psychology</p>
                <p><strong>Paper Abstract:</strong> Scientific discovery is a driving force for progress involving creative problem-solving processes to further our understanding of the world. The process of scientific discovery has historically been intensive and time-consuming; however, advances in computational power and algorithms have provided an efficient route to make new discoveries. Complex tools using artificial intelligence (AI) can efficiently analyze data as well as generate new hypotheses and theories. Along with AI becoming increasingly prevalent in our daily lives and the services we access, its application to different scientific domains is becoming more widespread. For example, AI has been used for the early detection of medical conditions, identifying treatments and vaccines (e.g., against COVID-19), and predicting protein structure. The application of AI in psychological science has started to become popular. AI can assist in new discoveries both as a tool that allows more freedom to scientists to generate new theories and by making creative discoveries autonomously. Conversely, psychological concepts such as heuristics have refined and improved artificial systems. With such powerful systems, however, there are key ethical and practical issues to consider. This article addresses the current and future directions of computational scientific discovery generally and its applications in psychological science more specifically.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2300.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2300.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Robot Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Robot Scientist (King et al., 2004)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated system that generates hypotheses from observations, designs and runs experiments, interprets results relative to its hypotheses, and iterates the scientific cycle in functional genomics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Functional genomic hypothesis generation and experimentation by a robot scientist</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Robot Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Automated laboratory system that: (1) generates hypotheses from observed data, (2) plans experimental tests for those hypotheses, (3) executes experiments using robotics, (4) analyzes results to accept/reject hypotheses and repeat the loop. Underlying approach described in the paper: automated hypothesis generation + automated experimental design + robotic execution and interpretation (knowledge-driven automation; domain-specific rules and experiment-scheduling).</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>functional genomics / biology</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>targeted experimental hypothesis testing (automated scientific cycle)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td>Empirical validation via automated experiments (acceptability of hypotheses determined by experimental outcomes); hypotheses are validated/invalidated by running experiments designed by the system.</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Iterative experiment-driven testing: hypotheses are ranked/selected by their testability and experimental outcomes guide further generation (implicit experimental validation as feasibility filter).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td>System-generated hypotheses were experimentally tested by the system; examples reported in the literature include successful hypothesis generation and experimental confirmation in functional genomics (no numerical novelty/feasibility scores reported in this review).</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>Used in functional genomics where automation and high-throughput experimentation are feasible; system leverages domain experimental affordances to validate hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Computational Scientific Discovery in Psychology', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2300.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2300.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GEMS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GEMS (Genetically Evolving Models in Science)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A genetic-programming-based system that automatically generates candidate cognitive models by evolving combinations of cognitive 'operators' and selecting models that best fit human experimental data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GEMS: Genetically evolving models in science</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GEMS (Genetically Evolving Models in Science)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Extends genetic programming: constructs programs (models) by composing domain-specific cognitive operators (e.g., put item in short-term memory, shift attention). A population of candidate models is evolved across generations; fitness is determined by how closely a candidate model's predictions match experimental human data. Operators are combined across cognitive domains, producing novel model structures that can generate novel predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>cognitive psychology / cognitive modeling</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>exploratory model/theory generation (interdisciplinary synthesis across cognitive operators)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td>Model fitness measured as closeness of model predictions to human experimental data (data-fit / predictive accuracy acts as feasibility/acceptability filter).</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Evolutionary search under a fitness function that rewards predictive fit to empirical data; novelty implicitly encouraged by recombination of diverse operators but not explicitly scored in reviewed description.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td>GEMS has been reported to semiautomatically generate scientific theories that were compared against human experimental data for several psychology experiments; the review reports qualitative success (validity of approach) but does not provide numeric novelty/feasibility trade-off metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Standard human-driven theory development / domain experts</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Reported advantages include reduced confirmation bias and the ability to combine operators across fields to generate unexpected predictions; no quantitative comparison scores provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>Particularly useful in cognitive tasks with limited data because it does not require large datasets and can combine mechanisms from memory, attention, and decision-making to yield novel hypotheses about their interplay.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Computational Scientific Discovery in Psychology', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2300.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2300.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KEKADA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KEKADA (Kulkarni & Simon, 1988)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A program that makes theoretical inferences, assesses the acceptability of its knowledge and theories, and models experimental tests; used to replicate historical scientific discoveries such as the Urea cycle.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The processes of scientific discovery: The strategy of experimentation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>KEKADA</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Computational system for theoretical inference and evaluation: generates causal hypotheses/models, assesses their acceptability, and simulates/modulates experimental testing strategies to adjudicate among hypotheses. Described as using heuristics and model-formation rules to search problem spaces and model experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>theoretical biology / general scientific discovery (historical case replication)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>replicative reconstruction of past scientific discoveries; targeted explanatory hypothesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td>Acceptability assessed via internal criteria and by modeling experimental tests (i.e., hypotheses judged by whether they would produce/fit available empirical results).</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Heuristic search through problem spaces with rules for model formation and experimental evaluation (no explicit novelty-feasibility optimization reported).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td>KEKADA was able to replicate Krebs's discovery of the Urea cycle when assessed against historical data and modeled experiments; the review reports this qualitative success without numeric novelty/feasibility measures.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>Demonstrated that domain-specific heuristics and modeling of experiments can replicate historically important biochemical discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Computational Scientific Discovery in Psychology', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2300.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2300.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DENDRAL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DENDRAL</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An early domain-specific AI system that inferred molecular structures by analyzing mass spectrometry data using domain heuristics and constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Applications of artificial intelligence for organic chemistry: The DENDRAL project</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DENDRAL</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Knowledge-based system for structure generation: uses domain-specific chemical knowledge and heuristics to generate candidate molecular structures consistent with mass spectrometry data, scoring candidates by consistency with observed spectra and chemical constraints; essentially generates structural hypotheses about molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>organic chemistry / computational chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>targeted hypothesis generation (structure identification)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td>Feasibility is operationalized as consistency of candidate structures with mass spectrometry observations and chemical constraints (data-fit score).</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Domain-knowledge-driven search with constraint satisfaction and scoring against spectral data (no explicit novelty scoring reported).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td>DENDRAL produced candidate structures that matched experimental data and provided chemically plausible explanations; the review notes its historical success but does not report quantified novelty/feasibility trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Human chemists' structure elucidation</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Historically effective and influential; review does not report numerical comparative metrics here.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>Highly effective where rich domain constraints and analytical data are available to validate generated hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Computational Scientific Discovery in Psychology', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2300.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2300.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Logic Theorist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logic Theorist (Newell et al., 1958)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>One of the earliest AI programs that generated proofs for propositional logic theorems using heuristics; considered a programmatic model of creative problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Logic Theorist</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Heuristic-based theorem-prover that searches the problem space of logical proofs using domain heuristics to generate candidate proofs; some generated proofs were judged more elegant than human-generated ones.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>mathematics / automated theorem proving</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>targeted symbolic discovery (theorem proving)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td>Feasibility judged by ability to produce correct proofs; heuristic selection favored proofs considered elegant/parsimonious.</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Heuristic-guided search optimizing for proof success and parsimony; no explicit novelty-feasibility trade-off metric reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td>Some program-generated proofs were considered more elegant than those by leading mathematicians, demonstrating machine creative output; no numeric novelty/feasibility metrics provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Human mathematicians' proofs</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Qualitative claim of some proofs being more elegant than human counterparts; no quantitative evaluation in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>Shows heuristic problem-space search can yield creative outcomes in symbolic domains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Computational Scientific Discovery in Psychology', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2300.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2300.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Automatic generation using Genetic Programming (Frias-Martinez & Gobet)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic generation of cognitive theories using genetic programming</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of genetic programming to evolve computational cognitive theories/models automatically by optimizing model fit to data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automatic generation of cognitive theories using genetic programming</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Genetic programming for theory generation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Uses evolutionary computation: a population of programmatic models (candidate theories) is evolved via mutation and recombination; a fitness function (e.g., predictive accuracy against experimental data) selects better models across generations, producing candidate cognitive theories automatically.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>cognitive science / psychology</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>exploratory model discovery and functional-form search</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td>Fitness to empirical data (predictive accuracy / data-fit) serves as primary feasibility metric; evolutionary selection enforces empirical plausibility.</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Evolutionary multi-step search under a fitness function focused on data-fit; novelty arises from recombination but is not explicitly scored in the reviewed description.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td>Reported to generate cognitive theories that match experimental data; review gives qualitative validation but no numeric novelty/feasibility trade-off results.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Standard analytic / human-derived cognitive models</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Demonstrated ability to rediscover known models and discover novel functional forms; no numerical comparisons provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>Effective for discovering nonlinear interactions and model functional forms in psychology experiments, even with modest dataset sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Computational Scientific Discovery in Psychology', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2300.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2300.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NINSUN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NINSUN (Lara-Dammer et al., 2019)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated system emulating human perception based on theories of discovery and perception that is able to produce scientific hypotheses in a simple artificial world.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A computational model of scientific discovery in a very simple world, aiming at psychological realism</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>NINSUN</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cognitive-simulation-based system that models human perception and discovery processes in a constrained artificial environment; generates hypotheses by simulating perceptual and inferential mechanisms and evaluating candidate explanations within the simple world.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>computational cognitive science / artificial-world discovery</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>open-ended exploratory hypothesis generation in toy/artificial environments</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td>Feasibility judged by internal coherence within the simulated world and by whether generated hypotheses explain observed simulated phenomena; evaluated in the context of the toy domain.</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Search and simulation informed by cognitive-theory priors; no explicit novelty-feasibility optimization described in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td>Reported to be able to make scientific hypotheses in the simple artificial world; evaluation is qualitative within the simulated domain and no numeric novelty/feasibility trade-offs are reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>Designed to probe psychologically realistic discovery processes in simplified settings rather than to discover real-world scientific hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Computational Scientific Discovery in Psychology', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2300.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2300.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scientific Regret Minimization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scientific Regret Minimization (Agrawal et al., 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative method that critiques a simple, interpretable scientific model against an unconstrained machine-learning model to find residuals that may indicate novel effects, which are then validated experimentally.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scaling up psychology via Scientific Regret Minimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Scientific Regret Minimization</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Iterative critique loop: (1) train an unconstrained ML model on large data, (2) compare predictions of a simple/interpretable psychological model to the ML model, (3) identify residuals (where simple model fails relative to ML) as candidate novel effects, (4) update/refine the simple model and repeat until convergence; residuals are then validated in separate experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>psychology / behavioral science</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>discovering novel predictive effects and refining interpretable theories from large datasets</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>Residuals between simple model predictions and unconstrained ML model predictions are treated as signals of novelty (i.e., unexplained variance by the simple model).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td>Feasibility assessed by validating residual-derived hypotheses in separate experiments (empirical validation); also by convergence of predictions between simple and ML models.</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Iterative refinement: minimize 'regret' (prediction gap) between simple model and ML model while maintaining interpretability, thereby implicitly balancing novelty (residuals) and feasibility (empirical validation and interpretability).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td>Method demonstrated feasibility: iterative critique yields residuals used to propose novel effects which are then validated experimentally; the review reports the method but does not provide numeric novelty/feasibility trade-off statistics here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Simple interpretable psychological models vs unconstrained machine-learning models</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Approach aims to combine ML predictive power with interpretable models; review reports qualitative success in identifying residuals for follow-up validation but no numeric comparisons in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>Useful in psychology for scaling up interpretable models using large datasets and ML while preserving testable, interpretable hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Computational Scientific Discovery in Psychology', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2300.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2300.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Peterson et al. ML approach</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Using large-scale experiments and machine learning to discover theories of human decision-making (Peterson et al., 2021)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale experimental + machine-learning pipeline that trained multiple ML models (including neural networks) on extensive behavioral data, compared them to theory-driven models, and extracted a new, interpretable theory of risky choice as a mixture of prior theories with context-dependent parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Using large-scale experiments and machine learning to discover theories of human decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Large-scale experiments + machine learning pipeline (Peterson et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Collect very large behavioral datasets, train progressively more complex ML models (including unconstrained neural networks) to maximize predictive performance, compare these to theory-based models, and use insights from ML (including optimized functional forms and parameters) to derive new interpretable theories; approach leverages ML to search functional form space and reveal mixtures of prior theories.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>decision-making / behavioral economics / psychology</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>data-driven theory discovery for human decision-making (open-ended discovery constrained by experimental tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>Improvements in predictive performance / variance explained relative to prior human-generated models serve as indicators of novelty (i.e., novel theory explains variance previous models could not).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td>Predictive accuracy on held-out data (cross-validation) and ability to be expressed as an interpretable theory that can be experimentally tested.</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td>Qualitative evidence: models of increased complexity explained more variance; derived interpretable theory is a context-dependent mixture of existing theories, suggesting a balance between predictive novelty and interpretability/feasibility.</td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Model-comparison-driven discovery: use ML to find structures that improve predictive fit, then distill interpretable theories (iterative refinement from unconstrained ML to interpretable theory).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td>Authors report that widely-used theory-based models could not explain large amounts of variance even after tuning; ML-derived theory explained more variance and produced novel predictions (no precise numerical values reported in this review).</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Human-generated theories (subjective-utility models, prospect theory, and other prior models)</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Reported that prior models could not account for large amounts of variance, while ML approaches revealed better-performing functional forms and mixtures; the review summarizes qualitatively without numeric comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>In risky-choice domain, discovered that people use sophisticated mixed strategies whose functional form and parameters depend on gamble details (max/min outcomes, variability), indicating context-dependent theory structure.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Computational Scientific Discovery in Psychology', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Functional genomic hypothesis generation and experimentation by a robot scientist <em>(Rating: 2)</em></li>
                <li>Automatic generation of cognitive theories using genetic programming <em>(Rating: 2)</em></li>
                <li>GEMS: Genetically evolving models in science <em>(Rating: 2)</em></li>
                <li>A computational model of scientific discovery in a very simple world, aiming at psychological realism <em>(Rating: 2)</em></li>
                <li>Scaling up psychology via Scientific Regret Minimization <em>(Rating: 2)</em></li>
                <li>Using large-scale experiments and machine learning to discover theories of human decision-making <em>(Rating: 2)</em></li>
                <li>Applications of artificial intelligence for organic chemistry: The DENDRAL project <em>(Rating: 2)</em></li>
                <li>Automatic discovery of scientific concepts: Replicating three recent discoveries in mechanics <em>(Rating: 1)</em></li>
                <li>The processes of scientific discovery: The strategy of experimentation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2300",
    "paper_id": "paper-251444259",
    "extraction_schema_id": "extraction-schema-64",
    "extracted_data": [
        {
            "name_short": "Robot Scientist",
            "name_full": "Robot Scientist (King et al., 2004)",
            "brief_description": "An automated system that generates hypotheses from observations, designs and runs experiments, interprets results relative to its hypotheses, and iterates the scientific cycle in functional genomics.",
            "citation_title": "Functional genomic hypothesis generation and experimentation by a robot scientist",
            "mention_or_use": "mention",
            "system_name": "Robot Scientist",
            "system_description": "Automated laboratory system that: (1) generates hypotheses from observed data, (2) plans experimental tests for those hypotheses, (3) executes experiments using robotics, (4) analyzes results to accept/reject hypotheses and repeat the loop. Underlying approach described in the paper: automated hypothesis generation + automated experimental design + robotic execution and interpretation (knowledge-driven automation; domain-specific rules and experiment-scheduling).",
            "research_domain": "functional genomics / biology",
            "problem_type": "targeted experimental hypothesis testing (automated scientific cycle)",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": "Empirical validation via automated experiments (acceptability of hypotheses determined by experimental outcomes); hypotheses are validated/invalidated by running experiments designed by the system.",
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": "Iterative experiment-driven testing: hypotheses are ranked/selected by their testability and experimental outcomes guide further generation (implicit experimental validation as feasibility filter).",
            "human_evaluation": true,
            "human_evaluation_results": "System-generated hypotheses were experimentally tested by the system; examples reported in the literature include successful hypothesis generation and experimental confirmation in functional genomics (no numerical novelty/feasibility scores reported in this review).",
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": "Used in functional genomics where automation and high-throughput experimentation are feasible; system leverages domain experimental affordances to validate hypotheses.",
            "uuid": "e2300.0",
            "source_info": {
                "paper_title": "Computational Scientific Discovery in Psychology",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "GEMS",
            "name_full": "GEMS (Genetically Evolving Models in Science)",
            "brief_description": "A genetic-programming-based system that automatically generates candidate cognitive models by evolving combinations of cognitive 'operators' and selecting models that best fit human experimental data.",
            "citation_title": "GEMS: Genetically evolving models in science",
            "mention_or_use": "mention",
            "system_name": "GEMS (Genetically Evolving Models in Science)",
            "system_description": "Extends genetic programming: constructs programs (models) by composing domain-specific cognitive operators (e.g., put item in short-term memory, shift attention). A population of candidate models is evolved across generations; fitness is determined by how closely a candidate model's predictions match experimental human data. Operators are combined across cognitive domains, producing novel model structures that can generate novel predictions.",
            "research_domain": "cognitive psychology / cognitive modeling",
            "problem_type": "exploratory model/theory generation (interdisciplinary synthesis across cognitive operators)",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": "Model fitness measured as closeness of model predictions to human experimental data (data-fit / predictive accuracy acts as feasibility/acceptability filter).",
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": "Evolutionary search under a fitness function that rewards predictive fit to empirical data; novelty implicitly encouraged by recombination of diverse operators but not explicitly scored in reviewed description.",
            "human_evaluation": true,
            "human_evaluation_results": "GEMS has been reported to semiautomatically generate scientific theories that were compared against human experimental data for several psychology experiments; the review reports qualitative success (validity of approach) but does not provide numeric novelty/feasibility trade-off metrics.",
            "comparative_baseline": "Standard human-driven theory development / domain experts",
            "comparative_results": "Reported advantages include reduced confirmation bias and the ability to combine operators across fields to generate unexpected predictions; no quantitative comparison scores provided in this review.",
            "domain_specific_findings": "Particularly useful in cognitive tasks with limited data because it does not require large datasets and can combine mechanisms from memory, attention, and decision-making to yield novel hypotheses about their interplay.",
            "uuid": "e2300.1",
            "source_info": {
                "paper_title": "Computational Scientific Discovery in Psychology",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "KEKADA",
            "name_full": "KEKADA (Kulkarni & Simon, 1988)",
            "brief_description": "A program that makes theoretical inferences, assesses the acceptability of its knowledge and theories, and models experimental tests; used to replicate historical scientific discoveries such as the Urea cycle.",
            "citation_title": "The processes of scientific discovery: The strategy of experimentation",
            "mention_or_use": "mention",
            "system_name": "KEKADA",
            "system_description": "Computational system for theoretical inference and evaluation: generates causal hypotheses/models, assesses their acceptability, and simulates/modulates experimental testing strategies to adjudicate among hypotheses. Described as using heuristics and model-formation rules to search problem spaces and model experiments.",
            "research_domain": "theoretical biology / general scientific discovery (historical case replication)",
            "problem_type": "replicative reconstruction of past scientific discoveries; targeted explanatory hypothesis generation",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": "Acceptability assessed via internal criteria and by modeling experimental tests (i.e., hypotheses judged by whether they would produce/fit available empirical results).",
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": "Heuristic search through problem spaces with rules for model formation and experimental evaluation (no explicit novelty-feasibility optimization reported).",
            "human_evaluation": true,
            "human_evaluation_results": "KEKADA was able to replicate Krebs's discovery of the Urea cycle when assessed against historical data and modeled experiments; the review reports this qualitative success without numeric novelty/feasibility measures.",
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": "Demonstrated that domain-specific heuristics and modeling of experiments can replicate historically important biochemical discoveries.",
            "uuid": "e2300.2",
            "source_info": {
                "paper_title": "Computational Scientific Discovery in Psychology",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "DENDRAL",
            "name_full": "DENDRAL",
            "brief_description": "An early domain-specific AI system that inferred molecular structures by analyzing mass spectrometry data using domain heuristics and constraints.",
            "citation_title": "Applications of artificial intelligence for organic chemistry: The DENDRAL project",
            "mention_or_use": "mention",
            "system_name": "DENDRAL",
            "system_description": "Knowledge-based system for structure generation: uses domain-specific chemical knowledge and heuristics to generate candidate molecular structures consistent with mass spectrometry data, scoring candidates by consistency with observed spectra and chemical constraints; essentially generates structural hypotheses about molecules.",
            "research_domain": "organic chemistry / computational chemistry",
            "problem_type": "targeted hypothesis generation (structure identification)",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": "Feasibility is operationalized as consistency of candidate structures with mass spectrometry observations and chemical constraints (data-fit score).",
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": "Domain-knowledge-driven search with constraint satisfaction and scoring against spectral data (no explicit novelty scoring reported).",
            "human_evaluation": true,
            "human_evaluation_results": "DENDRAL produced candidate structures that matched experimental data and provided chemically plausible explanations; the review notes its historical success but does not report quantified novelty/feasibility trade-offs.",
            "comparative_baseline": "Human chemists' structure elucidation",
            "comparative_results": "Historically effective and influential; review does not report numerical comparative metrics here.",
            "domain_specific_findings": "Highly effective where rich domain constraints and analytical data are available to validate generated hypotheses.",
            "uuid": "e2300.3",
            "source_info": {
                "paper_title": "Computational Scientific Discovery in Psychology",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "Logic Theorist",
            "name_full": "Logic Theorist (Newell et al., 1958)",
            "brief_description": "One of the earliest AI programs that generated proofs for propositional logic theorems using heuristics; considered a programmatic model of creative problem solving.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "Logic Theorist",
            "system_description": "Heuristic-based theorem-prover that searches the problem space of logical proofs using domain heuristics to generate candidate proofs; some generated proofs were judged more elegant than human-generated ones.",
            "research_domain": "mathematics / automated theorem proving",
            "problem_type": "targeted symbolic discovery (theorem proving)",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": "Feasibility judged by ability to produce correct proofs; heuristic selection favored proofs considered elegant/parsimonious.",
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": "Heuristic-guided search optimizing for proof success and parsimony; no explicit novelty-feasibility trade-off metric reported.",
            "human_evaluation": true,
            "human_evaluation_results": "Some program-generated proofs were considered more elegant than those by leading mathematicians, demonstrating machine creative output; no numeric novelty/feasibility metrics provided in this review.",
            "comparative_baseline": "Human mathematicians' proofs",
            "comparative_results": "Qualitative claim of some proofs being more elegant than human counterparts; no quantitative evaluation in this review.",
            "domain_specific_findings": "Shows heuristic problem-space search can yield creative outcomes in symbolic domains.",
            "uuid": "e2300.4",
            "source_info": {
                "paper_title": "Computational Scientific Discovery in Psychology",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "Automatic generation using Genetic Programming (Frias-Martinez & Gobet)",
            "name_full": "Automatic generation of cognitive theories using genetic programming",
            "brief_description": "Application of genetic programming to evolve computational cognitive theories/models automatically by optimizing model fit to data.",
            "citation_title": "Automatic generation of cognitive theories using genetic programming",
            "mention_or_use": "mention",
            "system_name": "Genetic programming for theory generation",
            "system_description": "Uses evolutionary computation: a population of programmatic models (candidate theories) is evolved via mutation and recombination; a fitness function (e.g., predictive accuracy against experimental data) selects better models across generations, producing candidate cognitive theories automatically.",
            "research_domain": "cognitive science / psychology",
            "problem_type": "exploratory model discovery and functional-form search",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": "Fitness to empirical data (predictive accuracy / data-fit) serves as primary feasibility metric; evolutionary selection enforces empirical plausibility.",
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": "Evolutionary multi-step search under a fitness function focused on data-fit; novelty arises from recombination but is not explicitly scored in the reviewed description.",
            "human_evaluation": true,
            "human_evaluation_results": "Reported to generate cognitive theories that match experimental data; review gives qualitative validation but no numeric novelty/feasibility trade-off results.",
            "comparative_baseline": "Standard analytic / human-derived cognitive models",
            "comparative_results": "Demonstrated ability to rediscover known models and discover novel functional forms; no numerical comparisons provided here.",
            "domain_specific_findings": "Effective for discovering nonlinear interactions and model functional forms in psychology experiments, even with modest dataset sizes.",
            "uuid": "e2300.5",
            "source_info": {
                "paper_title": "Computational Scientific Discovery in Psychology",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "NINSUN",
            "name_full": "NINSUN (Lara-Dammer et al., 2019)",
            "brief_description": "A simulated system emulating human perception based on theories of discovery and perception that is able to produce scientific hypotheses in a simple artificial world.",
            "citation_title": "A computational model of scientific discovery in a very simple world, aiming at psychological realism",
            "mention_or_use": "mention",
            "system_name": "NINSUN",
            "system_description": "Cognitive-simulation-based system that models human perception and discovery processes in a constrained artificial environment; generates hypotheses by simulating perceptual and inferential mechanisms and evaluating candidate explanations within the simple world.",
            "research_domain": "computational cognitive science / artificial-world discovery",
            "problem_type": "open-ended exploratory hypothesis generation in toy/artificial environments",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": "Feasibility judged by internal coherence within the simulated world and by whether generated hypotheses explain observed simulated phenomena; evaluated in the context of the toy domain.",
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": "Search and simulation informed by cognitive-theory priors; no explicit novelty-feasibility optimization described in this review.",
            "human_evaluation": true,
            "human_evaluation_results": "Reported to be able to make scientific hypotheses in the simple artificial world; evaluation is qualitative within the simulated domain and no numeric novelty/feasibility trade-offs are reported here.",
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": "Designed to probe psychologically realistic discovery processes in simplified settings rather than to discover real-world scientific hypotheses.",
            "uuid": "e2300.6",
            "source_info": {
                "paper_title": "Computational Scientific Discovery in Psychology",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "Scientific Regret Minimization",
            "name_full": "Scientific Regret Minimization (Agrawal et al., 2020)",
            "brief_description": "An iterative method that critiques a simple, interpretable scientific model against an unconstrained machine-learning model to find residuals that may indicate novel effects, which are then validated experimentally.",
            "citation_title": "Scaling up psychology via Scientific Regret Minimization",
            "mention_or_use": "mention",
            "system_name": "Scientific Regret Minimization",
            "system_description": "Iterative critique loop: (1) train an unconstrained ML model on large data, (2) compare predictions of a simple/interpretable psychological model to the ML model, (3) identify residuals (where simple model fails relative to ML) as candidate novel effects, (4) update/refine the simple model and repeat until convergence; residuals are then validated in separate experiments.",
            "research_domain": "psychology / behavioral science",
            "problem_type": "discovering novel predictive effects and refining interpretable theories from large datasets",
            "novelty_metric": "Residuals between simple model predictions and unconstrained ML model predictions are treated as signals of novelty (i.e., unexplained variance by the simple model).",
            "novelty_score": null,
            "feasibility_metric": "Feasibility assessed by validating residual-derived hypotheses in separate experiments (empirical validation); also by convergence of predictions between simple and ML models.",
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": "Iterative refinement: minimize 'regret' (prediction gap) between simple model and ML model while maintaining interpretability, thereby implicitly balancing novelty (residuals) and feasibility (empirical validation and interpretability).",
            "human_evaluation": true,
            "human_evaluation_results": "Method demonstrated feasibility: iterative critique yields residuals used to propose novel effects which are then validated experimentally; the review reports the method but does not provide numeric novelty/feasibility trade-off statistics here.",
            "comparative_baseline": "Simple interpretable psychological models vs unconstrained machine-learning models",
            "comparative_results": "Approach aims to combine ML predictive power with interpretable models; review reports qualitative success in identifying residuals for follow-up validation but no numeric comparisons in this review.",
            "domain_specific_findings": "Useful in psychology for scaling up interpretable models using large datasets and ML while preserving testable, interpretable hypotheses.",
            "uuid": "e2300.7",
            "source_info": {
                "paper_title": "Computational Scientific Discovery in Psychology",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "Peterson et al. ML approach",
            "name_full": "Using large-scale experiments and machine learning to discover theories of human decision-making (Peterson et al., 2021)",
            "brief_description": "A large-scale experimental + machine-learning pipeline that trained multiple ML models (including neural networks) on extensive behavioral data, compared them to theory-driven models, and extracted a new, interpretable theory of risky choice as a mixture of prior theories with context-dependent parameters.",
            "citation_title": "Using large-scale experiments and machine learning to discover theories of human decision-making",
            "mention_or_use": "mention",
            "system_name": "Large-scale experiments + machine learning pipeline (Peterson et al.)",
            "system_description": "Collect very large behavioral datasets, train progressively more complex ML models (including unconstrained neural networks) to maximize predictive performance, compare these to theory-based models, and use insights from ML (including optimized functional forms and parameters) to derive new interpretable theories; approach leverages ML to search functional form space and reveal mixtures of prior theories.",
            "research_domain": "decision-making / behavioral economics / psychology",
            "problem_type": "data-driven theory discovery for human decision-making (open-ended discovery constrained by experimental tasks)",
            "novelty_metric": "Improvements in predictive performance / variance explained relative to prior human-generated models serve as indicators of novelty (i.e., novel theory explains variance previous models could not).",
            "novelty_score": null,
            "feasibility_metric": "Predictive accuracy on held-out data (cross-validation) and ability to be expressed as an interpretable theory that can be experimentally tested.",
            "feasibility_score": null,
            "tradeoff_evidence": "Qualitative evidence: models of increased complexity explained more variance; derived interpretable theory is a context-dependent mixture of existing theories, suggesting a balance between predictive novelty and interpretability/feasibility.",
            "optimization_strategy": "Model-comparison-driven discovery: use ML to find structures that improve predictive fit, then distill interpretable theories (iterative refinement from unconstrained ML to interpretable theory).",
            "human_evaluation": true,
            "human_evaluation_results": "Authors report that widely-used theory-based models could not explain large amounts of variance even after tuning; ML-derived theory explained more variance and produced novel predictions (no precise numerical values reported in this review).",
            "comparative_baseline": "Human-generated theories (subjective-utility models, prospect theory, and other prior models)",
            "comparative_results": "Reported that prior models could not account for large amounts of variance, while ML approaches revealed better-performing functional forms and mixtures; the review summarizes qualitatively without numeric comparisons.",
            "domain_specific_findings": "In risky-choice domain, discovered that people use sophisticated mixed strategies whose functional form and parameters depend on gamble details (max/min outcomes, variability), indicating context-dependent theory structure.",
            "uuid": "e2300.8",
            "source_info": {
                "paper_title": "Computational Scientific Discovery in Psychology",
                "publication_date_yy_mm": "2022-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Functional genomic hypothesis generation and experimentation by a robot scientist",
            "rating": 2,
            "sanitized_title": "functional_genomic_hypothesis_generation_and_experimentation_by_a_robot_scientist"
        },
        {
            "paper_title": "Automatic generation of cognitive theories using genetic programming",
            "rating": 2,
            "sanitized_title": "automatic_generation_of_cognitive_theories_using_genetic_programming"
        },
        {
            "paper_title": "GEMS: Genetically evolving models in science",
            "rating": 2,
            "sanitized_title": "gems_genetically_evolving_models_in_science"
        },
        {
            "paper_title": "A computational model of scientific discovery in a very simple world, aiming at psychological realism",
            "rating": 2,
            "sanitized_title": "a_computational_model_of_scientific_discovery_in_a_very_simple_world_aiming_at_psychological_realism"
        },
        {
            "paper_title": "Scaling up psychology via Scientific Regret Minimization",
            "rating": 2,
            "sanitized_title": "scaling_up_psychology_via_scientific_regret_minimization"
        },
        {
            "paper_title": "Using large-scale experiments and machine learning to discover theories of human decision-making",
            "rating": 2,
            "sanitized_title": "using_largescale_experiments_and_machine_learning_to_discover_theories_of_human_decisionmaking"
        },
        {
            "paper_title": "Applications of artificial intelligence for organic chemistry: The DENDRAL project",
            "rating": 2,
            "sanitized_title": "applications_of_artificial_intelligence_for_organic_chemistry_the_dendral_project"
        },
        {
            "paper_title": "Automatic discovery of scientific concepts: Replicating three recent discoveries in mechanics",
            "rating": 1,
            "sanitized_title": "automatic_discovery_of_scientific_concepts_replicating_three_recent_discoveries_in_mechanics"
        },
        {
            "paper_title": "The processes of scientific discovery: The strategy of experimentation",
            "rating": 1,
            "sanitized_title": "the_processes_of_scientific_discovery_the_strategy_of_experimentation"
        }
    ],
    "cost": 0.01997575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ASSOCIATION FOR PSYCHOLOGICAL SCIENCE Computational Scientific Discovery in Psychology
2023</p>
<p>Laura K Bartlett l.bartlett@lse.ac.uk 
Centre for Philosophy of Natural and Social Science
School of Economics and Political Science
Centre for Philosophy of Natural and Social Science
School of Economics and Political Science
London, London</p>
<p>Laura K Bartlett 
Centre for Philosophy of Natural and Social Science
School of Economics and Political Science
Centre for Philosophy of Natural and Social Science
School of Economics and Political Science
London, London</p>
<p>Angelo Pirrone 
Centre for Philosophy of Natural and Social Science
School of Economics and Political Science
Centre for Philosophy of Natural and Social Science
School of Economics and Political Science
London, London</p>
<p>Noman Javed 
Centre for Philosophy of Natural and Social Science
School of Economics and Political Science
Centre for Philosophy of Natural and Social Science
School of Economics and Political Science
London, London</p>
<p>Fernand Gobet 
Centre for Philosophy of Natural and Social Science
School of Economics and Political Science
Centre for Philosophy of Natural and Social Science
School of Economics and Political Science
London, London</p>
<p>ASSOCIATION FOR PSYCHOLOGICAL SCIENCE Computational Scientific Discovery in Psychology</p>
<p>Perspectives on Psychological Science
1812023Corresponding Author:Article reuse guidelines: sagepubcom/journals-permissions Keywords computational scientific discoveryAIcreativityphilosophy of sciencepsychology
Scientific discovery is a driving force for progress involving creative problem-solving processes to further our understanding of the world. The process of scientific discovery has historically been intensive and time-consuming; however, advances in computational power and algorithms have provided an efficient route to make new discoveries. Complex tools using artificial intelligence (AI) can efficiently analyze data as well as generate new hypotheses and theories. Along with AI becoming increasingly prevalent in our daily lives and the services we access, its application to different scientific domains is becoming more widespread. For example, AI has been used for the early detection of medical conditions, identifying treatments and vaccines (e.g., against COVID-19), and predicting protein structure. The application of AI in psychological science has started to become popular. AI can assist in new discoveries both as a tool that allows more freedom to scientists to generate new theories and by making creative discoveries autonomously. Conversely, psychological concepts such as heuristics have refined and improved artificial systems. With such powerful systems, however, there are key ethical and practical issues to consider. This article addresses the current and future directions of computational scientific discovery generally and its applications in psychological science more specifically.</p>
<p>Scientific discovery involves solving complex problems creatively to advance our understanding of the world. The resulting knowledge can then be exploited to treat illness, develop new technologies, and solve critical practical problems. Scientific discovery typically takes a long time-a major discovery often requires intensive and iterative investigations. It is therefore crucial to understand the psychological mechanisms underpinning this behavior and how it could be augmented by technologies such as artificial intelligence (AI). This article first outlines psychological theories of scientific discovery and then argues that AI systems can demonstrate scientific creativity akin to humans, highlighting the ways in which AI has been utilized across different domains to both assist scientists and make new discoveries autonomously. The current and potential implications of computational scientific discovery for research in psychology are then explored, and the positive and negative impacts of using AI in scientific research are considered.</p>
<p>Scientific Creativity</p>
<p>We define scientific discovery as the generation of new and important theories and the uncovering of phenomena through scientific inquiry, in line with common usage . This process is critically important because advancing our knowledge of the world allows us to create new tools, treat illness, and benefit society. A recent example of the importance of scientific discovery for humankind is the design of vaccines against the SARS-CoV-2 virus responsible for the COVID-19 pandemic. To generate new discoveries more efficiently, the mechanisms underpinning creativity and scientific discovery must be understood. Creativity has intrigued humans since time immemorial and has been a focus of research in psychology for more than a century. Because creativity is subjective and culturally bound, it is hard to define. Although researchers agree that novelty is an essential element of creativity (Weisberg, 2015), the inclusion of other facets such as surprise, success, usefulness, aesthetics, and authenticity vary across the literature (Boden, 1998(Boden, , 2003Kharkhurin, 2014;Runco &amp; Jaeger, 2012). A number of psychological theories of scientific creativity have been developed, with psychology of science established as a field of research in its own right (Feist, 2008). Scientific creativity has been conceptualized as a combinatorial process (e.g., Simonton, 1997Simonton, , 2009, and an extensive field of research has sought to understand the ways in which concepts are combined (e.g., Gabora et al., 2008;Gabora &amp; Steel, 2017;Hampton, 1997). By analyzing 100 important scientific discoveries and 100 inventions, Thagard (2012) provided support for this combinatorial process, suggesting that all of the included discoveries could be accounted for by this framework.</p>
<p>A particularly influential theory posits that creativity can be described as selective search through a problem space (Klahr &amp; Simon, 2001;Newell et al., 1958). Given a starting state, operators are used to move to new states, hopefully reaching a goal state (solution) eventually (Newell &amp; Simon, 1972). Search is made more efficient by the use of heuristics (i.e., rules of thumb) that are likely to be successful. According to this theory, discovering a new scientific law is in essence no different from solving a puzzle such as the tower of Hanoi and could be achieved computationally.</p>
<p>Problem-space theory can be examined in relation to both classic scientific discovery and computational discovery. First, it should be possible to describe established scientific discoveries in terms of search, problem space, and heuristics. Second, it should be possible to replicate famous scientific discoveries in experiments in which the participants are provided with the data used for the discovery. Third, computer programs should be able to replicate or even make new scientific discoveries as they are highly efficient at searching large problem spaces, in particular when they are equipped with domain-specific heuristics. This includes the development of new computational theories in psychology. The following sections briefly review the extent to which these predictions are supported.</p>
<p>Scientific Discovery</p>
<p>Observational and laboratory studies of scientific discoveries</p>
<p>The way that scientists carry out research and make discoveries, often after detailed and systematic testing over an extended period of time, has been documented by historians and philosophers of science (e.g., Gillispie, 1960). More recently, scientists who have made important discoveries have been interviewed, and computational models of the processes have been developed (e.g., Karp, 1990). Taking an in vivo approach, Dunbar (1994) observed four research laboratories for 1 year to better understand the cognitive and social processes of scientific discovery. In line with problem-space theory, this analysis revealed similar heuristics across laboratories as well as similarities in fundamental aspects of the cognitive operators used by the scientists, differing mostly in the combinations of these operators. Team dynamics also played a role in the way researchers developed and tested hypotheses, which changed problem representations, indicating the importance of social context in scientific discovery.</p>
<p>Laboratory studies have also been conducted to better understand the cognitive mechanisms underpinning discovery. For example, Schunn and Anderson (1999) contrasted domain-specific experts (psychology faculty doing research in the domain of memory), domain-general experts (psychology faculty doing research not related to memory), and novices (nonpsychology undergraduates) in the design and interpretation of experiments in psychology. They found that domain-specific experts had the best solutions and demonstrated not only domain-specific skills but also many domain-general skills and heuristics. These domain-general skills were also demonstrated by domain-general experts compared with novices, who were missing many of these skills. These results indicate that domain-general skills (such as keeping experiments simple and considering relevant theories when making conclusions) are particularly important for scientific discovery.</p>
<p>Replicating scientific discoveries</p>
<p>Experimental studies have confirmed that many scientific discoveries of the past can be replicated and reduced to a heuristic search through a problem space, often with common strategies (e.g., Dunbar, 1993;Langley et al., 1987;Qin &amp; Simon, 1990;Zimmerman &amp; Klahr, 2018). For example, Qin and Simon (1990) found that a third of student participants were able to rediscover Kepler's third law of planetary motion when given the original data. (Note that the variables were not labeled semantically and the data source was unknown.) Likewise, undergraduate students were able to replicate discoveries in the molecular biology of genetic control (Dunbar, 1993), in which successful and unsuccessful answers were distinguished by the goals set by participants. Although creativity is often viewed in society as a mysterious concept, studies such as these support Newell et al.'s (1958) view that it can be explained by mechanisms known to underpin the solving behavior of simple problems such as puzzles.</p>
<p>Computational Scientific Discovery</p>
<p>If creativity is characterized by searching through a problem space, then computational programs using search should be able to show creativity-and in particular should be able to replicate old and make new scientific discoveries. Indeed, computer systems may do so more efficiently than humans because they can process large amounts of data very quickly.</p>
<p>Computational scientific discovery has a long history (e.g., Langley et al., 1987). For example, Newell et al. (1958) developed the Logic Theorist, which generated proofs for theorems in propositional logic using heuristics. Some of its proofs were more elegant than those proposed by leading mathematicians. This program arguably used a process that resulted in creativity. Indeed, a subfield of computer science (automated theorem proving) has since developed with the aim of verifying mathematical statements mechanically, and proof algorithms are nowadays routinely used for teaching fields of mathematics such as logic (Avigad, 2019). With technological developments, scientists are increasingly using computational processes to solve problems throughout the sciences. Advances in technology can often drive new scientific discoveries, and these discoveries can in turn lead to the development of new technologies (Thagard, 2012).</p>
<p>Genetic programming and neural networks are popular AI techniques being used for scientific discovery. Genetic programming (Koza, 1992) takes a population of programs and evolves them across generations, applying various operations (such as mutating parts of the programs) and finding the best program against a defined fitness function. Such evolutionary algorithms are consistent with the combinatorial nature of scientific discovery. Neural networks, on the other hand, are built of connected artificial units (akin to neurons in biological networks) with particular connection weights in a series of layers-the input layer is activated by an input and the output layer generates a response (Aggarwal, 2018). A system is termed "deep" when it includes a series of hidden layers between the input and output layers.</p>
<p>There are two linked goals in computational scientific-discovery research. The first is to uncover the mechanisms and conditions that led to discoveries by replicating famous examples, and the second is to use this knowledge to engineer AI programs to automate and generate new discoveries.</p>
<p>Replications</p>
<p>Computational replications of scientific discoveries have been conducted for many years and with increasingly refined computational procedures (e.g., Hakuk &amp; Reich, 2020;Kulkarni &amp; Simon, 1988). KEKADA (Kulkarni &amp; Simon, 1988) is a program that makes theoretical inferences, assesses the acceptability of its knowledge and theories, and models experimental tests. It was able to replicate Krebs's discovery of the Urea cycle in 1932, a Nobel Prize-winning discovery. Grahoff and May (1995) extended this research by analyzing available historical documents (notebooks, publications, etc.) for a number of case studies, implementing the common methodological rules that explained all of the studied cases into a computational cognitive model of discovery. The commonalities included heuristics relating to the formation of models and the rules governing the generation and evaluation of causal hypotheses. More recently, Hakuk and Reich (2020) replicated modern discoveries in mechanics (the subfield of physics studying motion) and described a new approach to automated discovery by using the methods and concepts of one discipline to find equivalent knowledge in a different discipline. Consistent with the combinatorial characterization of discovery, this method combined ideas from different domains.</p>
<p>New discoveries</p>
<p>Advancements in computational power and AI algorithms have led to a number of AI scientific discoveries. AI can be used in research both as a tool to assist humans, allowing researchers more freedom to generate discoveries, and as autonomous discoverers, displaying the creative problem-solving that characterizes scientific discovery and generating new and exciting results that further our knowledge of the world. A full discussion of this literature would cover several volumes, so we focus here on some of the key advances.</p>
<p>One of the earliest systems developed was DENDRAL (Lindsay et al., 1980), which discovered molecular structures by analyzing mass spectrometry data. More recently, the "robot scientist" (King et al., 2004) automated almost the entire scientific process in the domain of functional genomics, an area that had already embraced automation. This system generates hypotheses using observations, designs experiments and runs them, interprets the results in terms of the hypotheses, and then repeats the process. This allows scientists to expedite the process and make creative advances in the field. This instance of automation also serves as an example of where scientific research may be heading, with more automation across all fields.</p>
<p>Computational scientific discovery has led to groundbreaking discoveries in recent years. DeepMind's Alpha-Fold is an AI tool that predicts the three-dimensional structure of proteins, solving a 50-year "grand problem" in biology known as the "protein folding problem" ( Jumper et al., 2021). To date, this technology has predicted the structure of 350,000 proteins, including 98.5% of human proteins (the structure of only 17% of human proteins had been established experimentally). This in turn advances our understanding of the basic components of cells, leading to the discovery of more efficient drugs to treat illness as well as improved insight regarding gene variations that cause disease in different people. AlphaFold is clearly a powerful and creative tool, reflecting a substantial contribution of AI to scientific discovery. AI has also been used in multiple ways during the COVID-19 pandemic (Abd-Alrazaq et al., 2020), including for the highly efficient development of COVID-19 vaccines (Waltz, 2020), with less than 3 months between the detection of the virus and human trials. Combining messy real-world and experimental data, AI allowed insights and predictions regarding the virus and potential vaccine targets. AI can further assist in processing the large amount of anticipated adverse drug reactions to the vaccine, proving a valuable tool at all stages of this process.</p>
<p>Computational Discovery and Psychology</p>
<p>Developments in computational and AI systems have far-reaching applications in many domains, and psychology is no exception. AI helps formulate and describe psychological theories. Quite often, theories in the social sciences are verbal, informal, and lack precision; rather than explaining the data, they may be better characterized as a redescription of the data . Formal computational formulations require the fine details of a theory to be specified-all elements of a theory must be put into a coding language, which involves a detailed description of exactly what is going on. This avoids the use of vague language to describe certain constructs. For example, rather than saying that something is due to procedural memory, this concept is formally defined in a computer program such as ACT-R, along with relevant mechanisms (Anderson et al., 2004).</p>
<p>Although the adoption of AI in psychology is still at an early stage, its use extends into all domains of psychology. In addition to machine learning, which can be used to mine large data files (Dwyer et al., 2018) and eval uate psychological research questions (Elhai &amp; Montag, 2020), AI has led to the development of models and theories, alongside applied uses in clinical psychology. Although psychologists typically focus on explaining human behavior, Yarkoni and Westfall (2017) emphasized the importance of predicting behavior, particularly for applied domains such as clinical psychology. They and others outlined how the prediction of behavior could be achieved by applying ideas and techniques from machine learning such as the analysis of big data and cross-validation (Agrawal et al., 2020), which may address some of the critical problems facing psychology (e.g., the replication crisis and p-hacking).</p>
<p>Here, we focus on three examples of AI that benefit psychology: semiautomatic development of theories in psychology, modeling decision-making, and methods for refining clinical diagnostic criteria.</p>
<p>Semiautomatic development of theories</p>
<p>AI can improve model and theory development in psychological science. Cichy and Kaiser (2019) advocated particularly for the use of deep neural networks (DNNs) in exploratory cognitive science. They suggested that new ideas can come from exploring models; DNNs can serve as proof-of-principle demonstrations, and models can amend and refine fundamental scientific concepts. Agrawal et al. (2020) refined psychological models by comparing them to complex machine-learning models trained on a large data set of moral decisions and produced a theory-based, predictive, and interpretable model of moral decision-making. Genetic programming has been used to effectively describe the interactions of variables in psychometric and lexical access experiments (Westbury et al., 2003). With regard to scientific discovery, Lara-Dammer et al. (2019) demonstrated that a simulated system (NINSUN) emulating human perception based on theories of scientific discovery and perception, is able to make scientific hypotheses in a simple artificial world.</p>
<p>Genetically Evolving Models in Science (GEMS) is a system currently in development designed to automatically generate possible theories of human cognitive behavior Frias-Martinez &amp; Gobet, 2007). The system, which extends genetic programming, combines several "operators" (e.g., putting an item into short-term memory or moving covert attention) into a program (i.e., a model) whose predictions are then compared against experimental data from human participants. A population of models, with different combinations of operators, is evolved over a number of generations, with preference given to those most closely matching the human data (for a detailed description of the GEMS system using the delayedmatch-to-sample task as an example, see Frias-Martinez &amp; Gobet, 2007). A key benefit of GEMS over other methods is that it does not rely on large data sets. Although this system allows some bias because it requires human input in developing operators, coding the experimental conditions, and setting parameters, this bias is reduced compared with standard theory development. In particular, this system avoids confirmation bias and allows operators from different fields of cognitive psychology that are typically studied separately to be combined. By creating operators relating to these different fields (e.g., memory, attention, and decision-making), novel predictions can be generated that exploit the interplay between these domains. Whereas experts in a given area may fail to appreciate other factors that could influence their topic of interest, GEMS aims to avoid this issue while generating exciting and unexpected theories of human behavior.</p>
<p>In psychology, GEMS has successfully generated scientific theories semiautomatically for a number of experiments Frias-Martinez &amp; Gobet, 2007;Lane et al., 2016;Sozou et al., 2017). Although these are initial results that still need to be refined by future research, they establish the validity of the GEMS approach (e.g., Pirrone &amp; Gobet, 2020, 2021.</p>
<p>Modeling decision-making</p>
<p>Machine-learning techniques have been used to develop and refine theories in decision-making research (Bourgin et al., 2019;Erev et al., 2017;Fudenberg et al., 2022;Noti et al., 2016;Peterson et al., 2021;Peysakhovich &amp; Naecker, 2017;Plonsky et al., 2017). For example, Peterson et al. (2021) collected a large data set on riskychoice decisions in which participants had to choose between different gambles; several machine-learning models of increased complexity were then trained on the data. Each model was based on the literature and included previous (human-generated) theories such as the widely popular subjective utility models and prospect theory. The authors showed that those models could not explain large amounts of variance in the data, even when each model was fine-tuned using the largescale data set and neural networks that optimized the functional form and the parameters of the model. Interestingly, using machine learning, the authors discovered a new theory of risky choices in which participants adopt sophisticated strategies that are a mixture of previously proposed theories, whose functional form and parameters depend on the specific details of each gamble (such as maximum outcome, minimum outcome, and outcome variability). This new theory and its predictions are likely to drive future studies in the field of risky choices (Plonsky &amp; Erev, 2021) and pave the way for future similar applications in fields across psychology and the cognitive sciences (e.g., Battleday et al., 2020).</p>
<p>AI in clinical psychology</p>
<p>Alongside model and theory development, AI has the potential to extend the clinical understanding of mentalhealth conditions, thus allowing the discovery of previously unknown patterns of behavior and a better insight into how different classifications overlap. For example, data-mining techniques have been used to determine which variables can distinguish between groups of high and low suicide risk (Morales et al., 2017). AI can also be used to determine variables that can predict outcome and treatment adherence (Aafjes-van Doorn et al., 2021; D'Alfonso, 2020). AI also has the potential to refine diagnostic criteria (Tai et al., 2019), which could lead to new discoveries and improved knowledge of the factors contributing to different conditions. Short versions of time-intensive self-report measures of personality have been developed using genetic algorithms (Eisenbarth et al., 2015;Yarkoni, 2010), consistent with the original measures across samples, languages, and data-collection methods. This allows researchers to administer such measures more efficiently.</p>
<p>AI-led advances in drug development could drive breakthroughs in clinical research. New scientific discoveries may also be assisted by systems trained to be consultant experts in medical knowledge (e.g., IBM's Watson). It is likely that automated scientific discovery will have a considerable impact in clinical psychology, especially considering the long history of using formal methods in the domain, for example, to simulate clinicians (Rizzo et al., 2016;Weizenbaum, 1966) Luxton, 2014) and for providing successful clinical diagnoses (Grove et al., 2000).</p>
<p>Applying psychology to AI development</p>
<p>Although AI has many useful applications in psychology, principles from psychology have long been aiding the development of AI systems (Khetarpal et al., 2020;Lieto &amp; Pozzato, 2020b;Rogers &amp; Mcclelland, 2014;Taylor &amp; Taylor, 2021; for an overview, see Lieto, 2021), which in turn benefits psychological science. For example, the concept of a heuristic, which was central to the success of the Logic Theorist (Newell et al., 1958), was imported from psychology. More recently, the incorporation of human uncertainty (through the collection of an extensive amount of human-categorization data) has improved machine classification . Likewise, advances and theories in neuroscience have been applied to AI to improve both the efficiency and accuracy of such systems (for a review, see Hassabis et al., 2017).</p>
<p>The creative combining of concepts that is argued to be a key component of scientific discovery has been modeled by Lieto and Pozzato (2020a) in their typicality based compositional-logic framework. Informed by human cognitive heuristics, this framework makes it possible to automatically generate novel concepts in a human-like way (Chiodino et al., 2020;Lieto et al., 2019Lieto et al., , 2021. This framework applies knowledge and theories from cognitive science to overcome problems in artificial systems; correspondingly, the functioning of AI systems allows psychologists to refine their models of cognition. Likewise, with respect to human creativity, understanding the cognitive processes involved can assist in the development of new AI techniques, which in turn can allow psychologists to test theoretical hypotheses regarding creativity and its underlying processes. For example, Gabora and colleagues drew from research on cognitive psychology to develop more refined algorithms that can generate novel and aesthetically appealing artworks (DiPaola et al., 2018;DiPaola &amp; Gabora, 2007). Developments in psychology can inform AI techniques, and these techniques can help generate new discoveries in psychology, highlighting the importance of interdisciplinarity for scientific discovery.</p>
<p>Computational Scientific Discovery: a Critical Evaluation</p>
<p>The trend toward the automation of the scientific process is important for a number of reasons, not least because of the many new discoveries outlined above. The rate at which data are generated is increasing exponentially; effectively analyzing these data requires the development of tools that excel at efficiently and accurately processing data-tasks for which humans are not well equipped. Because computer systems are optimal for such tasks, they offer significant advantages for scientific discovery. In addition, automating these processes allows human scientists the freedom to engage in higher-level activities, such as theory building, rather than data crunching and manual work. Some see the advancements in AI as a threat to humans. However, because humans and computers excel at different tasks, AI can instead be considered a complementary form of intelligence rather than a replacement (de Mello &amp; de Souza, 2019).</p>
<p>AI is not limited to data processing-it can also produce creative and novel solutions to problems that humans may not have thought of, as illustrated in this article. This is beneficial because there are limits to human creativity and reasoning (Gobet &amp; Sala, 2019), consistent with Simon's (1956) theory of bounded rationality, which posits that the ability of a decision maker to make a rational choice is constrained by computational capacity and knowledge limitations. However, this forced selectivity can sometimes be advantageous; Gigerenzer and Brighton (2009) argued that heuristics help better manage uncertainty than unbiased, resourceintensive processing. Indeed, machine classification can be improved when human uncertainty is included . Knowledge can be detrimental and can lead to a preference for standard responses, even when creative and novel solutions are better (Bilali et al., 2008). Likewise, living in a society and in a certain context can impose constraints on human creativity; computational creativity is not limited by these constraints, allowing for interesting developments and ideas.</p>
<p>Ethical considerations</p>
<p>A number of ethical issues arise from using computation for scientific discovery. For example, if controversial theories or patterns in data are found by AI (e.g., using GEMS), the ethical implications must be carefully considered by researchers, particularly the potential negative impacts such discoveries could have. Another example could be taken from clinical psychology: Understanding variables that are predictive of clinical treatment outcomes could be used to minimize barriers for treatment adherence; however, the opposite could also occur, in which patients showing these characteristics are denied treatment. Such unintended uses must be considered and guarded against.</p>
<p>Relatedly, AI-generated discoveries could be a product of bias. Although AI systems are suggested to remove bias, they will likely still reflect the biases of the humans who created them and the data sets they are given. Much psychological research has been criticized as consisting of a limited sample, namely undergraduate psychology students, which is not representative of the general population (Henrich et al., 2010). Although AI can produce novel models of human behavior, these models will be constrained by the nonrepresentative data that they are provided with. This is clearly not an issue limited to AI; however, the power of such technologies and the common view that they are less biased could lead to more reliance on their outputs without considering their inputs.</p>
<p>The impact of biased data has been demonstrated with facial-recognition technology, in which racial and gender bias in data sets resulted in heightened misclassification of women and people of color (Buolamwini &amp; Gebru, 2018). Likewise, AI systems used in the criminal-justice system, from law enforcement to decisionmaking support (Zavrnik, 2020), may inherit historical bias-factors that may be used as predictive, such as previous arrests, may be racially biased as a result of systemic discriminatory practices in policing. As outlined by Sgaier et al. (2020), predictive AI can often equate correlation with causation, which can have disastrous consequences-they suggested that causal AI, in which the underlying causes of behavior are modeled, is the essential next step in AI utilization. Bias in AI can also be inherited from using data derived from biased tests; for example, intelligence tests have sometimes been criticized as being culturally biased (Greenfield, 1997;Lozano-Ruiz et al., 2021). Using AI to derive insights regarding intelligence will similarly be subject to the issues surrounding such tests. Although bias exists without it, AI is a powerful tool and so could exacerbate this problem. Similar problems may occur across all areas of AI-generated scientific discovery, and those working with this technology should be aware of these issues.</p>
<p>Although biases can be detrimental for discovery because they can lead scientists to develop and accept incorrect theories, steps can be taken to minimize their influence on AI systems, such as increasing diversity within both the team developing the technology and those testing it. If issues are found with the input data, more and better input can be fed into the systems to rectify this flaw-a much easier process for AI than for humans. Although individual biases are hard to overcome, new approaches and tools can help reduce at least some biases present in AI systems.</p>
<p>The field of AI ethics is very active (for an evaluation, see Hagendorff, 2020) because such a powerful tool can have huge and widespread implications to humans. Although AI systems are generally designed to contribute positively to science and humanity, their unintended negative consequences for some in society and ability to be exploited for damaging purposes must be considered responsibly. To overcome some of these ethical issues, scientists and researchers developing and using such technology should receive training on technological ethics to develop the knowledge and tools for understanding the risks and solving potential ethical issues effectively. Finally, it is important to consider who should be making decisions on whether something is ethical or not. For example, in psychology, experimental research is subject to ethics-committee approval; however, there currently is no parallel in AI.</p>
<p>Interpretability</p>
<p>To use AI systems for scientific discovery, outputs must be understandable to humans and allow interpretation in the context of current theories and knowledge ( Javed &amp; Gobet, 2021). A key issue is that, although the inputs and outputs of an AI system are known, the intermediate steps are sometimes difficult to understand (it is a "black box"), which can affect the interpretability of the output data. Different techniques have been used to address this issue. For example, an artist named Tom White is using AI to generate abstract versions of the images it is trained on to show how AI "sees" the world and demonstrate the underlying algorithms (see https:// aiartists.org/tom-white). The system represented various items with abstract blobs, and these blobs were shown to activate the object label more than the real images it was trained on.</p>
<p>Another technique being used is taken directly from psychological science: Psychlab (Leibo et al., 2018) is a resource created to better understand AI systems through classic psychological experiments. Akin to cognitive psychology, it seeks to understand the "cognitions" underlying the outputs given by AI systems and what they are actually doing. Likewise, Taylor and Taylor (2021) recently described the contributions that cognitive psychologists can make in the creation of explainable AI, arguing that the experience of studying the human mind via experimentation is applicable to AI. Although the outputs of AI systems are already leading to great advances in science, it is critically important to understand what is happening in these systems to be able to use the technology for applied purposes.</p>
<p>The importance of interpretability for psychological science has certainly been appreciated, and novel techniques to address this issue are being developed, often drawing from the machine-learning domain. Agrawal et al. (2020) outlined and demonstrated the feasibility of using machine-learning techniques to develop predictive models of psychological phenomena while maintaining interpretability for large data sets through an iterative process they termed "Scientific Regret Minimization", based on regret minimization in machine learning. Specifically, the fit of a simple psychological model is critiqued against an unconstrained machinelearning model trained on the same data set until the predictions converge. The residuals that may signify novel effects can then be validated in separate experiments. Understanding of the hidden layers of AI systems has also been improved by genetic programming. Evans et al. (2019) and Ferreira et al. (2020) used genetic programming to explain the behavior of a black-box model. Genetic programming does not see the original data but sees the predictions of the blackbox model and tries to recreate them. This approach is model-agnostic and can generate explanations for different machine-learning black-box models.</p>
<p>Clinical considerations</p>
<p>In clinical psychology, the information about a person that some systems gather can be more than a person wants to share. The ability of AI systems, including those aimed at making scientific discoveries, to collect and store such personal data allows for potential breaches of privacy. These systems must be secure to fully protect any sensitive information (Lustgarten et al., 2020), such as the information collected during simulated therapy. Fiske et al. (2019) outlined the following potential practical and ethical issues with using AI in clinical psychology settings: (a) ethical responsibilities related to risk assessment, (b) adherence to codes of practice and duty of care, (c) issues with consent and patients understanding when they are not interacting with a human, and (d) impacts of strong attachments made with AI applications, which are not reciprocal and may reduce real social interaction. Although the use of AI technology in clinical settings presents substantial benefits to well-being, these ethical and practical concerns need to be addressed. Advances in clinical psychology from computational scientific discovery must similarly ensure the safety of those using the systems and the protection of sensitive data.</p>
<p>Conclusions</p>
<p>AI is a powerful tool that can have a significant role in scientific discovery across all fields of research. It not only allows scientists the freedom to engage in highlevel theoretical thinking but also can demonstrate the creativity of autonomously generating novel ideas (e.g., King et al., 2004). The impact that these systems are already having is impressive, and the current trends suggest that they will continue to be more ingrained in the scientific process ( Jumper et al., 2021;Peterson et al., 2021). Although creativity has historically been thought of as an unexplainable human concept, evidence from historical records, replications of discoveries, and new discoveries using AI are consistent with creativity and scientific discovery as resulting from problem-solving. Particularly adept at efficiently solving problems, AI can demonstrate creativity and can have a dominant role in generating scientific discoveries in the future. In line with Newell et al. (1958), heuristics obtained through psychological science have been used to improve the efficiency and accuracy of AI, and many AI systems, including AlphaFold, can be described as selectively searching through a problem space. Alongside this, AI systems using evolutionary techniques such as genetic programming depend on random combinations, consistent with the combinatory nature of scientific creativity.</p>
<p>AI can provide innovative ideas that may have taken considerable time for humans, in part because it is less constrained by limits on available knowledge and biases. Scientific breakthroughs are historically the result of extensive experimentation and theorizing and can often be characterized as "thinking outside the box"; this can be achieved much more efficiently by a computational system. The creative advances that artificial systems can provide are critical to accelerating successful scientific inquiry and further pushing our knowledge and understanding of the world.</p>
<p>AI is already proving to be useful for scientific discovery in psychology (e.g., Aafjes-van Doorn et al., 2021;Frias-Martinez &amp; Gobet, 2007;Peterson et al., 2021). It can generate new psychological models of human cognition. The application of concepts from psychology to artificial systems can also constrain models of human behavior. AI can be used as a tool to assist psychologists, for example, in interpreting brain-scan analyses and improving clinical diagnoses. The adoption of such systems in psychology is beginning to grow; however, issues in terms of bias, data protection, interpretability of outputs, and potential unethical uses must be considered.</p>
<p>Transparency</p>
<p>Action Editor: Matthew Rhodes Editor: Laura A. King</p>
<p>Declaration of Conflicting Interests</p>
<p>The author(s) declared that there were no conflicts of interest with respect to the authorship or the publication of this article.</p>
<p>Funding</p>
<p>This work was supported by European Research Council Grant ERC-ADG-835002-GEMS.</p>
<p>ORCID iDs</p>
<p>Laura K. Bartlett https://orcid.org/0000-0001-5202-4504 Fernand Gobet https://orcid.org/0000-0002-9317-6886</p>
<p>and patients (e.g., de Mello &amp; de Souza, 2019; Fitzpatrick et al., 2017; Talbot &amp; Rizzo, 2019; for a review, see Fiske et al., 2019; for a detailed discussion, see</p>
<p>A scoping review of machine learning in psychotherapy research. K Aafjes-Van Doorn, C Kamsteeg, J Bate, M Aafjes, 10.1080/10503307.2020.1808729Psychotherapy Research. 311Aafjes-van Doorn, K., Kamsteeg, C., Bate, J., &amp; Aafjes, M. (2021). A scoping review of machine learning in psycho- therapy research. Psychotherapy Research, 31(1), 92-116. https://doi.org/10.1080/10503307.2020.1808729</p>
<p>Artificial intelligence in the fight against COVID-19: Scoping review. A Abd-Alrazaq, M Alajlani, D Alhuwail, J Schneider, S Al-Kuwari, Z Shah, M Hamdi, M Househ, 10.2196/20756Journal of Medical Internet Research. 2212Article e20756Abd-Alrazaq, A., Alajlani, M., Alhuwail, D., Schneider, J., Al-Kuwari, S., Shah, Z., Hamdi, M., &amp; Househ, M. (2020). Artificial intelligence in the fight against COVID- 19: Scoping review. Journal of Medical Internet Research, 22(12), Article e20756. https://doi.org/10.2196/20756</p>
<p>Semi-automatic generation of cognitive science theories. M Addis, F Gobet, P C R Lane, P D Sozou, Addis, M., Gobet, F., Lane, P. C. R., &amp; Sozou, P. D. (2019). Semi-automatic generation of cognitive science theories.</p>
<p>In M Addis, P C R Lane, Scientific discovery in the social sciences. P. D. Sozou, &amp; F. GobetIn M. Addis, P. C. R. Lane, P. D. Sozou, &amp; F. Gobet (Eds.), Scientific discovery in the social sciences (pp. 155-171).</p>
<p>. Springer, 10.1007/978-3-030-23769-1_10Springer. https://doi.org/10.1007/978-3-030-23769-1_10</p>
<p>Computational scientific discovery and cognitive science theories. M Addis, P D Sozou, P C R Lane, F Gobet, 10.1007/978-3-319-23291-1_6Computing and philosophy. V. C. MllerSpringerAddis, M., Sozou, P. D., Lane, P. C. R., &amp; Gobet, F. (2016). Computational scientific discovery and cognitive science theories. In V. C. Mller (Ed.), Computing and philosophy (pp. 83-97). Springer. https://doi.org/10.1007/978-3-319- 23291-1_6</p>
<p>Neural networks and deep learning: A textbook. C C Aggarwal, 10.1007/978-3-319-94463-0SpringerAggarwal, C. C. (2018). Neural networks and deep learning: A textbook. Springer. https://doi.org/10.1007/978-3-319- 94463-0</p>
<p>Scaling up psychology via Scientific Regret Minimization. M Agrawal, J C Peterson, T L Griffiths, 10.1073/pnas.1915841117Proceedings of the National Academy of Sciences. the National Academy of SciencesUSA117Agrawal, M., Peterson, J. C., &amp; Griffiths, T. L. (2020). Scaling up psychology via Scientific Regret Minimization. Proceedings of the National Academy of Sciences, USA, 117(16), 8825- 8835. https://doi.org/10.1073/pnas.1915841117</p>
<p>An integrated theory of the mind. J R Anderson, D Bothell, M D Byrne, S Douglass, C Lebiere, Y Qin, 10.1037/0033-295X.111.4.1036Psychological Review. 1114Anderson, J. R., Bothell, D., Byrne, M. D., Douglass, S., Lebiere, C., &amp; Qin, Y. (2004). An integrated theory of the mind. Psychological Review, 111(4), 1036-1060. https:// doi.org/10.1037/0033-295X.111.4.1036</p>
<p>Learning logic and proof with an interactive theorem prover. J Avigad, 10.1007/978-3-030-28483-1_13Proof technology in mathematics research and teaching. Mathematics education in the digital era. G. Hanna, D. Reid, &amp; M. de VilliersSpringer14Avigad, J. (2019). Learning logic and proof with an interactive theorem prover. In G. Hanna, D. Reid, &amp; M. de Villiers (Eds.), Proof technology in mathematics research and teaching. Mathematics education in the digital era, vol 14 (pp. 277-290). Springer. https://doi.org/10.1007/978- 3-030-28483-1_13</p>
<p>Capturing human categorization of natural images by combining deep networks and cognitive models. R M Battleday, J C Peterson, T L Griffiths, 10.1038/s41467-020-18946-zNature Communications. 11Battleday, R. M., Peterson, J. C., &amp; Griffiths, T. L. (2020). Capturing human categorization of natural images by combining deep networks and cognitive models. Nature Communications, 11, Article 5418. https://doi.org/10 .1038/s41467-020-18946-z</p>
<p>Inflexibility of experts-reality or myth? Quantifying the Einstellung effect in chess masters. M Bilali, P Mcleod, F Gobet, 10.1016/j.cogpsych.2007.02.001Cognitive Psychology. 562Bilali, M., McLeod, P., &amp; Gobet, F. (2008). Inflexibility of experts-reality or myth? Quantifying the Einstellung effect in chess masters. Cognitive Psychology, 56(2), 73-102. https://doi.org/10.1016/j.cogpsych.2007.02.001</p>
<p>Creativity and artificial intelligence. M A Boden, 10.1016/s0004-3702(98)00055-1Artificial Intelligence. 1031-2Boden, M. A. (1998). Creativity and artificial intelligence. Artificial Intelligence, 103(1-2), 347-356. https://doi.org/ 10.1016/s0004-3702(98)00055-1</p>
<p>The creative mind: Myths and mechanisms. M A Boden, 10.4324/97802035085272nd ed.Boden, M. A. (2003). The creative mind: Myths and mecha- nisms (2nd ed.). Routledge. https://doi.org/10.4324/978</p>
<p>Cognitive model priors for predicting human decisions. D D Bourgin, J C Peterson, D Reichman, S J Russell, T L Griffiths, Proceedings of Machine Learning Research. Machine Learning Research97Bourgin, D. D., Peterson, J. C., Reichman, D., Russell, S. J., &amp; Griffiths, T. L. (2019). Cognitive model priors for predict- ing human decisions. Proceedings of Machine Learning Research, 97, 8984-8992.</p>
<p>Gender shades: Intersectional accuracy disparities in commercial gender classification. J Buolamwini, T Gebru, Proceedings of Machine Learning Research. Machine Learning Research81Buolamwini, J., &amp; Gebru, T. (2018). Gender shades: Inter- sectional accuracy disparities in commercial gender clas- sification. Proceedings of Machine Learning Research, 81, 77-91.</p>
<p>A knowledge-based system for the dynamic generation and classification of novel contents in multimedia broadcasting. E Chiodino, D Di Luccio, A Lieto, A Messina, G L Pozzato, D Rubinetti, 10.3233/FAIA200154Frontiers in Artificial Intelligence and Applications. 325Chiodino, E., Di Luccio, D., Lieto, A., Messina, A., Pozzato, G. L., &amp; Rubinetti, D. (2020). A knowledge-based system for the dynamic generation and classification of novel con- tents in multimedia broadcasting. Frontiers in Artificial Intelligence and Applications, 325, 680-687. https://doi .org/10.3233/FAIA200154</p>
<p>Deep neural networks as scientific models. R M Cichy, D Kaiser, 10.1016/j.tics.2019.01.009Trends in Cognitive Sciences. 234Cichy, R. M., &amp; Kaiser, D. (2019). Deep neural networks as scientific models. Trends in Cognitive Sciences, 23(4), 305-317. https://doi.org/10.1016/j.tics.2019.01.009</p>
<p>AI in mental health. S D&apos;alfonso, 10.1016/j.copsyc.2020.04.005Current Opinion in Psychology. 36D'Alfonso, S. (2020). AI in mental health. Current Opinion in Psychology, 36, 112-117. https://doi.org/10.1016/j .copsyc.2020.04.005</p>
<p>Psychotherapy and artificial intelligence: A proposal for alignment. F L De Mello, S A Souza, 10.3389/fpsyg.2019.00263Frontiers in Psychology. 10de Mello, F. L., &amp; de Souza, S. A. (2019). Psychotherapy and artificial intelligence: A proposal for alignment. Frontiers in Psychology, 10, Article 263. https://doi.org/10.3389/ fpsyg.2019.00263</p>
<p>Incorporating characteristics of human creativity into an evolutionary art algorithm. S Dipaola, L Gabora, DiPaola, S., &amp; Gabora, L. (2007). Incorporating characteristics of human creativity into an evolutionary art algorithm.</p>
<p>10.1145/1274000.1274009GECCO '07: Proceedings of the 9th Annual Conference Companion on Genetic and Evolutionary Computation. Association for Computing MachineryIn GECCO '07: Proceedings of the 9th Annual Conference Companion on Genetic and Evolutionary Computation (pp. 2450-2456). Association for Computing Machinery. https://doi.org/10.1145/1274000.1274009</p>
<p>Informing artificial intelligence generative techniques using cognitive theories of human creativity. S Dipaola, L Gabora, G Mccaig, 10.1016/j.procs.2018.11.024Procedia Computer Science. 145DiPaola, S., Gabora, L., &amp; McCaig, G. (2018). Informing arti- ficial intelligence generative techniques using cognitive theories of human creativity. Procedia Computer Science, 145, 158-168. https://doi.org/10.1016/j.procs.2018.11.024</p>
<p>Concept discovery in a scientific domain. K Dunbar, 10.1207/s15516709cog1703_3Cognitive Science. 173Dunbar, K. (1993). Concept discovery in a scientific domain. Cognitive Science, 17(3), 397-434. https://doi.org/10.1207/ s15516709cog1703_3</p>
<p>How scientists really reason: Scientific reasoning in real-world laboratories. K Dunbar, 10.7551/mitpress/4879.003.0017The nature of insight. J. E. Davidson &amp; R. J. SternbergMIT PressDunbar, K. (1994). How scientists really reason: Scientific rea- soning in real-world laboratories. In J. E. Davidson &amp; R. J. Sternberg (Eds.), The nature of insight (pp. 365-395). MIT Press. https://doi.org/10.7551/mitpress/4879.003.0017</p>
<p>Machine learning approaches for clinical psychology and psychiatry. D B Dwyer, P Falkai, N Koutsouleris, 10.1146/annurev-clinpsy-032816-045037Annual Review of Clinical Psychology. 14Dwyer, D. B., Falkai, P., &amp; Koutsouleris, N. (2018). Machine learning approaches for clinical psychology and psychia- try. Annual Review of Clinical Psychology, 14, 91-118. https://doi.org/10.1146/annurev-clinpsy-032816-045037</p>
<p>Using a genetic algorithm to abbreviate the Psychopathic Personality Inventory-Revised (PPI-R). H Eisenbarth, S O Lilienfeld, T Yarkoni, 10.1037/pas0000032Psychological Assessment. 271Eisenbarth, H., Lilienfeld, S. O., &amp; Yarkoni, T. (2015). Using a genetic algorithm to abbreviate the Psychopathic Personality Inventory-Revised (PPI-R). Psychological Assessment, 27(1), 194-202. https://doi.org/10.1037/pas 0000032</p>
<p>The compatibility of theoretical frameworks with machine learning analyses in psychological research. J D Elhai, C Montag, 10.1016/j.copsyc.2020.05.002Current Opinion in Psychology. 36Elhai, J. D., &amp; Montag, C. (2020). The compatibility of theo- retical frameworks with machine learning analyses in psychological research. Current Opinion in Psychology, 36, 83-88. https://doi.org/10.1016/j.copsyc.2020.05.002</p>
<p>From anomalies to forecasts: Toward a descriptive model of decisions under risk, under ambiguity, and from experience. I Erev, E Ert, O Plonsky, D Cohen, O Cohen, 10.1037/rev0000062Psychological Review. 1244Erev, I., Ert, E., Plonsky, O., Cohen, D., &amp; Cohen, O. (2017). From anomalies to forecasts: Toward a descriptive model of decisions under risk, under ambiguity, and from expe- rience. Psychological Review, 124(4), 369-409. https://doi .org/10.1037/rev0000062</p>
<p>What's inside the black-box? A genetic programming method for interpreting complex machine learning models. B P Evans, B Xue, M Zhang, 10.1145/3321707.3321726GECCO 2019: Proceedings of the Genetic and Evolutionary Computation Conference. Association for Computing MachineryEvans, B. P., Xue, B., &amp; Zhang, M. (2019). What's inside the black-box? A genetic programming method for interpret- ing complex machine learning models. In GECCO 2019: Proceedings of the Genetic and Evolutionary Computation Conference (pp. 1012-1020). Association for Computing Machinery. https://doi.org/10.1145/3321707.3321726</p>
<p>The psychology of science and the origins of the scientific mind. G J Feist, Yale University PressFeist, G. J. (2008). The psychology of science and the origins of the scientific mind. Yale University Press.</p>
<p>Applying genetic programming to improve interpretability in machine learning models. L A Ferreira, F G Guimares, R Silva, 10.1109/CEC48606.2020.9185620IEEE Congress on Evolutionary Computation. Institute of Electrical and Electronics Engineers. Ferreira, L. A., Guimares, F. G., &amp; Silva, R. (2020). Applying genetic programming to improve interpretability in machine learning models. In 2020 IEEE Congress on Evolutionary Computation. Institute of Electrical and Electronics Engineers. https://doi.org/10.1109/CEC48606 .2020.9185620</p>
<p>Your robot therapist will see you now: Ethical implications of embodied artificial intelligence in psychiatry, psychology, and psychotherapy. A Fiske, P Henningsen, A Buyx, 10.2196/13216Article e13216. 21Fiske, A., Henningsen, P., &amp; Buyx, A. (2019). Your robot ther- apist will see you now: Ethical implications of embodied artificial intelligence in psychiatry, psychology, and psy- chotherapy. Journal of Medical Internet Research, 21(5), Article e13216. https://doi.org/10.2196/13216</p>
<p>Delivering cognitive behavior therapy to young adults with symptoms of depression and anxiety using a fully automated conversational agent (Woebot): A randomized controlled trial. K K Fitzpatrick, A Darcy, M Vierhile, 10.2196/mental.7785Article e19. 4Fitzpatrick, K. K., Darcy, A., &amp; Vierhile, M. (2017). Delivering cognitive behavior therapy to young adults with symp- toms of depression and anxiety using a fully automated conversational agent (Woebot): A randomized controlled trial. JMIR Mental Health, 4(2), Article e19. https://doi .org/10.2196/mental.7785</p>
<p>Automatic generation of cognitive theories using genetic programming. Minds and Machines. E Frias-Martinez, F Gobet, 10.1007/s11023-007-9070-617Frias-Martinez, E., &amp; Gobet, F. (2007). Automatic generation of cognitive theories using genetic programming. Minds and Machines, 17, 287-309. https://doi.org/10.1007/ s11023-007-9070-6</p>
<p>Measuring the completeness of economic models. D Fudenberg, J Kleinberg, A Liang, S Mullainathan, 10.1086/718371Journal of Political Economy. 1304Fudenberg, D., Kleinberg, J., Liang, A., &amp; Mullainathan, S. (2022). Measuring the completeness of economic mod- els. Journal of Political Economy, 130(4). https://doi .org/10.1086/718371</p>
<p>Toward an ecological theory of concepts. L Gabora, E Rosch, D Aerts, 10.1080/10407410701766676Ecological Psychology. 201Gabora, L., Rosch, E., &amp; Aerts, D. (2008). Toward an eco- logical theory of concepts. Ecological Psychology, 20(1), 84-116. https://doi.org/10.1080/10407410701766676</p>
<p>Autocatalytic networks in cognition and the origin of culture. L Gabora, M Steel, 10.1016/j.jtbi.2017.07.022Journal of Theoretical Biology. 431Gabora, L., &amp; Steel, M. (2017). Autocatalytic networks in cognition and the origin of culture. Journal of Theoretical Biology, 431, 87-95. https://doi.org/10.1016/j.jtbi.2017 .07.022</p>
<p>Homo heuristicus: Why biased minds make better inferences. G Gigerenzer, H Brighton, 10.1111/j.1756-8765.2008.01006.xTopics in Cognitive Science. 11Gigerenzer, G., &amp; Brighton, H. (2009). Homo heuristicus: Why biased minds make better inferences. Topics in Cognitive Science, 1(1), 107-143. https://doi.org/10.1111/j.1756- 8765.2008.01006.x</p>
<p>The edge of objectivity. C C Gillispie, Princeton University PressGillispie, C. C. (1960). The edge of objectivity. Princeton University Press.</p>
<p>Introduction: Scientific discovery in the social sciences. F Gobet, M Addis, P C R Lane, P D Sozou, Scientific discovery in the social sciences. M. Addis, P. C. R. Lane, P. D. Sozou, &amp; F. GobetGobet, F., Addis, M., Lane, P. C. R., &amp; Sozou, P. D. (2019). Introduction: Scientific discovery in the social sciences. In M. Addis, P. C. R. Lane, P. D. Sozou, &amp; F. Gobet (Eds.), Scientific discovery in the social sciences (pp. 1-7).</p>
<p>. Springer, 10.1007/978-3-030-23769-1_1Springer. https://doi.org/10.1007/978-3-030-23769-1_1</p>
<p>How artificial intelligence can help us understand human creativity. F Gobet, G Sala, 10.3389/fpsyg.2019.01401Frontiers in Psychology. 10Gobet, F., &amp; Sala, G. (2019). How artificial intelligence can help us understand human creativity. Frontiers in Psy- chology, 10, Article 1401. https://doi.org/10.3389/fpsyg .2019.01401</p>
<p>From historical case studies to systematic methods of discovery. G Grahoff, M May, AAAI Spring Symposium on Systematic Methods of Scientific Discovery. Association for the Advancement of Artificial IntelligenceGrahoff, G., &amp; May, M. (1995). From historical case stud- ies to systematic methods of discovery. In AAAI Spring Symposium on Systematic Methods of Scientific Discovery (pp. 46-57). Association for the Advancement of Artificial Intelligence.</p>
<p>You can't take it with you: Why ability assessments don't cross cultures. P M Greenfield, 10.1037/0003-066X.52.10.1115American Psychologist. 5210Greenfield, P. M. (1997). You can't take it with you: Why ability assessments don't cross cultures. American Psychologist, 52(10), 1115-1124. https://doi.org/10.1037/0003-066X .52.10.1115</p>
<p>Clinical versus mechanical prediction: A metaanalysis. W M Grove, D H Zald, B S Lebow, B E Snitz, C Nelson, 10.1037/1040-3590.12.1.19Psychological Assessment. 121Grove, W. M., Zald, D. H., Lebow, B. S., Snitz, B. E., &amp; Nelson, C. (2000). Clinical versus mechanical prediction: A meta- analysis. Psychological Assessment, 12(1), 19-30. https:// doi.org/10.1037/1040-3590.12.1.19</p>
<p>The ethics of AI ethics: An evaluation of guidelines. Minds and Machines. T Hagendorff, 10.1007/s11023-020-09517-830Hagendorff, T. (2020). The ethics of AI ethics: An evaluation of guidelines. Minds and Machines, 30, 99-120. https:// doi.org/10.1007/s11023-020-09517-8</p>
<p>Automated discovery of scientific concepts: Replicating three recent discoveries in mechanics. Y Hakuk, Y Reich, 10.1016/j.aei.2020.101080Advanced Engineering Informatics, 44, Article 101080Hakuk, Y., &amp; Reich, Y. (2020). Automated discovery of sci- entific concepts: Replicating three recent discoveries in mechanics. Advanced Engineering Informatics, 44, Article 101080. https://doi.org/10.1016/j.aei.2020.101080</p>
<p>Conceptual combination. J Hampton, 10.4324/9780203765418Knowledge, concepts and categories. K. Lamberts &amp; D. ShanksPsychology PressHampton, J. (1997). Conceptual combination. In K. Lamberts &amp; D. Shanks (Eds.), Knowledge, concepts and catego- ries (pp. 133-159). Psychology Press. https://doi.org/ 10.4324/9780203765418</p>
<p>Neuroscience-inspired artificial intelligence. D Hassabis, D Kumaran, C Summerfield, M Botvinick, 10.1016/j.neuron.2017.06.011Neuron. 952Hassabis, D., Kumaran, D., Summerfield, C., &amp; Botvinick, M. (2017). Neuroscience-inspired artificial intelligence. Neuron, 95(2), 245-258. https://doi.org/10.1016/j.neu ron.2017.06.011</p>
<p>The weirdest people in the world?. J Henrich, S J Heine, A Norenzayan, 10.1017/S0140525X0999152XBehavioral and Brain Sciences. 332-3Henrich, J., Heine, S. J., &amp; Norenzayan, A. (2010). The weirdest people in the world? Behavioral and Brain Sciences, 33(2-3), 61-83. https://doi.org/10.1017/S01405 25X0999152X</p>
<p>On-the-fly simplification of genetic programming models. N Javed, F Gobet, 10.1145/3412841.3441926SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing. Association for Computing MachineryJaved, N., &amp; Gobet, F. (2021). On-the-fly simplification of genetic programming models. In SAC '21: Proceedings of the 36th Annual ACM Symposium on Applied Computing (pp. 464-471). Association for Computing Machinery. https://doi.org/10.1145/3412841.3441926</p>
<p>Highly accurate protein structure prediction with AlphaFold. J Jumper, R Evans, A Pritzel, T Green, M Figurnov, O Ronneberger, K Tunyasuvunakool, R Bates, A dek, A Potapenko, A Bridgland, C Meyer, S A A Kohl, A J Ballard, A Cowie, B Romera-Paredes, S Nikolov, R Jain, J Adler, . . Hassabis, D , 10.1038/s41586-021-03819-2Nature. 596Jumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., Tunyasuvunakool, K., Bates, R., dek, A., Potapenko, A., Bridgland, A., Meyer, C., Kohl, S. A. A., Ballard, A. J., Cowie, A., Romera-Paredes, B., Nikolov, S., Jain, R., Adler, J., . . . Hassabis, D. (2021). Highly accurate protein structure prediction with AlphaFold. Nature, 596, 583-589. https://doi.org/10.1038/s41586-021-03819-2</p>
<p>Hypothesis formation as design. P D Karp, Computational models of scientific discovery and theory formation. J. Shrager &amp; P. LangleyKarp, P. D. (1990). Hypothesis formation as design. In J. Shrager &amp; P. Langley (Eds.), Computational models of scientific discovery and theory formation (pp. 275-318).</p>
<p>. Morgan Kaufmann, Morgan Kaufmann.</p>
<p>Creativity.4in1: Four-criterion construct of creativity. A V Kharkhurin, 10.1080/10400419.2014.929424Creativity Research Journal. 263Kharkhurin, A. V. (2014). Creativity.4in1: Four-criterion con- struct of creativity. Creativity Research Journal, 26(3), 338-352. https://doi.org/10.1080/10400419.2014.929424</p>
<p>What can I do here? A theory of affordances in reinforcement learning. K Khetarpal, Z Ahmed, G Comanici, D Abel, D Precup, Proceedings of Machine Learning Research. Machine Learning Research119Khetarpal, K., Ahmed, Z., Comanici, G., Abel, D., &amp; Precup, D. (2020). What can I do here? A theory of affordances in reinforcement learning. Proceedings of Machine Learning Research, 119, 5243-5253.</p>
<p>Functional genomic hypothesis generation and experimentation by a robot scientist. R D King, K E Whelan, F M Jones, P G K Reiser, C H Bryant, S H Muggleton, D B Kell, S G Oliver, 10.1038/nature02236Nature. 4276971King, R. D., Whelan, K. E., Jones, F. M., Reiser, P. G. K., Bryant, C. H., Muggleton, S. H., Kell, D. B., &amp; Oliver, S. G. (2004). Functional genomic hypothesis generation and experimentation by a robot scientist. Nature, 427(6971), 247-252. https://doi.org/10.1038/nature02236</p>
<p>What have psychologists (and others) discovered about the process of scientific discovery?. D Klahr, H A Simon, 10.1111/1467-8721.00119Current Directions in Psychological Science. 103Klahr, D., &amp; Simon, H. A. (2001). What have psychologists (and others) discovered about the process of scientific discovery? Current Directions in Psychological Science, 10(3), 75-79. https://doi.org/10.1111/1467-8721.00119</p>
<p>Genetic programming: On the programming of computers by means of natural selection. J R Koza, MIT PressKoza, J. R. (1992). Genetic programming: On the programming of computers by means of natural selection. MIT Press.</p>
<p>The processes of scientific discovery: The strategy of experimentation. D Kulkarni, H A Simon, 10.1016/0364-0213(88)90020-1Cognitive Science. 122Kulkarni, D., &amp; Simon, H. A. (1988). The processes of scien- tific discovery: The strategy of experimentation. Cognitive Science, 12(2), 139-175. https://doi.org/10.1016/0364- 0213(88)90020-1</p>
<p>Analysing psychological data by evolving computational models. P C R Lane, P D Sozou, F Gobet, M Addis, Studies in classification. A. Wilhelm &amp; H. KestlerAnalysis of large and complex data. data analysis, and knowledge organizationLane, P. C. R., Sozou, P. D., Gobet, F., &amp; Addis, M. (2016). Analysing psychological data by evolving computational models. In A. Wilhelm &amp; H. Kestler (Eds.), Analysis of large and complex data. Studies in classification, data analysis, and knowledge organization (pp. 587-597).</p>
<p>. Springer, 10.1007/978-3-319-25226-1_50Springer. https://doi.org/10.1007/978-3-319-25226-1_50</p>
<p>Scientific discovery: Computational explorations of the creative processes. P Langley, H A Simon, G L Bradshaw, J M Zytkow, MIT PressLangley, P., Simon, H. A., Bradshaw, G. L., &amp; Zytkow, J. M. (1987). Scientific discovery: Computational explorations of the creative processes. MIT Press.</p>
<p>A computational model of scientific discovery in a very simple world, aiming at psychological realism. F Lara-Dammer, D R Hofstadter, R L Goldstone, 10.1080/0952813X.2019.1592234Journal of Experimental and Theoretical Artificial Intelligence. 314Lara-Dammer, F., Hofstadter, D. R., &amp; Goldstone, R. L. (2019). A computational model of scientific discovery in a very simple world, aiming at psychological realism. Journal of Experimental and Theoretical Artificial Intelligence, 31(4), 637-658. https://doi.org/10.1080/0952813X.2019.1592234</p>
<p>J Z Leibo, C De Masson D&apos;autume, D Zoran, D Amos, C Beattie, K Anderson, A Garca Castaeda, M Sanchez, S Green, A Gruslys, S Legg, D Hassabis, M M Botvinick, 10.48550/arXiv.1801.08116Psychlab: A psychology laboratory for deep reinforcement learning agents. Leibo, J. Z., de Masson d'Autume, C., Zoran, D., Amos, D., Beattie, C., Anderson, K., Garca Castaeda, A., Sanchez, M., Green, S., Gruslys, A., Legg, S., Hassabis, D., &amp; Botvinick, M. M. (2018). Psychlab: A psychology labora- tory for deep reinforcement learning agents. arXiv. https:// doi.org/10.48550/arXiv.1801.08116</p>
<p>Cognitive design for artificial minds. A Lieto, 10.4324/9781315460536Lieto, A. (2021). Cognitive design for artificial minds. Routledge. https://doi.org/10.4324/9781315460536</p>
<p>Beyond subgoaling: A dynamic knowledge generation framework for creative problem solving in cognitive architectures. A Lieto, F Perrone, G L Pozzato, E Chiodino, 10.1016/j.cogsys.2019.08.005Cognitive Systems Research. 58Lieto, A., Perrone, F., Pozzato, G. L., &amp; Chiodino, E. (2019). Beyond subgoaling: A dynamic knowledge generation framework for creative problem solving in cognitive architectures. Cognitive Systems Research, 58, 305-316. https://doi.org/10.1016/j.cogsys.2019.08.005</p>
<p>A description logic framework for commonsense conceptual combination integrating typicality, probabilities and cognitive heuristics. A Lieto, G L Pozzato, 10.1080/0952813X.2019.1672799Journal of Experimental &amp; Theoretical Artificial Intelligence. 325Lieto, A., &amp; Pozzato, G. L. (2020a). A description logic framework for commonsense conceptual combination integrating typicality, probabilities and cognitive heu- ristics. Journal of Experimental &amp; Theoretical Artificial Intelligence, 32(5), 769-804. https://doi.org/10.1080/09 52813X.2019.1672799</p>
<p>What cognitive research can do for AI: A case study. A Lieto, G L Pozzato, Proceedings of the AIxIA 2020 Discussion Papers Workshop. the AIxIA 2020 Discussion Papers Workshop2776Lieto, A., &amp; Pozzato, G. L. (2020b). What cognitive research can do for AI: A case study. Proceedings of the AIxIA 2020 Discussion Papers Workshop, 2776, 41-48.</p>
<p>A commonsense reasoning framework for explanatory emotion attribution, generation and re-classification. Knowledge-Based Systems, 227, Article 107166. A Lieto, G L Pozzato, S Zoia, V Patti, R Damiano, 10.1016/j.knosys.2021.107166Lieto, A., Pozzato, G. L., Zoia, S., Patti, V., &amp; Damiano, R. (2021). A commonsense reasoning framework for explan- atory emotion attribution, generation and re-classification. Knowledge-Based Systems, 227, Article 107166. https:// doi.org/10.1016/j.knosys.2021.107166</p>
<p>Applications of artificial intelligence for organic chemistry: The DENDRAL project. R K Lindsay, B G Buchanan, E A Feigenbaum, J Lederberg, McGraw-HillLindsay, R. K., Buchanan, B. G., Feigenbaum, E. A., &amp; Lederberg, J. (1980). Applications of artificial intelligence for organic chemistry: The DENDRAL project. McGraw-Hill.</p>
<p>Cultural bias in intelligence assessment using a culture-free test in Moroccan children. A Lozano-Ruiz, A F Fasfous, I Ibanez-Casas, F Cruz-Quintana, M Perez-Garcia, M N Prez-Marfil, 10.1093/arclin/acab005Archives of Clinical Neuropsychology. 36Lozano-Ruiz, A., Fasfous, A. F., Ibanez-Casas, I., Cruz- Quintana, F., Perez-Garcia, M., &amp; Prez-Marfil, M. N. (2021). Cultural bias in intelligence assessment using a culture-free test in Moroccan children. Archives of Clinical Neuropsychology, 36, 1502-1510. https://doi.org/10.1093/ arclin/acab005</p>
<p>Digital privacy in mental healthcare: Current issues and recommendations for technology use. S D Lustgarten, Y L Garrison, M T Sinnard, A W Flynn, 10.1016/j.copsyc.2020.03.012Current Opinion in Psychology. 36Lustgarten, S. D., Garrison, Y. L., Sinnard, M. T., &amp; Flynn, A. W. (2020). Digital privacy in mental healthcare: Current issues and recommendations for technology use. Current Opinion in Psychology, 36, 25-31. https://doi .org/10.1016/j.copsyc.2020.03.012</p>
<p>Artificial intelligence in psychological practice: Current and future applications and implications. D D Luxton, 10.1037/a0034559Professional Psychology: Research and Practice. 455Luxton, D. D. (2014). Artificial intelligence in psychological practice: Current and future applications and implications. Professional Psychology: Research and Practice, 45(5), 332-339. https://doi.org/10.1037/a0034559</p>
<p>Acute mental discomfort associated with suicide behavior in a clinical sample of patients with affective disorders: Ascertaining critical variables using artificial intelligence tools. S Morales, J Barros, O Echvarri, F Garca, A Osses, C Moya, M Paz Maino, R Fischman, C Nez, T Szmulewicz, A Tomicic, 10.3389/fpsyt.2017.00007Frontiers in Psychiatry. 8Morales, S., Barros, J., Echvarri, O., Garca, F., Osses, A., Moya, C., Paz Maino, M., Fischman, R., Nez, C., Szmulewicz, T., &amp; Tomicic, A. (2017). Acute mental discomfort associated with suicide behavior in a clinical sample of patients with affective disorders: Ascertaining critical variables using artificial intelligence tools. Frontiers in Psychiatry, 8, Article 7. https://doi.org/10.3389/fpsyt.2017.00007</p>
<p>Elements of a theory of human problem solving. A Newell, J C Shaw, H A Simon, 10.1037/h0048495Psychological Review. 653Newell, A., Shaw, J. C., &amp; Simon, H. A. (1958). Elements of a theory of human problem solving. Psychological Review, 65(3), 151-166. https://doi.org/10.1037/h0048495</p>
<p>Human problem solving. A Newell, H A Simon, Prentice HallNewell, A., &amp; Simon, H. A. (1972). Human problem solving. Prentice Hall.</p>
<p>Behavior-based machine-learning: A hybrid approach for predicting human decision making. G Noti, E Levi, Y Kolumbus, A Daniely, Noti, G., Levi, E., Kolumbus, Y., &amp; Daniely, A. (2016). Behavior-based machine-learning: A hybrid approach for predicting human decision making. arXiv. http://arxiv .org/abs/1611.10228</p>
<p>Human uncertainty makes classification more robust. J C Peterson, R Battleday, T Griffiths, O Russakovsky, 10.1109/ICCV.2019.00971Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)Institute of Electrical and Electronics EngineersPeterson, J. C., Battleday, R., Griffiths, T., &amp; Russakovsky, O. (2019). Human uncertainty makes classification more robust. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (pp. 9616-9625). Institute of Electrical and Electronics Engineers. https:// doi.org/10.1109/ICCV.2019.00971</p>
<p>Using large-scale experiments and machine learning to discover theories of human decision-making. J C Peterson, D D Bourgin, M Agrawal, D Reichman, T L Griffiths, 10.1126/science.abe2629Science. 3726547Peterson, J. C., Bourgin, D. D., Agrawal, M., Reichman, D., &amp; Griffiths, T. L. (2021). Using large-scale experiments and machine learning to discover theories of human deci- sion-making. Science, 372(6547), 1209-1214. https://doi .org/10.1126/science.abe2629</p>
<p>Using methods from machine learning to evaluate behavioral models of choice under risk and ambiguity. A Peysakhovich, J Naecker, 10.1016/j.jebo.2016.08.017Journal of Economic Behavior &amp; Organization. 133Peysakhovich, A., &amp; Naecker, J. (2017). Using methods from machine learning to evaluate behavioral models of choice under risk and ambiguity. Journal of Economic Behavior &amp; Organization, 133, 373-384. https://doi.org/10.1016/j .jebo.2016.08.017</p>
<p>Modeling value-based decision-making policies using genetic programming: A proofof-concept study. A Pirrone, F Gobet, 10.1024/1421-0185/a000241Swiss Journal of Psychology. 793-4Pirrone, A., &amp; Gobet, F. (2020). Modeling value-based deci- sion-making policies using genetic programming: A proof- of-concept study. Swiss Journal of Psychology, 79(3-4), 113-121. https://doi.org/10.1024/1421-0185/a000241</p>
<p>GEMS: Genetically evolving models in science. A Pirrone, F Gobet, 10.1422/101896Sistemi Intelligenti. 34Pirrone, A., &amp; Gobet, F. (2021). GEMS: Genetically evolv- ing models in science. Sistemi Intelligenti, 34, 107-115. https://doi.org/10.1422/101896</p>
<p>To predict human choice, consider the context. O Plonsky, I Erev, 10.1016/j.tics.2021.07.007Trends in Cognitive Sciences. 2510Plonsky, O., &amp; Erev, I. (2021). To predict human choice, consider the context. Trends in Cognitive Sciences, 25(10), 819-820. https://doi.org/10.1016/j.tics.2021.07.007</p>
<p>Psychological forest: Predicting human behavior. O Plonsky, I Erev, T Hazan, M Tennenholtz, 10.1609/aaai.v31i1.10613Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence31Article 10613Plonsky, O., Erev, I., Hazan, T., &amp; Tennenholtz, M. (2017). Psychological forest: Predicting human behav- ior. Proceedings of the AAAI Conference on Artificial Intelligence, 31(1), Article 10613. https://doi.org/10.1609/ aaai.v31i1.10613</p>
<p>Laboratory replication of scientific discovery processes. Y Qin, H A Simon, 10.1016/0364-0213(90)90005-HCognitive Science. 142Qin, Y., &amp; Simon, H. A. (1990). Laboratory replication of scientific discovery processes. Cognitive Science, 14(2), 281-312. https://doi.org/10.1016/0364-0213(90)90005-H</p>
<p>Detection and computational analysis of psychological signals using a virtual human interviewing agent. A Rizzo, S Scherer, D Devault, J Gratch, R Artstein, A Hartholt, G Lucas, S Marsella, F Morbini, A Nazarian, G Stratou, D Traum, R Wood, J Boberg, L P Morency, Journal of Pain Management. 93Rizzo, A., Scherer, S., DeVault, D., Gratch, J., Artstein, R., Hartholt, A., Lucas, G., Marsella, S., Morbini, F., Nazarian, A., Stratou, G., Traum, D., Wood, R., Boberg, J., &amp; Morency, L. P. (2016). Detection and computational analysis of psychological signals using a virtual human interviewing agent. Journal of Pain Management, 9(3), 311-321.</p>
<p>Parallel distributed processing at 25: Further explorations in the microstructure of cognition. T T Rogers, J L Mcclelland, 10.1111/cogs.12148Cognitive Science. 386Rogers, T. T., &amp; Mcclelland, J. L. (2014). Parallel distributed processing at 25: Further explorations in the microstruc- ture of cognition. Cognitive Science, 38(6), 1024-1077. https://doi.org/10.1111/cogs.12148</p>
<p>The standard definition of creativity. M A Runco, G J Jaeger, 10.1080/10400419.2012.650092Creativity Research Journal. 241Runco, M. A., &amp; Jaeger, G. J. (2012). The standard definition of creativity. Creativity Research Journal, 24(1), 92-96. https://doi.org/10.1080/10400419.2012.650092</p>
<p>The generality/ specificity of expertise in scientific reasoning. C D Schunn, J R Anderson, 10.1207/s15516709cog2303_3Cognitive Science. 233Schunn, C. D., &amp; Anderson, J. R. (1999). The generality/ specificity of expertise in scientific reasoning. Cognitive Science, 23(3), 337-370. https://doi.org/10.1207/s155 16709cog2303_3</p>
<p>Rational choice and the structure of the environment. S K Sgaier, V Huang, G Charles, 10.1037/h0042769Stanford Social Innovation Review. 183Psychological ReviewSgaier, S. K., Huang, V., &amp; Charles, G. (2020). The case for causal AI. Stanford Social Innovation Review, 18(3), 50- 55. https://ssir.org/articles/entry/the_case_for_causal_ ai# Simon, H. A. (1956). Rational choice and the structure of the environment. Psychological Review, 63(2), 129-138. https://doi.org/10.1037/h0042769</p>
<p>Creative productivity: A predictive and explanatory model of career trajectories and landmarks. D K Simonton, 10.1037/0033-295X.104.1.66Psychological Review. 1041Simonton, D. K. (1997). Creative productivity: A predictive and explanatory model of career trajectories and land- marks. Psychological Review, 104(1), 66-89. https://doi .org/10.1037/0033-295X.104.1.66</p>
<p>Scientific creativity as a combinatorial process: The chance baseline. D K Simonton, Milieus of creativity. P. Meusburger, J. Funke, &amp; E. WunderSpringerSimonton, D. K. (2009). Scientific creativity as a combina- torial process: The chance baseline. In P. Meusburger, J. Funke, &amp; E. Wunder (Eds.), Milieus of creativity (pp. 39-51). Springer.</p>
<p>Computational scientific discovery. P D Sozou, P C R Lane, M Addis, F Gobet, 10.1007/978-3-319-30526-4_33Springer handbook of model-based science. L. Magnani &amp; T. BertolottiSpringerSozou, P. D., Lane, P. C. R., Addis, M., &amp; Gobet, F. (2017). Computational scientific discovery. In L. Magnani &amp; T. Bertolotti (Eds.), Springer handbook of model-based sci- ence (pp. 719-734). Springer. https://doi.org/10.1007/978- 3-319-30526-4_33</p>
<p>Machine learning and big data: Implications for disease modeling and therapeutic discovery in psychiatry. A M Y Tai, A Albuquerque, N E Carmona, M Subramanieapillai, D S Cha, M Sheko, Y Lee, R Mansur, R S Mcintyre, 10.1016/j.artmed.2019.101704Artificial Intelligence in Medicine. 99Tai, A. M. Y., Albuquerque, A., Carmona, N. E., Subramanieapillai, M., Cha, D. S., Sheko, M., Lee, Y., Mansur, R., &amp; McIntyre, R. S. (2019). Machine learning and big data: Implications for disease modeling and thera- peutic discovery in psychiatry. Artificial Intelligence in Medicine, 99, Article 101704. https://doi.org/10.1016/j .artmed.2019.101704</p>
<p>Virtual human standardized patients for clinical training. T Talbot, A Rizzo, 10.1007/978-1-4939-9482-3_17Virtual reality for psychological and neurocognitive interventions. A. Rizzo &amp; S. BouchardSpringerTalbot, T., &amp; Rizzo, A. (2019). Virtual human standardized patients for clinical training. In A. Rizzo &amp; S. Bouchard (Eds.), Virtual reality for psychological and neurocogni- tive interventions (pp. 387-405). Springer. https://doi .org/10.1007/978-1-4939-9482-3_17</p>
<p>Artificial cognition: How experimental psychology can help generate explainable artificial intelligence. J E T Taylor, G W Taylor, 10.3758/s13423-020-01825-5Psychonomic Bulletin &amp; Review. 28Taylor, J. E. T., &amp; Taylor, G. W. (2021). Artificial cognition: How experimental psychology can help generate explain- able artificial intelligence. Psychonomic Bulletin &amp; Review, 28, 454-475. https://doi.org/10.3758/s13423-020-01825-5</p>
<p>Creative combination of representations: Scientific discovery and technological invention. P Thagard, Psychology of science: Implicit and explicit processes. R. W. Proctor &amp; E. J. CapaldiThagard, P. (2012). Creative combination of representa- tions: Scientific discovery and technological invention. In R. W. Proctor &amp; E. J. Capaldi (Eds.), Psychology of science: Implicit and explicit processes (pp. 389-404).</p>
<p>. 10.1093/acprof:oso/9780199753628.003.0016oso/9780199753628.003.0016Oxford University PressOxford University Press. https://doi.org/10.1093/acprof :oso/9780199753628.003.0016</p>
<p>AI takes its best shot: What AI can-and can't-do in the race for a coronavirus vaccine. E Waltz, 10.1109/MSPEC.2020.9205545IEEE Spectrum. 5710Waltz, E. (2020). AI takes its best shot: What AI can-and can't-do in the race for a coronavirus vaccine. IEEE Spectrum, 57(10), 24-67. https://doi.org/10.1109/ MSPEC.2020.9205545</p>
<p>On the usefulness of "value" in the definition of creativity. R W Weisberg, 10.1080/10400419.2015.1030320Creativity Research Journal. 272Weisberg, R. W. (2015). On the usefulness of "value" in the definition of creativity. Creativity Research Journal, 27(2), 111-124. https://doi.org/10.1080/10400419.2015.1030320</p>
<p>ELIZA -A computer program for the study of natural language communication between man and machine. J Weizenbaum, Communications of the ACM. 91Weizenbaum, J. (1966). ELIZA -A computer program for the study of natural language communication between man and machine. Communications of the ACM, 9(1), 36-45.</p>
<p>Using genetic programming to discover nonlinear variable interactions. C Westbury, L Buchanan, M Sanderson, M Rhemtulla, L Phillips, 10.3758/BF03202543Behavior Research Methods, Instruments, &amp; Computers. 352Westbury, C., Buchanan, L., Sanderson, M., Rhemtulla, M., &amp; Phillips, L. (2003). Using genetic programming to dis- cover nonlinear variable interactions. Behavior Research Methods, Instruments, &amp; Computers, 35(2), 202-216. https://doi.org/10.3758/BF03202543</p>
<p>The abbreviation of personality, or how to measure 200 personality scales with 200 items. T Yarkoni, 10.1016/j.jrp.2010.01.002Journal of Research in Personality. 442Yarkoni, T. (2010). The abbreviation of personality, or how to measure 200 personality scales with 200 items. Journal of Research in Personality, 44(2), 180-198. https://doi .org/10.1016/j.jrp.2010.01.002</p>
<p>Choosing prediction over explanation in psychology: Lessons from machine learning. T Yarkoni, J Westfall, 10.1177/1745691617693393Perspectives on Psychological Science. 126Yarkoni, T., &amp; Westfall, J. (2017). Choosing prediction over explanation in psychology: Lessons from machine learn- ing. Perspectives on Psychological Science, 12(6), 1100- 1122. https://doi.org/10.1177/1745691617693393</p>
<p>Criminal justice, artificial intelligence systems, and human rights. A Zavrnik, 10.1007/s12027-020-00602-0ERA Forum. 204Zavrnik, A. (2020). Criminal justice, artificial intelligence systems, and human rights. ERA Forum, 20(4), 567-583. https://doi.org/10.1007/s12027-020-00602-0</p>
<p>Development of scientific thinking. C Zimmerman, D Klahr, 10.1002/9781119170174.epcn407Stevens' handbook of experimental psychology and cognitive neuroscience. J. T. WixtedJohn Wiley &amp; Sons4th ed.Zimmerman, C., &amp; Klahr, D. (2018). Development of scien- tific thinking. In J. T. Wixted (Ed.), Stevens' handbook of experimental psychology and cognitive neuroscience (4th ed., pp. 1-25). John Wiley &amp; Sons. https://doi.org/ 10.1002/9781119170174.epcn407</p>            </div>
        </div>

    </div>
</body>
</html>
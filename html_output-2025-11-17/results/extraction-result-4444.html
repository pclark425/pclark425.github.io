<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4444 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4444</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4444</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-5b94941f9f13e3c7dc3c8b16ba341fb0f27af62b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5b94941f9f13e3c7dc3c8b16ba341fb0f27af62b" target="_blank">Analysis of Accelerating Ideation Process with Large Language Model</a></p>
                <p><strong>Paper Venue:</strong> Applied and Computational Engineering</p>
                <p><strong>Paper TL;DR:</strong> This research proposes three stages in research idea generation pipeline, pre-ideation (knowledge preparation), ideation (generation and iteration), post-ideation (evaluation), and summarizes different metrics and angles of evaluating generated research ideas.</p>
                <p><strong>Paper Abstract:</strong> Abstract. Large Language Models (LLMs) have rapidly advanced, demonstrating near-human intelligence in language comprehension, problem-solving, and autonomous decision-making, which can be leveraged in scientific discovery. Previous research of LLM for science often focuses on experiment execution part and the first stage of research, i.e., idea generation with large language models is lack of research. This paper fills this gap by surveying the use of LLM-based agent systems in research idea generation. This research first proposes three stages in research idea generation pipeline, pre-ideation (knowledge preparation), ideation (generation and iteration), post-ideation (evaluation), and provide detailed summary in different methods in each stage, then it summarizes different metrics and angles of evaluating generated research ideas, finally it discusses limitations and ethical concerns of existing works and suggest potential solutions and future directions. These results aim to provide a solid foundation for future research in improving LLM-based ideation systems and fostering responsible AI usage in scientific discovery.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4444.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4444.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Inspiration evaluation rubric</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inspiration evaluation rubric (idea quality, idea space, impact on users, social acceptance, human alignment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-angle rubric proposed to evaluate LLM-generated research ideas across five high-level perspectives: idea quality, idea space, user impact, social acceptance, and human alignment, with submetrics such as novelty, feasibility, diversity, usefulness, and fluency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Towards an evaluation of LLM-generated inspiration by developing and validating inspiration scale</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Five-angle Inspiration Evaluation Rubric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Rate candidate ideas along five high-level axes: (1) Idea Quality — assesses novelty, originality, feasibility, elaboration, completeness; (2) Idea Space — measures quantity, diversity, evenness, and depth of ideas produced; (3) Impact on Users — evaluates surprise, usefulness, motivational effects, and task influence; (4) Social Acceptance — judges acceptance, appropriateness, value, realism, and flexibility; (5) Human Alignment — checks relevance, elaboration, and fluency relative to human expectations. Each axis is composed of multiple sub-criteria which can be scored and aggregated to form composite measures of ideational quality and inspiration.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Novelty, originality, feasibility, elaboration/completeness, quantity, diversity, evenness, depth, surprise, usefulness, motivational value, task influence, social acceptance (appropriateness, value, realism, flexibility), human-alignment (relevance, elaboration, fluency).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general / cross-domain (applies to LLM-generated research ideas across fields)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>open-ended research ideas and hypotheses (creative ideation rather than formal mechanistic theories)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>This paper (survey) reports existence of the rubric from [25] but does not report numerical scores or experimental results; the rubric is presented as a comprehensive higher-level assessment used by some prior work to evaluate LLM-generated inspiration.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Designed for human-rated scoring (human raters score subcriteria); can be aggregated into automated metrics if proxies exist but the original rubric is human-centered.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Described in the referenced work [25] (title above) as being developed and validated — the survey notes the rubric but does not provide details of validation metrics here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Complexity of aggregation across heterogeneous axes; potential overlap between axes (e.g., depth vs elaboration); requires human raters (labour-intensive) and clear rater guidelines to ensure reliability; calibration across scientific domains may be needed.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Analysis of Accelerating Ideation Process with Large Language Model', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4444.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4444.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCIMON novelty assessment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SCIMON novelty-driven evaluation and iteration (Scientific Inspiration Machines Optimized for Novelty)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline that generates hypotheses grounded in prior literature and assesses novelty by comparing proposed hypotheses against existing research, prompting iterative revision until a novelty threshold is reached.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SCIMON: Scientific inspiration machines optimized for novelty</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Iterative novelty assessment via literature comparison (SCIMON)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Generate an initial hypothesis using retrieved inspirations (semantic neighbors, knowledge-graph neighbors, citation neighbors), compare the generated hypothesis to existing literature to assess similarity/novelty, and if similarity is too high, prompt the model to revise until novelty meets a target threshold. Novelty is operationalized by similarity measures against corpora of existing work (semantic/citation/knowledge-graph-based comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Novelty (similarity to existing literature), groundedness (connection to prior work), and iterative improvement until novelty threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general scientific literature (paper focuses on cross-domain scientific hypothesis generation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypotheses and research questions (novel hypothesis generation)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>The survey reports SCIMON's workflow (generate → compare → revise) and that the iteration stops when hypothesis novelty is sufficient; no numerical outcomes are reported in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Primarily automated (similarity metrics against corpora / knowledge graph comparisons) with optional human-in-the-loop for final judgment in downstream use; the survey does not report specific human evaluation details for SCIMON.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Novelty assessed via comparisons to existing literature and knowledge-graph neighborhood; the survey does not detail cross-validation against human judgments within SCIMON.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Novelty metrics based on similarity can be brittle (semantic paraphrases may appear novel but are incremental); reliance on coverage of the literature corpus; threshold-setting for 'sufficient' novelty is nontrivial and domain-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Uses prior literature corpora and knowledge-graph neighbors as the comparison set (no single standardized benchmark specified in the survey).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Analysis of Accelerating Ideation Process with Large Language Model', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4444.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4444.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reviewing-agent evaluation (NeurIPS guideline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reviewing-agent evaluation using conference review guidelines (example: NeurIPS review criteria)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Autonomous or semi-autonomous reviewing agents score generated research outputs using established conference review guidelines (e.g., NeurIPS) and those agent scores are compared to human reviewer judgments to assess alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The AI Scientist: Towards fully automated open-ended scientific discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Conference-guideline-based reviewing-agent evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Deploy reviewing agents that generate reviews and scores for candidate ideas/papers based on established human review guidelines (for example, NeurIPS review criteria). Use these agent-generated reviews as evaluation signals for ideation modules and optionally feed reviewer feedback back into generation loops. Alignment between agent and human reviewers is assessed by correlating agent scores with human expert scores.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Conference review axes (typically novelty, technical quality/significance, clarity, relevance, reproducibility/feasibility depending on the guideline used).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning / general scientific outputs (applied where conference guidelines are relevant)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>research papers and research ideas (evaluated according to peer-review criteria)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Survey notes that systems using reviewing agents (e.g., AI Scientist, ResearchAgent) compare agent evaluations to human experts by computing correlations between scores [14,17,18], but the survey does not report specific correlation values or statistical outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid: automated agent-generated reviews are compared against human expert reviews to measure alignment; agent reviews are automated but validation uses human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Correlation of agent scores with human expert scores (as reported across multiple works cited); exact validation statistics are not provided in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Agents may reproduce human reviewer biases; differences in interpretive standards can reduce correlation; lack of shared calibration between agent and human reviewers; potential circularity if agents are trained on similar review data.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>No standardized benchmark specified in the survey; systems use target conference guidelines and relevant corpora (e.g., Semantic Scholar) for similarity checks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Analysis of Accelerating Ideation Process with Large Language Model', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4444.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4444.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human interestingness scoring</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human-expert interestingness evaluation of generated research ideas</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human experts (researchers) are asked to rate generated ideas for interestingness and related subjective criteria to assess the creative value of LLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generation and human-expert evaluation of interesting research ideas using knowledge graphs and large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Human expert interestingness rating</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Collect human expert ratings on 'interestingness' (and often related dimensions such as novelty, plausibility, and significance) for each generated idea; aggregate scores across experts to produce an evaluation of idea quality and creativity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Interestingness (primary), often supplemented with novelty, plausibility/feasibility, and significance.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>varies by study; applied to domain-specific research ideas (e.g., biomedical, chemistry) or open-domain depending on dataset used by the referenced work.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>research questions and hypotheses (creative ideation outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Survey references human-expert scoring being used in prior work [23] but does not provide numerical aggregates or inter-rater reliability figures in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Human-based evaluation (expert raters); often used as ground truth to validate automated metrics or agent reviewers.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Human ratings serve as validation baseline; referenced works use inter-rater aggregation but the survey does not report specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Subjectivity of 'interestingness', rater variability, cost and scalability of expert ratings, potential domain knowledge mismatch between raters and generated idea content.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Analysis of Accelerating Ideation Process with Large Language Model', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4444.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4444.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Citation-frequency creativity proxy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Citation-frequency as inverse proxy for idea creativity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple metric where the frequency with which concepts have been cited in prior literature is used as a proxy for creativity/interestingness: more frequently cited concepts correlate with lower perceived novelty/interestingness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SCIMON: Scientific inspiration machines optimized for novelty</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Concept citation-frequency creativity metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute citation frequencies for concepts or concept pairs in the literature; use high citation frequency to indicate well-trodden (less creative) ideas, and low citation frequency to flag potentially more novel/creative ideas. This metric can be used alone or combined with other novelty measures.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Citation counts/frequency (used inversely as a proxy for novelty/interestingness).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>bibliometric / literature-based across scientific domains</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>novel research ideas and conceptual combinations</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Survey states the observation that higher citation frequency of a concept correlates with lower human-evaluated interestingness per [13]; no numerical effect sizes are reported in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated bibliometric metric (citation counts) often compared to human judgments in referenced work, but specifics are not provided in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Correlation analyses between citation-frequency metrics and human ratings are implied in the referenced study; the survey does not provide details.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Citation frequency conflates popularity with maturity — some highly-cited concepts may still yield creative new combinations; citation data lag; domain-dependent citation practices; biases from field size and publication habits.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Uses bibliometric citation counts drawn from literature corpora (no single benchmark dataset specified in the survey).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Analysis of Accelerating Ideation Process with Large Language Model', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4444.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4444.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Impact prediction (TNCSI)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Impact prediction via Topic-Normalized Citation Success Index (TNCSI)-fine-tuned LLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that fine-tunes an LLM on a topic-normalized citation success index to predict the future impact of new articles or ideas based solely on textual content.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>From words to worth: Newborn article impact prediction with LLM</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LLM-based impact prediction using TNCSI fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Fine-tune an LLM on historical articles paired with a topic-normalized citation success index for a given period; use the fine-tuned model to predict impact scores for new articles/ideas using only their textual content, thereby estimating potential scientific impact without external signals.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Predicted impact score (topic-normalized citation success), which is intended to reflect future citation-based impact adjusted for topic prevalence.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>bibliometrics / cross-domain (prediction of article impact across topics)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>predictive assessment of research impact (not mechanistic theories but impact forecasting)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Survey notes the existence of this approach [24] but does not report predictive performance metrics in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (model predicts numeric impact scores); validation in the referenced work likely involves comparing predicted scores to observed citation outcomes, but the survey does not include those results.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Reportedly validated by training on historical article outcomes (topic-normalized citation indices) in the referenced study; the survey provides no details here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Citation-based impact is a noisy and delayed proxy for scientific value; topic normalization reduces but does not eliminate confounds; predictions can be biased by training data and may not capture societal or practical impact.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Trained on historical articles paired with topic-normalized citation indices (no standard public benchmark specified in the survey).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Analysis of Accelerating Ideation Process with Large Language Model', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4444.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4444.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Similarity-check + iteration (AI Scientist / ResearchAgent)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Similarity checking against literature (Semantic Scholar/web) with iterative revision loop</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Systems check similarity between generated ideas and existing studies using APIs (e.g., Semantic Scholar) or web access and then iteratively revise ideas to improve novelty or quality, sometimes using chain-of-thought and self-reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Similarity-based novelty filter with iterative refinement</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>After idea generation, compute similarity between the generated idea and existing literature using retrieval APIs (e.g., Semantic Scholar) or web searches; if similarity is high (idea appears close to prior work), prompt the LLM to refine or produce a more novel variant. Iteration continues until a novelty or dissimilarity threshold is met or a maximum iteration budget is reached. Techniques such as chain-of-thought prompting and self-reflection are used to improve idea quality during iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Semantic similarity to existing literature (low similarity desired for novelty), plus secondary criteria like clarity and feasibility assessed during iterative review.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general / cross-domain (applies to literature-grounded ideation pipelines)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>generated research ideas and hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Survey reports that AI Scientist and other pipelines perform similarity checks and iterative refinement and that reviewing agents' evaluations are compared with human reviewers for alignment; no numeric performance metrics are provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid: automated similarity measures drive iteration; human reviewer judgments or reviewing-agent outputs are used for final assessment and alignment checks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Alignment validated by comparing agent/retrieval-guided novelty decisions with human judgments in cited works; the survey does not include numeric validation outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Similarity metrics may misclassify paraphrased but incremental work as novel; retrieval coverage limits (incomplete corpora) can create false novelty; setting iteration stopping criteria is ad hoc; dependence on external APIs raises reproducibility issues.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Uses literature corpora accessible via Semantic Scholar and web; no single standardized benchmark is specified in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Analysis of Accelerating Ideation Process with Large Language Model', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>SCIMON: Scientific inspiration machines optimized for novelty <em>(Rating: 2)</em></li>
                <li>ResearchAgent: Iterative research idea generation over scientific literature with large language models <em>(Rating: 2)</em></li>
                <li>The AI Scientist: Towards fully automated open-ended scientific discovery <em>(Rating: 2)</em></li>
                <li>Towards an evaluation of LLM-generated inspiration by developing and validating inspiration scale <em>(Rating: 2)</em></li>
                <li>Generation and human-expert evaluation of interesting research ideas using knowledge graphs and large language models <em>(Rating: 2)</em></li>
                <li>From words to worth: Newborn article impact prediction with LLM <em>(Rating: 2)</em></li>
                <li>Large language models are zero shot hypothesis proposers <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4444",
    "paper_id": "paper-5b94941f9f13e3c7dc3c8b16ba341fb0f27af62b",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [
        {
            "name_short": "Inspiration evaluation rubric",
            "name_full": "Inspiration evaluation rubric (idea quality, idea space, impact on users, social acceptance, human alignment)",
            "brief_description": "A multi-angle rubric proposed to evaluate LLM-generated research ideas across five high-level perspectives: idea quality, idea space, user impact, social acceptance, and human alignment, with submetrics such as novelty, feasibility, diversity, usefulness, and fluency.",
            "citation_title": "Towards an evaluation of LLM-generated inspiration by developing and validating inspiration scale",
            "mention_or_use": "mention",
            "evaluation_method_name": "Five-angle Inspiration Evaluation Rubric",
            "evaluation_method_description": "Rate candidate ideas along five high-level axes: (1) Idea Quality — assesses novelty, originality, feasibility, elaboration, completeness; (2) Idea Space — measures quantity, diversity, evenness, and depth of ideas produced; (3) Impact on Users — evaluates surprise, usefulness, motivational effects, and task influence; (4) Social Acceptance — judges acceptance, appropriateness, value, realism, and flexibility; (5) Human Alignment — checks relevance, elaboration, and fluency relative to human expectations. Each axis is composed of multiple sub-criteria which can be scored and aggregated to form composite measures of ideational quality and inspiration.",
            "evaluation_criteria": "Novelty, originality, feasibility, elaboration/completeness, quantity, diversity, evenness, depth, surprise, usefulness, motivational value, task influence, social acceptance (appropriateness, value, realism, flexibility), human-alignment (relevance, elaboration, fluency).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "general / cross-domain (applies to LLM-generated research ideas across fields)",
            "theory_type": "open-ended research ideas and hypotheses (creative ideation rather than formal mechanistic theories)",
            "human_comparison": null,
            "evaluation_results": "This paper (survey) reports existence of the rubric from [25] but does not report numerical scores or experimental results; the rubric is presented as a comprehensive higher-level assessment used by some prior work to evaluate LLM-generated inspiration.",
            "automated_vs_human_evaluation": "Designed for human-rated scoring (human raters score subcriteria); can be aggregated into automated metrics if proxies exist but the original rubric is human-centered.",
            "validation_method": "Described in the referenced work [25] (title above) as being developed and validated — the survey notes the rubric but does not provide details of validation metrics here.",
            "limitations_challenges": "Complexity of aggregation across heterogeneous axes; potential overlap between axes (e.g., depth vs elaboration); requires human raters (labour-intensive) and clear rater guidelines to ensure reliability; calibration across scientific domains may be needed.",
            "benchmark_dataset": null,
            "uuid": "e4444.0",
            "source_info": {
                "paper_title": "Analysis of Accelerating Ideation Process with Large Language Model",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "SCIMON novelty assessment",
            "name_full": "SCIMON novelty-driven evaluation and iteration (Scientific Inspiration Machines Optimized for Novelty)",
            "brief_description": "A pipeline that generates hypotheses grounded in prior literature and assesses novelty by comparing proposed hypotheses against existing research, prompting iterative revision until a novelty threshold is reached.",
            "citation_title": "SCIMON: Scientific inspiration machines optimized for novelty",
            "mention_or_use": "mention",
            "evaluation_method_name": "Iterative novelty assessment via literature comparison (SCIMON)",
            "evaluation_method_description": "Generate an initial hypothesis using retrieved inspirations (semantic neighbors, knowledge-graph neighbors, citation neighbors), compare the generated hypothesis to existing literature to assess similarity/novelty, and if similarity is too high, prompt the model to revise until novelty meets a target threshold. Novelty is operationalized by similarity measures against corpora of existing work (semantic/citation/knowledge-graph-based comparisons).",
            "evaluation_criteria": "Novelty (similarity to existing literature), groundedness (connection to prior work), and iterative improvement until novelty threshold.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "general scientific literature (paper focuses on cross-domain scientific hypothesis generation)",
            "theory_type": "hypotheses and research questions (novel hypothesis generation)",
            "human_comparison": null,
            "evaluation_results": "The survey reports SCIMON's workflow (generate → compare → revise) and that the iteration stops when hypothesis novelty is sufficient; no numerical outcomes are reported in this survey.",
            "automated_vs_human_evaluation": "Primarily automated (similarity metrics against corpora / knowledge graph comparisons) with optional human-in-the-loop for final judgment in downstream use; the survey does not report specific human evaluation details for SCIMON.",
            "validation_method": "Novelty assessed via comparisons to existing literature and knowledge-graph neighborhood; the survey does not detail cross-validation against human judgments within SCIMON.",
            "limitations_challenges": "Novelty metrics based on similarity can be brittle (semantic paraphrases may appear novel but are incremental); reliance on coverage of the literature corpus; threshold-setting for 'sufficient' novelty is nontrivial and domain-dependent.",
            "benchmark_dataset": "Uses prior literature corpora and knowledge-graph neighbors as the comparison set (no single standardized benchmark specified in the survey).",
            "uuid": "e4444.1",
            "source_info": {
                "paper_title": "Analysis of Accelerating Ideation Process with Large Language Model",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Reviewing-agent evaluation (NeurIPS guideline)",
            "name_full": "Reviewing-agent evaluation using conference review guidelines (example: NeurIPS review criteria)",
            "brief_description": "Autonomous or semi-autonomous reviewing agents score generated research outputs using established conference review guidelines (e.g., NeurIPS) and those agent scores are compared to human reviewer judgments to assess alignment.",
            "citation_title": "The AI Scientist: Towards fully automated open-ended scientific discovery",
            "mention_or_use": "mention",
            "evaluation_method_name": "Conference-guideline-based reviewing-agent evaluation",
            "evaluation_method_description": "Deploy reviewing agents that generate reviews and scores for candidate ideas/papers based on established human review guidelines (for example, NeurIPS review criteria). Use these agent-generated reviews as evaluation signals for ideation modules and optionally feed reviewer feedback back into generation loops. Alignment between agent and human reviewers is assessed by correlating agent scores with human expert scores.",
            "evaluation_criteria": "Conference review axes (typically novelty, technical quality/significance, clarity, relevance, reproducibility/feasibility depending on the guideline used).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "machine learning / general scientific outputs (applied where conference guidelines are relevant)",
            "theory_type": "research papers and research ideas (evaluated according to peer-review criteria)",
            "human_comparison": true,
            "evaluation_results": "Survey notes that systems using reviewing agents (e.g., AI Scientist, ResearchAgent) compare agent evaluations to human experts by computing correlations between scores [14,17,18], but the survey does not report specific correlation values or statistical outcomes.",
            "automated_vs_human_evaluation": "Hybrid: automated agent-generated reviews are compared against human expert reviews to measure alignment; agent reviews are automated but validation uses human judgments.",
            "validation_method": "Correlation of agent scores with human expert scores (as reported across multiple works cited); exact validation statistics are not provided in this survey.",
            "limitations_challenges": "Agents may reproduce human reviewer biases; differences in interpretive standards can reduce correlation; lack of shared calibration between agent and human reviewers; potential circularity if agents are trained on similar review data.",
            "benchmark_dataset": "No standardized benchmark specified in the survey; systems use target conference guidelines and relevant corpora (e.g., Semantic Scholar) for similarity checks.",
            "uuid": "e4444.2",
            "source_info": {
                "paper_title": "Analysis of Accelerating Ideation Process with Large Language Model",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Human interestingness scoring",
            "name_full": "Human-expert interestingness evaluation of generated research ideas",
            "brief_description": "Human experts (researchers) are asked to rate generated ideas for interestingness and related subjective criteria to assess the creative value of LLM outputs.",
            "citation_title": "Generation and human-expert evaluation of interesting research ideas using knowledge graphs and large language models",
            "mention_or_use": "mention",
            "evaluation_method_name": "Human expert interestingness rating",
            "evaluation_method_description": "Collect human expert ratings on 'interestingness' (and often related dimensions such as novelty, plausibility, and significance) for each generated idea; aggregate scores across experts to produce an evaluation of idea quality and creativity.",
            "evaluation_criteria": "Interestingness (primary), often supplemented with novelty, plausibility/feasibility, and significance.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "varies by study; applied to domain-specific research ideas (e.g., biomedical, chemistry) or open-domain depending on dataset used by the referenced work.",
            "theory_type": "research questions and hypotheses (creative ideation outputs)",
            "human_comparison": true,
            "evaluation_results": "Survey references human-expert scoring being used in prior work [23] but does not provide numerical aggregates or inter-rater reliability figures in this survey.",
            "automated_vs_human_evaluation": "Human-based evaluation (expert raters); often used as ground truth to validate automated metrics or agent reviewers.",
            "validation_method": "Human ratings serve as validation baseline; referenced works use inter-rater aggregation but the survey does not report specifics.",
            "limitations_challenges": "Subjectivity of 'interestingness', rater variability, cost and scalability of expert ratings, potential domain knowledge mismatch between raters and generated idea content.",
            "benchmark_dataset": null,
            "uuid": "e4444.3",
            "source_info": {
                "paper_title": "Analysis of Accelerating Ideation Process with Large Language Model",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Citation-frequency creativity proxy",
            "name_full": "Citation-frequency as inverse proxy for idea creativity",
            "brief_description": "A simple metric where the frequency with which concepts have been cited in prior literature is used as a proxy for creativity/interestingness: more frequently cited concepts correlate with lower perceived novelty/interestingness.",
            "citation_title": "SCIMON: Scientific inspiration machines optimized for novelty",
            "mention_or_use": "mention",
            "evaluation_method_name": "Concept citation-frequency creativity metric",
            "evaluation_method_description": "Compute citation frequencies for concepts or concept pairs in the literature; use high citation frequency to indicate well-trodden (less creative) ideas, and low citation frequency to flag potentially more novel/creative ideas. This metric can be used alone or combined with other novelty measures.",
            "evaluation_criteria": "Citation counts/frequency (used inversely as a proxy for novelty/interestingness).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "bibliometric / literature-based across scientific domains",
            "theory_type": "novel research ideas and conceptual combinations",
            "human_comparison": null,
            "evaluation_results": "Survey states the observation that higher citation frequency of a concept correlates with lower human-evaluated interestingness per [13]; no numerical effect sizes are reported in the survey.",
            "automated_vs_human_evaluation": "Automated bibliometric metric (citation counts) often compared to human judgments in referenced work, but specifics are not provided in this survey.",
            "validation_method": "Correlation analyses between citation-frequency metrics and human ratings are implied in the referenced study; the survey does not provide details.",
            "limitations_challenges": "Citation frequency conflates popularity with maturity — some highly-cited concepts may still yield creative new combinations; citation data lag; domain-dependent citation practices; biases from field size and publication habits.",
            "benchmark_dataset": "Uses bibliometric citation counts drawn from literature corpora (no single benchmark dataset specified in the survey).",
            "uuid": "e4444.4",
            "source_info": {
                "paper_title": "Analysis of Accelerating Ideation Process with Large Language Model",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Impact prediction (TNCSI)",
            "name_full": "Impact prediction via Topic-Normalized Citation Success Index (TNCSI)-fine-tuned LLM",
            "brief_description": "A method that fine-tunes an LLM on a topic-normalized citation success index to predict the future impact of new articles or ideas based solely on textual content.",
            "citation_title": "From words to worth: Newborn article impact prediction with LLM",
            "mention_or_use": "mention",
            "evaluation_method_name": "LLM-based impact prediction using TNCSI fine-tuning",
            "evaluation_method_description": "Fine-tune an LLM on historical articles paired with a topic-normalized citation success index for a given period; use the fine-tuned model to predict impact scores for new articles/ideas using only their textual content, thereby estimating potential scientific impact without external signals.",
            "evaluation_criteria": "Predicted impact score (topic-normalized citation success), which is intended to reflect future citation-based impact adjusted for topic prevalence.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "bibliometrics / cross-domain (prediction of article impact across topics)",
            "theory_type": "predictive assessment of research impact (not mechanistic theories but impact forecasting)",
            "human_comparison": null,
            "evaluation_results": "Survey notes the existence of this approach [24] but does not report predictive performance metrics in this paper.",
            "automated_vs_human_evaluation": "Automated (model predicts numeric impact scores); validation in the referenced work likely involves comparing predicted scores to observed citation outcomes, but the survey does not include those results.",
            "validation_method": "Reportedly validated by training on historical article outcomes (topic-normalized citation indices) in the referenced study; the survey provides no details here.",
            "limitations_challenges": "Citation-based impact is a noisy and delayed proxy for scientific value; topic normalization reduces but does not eliminate confounds; predictions can be biased by training data and may not capture societal or practical impact.",
            "benchmark_dataset": "Trained on historical articles paired with topic-normalized citation indices (no standard public benchmark specified in the survey).",
            "uuid": "e4444.5",
            "source_info": {
                "paper_title": "Analysis of Accelerating Ideation Process with Large Language Model",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Similarity-check + iteration (AI Scientist / ResearchAgent)",
            "name_full": "Similarity checking against literature (Semantic Scholar/web) with iterative revision loop",
            "brief_description": "Systems check similarity between generated ideas and existing studies using APIs (e.g., Semantic Scholar) or web access and then iteratively revise ideas to improve novelty or quality, sometimes using chain-of-thought and self-reflection.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Similarity-based novelty filter with iterative refinement",
            "evaluation_method_description": "After idea generation, compute similarity between the generated idea and existing literature using retrieval APIs (e.g., Semantic Scholar) or web searches; if similarity is high (idea appears close to prior work), prompt the LLM to refine or produce a more novel variant. Iteration continues until a novelty or dissimilarity threshold is met or a maximum iteration budget is reached. Techniques such as chain-of-thought prompting and self-reflection are used to improve idea quality during iterations.",
            "evaluation_criteria": "Semantic similarity to existing literature (low similarity desired for novelty), plus secondary criteria like clarity and feasibility assessed during iterative review.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "general / cross-domain (applies to literature-grounded ideation pipelines)",
            "theory_type": "generated research ideas and hypotheses",
            "human_comparison": true,
            "evaluation_results": "Survey reports that AI Scientist and other pipelines perform similarity checks and iterative refinement and that reviewing agents' evaluations are compared with human reviewers for alignment; no numeric performance metrics are provided in the survey.",
            "automated_vs_human_evaluation": "Hybrid: automated similarity measures drive iteration; human reviewer judgments or reviewing-agent outputs are used for final assessment and alignment checks.",
            "validation_method": "Alignment validated by comparing agent/retrieval-guided novelty decisions with human judgments in cited works; the survey does not include numeric validation outcomes.",
            "limitations_challenges": "Similarity metrics may misclassify paraphrased but incremental work as novel; retrieval coverage limits (incomplete corpora) can create false novelty; setting iteration stopping criteria is ad hoc; dependence on external APIs raises reproducibility issues.",
            "benchmark_dataset": "Uses literature corpora accessible via Semantic Scholar and web; no single standardized benchmark is specified in this survey.",
            "uuid": "e4444.6",
            "source_info": {
                "paper_title": "Analysis of Accelerating Ideation Process with Large Language Model",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "SCIMON: Scientific inspiration machines optimized for novelty",
            "rating": 2,
            "sanitized_title": "scimon_scientific_inspiration_machines_optimized_for_novelty"
        },
        {
            "paper_title": "ResearchAgent: Iterative research idea generation over scientific literature with large language models",
            "rating": 2,
            "sanitized_title": "researchagent_iterative_research_idea_generation_over_scientific_literature_with_large_language_models"
        },
        {
            "paper_title": "The AI Scientist: Towards fully automated open-ended scientific discovery",
            "rating": 2,
            "sanitized_title": "the_ai_scientist_towards_fully_automated_openended_scientific_discovery"
        },
        {
            "paper_title": "Towards an evaluation of LLM-generated inspiration by developing and validating inspiration scale",
            "rating": 2,
            "sanitized_title": "towards_an_evaluation_of_llmgenerated_inspiration_by_developing_and_validating_inspiration_scale"
        },
        {
            "paper_title": "Generation and human-expert evaluation of interesting research ideas using knowledge graphs and large language models",
            "rating": 2,
            "sanitized_title": "generation_and_humanexpert_evaluation_of_interesting_research_ideas_using_knowledge_graphs_and_large_language_models"
        },
        {
            "paper_title": "From words to worth: Newborn article impact prediction with LLM",
            "rating": 2,
            "sanitized_title": "from_words_to_worth_newborn_article_impact_prediction_with_llm"
        },
        {
            "paper_title": "Large language models are zero shot hypothesis proposers",
            "rating": 1,
            "sanitized_title": "large_language_models_are_zero_shot_hypothesis_proposers"
        }
    ],
    "cost": 0.012447,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Analysis of Accelerating Ideation Process with Large Language Model</h1>
<p>Yuhan Wei<br>Applied Maths / Computer Science Track, Duke Kunshan University, Suzhou, China</p>
<p>yw641@duke.edu</p>
<h4>Abstract</h4>
<p>Large Language Models (LLMs) have rapidly advanced, demonstrating near-human intelligence in language comprehension, problem-solving, and autonomous decision-making, which can be leveraged in scientific discovery. Previous research of LLM for science often focuses on experiment execution part and the first stage of research, i.e., idea generation with large language models is lack of research. This paper fills this gap by surveying the use of LLMbased agent systems in research idea generation. This research first proposes three stages in research idea generation pipeline, pre-ideation (knowledge preparation), ideation (generation and iteration), post-ideation (evaluation), and provide detailed summary in different methods in each stage, then it summarizes different metrics and angles of evaluating generated research ideas, finally it discusses limitations and ethical concerns of existing works and suggest potential solutions and future directions. These results aim to provide a solid foundation for future research in improving LLM-based ideation systems and fostering responsible AI usage in scientific discovery.</p>
<p>Keywords: LLM-based agent, research idea generation, scientific discovery, multi-agent collaboration.</p>
<h2>1. Introduction</h2>
<p>Large language models (LLMs) have rapidly advanced, demonstrating capabilities that approach human-level intelligence. Trained on massive datasets, they have been applied across fields such as language comprehension, problem-solving, and autonomous decision-making. These developments have led to the rise of autonomous agents-systems powered by LLMs that can perform complex tasks like planning and interacting with their environment without human intervention [1, 2]. An example is the generative AI town simulation, where multiple agents simulate human behaviors and decisionmaking processes. This highlights the potential of LLMs for broader applications, particularly in scientific discovery, where their ability to process vast amounts of data and generate insights is promising.</p>
<p>Existing research on LLMs in scientific fields has largely focused on accelerating experimental processes [3, 4]. In chemistry and drug discovery, LLMs have been used to design experiments, automate data analysis, and simulate results, significantly speeding up research. In biology and physics, these models assist in generating predictions and models, transforming how experiments are conducted and allowing for faster research cycles. A major contribution in this domain is the integration of knowledge graphs with LLMs to link scientific data and identify relationships that are not immediately</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>obvious. Literature-based discovery (LBD) techniques have been used to uncover connections between disparate knowledge, making LLMs valuable tools in automating literature reviews, hypothesis generation, and experiment validation. Some research's details how these models can process large volumes of scientific literature, summarize key findings, and suggest new research directions, improving researchers' productivity [3]. Despite these advances, the use of LLMs in the ideation process-the generation of novel research ideas-remains underexplored. Most current works focus on experiment design and hypothesis validation rather than the early stages of scientific inquiry. This presents a significant opportunity to expand the role of LLMs in ideation, helping researchers formulate innovative research questions.</p>
<p>Generating new ideas is critical to scientific discovery, requiring not only comprehensive literature review but also the ability to synthesize information into novel hypotheses. LLMs, with their capacity to process large datasets and identify patterns across disciplines, offer a promising solution. They can help researchers uncover new research directions and identify gaps in existing knowledge that might otherwise be overlooked. LLMs have the potential to assist significantly in this ideation process. By handling large amounts of information, they can identify research opportunities and suggest areas for further exploration that would be difficult for individual researchers to pinpoint. The ability of LLMs to integrate external knowledge from various disciplines also enhances their capacity for generating interdisciplinary research ideas, an increasingly important aspect of modern scientific breakthroughs.</p>
<p>This paper will talk about the advancements in LLM-based autonomous systems for research idea generation. It will explore how these systems leverage key components like profiling, memory, planning, and action modules to generate novel ideas, and how methods such as verbal self-reflection, multi-agent debates, and role-playing can enhance both creativity and factual accuracy in ideation processes. The paper will further examine the stages of the research ideation pipeline, from knowledge preparation to post-ideation evaluation, and discuss the role of human-AI collaboration in improving the quality of research ideas. Finally, it will address the current limitations, including repetitive idea generation and ethical challenges, offering insights into future directions for refining LLM-based research systems.</p>
<h1>2. Autonomous agent systems in research idea generation</h1>
<p>In recent years, achievements in LLMs have shown great promise in reaching human level intelligence. Existing studies have also leveraged this intelligence to build LLM-based agents, which are expected to perform diverse tasks [1]. A typical autonomous LLM agent consists of several essential components: a profiling module, a memory module, a planning module, and an action module. The profiling module allows the agent to understand the task context and user preferences. The memory module stores and retrieves information from past interactions, enabling the agent to learn and improve over time. The planning module is responsible for decision-making, helping the agent figure out how to approach and complete tasks. Finally, the action module enables the agent to execute tasks, such as generating responses, performing web search or interacting with external environments [2].</p>
<p>With ability to understand and generate natural language, as well as make autonomous decisions, LLM-based agents can not only process complex information but also independently complete tasks through built-in memory modules and planning capabilities. These agents are able to perform reasoning and generate new ideas by analyzing vast datasets and utilizing their own integrated knowledge, demonstrating human-like abilities in problem-solving and creative idea generation. In addition, when LLMs are leveraged in autonomous system to complete tasks, methods to enhance the factuality and creativity are explored [5-7], empowering their use in idea generation. Reflextion introduces verbal selfreflection, allowing agents to learn from their previous actions through natural language feedback stored in memory and continuously improve their performance [5]. LLM Debate leverages a multi-agent debate framework where each agent presents its arguments in favor or against a solution, while a judge evaluates the strength of their claims [6]. This improves the factuality of LLM generation by encouraging divergent thinking and mitigating the Degeneration of Thought (DoT) problem, which can occur when a single model sticks to an incorrect answer due to overconfidence. LLM Discussion builds a multi-agent discussion framework which involves initiation, discussion, and convergence stages to</p>
<p>stimulate both divergent and convergent thinking [7]. The role-playing technique further boosts creativity of the system by assigning diverse roles to each LLM, ensuring that the agents bring different perspectives and approaches to the discussion.</p>
<p>These advancements in LLM-based autonomous systems-through verbal self-reflection, debate, and multi-agent discussion frameworks-significantly improve both the factuality and creativity of research idea generation. By leveraging these methods, LLMs show promise to autonomously generate novel and accurate research ideas. Building on this foundation, recent scientific ideation studies take these innovations a step further, diving into agent systems to generate scientific hypotheses, explore new research directions, evaluate and iteratively refine these research ideas.</p>
<h1>3. Scientific ideation pipeline</h1>
<p>Similar to the process of human researchers, ideation pipeline for LLM-based agents also consists of three stages as shown in Fig.1. Pre-ideation stage starts from taking in knowledge and reading large amounts of papers and formalize this knowledge into knowledge base. Ideation stage requires researchers to find unexplored potential relationship between knowledge or get inspiration by some of the papers and initiate query to form new research ideas. During the ideation process, review and reflect on the proposed ideas, search the previous work to ensure its originality and iterate them to make them better are also important. Post-ideation includes more evaluations like further validation on novelty, assessment on feasibility and check on human-alignment. This section provides a concise summary of previous works in each stage.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. The Research Ideation Pipeline Using LLM-based Agents (Photo/Picture credit: Original).
When doing scientific research, abundant knowledge input and comprehensive literature review process are usually crucial inductive for an innovative and compelling idea. While there are several works trying to propose new research ideas and do scientific discovery simply based on text mining on large datasets of scientific literature [8, 9] or link prediction in knowledge graph [10-12], these ideas have less flexibility and are limited by the relationship between entities [13, 14]. Several studies start to leverage the internal scientific knowledge in large language models and combine few shot examples or a set of relevance papers in the prompt serving for the literature review purpose [15-18]. Another study utilizes pure web corpus instead of manually selected paper sets, targeting at a more open-domain setting [15].</p>
<p>Based on the knowledge graph, there are several ideation methods build models to discover potential research problems without using large language models. For example, the ComplEx model formalized hypothesis prediction task as link prediction and generate new hypotheses by predicting potential links in complex, multi-relational data [10]. In chemistry or drug discovery, a research uses Generative Flow Net (GFlowNet) to generate high-quality hypothesis on molecules satisfying some criteria [16-25]. In the biomedical field, frameworks (e.g., ExEmPLAR [26]) use knowledge graphs to link heterogeneous data, helping researchers to reason over complex datasets and generate novel hypotheses. Simply based on the relationship between entities, the research ideas generated seem to be limited and less capable in exploration of more open-ended research directions. This part will focus on reviewing LLM-based</p>
<p>research ideation framework. Ideation process with large language models can be formalized as input the prompt into large language models and get the output ideas:</p>
<p>$$
\mathrm{o}=\operatorname{LLM}(\mathrm{T}({10,11, \ldots, \ln }))
$$</p>
<p>In term of the prompt formats and what is incorporated in the prompt, several works have explored different methods and frameworks to improve the quality of the generated ideas. One popular way of prompt used in both experiments in zero shot [17] and AI scientist [18] is input all the seeds ideas and their experiment details. These seeds ideas are records of past generated ideas which serves as inspiration for the new round of idea generation. With all the seeds ideas, new ideation is kept relevant with the intended topic and can be adaptation and improvement of the previous ones. The work zero shot [17] also compares the quality of ideas generated by few-shot and zero-shot prompt. It finds that increasing uncertainty and model's space through zero-shot prompt enhances model's proficiency in generating hypotheses.</p>
<p>Another classical way of designing prompt is shown in ResearchAgent [14]. Like human researchers often get inspiration from one certain research paper as well as checking its relevant papers, ResearchAgent starts with a single targeted paper and also incorporates relevant papers according to their reference list in the prompt. In order to encourage more interdisciplinary research ideas and not limited by the narrow scope of the set of relevant papers, ResearchAgent also leverages external entitycentric knowledge store and retrieves top-k relevant entities besides the existing literature set. Adding these entities to prompts helps to generate more interdisciplinary ideas and tackle open-ended scenarios in scientific discovery. In term of iteration process which keeps improving ideas, previous studies have also designed different methods or built various multi-agent collaboration frameworks to enhance the proficiency in generating research ideas. Zero shot [17] designs a role-play framework in the hypothesis generation loop, entailing an analyst who interprets and extracts key words from literatures, an engineer who performs search based on the key words and compiles and structures the findings, a scientist who offers fresh interpretation and crafts hypotheses based on the findings, and a critic scrutinizing hypotheses. In addition, the framework also implements tool use methods like ReAct [19] in the engineer part, allowing the engineer agent to think before taking action, do the planning and make observations on the feedback from the environment. In ResearchAgent [14], the multi-agent collaboration occurs primarily through the interaction between research agents and reviewing agents. Three research agents responsible for problem identification, method development and experiment design have a reviewing agent for each process respectively. Based on the feedback of the reviewing agent, each research agent iteratively refines its proposal several times before their proposal is combined into the prompt for the research agent in the next stage. SciMON retrieves inspiration from related prior literature, including semantic neighbors, knowledge graph neighbors, and citation neighbors [13]. These inspirations are combined into the prompt to the large language model to generate an initial hypothesis grounded in previous work. SciMON then compares it against existing research to assess its novelty. If the hypothesis is too similar to existing ideas, the system prompts the model to revise and enhance the hypothesis to be more novel. The iteration process stops when the hypothesis reaches a sufficient level of novelty. The AI scientist builds a complete scientific research pipeline more than just idea generation [18]. In the idea generation stage, the AI scientist uses multiple rounds of chain-of-thought [20] and self-reflection [5] to refine each idea. It also checks the similarity of the idea with existing studies using tools like the Semantic Scholar API and web access [21]. After the whole pipeline of generating a paper, the AI scientist leverages a reviewing agent to review the paper, and the feedback can be given back to the idea generation agent for refinement.</p>
<p>Besides the autonomous agent framework, human-AI collaboration framework is another option for leveraging large language models to accelerate research ideation process. In the CoQuest system [22], the user first provides an initial idea or a broad topic for AI and then AI can generate research ideas through the breadth-first generation approach for the user to explore multiple research directions at the same time and be more creative or through depth-first generation for more innovative ideation. The AI Thoughts panel integrates a Paper Graph Visualizer which allows users to explore relevant academic</p>
<p>papers related to the generated research questions, ensuring that they are grounded in existing research. By retrieving and visualizing related literature, this design also helps users to understand the research landscape and identify potential gaps. The panel also offers explanations of why the AI generated specific research questions, giving transparency to the reasoning behind RQ generation, improving users' trust and encouraging them to refine their questions further. The user can provide feedback on the generated questions and AI will refine and regenerate new research ideas. A research idea can be evaluated from different angles as shown in Table 1. Various evaluation metrics are designed, but they usually contain the following angles: novelty, significance and feasibility. Ideas' relevance to one topic is also assessed [14, 17]. In addition, clarity also serves as one evaluation criteria [14, 18]. Human researchers score interestingness of ideas [23].</p>
<p>Wide-accepted angles human accessing ideas and research works can be referred in the conferences' review guidelines. For example, in [18], the reviewing agents need to provide reviews based on NeurIPS review guidelines. In these autonomous agent system with reviewing agents, the alignment between agent evaluations and those of human experts is frequently assessed by checking the correlation of the two scores $[14,17,18]$.</p>
<p>Observations on evaluation scores provide new insights for metrics designing. One observation indicates that citation of one concept can be used to evaluate the creativity of one idea: the more frequently a concept has been cited, the less interesting the research projects are evaluated [13]. Several studies also work on predict the impact of new papers or ideas. A study leverages knowledge graph and concept pairs to assess the impact of one idea [13]. Another study finetunes the large language model on Topic Normalized Citation Success Index for the same period, allowing the model to make impact prediction based only on the content of the paper instead of the external information [24]. Others provide a comprehensive higher-level assessment metrics [25]. This assessment rubric evaluates an idea from five angles: idea quality, idea space, impact of ideas on users, social acceptance, human alignment. Novelty, originality, feasibility, elaboration and completeness all fall into the metrics of idea quality. From the angle of idea space, quantity, diversity, evenness and depth are evaluated, which are slightly overlap with metrics in idea quality angle. From ideas' impact and users' perspective, metrics from computer human interaction is included like surprise, usefulness, motivational, task influence. Social acceptance angle evaluates on acceptance, appropriateness, value, realistic and flexibility. Human Alignment angle evaluates on relevance, elaboration and fluency.</p>
<p>Table 1. Comparison of Autonomous Agent Systems for Ideation Enhancement</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Idea Quality</th>
<th style="text-align: left;">Novelty</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Originality</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Creativity</td>
</tr>
<tr>
<td style="text-align: left;">Idea Space</td>
<td style="text-align: left;">Relevance</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Feasibility</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Quantity</td>
</tr>
<tr>
<td style="text-align: left;">Idea Impact on Users</td>
<td style="text-align: left;">Diversity</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Depth</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Surprise</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Importance</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Human-alignment</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Usefulness</td>
</tr>
</tbody>
</table>
<h1>4. Limitations and prospects</h1>
<p>While the advancements in LLM-based systems for research idea generation demonstrate significant potential, limitations and ethical concerns must be considered. One limitation is that similar ideas are found generated during multiple iterations. This may because large language models heavily rely on existing data and knowledge, and in multiple times of similar prompts, large language models are constraint into a narrow domain and may produce research ideas that lack novelty. Future work can</p>
<p>focus on improving the ability of LLMs to generate more diverse and novel research ideas. This could be achieved by incorporating more dynamic prompts, leveraging broader datasets, and integrating mechanisms that encourage interdisciplinary thinking. Degeneration of Thought problem, when large language models are overconfident about their reply, may also result in this repetitive idea generation problem. Future studies may work on the homogeneity problem of large language models by incorporating multi-agent role-play and conversation, which leverages group intelligence to increase the creativity of the system.</p>
<p>Better incorporating Human-AI Collaboration into agent system can also be a direction of improving the idea quality. Combining the strengths of human intuition and expertise with LLMs can help to overcome some of the limitations in autonomous idea generation. Human-AI collaboration frameworks, such as interactive systems where humans provide feedback to refine and adjust the generated ideas, can enhance both the creativity and feasibility of research outcomes. The ease with which LLMs can generate ideas at scale introduces ethical concerns and the possibility of misuse. For instance, these systems could be exploited to flood academic forums with low-quality or superficially novel ideas, potentially overwhelming peer review systems. Moreover, the lack of a solid ethical framework around the use of LLMs in research could result in the propagation of biased, harmful, or dangerous research ideas. Ensuring ethical compliance into the ideation system is also an important future direction.</p>
<h1>5. Conclusion</h1>
<p>To sum up, this study provides a comprehensive review of LLM-based research ideation systems, systematically examining the full pipeline from pre-ideation knowledge preparation, through ideation and iteration, to post-ideation evaluation. It categorized different approaches, such as knowledge graph utilization, prompt design techniques, and multi-agent collaboration frameworks, demonstrating how these methods contribute to improving quality of the generated research ideas. Additionally, this research highlighted key mechanisms like verbal self-reflection, multi-agent debate, and role-playing techniques, which have been employed to further enhance idea generation by LLMs. Despite these advancements, challenges remain. Repetitive idea generation and ethical concerns surrounding the use of LLMs in autonomous research present significant limitations. Addressing these issues will require more dynamic prompting strategies, interdisciplinary collaboration, and robust ethical frameworks to ensure responsible AI usage in research settings. These results aim to provide a solid foundation for understanding the current capabilities and limitations of LLM-based ideation systems. it is hoped that this work will inspire future research in improving these systems, particularly in fostering greater novelty, promoting ethical considerations, and advancing human-AI collaboration in scientific discovery. By tackling these challenges, LLMs can become even more effective tools for accelerating research and generating innovative ideas.</p>
<h2>References</h2>
<p>[1] Wang L, Ma C, Feng X, et al 2024 A survey on large language model based autonomous agents. Frontiers of Computer Science vol 18(6) p 186345.
[2] Xi Z, Chen W, Guo X, et al 2023 The rise and potential of large language model based agents: A survey arXiv: 230907864
[3] Zhang Y, Chen X, Jin B, Wang S, Ji S, Wang W and Han J 2024 A comprehensive survey of scientific large language models and their applications in scientific discovery arXiv: 240610833
[4] Boiko D A, MacKnight R and Gomes G 2023 Emergent autonomous scientific research capabilities of large language models arXiv: 230405332
[5] Shinn N, Cassano F, Gopinath A, Narasimhan K and Yao S 2023 Reflexion: Language agents with verbal reinforcement learning 37th Conference on Neural Information Processing Systems NeurIPS p 12.
[6] Liang T, He Z, Jiao W, et al 2023 Encouraging divergent thinking in large language models through multi-agent debate arXiv: 230519118</p>
<p>[7] Lu L C, Chen S J, Pai T M, et al 2024 LLM discussion: Enhancing the creativity of large language models via discussion framework and role-play arXiv: 240506373
[8] Srinivasan P 2003 Text mining: Generating hypotheses from MEDLINE Journal of the American Society for Information Science and Technology vol 55(5) pp 396-413
[9] Spangler S, Wilkins A D, Bachman B J, et al 2014 Automated hypothesis generation based on mining scientific literature Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining pp. 1877-1886.
[10] de Haan R, Tiddi I and Beek W 2021 Discovering research hypotheses in social science using knowledge graph embeddings The Semantic Web ESWC 2021 Lecture Notes in Computer Science (Vol 12731) Springer Cham
[11] Liu N F, Lin K, Hewitt J, Paranjape A, Bevilacqua M, Petroni F and Liang P 2023 Lost in the middle: How language models use long contexts Transactions of the Association for Computational Linguistics vol 12 pp 157-173
[12] Nadkarni R K, Wadden D, Beltagy I, Smith N A, Hajishirzi H and Hope T 2021 Scientific language models for biomedical knowledge base completion: An empirical study arXiv: 210609700
[13] Wang Q, Downey D, Ji H and Hope T 2023 SCIMON: Scientific inspiration machines optimized for novelty arXiv preprint arXiv: 230514259
[14] Baek J, Jauhar S K, Cucerzan S and Hwang S J 2024 ResearchAgent: Iterative research idea generation over scientific literature with large language models arXiv: 240407738
[15] Yang Z, Du X, Li J, Zheng J, Poria S and Cambria E 2024 Large language models for automated open-domain scientific hypotheses discovery 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024)
[16] Jain M, Deleu T, Hartford J, Liu C H, Hernandez-Garcia A and Bengio Y 2023 GFlowNets for AI-driven scientific discovery Digital Discovery vol 2(2023) pp 557-577
[17] Qi B, Zhang K, Li H, Tian K, Zeng S, Chen Z R and Zhou B 2023 Large language models are zero shot hypothesis proposers In Instruction Workshop @ NeurIPS 2023 arXiv: 231105965
[18] Chris L, Cong L, Robert T, et al 2024 The AI Scientist: Towards fully automated open-ended scientific discovery arXiv preprint arXiv: 240806292 2024b
[19] Yao S, Zhao J, Yu D, Du N, Shafran I, Narasimhan K and Cao Y 2023 ReAct: Synergizing reasoning and acting in language models arXiv: 221003629
[20] Wei J, Wang X, Schuurmans D, et al 2022 Chain-of-thought prompting elicits reasoning in large language models In Advances in Neural Information Processing Systems vol 35 pp 2482424837
[21] Fricke S 2018 Semantic scholar Journal of the Medical Library Association: JMLA vol 106(1) p 145 .
[22] Liu Y, Chen S, Cheng H et al 2024 CoQuest: Exploring research question co-creation with an LLM-based agent CHI Conference on Human Factors in Computing Systems (CHI '24) p 18.
[23] Gu X and Krenn M 2024 Generation and human-expert evaluation of interesting research ideas using knowledge graphs and large language models arXiv: 240517044
[24] Zhao P, Xing Q, Dou K, et al 2024 From words to worth: Newborn article impact prediction with LLM arXiv: 240803934
[25] Shin H, Choi S, Lim H, et al 2024 Towards an evaluation of LLM-generated inspiration by developing and validating inspiration scale 1st HEAL Workshop at the CHI Conference on Human Factors in Computing Systems p 16
[26] Beasley J M T, Korn D R, Tucker N N, et al 2024 ExEmPLAR (Extracting Exploring and Embedding Pathways Leading to Actionable Research): A user-friendly interface for knowledge graph mining Bioinformatics vol 40(1) p 779</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>(C) 2024 The Authors. This is an open access article distributed under the terms of the Creative Commons Attribution License 4.0 (https://creativecommons.org/licenses/by/4.0/).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2392 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2392</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2392</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-64.html">extraction-schema-64</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <p><strong>Paper ID:</strong> paper-a6aad9ccdcb7674a3b7ab710f0a80d205834f847</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a6aad9ccdcb7674a3b7ab710f0a80d205834f847" target="_blank">Learning Symbolic Physics with Graph Networks</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> An approach for imposing physically motivated inductive biases on graph networks to learn interpretable representations and improved zero-shot generalization is introduced and offers a valuable technique for interpreting and inferring explicit causal theories about the world from implicit knowledge captured by deep learning.</p>
                <p><strong>Paper Abstract:</strong> We introduce an approach for imposing physically motivated inductive biases on graph networks to learn interpretable representations and improved zero-shot generalization. Our experiments show that our graph network models, which implement this inductive bias, can learn message representations equivalent to the true force vector when trained on n-body gravitational and spring-like simulations. We use symbolic regression to fit explicit algebraic equations to our trained model's message function and recover the symbolic form of Newton's law of gravitation without prior knowledge. We also show that our model generalizes better at inference time to systems with more bodies than had been experienced during training. Our approach is extensible, in principle, to any unknown interaction law learned by a graph network, and offers a valuable technique for interpreting and inferring explicit causal theories about the world from implicit knowledge captured by deep learning.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2392.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2392.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GN+SR pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Network plus Symbolic Regression pipeline for law discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage method that trains a graph network to predict physical dynamics, extracts the learned pairwise message vectors, and applies symbolic regression to those messages to produce explicit algebraic laws (hypotheses) explaining the interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Graph Network + Symbolic Regression pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Supervised training of a Graph Network (GN) to predict per-node velocity updates in simulated physical systems; the GN's edge 'message' vectors (e_k') are recorded across many trajectories and treated as target data for symbolic regression. Symbolic regression (here implemented with Eureqa) searches expressions formed from arithmetic operators, analytic functions, input variables (e.g., Δx, Δy, r, m_2) and constants to fit each component of the learned message function φ^e, returning candidate algebraic laws ranked by fit (MSE) and expression complexity. Architectural inductive biases (e.g., setting message dimension L^{e'} = D) are used to encourage messages to be linear transformations of true physical forces, improving interpretability and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>physics / scientific discovery (automated law discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>open-ended discovery of governing equations / physical laws</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td>The pipeline uses model selection that trades expression complexity against fit (MSE) when choosing symbolic expressions, but the paper does not measure 'novelty' or 'feasibility' of generated hypotheses directly; it reports the selection procedure (see Occam fractional-drop metric) and qualitative generalization behavior when architectural constraints (L^{e'} = D) are applied.</td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Two complementary strategies: (1) architectural inductive bias (minimize message dimension L^{e'} to physical dimension D) to encourage interpretable/message-level representations; (2) symbolic-regression model selection that balances complexity vs MSE (see Occam fractional-drop criterion) to select concise analytical hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Graph Networks with larger message-passing dimension (L^{e'} > D)</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Qualitative: GNs with L^{e'} > D trained on a fixed number of bodies showed loss blow-up when tested on systems with more bodies, whereas GNs with L^{e'} set to the physical dimension (D) generalized better; symbolic regression recovered compact expressions (e.g., (0.46 m_2 Δy - 1.55 m_2 Δx) / r^2) that correspond to linear transformations of true forces.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>For n-body physics, constraining the GN message dimension to the physical space dimension (L^{e'} = D) encourages the message function to learn projections of true forces, facilitating symbolic regression to recover governing equations and improving zero-shot generalization to systems with more bodies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Symbolic Physics with Graph Networks', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2392.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2392.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Eureqa</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Eureqa (Schmidt & Lipson, symbolic regression software)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A symbolic regression system that searches over combinations of mathematical operators, functions, variables, and constants (via evolutionary/random search) to fit algebraic expressions to data, returning models at different complexities and fit levels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Distilling free-form natural laws from experimental data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Eureqa (symbolic regression)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Eureqa performs free-form symbolic regression by combining building blocks (operators + - × ÷, functions, constants, and input variables) to propose candidate algebraic expressions and uses an iterative search procedure to optimize fit to data (here, the GN-produced message components). The system returns a Pareto-like set of models trading complexity and fit (MSE). In this paper it was configured to allow arithmetic operators, input variables Δx, Δy, r, m_2, and real constants; expression complexity was measured by counting occurrences of operators, constants, and variables.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>general scientific discovery / equation discovery</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>open-ended discovery of analytical expressions / governing equations</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td>Expression complexity counted as number of occurrences of operators, constants, and input variables (used as a complexity score for model selection).</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td>Eureqa produces models at different complexities and fit levels; the authors used these outputs combined with an Occam-style selection rule to trade-off complexity (simplicity) against MSE (fit). The paper does not quantify novelty or feasibility beyond complexity and MSE.</td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Evolutionary/random search over expression space, with a post-hoc selection that trades model complexity vs MSE (see fractional log-MSE drop criterion).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>When given GN message data and constrained operators/variables, Eureqa recovered algebraic expressions that matched linear combinations of the true force components, demonstrating its utility for extracting interpretable physical laws from learned representations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Symbolic Physics with Graph Networks', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2392.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2392.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Occam fractional-drop</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Occam-style fractional drop in log MSE per complexity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-selection criterion introduced in this paper that selects the symbolic regression model complexity c that maximizes the fractional drop in mean square error relative to the next simplest model: argmax_c ( -Δ log(MSE_c) / Δ c ).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Occam fractional-drop model selection</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>After symbolic regression returns candidate models at multiple complexities, models are sorted by complexity c. The selection metric chooses the complexity c that maximizes the fractional decrease in MSE versus the next-simplest complexity, i.e., maximize -Δ log(MSE_c) / Δ c, which operationalizes an Occam's-razor trade-off between fit and complexity to pick a compact analytical hypothesis.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>model selection for symbolic regression / scientific law discovery</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>selection among candidate hypotheses balancing simplicity and data fit</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>Proxy: model 'complexity' (count of operators, constants, and variables); novelty not explicitly measured but simpler/more compact models are preferred by this metric.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td>Fit to data measured by mean square error (MSE); trade-off metric is fractional log-MSE drop per unit complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td>This metric explicitly encodes a trade-off between fit (MSE) and complexity: it selects the model complexity at which increasing complexity yields diminishing returns in fit, but the paper does not map this trade-off to external notions of 'feasibility' or 'novelty' beyond preferring compact models with substantial reductions in MSE.</td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Single-objective maximization of the fractional reduction in log MSE per unit increase in complexity (argmax_c -Δ log(MSE_c) / Δ c).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Other possible selection rules (e.g., pure MSE minimization, AIC/BIC-style penalties) are not used here; the chosen metric is reported as the authors' parametrization of Occam's razor.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Not quantitatively compared to other information-criterion methods in the paper; used to pick the 'best' symbolic model from Eureqa's Pareto frontier.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>Used successfully to pick compact analytical expressions that correspond to physical force laws when symbolic regression is applied to GN-extracted messages.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Symbolic Physics with Graph Networks', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Distilling free-form natural laws from experimental data <em>(Rating: 2)</em></li>
                <li>Relational inductive biases, deep learning, and graph networks <em>(Rating: 2)</em></li>
                <li>Interaction networks for learning about objects, relations and physics <em>(Rating: 2)</em></li>
                <li>Automated adaptive inference of phenomenological dynamical models <em>(Rating: 2)</em></li>
                <li>Geometry from a time series <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2392",
    "paper_id": "paper-a6aad9ccdcb7674a3b7ab710f0a80d205834f847",
    "extraction_schema_id": "extraction-schema-64",
    "extracted_data": [
        {
            "name_short": "GN+SR pipeline",
            "name_full": "Graph Network plus Symbolic Regression pipeline for law discovery",
            "brief_description": "A two-stage method that trains a graph network to predict physical dynamics, extracts the learned pairwise message vectors, and applies symbolic regression to those messages to produce explicit algebraic laws (hypotheses) explaining the interactions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Graph Network + Symbolic Regression pipeline",
            "system_description": "Supervised training of a Graph Network (GN) to predict per-node velocity updates in simulated physical systems; the GN's edge 'message' vectors (e_k') are recorded across many trajectories and treated as target data for symbolic regression. Symbolic regression (here implemented with Eureqa) searches expressions formed from arithmetic operators, analytic functions, input variables (e.g., Δx, Δy, r, m_2) and constants to fit each component of the learned message function φ^e, returning candidate algebraic laws ranked by fit (MSE) and expression complexity. Architectural inductive biases (e.g., setting message dimension L^{e'} = D) are used to encourage messages to be linear transformations of true physical forces, improving interpretability and generalization.",
            "research_domain": "physics / scientific discovery (automated law discovery)",
            "problem_type": "open-ended discovery of governing equations / physical laws",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": "The pipeline uses model selection that trades expression complexity against fit (MSE) when choosing symbolic expressions, but the paper does not measure 'novelty' or 'feasibility' of generated hypotheses directly; it reports the selection procedure (see Occam fractional-drop metric) and qualitative generalization behavior when architectural constraints (L^{e'} = D) are applied.",
            "optimization_strategy": "Two complementary strategies: (1) architectural inductive bias (minimize message dimension L^{e'} to physical dimension D) to encourage interpretable/message-level representations; (2) symbolic-regression model selection that balances complexity vs MSE (see Occam fractional-drop criterion) to select concise analytical hypotheses.",
            "human_evaluation": false,
            "human_evaluation_results": null,
            "comparative_baseline": "Graph Networks with larger message-passing dimension (L^{e'} &gt; D)",
            "comparative_results": "Qualitative: GNs with L^{e'} &gt; D trained on a fixed number of bodies showed loss blow-up when tested on systems with more bodies, whereas GNs with L^{e'} set to the physical dimension (D) generalized better; symbolic regression recovered compact expressions (e.g., (0.46 m_2 Δy - 1.55 m_2 Δx) / r^2) that correspond to linear transformations of true forces.",
            "domain_specific_findings": "For n-body physics, constraining the GN message dimension to the physical space dimension (L^{e'} = D) encourages the message function to learn projections of true forces, facilitating symbolic regression to recover governing equations and improving zero-shot generalization to systems with more bodies.",
            "uuid": "e2392.0",
            "source_info": {
                "paper_title": "Learning Symbolic Physics with Graph Networks",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Eureqa",
            "name_full": "Eureqa (Schmidt & Lipson, symbolic regression software)",
            "brief_description": "A symbolic regression system that searches over combinations of mathematical operators, functions, variables, and constants (via evolutionary/random search) to fit algebraic expressions to data, returning models at different complexities and fit levels.",
            "citation_title": "Distilling free-form natural laws from experimental data",
            "mention_or_use": "use",
            "system_name": "Eureqa (symbolic regression)",
            "system_description": "Eureqa performs free-form symbolic regression by combining building blocks (operators + - × ÷, functions, constants, and input variables) to propose candidate algebraic expressions and uses an iterative search procedure to optimize fit to data (here, the GN-produced message components). The system returns a Pareto-like set of models trading complexity and fit (MSE). In this paper it was configured to allow arithmetic operators, input variables Δx, Δy, r, m_2, and real constants; expression complexity was measured by counting occurrences of operators, constants, and variables.",
            "research_domain": "general scientific discovery / equation discovery",
            "problem_type": "open-ended discovery of analytical expressions / governing equations",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": "Expression complexity counted as number of occurrences of operators, constants, and input variables (used as a complexity score for model selection).",
            "feasibility_score": null,
            "tradeoff_evidence": "Eureqa produces models at different complexities and fit levels; the authors used these outputs combined with an Occam-style selection rule to trade-off complexity (simplicity) against MSE (fit). The paper does not quantify novelty or feasibility beyond complexity and MSE.",
            "optimization_strategy": "Evolutionary/random search over expression space, with a post-hoc selection that trades model complexity vs MSE (see fractional log-MSE drop criterion).",
            "human_evaluation": false,
            "human_evaluation_results": null,
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": "When given GN message data and constrained operators/variables, Eureqa recovered algebraic expressions that matched linear combinations of the true force components, demonstrating its utility for extracting interpretable physical laws from learned representations.",
            "uuid": "e2392.1",
            "source_info": {
                "paper_title": "Learning Symbolic Physics with Graph Networks",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Occam fractional-drop",
            "name_full": "Occam-style fractional drop in log MSE per complexity",
            "brief_description": "A model-selection criterion introduced in this paper that selects the symbolic regression model complexity c that maximizes the fractional drop in mean square error relative to the next simplest model: argmax_c ( -Δ log(MSE_c) / Δ c ).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Occam fractional-drop model selection",
            "system_description": "After symbolic regression returns candidate models at multiple complexities, models are sorted by complexity c. The selection metric chooses the complexity c that maximizes the fractional decrease in MSE versus the next-simplest complexity, i.e., maximize -Δ log(MSE_c) / Δ c, which operationalizes an Occam's-razor trade-off between fit and complexity to pick a compact analytical hypothesis.",
            "research_domain": "model selection for symbolic regression / scientific law discovery",
            "problem_type": "selection among candidate hypotheses balancing simplicity and data fit",
            "novelty_metric": "Proxy: model 'complexity' (count of operators, constants, and variables); novelty not explicitly measured but simpler/more compact models are preferred by this metric.",
            "novelty_score": null,
            "feasibility_metric": "Fit to data measured by mean square error (MSE); trade-off metric is fractional log-MSE drop per unit complexity.",
            "feasibility_score": null,
            "tradeoff_evidence": "This metric explicitly encodes a trade-off between fit (MSE) and complexity: it selects the model complexity at which increasing complexity yields diminishing returns in fit, but the paper does not map this trade-off to external notions of 'feasibility' or 'novelty' beyond preferring compact models with substantial reductions in MSE.",
            "optimization_strategy": "Single-objective maximization of the fractional reduction in log MSE per unit increase in complexity (argmax_c -Δ log(MSE_c) / Δ c).",
            "human_evaluation": false,
            "human_evaluation_results": null,
            "comparative_baseline": "Other possible selection rules (e.g., pure MSE minimization, AIC/BIC-style penalties) are not used here; the chosen metric is reported as the authors' parametrization of Occam's razor.",
            "comparative_results": "Not quantitatively compared to other information-criterion methods in the paper; used to pick the 'best' symbolic model from Eureqa's Pareto frontier.",
            "domain_specific_findings": "Used successfully to pick compact analytical expressions that correspond to physical force laws when symbolic regression is applied to GN-extracted messages.",
            "uuid": "e2392.2",
            "source_info": {
                "paper_title": "Learning Symbolic Physics with Graph Networks",
                "publication_date_yy_mm": "2019-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Distilling free-form natural laws from experimental data",
            "rating": 2
        },
        {
            "paper_title": "Relational inductive biases, deep learning, and graph networks",
            "rating": 2
        },
        {
            "paper_title": "Interaction networks for learning about objects, relations and physics",
            "rating": 2
        },
        {
            "paper_title": "Automated adaptive inference of phenomenological dynamical models",
            "rating": 2
        },
        {
            "paper_title": "Geometry from a time series",
            "rating": 1
        }
    ],
    "cost": 0.01088425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Learning Symbolic Physics with Graph Networks</h1>
<p>Miles D. Cranmer<br>Princeton University<br>Princeton, NJ, USA<br>mcranmer@princeton.edu</p>
<p>Rui Xu<br>Princeton University<br>Princeton, NJ, USA<br>ruix@princeton.edu</p>
<p>Peter Battaglia<br>DeepMind<br>London, UK<br>peterbattaglia@google.com<br>Shirley Ho<br>Flatiron Institute<br>New York City, NY, USA<br>shirleyho@flatironinstitute.org</p>
<h2>Abstract</h2>
<p>We introduce an approach for imposing physically motivated inductive biases on graph networks to learn interpretable representations and improved zero-shot generalization. Our experiments show that our graph network models, which implement this inductive bias, can learn message representations equivalent to the true force vector when trained on n-body gravitational and spring-like simulations. We use symbolic regression to fit explicit algebraic equations to our trained model's message function and recover the symbolic form of Newton's law of gravitation without prior knowledge. We also show that our model generalizes better at inference time to systems with more bodies than had been experienced during training. Our approach is extensible, in principle, to any unknown interaction law learned by a graph network, and offers a valuable technique for interpreting and inferring explicit causal theories about the world from implicit knowledge captured by deep learning.</p>
<h2>1 Introduction</h2>
<p>Discovering laws through observation of natural phenomenon is the central challenge of the sciences. Modern deep learning also involves discovering knowledge about the world but focuses mostly on implicit knowledge representations, rather than explicit and interpretable ones. One reason is that the goal of deep learning is often optimizing test accuracy and learning efficiency in narrowly specified domains, while science seeks causal explanations and general-purpose knowledge across a wide range of phenomena. Here we explore an approach for imposing physically motivated inductive biases on neural networks, training them to predict the dynamics of physical systems and interpreting their learned representations and computations to discover the symbolic physical laws which govern the systems. Moreover, our results also show that this approach improves the generalization performance of the learned models.
The first ingredient in our approach is the "graph network" (GN) [Battaglia et al., 2018], a type of graph neural network [Scarselli et al., 2009, Bronstein et al., 2017, Gilmer et al., 2017], which is effective at learning the dynamics of complex physical systems [Battaglia et al., 2016, Chang et al., 2016, Sanchez-Gonzalez et al., 2018, Mrowca et al., 2018, Li et al., 2018, Kipf et al., 2018]. We impose inductive biases on the architecture and train models with supervised learning to predict the dynamics of 2D and 3D n-body gravitational systems and a hanging string. If the trained models can accurately predict the physical dynamics of held-out test data, we can assume they have discovered some level of general-purpose physical knowledge, which is implicitly encoded in their</p>
<p>weights. Crucially, we recognize that the forms of the graph network’s message and pooling functions have correspondences to the forms of force and superposition in classical mechanics, respectively. The message pooling is what we call a “linearized latent space:” a vector space where latent representations of the interactions between bodies (forces or messages) are linear (summable). By imposing our inductive bias, we encourage the GN’s linearized latent space to match the true one. Some other interesting approaches for learning low-dimensional general dynamical models include <em>Packard et al. (1980); Daniels and Nemenman (2015)</em>, and <em>Jaques et al. (2019)</em>.</p>
<p>The second ingredient is using symbolic regression — we use eureqa from <em>Schmidt and Lipson (2009)</em> — to fit compact algebraic expressions to a set of inputs and messages produced by our trained model. eureqa works by randomly combining mathematical building blocks such as mathematical operators, analytic functions, constants, and state variables, and iteratively searches the space of mathematical expressions to find the model that best fits a given dataset. The resulting symbolic expressions are interpretable and readily comparable with physical laws.</p>
<p>The contributions of this paper are:</p>
<ol>
<li>A modified GN with inductive biases that promote learning general-purpose physical laws.</li>
<li>Using symbolic regression to extract analytical physical laws from trained neural networks.</li>
<li>Improved zero-shot generalization to larger systems than those in training.</li>
</ol>
<h2>2 Model</h2>
<p>Graph networks are a type of deep neural network which operates on graph-structured data. The format of the graphs on which GNs operate is defined as 3-tuples, $G=(\mathbf{u},V,E)$, where:</p>
<p>$\mathbf{u}\in\mathbb{R}^{L^{u}}$ is a global attribute vector of length $L^{u}$,
$V={\mathbf{v}<em i="1:N^{v">{i}}</em>}}$ is a set of node attribute vectors, $\mathbf{v<em k="k">{i}\in\mathbb{R}^{L^{v}}$ of length $L^{v}$, and
$E={(\mathbf{e}</em>)}},r_{k},s_{k<em k="k">{k=1:N^{v}}$ is a set of edge attribute vectors, $\mathbf{e}</em>$, and indices
$r_{k},s_{k}\in{1:N^{v}}$ of the “receiver” and “sender” nodes connected by the $k$-th edge.}\in\mathbb{R}^{L^{e}}$ of length $L^{e</p>
<p>Our GN implementation is depicted in fig. 1. Note: it does not include global and edge attributes. This GN processes a graph by first computing pairwise interactions, or “messages”, $\mathbf{e}<em i="i">{k}^{\prime}$, between nodes connected by edges, with a “message function”, $\phi^{e}:\mathbb{R}^{L^{v}}\times\mathbb{R}^{L^{e}}\times\mathbb{R}^{L^{v}}\rightarrow\mathbb{R}^{L^{e^{\prime}}}$. Next, the set of messages incident on each $i$-th receiver node are pooled into $\overline{\mathbf{e}}</em>}^{\prime}=\rho^{e\rightarrow v}({\mathbf{e<em r__k="r_{k">{k}^{\prime}}</em>}=i,k=1:N^{e}})$, where $\overline{\mathbf{e}<em i="i">{i}^{\prime}\in\mathbb{R}^{L^{e^{\prime}}}$, and $\rho^{e\rightarrow v}$ is a permutation-invariant operation which can take variable numbers of input vectors, such as elementwise summation. Finally, the pooled messages are used to compute node updates, $\mathbf{v}</em>$. Our specific architectural implementation is very similar to the “interaction network” (IN) variant }^{\prime}$, with a “node update function”, $\phi^{v}:\mathbb{R}^{L^{v}}\times\mathbb{R}^{L^{e^{\prime}}}\rightarrow\mathbb{R}^{L^{e^{\prime}}<em>(Battaglia et al., 2016)</em>.</p>
<p>The forms of $\phi^{e}$, $\rho^{e\rightarrow v}$, $\phi^{v}$, and the associated input and output attribute vectors have correspondences to Newton’s formulation of classical mechanics, which motivated the original development of INs. The key observation is that $\mathbf{e}<em k="k">{k}^{\prime}$ could learn to correspond to the force vector imposed on the $r</em>$ to be the force.}$-th body due to its interaction with the $s_{k}$-th body. In our examples, the force vector is equal to the derivative of the Lagrangian: $\frac{\delta\mathcal{L}}{\delta\mathbf{q}}$, and this could be generally imposed if one knows $\frac{d}{d\mathfrak{l}}(\frac{\delta\mathcal{L}}{\delta\mathbf{q}})$ and manually integrates the ODE with the output of the graph net. In a general n-body gravitational system in $n$ dimensions, note that the forces are minimally represented in an $\mathbb{R}^{n}$ vector space. Thus, if $L^{e^{\prime}}=n$, we exploit the GN’s “linearized latent space” for physical interpretability: we encourage $\mathbf{e}_{k}^{\prime</p>
<p>We sketch a non-rigorous proof-like demonstration of our hypothesis. Newtonian mechanics prescribes that force vectors, $\mathbf{f}<em k="k">{k}\in\mathcal{F}$, can be summed to produce a net force, $\sum</em>}\mathbf{f<em i="i">{k}=\overline{\mathbf{f}}\in\mathcal{F}$, which can then be used to update the dynamics of a body. Our model uses the $i$-th body’s pooled messages, $\overline{\mathbf{e}}</em>}^{\prime}$, to update the body’s velocity via Euler integration, $\mathbf{v<em i="i">{i}^{\prime}=\mathbf{v}</em>}+\phi^{v}(\mathbf{v<em i="i">{i},\overline{\mathbf{e}}</em>}^{\prime})$. If we assume our GN is trained to predict velocity updates perfectly for any number of bodies, this means $\overline{\mathbf{f}<em r__k="r_{k">{i}=\sum</em>}=i}\mathbf{f<em i="i">{k}=\phi</em>}^{v}(\sum_{r_{k}=i}\mathbf{e<em i="i">{k}^{\prime})=\phi</em>}^{v}(\overline{\mathbf{e}<em i="i">{i}^{\prime})$, where $\phi</em>}^{v}(\cdot)=\phi^{v}(\mathbf{v<em i="i">{i},\cdot)$. We have the result for a single interaction: $\overline{\mathbf{f}}</em>}=\mathbf{f<em k="k">{k,r</em>}=i}=\phi_{i}^{v}(\mathbf{e<em k="k">{k,r</em>}=i}^{\prime})=\phi_{i}^{v}(\overline{\mathbf{e}<em r__k="r_{k">{i}^{\prime})$. Thus, we can substitute into the multi-interaction case: $\sum</em>}=i}\phi_{i}^{v}(\mathbf{e<em i="i">{k}^{\prime})=\phi</em>}^{v}(\overline{\mathbf{e}<em i="i">{i}^{\prime})=\phi</em>}^{v}(\sum_{r_{k}=i}\mathbf{e<em i="i">{k}^{\prime})$, and so $\phi</em>$ has to be a linear}^{v</p>
<p>[table]1]We adhere closely to the notation used in <em>Battaglia et al. (2018)</em> to formalize our model.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: A schematic depicting how we extract physical knowledge from a GN.
transformation. Therefore, for cases where $\phi_{i}^{o}$ is invertible (mapping between the same dimensional space), $\mathbf{e}<em i="i">{k}^{\prime}=\left(\phi</em>=D$. We demonstrate this hypothesis on trained GNs in section 3.}^{o}\right)^{-1}\left(\mathbf{f}_{k}\right)$, and so the message vectors are linear transformations of the true forces when $L^{e^{\prime}</p>
<h1>3 Experiments</h1>
<p>We set up 100,000 simulations with random masses and initial conditions for both a $1 / r$ and $1 / r^{2}$ force law in 2D, a $1 / r^{2}$ law in 3D, and a string with an $r^{2}$ force law between nodes in 2D with a global gravity, for 1000 time steps each. These laws are chosen arbitrarily as examples of different symbolic forms. The three n-body problems have six bodies in their training set, and the string has ten nodes, of which the two end nodes are fixed. We train a GN on each of these problems where we choose $L^{e^{\prime}}=D$, i.e., the length of the message vectors in the GN matches the dimensionality of the force vectors: 2 for the 2D simulations and 3 for the 3D simulations. Our GN, a pure TensorFlow [Abadi et al., 2015] model, has both $\phi^{e}$ and $\phi^{s}$ as three-hidden-layer multilayer perceptrons (MLPs) with 128 hidden nodes per layer with ReLU activations. We optimize the L1 loss between the predicted velocity update and the true velocity update of each node.
Once we have a trained model, we record the messages, $\mathbf{e}<em k="k">{k}^{\prime}$, for all bodies over 1000 new simulations for each environment. We fit a linear combination of the vector components of the true force, $\mathbf{f}</em>}$, to each component of $\mathbf{e<em k="k">{k}^{\prime}$, as can be seen in fig. 2 for $1 / r$. The results for each system show that the $\mathbf{e}</em>=D$. We see similar linear relations for all other simulations.}^{\prime}$ vectors have learned to be a linear combination of the components when $L^{e^{\prime}</p>
<p>We are also able to find the force law when it is unknown by using symbolic regression to fit an algebraic function that approximates $\phi^{e}$. We demonstrate this on the trained GN for the $1 / r$ problem using eureqa from Schmidt and Lipson [2009] to fit algebraic equations that fit the message. We allow it to use algebraic operators,,$+- \times, /$, as well as input variables ( $\Delta x$ and $\Delta y$ for component separation, $r$ for distance, and $m_{2}$ for sending body mass) and real constants. Complexity is scored by counting the number of occurences of each operator, constant, and input variable. This returns a list of the models with the lowest mean square error at each complexity. We parametrize Occam's razor to find the "best" algebraic model by first sorting the best models by complexity, and then taking the model that maximizes the fractional drop in mean square error (MSE) over the next simplest model: $\arg \max <em c="c">{c}\left(-\Delta \log \left(\mathrm{MSE}</em>$,}\right) / \Delta c\right)$, where $c$ is the complexity. The best model found by the symbolic regression for the first output element of $\phi^{e}$ is $\left(0.46 m_{2} \Delta y-1.55 m_{2} \Delta x\right) / r^{2</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: These plots demonstrate that the graph network's messages have learned to be linear transformations of the two vector components of the true force: $f_{x}$ and $f_{y}$, for the $1 / r$ law in 2D.
which is a linear combination of the components of the true force, $m_{2} \hat{r} / r$. We can see this is approximately the same linear transformation as the components in the left plot of fig. 2, but this algebraic expression was learned from scratch.
We now test whether the GN will generalize to more nodes better than a GN with a larger $L^{e^{\prime}}$. This is because it is possible for a GN to "cheat" with a high dimension message-passing space, trained on a fixed number of bodies. One example of cheating would be for $\phi^{e}$ to concatenate each sending node's properties along the message, and $\phi^{e}$ to calculate forces from these and add them. When a new body is added, this calculation might break. While it is still possible for $\phi^{e}$ to develop an elaborate encoding scheme with $L^{e^{\prime}}=D$ to cheat at this problem, it seems more natural for $\phi^{e}$ to learn the true force when $L^{e^{\prime}}=D$ and therefore show improved generalization to a greater number of nodes.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: These plots demonstrate the improvement in generalization from minimizing the message passing space. The loss of GNs with different message-passing space dimension ( $L^{e^{\prime}}$ ), trained on a 6-body and 4-body system, in the left and right plots, respectively (indicated by the vertical line), are tested on a variable number of bodies in a $1 / r^{2}$ simulation in 3D.</p>
<p>We test the hypothesis of better generalization with $L^{e^{\prime}}=D$ in fig. 3, by training GNs with different $L^{e^{\prime}}$ on the 3D $1 / r^{2}$ simulations. The observed trend is that systems with $L^{e^{\prime}}&gt;D$ see their loss blow up with a larger number of bodies - presumably because they have "cheated" slightly and not learned the force law in $\phi^{e}$ but in a combination of $\phi^{e}$ and $\phi^{e}$, whereas the $L^{e^{\prime}} \in{2,3}$ systems' $\phi^{e}$ has learned a projection of the true forces and is able to generalize better for greater number of bodies. A conclusion of this may be that one can optimize GNs by minimizing $L^{e^{\prime}}$ to the known minimum dimension required to transmit information (e.g., 3 for 3D forces), or, if this dimension is unknown, until the loss drops off.</p>
<h1>4 Conclusion</h1>
<p>We have demonstrated an approach for imposing physically motivated inductive biases on graph networks to learn interpretable representations and improved zero-shot generalization. We have shown through experiment that our graph network models which implement this inductive bias can learn message representations equivalent to the true force vector for n-body gravitational and spring-like simulations in 2D and 3D. We also have demonstrated a generic technique for finding an unknown force law: symbolic regression models to fit explicit algebraic equations to our trained model's message function. Because GNs have more explicit sub-structure than their more homogeneous deep learning relatives (e.g., plain MLPs, convolutional networks), we can draw more fine-grained interpretations of their learned representations and computations. Finally, we have demonstrated that our model generalizes better at inference time to systems with more bodies than had been experienced during training.
Acknowledgments: Miles Cranmer and Rui Xu thank Professor S.Y. Kung for insightful suggestions on early work, as well as Zejiang Hou for his comments on an early presentation. Miles Cranmer would like to thank David Spergel for advice on this project, and Thomas Kipf, Alvaro Sanchez, and members of the DeepMind team for helpful comments on a draft of this paper. We thank the referees for insightful comments that both improved this paper and inspired future work.</p>
<h2>References</h2>
<p>M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Mané, R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Viégas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL https: //www.tensorflow.org/. Software available from tensorflow.org.
P. Battaglia, R. Pascanu, M. Lai, D. J. Rezende, et al. Interaction networks for learning about objects, relations and physics. In Advances in neural information processing systems, pages 4502-4510, 2016.
P. W. Battaglia, J. B. Hamrick, V. Bapst, A. Sanchez-Gonzalez, V. Zambaldi, M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018.
M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst. Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine, 34(4):18-42, 2017.
M. B. Chang, T. Ullman, A. Torralba, and J. B. Tenenbaum. A compositional object-based approach to learning physical dynamics. arXiv preprint arXiv:1612.00341, 2016.
B. C. Daniels and I. Nemenman. Automated adaptive inference of phenomenological dynamical models. Nature Communications, 6(1):1-8, Aug. 2015. ISSN 2041-1723.
J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl. Neural message passing for quantum chemistry. In Proceedings of the 34th International Conference on Machine LearningVolume 70, pages 1263-1272. JMLR. org, 2017.
M. Jaques, M. Burke, and T. Hospedales. Physics-as-Inverse-Graphics: Joint Unsupervised Learning of Objects and Physics from Video. arXiv:1905.11169 [cs], May 2019.
T. Kipf, E. Fetaya, K.-C. Wang, M. Welling, and R. Zemel. Neural relational inference for interacting systems. arXiv preprint arXiv:1802.04687, 2018.
Y. Li, J. Wu, R. Tedrake, J. B. Tenenbaum, and A. Torralba. Learning particle dynamics for manipulating rigid bodies, deformable objects, and fluids. arXiv preprint arXiv:1810.01566, 2018.
D. Mrowca, C. Zhuang, E. Wang, N. Haber, L. F. Fei-Fei, J. Tenenbaum, and D. L. Yamins. Flexible neural representation for physics prediction. In Advances in Neural Information Processing Systems, pages 8799-8810, 2018.</p>
<p>N. H. Packard, J. P. Crutchfield, J. D. Farmer, and R. S. Shaw. Geometry from a time series. Physical review letters, 45(9):712, 1980.
A. Sanchez-Gonzalez, N. Heess, J. T. Springenberg, J. Merel, M. Riedmiller, R. Hadsell, and P. Battaglia. Graph networks as learnable physics engines for inference and control. arXiv preprint arXiv:1806.01242, 2018.
F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61-80, 2009.
M. Schmidt and H. Lipson. Distilling free-form natural laws from experimental data. Science, 324 (5923):81-85, 2009.</p>            </div>
        </div>

    </div>
</body>
</html>
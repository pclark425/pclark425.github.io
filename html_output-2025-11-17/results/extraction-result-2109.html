<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2109 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2109</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2109</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-54.html">extraction-schema-54</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <p><strong>Paper ID:</strong> paper-276575610</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.16069v1.pdf" target="_blank">Curie: Toward Rigorous and Automated Scientific Experimentation with AI Agents</a></p>
                <p><strong>Paper Abstract:</strong> Scientific experimentation, a cornerstone of human progress, demands rigor in reliability, methodical control, and interpretability to yield meaningful results. Despite the growing capabilities of large language models (LLMs) in automating different aspects of the scientific process, automating rigorous experimentation remains a significant challenge. To address this gap, we propose Curie, an AI agent framework designed to embed rigor into the experimentation process through three key components: an intra-agent rigor module to enhance reliability, an inter-agent rigor module to maintain methodical control, and an experiment knowledge module to enhance interpretability. To evaluate Curie, we design a novel experimental benchmark composed of 46 questions across four computer science domains, derived from influential research papers, and widely adopted open-source projects. Compared to the strongest baseline tested, we achieve a 3.4$\times$ improvement in correctly answering experimental questions. Curie is open-sourced at https://github.com/Just-Curieous/Curie.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2109.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2109.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Curie</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Curie: AI agent framework for rigorous automated experimentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-agent framework that embeds rigor into end-to-end computational experimentation via an Experimental Rigor Engine consisting of intra-agent validators, inter-agent control, and a structured experiment knowledge store; evaluated on a 46-task CS benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Curie</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Multi-agent system (Architect + Technician agents) mediated by an Experimental Rigor Engine (Intra-ARM, Inter-ARM, Experiment Knowledge Module) to generate, execute, validate, and document computational experiments in software/ML/cloud domains.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer science (LLM reasoning, vector DB, cloud computing, ML training); envisioned for broader scientific use but evaluated in CS</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>simulated</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Validation is performed computationally: (1) Intra-ARM runs modular validators (Experimental Setup Validator and Execution Validator) which check plan alignment, I/O correctness, placeholder detection, and execution in 'clean' environments; (2) reproducibility checks by executing workflows multiple times; (3) results and metadata are stored in the Experiment Knowledge Module (time-machine provenance) for traceability; (4) automated evaluation via an LLM judge for design/setup/conclusion checks, with manual expert assessment for implementation-alignment (semantic code correctness); (5) benchmark comparisons against ground-truth derived from published papers and open-source benchmarks, reporting averaged metrics across five independent trials per task.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>empirical software-execution fidelity: experiments are real code runs in isolated/clean computational environments (not physics or wet-lab). Fidelity is high for software/ML experiments insofar as the code executes in the provided environment, but limited by starter-code completeness, environment dependencies, and LLM hallucinated code; not a physics or instrument-level high-fidelity simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Paper argues computational validation is sufficient for the targeted CS domains (software, ML experiments) where reproducible code execution and comparison to benchmark results are domain norms; the paper explicitly notes this validation is not sufficient for wet-lab or physical sciences where physical/experimental validation is required and recommends domain-specific adaptation for such disciplines.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>Reported benchmark metrics: Curie weighted-average metrics across tasks: Design 97.9%, Execution (setup) 78.1%, Implementation Alignment 73.4%, Conclusion Correctness 36.1% (these are the paper's reported aggregated percentages). Domain-specific: LLM reasoning conclusion accuracy 44.9%; execution-setup performance ranges (e.g., 66.7% to 92.7% reported for certain categories).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>No physical (wet-lab or hardware-in-the-loop) experiments were performed. Validation consisted of running generated experimental code in isolated computational environments, multiple runs per task (five trials) to assess reproducibility, and automated/manual evaluation against ground-truth derived from papers/benchmarks. The paper notes lack of physical experiments as a limitation for non-CS domains.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>The paper compares automated LLM-judge verification to manual expert assessment for implementation alignment: LLM judge used for design/setup/conclusion where ground truth is available; manual checking used for semantic code alignment (implementation-alignment). It also compares Curie vs. baseline agents (OpenHands, Magentic) across the same validation pipeline, showing Curie substantially outperforms baselines; no wet-lab vs simulation comparisons because all evaluations are computational.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Failures/limitations documented: (1) Cascading errors when earlier pipeline steps fail (incorrect setups, unresolved dependencies) reduce conclusion correctness; (2) Baselines and sometimes Curie encountered execution failures (syntax errors, logic mistakes, unresolved dependencies); (3) Conclusion correctness remains relatively low (36.1% overall), indicating validation/experimentation pipeline still fails to produce correct conclusions frequently; (4) LLM hallucination can inject incorrect implementations or metadata; (5) domain transfer limitations — CS-style computational validation insufficient for biological/physical sciences.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Successes documented: (1) Curie reproduced and extended findings from LLM-reasoning case studies (e.g., confirming repeated-sampling benefits and exploring sampling temperature effects); (2) High design correctness rates (~97.9%) and strong execution/setup reliability in many tasks (e.g., execution setup up to 92.7% in some domains); (3) In domains with well-established benchmarks and shorter runs (Vector DB), alignment and execution succeed at higher rates (alignment up to ~77.2% in domain-specific tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Ground truth derived from published research results and official open-source project benchmarks; agents' outputs were compared to these ground-truths using the LLM judge and manual checks. The paper reports per-metric success rates based on these comparisons (see aggregated percentages).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>The paper enforces reproducibility practices within the system: runs each task five independent trials and reports averages; Experiment Knowledge Module records versioned 'time-machine' provenance. External independent replication by outside groups is not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Paper discusses resource constraints qualitatively (GPU/agent availability, long-running tasks like cloud computing require scheduling and monitoring) and describes partition scheduling to improve efficiency. No quantitative monetary cost or wall-clock time breakdowns are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>States that for software/ML/cloud (CS) domains reproducible code execution and benchmark comparisons are accepted norms; explicitly notes that biological and materials science require wet-lab/physical validation and that Curie needs domain-specific adaptations for such norms.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Uncertainty is handled by multiple runs (five trials) and reporting averaged metrics; the system logs execution errors and anomalies to help detect hidden dependencies. No formal statistical confidence intervals or probabilistic uncertainty estimates are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Limitations include reliance on LLMs prone to hallucination, dependence on provided starter code/environments, lack of physical experiment capabilities, limited statistical treatment of uncertainty (no CIs), and residual low conclusion accuracy indicating validation pipeline incompleteness.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td>Combines automated computational execution + automated LLM-judge evaluation + manual expert assessment for semantic alignment. Intra-ARM validators run automatically; LLM judge checks design/setup/conclusion; manual human review is used for implementation-alignment checks on a subset to validate and calibrate the judge.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2109.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2109.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Intra-ARM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Intra-Agent Rigor Module (Intra-ARM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A suite of modular validators that enforce step-by-step reliability inside individual agents by validating experiment plans, setups, and executions before forwarding to technicians or committing results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Intra-ARM (Experimental Rigor Engine component)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Provides continuous, modular validation across stages (e.g., Experimental Setup Validator, Execution Validator) to detect errors early, validate alignment with the plan, and ensure reproducibility via controlled environment execution.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer science experimentation and software/ML experiments</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>simulated</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Contains validators that (a) check that experimental setups align with the experimental plan and variables, detect placeholders/hardcoded values, verify I/O argument handling, and require documentation of intermediate steps; (b) execute the setup in a clean environment to verify error-free execution, log errors, and perform reproducibility checks by rerunning workflows multiple times.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>Software-run validation: high fidelity for checking code execution in the provided environment (unit/functional level), but does not simulate external hardware or physical processes.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Sufficient for detecting coding/runtime issues and ensuring computational reproducibility in CS tasks; not sufficient for physical experiments in other domains.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>No single numeric accuracy reported for Intra-ARM alone; contributes to aggregate execution/setup scores (Curie execution_setup ~78.1% weighted average); example ranges in text: execution setup performance between ~66.7% and 92.7% for certain tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>Validators perform real code execution in isolated computational environments as part of the system; authors ran many tasks through these validators in the benchmark, but no physical experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>The module is contrasted with naive end-to-end validation approaches — the paper argues Intra-ARM's continuous/modular validation is superior because it enables error isolation and avoids full reruns. No quantitative head-to-head numbers for validators vs. end-to-end only approach beyond aggregate system performance.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Limitations include inability to detect higher-level semantic mismatches between methodology and code without manual inspection, and residual failures when environment dependencies or incomplete starter code exist.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Successfully caught syntax mistakes, placeholder/hardcoded inputs, missing I/O handling, and runtime exceptions; contributed to improved setup/execution rates relative to baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Validation focuses on internal correctness and reproducibility rather than comparison to external ground truth; final outputs are compared to ground truth at evaluation stage.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Enforced via multiple runs and clean-environment execution to improve reproducibility; recorded provenance in Experiment Knowledge Module.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Introduces overhead for repeated runs and staged validation but reduces cost of large reruns by catching early-stage errors; no numeric cost/time provided.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Appropriate for software and computational experiment norms (unit runs, repeated trials); not adequate for wet-lab norms.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Relies on repeated runs to detect non-determinism and anomalies; no probabilistic uncertainty models reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Cannot fully verify semantic alignment of complex methodology with code (requires manual review), limited by the completeness of the environment and starter code, and subject to LLM-generated incorrect code.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td>Integrated with Inter-ARM and Experiment Knowledge Module; validators produce artifacts consumed by automated LLM judge and by human reviewers for final alignment checks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2109.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2109.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Experimental Setup Validator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Experimental Setup Validator (component of Intra-ARM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Validator that inspects proposed experiment setups for methodological soundness, plan alignment, correct I/O handling, absence of placeholders/hardcoding, and presence of documented intermediate steps and expected results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Experimental Setup Validator</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Static and semantic checks on experiment setup artifacts (code and metadata) to ensure alignment with the research question and experimental plan before execution.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer science experiments / software engineering workflows</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>simulated</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Performs automated checks that: (1) variables (independent/dependent/constant) match the experimental plan; (2) input/output args are handled correctly; (3) no placeholders/hardcoded values remain; (4) intermediate steps and expected results are documented. Blocks progression to execution until checks pass.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>Static analysis and metadata checking: high fidelity for syntactic/structural issues and some semantic checks but limited on deep behavioral semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Sufficient to catch many classes of setup errors common in computational experiments, though insufficient to guarantee semantic correctness of complex experimental algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>No scalar accuracy given; contributes to overall design/execution pipeline improvements shown in system-level metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>Used across the benchmark to gate execution; the paper provides example errors detected (placeholders, missing I/O), but no per-validator statistics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Framed as superior to one-time end-of-run validation; no direct numerical comparison presented.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Cannot detect subtle mismatches where code superficially aligns with plan but implements incorrect methodology; requires execution and manual inspection for deeper verification.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Detected and prevented many malformed setups from executing, reducing cascading errors downstream.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Not directly compared to ground truth; used to ensure setup fidelity prior to running experiments that are compared to ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>By enforcing complete metadata and removing hardcoded parts, improves reproducibility across runs.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Adds validation latency before execution but reduces cost of re-running failed experiments; no numeric trade-offs provided.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Matches CS norms of reproducible, runnable experimental code; insufficient for wet-lab protocol validation.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>No explicit uncertainty metrics; functions deterministically as a gate.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Limited semantic checking; may pass setups that are syntactically correct but methodologically wrong.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td>Works together with Execution Validator and higher-level LLM judge/manual review; acts as the first gate in a hybrid multi-stage validation pipeline.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2109.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2109.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Execution Validator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Execution Validator (component of Intra-ARM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A runtime validator that executes the experimental setup in a controlled clean environment to ensure error-free runs and reproducibility across multiple executions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Execution Validator</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Runs the experiment in isolated environments, logs errors with actionable debugging details, and reruns workflows to check reproducibility and detect anomalies or hidden dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer science experiments and software/ML workloads</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>simulated</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Automated execution in clean containers/environments, capturing runtime logs, error traces, and producing reproducibility metrics via multiple runs. Ensures outputs align with expected artifacts and that workflows complete without crashes.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>High-fidelity execution for software experiments within the provided environment; cannot emulate external infrastructures beyond the configured environment.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Sufficient to detect execution/runtime issues in computational experiments; combined with setup validation, it addresses many practical reproducibility requirements in CS.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>No single accuracy number for validator; system-level execution/setup metric for Curie aggregated to ~78.1% weighted average indicates substantial but not perfect execution reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>Experiment runs are real code executions used across the benchmark; errors and reproducibility issues are logged and used for iterative refinement by agents.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Compared qualitatively to naive single-run validation; multiple runs and clean environment execution cited as stronger practice; no direct numeric comparison given.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Fails when environment dependencies are missing, when code is semantically incorrect despite running, or when hidden nondeterministic dependencies exist (leading to inconsistent outputs across runs).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Successfully identified and prevented propagation of runtime errors and non-deterministic behaviours for many benchmark tasks, contributing to higher setup/execution scores relative to baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Execution Validator ensures runnable outputs that are then compared to ground truth at evaluation stage; it itself does not compare scientific results to gold standards.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Designed to increase reproducibility by repeating runs and using clean isolated environments; five-trial averaging for tasks demonstrates application.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Rerunning workflows increases runtime cost; scheduler and partitioning aim to manage resource/time trade-offs for large experiments. No numeric cost/time reported.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Aligns with software-engineering norms for isolated, repeatable execution; not intended for physical experiment protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Detects run-to-run variability via repeated execution; no probabilistic uncertainty estimates provided.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Cannot detect conceptual methodological errors that still run without errors; dependent on quality of the environment and input data.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td>Feeds execution artifacts and logs into the Experiment Knowledge Module and LLM judge for higher-level evaluation and into manual review when needed.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2109.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2109.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Experiment Knowledge Module</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Experiment Knowledge Module (structured knowledge/time-machine)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A structured experiment metadata and provenance store that records plans, execution artifacts, versioned changes, and a DAG-like time-machine history to improve interpretability and traceability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Experiment Knowledge Module</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Organizes and enforces structured reads/writes, time-stamped provenance, tiered write access, and a DAG-like history for recovering past states and diagnosing issues, thereby supporting validation and reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>All experimental domains but implemented/evaluated in computer science experiments</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>none</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Not a validator per se but a provenance/knowledge system that supports validation by storing experimental plans, intermediate results, and change histories; enforces tiered write access and validates metadata before committing.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>N/A (data-store and metadata fidelity depends on inputs and validators that generate records).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Essential supporting infrastructure for validation (traceability, reproducibility) but not sufficient to validate scientific correctness by itself.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>Not applicable as it stores metadata; accuracy depends on upstream validators and agents writing correct info. The paper reports that it helped Curie achieve better recall and adaptation, improving conclusion accuracy indirectly.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>Stores the outputs of computational experiment runs and validation checks; used across benchmark runs to reconstruct experiments and audit agent decisions; authors used it in system evaluation but did not report independent validation experiments for the store itself.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Compared conceptually to relying solely on LLM memory (which is inconsistent); Experiment Knowledge Module provides structured provenance which is argued to be superior. No numeric comparison provided.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Potential failure modes include garbage-in/garbage-out if LLM agents write hallucinated or incorrect records; mitigated via tiered write access and validators but not eliminated.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Improved interpretability and enabled rollback and tracing of failures during benchmark experiments; credited with improving Curie's adaptation and recall in iterative experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Acts as the provenance store for experiments compared to ground truth; no direct comparison metric.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Enables reproducing past experiment states via recorded DAG-like history; used to rerun partitions and reconstruct prior conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Overhead for structured writes and validation checks; reduces long-term debugging and rerun costs by preserving provenance. No quantitative overhead reported.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Matches good-practice norms across domains: provenance, auditable records, and restricted edits are required for reproducibility and auditability.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Records error logs and anomalies but does not itself quantify statistical uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Dependent on correctness of writes; LLM hallucinations risk corrupting records despite tiered access. Requires validators to maintain integrity.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td>Works as the central store that integrates outputs from automated validators, LLM judge assessments, and human reviewer annotations to form a hybrid validation evidence trail.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2109.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2109.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM Judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated LLM-based experiment verifier (LLM judge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based automated evaluator used to verify experiment design, runnable setup, and conclusion correctness where ground truth is available; supplemented by manual checks for implementation-alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM Judge</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An automated LLM verifier invoked to check experiment design, execution setup, and conclusions against provided ground truth and logs; designed with a strict system prompt and used at scale for benchmark evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General-purpose experiment verification across computational scientific tasks (applied here to CS benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>The LLM judge consumes experiment logs, the original experiment question, and ground truth (where available) to produce structured pass/fail assessments for design, execution, implementation alignment (where possible), and conclusion correctness; a subset of its outputs was cross-checked by human experts to calibrate and measure agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>N/A (automated judgment rather than simulation); fidelity depends on prompt design and LLM capability.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Used as a scalable verifier for tasks with clear ground truth; the paper acknowledges that detecting semantic discrepancies in code (implementation alignment) is non-trivial and requires manual assessment, so the LLM judge is not by itself sufficient for full validation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>No absolute accuracy number given for the judge alone; the paper reports that they cross-checked a subset of evaluations against expert annotations to measure agreement and refine prompts, but exact agreement rates are not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>Used during benchmark evaluation to grade many tasks automatically; where ground truth exists the judge performs direct comparisons; manual checks used for hard semantic checks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Compared qualitatively to manual assessment: LLM judge scales but can miss semantic code issues; manual review remains necessary for implementation-alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Known failure mode: difficulty detecting semantic mismatches between intended methodology and generated code, which can lead to false passes on implementation alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Used effectively for design/setup/conclusion checks when ground truth is explicit and for scaling the evaluation across 46 tasks and 5 trials each.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Directly compares logs and reported conclusions to provided ground truth in tasks; used as the primary automated verification mechanism for benchmark scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Authors cross-checked a subset of LLM-judge outputs with expert annotation to validate the judge's reliability; no independent external replication reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Automates large-scale verification saving human time; the trade-off is possible missed semantic errors requiring human follow-up. No quantitative time savings provided.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Acceptable for automated checks where ground truth is explicit and tasks are computational; insufficient for deep semantic code verification or non-computational domains without human review.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>No formal uncertainty estimates reported for judge outputs; calibration performed via cross-check with experts but no numeric confidence metrics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Prone to mistakes on semantic code correctness and subject to prompt sensitivity; needs human calibration and spot-checking.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td>LLM judge outputs are combined with manual expert checks for implementation alignment; used together with Intra-ARM validators and Experiment Knowledge Module to form a hybrid verification pipeline.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Toward rigorous assessment of language agents for data-driven scientific discovery <em>(Rating: 2)</em></li>
                <li>The ai scientist: Towards fully automated open-ended scientific discovery <em>(Rating: 2)</em></li>
                <li>Large language monkeys: Scaling inference compute with repeated sampling <em>(Rating: 1)</em></li>
                <li>Agent laboratory: Using llm agents as research assistants <em>(Rating: 2)</em></li>
                <li>Mlagentbench: Evaluating language agents on machine learning experimentation <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2109",
    "paper_id": "paper-276575610",
    "extraction_schema_id": "extraction-schema-54",
    "extracted_data": [
        {
            "name_short": "Curie",
            "name_full": "Curie: AI agent framework for rigorous automated experimentation",
            "brief_description": "An LLM-agent framework that embeds rigor into end-to-end computational experimentation via an Experimental Rigor Engine consisting of intra-agent validators, inter-agent control, and a structured experiment knowledge store; evaluated on a 46-task CS benchmark.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Curie",
            "system_description": "Multi-agent system (Architect + Technician agents) mediated by an Experimental Rigor Engine (Intra-ARM, Inter-ARM, Experiment Knowledge Module) to generate, execute, validate, and document computational experiments in software/ML/cloud domains.",
            "scientific_domain": "Computer science (LLM reasoning, vector DB, cloud computing, ML training); envisioned for broader scientific use but evaluated in CS",
            "validation_type": "simulated",
            "validation_description": "Validation is performed computationally: (1) Intra-ARM runs modular validators (Experimental Setup Validator and Execution Validator) which check plan alignment, I/O correctness, placeholder detection, and execution in 'clean' environments; (2) reproducibility checks by executing workflows multiple times; (3) results and metadata are stored in the Experiment Knowledge Module (time-machine provenance) for traceability; (4) automated evaluation via an LLM judge for design/setup/conclusion checks, with manual expert assessment for implementation-alignment (semantic code correctness); (5) benchmark comparisons against ground-truth derived from published papers and open-source benchmarks, reporting averaged metrics across five independent trials per task.",
            "simulation_fidelity": "empirical software-execution fidelity: experiments are real code runs in isolated/clean computational environments (not physics or wet-lab). Fidelity is high for software/ML experiments insofar as the code executes in the provided environment, but limited by starter-code completeness, environment dependencies, and LLM hallucinated code; not a physics or instrument-level high-fidelity simulation.",
            "validation_sufficiency": "Paper argues computational validation is sufficient for the targeted CS domains (software, ML experiments) where reproducible code execution and comparison to benchmark results are domain norms; the paper explicitly notes this validation is not sufficient for wet-lab or physical sciences where physical/experimental validation is required and recommends domain-specific adaptation for such disciplines.",
            "validation_accuracy": "Reported benchmark metrics: Curie weighted-average metrics across tasks: Design 97.9%, Execution (setup) 78.1%, Implementation Alignment 73.4%, Conclusion Correctness 36.1% (these are the paper's reported aggregated percentages). Domain-specific: LLM reasoning conclusion accuracy 44.9%; execution-setup performance ranges (e.g., 66.7% to 92.7% reported for certain categories).",
            "experimental_validation_performed": false,
            "experimental_validation_details": "No physical (wet-lab or hardware-in-the-loop) experiments were performed. Validation consisted of running generated experimental code in isolated computational environments, multiple runs per task (five trials) to assess reproducibility, and automated/manual evaluation against ground-truth derived from papers/benchmarks. The paper notes lack of physical experiments as a limitation for non-CS domains.",
            "validation_comparison": "The paper compares automated LLM-judge verification to manual expert assessment for implementation alignment: LLM judge used for design/setup/conclusion where ground truth is available; manual checking used for semantic code alignment (implementation-alignment). It also compares Curie vs. baseline agents (OpenHands, Magentic) across the same validation pipeline, showing Curie substantially outperforms baselines; no wet-lab vs simulation comparisons because all evaluations are computational.",
            "validation_failures": "Failures/limitations documented: (1) Cascading errors when earlier pipeline steps fail (incorrect setups, unresolved dependencies) reduce conclusion correctness; (2) Baselines and sometimes Curie encountered execution failures (syntax errors, logic mistakes, unresolved dependencies); (3) Conclusion correctness remains relatively low (36.1% overall), indicating validation/experimentation pipeline still fails to produce correct conclusions frequently; (4) LLM hallucination can inject incorrect implementations or metadata; (5) domain transfer limitations — CS-style computational validation insufficient for biological/physical sciences.",
            "validation_success_cases": "Successes documented: (1) Curie reproduced and extended findings from LLM-reasoning case studies (e.g., confirming repeated-sampling benefits and exploring sampling temperature effects); (2) High design correctness rates (~97.9%) and strong execution/setup reliability in many tasks (e.g., execution setup up to 92.7% in some domains); (3) In domains with well-established benchmarks and shorter runs (Vector DB), alignment and execution succeed at higher rates (alignment up to ~77.2% in domain-specific tasks).",
            "ground_truth_comparison": "Ground truth derived from published research results and official open-source project benchmarks; agents' outputs were compared to these ground-truths using the LLM judge and manual checks. The paper reports per-metric success rates based on these comparisons (see aggregated percentages).",
            "reproducibility_replication": "The paper enforces reproducibility practices within the system: runs each task five independent trials and reports averages; Experiment Knowledge Module records versioned 'time-machine' provenance. External independent replication by outside groups is not reported.",
            "validation_cost_time": "Paper discusses resource constraints qualitatively (GPU/agent availability, long-running tasks like cloud computing require scheduling and monitoring) and describes partition scheduling to improve efficiency. No quantitative monetary cost or wall-clock time breakdowns are reported.",
            "domain_validation_norms": "States that for software/ML/cloud (CS) domains reproducible code execution and benchmark comparisons are accepted norms; explicitly notes that biological and materials science require wet-lab/physical validation and that Curie needs domain-specific adaptations for such norms.",
            "uncertainty_quantification": "Uncertainty is handled by multiple runs (five trials) and reporting averaged metrics; the system logs execution errors and anomalies to help detect hidden dependencies. No formal statistical confidence intervals or probabilistic uncertainty estimates are reported.",
            "validation_limitations": "Limitations include reliance on LLMs prone to hallucination, dependence on provided starter code/environments, lack of physical experiment capabilities, limited statistical treatment of uncertainty (no CIs), and residual low conclusion accuracy indicating validation pipeline incompleteness.",
            "hybrid_validation_approach": true,
            "hybrid_validation_details": "Combines automated computational execution + automated LLM-judge evaluation + manual expert assessment for semantic alignment. Intra-ARM validators run automatically; LLM judge checks design/setup/conclusion; manual human review is used for implementation-alignment checks on a subset to validate and calibrate the judge.",
            "uuid": "e2109.0"
        },
        {
            "name_short": "Intra-ARM",
            "name_full": "Intra-Agent Rigor Module (Intra-ARM)",
            "brief_description": "A suite of modular validators that enforce step-by-step reliability inside individual agents by validating experiment plans, setups, and executions before forwarding to technicians or committing results.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Intra-ARM (Experimental Rigor Engine component)",
            "system_description": "Provides continuous, modular validation across stages (e.g., Experimental Setup Validator, Execution Validator) to detect errors early, validate alignment with the plan, and ensure reproducibility via controlled environment execution.",
            "scientific_domain": "Computer science experimentation and software/ML experiments",
            "validation_type": "simulated",
            "validation_description": "Contains validators that (a) check that experimental setups align with the experimental plan and variables, detect placeholders/hardcoded values, verify I/O argument handling, and require documentation of intermediate steps; (b) execute the setup in a clean environment to verify error-free execution, log errors, and perform reproducibility checks by rerunning workflows multiple times.",
            "simulation_fidelity": "Software-run validation: high fidelity for checking code execution in the provided environment (unit/functional level), but does not simulate external hardware or physical processes.",
            "validation_sufficiency": "Sufficient for detecting coding/runtime issues and ensuring computational reproducibility in CS tasks; not sufficient for physical experiments in other domains.",
            "validation_accuracy": "No single numeric accuracy reported for Intra-ARM alone; contributes to aggregate execution/setup scores (Curie execution_setup ~78.1% weighted average); example ranges in text: execution setup performance between ~66.7% and 92.7% for certain tasks.",
            "experimental_validation_performed": false,
            "experimental_validation_details": "Validators perform real code execution in isolated computational environments as part of the system; authors ran many tasks through these validators in the benchmark, but no physical experiments.",
            "validation_comparison": "The module is contrasted with naive end-to-end validation approaches — the paper argues Intra-ARM's continuous/modular validation is superior because it enables error isolation and avoids full reruns. No quantitative head-to-head numbers for validators vs. end-to-end only approach beyond aggregate system performance.",
            "validation_failures": "Limitations include inability to detect higher-level semantic mismatches between methodology and code without manual inspection, and residual failures when environment dependencies or incomplete starter code exist.",
            "validation_success_cases": "Successfully caught syntax mistakes, placeholder/hardcoded inputs, missing I/O handling, and runtime exceptions; contributed to improved setup/execution rates relative to baselines.",
            "ground_truth_comparison": "Validation focuses on internal correctness and reproducibility rather than comparison to external ground truth; final outputs are compared to ground truth at evaluation stage.",
            "reproducibility_replication": "Enforced via multiple runs and clean-environment execution to improve reproducibility; recorded provenance in Experiment Knowledge Module.",
            "validation_cost_time": "Introduces overhead for repeated runs and staged validation but reduces cost of large reruns by catching early-stage errors; no numeric cost/time provided.",
            "domain_validation_norms": "Appropriate for software and computational experiment norms (unit runs, repeated trials); not adequate for wet-lab norms.",
            "uncertainty_quantification": "Relies on repeated runs to detect non-determinism and anomalies; no probabilistic uncertainty models reported.",
            "validation_limitations": "Cannot fully verify semantic alignment of complex methodology with code (requires manual review), limited by the completeness of the environment and starter code, and subject to LLM-generated incorrect code.",
            "hybrid_validation_approach": true,
            "hybrid_validation_details": "Integrated with Inter-ARM and Experiment Knowledge Module; validators produce artifacts consumed by automated LLM judge and by human reviewers for final alignment checks.",
            "uuid": "e2109.1"
        },
        {
            "name_short": "Experimental Setup Validator",
            "name_full": "Experimental Setup Validator (component of Intra-ARM)",
            "brief_description": "Validator that inspects proposed experiment setups for methodological soundness, plan alignment, correct I/O handling, absence of placeholders/hardcoding, and presence of documented intermediate steps and expected results.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Experimental Setup Validator",
            "system_description": "Static and semantic checks on experiment setup artifacts (code and metadata) to ensure alignment with the research question and experimental plan before execution.",
            "scientific_domain": "Computer science experiments / software engineering workflows",
            "validation_type": "simulated",
            "validation_description": "Performs automated checks that: (1) variables (independent/dependent/constant) match the experimental plan; (2) input/output args are handled correctly; (3) no placeholders/hardcoded values remain; (4) intermediate steps and expected results are documented. Blocks progression to execution until checks pass.",
            "simulation_fidelity": "Static analysis and metadata checking: high fidelity for syntactic/structural issues and some semantic checks but limited on deep behavioral semantics.",
            "validation_sufficiency": "Sufficient to catch many classes of setup errors common in computational experiments, though insufficient to guarantee semantic correctness of complex experimental algorithms.",
            "validation_accuracy": "No scalar accuracy given; contributes to overall design/execution pipeline improvements shown in system-level metrics.",
            "experimental_validation_performed": false,
            "experimental_validation_details": "Used across the benchmark to gate execution; the paper provides example errors detected (placeholders, missing I/O), but no per-validator statistics reported.",
            "validation_comparison": "Framed as superior to one-time end-of-run validation; no direct numerical comparison presented.",
            "validation_failures": "Cannot detect subtle mismatches where code superficially aligns with plan but implements incorrect methodology; requires execution and manual inspection for deeper verification.",
            "validation_success_cases": "Detected and prevented many malformed setups from executing, reducing cascading errors downstream.",
            "ground_truth_comparison": "Not directly compared to ground truth; used to ensure setup fidelity prior to running experiments that are compared to ground truth.",
            "reproducibility_replication": "By enforcing complete metadata and removing hardcoded parts, improves reproducibility across runs.",
            "validation_cost_time": "Adds validation latency before execution but reduces cost of re-running failed experiments; no numeric trade-offs provided.",
            "domain_validation_norms": "Matches CS norms of reproducible, runnable experimental code; insufficient for wet-lab protocol validation.",
            "uncertainty_quantification": "No explicit uncertainty metrics; functions deterministically as a gate.",
            "validation_limitations": "Limited semantic checking; may pass setups that are syntactically correct but methodologically wrong.",
            "hybrid_validation_approach": true,
            "hybrid_validation_details": "Works together with Execution Validator and higher-level LLM judge/manual review; acts as the first gate in a hybrid multi-stage validation pipeline.",
            "uuid": "e2109.2"
        },
        {
            "name_short": "Execution Validator",
            "name_full": "Execution Validator (component of Intra-ARM)",
            "brief_description": "A runtime validator that executes the experimental setup in a controlled clean environment to ensure error-free runs and reproducibility across multiple executions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Execution Validator",
            "system_description": "Runs the experiment in isolated environments, logs errors with actionable debugging details, and reruns workflows to check reproducibility and detect anomalies or hidden dependencies.",
            "scientific_domain": "Computer science experiments and software/ML workloads",
            "validation_type": "simulated",
            "validation_description": "Automated execution in clean containers/environments, capturing runtime logs, error traces, and producing reproducibility metrics via multiple runs. Ensures outputs align with expected artifacts and that workflows complete without crashes.",
            "simulation_fidelity": "High-fidelity execution for software experiments within the provided environment; cannot emulate external infrastructures beyond the configured environment.",
            "validation_sufficiency": "Sufficient to detect execution/runtime issues in computational experiments; combined with setup validation, it addresses many practical reproducibility requirements in CS.",
            "validation_accuracy": "No single accuracy number for validator; system-level execution/setup metric for Curie aggregated to ~78.1% weighted average indicates substantial but not perfect execution reliability.",
            "experimental_validation_performed": false,
            "experimental_validation_details": "Experiment runs are real code executions used across the benchmark; errors and reproducibility issues are logged and used for iterative refinement by agents.",
            "validation_comparison": "Compared qualitatively to naive single-run validation; multiple runs and clean environment execution cited as stronger practice; no direct numeric comparison given.",
            "validation_failures": "Fails when environment dependencies are missing, when code is semantically incorrect despite running, or when hidden nondeterministic dependencies exist (leading to inconsistent outputs across runs).",
            "validation_success_cases": "Successfully identified and prevented propagation of runtime errors and non-deterministic behaviours for many benchmark tasks, contributing to higher setup/execution scores relative to baselines.",
            "ground_truth_comparison": "Execution Validator ensures runnable outputs that are then compared to ground truth at evaluation stage; it itself does not compare scientific results to gold standards.",
            "reproducibility_replication": "Designed to increase reproducibility by repeating runs and using clean isolated environments; five-trial averaging for tasks demonstrates application.",
            "validation_cost_time": "Rerunning workflows increases runtime cost; scheduler and partitioning aim to manage resource/time trade-offs for large experiments. No numeric cost/time reported.",
            "domain_validation_norms": "Aligns with software-engineering norms for isolated, repeatable execution; not intended for physical experiment protocols.",
            "uncertainty_quantification": "Detects run-to-run variability via repeated execution; no probabilistic uncertainty estimates provided.",
            "validation_limitations": "Cannot detect conceptual methodological errors that still run without errors; dependent on quality of the environment and input data.",
            "hybrid_validation_approach": true,
            "hybrid_validation_details": "Feeds execution artifacts and logs into the Experiment Knowledge Module and LLM judge for higher-level evaluation and into manual review when needed.",
            "uuid": "e2109.3"
        },
        {
            "name_short": "Experiment Knowledge Module",
            "name_full": "Experiment Knowledge Module (structured knowledge/time-machine)",
            "brief_description": "A structured experiment metadata and provenance store that records plans, execution artifacts, versioned changes, and a DAG-like time-machine history to improve interpretability and traceability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Experiment Knowledge Module",
            "system_description": "Organizes and enforces structured reads/writes, time-stamped provenance, tiered write access, and a DAG-like history for recovering past states and diagnosing issues, thereby supporting validation and reproducibility.",
            "scientific_domain": "All experimental domains but implemented/evaluated in computer science experiments",
            "validation_type": "none",
            "validation_description": "Not a validator per se but a provenance/knowledge system that supports validation by storing experimental plans, intermediate results, and change histories; enforces tiered write access and validates metadata before committing.",
            "simulation_fidelity": "N/A (data-store and metadata fidelity depends on inputs and validators that generate records).",
            "validation_sufficiency": "Essential supporting infrastructure for validation (traceability, reproducibility) but not sufficient to validate scientific correctness by itself.",
            "validation_accuracy": "Not applicable as it stores metadata; accuracy depends on upstream validators and agents writing correct info. The paper reports that it helped Curie achieve better recall and adaptation, improving conclusion accuracy indirectly.",
            "experimental_validation_performed": null,
            "experimental_validation_details": "Stores the outputs of computational experiment runs and validation checks; used across benchmark runs to reconstruct experiments and audit agent decisions; authors used it in system evaluation but did not report independent validation experiments for the store itself.",
            "validation_comparison": "Compared conceptually to relying solely on LLM memory (which is inconsistent); Experiment Knowledge Module provides structured provenance which is argued to be superior. No numeric comparison provided.",
            "validation_failures": "Potential failure modes include garbage-in/garbage-out if LLM agents write hallucinated or incorrect records; mitigated via tiered write access and validators but not eliminated.",
            "validation_success_cases": "Improved interpretability and enabled rollback and tracing of failures during benchmark experiments; credited with improving Curie's adaptation and recall in iterative experiments.",
            "ground_truth_comparison": "Acts as the provenance store for experiments compared to ground truth; no direct comparison metric.",
            "reproducibility_replication": "Enables reproducing past experiment states via recorded DAG-like history; used to rerun partitions and reconstruct prior conditions.",
            "validation_cost_time": "Overhead for structured writes and validation checks; reduces long-term debugging and rerun costs by preserving provenance. No quantitative overhead reported.",
            "domain_validation_norms": "Matches good-practice norms across domains: provenance, auditable records, and restricted edits are required for reproducibility and auditability.",
            "uncertainty_quantification": "Records error logs and anomalies but does not itself quantify statistical uncertainty.",
            "validation_limitations": "Dependent on correctness of writes; LLM hallucinations risk corrupting records despite tiered access. Requires validators to maintain integrity.",
            "hybrid_validation_approach": true,
            "hybrid_validation_details": "Works as the central store that integrates outputs from automated validators, LLM judge assessments, and human reviewer annotations to form a hybrid validation evidence trail.",
            "uuid": "e2109.4"
        },
        {
            "name_short": "LLM Judge",
            "name_full": "Automated LLM-based experiment verifier (LLM judge)",
            "brief_description": "An LLM-based automated evaluator used to verify experiment design, runnable setup, and conclusion correctness where ground truth is available; supplemented by manual checks for implementation-alignment.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "LLM Judge",
            "system_description": "An automated LLM verifier invoked to check experiment design, execution setup, and conclusions against provided ground truth and logs; designed with a strict system prompt and used at scale for benchmark evaluation.",
            "scientific_domain": "General-purpose experiment verification across computational scientific tasks (applied here to CS benchmark)",
            "validation_type": "hybrid",
            "validation_description": "The LLM judge consumes experiment logs, the original experiment question, and ground truth (where available) to produce structured pass/fail assessments for design, execution, implementation alignment (where possible), and conclusion correctness; a subset of its outputs was cross-checked by human experts to calibrate and measure agreement.",
            "simulation_fidelity": "N/A (automated judgment rather than simulation); fidelity depends on prompt design and LLM capability.",
            "validation_sufficiency": "Used as a scalable verifier for tasks with clear ground truth; the paper acknowledges that detecting semantic discrepancies in code (implementation alignment) is non-trivial and requires manual assessment, so the LLM judge is not by itself sufficient for full validation.",
            "validation_accuracy": "No absolute accuracy number given for the judge alone; the paper reports that they cross-checked a subset of evaluations against expert annotations to measure agreement and refine prompts, but exact agreement rates are not reported.",
            "experimental_validation_performed": null,
            "experimental_validation_details": "Used during benchmark evaluation to grade many tasks automatically; where ground truth exists the judge performs direct comparisons; manual checks used for hard semantic checks.",
            "validation_comparison": "Compared qualitatively to manual assessment: LLM judge scales but can miss semantic code issues; manual review remains necessary for implementation-alignment.",
            "validation_failures": "Known failure mode: difficulty detecting semantic mismatches between intended methodology and generated code, which can lead to false passes on implementation alignment.",
            "validation_success_cases": "Used effectively for design/setup/conclusion checks when ground truth is explicit and for scaling the evaluation across 46 tasks and 5 trials each.",
            "ground_truth_comparison": "Directly compares logs and reported conclusions to provided ground truth in tasks; used as the primary automated verification mechanism for benchmark scoring.",
            "reproducibility_replication": "Authors cross-checked a subset of LLM-judge outputs with expert annotation to validate the judge's reliability; no independent external replication reported.",
            "validation_cost_time": "Automates large-scale verification saving human time; the trade-off is possible missed semantic errors requiring human follow-up. No quantitative time savings provided.",
            "domain_validation_norms": "Acceptable for automated checks where ground truth is explicit and tasks are computational; insufficient for deep semantic code verification or non-computational domains without human review.",
            "uncertainty_quantification": "No formal uncertainty estimates reported for judge outputs; calibration performed via cross-check with experts but no numeric confidence metrics provided.",
            "validation_limitations": "Prone to mistakes on semantic code correctness and subject to prompt sensitivity; needs human calibration and spot-checking.",
            "hybrid_validation_approach": true,
            "hybrid_validation_details": "LLM judge outputs are combined with manual expert checks for implementation alignment; used together with Intra-ARM validators and Experiment Knowledge Module to form a hybrid verification pipeline.",
            "uuid": "e2109.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Toward rigorous assessment of language agents for data-driven scientific discovery",
            "rating": 2
        },
        {
            "paper_title": "The ai scientist: Towards fully automated open-ended scientific discovery",
            "rating": 2
        },
        {
            "paper_title": "Large language monkeys: Scaling inference compute with repeated sampling",
            "rating": 1
        },
        {
            "paper_title": "Agent laboratory: Using llm agents as research assistants",
            "rating": 2
        },
        {
            "paper_title": "Mlagentbench: Evaluating language agents on machine learning experimentation",
            "rating": 2
        }
    ],
    "cost": 0.017355,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Curie: Toward Rigorous and Automated Scientific Experimentation with AI Agents
26 Feb 2025</p>
<p>Patrick Tser 
Jern Kon 
Department of Computer Science and Engineering
University of Michigan</p>
<p>Jiachen Liu 
Department of Computer Science and Engineering
University of Michigan</p>
<p>Qiuyi Ding 
Department of Computer Science and Engineering
University of Michigan</p>
<p>Yiming Qiu 
Zhenning Yang 
Department of Computer Science and Engineering
University of Michigan</p>
<p>Yibo Huang 
Department of Computer Science and Engineering
University of Michigan</p>
<p>Jayanth Srinivasa 
Department of Computer Science and Engineering
University of Michigan</p>
<p>Cisco Systems</p>
<p>Myungjin Lee 
Cisco Systems</p>
<p>Mosharaf Chowdhury 
Department of Computer Science and Engineering
University of Michigan</p>
<p>Ang Chen 
Department of Computer Science and Engineering
University of Michigan</p>
<p>Curie: Toward Rigorous and Automated Scientific Experimentation with AI Agents
26 Feb 2025B6A409D7B1A744479206BB7AA7280F28arXiv:2502.16069v2[cs.AI]
Scientific experimentation, a cornerstone of human progress, demands rigor in reliability, methodical control, and interpretability to yield meaningful results.Despite the growing capabilities of large language models (LLMs) in automating different aspects of the scientific process, automating rigorous experimentation remains a significant challenge.To address this gap, we propose Curie, an AI agent framework designed to embed rigor into the experimentation process through three key components: an intra-agent rigor module to enhance reliability, an inter-agent rigor module to maintain methodical control, and an experiment knowledge module to enhance interpretability.To evaluate Curie, we design a novel experimental benchmark composed of 46 questions across four computer science domains, derived from influential research papers, and widely adopted open-source projects.Compared to the strongest baseline tested, we achieve a 3.4× improvement in correctly answering experimental questions.Curie is open-sourced at https: //github.com/Just-Curieous/Curie.</p>
<p>Introduction</p>
<p>Scientific research drives human progress, advancing medicine, technology, and our understanding of the universe.At the heart of this endeavor lies experimentation-a disciplined intellectual pursuit that transforms human curiosity, expressed through bold hypotheses, into verifiable knowledge.Experimentation thrives on creativity, as new ideas fuel discovery.Yet it also depends on rigor-ensuring that research is methodologically sound and its findings are trustworthy (Armour et al., 2009;Gill &amp; Gill, 2020).If science isn't rigorous, it's reckless (Hofseth, 2018).</p>
<p>In recent years, numerous works (Zhang et al., 2024b;Kramer et al., 2023;Lu et al., 2024) leveraging large language models (LLMs) to automate scientific research have emerged ( §2.3).These solutions typically rely on ad-hoc prompt-based methods to mimic scientific workflows, which are prone to hallucination.While effective for creative tasks such as literature review and brainstorming, these approaches remain limited in their ability to support rigorous experimentation, a largely unexplored capability.</p>
<p>More specifically, rigorous experimentation ( §2.2) involves a methodical procedure that includes formulating hypotheses, designing experiments, executing controlled trials, and analyzing results.Achieving reliability at every step is essential to ensure that the results are accurate, reproducible, and scientifically meaningful.Finally, all procedures and results must be documented in a well-structured and interpretable manner, facilitating verification, reproducibility, and collaboration across the scientific community.</p>
<p>To meet these requirements, we propose Curie, an AI agent framework representing the first step toward rigorous and automated experimentation ( §3).As shown in Fig. 1, Curie takes an experimental question and relevant context (e.g., domain-specific knowledge or starter code) as input.The Architect Agent generates high-level experimental plans, coordinates the process, and reflects on findings to guide subsequent steps.Working in unison, our Technician Agents focus on carefully implementing and executing Curie can help researchers validate, expand, and critique existing research on the benefits of repeated sampling in LLM reasoning (Brown et al., 2024).The first panel (Original Finding) presents a result from the original paper.The second panel (Reproduce) has Curie confirming this finding through rigorous experimentation.The third panel (Extend) has Curie exploring the impact of sampling temperature on repeated sampling.The final panel (Challenge) shows Curie identifying a limitation in the original methodology, suggesting an avenue for future research.controlled experiments following these plans.</p>
<p>At the core of Curie, the Experimental Rigor Engine preserves agent creativity while embedding rigor seamlessly throughout the experimentation process.This is achieved via three key modules: (1) The Intra-Agent Rigor Module safeguards reliability within individual agents by enforcing a set of extensible rigor policies (e.g., validating that experiment plans align with objectives and setups are reproducible).(2) The Inter-Agent Rigor Module maintains methodical control over agent coordination, ensuring correct task transitions and efficient task scheduling.(3) Finally, the Experiment Knowledge Module enhances interpretability by maintaining well-structured documentation, enabling seamless collaboration in large-scale experiments.</p>
<p>Though our architecture suggests applications across various disciplines, this paper focuses on addressing research problems in computer science by leveraging existing LLMfriendly interfaces for computer access (Anthropic, 2024;Yang et al., 2024).To evaluate Curie, we introduce an Experimentation Benchmark comprising 46 tasks of varying complexity across multiple domains within computer science ( §4).We derive these questions directly from influential research papers and widely adopted open-source projects, in order to reflect real-world challenges and practical significance.As shown in Fig. 2, Curie enables researchers to reproduce, extend, and challenge existing research through rigorous experimentation.</p>
<p>We benchmarked Curie ( §5) against several state-ofthe-art agents: OpenHands (Wang et al., 2024c) (a topperforming coding agent on SWE-Bench (Jimenez et al., 2023)), and Microsoft Magentic (Fourney et al., 2024) (a state-of-the-art generalist multi-agent system).Our empirical findings show that Curie achieves a 3.4× improvement in correctly answering experimental questions, compared to the strongest baseline tested, among other aspects.These results underscore Curie's ability to automate complex and rigorous experimentation tasks, making it a promising step toward accelerating scientific research.</p>
<p>Background</p>
<p>Science Experimentation</p>
<p>Scientific experimentation often starts with researchers posing testable hypotheses based on their past results, domain knowledge, and intuition.This experimentation process then unfolds across three key stages: (1) Experimental Design, where researchers plan the controlled experiment by identifying variables, selecting methodologies, and outlining procedures to enhance reproducibility and validity.(2) Experiment Execution, where researchers set up the complex experiment environments and iteratively explore vast search spaces, and (3) Data Documentation and Analysis, where researchers systematically gather data, apply analytical techniques, and extract insights to validate or refine their hypotheses.This process is iterative, as insights gained from data analysis often lead to the refinement of hypotheses, leading to subsequent rounds of these three steps.</p>
<p>Rigor in Experimentation</p>
<p>Rigor is essential in scientific research, ensuring systematic, precise, and reliable findings (Armour et al., 2009).If science isn't rigorous, it's reckless.(Hofseth, 2018).More precisely, experimental rigor is grounded in three core principles (Gill &amp; Gill, 2020):</p>
<p>Methodical Procedure: Experimentation must adhere to a principled and systematic methodology throughout all aforementioned stages, from hypothesis formulation to data documentation.Such a structured procedure ensures that no critical procedures are overlooked or performed incompletely, thereby preserving the integrity of the research.</p>
<p>Reliability: Every stage in the experimental pipeline-such as experiment design and environment setup-needs to be reliable and reproducible so that any final findings rest on solid ground.For instance, it encompasses correct variable identification, controlled experimental design, and rigorous code verification.By meticulously verifying each stage, reliability minimizes the risk of cascading errors, thereby ensuring that the results are trustworthy.</p>
<p>Interpretability: All processes and outcomes need to be clearly documented in a consistent manner.This makes it easier for researchers or agents to replicate experiments, understand results, and extend research.</p>
<p>Related Work</p>
<p>AI Agents for Science.Prior work has leveraged AI to accelerate scientific discovery (Berens et al., 2023;Kitano, 2021), focusing on various stages of the research lifecycle, including literature reviews (Agarwal et al., 2024;Tyser et al., 2024), brainstorming ideas (Gu &amp; Krenn, 2024;Bran et al., 2024), hypothesis generation (Sourati &amp; Evans, 2023;Zhou et al., 2024;Wang et al., 2024a;Qi et al., 2024) and data analysis (Hong et al., 2024a;Chen et al., 2024).While these efforts works on various aspects of the scientific lifecycle, experimentation-a critical, rigor-intensive step-remains underexplored.</p>
<p>Existing agents for end-to-end scientific research (Schmidgall et al., 2025;Lu et al., 2024;Yuan et al., 2025;Ghafarollahi &amp; Buehler, 2024) rely on ad-hoc prompts to guide predefined workflows, from idea generation to paper writing.Their open-sourced frameworks often require ex-perimental code to follow constrained, framework-specific formats, adding overhead and hindering their usability.These solutions mimic experimentation processes using multi-agent systems but lack systematic enforcement of a methodical procedure, reliability, and interpretability.Without these core principles, such agents struggle to deliver meaningful and reproducible results, limiting their practical utility in real-world scientific research.</p>
<p>AI Agent Task Benchmarks.A wide range of benchmarks have been developed to assess the capabilities of AI agents across diverse domains.Existing benchmarks primarily focus on logical reasoning (Cobbe et al., 2021;Hendrycks et al., 2021a;Bang et al., 2023), problem-solving (Hendrycks et al., 2021b;Frieder et al., 2023;Wang et al., 2024b;Sun et al., 2024a;Chevalier et al., 2024), knowledge retrieval tasks (Sun et al., 2024b) and machine learning training (Huang et al., 2024;Zhang et al., 2023;2024a).These benchmarks evaluate agents on well-defined tasks that typically have clear, deterministic solutions.</p>
<p>In contrast, our benchmark focuses on experimentation, which requires a more rigorous and systematic approach beyond problem-solving.Experimental tasks require iterative hypothesis refinement, complex experiment setup and execution, and robust result interpretation.Our benchmark captures these challenges by evaluating AI systems on real-world experimentation tasks derived from influential research papers and widely adopted open-source projects.</p>
<p>Curie: Rigorous Experimentation</p>
<p>Architectural Overview</p>
<p>As shown in Fig. 3, Curie is composed of two types of LLM-based agents (an Architect Agent and a host of Tech- nician Agents), sandwiched between them is our main innovation, the Experimental Rigor Engine that injects rigor throughout the experimental process.</p>
<p>High-level workflow.Given an experimental question, our Architect will 1 designs high-level experimental plans (e.g., defining hypotheses, variables), completing its turn.Our Inter-Agent Rigor Module (Inter-ARM ) will A intercept and enforce methodical procedure.Since the plan is new, it is broken into smaller partitions for finer-grained execution.Inter-ARM applies control flow policies to determine the next step for each partition.In this case, it decides go through the B the Intra-Agent Rigor Module (Intra-ARM ) validation, which enhances reliability by verifying partition integrity (e.g., assessing relevance to the experimental question).Similarly, Inter-ARM repeats this process based on the validation results, eventually C forwarding the partition to a Technician to 2 set up the controlled experiment.The remaining steps are omitted for brevity, but at a high level, every agent action follows the same structured workflow: A interception by Inter-ARM, B validation by Intra-ARM, and C forwarding to the next appropriate agent.Finally, all of the above components will make use of our Experiment Knowledge Module for storing and tracking experimental progress, providing interpretability.For example, the Architect stores refined experimental plans in a structured, metadata-enriched format, making them easier to analyze, track, and validate over time.</p>
<p>Intra-Agent Rigor Module -Reliability</p>
<p>Large-scale and long-running experiments involve complex, interdependent steps where early-stage errors can propagate and compromise final results.This is especially critical to LLM-based experimentation since: (1) LLM-based agents are prone to hallucination, and (2) experimental processes are inherently exploratory, requiring iterative refinements to hypotheses, setups, and designs in response to new or unexpected findings.Despite this, existing works (Lu et al., 2024;Schmidgall et al., 2025) largely overlook the need for  continuous validation throughout the experimental process.A naive approach is to perform end-to-end validation only after an experiment concludes.However, this lacks the ability to backtrack to intermediate stages, preventing error isolation and correction, and forcing researchers to either discard progress or rerun the entire experiment-an inefficient and costly approach.To address this, we introduce Intra-ARM, a validation module that verifies the assigned tasks of our Architect and Technicians step by step, improving reliability and reproducibility to align with the overarching experimental objectives.Inspired by process supervision (Lightman et al., 2023), Intra-ARM utilizes modular validation, where a suite of validators continuously verifies each stage of the experiment (Fig. 3), so that errors can be proactively detected and addressed early.Moreover, Intra-ARM 's validators are extensible, allowing new ones to be incorporated as needed.We focus on two key validators here for brevity:</p>
<p>Experimental Setup Validator.This component (Fig. 4) verifies that the experimental setup by our technicians aligns with the plan before execution, ensuring methodological soundness and logical consistency.Each enforced policy checks alignment within a specific part of the experiment setup.This includes (Fig. 5a): (1) confirming the setup aligns with the experimental plan, including the research question and all specified variables (independent, dependent, and constant).( 2) Analyzing all procedures for correct handling of input/output arguments; and detecting placeholders, hardcoded values, or incomplete variables to ensure meaningful results.(3) Checking that the setup documents all intermediate steps and expected results, including any identified issues for future analysis.</p>
<p>Execution Validator.Once the setup passes the experimental setup validator, this validator enhances reproducibility by executing it in a controlled and clean environment to detect and resolve potential errors, a sample of which is illustrated in Fig. 5b.(1) Error-Free Execution: The setup is executed in a clean environment, verifying that it operates without errors.Any encountered errors are logged in detail, providing actionable feedback for debugging and iterative refinement.(2) Reproducibility Checks: The workflow is also run multiple times to enhance consistency in outputs and detect anomalies or hidden dependencies.Finally, the results are validated to ensure alignment with the experimental plan and compliance with predefined quality standards.</p>
<p>Inter-Agent Rigor Module -Methodical Control</p>
<p>Experimental processes must follow a methodical precedure ( §2.2) while balancing resource constraints (e.g., GPU availability), and experiment priorities.Traditional agentic conversational patterns (AutoGen, 2024)-such as naive LLM-based coordination, sequential, or round-robin execution-are thus ill-suited for such a workflow.To ensure task coordination and optimize resource efficiency, Inter-ARM enables seamless collaboration between our Architect, Technicians and Intra-ARM through three key functions (illustrated in Fig. 6).We discuss each in turn.</p>
<p>Fine-grained Plan Partitioning.Inter-ARM first breaks down new complex experimental plans generated by the Architect into smaller, independent partitions: defined as a distinct subset of independent variable values within the plan.By creating smaller, self-contained tasks, this facilitates modular execution and enables parallelization, making experimentation more scalable.In addition, this enables our Architect to track intermediate progress and results, making real-time decisions as new insights emerge (e.g., reprioritizing partitions by updating their execution priority).</p>
<p>Control Flow Enforcement.This component ensures that transitions between our Architect, Technicians, and Intra-ARM follow a logical sequence aligned with the experimentation lifecycle.This is critical to maintaining consistent, error-free progress.Without structured coordination, tasks may be executed out of order or without necessary dependencies, leading to wasted effort and erroneous conclusions.For instance, it prevents Technicians from directly executing experiment setups before validation by Intra-ARM 's setup validator, to reduce the risk of erroneous data propagation.This is done in two steps: (1) State Evaluation: First, it evaluates the current state of each partition (within an experimental plan) that has been modified by any given agent, e.g., a Technician who produced experimental results and recorded its progress via the Experiment Knowledge Module.(2) Permissible State Transitions: Based on the current state of the partition(s), this component produces a set of allowed state transitions for the given partition, e.g., newly produced experimental results for a given partition need to be validated by Intra-ARM first.It also gathers relevant context that would be useful if the transition were to be executed.This state transition information will be consumed by our scheduler (defined below).Partition Scheduling.Executing large-scale experiments can be resource-intensive and time-consuming, requiring careful scheduling and prioritization of tasks to improve efficiency.Our scheduler currently utilizes three key parameters for partition scheduling: (1) partition execution priorities set by our Architect, (2) allowed partition state transitions, and (3) the availability of our agents (that may be busy handling other partitions).Overall, this adaptive scheduling strategy enables large-scale experimentation by improving resource efficiency while adhering to methodical experimental procedures.</p>
<p>Experiment Knowledge Module -Interpretability</p>
<p>Interpretability is fundamental to experimentation-not only for scientific accountability but also for effective experiment management.Specifically, all other components within Curie require this for real-time visibility, enabling informed decision-making, efficient troubleshooting, and adaptability as new insights emerge.A naive approach would be to delegate experimental knowledge management entirely to LLM-based agents.However, LLMs alone are ill-suited for this task for two reasons: (1) Inconsistent Reads: LLMs have inconsistent recall and are prone to forgetting (Xu et al., 2024).Without a structured and verifiable record of experimental progress, they may retrieve outdated, irrelevant, or hallucinated information, leading to misinterpretations, flawed conclusions, and compounding errors over time.(2) Inconsistent Writes: LLMs tend to hallucinate, particularly when managing large-scale experimental data.This lack of structured control risks corrupting experimental records, propagating inaccuracies, and ultimately compromising the integrity of the experimentation process.Unlike databases, LLMs do not inherently track provenance (Hoque et al., 2024), making it difficult to reconstruct how conclusions were reached.We address these two challenges in turn:</p>
<p>Structured Knowledge Reads.This mechanism organizes experimental progress in a structured format.The process begins by restructuring new experimental plans that were written by our Architect into an enriched format with critical metadata-such as setups, execution status, and results.Subsequent modifications to any part of the plan are recorded as a time machine (Fig. 7) for experimental progression, maintaining a structured, DAG-like history of changes.This historical record captures hypotheses tested, variable changes, and the reasoning behind key decisions.By preserving this evolution, Curie can reconstruct past states, trace decision rationales, and diagnose issues with greater precision.Tiered Write Access.To maintain experimental integrity and minimize the risk of errors, the interface enforces a tiered write access policy that restricts and validates updates made to the experimental plan.This ensures that our other components can only modify the portions of the plan they are responsible for, while all changes undergo rigorous validation.Our LLM-based Architect and Technicians are granted fine-grained write permissions tailored to their roles.For example, Technicians are permitted to append experimental results to their assigned partitions but cannot modify unrelated sections of the plan.Similarly, architects have broader write access, including the ability to create or remove entire partitions, but their modifications are still constrained to specific attributes, such as updating variable values or marking partitions for re-execution.Every write operation is validated before being committed to the knowledge bank.This process ensures proper structuring of inputs and enforces semantic integrity (e.g., that result file paths are valid).If errors are detected, the system returns concise error messages, enabling agents to quickly identify and resolve issues.Through this, Curie enhances robustness and error resistance in collaboration.</p>
<p>Experimentation Benchmark</p>
<p>We design a novel benchmark to stress test Curie's ability to automate experiments while enforcing rigor in the face Investigates strategies for scaling test-time computation in LLMs, focusing on balancing accuracy, latency, and cost.</p>
<p>Research papers: (Brown et al., 2024), (Jin et al., 2024).</p>
<p>Vector Indexing 6 6 3</p>
<p>Examines efficient vector indexing methods for similarity search, analyzing its trade-offs in retrieval recall, memory, and latency.</p>
<p>Open-source project: Faiss (Douze et al.,</p>
<p>Experiment-Centric Task Design</p>
<p>Instead of treating tasks as isolated problems with fixed solutions, we structure each task as a full experimental process.This means that tasks require hypothesis formation, iterative refinement, and rigorous validation, mirroring real-world experiment workflows rather than one-shot problem-solving.</p>
<p>The process begins with distilling high-level contributions from research papers (e.g., theoretical insights or empirical findings), or core system behaviors from open-source projects (e.g., the interplay between configuration parameters and performance).These insights are then translated into testable questions framed with explicit configurations, metrics, and expected outcomes.Ground truth data is derived from published results or official benchmarks provided by open-source projects.We use these findings to design tasks with three key components:</p>
<ol>
<li>Experiment Formulation: Each task specifies the (a) Experiment Question (e.g., optimizing performance, identifying relationships); (b) Practical constraints (e.g., resource budgets); (c) High-level Setup Requirements -Contextual details such as datasets, and experimental environments.This framing ensures that tasks are open-ended, requiring iterative exploration rather than one-shot solutions.3. Ground Truth: This is defined in two key areas: (a) Experimental Design: Does the agent correctly formulate the experiment, identifying relevant variables and methodologies?(b) Result Analysis: Does the agent correctly interpret findings, and justify its conclusions?We outline the expected outcomes or acceptable solution ranges.</li>
</ol>
<p>Experimental Context</p>
<p>Experimental Complexity</p>
<p>Experimental research varies in complexity across different dimensions.Our benchmark reflects this by structuring tasks into a hierarchical framework, assessing an agent's ability to handle increasingly sophisticated experimentation tasks.Unlike standard benchmarks that classify tasks by a single difficulty metric (e.g., easy, medium, hard), ours structures complexity along experiment-driven dimensions (detailed definitions in App.A):</p>
<p>1).Design Complexity: The complexity of structuring an experiment (e.g., requiring hypothesis refinement), including defining the scope of exploration, selecting key variables, and structuring parameter spaces-ranging from discrete to continuous and from sparse to dense configurations.</p>
<p>2). Experiment Setup Complexity:</p>
<p>The difficulty of initializing and configuring the experimental environment, from simple predefined setups to intricate dependencies requiring multi-step configuration.</p>
<p>3).Relationship Complexity: The interactions between variables and outcomes, from simple linear dependencies to complex non-monotonic relationships.</p>
<p>4). Experiment Goal Complexity:</p>
<p>The number of compet-</p>
<p>Evaluation</p>
<p>We evaluate Curie using our experimentation benchmark, which consists of 46 research tasks spanning varying complexity levels across four key domains ( §4).To enhance statistical robustness, each task is executed independently for five trials for each of our baselines (below) and Curie, and we report the average performance across these trials.Apart from our main results described in §5.1, our evaluation includes our case studies (Fig. 2 and App.B), and additional results (App.C).</p>
<p>Baselines.We compare Curie with two state-of-theart AI agents as our baselines: OpenHands (Wang et al., 2024c), a top-performing coding agent, and Microsoft Magentic (Fourney et al., 2024), a generalist multi-agent system.These baselines were selected because our benchmark primarily focuses on coding-related tasks within computer science, where both models demonstrate strong performance, with the expectation that Magentic, as a generalist multiagent system, may be able to generalize to experimental tasks too.To ensure fairness, each baseline is provided with a detailed system prompt instructing them to act as a professional experimenter (see App. E.1).All baselines and Curie utilize GPT-4o as the underlying LLM.</p>
<p>Performance Metrics.We assess performance using four key metrics, each evaluated as a binary score per task, ensuring rigor at every stage of the experimentation process:</p>
<ol>
<li>
<p>Experiment Design -Ability to structure the high-level experiment plan to address the research question.</p>
</li>
<li>
<p>Execution Setup -Ensuring that the generated code (experiment setup) is executable and produces consistent results across multiple runs.</p>
</li>
<li>
<p>Implementation Alignment -Faithfulness of the experimental setup with the proposed plan.</p>
</li>
</ol>
<p>Conclusion Correctness -Accuracy in reflecting the</p>
<p>ground truth answer to the experimental question.</p>
<p>Evaluator.We employ an LLM judge (Zheng et al., 2023) for straightforward verification such as checking design, setup and conclusion, where the ground truth is provided.However, we manually assess the implementation alignment, as detecting semantic discrepancies between the intended methodology and code is non-trivial.To ensure accuracy, we also verify the LLM judge's assessments by cross-checking a subset of its evaluations against expert annotations, measuring agreement rates, and refining the judge system prompt.Details of the evaluation prompts are provided in App.E.2.This hybrid evaluation approach enables reliable and scalable assessment of experimentation performance.</p>
<p>Benchmark Performance</p>
<p>Table 2 shows aggregated success rates across all performance metrics and benchmark task domains.</p>
<p>Performance Breakdown By Metric.Across all four metrics, Curie consistently outperforms the baselines, demonstrating the benefits of our Experimental Rigor Engine in improving experimentation performance.(i) For experiment design correctness, all frameworks perform well since the current tasks are relatively straightforward and do not require iterative refinement.However, for more complex research tasks, Curie holds an advantage by dynamically refining hypotheses based on intermediate observations, whereas baselines rely on static planning.Our experimental knowledge module further enhances performance by improving recall and adaptation.(ii) For execution setup and implementation alignment, Curie demonstrates higher reliability, as Intra-ARM proactively validates and corrects execution steps, while Inter-ARM guarantees that we follow methodical task transitions.This results in particularly strong execution setup performance, from 66.7% to 92.7%.Open-Hands (with 32.4% and 40.2%), as a coding-specialized agent, outperforms Magentic in this aspect.However, it still struggles with incomplete or erroneous setups, including getting stuck in loops, syntax errors, logic mistakes, and unresolved dependencies-leading to execution failures in Curie outperforms the others consistently, with performance generally dropping as complexity increases.</p>
<p>complex environments.Magentic, in particular, performs poorly in locating the correct files in the task starter file and handling script input/output.(iii) Finally, for conclusion correctness, its accuracy is largely constrained by earlier errors, as conclusions rely on the correctness of experimental results.However, Curie maintains a strong lead due to its Experiment Knowledge Module, which systematically documents experimental results for structured data analysis.This enables Curie to achieve a significantly higher conclusion score of 36.1%, compared to 10.5% for OpenHands and 2.3% for Magentic.While Magentic demonstrates relatively decent alignment, it struggles to translate this into meaningful conclusions because of previous cascading errors.</p>
<p>Performance Breakdown By Domain.Across all four task domains, Curie consistently outperforms the baselines, demonstrating Curie's ability to adapt to different research domains.(i) First, for LLM reasoning tasks, Curie performed exceptionally well, achieving the highest conclusion accuracy at 44.9%.OpenHands had its best performance in this category (14.2%), while Magentic attained its only non-zero score of 6.7%.We attribute this to the inherent intuitiveness of conclusions for our tasks in this domain.(ii) For Vector DB tasks, both OpenHands and Magentic achieved their highest alignment scores-52.3%and 63.6%, respectively-likely due to the familiarity of the task.Alignment was also easier given the availability of well-established open-source benchmarks and shorter execution runs, which provided faster feedback.(iii) For Cloud Computing tasks, Curie outperformed OpenHands significantly in all aspects (e.g., 6.5× the conclusion accuracy).This is because these tasks often involve long-running experiments, which requires robust execution tracking and dynamical experimentation workflows adjustment based on partial results.(iv) Finally, for ML Training tasks, all agents underperformed in alignment and execution as the detailed environment setup instructions are not provided for these tasks.Despite this, Curie can figure out the correct setup by reflection and refinement, achieving a 7.3× higher conclusion accuracy than OpenHands.</p>
<p>Performance Breakdown by Complexity.Next, we analyze how each framework performs as we increase difficulty within each complexity dimension.Fig. 8 reports the aggregated performance score, computed as the average across all four evaluation metrics.We observe that increasing complexity difficulties across all dimensions correlates with a decline in performance across all agents.However, the rate of degradation varies across complexity types and agent architectures.Notably, Magentic consistently underperforms across all complexity levels, highlighting the robustness of our complexity-based difficulty scaling in distinguishing agent capabilities.Further, we observe a sublinear decline in performance as task complexity increases, suggesting that our hardest tasks could be made even more challenging.Despite this, our current results demonstrate Curie's capabilities, supported by our case studies.Exploring the limit of experimentation difficulty and its impact on model performance remains an open direction for future work.</p>
<p>In summary, our findings underscore the importance of rigorous evaluation across all stages of the experimentation process, shedding light on each framework's strengths and limitations under varying complexity conditions.</p>
<p>Conclusion and Future Work</p>
<p>We introduced Curie, an AI agent framework designed to automate and enhance the rigor of scientific experimentation.Central to its design is the Experimental Rigor Engine, which enforces methodical control, reliability, and interpretability.To assess Curie's effectiveness, we developed a new Experimentation Benchmark featuring real-world research-level challenges.Our empirical evaluation, comparing Curie against state-of-the-art AI agents, demonstrated its capability to automate rigorous experimentation.</p>
<p>We hope Curie inspires further advancements toward fully autonomous and rigorous experimentation in the era of AI agent-driven scientific research.Several open research challenges remain: For instance, adapting Curie for interdisciplinary research requires accommodating domainspecific methodologies, uncertainty control, and extended time scales, such as long-term biological studies (Hilty et al., 2021).Moreover, enabling knowledge reuse (Wang et al., 2024d) across experiments could enhance efficiency and further accelerate discovery.</p>
<p>Impact Statement</p>
<p>We introduce Curie, an AI agent framework designed to ensure methodical control, execution reliability, and structured knowledge management throughout the experimentation lifecycle.We introduce a novel experimentation benchmark, spanning four key domains in computer science, to evaluate the reliability and effectiveness of AI agents in conducting scientific research.Our empirical results demonstrate that Curie achieves higher conclusion accuracy and execution reliability, significantly outperforming state-ofthe-art AI agents.</p>
<p>Curie has broad implications across multiple scientific disciplines, including machine learning, cloud computing, and database systems, where rigorous experimentation is essential.Beyond computer science, our framework has the potential to accelerate research in materials science, physics, and biomedical research, where complex experimental setups and iterative hypothesis testing are critical for discovery.By automating experimental workflows with built-in validation, Curie can enhance research productivity, reduce human error, and facilitate large-scale scientific exploration.</p>
<p>Ensuring transparency, fairness, and reproducibility in AIdriven scientific research is paramount.Curie explicitly enforces structured documentation and interpretability, making experimental processes auditable and traceable.However, over-reliance on AI for scientific discovery raises concerns regarding bias in automated decision-making and the need for human oversight.We advocate for hybrid human-AI collaboration, where AI assists researchers rather than replacing critical scientific judgment.</p>
<p>Curie lays the foundation for trustworthy AI-driven scientific experimentation, opening avenues for self-improving agents that refine methodologies through continual learning.Future research could explore domain-specific adaptations, enabling AI to automate rigorous experimentation in disciplines such as drug discovery, materials engineering, and high-energy physics.By bridging AI and the scientific method, Curie has the potential to shape the next generation of AI-powered research methodologies, driving scientific discovery at an unprecedented scale.In Fig. 9b, the objective of this experiment is to examine the relationship between task complexity and the optimal length of reasoning chains in large language models (LLMs).The experiment maintains constant variables, including the model (gpt-4o-mini), the method (auto cot), and the environment setup (OpenAI credentials and a Conda environment).The independent variable is the number of reasoning steps, controlled through different demo files, while the dependent variable is the model's accuracy, as reported in the log files.The experiment consists of a control group and experimental groups.The control group uses the gsm8k 1 demo file with a single reasoning step to establish a baseline accuracy.The experimental groups involve testing gsm8k with reasoning steps from gsm8k 2 and gsm8k 3, and last letters with reasoning steps ranging from last letters 1 to last letters 10.The results will help determine whether task complexity influences the optimal number of reasoning steps required for maximizing accuracy in LLMs.</p>
<p>Curie extends the scope by analyzing how task complexity relates to the optimal length of reasoning chains.This study differentiates between problem types (e.g., logical inference and mathematical operations) and systematically evaluates the effect of reasoning step count within different datasets (gsm8k and last letters).By introducing controlled experimental conditions, Curie enables a more detailed exploration of how task complexity interacts with reasoning steps to optimize model performance.</p>
<p>C. Extended Evaluation: Fine-grained Performance Breakdown by Individual Metrics</p>
<p>We detail fine-grained breakdowns for each of our performance metrics mentioned in §5.Here we observe the general trend that increasing complexity across all dimensions causes reductions in average metric scores, as shown in Fig. 10, Fig. 11 and Fig. 12, respectively.In particular, we observe that conclusion scores are most heavily affected as complexity increases across dimensions, reaching 0% on many occasions for Magentic in particular.For design complexity on the other hand, we observe that we're able to maintain a relatively high average score across all baselines and Curie, but this tapers down as the difficulty increases across dimensions.-Did you identify a clear, correct hypothesis?-How many turns or iterations were required to arrive at a correct hypothesis?</p>
<ol>
<li>Experimental Setup:</li>
</ol>
<p>-Is the experimental setup reproducible, usable, and interpretable?-Does it meet the rigor required by the scientific method?</p>
<ol>
<li>Results Generation:</li>
</ol>
<p>-Are the results actually produced through experimentation?-Are the results accurate and sufficient to justify your conclusions?</p>
<p>Conclusion Derivation:</p>
<p>-Are the conclusions correct and logically derived from the results?-Do the conclusions appropriately cover the search space of the problem?</p>
<p>Workflow Design:</p>
<p>-Is the experimental workflow cohesive and callable as a single program?-Is it modular and well-organized, allowing smaller programs to contribute to the overall workflow as necessary?</p>
<p>Expectations for Your Behavior:</p>
<p>-Think like a scientist.Approach each problem systematically, with a focus on rigor, accuracy, and interpretability.</p>
<p>-Produce experiments and results that can be scrutinized, reproduced, and used by others.</p>
<p>-Justify your steps and decisions clearly, and ensure your results align with the problem's requirements.</p>
<p>-Your success depends on delivering usable, rigorous, and interpretable experimental workflows that solve the given questions effectively.</p>
<p>-</p>
<p>Figure 1 .
1
Figure 1.Curie overview.</p>
<p>Figure 2 .
2
Figure 2. Case Study.Curie can help researchers validate, expand, and critique existing research on the benefits of repeated sampling in LLM reasoning (Brown et al., 2024).The first panel (Original Finding) presents a result from the original paper.The second panel (Reproduce) has Curie confirming this finding through rigorous experimentation.The third panel (Extend) has Curie exploring the impact of sampling temperature on repeated sampling.The final panel (Challenge) shows Curie identifying a limitation in the original methodology, suggesting an avenue for future research.</p>
<p>Figure 3 .
3
Figure3.Curie workflow with an example task in LLM reasoning.The Architect is responsible for designing high-level plans and reflects on the new findings.The Technician is responsible for implementing and executing the experiments based on the plans.Whenever an agent completes its action (step 1 , 2 , 3 , 4 , 5 ), the Experimental Rigor Engine (steps A ⇀ B ⇀ C ) validates the action, determines next steps, assigns tasks and maintains interpretable experimental progress, ensuring rigor throughout the entire process.</p>
<p>Figure 4 .
4
Figure 4. Intra-ARM setup validation high-level workflow.</p>
<p>(a) Example errors that can be captured by the setup validator.(b) Example errors that can be captured by the execution validator.</p>
<p>Figure 5 .
5
Figure 5. Errors detected by two of Intra-ARM 's many validators.</p>
<p>Figure 6 .
6
Figure 6.Simplified Inter-ARM workflow with a partition state snapshot.Partition, control flow, and scheduling policies are customizable.</p>
<p>Figure 7 .
7
Figure 7. Simplified partial snapshot of an example Time Machine.</p>
<p>:</p>
<p>To ensure agents correctly interpret and execute tasks, the benchmark provides detailed context for each question.This includes: (a) Domain Knowl-edge -Background information essential for interpreting the problem.(b) Starter Code &amp; Tools -Predefined scaffolding to simulate real-world research workflows.</p>
<p>Figure 9 .
9
Figure 9. Case studies on LLM reasoning tasks.</p>
<p>Figure 12 .
12
Figure12.Average design scores across different complexity dimensions at varying difficulty levels for Curie, OpenHands, and Magentic.Curie outperforms the others consistently, with performance generally dropping as complexity increases.</p>
<p>Table 1 .
1
Experimentation benchmark overview.
DomainComplexity Dist. Easy Med. HardDescriptionSourcesLLM Reasoning457</p>
<p>Table 2 .
2
Main benchmark results in terms of four metrics introduced in §5.We aggregate and average the success rate among all tasks within each domain.The final row presents the weighted average, computed based on the number of tasks in each domain.One Des.Exec.Align.Con.Des.Exec.Align.Con.Des.Exec.Align.Con.
Curie Microsoft Magentic-LLM Reason. OpenHands 98.3 83.3 76.7 44.9 86.7 24.6 36.7 14.2 72.0 9.3 146.7Vector DB97.871.777.225.6 85.0 48.352.311.7 85.06.463.60.0Cloud Comp. 100.0 92.796.932.3 96.9 25.249.25.095.06.333.80.0ML Training95.266.739.341.7 63.1 24.316.75.790.02.925.70.0Weighted Avg. 97.978.173.436.1 83.6 32.440.210.5 82.96.835.22.3ing objectives and trade-offs involved, from single-metricoptimization to multi-objective balancing under constraints.</p>
<p>Average scores across different complexity dimensions at varying difficulty levels for Curie, OpenHands, and Magentic.
90Average score (%)25 50 75CurieOpenHandsMagentic0Easy Medium HardEasy Medium HardEasy Medium HardEasy Medium HardEasy Medium HardFigure 8.</p>
<p>Average alignment scores across different complexity dimensions at varying difficulty levels for Curie, OpenHands, and Magentic.Curie outperforms the others consistently, with performance generally dropping as complexity increases.Average conclusion scores across different complexity dimensions at varying difficulty levels for Curie, OpenHands, and Magentic.Curie outperforms the others consistently, with performance generally dropping as complexity increases.
100Average score (%)25 50 75CurieOpenHandsMagentic0Easy Medium HardEasy Medium HardEasy Medium HardEasy Medium HardEasy Medium Hard0 100 Figure 10. Easy Medium Hard 25 50 75 Average score (%)Easy Medium Hard CurieEasy Medium Hard OpenHandsEasy Medium Hard MagenticEasy Medium Hard0 100 Figure 11. Easy Medium Hard 25 50 75 Average score (%)Easy Medium Hard CurieEasy Medium Hard OpenHandsEasy Medium Hard MagenticEasy Medium Hard</p>
<p>Make sure you provide a reproducible experimental workflow (i.e., verify that it is runnable multiple times to produce acceptable results) that can be callable through a single program; name it experimental_workflow.shReminder:Yourrole is to conduct actual experiments and generate real results, no simulations, placeholders, or unverified assumptions are allowed.E.2.LLM Judge System Prompt[System Prompt] You are an strict Experimentation Agent Verifier, responsible for evaluating whether an experimentation agent correctly conducted an experiment based on the experimentation question.You are provided with an experiment log chunk, the original experimentation question, and the ground truth (only contains the conclusion).Your assessment should focus on: 1. Experiment Design -Did the agent structure the correct high-level plan to address the experimentation question?It does not need to write implementation code or execute the plan.2.Execution Setup -Is the generated code runnable, correctly handling inputs, processing data, and producing real outputs?Is the whole experimental workflow generated for reproducibility?3.Implementation Alignment-Is the code properly aligned with the experimentation design and accurately implementing the intended methodology?Ensure: Legitimate handling of inputs and outputs.No hardcoded or mock data.4.Conclusion Correctness -Is the conclusion acceptable by the ground truth?Analyze the provided chunked Log File, and provide a structured evaluation based on the criteria below: Analyze this log chunk and provide your evaluation in the specified JSON format.
Response Format<em> Overall Verdict: Correct / Incorrect</em> Detailed Assessment:<em> Experiment Design: [Pass/Fail]</em> Execution Setup: [Pass/Fail]<em> Implementation Alignment : [Pass/Fail]</em> Conclusion Correctness: [Pass/Fail]* Explanation: [Concisely explanation about the failure reasons, no reasonneeded if the step is missing]"""user_prompt = f"""&gt; Original Experimentation Question:{question}&gt; Ground Truth:{ground_truth}&gt; Log Chunk:{log_chunk}
A. Curie Benchmark Complexity ExplanationWe describe in detail our complexity level definitions in Table.3.B. Case Studies for CurieWe provide two example case studies for LLM reasoning tasks that Curie was able to extend from the paper The Impact of Reasoning Step Length on Large Language Models(Jin et al., 2024).In Fig.9a, the objective of this experiment is to examine whether different models exhibit varying accuracy levels based on the number of reasoning steps.The experiment maintains constant variables, including the dataset (last letters), the method (auto cot), and the evaluation metric (accuracy).The independent variables include the model type (gpt-4o-mini vs. gpt-4o) and the number of reasoning steps(1,2,3,4,5,6,10), while the dependent variable is the model's accuracy.The experiment consists of a control group and experimental groups.The control group uses gpt-4o-mini with a single reasoning step to establish a baseline accuracy.The experimental groups involve testing gpt-4o-mini with reasoning steps ranging from 2 to 10 and gpt-4o with reasoning steps from 1 to 10.The results will help determine whether reasoning step variations impact accuracy differently across models.Curie extends the original investigation by examining whether different LLMs exhibit varying accuracy using GPT-4o and GPT-4o-mini.What is the best AWS EC2 instance type within the c5 family (instances listed below) for running an e-commerce web application serving 500 concurrent requests to its add to cart function?Do not terminate until you identify the best instance type concretely.Easy Medium Easy Medium MediumWhat is the best AWS EC2 instance type within the c5 family (instances listed below) for running an e-commerce web application serving 500 concurrent requests to its add to cart function, aiming to minimise cost while maintaining a 99th percentile latency below 150ms?Do not terminate until you identify the best instance type concretely.Easy Easy Medium Hard MediumWhat is the best AWS EC2 instance type within the c5 family (instances listed below) for running an e-commerce web application serving 500 concurrent requests to its add to cart function, aiming to minimise cost while maintaining a 99th percentile latency below 150ms?Do not terminate until you identify the best instance type concretely.Easy Medium Medium Medium MediumWhat is the best AWS EC2 instance type within the c5 and t3 families (instances listed below) for running an e-commerce web application serving 500 concurrent requests to its add to cart function, aiming to minimise cost while maintaining a 99th percentile latency below 150ms?Do not terminate until you identify the best instance type concretely.
S Agarwal, I H Laradji, L Charlin, C Pal, Litllm, arXiv:2402.01788A toolkit for scientific literature review. 2024arXiv preprinta new claude 3.5 sonnet, and claude 3.5 haiku. 2024.</p>
<p>Using context to build rigor: Application to two hermeneutic phenomenological studies. M Armour, S L Rivaux, H Bell, 10.1177/1473325008100424Qualitative Social Work. 1473-325081Mar 2009</p>
<p>Conversation patterns. Autogen , 2024</p>
<p>A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. Y Bang, S Cahyawijaya, N Lee, W Dai, D Su, B Wilie, H Lovenia, Z Ji, T Yu, W Chung, Q V Do, Y Xu, P Fung, 2023</p>
<p>Ai for science: An emerging agenda. P Berens, K Cranmer, N D Lawrence, U Von Luxburg, J Montgomery, 2023</p>
<p>Knowledge graph extraction from total synthesis documents. A M Bran, Z Jončev, P Schwaller, Proceedings of the 1st Workshop on Language+ Molecules (L+ M 2024). the 1st Workshop on Language+ Molecules (L+ M 2024)2024</p>
<p>Large language monkeys: Scaling inference compute with repeated sampling. B Brown, J Juravsky, R Ehrlich, R Clark, Q V Le, C Ré, A Mirhoseini, arXiv:2407.217872024arXiv preprint</p>
<p>Z Chen, S Chen, Y Ning, Q Zhang, B Wang, B Yu, Y Li, Z Liao, C Wei, Z Lu, arXiv:2410.05080Toward rigorous assessment of language agents for data-driven scientific discovery. 2024arXiv preprint</p>
<p>Language models as science tutors. A Chevalier, J Geng, A Wettig, H Chen, S Mizera, T Annala, M J Aragon, A R Fanlo, S Frieder, S Machado, A Prabhakar, E Thieu, J T Wang, Z Wang, X Wu, M Xia, W Xia, J Yu, J.-J Zhu, Z J Ren, S Arora, D Chen, 2024</p>
<p>Training verifiers to solve math word problems. K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, C Hesse, J Schulman, 2021</p>
<p>The faiss library. M Douze, A Guzhva, C Deng, J Johnson, G Szilvasy, P.-E Mazaré, M Lomeli, L Hosseini, H Jégou, 2024</p>
<p>Magentic-one: A generalist multi-agent system for solving complex tasks. A Fourney, G Bansal, H Mozannar, C Tan, E Salinas, F Niedtner, G Proebsting, G Bassman, J Gerrits, J Alber, arXiv:2411.044682024arXiv preprint</p>
<p>Mathematical capabilities of chatgpt. S Frieder, L Pinchetti, R.-R Griffiths, T Salvatori, T Lukasiewicz, P Petersen, J Berner, Advances in Neural Information Processing Systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>Automating scientific discovery through multi-agent intelligent graph reasoning. A Ghafarollahi, M J Buehler, Sciagents, 2024</p>
<p>Informing Science: The International Journal of an Emerging Transdiscipline. T Gill, T Gill, 10.28945/4528232020What is research rigor? lessons for a transdiscipline</p>
<p>Generation and human-expert evaluation of interesting research ideas using knowledge graphs and large language models. X Gu, M Krenn, arXiv:2405.170442024arXiv preprint</p>
<p>Measuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, 2021a</p>
<p>Measuring mathematical problem solving with the math dataset. D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, 2021b</p>
<p>Plant growth: the what, the how, and the why. J Hilty, B Muller, F Pantin, S Leuzinger, 10.1111/nph.17610New Phytologist. 23212021</p>
<p>Getting rigorous with scientific rigor. L J Hofseth, Carcinogenesis. 391January 2018</p>
<p>Data interpreter: An llm agent for data science. S Hong, Y Lin, B Liu, B Liu, B Wu, C Zhang, C Wei, D Li, J Chen, J Zhang, arXiv:2402.186792024aarXiv preprint</p>
<p>Meta programming for a multi-agent collaborative framework. S Hong, M Zhuge, J Chen, X Zheng, Y Cheng, J Wang, C Zhang, Z Wang, S K S Yau, Z Lin, L Zhou, C Ran, L Xiao, C Wu, J Schmidhuber, Metagpt, The Twelfth International Conference on Learning Representations. 2024b</p>
<p>The hallmark effect: Supporting provenance and transparent use of large language models in writing with interactive visualization. M N Hoque, T Mashiat, B Ghai, C D Shelton, F Chevalier, K Kraus, N Elmqvist, Proceedings of the CHI Conference on Human Factors in Computing Systems. the CHI Conference on Human Factors in Computing Systems2024</p>
<p>Mlagentbench: Evaluating language agents on machine learning experimentation. Q Huang, J Vora, P Liang, J Leskovec, 2024</p>
<p>C E Jimenez, J Yang, A Wettig, S Yao, K Pei, O Press, K Narasimhan, arXiv:2310.06770Swe-bench: Can language models resolve real-world github issues?. 2023arXiv preprint</p>
<p>Nobel turing challenge: creating the engine for scientific discovery. M Jin, Q Yu, D Shu, H Zhao, W Hua, Y Meng, Y Zhang, M Du, 10.1038/s41540-021-00189-3arXiv:2401.04925Systems Biology and Applications. 2056-718971292024. Jun 2021arXiv preprintThe impact of reasoning step length on large language models</p>
<p>Automated scientific discovery: From equation discovery to autonomous discovery systems. S Kramer, M Cerrato, S Džeroski, R King, 2023</p>
<p>Let's verify step by step. H Lightman, V Kosaraju, Y Burda, H Edwards, B Baker, T Lee, J Leike, J Schulman, I Sutskever, K Cobbe, arXiv:2305.200502023arXiv preprint</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. C Lu, C Lu, R T Lange, J Foerster, J Clune, D Ha, arXiv:2408.062922024arXiv preprint</p>
<p>Large language models as biomedical hypothesis generators: A comprehensive evaluation. B Qi, K Zhang, K Tian, H Li, Z.-R Chen, S Zeng, E Hua, H Jinfang, B Zhou, 2024</p>
<p>S Schmidgall, Y Su, Z Wang, X Sun, J Wu, X Yu, J Liu, Z Liu, E Barsoum, arXiv:2501.04227Agent laboratory: Using llm agents as research assistants. 2025arXiv preprint</p>
<p>Accelerating science with humanaware artificial intelligence. J Sourati, J A Evans, Nature human behaviour. 7102023</p>
<p>A multi-level large language model evaluation benchmark for scientific research. L Sun, Y Han, Z Zhao, D Ma, Z Shen, B Chen, L Chen, K Yu, Scieval, 10.1609/aaai.v38i17.29872Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceMar. 2024a38</p>
<p>W Sun, L Yan, X Ma, S Wang, P Ren, Z Chen, D Yin, Z Ren, Is chatgpt good at search? investigating large language models as re-ranking agents. 2024b</p>
<p>Ai-driven review systems: Evaluating llms in scalable and bias-aware academic reviews. K Tyser, B Segev, G Longhitano, X.-Y Zhang, Z Meeks, J Lee, U Garg, N Belsten, A Shporer, M Udell, D Te'eni, I Drori, 2024</p>
<p>R Wang, E Zelikman, G Poesia, Y Pu, N Haber, N D Goodman, Hypothesis search: Inductive reasoning with language models. 2024a</p>
<p>Evaluating college-level scientific problem-solving abilities of large language models. X Wang, Z Hu, P Lu, Y Zhu, J Zhang, S Subramaniam, A R Loomba, S Zhang, Y Sun, W Wang, Scibench, 2024b</p>
<p>X Wang, B Li, Y Song, F F Xu, X Tang, M Zhuge, J Pan, Y Song, B Li, J Singh, arXiv:2407.16741An open platform for ai software developers as generalist agents. 2024carXiv preprint</p>
<p>Agent workflow memory. Z Z Wang, J Mao, D Fried, G Neubig, 2024d</p>
<p>R Xu, Z Qi, Z Guo, C Wang, H Wang, Y Zhang, W Xu, arXiv:2403.08319Knowledge conflicts for llms: A survey. 2024arXiv preprint</p>
<p>J Yang, C E Jimenez, A Wettig, K Lieret, S Yao, K Narasimhan, O Press, arXiv:2405.15793Swe-agent: Agent-computer interfaces enable automated software engineering. 2024arXiv preprint</p>
<p>Dolphin: Closed-loop open-ended auto-research through thinking, practice, and feedback. J Yuan, X Yan, B Shi, T Chen, W Ouyang, B Zhang, L Bai, Y Qiao, B Zhou, 2025</p>
<p>Unleashing the power of large language models in solving machine learning tasks. L Zhang, Y Zhang, K Ren, D Li, Y Yang, Mlcopilot, 2024a</p>
<p>Automl-gpt: Automatic machine learning with gpt. S Zhang, C Gong, L Wu, X Liu, M Zhou, 2023</p>
<p>A comprehensive survey of scientific large language models and their applications in scientific discovery. Y Zhang, X Chen, B Jin, S Wang, S Ji, W Wang, J Han, arXiv:2406.108332024barXiv preprint</p>
<p>Judging llm-as-ajudge with mt-bench and chatbot arena. L Zheng, W.-L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E Xing, Advances in Neural Information Processing Systems. 202336</p>
<p>Hypothesis generation with large language models. Y Zhou, H Liu, T Srivastava, H Mei, C Tan, 10.18653/v1/2024.nlp4science-1.10Proceedings of the 1st Workshop on NLP for Science (NLP4Science). the 1st Workshop on NLP for Science (NLP4Science)Association for Computational Linguistics2024</p>            </div>
        </div>

    </div>
</body>
</html>
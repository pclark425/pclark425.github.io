<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5888 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5888</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5888</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-118.html">extraction-schema-118</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <p><strong>Paper ID:</strong> paper-88884b8806262a4095036041e3567d450dba39f7</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/88884b8806262a4095036041e3567d450dba39f7" target="_blank">Active Retrieval Augmented Generation</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work proposes Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens.</p>
                <p><strong>Paper Abstract:</strong> Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5888",
    "paper_id": "paper-88884b8806262a4095036041e3567d450dba39f7",
    "extraction_schema_id": "extraction-schema-118",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.005954,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Active Retrieval Augmented Generation</h1>
<p>Zhengbao Jiang ${ }^{1 <em>}$ Frank F. Xu ${ }^{1 </em>}$ Luyu Gao ${ }^{1 <em>}$ Zhiqing Sun ${ }^{1 </em>}$ Qian Liu ${ }^{2}$<br>Jane Dwivedi-Yu ${ }^{3}$ Yiming Yang ${ }^{1}$ Jamie Callan ${ }^{1}$ Graham Neubig ${ }^{1}$<br>${ }^{1}$ Language Technologies Institute, Carnegie Mellon University<br>${ }^{2}$ Sea AI Lab ${ }^{3}$ FAIR, Meta<br>{zhengbaj, fangzhex, luyug, zhiqings, gneubig}@cs.cmu.edu</p>
<h4>Abstract</h4>
<p>Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 longform knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Generative language models (LMs) (Brown et al., 2020; Ouyang et al., 2022; OpenAI, 2023; Chowdhery et al., 2022; Zhang et al., 2022; Touvron et al., 2023; Zhao et al., 2023) have become a foundational component in natural language processing (NLP) systems with their remarkable abilities. Although LMs have memorized some world knowledge during training (Petroni et al., 2019; Roberts et al., 2020; Jiang et al., 2020), they still tend to</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>hallucinate and create imaginary content (Maynez et al., 2020; Zhou et al., 2021). Augmenting LMs with retrieval components that look up relevant information from external knowledge resources is a promising direction to address hallucination (Khandelwal et al., 2020; Izacard et al., 2022).</p>
<p>Retrieval augmented LMs commonly use a retrieve-and-generate setup where they retrieve documents based on the user's input, and then generate a complete answer conditioning on the retrieved documents (Chen et al., 2017; Guu et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021; Sachan et al., 2021; Lee et al., 2021; Jiang et al., 2022; Izacard et al., 2022; Nakano et al., 2021; Qian et al., 2023; Lazaridou et al., 2022; Shi et al., 2023). These single-time retrieval augmented LMs outperform purely parametric LMs, particularly for shortform knowledge-intensive generation tasks such as factoid question answering (QA) (Kwiatkowski et al., 2019; Joshi et al., 2017), where the information needs are clear in the user's input, and it is sufficient to retrieve relevant knowledge once solely based on the input.</p>
<p>Increasingly powerful large LMs have also demonstrated abilities in more complex tasks that involve generating long-form output, such as longform QA (Fan et al., 2019; Stelmakh et al., 2022), open-domain summarization (Cohen et al., 2021; Hayashi et al., 2021; Giorgi et al., 2022), and (chain-of-thought; CoT) reasoning (Wei et al., 2022; Ho et al., 2020; Geva et al., 2021; Hendrycks et al., 2020). In contrast to short-form generation, long-form generation presents complex information needs that are not always evident from the input alone. Similar to how humans gradually gather information as we create content such as papers, essays, or books, long-form generation with LMs would require gathering multiple pieces of knowledge throughout the generation process. For example, to generate a summary about a particular topic, the initial retrieval based on the topic name</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user input <strong>x</strong> and initial retrieval results <strong>D<sub>x</sub></strong>, FLARE iteratively generates a temporary next sentence (shown in <em>gray italic</em>) and checks whether it contains low-probability tokens (indicated with <em>underline</em>). If so (step 2 and 3), the system retrieves relevant documents and regenerates the sentence.</p>
<p>(e.g., Joe Biden) may not cover all aspects and details. It is crucial to retrieve extra information as needed during generation, such as when generating a certain aspect (e.g., Joe Biden's education history) or a specific detail (e.g., the date of Joe Biden's presidential campaign announcement).</p>
<p>Several attempts have been made to retrieve multiple times throughout generation. These attempts include methods that passively use the past context to retrieve additional information at a fixed interval (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022) which might not accurately reflect what LMs intend to generate in the future or retrieve at inappropriate points. Some works in multihop QA decompose the full question into sub-questions, each of which is used to retrieve extra information (Press et al., 2022; Yao et al., 2022; Khot et al., 2022; Khattab et al., 2022).</p>
<p>We ask the following question: can we create a simple and generic retrieval augmented LM that <em>actively decides when and what to retrieve</em> throughout the generation process, and are applicable to a variety of long-form generation tasks? We provide a generalized view of active retrieval augmented generation. Our hypothesis regarding <em>when to retrieve</em> is that LMs should retrieve information only when they lack the required knowledge to avoid unnecessary or inappropriate retrieval that occurs in passive retrieval augmented LMs (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022). Given the observation that large LMs tend to be well-calibrated and low probability/confidence often indicates a lack of knowledge (Kadavath et al., 2022), we adopt an active retrieval strategy that only retrieves when LMs generate low-probability tokens. When deciding <em>what to retrieve</em>, it is important to consider what LMs intend to generate in the future, as the goal of active retrieval is to benefit future generations. Therefore, we propose anticipating the future by generating a temporary next sentence, using it as a query to retrieve relevant documents, and then regenerating the next sentence conditioning on the retrieved documents. Combining the two aspects, we propose <strong>F</strong>orward-<strong>L</strong>ooking <strong>A</strong>ctive <strong>R</strong>Etrieval augmented generation (<strong>FLARE</strong>), as illustrated in Figure 1. FLARE iteratively generates <em>a temporary next sentence</em>, use it as the query to retrieve relevant documents <em>if it contains low-probability tokens</em> and regenerate the next sentence until reaches the end.</p>
<p>FLARE is applicable to any existing LMs at inference time without additional training.</p>
<p>sidering the impressive performance achieved by GPT-3.5 <em>Ouyang et al. (2022)</em> on a variety of tasks, we examine the effectiveness of our methods on text-davinci-003. We evaluate FLARE on 4 diverse tasks/datasets involving generating long outputs, including multihop QA (2WikiMultihopQA), commonsense reasoning (StrategyQA), long-form QA (ASQA), and open-domain summarization <em>WikiAsp (Ho et al., 2020; Geva et al., 2021; Stelmakh et al., 2022; Hayashi et al., 2021)</em>. Over all tasks, FLARE achieves superior or competitive performance compared to single-time and multi-time retrieval baselines, demonstrating the effectiveness and generalizability of our method.</p>
<h2>2 Retrieval Augmented Generation</h2>
<p>We formally define single-time retrieval augmented generation and propose the framework of active retrieval augmented generation.</p>
<h3>2.1 Notations and Definitions</h3>
<p>Given a user input $\boldsymbol{x}$ and a document corpus $\mathcal{D}=\left{\boldsymbol{d}<em i="1">{i}\right}</em>}^{|\mathcal{D}|}$ (such as all Wikipedia articles), the goal of retrieval augmented LMs is to generate the answer $\boldsymbol{y}=\left[\boldsymbol{s<em 2="2">{1}, \boldsymbol{s}</em>}, \ldots, \boldsymbol{s<em 1="1">{m}\right]=\left[w</em>\right]$ containing $m$ sentences or $n$ tokens leveraging information retrieved from the corpus.}, w_{2}, \ldots, w_{n</p>
<p>In retrieval augmented LM, the LM typically pairs with a retriever that can retrieve a list of documents $\mathcal{D}<em _boldsymbol_q="\boldsymbol{q">{\boldsymbol{q}}=\operatorname{ret}(\boldsymbol{q})$ for a query $\boldsymbol{q}$; the LM conditions on both the user input $\boldsymbol{x}$ and retrieved documents $\mathcal{D}</em>$ to generate the answer. Since we focus on examining various methods of determining when and what to retrieve, we follow existing methods }<em>Ram et al. (2023); Trivedi et al. (2022)</em> to prepend the retrieved documents before the user input to aid future generation for both baselines and our method for fair comparisons: $\boldsymbol{y}=\operatorname{LM}\left(\left[\mathcal{D}_{\boldsymbol{q}}, \boldsymbol{x}\right]\right)$, where $[\cdot, \cdot]$ is concatenation following the specified order.</p>
<h3>2.2 Single-time Retrieval Augmented Generation</h3>
<p>The most common choice is to directly use the user input as the query for retrieval and generate the complete answer at once $\boldsymbol{y}=\operatorname{LM}\left(\left[\mathcal{D}_{\boldsymbol{x}}, \boldsymbol{x}\right]\right)$.</p>
<h3>2.3 Active Retrieval Augmented Generation</h3>
<p>To aid long-form generation with retrieval, we propose active retrieval augmented generation. It is a generic framework that actively decides when and what to retrieve through the generation process, resulting in the interleaving of retrieval and generation. Formally, at step $t(t \geq 1)$, the retrieval query $\boldsymbol{q}<em _t="&lt;t">{t}$ is formulated based on both the user input $\boldsymbol{x}$ and previously generated output $\boldsymbol{y}</em>}=\left[\boldsymbol{y<em t-1="t-1">{0}, \ldots, \boldsymbol{y}</em>\right]$ :</p>
<p>$$
\boldsymbol{q}<em _t="&lt;t">{t}=\operatorname{qry}\left(\boldsymbol{x}, \boldsymbol{y}</em>\right)
$$</p>
<p>where $\operatorname{qry}(\cdot)$ is the query formulation function. At the beginning $(t=1)$, the previous generation is empty $\left(\boldsymbol{y}<em 1="1">{&lt;1}=\emptyset\right)$, and the user input is used as the initial query $\left(\boldsymbol{q}</em>}=\boldsymbol{x}\right)$. Given retrieved documents $\mathcal{D<em t="t">{\boldsymbol{q}</em>$, LMs continually generate the answer until the next retrieval is triggered or reaches the end:}</p>
<p>$$
\boldsymbol{y}<em _boldsymbol_q="\boldsymbol{q">{t}=\operatorname{LM}\left(\left[\mathcal{D}</em><em _t="&lt;t">{t}}, \boldsymbol{x}, \boldsymbol{y}</em>\right]\right)
$$</p>
<p>where $\boldsymbol{y}<em _boldsymbol_q="\boldsymbol{q">{t}$ represents the generated tokens at the current step $t$, and the input to LMs is the concatenation of the retrieved documents $\mathcal{D}</em><em _t="&lt;t">{t}}$, the user input $\boldsymbol{x}$, and the previous generation $\boldsymbol{y}</em>}$. We discard previously retrieved documents $\cup_{t^{\prime}&lt;t} \mathcal{D<em t_prime="t^{\prime">{\boldsymbol{q}</em>$ and only use the retrieved documents from the current step to condition the next generation to prevent reaching the input length limit of LMs.}}</p>
<h2>3 FLARE: Forward-Looking Active REtrieval Augmented Generation</h2>
<p>Our intuition is that (1) LMs should only retrieve information when they do not have the necessary knowledge to avoid unnecessary or inappropriate retrieval, and (2) the retrieval queries should reflect the intents of future generations. We propose two forward-looking active retrieval augmented generation (FLARE) methods to implement the active retrieval augmented generation framework. The first method prompts the LM to generate retrieval queries when necessary while generating the answer using retrieval-encouraging instructions, denoted as $\mathrm{FLARE}<em _direct="{direct" _text="\text">{\text {instruct }}$. The second method directly uses the LM's generation as search queries, denoted as $\mathrm{FLARE}</em>$, which iteratively generates the next sentence to gain insight into the future topic, and if uncertain tokens are present, retrieves relevant documents to regenerate the next sentence.}</p>
<h3>3.1 FLARE with Retrieval Instructions</h3>
<p>Inspired by Toolformer <em>Schick et al. (2023)</em>, a straightforward way of expressing information needs for retrieval is to generate "[Search(query)]" when additional information is needed <em>Schick et al. (2023)</em>, e.g., "The colors on the flag of Ghana have the following meanings. Red is for [Search(Ghana flag red meaning)] the blood of martyrs, ..." When working with GPT-3.5 models that</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: An illustration of forward-looking active retrieval augmented generation with retrieval instructions (FLARE ${ }_{\text {instruct }}$ ). It iteratively generates search queries (shown in gray italic) to retrieve relevant information to aid future generations.
offer only API access, we elicit such behavior by few-shot prompting (Brown et al., 2020).</p>
<p>Specifically, for a downstream task, we place the search-related instruction and exemplars at the beginning as skill 1, followed by the instruction and exemplars of the downstream task as skill 2. Given a test case, we ask LMs to combine skills 1 and 2 to generate search queries while performing the task. The structure of the prompt is shown in Prompt 3.1, and full details can be found in Prompt D.3.</p>
<h2>Prompt 3.1: retrieval instructions</h2>
<p>Skill 1. An instruction to guide LMs to generate search queries.
Several search-related exemplars.
Skill 2. An instruction to guide LMs to perform a specific downstream task (e.g., multihop QA).
Several task-related exemplars.
An instruction to guide LMs to combine skills 1 and 2 for the test case.
The input of the test case.
As shown in Figure 2, when the LM generates "[Search(query)]" (shown in gray italic), we stop the generation and use the query terms to retrieve relevant documents, which are prepended before the user input to aid future generation until the next search query is generated or reaches the end. Additional implementation details are included in Appendix A.</p>
<h3>3.2 Direct FLARE</h3>
<p>Since we cannot fine-tune black-box LMs, we found queries generated by $\mathrm{FLARE}_{\text {instruct }}$ through
retrieval instructions might not be reliable. Therefore, we propose a more direct way of forwardlooking active retrieval that uses the next sentence to decide when and what to retrieve.</p>
<h3>3.2.1 Confidence-based Active Retrieval</h3>
<p>As shown in Figure 1, at step $t$, we first generate a temporary next sentence $\hat{s}<em _t="&lt;t">{t}=\operatorname{LM}\left(\left[\boldsymbol{x}, \boldsymbol{y}</em>}\right]\right)$ without conditioning on retrieved documents. Then we decide whether to trigger retrieval and formulate queries based on $\hat{s<em t="t">{t}$. If the LM is confident about $\hat{s}</em>}$, we accept it without retrieving additional information; if not, we use $\hat{s<em t="t">{t}$ to formulate search queries $\boldsymbol{q}</em>$. The reason we utilize sentences as the basis of our iteration is due to their significance as semantic units that are neither too short nor too lengthy like phrases and paragraphs. However, our approach can also utilize phrases or paragraphs as the basis.}$ to retrieve relevant documents, and then regenerate the next sentence $\boldsymbol{s}_{t</p>
<p>Since LMs tend to be well-calibrated that low probability/confidence often indicates a lack of knowledge (Jiang et al., 2021; Kadavath et al., 2022; Varshney et al., 2022), we actively trigger retrieval if any token of $\hat{s}<em t="t">{t}$ has a probability lower than a threshold $\theta \in[0,1] . \theta=0$ means retrieval is never triggered, while $\theta=1$ triggers retrieval every sentence.
$\boldsymbol{y}</em>}= \begin{cases}\hat{\boldsymbol{s}<em t="t">{t} &amp; \text { if all tokens of } \hat{s}</em>} \text { have probs } \geq \theta \ \boldsymbol{s<em _boldsymbol_q="\boldsymbol{q">{t}=\operatorname{LM}\left(\left[\mathcal{D}</em><em _t="&lt;t">{t}}, \boldsymbol{x}, \boldsymbol{y}</em>$
where the query $\boldsymbol{q}}\right]\right) &amp; \text { otherwise }\end{cases<em t="t">{t}$ is formulated based on $\hat{s}</em>$.</p>
<h3>3.2.2 Confidence-based Query Formulation</h3>
<p>One way to perform retrieval is to directly use the next sentence $\hat{s}<em t="t">{t}$ as the query $\boldsymbol{q}</em>$. This shares a similar spirit with methods that use generated hypothetical titles or paragraphs from LMs as retrieval queries or evidences (Gao et al., 2022; Sun et al., 2022; Yu et al., 2022; Mao et al., 2021). We generalize such techniques to long-form generation where active information access is essential.</p>
<p>We found retrieving with the next sentence achieves significantly better results than with the previous context, as shown later in subsection 6.2. However, it has a risk of perpetuating errors contained in it. For example, if the LM produces the sentence "Joe Biden attended the University of Pennsylvania" instead of the correct fact that he attended the University of Delaware, using this erroneous sentence as a query might retrieve mislead-</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Ask a question to which the answer is "the University of Pennsylvania" Ask a question to which the answer is "a law degree"</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>What university did Joe Biden attend? What degree did Joe Biden earn?</p>
<p>Figure 3: Implicit and explicit query formulation. Tokens with low probabilities are marked with underlines.
ing information. We propose two simple methods to overcome this issue as illustrated in Figure 3.</p>
<p>Masked sentences as implicit queries. The first method masks out low-confidence tokens in $\hat{s}_{t}$ with probabilities below a threshold $\beta \in[0,1]$, where a higher $\beta$ results in more aggressive masking. This removes potential distractions from the sentence to improve retrieval accuracy.</p>
<p>Generated questions as explicit queries. Another method is to generate explicit questions that target the low-confident span in $\hat{s}<em t="t">{t}$. For example, if the LM is uncertain about "the University of Pennsylvania", a question like "Which university did Joe Biden attend?" can help retrieve relevant information. Self-ask (Press et al., 2022) achieved this by manually inserting follow-up questions into downstream task exemplars as shown later in Prompt D.2, which requires task-specific annotation efforts. Instead, we developed a universal approach that generates questions for low-confidence spans without additional annotation. Specifically, We first extract all spans from $\hat{s}</em>$ that can be answered with the span:}$ with probabilities below $\beta$. For each extracted span $z$, we prompt gpt-3.5-turbo to generate a question $\boldsymbol{q}_{t, z</p>
<h2>Prompt 3.2: zero-shot question generation</h2>
<p>User input $\boldsymbol{x}$.
Generated output so far $\boldsymbol{y}_{\leq t}$.
Given the above passage, ask a question to which the answer is the term/entity/phrase " $z$ ".</p>
<p>We retrieve using each generated question and interleave the returned documents into a single ranking list to aid future generations. In summary, queries $\boldsymbol{q}<em t="t">{t}$ are formulated based on $\hat{s}</em>$ as follows:
$\boldsymbol{q}<em t="t">{t}= \begin{cases}\emptyset &amp; \text { if all tokens of } \hat{s}</em>} \text { have probs } \geq \theta \ \operatorname{mask}\left(\hat{s<em t="t">{t}\right) \text { or } \operatorname{qgen}\left(\hat{s}</em>$}\right) &amp; \text { otherwise }\end{cases</p>
<h3>3.3 Implementation Details</h3>
<p>Base LM We validate our method on one of the most advanced GPT-3.5 LMs text-davinci-003 by iteratively querying their API. ${ }^{2}$</p>
<p>Document corpus and retrievers. Since we focus on the integration of retrieval and generation, we use off-the-shelf retrievers that take queries as inputs and return a list of relevant documents. For datasets that mainly rely on knowledge from Wikipedia, we use the Wikipedia dump from Karpukhin et al. (2020) and employ BM25 (Robertson and Zaragoza, 2009) as the retriever. For datasets that rely on knowledge from the open web, we use the Bing search engine as our retriever. ${ }^{3}$</p>
<p>Retrieved document formatting. Multiple retrieved documents are linearized according to their ranking and then added to the beginning of the user input using Prompt D.1.</p>
<p>Other implementation details such as sentence tokenization and efficiency are included Appendix A.</p>
<h2>4 Multi-time Retrieval Baselines</h2>
<p>Existing passive multi-time retrieval augmented LMs can also be formulated using our framework (subsection 2.3). In this section, we formally introduce three baseline categories based on when and what to retrieve. These baselines are not exact reproductions of the corresponding paper because many design choices differ which makes direct comparisons impossible. We implemented them using the same settings, with the only variation being when and what to retrieve.</p>
<p>Previous-window approaches trigger retrieval every $l$ tokens, where $l$ represents the window size. Generated tokens from the previous window are used as the query:</p>
<p>$$
\begin{aligned}
&amp; \boldsymbol{q}<em t-1="t-1">{t}=\boldsymbol{y}</em> \quad(t \geq 2) \
&amp; \boldsymbol{y}<em _t-1_="(t-1)" l_1="l+1">{t}=\left[w</em>\right]
\end{aligned}
$$}, \ldots, w_{t l</p>
<p>Some existing methods in this category are RETRO (Borgeaud et al., 2022), IC-RALM (Ram et al.,</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>2023), which retrieve every few tokens, and KNN-LM (Khandelwal et al., 2020), which retrieves every token. We follow Ram et al. (2023) to use a window size of $l=16$.</p>
<p>Previous-sentence approaches trigger retrieval every sentence and use the previous sentence as the query, and IRCoT (Trivedi et al., 2022) belongs to this category:</p>
<p>$$
\begin{aligned}
&amp; \boldsymbol{q}<em t-1="t-1">{t}=\boldsymbol{y}</em> \quad(t \geq 2) \
&amp; \boldsymbol{y}<em t="t">{t}=\boldsymbol{s}</em>
\end{aligned}
$$</p>
<p>Question decomposition approaches manually annotated task-specific exemplars to guide LMs to generate decomposed sub-questions while producing outputs. For example, self-ask (Press et al., 2022), a method in this category, manually inserts sub-questions in exemplars using Prompt D.2. For the test case, retrieval is triggered dynamically whenever the model generates a sub-question.</p>
<p>The aforementioned approaches can retrieve additional information while generating. However, they have notable drawbacks: (1) Using previously generated tokens as queries might not reflect what LMs intend to generate in the future. (2) Retrieving information at a fixed interval can be inefficient because it might occur at inappropriate points. (3) Question decomposition approaches require taskspecific prompt engineering, which restricts their generalizability in new tasks.</p>
<h2>5 Experimental Setup</h2>
<p>We evaluate the effectiveness of FLARE on 4 di-verse knowledge-intensive tasks using few-shot incontext learning (Radford et al., 2019; Brown et al., 2020; Liu et al., 2023). We follow previous works (Trivedi et al., 2022) to sub-sample at most 500 examples from each dataset due to the cost of running experiments. Datasets, metrics, and settings are summarized in Table 7 of Appendix B. The hyperparameters of FLARE are selected based on the development set and listed in Table 9. FLARE refers to $\mathrm{FLARE}_{\text {direct }}$ if not specifically stated.</p>
<p>Multihop QA The goal of multihop QA is to answer complex questions through information retrieval and reasoning. We use 2WikiMultihopQA (Ho et al., 2020) which contains 2-hop complex</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>questions sourced from Wikipedia articles that require composition, comparison, or inference, e.g., "Why did the founder of Versus die?" We follow Wang et al. (2022) to generate both the chain-ofthought and the final answer. Experimental setting details are included in Appendix B.</p>
<p>We use regular expressions to extract the final answer from the output and compare it with the reference answer using exact match (EM), and tokenlevel $\mathrm{F}_{1}$, precision, and recall.</p>
<p>Commonsense reasoning Commonsense reasoning requires world and commonsense knowledge to generate answers. We use StrategyQA (Geva et al., 2021) which is a collection of crowdsourced yes/no questions, e.g., "Would a pear sink in water?" We follow Wei et al. (2022) to generate both the chain-of-thought and the final yes/no answer. Details are included in Appendix B.</p>
<p>We extract the final answer and match it against the gold answer using exact match.</p>
<p>Long-form QA Long-form QA aims to generate comprehensive answers to questions seeking complex information (Fan et al., 2019; Stelmakh et al., 2022). We use ASQA (Stelmakh et al., 2022) as our testbed where inputs are ambiguous questions with multiple interpretations, and outputs should cover all of them. For example, "Where do the Philadelphia Eagles play their home games?" could be asking about the city, sports complex, or stadium. We found in many cases it is challenging even for humans to identify which aspect of the question is ambiguous. Therefore, we created another setting (ASQA-hint) where we provide a brief hint to guide LMs to stay on track when generating answers. The hint for the above case is "This question is ambiguous in terms of which specific location or venue is being referred to." Experimental setting details are included in Appendix B.</p>
<p>We use metrics from Stelmakh et al. (2022), including EM, RoBERTa-based QA score (Disambig$\mathrm{F}<em 1="1">{1}$ ), ROUGE (Lin, 2004), and an overall score combining Disambig- $\mathrm{F}</em>$ and ROUGE (DR).</p>
<p>Open-domain summarization The goal of opendomain summarization is to generate a comprehensive summary about a topic by gathering information from open web (Giorgi et al., 2022). We use WikiAsp (Hayashi et al., 2021) which aims to generate aspect-based summaries about entities from 20 domains in Wikipedia, e.g., "Generate a summary about Echo School (Oregon) including the</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 4: Comparision between FLARE and baselines across all tasks/datasets. We report the primary metric for each dataset: EM for 2WikiMultihopQA, StrategyQA, and ASQA, and UniEval for WikiAsp.</p>
<p>following aspects: academics, history.‚Äù Experimental setting details are included in Appendix B.</p>
<p>Metrics include ROUGE, named entity-based F1, and UniEval (Zhong et al., 2022) which measures factual consistency.</p>
<h2>6 Experimental Results</h2>
<p>We first report overall results across 4 tasks/datasets and compare the performance of FLARE with all the baselines introduced in section 4. We then run ablation experiments to study the efficacy of various design choices of our method.</p>
<h3>6.1 Comparison with Baselines</h3>
<p><strong>Overall results.</strong> The overall performance of FLARE and baseline across all tasks/datasets are reported in Figure 4. FLARE outperforms all baseline on all tasks/datasets, indicating that FLARE is a generic method that can effectively retrieve additional information throughout the generation.</p>
<p>Among various tasks, multihop QA shows the most significant improvement. This is largely due to the task's clear definition and specific objective of producing the final answer through a 2-hop reasoning process, which makes it easier for LMs to generate on-topic output. In contrast, ASQA and WikiAsp are more open-ended, which increases the difficulty of both generation and evaluation. The improvement on ASQA-hint is larger than that of ASQA because identifying ambiguous aspects is challenging even for humans in many cases, and providing a generic hint helps LMs to stay on topic.</p>
<p><strong>Thorough comparisons with baselines.</strong> The performance of all baselines on 2WikiMultihopQA are reported in Table 1. FLARE outperforms all baselines by a large margin, which confirms that forward-looking active retrieval is highly effective. Most multi-time retrieval augmented approaches outperform single-time retrieval but with different margins. The improvement of retrieving using the previous sentence is relatively small which we hypothesize is mainly because the previous sentence often describes entities or relations different from those in the next sentence in 2WikiMultihopQA. While the previous-window approach might use the first half of a sentence to retrieve information potentially helpful for generating the second half. Among all baselines, the question decomposition approach (Press et al., 2022) achieves the best performance, which is not surprising since the in-context exemplars manually annotated with decomposed sub-questions (Prompt D.2) guide LMs to generate sub-questions that align with the topic/intent of future generations. FLARE outperforms this baseline, indicating that manual exemplar annotation is not necessary for effective future-aware retrieval. The gap between FLARE_{instruct} and question decomposition is large, indicating that teaching LMs to generate search queries using task-generic retrieval instructions and exemplars is challenging.</p>
<p>We report all metrics for the other datasets in Table 2. FLARE outperforms baselines with respect to all metrics. Retrieval using the previ-</p>
<table>
<thead>
<tr>
<th>Methods</th>
<th>EM</th>
<th>F1</th>
<th>Prec.</th>
<th>Rec.</th>
</tr>
</thead>
<tbody>
<tr>
<td>No retrieval</td>
<td>28.2</td>
<td>36.8</td>
<td>36.5</td>
<td>38.6</td>
</tr>
<tr>
<td>Single-time retrieval</td>
<td>39.4</td>
<td>48.8</td>
<td>48.6</td>
<td>51.5</td>
</tr>
<tr>
<td><em>Multi-time retrieval</em></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Previous-window</td>
<td>43.2</td>
<td>52.3</td>
<td>51.7</td>
<td>54.5</td>
</tr>
<tr>
<td>Previous-sentence</td>
<td>39.0</td>
<td>49.2</td>
<td>48.9</td>
<td>51.8</td>
</tr>
<tr>
<td>Question decomposition</td>
<td>47.8</td>
<td>56.4</td>
<td>56.1</td>
<td>58.6</td>
</tr>
<tr>
<td>FLARE_{instruct} (ours)</td>
<td>42.4</td>
<td>49.8</td>
<td>49.1</td>
<td>52.5</td>
</tr>
<tr>
<td>FLARE_{detect} (ours)</td>
<td>51.0</td>
<td>59.7</td>
<td>59.1</td>
<td>62.6</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Datasets</th>
<th>StrategyQA</th>
<th>ASQA</th>
<th></th>
<th></th>
<th>ASQA-hint</th>
<th></th>
<th></th>
<th>WikiAsp</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Metrics</td>
<td>EM</td>
<td>EM</td>
<td>D-F1</td>
<td>R-L</td>
<td>DR</td>
<td>EM</td>
<td>D-F1</td>
<td>R-L</td>
<td>DR</td>
<td>UniEval</td>
</tr>
<tr>
<td>No retrieval</td>
<td>72.9</td>
<td>33.8</td>
<td>24.2</td>
<td>33.3</td>
<td>28.4</td>
<td>40.1</td>
<td>32.5</td>
<td>36.4</td>
<td>34.4</td>
<td>47.1</td>
</tr>
<tr>
<td>Single-time retrieval</td>
<td>68.6</td>
<td>40.0</td>
<td>27.1</td>
<td>34.0</td>
<td>30.4</td>
<td>43.2</td>
<td>34.8</td>
<td>37.4</td>
<td>36.0</td>
<td>52.4</td>
</tr>
<tr>
<td>Multi-time retrieval</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Previous-window</td>
<td>71.2</td>
<td>39.9</td>
<td>27.0</td>
<td>34.3</td>
<td>30.4</td>
<td>43.7</td>
<td>35.7</td>
<td>37.5</td>
<td>36.6</td>
<td>51.8</td>
</tr>
<tr>
<td>Previous-sentence</td>
<td>71.0</td>
<td>39.9</td>
<td>27.9</td>
<td>34.3</td>
<td>30.9</td>
<td>44.7</td>
<td>35.9</td>
<td>37.5</td>
<td>36.7</td>
<td>52.6</td>
</tr>
<tr>
<td>FLARE (ours)</td>
<td>77.3</td>
<td>41.3</td>
<td>28.2</td>
<td>34.3</td>
<td>31.1</td>
<td>46.2</td>
<td>36.7</td>
<td>37.7</td>
<td>37.2</td>
<td>53.4</td>
</tr>
</tbody>
</table>
<p>Table 2: Comparison between FLARE and baselines on StrategyQA, ASQA, ASQA-hint, and WikiAsp. D-F1 is Disambig-F1, R-L is ROUGE-L, and E-F1 is named entity-based F1.</p>
<table>
<thead>
<tr>
<th></th>
<th>2WikiMultihopQA</th>
<th></th>
<th></th>
<th>ASQA-hint</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>EM</td>
<td>F1</td>
<td>Prec.</td>
<td>Rec.</td>
<td>EM</td>
<td>D-F1</td>
</tr>
<tr>
<td>Previous</td>
<td>39.0</td>
<td>49.2</td>
<td>48.9</td>
<td>51.8</td>
<td>42.5</td>
<td>34.1</td>
</tr>
<tr>
<td>Next</td>
<td>48.8</td>
<td>57.6</td>
<td>57.1</td>
<td>60.5</td>
<td>45.9</td>
<td>35.7</td>
</tr>
</tbody>
</table>
<p>Table 3: A head-to-head comparison between using the previous sentence and the next sentence for retrieval.</p>
<table>
<thead>
<tr>
<th>#Tokens</th>
<th>EM</th>
<th>F1</th>
<th>Prec.</th>
<th>Rec.</th>
</tr>
</thead>
<tbody>
<tr>
<td>16</td>
<td>43.2</td>
<td>52.3</td>
<td>51.7</td>
<td>54.5</td>
</tr>
<tr>
<td>32</td>
<td>43.6</td>
<td>52.4</td>
<td>52.0</td>
<td>55.0</td>
</tr>
<tr>
<td>48</td>
<td>40.0</td>
<td>49.3</td>
<td>49.0</td>
<td>52.0</td>
</tr>
<tr>
<td>All</td>
<td>39.0</td>
<td>48.5</td>
<td>48.2</td>
<td>51.1</td>
</tr>
</tbody>
</table>
<p>Table 4: Previous-window approaches using different numbers of tokens as queries.</p>
<p>ous window underperforms single-time retrieval on ASQA, which we hypothesize is because the previous window does not accurately reflect future intent. Since we focus on evaluating factuality, metrics with an emphasis on factual content (such as EM, Disambig-F1, UniEval) are more reliable than metrics computed over all tokens (ROUGE-L).</p>
<h3>6.2 Ablation Study</h3>
<p>Importance of forward-looking retrieval. We first validate that forward-looking retrieval is more effective than past-context-based retrieval. We run ablation experiments on 2WikiMultihopQA and ASQA-hint comparing retrieval using the previous versus the next sentence. Specifically, both methods retrieve every sentence and directly use the complete previous/next sentence as queries. As shown in Table 3, using the next sentence to retrieve is clearly better than using the previous sentence, confirming our hypothesis.</p>
<p>We also run previous-window approaches using different numbers of past tokens as queries. As shown in Table 4, using too many tokens (&gt; 32) in</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 5: Performance (EM) of FLARE with respect to the percentage of steps/sentences with retrieval on 2WikiMultihopQA and StrategyQA.</p>
<p>the past hurts the performance, further confirming our hypothesis that previous context might not be relevant to intent of future generations.</p>
<p>Importance of active retrieval. Next, we investigate how active retrieval threshold $\theta$ affects performance. To alter our method from not retrieving to retrieving every sentence, we adjust the confidence threshold $\theta$ that determines when to trigger retrieval from 0 to 1. We then calculate the proportion of steps/sentences where retrieval is activated, and present the performance based on it. As shown in Figure 5, on 2WikiMultihopQA, the performance plateaus when the retrieval percentage exceeds 60%, indicating that retrieval when LMs are confident is not necessary. On StrategyQA, the performance drops when the retrieval percentage exceeds 50%, indicating that unnecessary retrieval can introduce noise and impede the original generation process. We found triggering retrieval for 40%-80% of sentences usually leads to a good performance across tasks/datasets.</p>
<p>Effectiveness of different query formulation methods We study implicit query formation by masking and explicit query formulation through question generation. In Table 5, we compare the performance of FLARE with different masking</p>
<table>
<thead>
<tr>
<th>$\beta$</th>
<th>EM</th>
<th>$\mathbf{F}_{1}$</th>
<th>Prec.</th>
<th>Rec.</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.0</td>
<td>0.488</td>
<td>0.576</td>
<td>0.571</td>
<td>0.605</td>
</tr>
<tr>
<td>0.2</td>
<td>0.498</td>
<td>0.588</td>
<td>0.582</td>
<td>0.616</td>
</tr>
<tr>
<td>0.4</td>
<td>0.510</td>
<td>0.597</td>
<td>0.591</td>
<td>0.627</td>
</tr>
<tr>
<td>0.6</td>
<td>0.506</td>
<td>0.593</td>
<td>0.586</td>
<td>0.622</td>
</tr>
</tbody>
</table>
<p>Table 5: Performance of FLARE with respect to the masking threshold $\beta$ on 2WikiMultihopQA.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">ASQA-hint</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">WikiAsp</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">D-F</td>
<td style="text-align: center;">R-L</td>
<td style="text-align: center;">DR</td>
<td style="text-align: center;">UniEval</td>
<td style="text-align: center;">E-F</td>
</tr>
<tr>
<td style="text-align: center;">Implicit</td>
<td style="text-align: center;">45.7</td>
<td style="text-align: center;">36.9</td>
<td style="text-align: center;">37.7</td>
<td style="text-align: center;">37.3</td>
<td style="text-align: center;">53.4</td>
<td style="text-align: center;">18.8</td>
</tr>
<tr>
<td style="text-align: center;">Explicit</td>
<td style="text-align: center;">46.2</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">37.7</td>
<td style="text-align: center;">37.2</td>
<td style="text-align: center;">53.4</td>
<td style="text-align: center;">18.9</td>
</tr>
</tbody>
</table>
<p>Table 6: A comparison between implicit and explicit query formulation methods in FLARE.
thresholds $\beta$. Retrieving directly with the complete sentence $(\beta=0)$ is worse than masking tokens with low probabilities, confirming our hypothesis that low-confidence erroneous tokens can distract retrievers. We compare implicit and explicit query formulation methods in Table 6. Performances of both methods are similar, indicating that both methods can effectively reflect information needs.</p>
<h2>7 Related Work</h2>
<p>We refer to subsection 2.2 and section 4 for extensively discussion on single-time and multi-time retrieval augmented LMs, which is the most relevant area to this paper.</p>
<p>Iterative and adaptive retrieval Iterative retrieval and refinement has been studied in both text and code generation tasks (Peng et al., 2023; Zhang et al., 2023; Zemlyanskiy et al., 2022; Yu et al., 2023). FLARE differs from these methods in the granularity of generation and retrieval strategies. Adaptive retrieval has been studied in single-time retrieval scenarios based on either question popularity or generation probabilities (Mallen et al., 2022; Li et al., 2023), while we focus on long-form generation requiring active information access.</p>
<p>Browser-enhanced LMs WebGPT (Nakano et al., 2021) and WebCPM (Qin et al., 2023) train LMs to interact with browser to enhance factuality using reinforcement learning or supervised training where multiple queries can be triggered before generation. FLARE is built on text-based retrievers but can be combined with a browser to potentially improve retrieval quality.</p>
<h2>8 Conclusion</h2>
<p>To aid long-form generation with retrieval augmentation, we propose an active retrieval augmented generation framework that decides when and what to retrieve during generation. We implement this framework with forward-looking active retrieval that iteratively uses the upcoming sentence to retrieve relevant information if it contains lowconfidence tokens and regenerates the next sentence. Experimental results on 4 tasks/datasets demonstrate the effectiveness of our methods. Future directions include better strategies for active retrieval and developing efficient LM architectures for active information integration.</p>
<h2>9 Limitations</h2>
<p>We also conduct experiments on Wizard of Wikipedia (Dinan et al., 2019) and ELI5 (Fan et al., 2019), and found that FLARE did not provide significant gains. Wizard of Wikipedia is a knowledgeintensive dialogue generation dataset where the output is relatively short ( $\sim 20$ tokens on average) so retrieving multiple disparate pieces of information might not be necessary. ELI5 (Fan et al., 2019) is a long-form QA dataset requiring in-depth answers to open-ended questions. Due to issues mentioned in Krishna et al. (2021) such as difficulties of grounding generation in retrieval and evaluation, both single-time retrieval and FLARE did not provide significant gains over not using retrieval. From an engineering perspective, interleaving generation and retrieval with a naive implementation increases both overheads and the cost of generation. LMs need to be activated multiple times (once for each retrieval) and a caching-free implementation also requires recomputing the previous activation each time after retrieval. This issue can be potentially alleviated with special architectural designs that encode the retrieved documents $\mathcal{D}<em t="t">{\boldsymbol{q}</em>\right)$ independently.}}$ and the input/generation $\left(\boldsymbol{x} / \boldsymbol{y}_{&lt;t</p>
<h2>Acknowledgements</h2>
<p>This work was supported in part by a grant from the Singapore Defence Science and Technology Agency and the IBM PhD Fellowship. We thank Chunting Zhou, Amanda Bertsch, Uri Alon, Hiroaki Hayashi, Harsh Trivedi, Patrick Lewis, Timo Schick, Kaixin Ma, Shuyan Zhou, and Songwei Ge for their insightful discussions and help with the experiments.</p>
<h2>References</h2>
<p>Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre. 2022. Improving language models by retrieving from trillions of tokens. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 2206-2240. PMLR.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading wikipedia to answer opendomain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 August 4, Volume 1: Long Papers, pages 1870-1879. Association for Computational Linguistics.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. CoRR, abs/2204.02311.</p>
<p>Nachshon Cohen, Oren Kalinsky, Yftah Ziser, and Alessandro Moschitti. 2021. Wikisum: Coherent summarization dataset for efficient human-evaluation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 2: Short Papers), Virtual Event, August 1-6, 2021, pages 212219. Association for Computational Linguistics.</p>
<p>Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. 2019. Wizard of wikipedia: Knowledge-powered conversational agents. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.</p>
<p>Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019. ELI5: long form question answering. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 3558-3567. Association for Computational Linguistics.</p>
<p>Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2022. Precise zero-shot dense retrieval without relevance labels. CoRR, abs/2212.10496.</p>
<p>Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346361.</p>
<p>John M. Giorgi, Luca Soldaini, Bo Wang, Gary D. Bader, Kyle Lo, Lucy Lu Wang, and Arman Cohan. 2022. Exploring the challenges of open domain multi-document summarization. CoRR, abs/2212.10526.</p>
<p>Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. REALM: retrievalaugmented language model pre-training. CoRR, abs/2002.08909.</p>
<p>Hiroaki Hayashi, Prashant Budania, Peng Wang, Chris Ackerson, Raj Neervannan, and Graham Neubig. 2021. Wikiasp: A dataset for multi-domain aspectbased summarization. Trans. Assoc. Comput. Linguistics, 9:211-225.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. CoRR, abs/2009.03300.</p>
<p>Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing A multi-hop QA dataset for comprehensive evaluation of reasoning steps. In Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13,</p>
<p>2020, pages 6609-6625. International Committee on Computational Linguistics.</p>
<p>Gautier Izacard and Edouard Grave. 2021. Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021, pages 874880. Association for Computational Linguistics.</p>
<p>Gautier Izacard, Patrick S. H. Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2022. Few-shot learning with retrieval augmented language models. CoRR, abs/2208.03299.</p>
<p>Zhengbao Jiang, Jun Araki, Haibo Ding, and Graham Neubig. 2021. How can we know When language models know? on the calibration of language models for question answering. Trans. Assoc. Comput. Linguistics, 9:962-977.</p>
<p>Zhengbao Jiang, Luyu Gao, Jun Araki, Haibo Ding, Zhiruo Wang, Jamie Callan, and Graham Neubig. 2022. Retrieval as attention: End-to-end learning of retrieval and reading within a single transformer. CoRR, abs/2212.02027.</p>
<p>Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language models know. Trans. Assoc. Comput. Linguistics, 8:423-438.</p>
<p>Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers, pages 1601-1611. Association for Computational Linguistics.</p>
<p>Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. 2022. Language models (mostly) know what they know. CoRR, abs/2207.05221.</p>
<p>Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 6769-6781. Association for Computational Linguistics.</p>
<p>Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020. Generalization through memorization: Nearest neighbor language models. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. 2022. Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive NLP. CoRR, abs/2212.14024.</p>
<p>Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. 2022. Decomposed prompting: A modular approach for solving complex tasks. CoRR, abs/2210.02406.</p>
<p>Kalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021. Hurdles to progress in long-form question answering. In North American Association for Computational Linguistics.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: a benchmark for question answering research. Trans. Assoc. Comput. Linguistics, 7:452466.</p>
<p>Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. 2022. Internetaugmented language models through few-shot prompting for open-domain question answering. CoRR, abs/2203.05115.</p>
<p>Haejun Lee, Akhil Kedia, Jongwon Lee, Ashwin Paranjape, Christopher D. Manning, and Kyoung-Gu Woo. 2021. You only need one model for open-domain question answering. CoRR, abs/2112.07381.</p>
<p>Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Junyi Li, Tianyi Tang, Wayne Xin Zhao, Jingyuan Wang, Jian-Yun Nie, and Ji-Rong Wen. 2023. The web can be your oyster for improving large language models. CoRR, abs/2305.10998.</p>
<p>Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics.</p>
<p>Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Comput. Surv., 55(9):195:1-195:35.</p>
<p>Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, and Daniel Khashabi. 2022. When not to trust language models: Investigating effectiveness and limitations of parametric and nonparametric memories. CoRR, abs/2212.10511.</p>
<p>Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, and Weizhu Chen. 2021. Generation-augmented retrieval for opendomain question answering. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 4089-4100. Association for Computational Linguistics.</p>
<p>Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906-1919, Online. Association for Computational Linguistics.</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. 2021. Webgpt: Browserassisted question-answering with human feedback. CoRR, abs/2112.09332.</p>
<p>OpenAI. 2023. GPT-4 technical report. CoRR, abs/2303.08774.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. CoRR, abs/2203.02155.</p>
<p>Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, and Jianfeng Gao. 2023. Check your facts and try again: Improving large language models with external knowledge and automated feedback. CoRR, abs/2302.12813.</p>
<p>Fabio Petroni, Tim Rockt√§schel, Sebastian Riedel, Patrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander H. Miller. 2019. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing,</p>
<p>EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 2463-2473. Association for Computational Linguistics.</p>
<p>Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. 2022. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350.</p>
<p>Hongjing Qian, Yutao Zhu, Zhicheng Dou, Haoqi Gu, Xinyu Zhang, Zheng Liu, Ruofei Lai, Zhao Cao, Jian-Yun Nie, and Ji-Rong Wen. 2023. Webbrain: Learning to generate factually correct articles for queries by grounding on large web corpus. CoRR, abs/2304.04358.</p>
<p>Yujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao Liang, Kunlun Zhu, Yankai Lin, Xu Han, Ning Ding, Huadong Wang, Ruobing Xie, Fanchao Qi, Zhiyuan Liu, Maosong Sun, and Jie Zhou. 2023. Webcpm: Interactive web search for chinese long-form question answering. CoRR, abs/2305.06849.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI Blog, 1(8).</p>
<p>Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-context retrieval-augmented language models. arXiv preprint arXiv:2302.00083.</p>
<p>Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the parameters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 5418-5426. Association for Computational Linguistics.</p>
<p>Stephen E. Robertson and Hugo Zaragoza. 2009. The probabilistic relevance framework: BM25 and beyond. Found. Trends Inf. Retr., 3(4):333-389.</p>
<p>Devendra Singh Sachan, Siva Reddy, William L. Hamilton, Chris Dyer, and Dani Yogatama. 2021. End-toend training of multi-document reader and retriever for open-domain question answering. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 25968-25981.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dess√¨, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools.</p>
<p>Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023. REPLUG: retrieval-augmented black-box language models. CoRR, abs/2301.12652.</p>
<p>Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and MingWei Chang. 2022. ASQA: factoid questions meet long-form answers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 8273-8288. Association for Computational Linguistics.</p>
<p>Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. 2022. Recitation-augmented language models. CoRR, abs/2210.01296.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, Aur√©lien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971.</p>
<p>Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. CoRR, abs/2212.10509.</p>
<p>Neeraj Varshney, Man Luo, and Chitta Baral. 2022. Can open-domain QA reader utilize external knowledge efficiently like humans? CoRR, abs/2211.12707.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, and Denny Zhou. 2022. Selfconsistency improves chain of thought reasoning in language models. CoRR, abs/2203.11171.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. CoRR, abs/2201.11903.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. CoRR, abs/2210.03629.</p>
<p>Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. 2022. Generate rather than retrieve: Large language models are strong context generators. CoRR, abs/2209.10063.</p>
<p>Wenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng Jiang, and Ashish Sabharwal. 2023. Improving language models via plug-and-play retrieval feedback. CoRR, abs/2305.14002.</p>
<p>Yury Zemlyanskiy, Michiel de Jong, Joshua Ainslie, Panupong Pasupat, Peter Shaw, Linlu Qiu, Sumit Sanghai, and Fei Sha. 2022. Generate-and-retrieve: Use your predictions to improve retrieval for semantic parsing. In Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022, Gyeongju, Republic of Korea, October 12-17, 2022, pages 4946-4951. International Committee on Computational Linguistics.</p>
<p>Fengji Zhang, Bei Chen, Yue Zhang, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen. 2023. Repocoder: Repository-level code completion through iterative retrieval and generation. CoRR, abs/2303.12570.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. Opt: Open pre-trained transformer language models. ArXiv, abs/2205.01068.</p>
<p>Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A survey of large language models. CoRR, abs/2303.18223.</p>
<p>Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and Jiawei Han. 2022. Towards a unified multidimensional evaluator for text generation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 2023-2038. Association for Computational Linguistics.</p>
<p>Chunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab, Francisco Guzm√°n, Luke Zettlemoyer, and Marjan Ghazvininejad. 2021. Detecting hallucinated content in conditional neural sequence generation. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1393-1404, Online. Association for Computational Linguistics.</p>
<h2>A FLARE Implementation Details</h2>
<p>$\mathbf{F L A R E}_{\text {instruct }}$ implementation details We found that LMs can effectively combine retrieval and downstream task-related skills and generate meaningful search queries while performing the task. However, there are two issues: (1) LMs tend to generate fewer search queries than necessary. (2) Generating excessive search queries can disrupt answer generation and adversely affect performance. We address these issues using two methods respectively. First, we increase the logit of the token "[" by 2.0 to improve the chances of LMs generating "[Search(query)]". Second, whenever LMs generate a search query, we use it to retrieve relevant information, promptly remove it from the generation, and generate the next few tokens while forbidding "[" by adding a large negative value to the logit of "[".</p>
<p>The initial query of FLARE. FLARE starts with the user input $\boldsymbol{x}$ as the initial query to retrieve documents to generate the first sentence $\hat{\boldsymbol{s}}<em _boldsymbol_x="\boldsymbol{x">{1}=\operatorname{LM}\left(\left[\mathcal{D}</em>\right]\right)$ to bootstrap the iterative generation process. For the following steps, the temporary forward-looking sentence is generated without retrieved documents.}}, \boldsymbol{x</p>
<p>Sentence tokenization. For each step $t$, we generate 64 tokens which are longer than most sentences, and use NLTK sentence tokenizer to extract the first sentence and discard the rest.</p>
<p>Efficiency As shown in subsection 6.2, on average retrieval is triggered for $30 \% \sim 60 \%$ of sentences depending on downstream tasks. In comparision, KNN-LM (Khandelwal et al., 2020) retrieves every token, RETRO or IC-RALM (Borgeaud et al., 2022; Ram et al., 2023) retrievers every $4 \sim 32$ tokens, and IRCoT (Trivedi et al., 2022) retrieves every sentence. Compared to single-time retrieval, however, interleaving retrieval and generation with a naive implementation indeed increases overheads, which we discuss in the limitation section (section 9).</p>
<h2>B Datasets and Settings</h2>
<p>Datasets, metrics, and experimental settings are summarized in Table 7.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Multihop QA For "Why did the founder of Versus die?", the output we aim to generate is "The founder of Versus was Gianni Versace. Gianni Versace was shot and killed on the steps of his Miami Beach mansion on July 15, 1997. So the answer is shot." We use 8 exemplars from Trivedi et al. (2022) listed in Prompt D. 4 for in-context learning, BM25 as the retriever, and Wikipedia articles as the retrieval corpus. Similar to the observation in Trivedi et al. (2022), we found incorporating retrieval results for exemplars improves the performance, we use the input $\boldsymbol{x}$ of each exemplar to retrieve several documents and then add them using the format in Prompt D.1. We found increasing the number of retrieval documents often increases performance. Therefore, we use the maximum number of documents that can fit within the input length limit of text-davinci-003, which is 2 for 2WikiMultihopQA.</p>
<p>Commonsense Reasoning For "Would a pear sink in water?", the output we aim to generate is "The density of a pear is about $0.6 \mathrm{~g} / \mathrm{cm}^{3}$, which is less than water. Objects less dense than water float. Thus, a pear would float. So the final answer is no." We use 6 exemplars from Wei et al. (2022) listed in Prompt D.5, BM25 on the Wikipedia corpus, and 3 retrieved documents to run experiments.</p>
<p>Long-form QA For "Where do the Philadelphia Eagles play their home games?", the output we aim to generate is "We need to consider the different possible locations or venues that could be considered the home field of the Philadelphia Eagles. These include the city, the sports complex, or the stadium. Therefore, this question has 3 interpretations and the answers are: (1) The city is Philadelphia. (2) The sports complex is the South Philadelphia Sports Complex. (3) The stadium is the Lincoln Financial Field stadium." For both the original setting (ASQA) and the setting with hints (ASQA-hint), we manually annotate 8 exemplars (Prompt D. 6 and D.8), use BM25 on the Wikipedia corpus, and 3 retrieved documents to run experiments.</p>
<p>Open-domain Summarization The original WikiAsp dataset is designed for multi-document summarization and provides a list of references to systems. We converted it into the open-domain setting by removing the associated references and instead gathering information from the open web. For "Generate a summary about Echo School (Ore-</p>
<p>gon) including the following aspects: academics, history.", the output we aim to generate is "# Academics. In 2008, $91 \%$ of the school's seniors received their high school diploma... # History. The class of 2008 was the 100th class in the school's history." where # is used to indicate aspects. We manually annotate 4 exemplars (Prompt D.10), and use the Bing search engine to retrieve 5 documents from the open web. To avoid leaking, we exclude several Wikipedia-related domains listed in Table 8 from Bing's search results.</p>
<h1>C Hyperparameters</h1>
<p>Hyperparameters of FLARE on different datasets are listed in Table 9.</p>
<h2>D Prompts and Few-shot exemplars</h2>
<p>The prompt used to linearize multiple documents is shown in Prompt D.1. The prompt used in selfask (Press et al., 2022) is shown in Prompt D.2. Prompts and exemplars of different tasks/datasets are shown in Prompt D.3, D.4, D.5, D.6, D.8, and D.10, respectively.</p>
<h2>Prompt D.1: document formatting</h2>
<p>Search results:
[1] Document 1
[2] Document 2
The user input $x$</p>
<h2>Prompt D.2: multihop QA with self-ask</h2>
<p>Question: Who lived longer, Theodor Haecker or Harry Vaughan Watkins?
Are follow up questions needed here: Yes.
Follow up: How old was Theodor Haecker when he died?
Intermediate answer: Theodor Haecker was 65 years old when he died.
Follow up: How old was Harry Vaughan Watkins when he died?
Intermediate answer: Harry Vaughan Watkins was 69 years old when he died.
So the final answer is: Harry Vaughan Watkins.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Settings</th>
<th style="text-align: center;">2WikiMultihopQA <br> (Ho et al., 2020)</th>
<th style="text-align: center;">StrategyQA <br> (Geva et al., 2021)</th>
<th style="text-align: center;">ASQA <br> (Stelmakh et al., 2022)</th>
<th style="text-align: center;">WikiAsp <br> (Hayashi et al., 2021)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Dataset statistics</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Task <br> #Examples</td>
<td style="text-align: center;">multihop QA <br> 500</td>
<td style="text-align: center;">commonsense QA <br> 229</td>
<td style="text-align: center;">long-form QA <br> 500</td>
<td style="text-align: center;">open-domain summarization <br> 500</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Evaluation settings</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Metrics</td>
<td style="text-align: center;">EM, $\mathrm{F}_{1}$, Prec., Rec.</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">EM, Disambig- $\mathrm{F}_{1}$, ROUGE, DR</td>
<td style="text-align: center;">UniEval, entity- $\mathrm{F}_{1}$, ROUGE</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Retrieval settings</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Corpus <br> Retriever <br> Top-k</td>
<td style="text-align: center;">Wikipedia <br> BM25 <br> 2</td>
<td style="text-align: center;">Wikipedia <br> BM25 <br> 3</td>
<td style="text-align: center;">Wikipedia <br> BM25 <br> 3</td>
<td style="text-align: center;">open web <br> Bing <br> 5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Prompt format</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">#Exemplars <br> Ret. for exemplars</td>
<td style="text-align: center;">8 <br> $\checkmark$</td>
<td style="text-align: center;">6 <br> $\boldsymbol{x}$</td>
<td style="text-align: center;">8 <br> $\boldsymbol{x}$</td>
<td style="text-align: center;">4 <br> $\boldsymbol{x}$</td>
</tr>
</tbody>
</table>
<p>Table 7: Dataset statistics and experimental settings of different tasks.
wikipedia.org, wikiwand.com, wiki2.org, wikimedia.org
Table 8: Wikipedia-related domains excluded from Bing's search results.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">$\theta$</th>
<th style="text-align: center;">$\beta$</th>
<th style="text-align: center;">Query formulation</th>
<th style="text-align: center;">Combine single- \&amp; multi-time retrieval</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">2WikiMultihopQA</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">implicit</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">StrategyQA</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">implicit</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">ASQA \&amp; ASQA-hint</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">explicit</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">WikiAsp</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">explicit</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>Table 9: Hyperparameters of FLARE on different datasets.</p>
<h1>Prompt D.3: retrieval instructions for 2WikiMultilogQA</h1>
<p>Skill 1. Use the Search API to look up relevant information by writing "[Search(term)]" where "term" is the search term you want to look up. For example:</p>
<p>Question: But what are the risks during production of nanomaterials?
Answer (with Search): [Search(nanomaterial production risks)] Some nanomaterials may give rise to various kinds of lung damage.</p>
<p>Question: The colors on the flag of Ghana have the following meanings.
Answer (with Search): Red is for [Search(Ghana flag red meaning)] the blood of martyrs, green for forests, and gold for mineral wealth.</p>
<p>Question: Metformin is the first-line drug for what?
Answer (with Search): [Search(Metformin first-line drug)] patients with type 2 diabetes and obesity.
Skill 2. Answer questions by thinking step-by-step. First, write out the reasoning steps, then draw the conclusion. For example:</p>
<p>Question: When did the director of film Hypocrite (Film) die?
Answer (with step-by-step): The film Hypocrite was directed by Miguel Morayta. Miguel Morayta died on 19 June 2013. So the answer is 19 June 2013.</p>
<p>Question: Are both Kurram Garhi and Trojkrsti located in the same country?
Answer (with step-by-step): Kurram Garhi is located in the country of Pakistan. Trojkrsti is located in the country of Republic of Macedonia. Thus, they are not in the same country. So the answer is no.</p>
<p>Question: Do director of film Coolie No. 1 (1995 Film) and director of film The Sensational Trial have the same nationality?
Answer (with step-by-step): Coolie No. 1 (1995 film) was directed by David Dhawan. The Sensational Trial was directed by Karl Freund. David Dhawan's nationality is India. Karl Freund's nationality is Germany. Thus, they do not have the same nationality. So the answer is no.</p>
<p>Question: Who is Boraqchin (Wife Of √ñgedei)'s father-in-law?
Answer (with step-by-step): Boraqchin is married to √ñgedei Khan. √ñgedei Khan's father is Genghis Khan. Thus, Boraqchin's father-in-law is Genghis Khan. So the answer is Genghis Khan.</p>
<p>Question: Who was born first out of Martin Hodge and Ivania Martinich?
Answer (with step-by-step): Martin Hodge was born on 4 February 1959. Ivania Martinich was born on 25 July 1995. Thus, Martin Hodge was born first. So the answer is Martin Hodge.</p>
<p>Question: When did the director of film Laughter In Hell die?
Answer (with step-by-step): The film Laughter In Hell was directed by Edward L. Cahn. Edward L. Cahn died on August 25, 1963. So the answer is August 25, 1963.</p>
<p>Question: Which film has the director died later, The Gal Who Took the West or Twenty Plus Two?
Answer (with step-by-step): The film Twenty Plus Two was directed by Joseph M. Newman. The Gal Who Took the West was directed by Frederick de Cordova. Joseph M. Newman died on January 23, 2006. Fred de Cordova died on September 15, 2001. Thus, the person to die later from the two is Twenty Plus Two. So the answer is Twenty Plus Two.</p>
<p>Question: Who is the grandchild of Krishna Shah (Nepalese Royal)?
Answer (with step-by-step): Krishna Shah has a child named Rudra Shah. Rudra Shah has a child named Prithvipati Shah. Thus, Krishna Shah has a grandchild named Prithvipati Shah. So the answer is Prithvipati Shah.</p>
<p>Now, combine the aforementioned two skills. First, write out the reasoning steps, then draw the conclusion, where the reasoning steps should also utilize the Search API "[Search(term)]" whenever possible.</p>
<p>Question: Where did Minbyauk Thihapate's wife die?
Answer (with step-by-step \&amp; Search):</p>
<h1>Prompt D.4: exemplars of 2WikiMultihopQA</h1>
<p>Question: When did the director of film Hypocrite (Film) die?
Answer: The film Hypocrite was directed by Miguel Morayta. Miguel Morayta died on 19 June 2013. So the answer is 19 June 2013.</p>
<p>Question: Are both Kurram Garhi and Trojkrsti located in the same country?
Answer: Kurram Garhi is located in the country of Pakistan. Trojkrsti is located in the country of Republic of Macedonia. Thus, they are not in the same country. So the answer is no.</p>
<p>Question: Do director of film Coolie No. 1 (1995 Film) and director of film The Sensational Trial have the same nationality?
Answer: Coolie No. 1 (1995 film) was directed by David Dhawan. The Sensational Trial was directed by Karl Freund. David Dhawan's nationality is India. Karl Freund's nationality is Germany. Thus, they do not have the same nationality. So the answer is no.</p>
<p>Question: Who is Boraqchin (Wife Of √ñgedei)'s father-in-law?
Answer: Boraqchin is married to √ñgedei Khan. √ñgedei Khan's father is Genghis Khan. Thus, Boraqchin's father-in-law is Genghis Khan. So the answer is Genghis Khan.</p>
<p>Question: Who was born first out of Martin Hodge and Ivania Martinich?
Answer: Martin Hodge was born on 4 February 1959. Ivania Martinich was born on 25 July 1995. Thus, Martin Hodge was born first. So the answer is Martin Hodge.</p>
<p>Question: When did the director of film Laughter In Hell die?
Answer: The film Laughter In Hell was directed by Edward L. Cahn. Edward L. Cahn died on August 25, 1963. So the answer is August 25, 1963.</p>
<p>Question: Which film has the director died later, The Gal Who Took the West or Twenty Plus Two?
Answer: The film Twenty Plus Two was directed by Joseph M. Newman. The Gal Who Took the West was directed by Frederick de Cordova. Joseph M. Newman died on January 23, 2006. Fred de Cordova died on September 15, 2001. Thus, the person to die later from the two is Twenty Plus Two. So the answer is Twenty Plus Two.</p>
<p>Question: Who is the grandchild of Krishna Shah (Nepalese Royal)?
Answer: Krishna Shah has a child named Rudra Shah. Rudra Shah has a child named Prithvipati Shah. Thus, Krishna Shah has a grandchild named Prithvipati Shah. So the answer is Prithvipati Shah.</p>
<p>Question: Which country the director of film Citizen Mavzik is from?
Answer:</p>
<h1>Prompt D.5: exemplars of StrategyQA</h1>
<p>Generate a yes or no answer to the following question.
Question: Do hamsters provide food for any animals?
Answer: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals. So the final answer is yes.</p>
<p>Generate a yes or no answer to the following question.
Question: Could Brooke Shields succeed at University of Pennsylvania?
Answer: Brooke Shields went to Princeton University. Princeton University is about as academically rigorous as the University of Pennsylvania. Thus, Brooke Shields could also succeed at the University of Pennsylvania. So the final answer is yes.</p>
<p>Generate a yes or no answer to the following question.
Question: Hydrogen's atomic number squared exceeds number of Spice Girls?
Answer: Hydrogen has an atomic number of 1.1 squared is 1 . There are 5 Spice Girls. Thus, Hydrogen's atomic number squared is less than 5 . So the final answer is no.</p>
<p>Generate a yes or no answer to the following question.
Question: Is it common to see frost during some college commencements?
Answer: College commencement ceremonies can happen in December, May, and June. December is in the winter, so there can be frost. Thus, there could be frost at some commencements. So the final answer is yes.</p>
<p>Generate a yes or no answer to the following question.
Question: Could a llama birth twice during War in Vietnam (1945-46)?
Answer: The War in Vietnam was 6 months. The gestation period for a llama is 11 months, which is more than 6 months. Thus, a llama could not give birth twice during the War in Vietnam. So the final answer is no.</p>
<p>Generate a yes or no answer to the following question.
Question: Would a pear sink in water?
Answer: The density of a pear is about $0.6 \mathrm{~g} / \mathrm{cm}^{3}$, which is less than water. Objects less dense than water float. Thus, a pear would float. So the final answer is no.</p>
<p>Generate a yes or no answer to the following question.
Question: Did Spiderman fight against Falcon in the MCU?
Answer:</p>
<h1>Prompt D.6: exemplars of ASQA</h1>
<p>Given an ambiguous question, figure out its interpretations and answer them one by one.
Question: Who played bonnie in gone with the wind?
Answer: This question is ambiguous in terms of which version or adaptation of Gone with the Wind is being referred to. In order to figure out its interpretations, we need to consider different versions or adaptations of Gone with the Wind. Gone with the Wind has two versions or adaptations: the 1939 film Gone with the Wind or the 2008 musical Gone with the Wind. Therefore, this question has 2 interpretations: (1) Who played Bonnie in the 1939 film Gone with the Wind? (2) Who played Bonnie in the 2008 musical Gone with the Wind? The answers to all interpretations are: (1) The 1939 film Gone with the Wind's character Bonnie was played by Eleanore Cammack "Cammie" King. (2) The 2008 musical Gone with the Wind's character Bonnie was played by Leilah de Meza.</p>
<p>Given an ambiguous question, figure out its interpretations and answer them one by one.
Question: What is the second largest city in the usa?
Answer: This question is ambiguous in terms of the criteria being used to determine the second largest city in the USA. In order to figure out its interpretations, we need to consider different criteria to determine a city's size. City size can be measured by two criteria: population or area. Therefore, this question has 2 interpretations: (1) What is the second largest city in the USA by population? (2) What is the second largest city in the USA by area? The answers to all interpretations are: (1) The second largest city in the USA by population is Los Angeles, California. (2) The second largest city in the USA by area is Juneau, Alaska.</p>
<p>Given an ambiguous question, figure out its interpretations and answer them one by one.
Question: When was bohemian rhapsody released as a single?
Answer: This question is ambiguous in terms of which country's release of the single is being referred to. In order to figure out its interpretations, we need to consider different countries where Bohemian Rhapsody is released. Bohemian Rhapsody was released in the United Kingdom and in the United States on different dates. Therefore, this question has 2 interpretations: (1) When was Bohemian Rhapsody released as a single in the United Kingdom? (2) When was Bohemian Rhapsody released as a single in the United States? The answers to all interpretations are: (1) Bohemian Rhapsody was released as a single in the United Kingdom on 31 October 1975. (2) Bohemian Rhapsody was released as a single in the United States on December 1975.</p>
<p>Given an ambiguous question, figure out its interpretations and answer them one by one.
Question: Where do the philadelphia eagles play their home games?
Answer: This question is ambiguous in terms of which specific location or venue is being referred to. In order to figure out its interpretations, we need to consider the different possible locations or venues that could be considered the home field of the Philadelphia Eagles. These include the city, the sports complex, or the stadium. Therefore, this question has 3 interpretations: (1) What city do the Philadelphia Eagles play their home games? (2) In what sports complex do the Philadelphia Eagles play their home games? (3) What stadium do the Philadelphia Eagles play their home games? The answers to all interpretations are: (1) Philadelphia Eagles play their home games in the city Philadelphia. (2) Philadelphia Eagles play their home games in the South Philadelphia Sports Complex. (3) Philadelphia Eagles play their home games in the Lincoln Financial Field stadium.</p>
<p>Given an ambiguous question, figure out its interpretations and answer them one by one.
Question: When did xbox one come out in australia?
Answer: This question is ambiguous in terms of which specific version of the Xbox One is being referred to. In order to figure out its interpretations, we need to consider the different versions of the Xbox One that have been released. Xbox One has two versions: the Xbox One video game console or the Xbox One X high-end model. Therefore, this question has 2 interpretations: (1) When did the Xbox One release in Australia? (2) When did the Xbox One X release in Australia? The answers to all interpretations are: (1) The Xbox One video game console was released in Australia on November 22, 2013. (2) The Xbox One X video game console was released in Australia on November 7, 2017.</p>
<p>Given an ambiguous question, figure out its interpretations and answer them one by one.
Question: When does the movie summer of 84 come out?
Answer: This question is ambiguous in terms of which release of the movie is being referred to. In order to figure out its interpretations, we need to consider different releases of the movie Summer of ' 84 . The movie Summer of ' 84 is first released at the Sundance Festival before it's released throughout the US. Therefore, this question has 2 interpretations: (1) When did the movie Summer of ' 84 first release at the Sundance Festival? (2) When did the movie Summer of ' 84 first release throughout the US? The answers to all interpretations are: (1) Summer of ' 84 was released at the Sundance Festival on January 22, 2018. (2) Summer of ' 84 was released throughout the US on August 10, 2018.</p>
<h1>Prompt D.7: exemplars of ASQA (cont.)</h1>
<p>Given an ambiguous question, figure out its interpretations and answer them one by one.
Question: What was roy orbison's first number one hit?
Answer: This question is ambiguous in terms of which specific chart or region is being referred to. In order to figure out its interpretations, we need to consider the different charts and regions where Roy Orbison's music was popular. Roy Orbison is popular in both the US Hot 100 and Canada, and the UK and Ireland. Therefore, this question has 2 interpretations: (1) What was Roy Orbison's first number one hit in the US Hot 100 and Canada? (2) What was Roy Orbison's first number one hit in the UK and Ireland? The answers to all interpretations are: (1) Running Scared was the first number one hit for Roy Orbison in the US Hot 100 and Canada. (2) Only the Lonely (Know the Way I Feel) was the first number one hit for Roy Orbison in the UK and Ireland.</p>
<p>Given an ambiguous question, figure out its interpretations and answer them one by one.
Question: What is the criminal's name in the breakfast club?
Answer: This question is ambiguous in terms of which specific name is being referred to - the character's name or the actor's name. In order to figure out its interpretations, we need to consider both possibilities: the character's name or the actor's name. Therefore, this question has 2 interpretations: (1) What is the criminal's character name in The Breakfast Club? (2) What is the the name of the actor who played the criminal in The Breakfast Club? The answers to all interpretations are: (1) John Bender was the name of the criminal's character in The Breakfast Club. (2) Judd Nelson was the actor of the criminal in The Breakfast Club.</p>
<p>Given an ambiguous question, figure out its interpretations and answer them one by one.
Question: How many state parks are there in virginia?
Answer:</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://www.nltk.org/api/nltk.tokenize. PunktSentenceTokenizer.html&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2305 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2305</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2305</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-63.html">extraction-schema-63</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <p><strong>Paper ID:</strong> paper-221139633</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2008.07326v1.pdf" target="_blank">Progressing Towards Responsible AI</a></p>
                <p><strong>Paper Abstract:</strong> The field of Artificial Intelligence (AI) and, in particular, the Machine Learning area, counts on a wide range of performance metrics and benchmark data sets to assess the problem-solving effectiveness of its solutions. However, the appearance of research centres, projects or institutions addressing AI solutions from a multidisciplinary and multi-stakeholder perspective suggests a new approach to assessment comprising ethical guidelines, reports or tools and frameworks to help both academia and business to move towards a responsible conceptualisation of AI. They all highlight the relevance of three key aspects: (i) enhancing cooperation among the different stakeholders involved in the design, deployment and use of AI; (ii) promoting multidisciplinary dialogue, including different domains of expertise in this process; and (iii) fostering public engagement to maximise a trusted relation with new technologies and practitioners. In this paper, we introduce the Observatory on Society and Artificial Intelligence (OSAI), an initiative grew out of the project AI4EU aimed at stimulating reflection on a broad spectrum of issues of AI (ethical, legal, social, economic and cultural). In particular, we describe our work in progress around OSAI and suggest how this and similar initiatives can promote a wider appraisal of progress in AI. This will give us the opportunity to present our vision and our modus operandi to enhance the implementation of these three fundamental dimensions.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2305.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2305.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deep learning (retinal diagnosis)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Clinically Applicable Deep Learning for Diagnosis and Referral in Retinal Disease</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Deep learning applied to retinal medical imaging to perform diagnosis and referral decisions from retinal scans; cited as an example where ML achieved clinically relevant performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Clinically Applicable Deep Learning for Diagnosis and Referral in Retinal Disease</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>medical imaging / ophthalmology (retinal disease diagnosis)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Automated diagnosis and referral decision support from retinal scans to detect disease and determine referral needs.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>2D medical retinal images (labeled clinical images)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Clinical ophthalmology is a mature domain; application of deep learning to this domain is an emerging, high-impact area.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>high - clinical deployment requires interpretability, validation and explainability for safety and trust.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Deep learning (convolutional neural networks / deep neural networks)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Supervised deep learning models trained on labeled retinal image datasets to predict diagnosis/referral; the paper cites the study as a representative successful application but does not provide architecture or training details in the current text.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning / deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable and presented as clinically promising in the cited study; appropriate when labeled clinical imaging data and clinical validation are available.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Qualitatively cited as a successful, clinically applicable use of deep learning in healthcare (used as an example of domains where deep learning 'hit the mark').</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — potential to support diagnostics and referral workflows in healthcare if validated and trusted.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Access to labeled clinical image data and multidisciplinary validation (clinical experts), and modern deep learning methods.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Deep learning can deliver clinically relevant performance in medical imaging when sufficient labeled image data and clinical validation are available, but clinical deployment requires interpretability and trust mechanisms.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2305.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2305.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ImageNet / large-scale object classification</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ImageNet large-scale visual recognition (dataset and associated competitions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A very large labeled image database (ImageNet) and associated competitions enabled training of deep learning models that dramatically reduced object classification error rates on benchmark tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>computer vision / object classification</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Assign class labels to images across many object categories (large-scale multi-class image classification).</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Abundant: ImageNet reportedly contains >14 million images annotated by thousands of crowdworkers and is structured around WordNet; large-scale labeled data publicly available for research and competitions.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Unstructured image data (2D natural images) with class labels; large-scale annotated dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High-dimensional visual data, multi-class classification across many categories; significant intra-class variability and real-world complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Mature as a benchmark-driven subfield; the community uses standardized datasets and competitions to track progress.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>low-medium - black-box deep models acceptable for benchmark performance but interpretability is increasingly valued for deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Deep convolutional neural networks (deep learning)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Supervised training of CNNs on millions of labeled images; evaluated via benchmark competitions (ImageNet LSVRC) and standard performance metrics; specific architectures not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning / deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable — the availability of massive labeled image datasets makes supervised deep learning appropriate for object classification benchmarks and many real-world vision tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Cited benchmark result: classification error of best-performing algorithms in large-scale object classification fell from 0.28 to 0.023 (results referenced from CVPR Workshop 2017).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Deep learning achieved major breakthroughs and dominates vision benchmarks; rapid performance improvements observed on ImageNet-style tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — accelerated progress in computer vision, enabled many downstream applications in industry and research.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Implicit comparison: deep learning substantially outperformed prior methods on large-scale vision benchmarks; detailed comparisons not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Massive labeled dataset (ImageNet), community competitions/benchmarks, and improvements in compute resources (CPUs/GPUs).</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Supervised deep learning scales with large labeled image datasets and compute, yielding substantial performance gains on vision benchmarks, but benchmark-driven progress can mask real-world limitations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2305.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2305.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Google Flu Trends (case study)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The Parable of Google Flu: Traps in Big Data Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An example where large-scale data-driven models (search-query based) were used for influenza surveillance but produced misleading results, illustrating limits of big-data ML without robust domain validation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Parable of Google Flu: Traps in Big Data Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>epidemiology / disease surveillance</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predict population-level influenza incidence from search-query (big-data) signals as an alternative or augmentation to traditional surveillance.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Large volumes of search-query data (big data) were available, but the paper highlights concerns about representativeness and the data's relationship to ground truth surveillance.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Aggregate search-query time-series / counts (temporal aggregated signals).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: non-stationary human behavior, confounders, and sensitivity to shifts in search behavior make modelling challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Epidemiology is a mature scientific domain; application of big-data ML to surveillance is newer and contested.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>high - causal and domain understanding required to ensure robustness and avoid spurious correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Statistical / machine learning models on search-query data (big-data analytics)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Models leveraging correlations between query frequencies and influenza incidence; criticized for over-reliance on correlation and insufficient validation against epidemiological ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised/statistical learning (population-level prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Questionable — initial promise undermined by lack of robustness and representativeness; requires integration with domain expertise and validation.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Cited as an illustrative failure/trap in big-data analysis; demonstrates that big data alone does not guarantee accurate scientific prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Primarily a cautionary impact: motivated better validation practices and highlighted risks of over-reliance on blind data-driven approaches in scientific domains.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Implicitly compared to traditional epidemiological surveillance methods, which proved more reliable when properly validated; no numerical comparison provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Large data volume was insufficient; lack of representativeness and domain-informed validation undermined effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Abundant data without representativeness and causal validation can lead ML systems to fail in scientific prediction tasks; domain expertise and rigorous validation are essential.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2305.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2305.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlphaGo / Deep RL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mastering the game of go without human knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reinforcement learning system using self-play and deep neural networks that achieved superhuman performance in the game of Go, demonstrating the power of deep RL on extremely large sequential decision problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mastering the game of go without human knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>sequential decision-making / game-playing benchmark (Go)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Learn to play Go at superhuman level via reinforcement learning and self-play without relying on human game records.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Training relied on synthetic data generated via self-play rather than labeled human data; effectively abundant training data through simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Structured board states represented as tensors (discrete positions and move sequences).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Extremely large combinatorial search/decision space; high non-linearity and long-range dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Games are established benchmarks in AI; deep RL matured to deliver high performance on such tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>low - for benchmark achievement, black-box policy/value networks are acceptable though interpretability is desirable for broader application.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Deep reinforcement learning with self-play (policy and value networks combined with Monte Carlo Tree Search)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Deep neural networks trained via reinforcement learning and self-play to learn policy and value functions; combined with search (MCTS) for decision-making. The current paper only cites the result and does not detail the architecture or training regimen.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>reinforcement learning / deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable to complex sequential decision tasks when simulation/self-play can generate abundant experience; demonstrated clear success in Go.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Cited as a major breakthrough where deep RL achieved superhuman play in Go; considered a landmark for RL capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for AI research and potential transfer to other sequential decision problems (e.g., planning, control) where simulation is feasible.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Implied to outperform previous Go programs and human players; specific numeric comparisons are not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Self-play data generation, effective neural network architectures, algorithmic innovations (combining RL with search), and large compute resources.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Deep RL combined with simulation/self-play can solve extremely large sequential decision problems when abundant simulated experience and compute are available.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2305.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2305.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Benchmarks & competitions (UCI, GLUE, SQuAD, Papers With Code)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Benchmark datasets and organized competitions (UCI Machine Learning Repository, GLUE, SQuAD, ImageNet LSVRC, Papers With Code, CodaLab)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standardized datasets and competitions used to evaluate and compare ML algorithms across domains (tabular, vision, NLP), driving progress but also raising questions about representativeness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>general machine learning evaluation across domains (vision, NLP, tabular data)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Provide shared tasks and datasets for systematic performance evaluation, comparison and competition among ML methods.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Varied: small-to-moderate labeled tabular datasets (UCI) up to large-scale corpora and image sets (ImageNet, GLUE, SQuAD); generally publicly accessible.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Diverse: structured tabular data, unstructured text (NLP corpora), and image data; labeled per task requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Varies by benchmark — some are low-dimensional or well-structured (UCI), others are high-dimensional and complex (ImageNet, GLUE, SQuAD).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Benchmark-driven subfields are well-established with mature evaluation protocols, but the paper highlights critiques about whether benchmarks capture real-world complexities.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>varies - many benchmarks accept black-box models for winning metrics; some applications require higher interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Various supervised learning algorithms and specialized architectures (CNNs for vision, transformers and other deep models for NLP, classical ML for tabular data)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Researchers train and evaluate a wide range of algorithms on standardized splits and metrics provided by benchmarks and competitions; the paper discusses the role of these benchmarks in progress assessment rather than describing particular models.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning; benchmarking/evaluation frameworks</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Suitable for method development and comparative evaluation, but caution advised about overfitting to benchmarks and lack of real-world representativeness.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Benchmarks accelerate technical progress but can give an illusion of progress if datasets and metrics do not reflect real-world tasks; the paper discusses both benefits and limits.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for driving algorithmic innovations and establishing standards, but risk of misaligned incentives if benchmarks dominate research goals.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>The paper criticizes exclusive reliance on benchmarks and suggests complementing them with broader, ELSEC-aware evaluation criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Public availability of labeled datasets, standardized metrics, community-organized competitions and reproducibility platforms (Papers With Code, CodaLab).</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Benchmarks are powerful drivers of technical progress when ample labeled data exist, but they must be complemented with domain-aware validation to avoid misleading conclusions about real-world effectiveness.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2305.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2305.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fairness & auditing toolkits</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI Fairness 360, Aequitas and end-to-end algorithmic auditing frameworks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Software toolkits and frameworks to detect, report and mitigate discrimination and to support internal algorithmic audits for fairness, accountability and transparency in ML systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ai Fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>algorithmic fairness / socio-technical evaluation of ML systems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Diagnose, report and mitigate bias and disparate impact in ML models and support organizational auditing practices prior to deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Typically applied to structured tabular datasets with labels and protected attributes, but toolkits claim applicability across data types and ML pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High socio-technical complexity involving statistical, legal, ethical and domain-specific considerations; context-dependent fairness definitions.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging: multiple toolkits and frameworks exist but practices are incomplete, scattered and still being operationalized.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>high - transparency and interpretability are required for accountability, auditability and stakeholder trust.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Bias detection and mitigation algorithms and audit procedures (toolkits combining metrics, diagnostics and mitigation methods)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Toolkits provide a set of fairness metrics, statistical tests, reporting utilities and mitigation routines to examine and reduce discriminatory behaviour in trained models; the paper lists examples but does not detail algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>post-hoc analysis / fairness auditing toolkits (software + procedural frameworks)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable across sectors where fairness is a concern; intended to be integrated into development workflows and organizational governance.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Toolkits are useful starting points but the paper notes they are insufficient alone, often lack completeness, and must be combined with governance, diverse teams and contextual analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Potentially high if integrated with organizational culture, multidisciplinary review, audits and regulatory frameworks; otherwise limited.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Positioned as complementary to checklists, canvases, and governance mechanisms; no quantitative comparisons provided.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Integration into existing workflows, organizational commitment, multidisciplinary teams, governance (audits/certification), and context-aware application.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Technical fairness/tooling is necessary but not sufficient; effectiveness depends on organizational processes, diverse expertise, and governance structures to translate tool outputs into responsible outcomes.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Clinically Applicable Deep Learning for Diagnosis and Referral in Retinal Disease <em>(Rating: 2)</em></li>
                <li>Mastering the game of go without human knowledge <em>(Rating: 2)</em></li>
                <li>The Parable of Google Flu: Traps in Big Data Analysis <em>(Rating: 2)</em></li>
                <li>Ai Fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias <em>(Rating: 2)</em></li>
                <li>AI Watch Methodology to Monitor the Evolution of AI Technologies <em>(Rating: 1)</em></li>
                <li>Guidelines for Human-AI Interaction <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2305",
    "paper_id": "paper-221139633",
    "extraction_schema_id": "extraction-schema-63",
    "extracted_data": [
        {
            "name_short": "Deep learning (retinal diagnosis)",
            "name_full": "Clinically Applicable Deep Learning for Diagnosis and Referral in Retinal Disease",
            "brief_description": "Deep learning applied to retinal medical imaging to perform diagnosis and referral decisions from retinal scans; cited as an example where ML achieved clinically relevant performance.",
            "citation_title": "Clinically Applicable Deep Learning for Diagnosis and Referral in Retinal Disease",
            "mention_or_use": "mention",
            "scientific_problem_domain": "medical imaging / ophthalmology (retinal disease diagnosis)",
            "problem_description": "Automated diagnosis and referral decision support from retinal scans to detect disease and determine referral needs.",
            "data_availability": null,
            "data_structure": "2D medical retinal images (labeled clinical images)",
            "problem_complexity": null,
            "domain_maturity": "Clinical ophthalmology is a mature domain; application of deep learning to this domain is an emerging, high-impact area.",
            "mechanistic_understanding_requirements": "high - clinical deployment requires interpretability, validation and explainability for safety and trust.",
            "ai_methodology_name": "Deep learning (convolutional neural networks / deep neural networks)",
            "ai_methodology_description": "Supervised deep learning models trained on labeled retinal image datasets to predict diagnosis/referral; the paper cites the study as a representative successful application but does not provide architecture or training details in the current text.",
            "ai_methodology_category": "supervised learning / deep learning",
            "applicability": "Applicable and presented as clinically promising in the cited study; appropriate when labeled clinical imaging data and clinical validation are available.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Qualitatively cited as a successful, clinically applicable use of deep learning in healthcare (used as an example of domains where deep learning 'hit the mark').",
            "impact_potential": "High — potential to support diagnostics and referral workflows in healthcare if validated and trusted.",
            "comparison_to_alternatives": null,
            "success_factors": "Access to labeled clinical image data and multidisciplinary validation (clinical experts), and modern deep learning methods.",
            "key_insight": "Deep learning can deliver clinically relevant performance in medical imaging when sufficient labeled image data and clinical validation are available, but clinical deployment requires interpretability and trust mechanisms.",
            "uuid": "e2305.0"
        },
        {
            "name_short": "ImageNet / large-scale object classification",
            "name_full": "ImageNet large-scale visual recognition (dataset and associated competitions)",
            "brief_description": "A very large labeled image database (ImageNet) and associated competitions enabled training of deep learning models that dramatically reduced object classification error rates on benchmark tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "computer vision / object classification",
            "problem_description": "Assign class labels to images across many object categories (large-scale multi-class image classification).",
            "data_availability": "Abundant: ImageNet reportedly contains &gt;14 million images annotated by thousands of crowdworkers and is structured around WordNet; large-scale labeled data publicly available for research and competitions.",
            "data_structure": "Unstructured image data (2D natural images) with class labels; large-scale annotated dataset.",
            "problem_complexity": "High-dimensional visual data, multi-class classification across many categories; significant intra-class variability and real-world complexity.",
            "domain_maturity": "Mature as a benchmark-driven subfield; the community uses standardized datasets and competitions to track progress.",
            "mechanistic_understanding_requirements": "low-medium - black-box deep models acceptable for benchmark performance but interpretability is increasingly valued for deployment.",
            "ai_methodology_name": "Deep convolutional neural networks (deep learning)",
            "ai_methodology_description": "Supervised training of CNNs on millions of labeled images; evaluated via benchmark competitions (ImageNet LSVRC) and standard performance metrics; specific architectures not detailed in this paper.",
            "ai_methodology_category": "supervised learning / deep learning",
            "applicability": "Highly applicable — the availability of massive labeled image datasets makes supervised deep learning appropriate for object classification benchmarks and many real-world vision tasks.",
            "effectiveness_quantitative": "Cited benchmark result: classification error of best-performing algorithms in large-scale object classification fell from 0.28 to 0.023 (results referenced from CVPR Workshop 2017).",
            "effectiveness_qualitative": "Deep learning achieved major breakthroughs and dominates vision benchmarks; rapid performance improvements observed on ImageNet-style tasks.",
            "impact_potential": "High — accelerated progress in computer vision, enabled many downstream applications in industry and research.",
            "comparison_to_alternatives": "Implicit comparison: deep learning substantially outperformed prior methods on large-scale vision benchmarks; detailed comparisons not provided in this paper.",
            "success_factors": "Massive labeled dataset (ImageNet), community competitions/benchmarks, and improvements in compute resources (CPUs/GPUs).",
            "key_insight": "Supervised deep learning scales with large labeled image datasets and compute, yielding substantial performance gains on vision benchmarks, but benchmark-driven progress can mask real-world limitations.",
            "uuid": "e2305.1"
        },
        {
            "name_short": "Google Flu Trends (case study)",
            "name_full": "The Parable of Google Flu: Traps in Big Data Analysis",
            "brief_description": "An example where large-scale data-driven models (search-query based) were used for influenza surveillance but produced misleading results, illustrating limits of big-data ML without robust domain validation.",
            "citation_title": "The Parable of Google Flu: Traps in Big Data Analysis",
            "mention_or_use": "mention",
            "scientific_problem_domain": "epidemiology / disease surveillance",
            "problem_description": "Predict population-level influenza incidence from search-query (big-data) signals as an alternative or augmentation to traditional surveillance.",
            "data_availability": "Large volumes of search-query data (big data) were available, but the paper highlights concerns about representativeness and the data's relationship to ground truth surveillance.",
            "data_structure": "Aggregate search-query time-series / counts (temporal aggregated signals).",
            "problem_complexity": "High: non-stationary human behavior, confounders, and sensitivity to shifts in search behavior make modelling challenging.",
            "domain_maturity": "Epidemiology is a mature scientific domain; application of big-data ML to surveillance is newer and contested.",
            "mechanistic_understanding_requirements": "high - causal and domain understanding required to ensure robustness and avoid spurious correlations.",
            "ai_methodology_name": "Statistical / machine learning models on search-query data (big-data analytics)",
            "ai_methodology_description": "Models leveraging correlations between query frequencies and influenza incidence; criticized for over-reliance on correlation and insufficient validation against epidemiological ground truth.",
            "ai_methodology_category": "supervised/statistical learning (population-level prediction)",
            "applicability": "Questionable — initial promise undermined by lack of robustness and representativeness; requires integration with domain expertise and validation.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Cited as an illustrative failure/trap in big-data analysis; demonstrates that big data alone does not guarantee accurate scientific prediction.",
            "impact_potential": "Primarily a cautionary impact: motivated better validation practices and highlighted risks of over-reliance on blind data-driven approaches in scientific domains.",
            "comparison_to_alternatives": "Implicitly compared to traditional epidemiological surveillance methods, which proved more reliable when properly validated; no numerical comparison provided here.",
            "success_factors": "Large data volume was insufficient; lack of representativeness and domain-informed validation undermined effectiveness.",
            "key_insight": "Abundant data without representativeness and causal validation can lead ML systems to fail in scientific prediction tasks; domain expertise and rigorous validation are essential.",
            "uuid": "e2305.2"
        },
        {
            "name_short": "AlphaGo / Deep RL",
            "name_full": "Mastering the game of go without human knowledge",
            "brief_description": "A reinforcement learning system using self-play and deep neural networks that achieved superhuman performance in the game of Go, demonstrating the power of deep RL on extremely large sequential decision problems.",
            "citation_title": "Mastering the game of go without human knowledge",
            "mention_or_use": "mention",
            "scientific_problem_domain": "sequential decision-making / game-playing benchmark (Go)",
            "problem_description": "Learn to play Go at superhuman level via reinforcement learning and self-play without relying on human game records.",
            "data_availability": "Training relied on synthetic data generated via self-play rather than labeled human data; effectively abundant training data through simulation.",
            "data_structure": "Structured board states represented as tensors (discrete positions and move sequences).",
            "problem_complexity": "Extremely large combinatorial search/decision space; high non-linearity and long-range dependencies.",
            "domain_maturity": "Games are established benchmarks in AI; deep RL matured to deliver high performance on such tasks.",
            "mechanistic_understanding_requirements": "low - for benchmark achievement, black-box policy/value networks are acceptable though interpretability is desirable for broader application.",
            "ai_methodology_name": "Deep reinforcement learning with self-play (policy and value networks combined with Monte Carlo Tree Search)",
            "ai_methodology_description": "Deep neural networks trained via reinforcement learning and self-play to learn policy and value functions; combined with search (MCTS) for decision-making. The current paper only cites the result and does not detail the architecture or training regimen.",
            "ai_methodology_category": "reinforcement learning / deep learning",
            "applicability": "Highly applicable to complex sequential decision tasks when simulation/self-play can generate abundant experience; demonstrated clear success in Go.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Cited as a major breakthrough where deep RL achieved superhuman play in Go; considered a landmark for RL capabilities.",
            "impact_potential": "High for AI research and potential transfer to other sequential decision problems (e.g., planning, control) where simulation is feasible.",
            "comparison_to_alternatives": "Implied to outperform previous Go programs and human players; specific numeric comparisons are not provided in this paper.",
            "success_factors": "Self-play data generation, effective neural network architectures, algorithmic innovations (combining RL with search), and large compute resources.",
            "key_insight": "Deep RL combined with simulation/self-play can solve extremely large sequential decision problems when abundant simulated experience and compute are available.",
            "uuid": "e2305.3"
        },
        {
            "name_short": "Benchmarks & competitions (UCI, GLUE, SQuAD, Papers With Code)",
            "name_full": "Benchmark datasets and organized competitions (UCI Machine Learning Repository, GLUE, SQuAD, ImageNet LSVRC, Papers With Code, CodaLab)",
            "brief_description": "Standardized datasets and competitions used to evaluate and compare ML algorithms across domains (tabular, vision, NLP), driving progress but also raising questions about representativeness.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "general machine learning evaluation across domains (vision, NLP, tabular data)",
            "problem_description": "Provide shared tasks and datasets for systematic performance evaluation, comparison and competition among ML methods.",
            "data_availability": "Varied: small-to-moderate labeled tabular datasets (UCI) up to large-scale corpora and image sets (ImageNet, GLUE, SQuAD); generally publicly accessible.",
            "data_structure": "Diverse: structured tabular data, unstructured text (NLP corpora), and image data; labeled per task requirements.",
            "problem_complexity": "Varies by benchmark — some are low-dimensional or well-structured (UCI), others are high-dimensional and complex (ImageNet, GLUE, SQuAD).",
            "domain_maturity": "Benchmark-driven subfields are well-established with mature evaluation protocols, but the paper highlights critiques about whether benchmarks capture real-world complexities.",
            "mechanistic_understanding_requirements": "varies - many benchmarks accept black-box models for winning metrics; some applications require higher interpretability.",
            "ai_methodology_name": "Various supervised learning algorithms and specialized architectures (CNNs for vision, transformers and other deep models for NLP, classical ML for tabular data)",
            "ai_methodology_description": "Researchers train and evaluate a wide range of algorithms on standardized splits and metrics provided by benchmarks and competitions; the paper discusses the role of these benchmarks in progress assessment rather than describing particular models.",
            "ai_methodology_category": "supervised learning; benchmarking/evaluation frameworks",
            "applicability": "Suitable for method development and comparative evaluation, but caution advised about overfitting to benchmarks and lack of real-world representativeness.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Benchmarks accelerate technical progress but can give an illusion of progress if datasets and metrics do not reflect real-world tasks; the paper discusses both benefits and limits.",
            "impact_potential": "High for driving algorithmic innovations and establishing standards, but risk of misaligned incentives if benchmarks dominate research goals.",
            "comparison_to_alternatives": "The paper criticizes exclusive reliance on benchmarks and suggests complementing them with broader, ELSEC-aware evaluation criteria.",
            "success_factors": "Public availability of labeled datasets, standardized metrics, community-organized competitions and reproducibility platforms (Papers With Code, CodaLab).",
            "key_insight": "Benchmarks are powerful drivers of technical progress when ample labeled data exist, but they must be complemented with domain-aware validation to avoid misleading conclusions about real-world effectiveness.",
            "uuid": "e2305.4"
        },
        {
            "name_short": "Fairness & auditing toolkits",
            "name_full": "AI Fairness 360, Aequitas and end-to-end algorithmic auditing frameworks",
            "brief_description": "Software toolkits and frameworks to detect, report and mitigate discrimination and to support internal algorithmic audits for fairness, accountability and transparency in ML systems.",
            "citation_title": "Ai Fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias",
            "mention_or_use": "mention",
            "scientific_problem_domain": "algorithmic fairness / socio-technical evaluation of ML systems",
            "problem_description": "Diagnose, report and mitigate bias and disparate impact in ML models and support organizational auditing practices prior to deployment.",
            "data_availability": null,
            "data_structure": "Typically applied to structured tabular datasets with labels and protected attributes, but toolkits claim applicability across data types and ML pipelines.",
            "problem_complexity": "High socio-technical complexity involving statistical, legal, ethical and domain-specific considerations; context-dependent fairness definitions.",
            "domain_maturity": "Emerging: multiple toolkits and frameworks exist but practices are incomplete, scattered and still being operationalized.",
            "mechanistic_understanding_requirements": "high - transparency and interpretability are required for accountability, auditability and stakeholder trust.",
            "ai_methodology_name": "Bias detection and mitigation algorithms and audit procedures (toolkits combining metrics, diagnostics and mitigation methods)",
            "ai_methodology_description": "Toolkits provide a set of fairness metrics, statistical tests, reporting utilities and mitigation routines to examine and reduce discriminatory behaviour in trained models; the paper lists examples but does not detail algorithms.",
            "ai_methodology_category": "post-hoc analysis / fairness auditing toolkits (software + procedural frameworks)",
            "applicability": "Applicable across sectors where fairness is a concern; intended to be integrated into development workflows and organizational governance.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Toolkits are useful starting points but the paper notes they are insufficient alone, often lack completeness, and must be combined with governance, diverse teams and contextual analysis.",
            "impact_potential": "Potentially high if integrated with organizational culture, multidisciplinary review, audits and regulatory frameworks; otherwise limited.",
            "comparison_to_alternatives": "Positioned as complementary to checklists, canvases, and governance mechanisms; no quantitative comparisons provided.",
            "success_factors": "Integration into existing workflows, organizational commitment, multidisciplinary teams, governance (audits/certification), and context-aware application.",
            "key_insight": "Technical fairness/tooling is necessary but not sufficient; effectiveness depends on organizational processes, diverse expertise, and governance structures to translate tool outputs into responsible outcomes.",
            "uuid": "e2305.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Clinically Applicable Deep Learning for Diagnosis and Referral in Retinal Disease",
            "rating": 2,
            "sanitized_title": "clinically_applicable_deep_learning_for_diagnosis_and_referral_in_retinal_disease"
        },
        {
            "paper_title": "Mastering the game of go without human knowledge",
            "rating": 2,
            "sanitized_title": "mastering_the_game_of_go_without_human_knowledge"
        },
        {
            "paper_title": "The Parable of Google Flu: Traps in Big Data Analysis",
            "rating": 2,
            "sanitized_title": "the_parable_of_google_flu_traps_in_big_data_analysis"
        },
        {
            "paper_title": "Ai Fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias",
            "rating": 2,
            "sanitized_title": "ai_fairness_360_an_extensible_toolkit_for_detecting_and_mitigating_algorithmic_bias"
        },
        {
            "paper_title": "AI Watch Methodology to Monitor the Evolution of AI Technologies",
            "rating": 1,
            "sanitized_title": "ai_watch_methodology_to_monitor_the_evolution_of_ai_technologies"
        },
        {
            "paper_title": "Guidelines for Human-AI Interaction",
            "rating": 1,
            "sanitized_title": "guidelines_for_humanai_interaction"
        }
    ],
    "cost": 0.02091175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Progressing Towards Responsible AI</p>
<p>Teresa Scantamburlo 
Atia Cortés 
Marie Schacht 
Progressing Towards Responsible AI</p>
<p>The field of Artificial Intelligence (AI) and, in particular, the Machine Learning area, counts on a wide range of performance metrics and benchmark data sets to assess the problem-solving effectiveness of its solutions. However, the appearance of research centres, projects or institutions addressing AI solutions from a multidisciplinary and multi-stakeholder perspective suggests a new approach to assessment comprising ethical guidelines, reports or tools and frameworks to help both academia and business to move towards a responsible conceptualisation of AI. They all highlight the relevance of three key aspects: (i) enhancing cooperation among the different stakeholders involved in the design, deployment and use of AI; (ii) promoting multidisciplinary dialogue, including different domains of expertise in this process; and (iii) fostering public engagement to maximise a trusted relation with new technologies and practitioners. In this paper, we introduce the Observatory on Society and Artificial Intelligence (OSAI), an initiative grew out of the project AI4EU aimed at stimulating reflection on a broad spectrum of issues of AI (ethical, legal, social, economic and cultural). In particular, we describe our work in progress around OSAI and suggest how this and similar initiatives can promote a wider appraisal of progress in AI. This will give us the opportunity to present our vision and our modus operandi to enhance the implementation of these three fundamental dimensions.</p>
<p>Introduction</p>
<p>In science, nothing has been more controversial than the notion of progress. Debates on what progress is and if it does really exist abound in the philosophy of science and are closely related to questions about the goals and the methods of a scientific discipline [54]. Although between the 1960s and 1970s philosophers of science put forward thought-provoking views, the idea of progress is commonly associated with the incremental acquisition of knowledge in a particular domain. The same idea is still prevalent in toady's research practice, including Artificial Intelligence (AI). For instance, a study discussing the flaws in Google Flu Trends claimed that "science is a cumulative endeavour, and to stand on the shoulders of giants requires that scientists be able to continually assess work on which they are building" [47, p 1205].</p>
<p>In this paper, we commit to the view that the progress of a scientific discipline can be measured by the problem-solving 4 effectiveness of its theories [46]. This view applies to, not only science, but also to different intellectual endeavours, including areas where solutions consist of technical artefacts such as algorithms and computing systems.</p>
<p>Typically, the problem-solving effectiveness of AI solutions is a matter of performance testing. For example, in supervised learning, we assess algorithms based on the number of errors that they make on new, unseen data. Performance lies at the very heart of any learning algorithm, which is, by definition, a computer program that improves through experience.</p>
<p>Intuitively, a Machine Learning (ML) technique is progressive as it performs well along with distinct criteria, including the task at hand, the benchmark data set and the computed performance measure. Although there is no hint of absolute progress in the field -in that no algorithm has proved to be the best at any possible task or condition -, deep learning methods have nevertheless hit the mark in multiple domains ranging from diagnosing eye diseases [37] to playing game [58]. Other signs of progress regard the time of processing [55] which relies on the evolution of CPU capabilities.</p>
<p>However, the introduction of AI algorithms into large portions of human life has suggested that technical performance is not enough. The adequacy of AI solutions depends on a broader set of considerations accounting for the behaviour of AI systems within the environment they are embedded. Repeated facts of algorithmic discrimination and lack of transparency shifted the focus from performance to accountability, from advances in accuracy and speed of computation to the protection of human rights and democratic values. In other words, the appraisal of AI progress is moving away from a purely technical assessment and becoming a multi-factorial affair which integrates aspects of privacy, fairness and transparency, among others.</p>
<p>The transition towards a broader notion of AI assessment is the main focus of the present work. As we will see, a new ethical turn prompted the rise of centres and networks addressing AI solutions from a multidisciplinary and multi-stakeholder perspective.</p>
<p>Our maim claim is that these initiatives have contributed to moving towards a different notion of progress in AI that goes beyond technical performance to foster responsible development and dissemination of AI, acting on three main elements: (i) promoting an interdisciplinary dialogue around AI; (ii) involving diverse stakeholders and (iii) engaging the public. In particular, we would like to present the Observatory on Society and AI as an example of these multidisciplinary and multi-stakeholder initiatives.</p>
<p>The paper is structured as follows. In Section 2, we describe traditional practices for the assessment of AI, focusing in particular on ML, and the attempts to change them. Section 3 introduces the Observatory for Society and AI as an example of an initiative which may contribute to the ethical turn of AI. In Section 4, we survey some responsible practices which will be part of the inventory of resources to be explored by the Observatory. We will conclude in Section 5 with some final remarks.</p>
<p>The assessment of progress in AI</p>
<p>One of the keys to understanding the progress of a scientific discipline is to assess its ability to solve problems. Disciplines have developed several methodologies to assess their own problem-solving effectiveness, but what it means for a problem to be solved can vary across the fields. For example, in logic, a problem can be viewed as solved when a theorem has been proved, in philosophy when a thesis is well-argued, in medicine when a treatment cures a disease, and so forth. Each discipline may develop different standards and processes for accepting solutions. The efficacy of a philosophical argument is thus assessed differently from the efficacy of an experimental result.</p>
<p>The field of ML has developed various practices to assess the effectiveness of learning techniques. Most of them reflect the ideal of experimentation in natural science, a model recommended particularly in the early days of ML research [45], then taken as a sign of maturity and objectivity of the field [35]. Usually, testing ML algorithms involve the use of benchmark data sets, the selection of performance measures, multiple tests and comparisons with competing methods. In a classification task, for example, a typical performance metric is accuracy, a scalar value which represents the fraction of predictions that an algorithm got right. More sophisticated methods look at the optimal trade-off between benefits (the true positive rate) and costs (the false-negative rate) such as ROC analysis.</p>
<p>The choice of the performance measure is a crucial task as the available metrics have a distinct meaning which depends on the context of an application. In other words, the measured value represents something we care about [35]. For example, in a system predicting fraud attempts by loan applicants, testing for precision might be sufficient and more informative than other metrics such as specificity. Things would change if the outcome referred to cancer detection, where misclassification comes at different cost.</p>
<p>The creation and maintenance of large data repositories is another influential component of ML testing practice. Since the appearance of UCI collection [28], the field has mostly committed to a benchmark-oriented attitude where data sets from disparate domains become the reference point for comparing algorithms' performance. In recent times, the activity of data collection has witnessed a massive surge and new large-scale databases, as well as more productive data annotation practices, have come to the surface.</p>
<p>The ImageNet project [17] generated more than 14 million images annotated by thousands of crowdworkers and structured around the WordNet hierarchy [29]. ImageNet has also inspired contests (the socalled ImageNet Large Scale Visual Recognition Challenge 5 ) where ML practitioners can compete and test their models in different specific tasks, such as object detection and image segmentation. Benchmarks and competitions abound also in natural language processing where we count challenges for question-answering, reasoning and sentiment analysis 6 .</p>
<p>In recent times, the assessment of progress in AI was felicitated by initiatives tracking algorithms' performance during competitions, open repositories and code platforms. These include, for example, the AI index initiative [55] and the AI Watch methodology for the monitoring of AI progress [50]. Thanks to them, one can get a glimpse of the significant breakthroughs achieved by the field. For example, in large-scale object classification tasks, the classification error of best-performing algorithms fell from 0.28 to 0.023 (see the results presented at CVPR Workshop 2017).</p>
<p>While the systematic analysis of technical performance tells us that the field is progressing at a fast pace, there were AI researchers casting doubts on the robustness of standard assessment approach and claiming that, in reality, what we call progress could be only an illusion [41].</p>
<p>The ethical turn</p>
<p>The discontent with standard testing practice in ML research is not a recent phenomenon. As early as 2006 Chris Drummond [35] criticised the standard testing approach raising three key points:</p>
<ol>
<li>Performance measures: he observes that there are other factors influencing a performance measure like accuracy and these may include misclassification costs, the stability of the error rate, and the needs of the end users, among others. 2. Statistical tests: following criticism in Psychology, he highlights that statistical tests are frequently misinterpreted, for example, as a confirmation of the alternative to the null hypothesis 7 . While their employment in ML experimental practice is often taken as a sign of rigor and objectivity, he contends that they do not give the degree of evidence that many people believe. 3. Benchmark data sets: while the use of benchmark data sets allow us to easily compare algorithms' performance, they suffer from serious limitations. For example, drawing on previous analysis, he raises concerns as to whether benchmark data sets are really representative of reality and how much the data collection and construction account for differences in class distribution.</li>
</ol>
<p>Further impulse to renovate the experimental practice in ML came from the movement of reproducible research requiring the publication of code and data to reproduce the results reported in scientific articles [59]. In a similar spirit, important ML conferences and journals encourage authors to adhere to best practices by making available data and software tools 8 . Other relevant initiatives include open source repositories and platforms allowing AI practitioners to share data and AI models, such as Papers with Code [26] and Open ML [25]. Another recent work proposed a more comprehensive view of assessment integrating classical performance measures with often neglected costs connected to the development and deployment of an AI system [49].</p>
<p>While these efforts operated within the edges of traditional scientific principles such as transparency and reproducibility, other assessment criteria emerged as a consequence of numerous debates around the impact of AI on society along with plans for action, what we call the ethical turn of AI. The new wave of optimism prompted by the successful application of AI to big data [40] was followed by critical analyses raising issues for culture [34,44] and human decisionmaking [31]. Other failures and overstatements in the application of AI to transport and healthcare stimulated many initiatives around the world. These comprise research projects, journalistic investigation, centres and networks focused on the impact of AI on our lives.</p>
<p>In the few last years, we have witnessed more than one hundred declarations of AI principles from governments, organizations and multi-stakeholder initiatives, aimed at providing normative guidance for ethical, rights-respecting, and socially beneficial development and use of AI technologies. Most of those guidelines are aligned on the following eight key themes: Privacy, Accountability, Safety and Security, Transparency and Explainability, Fairness and Nondiscrimination, Human Control of Technology, Professional Responsibility, and Promotion of Human Values [38].</p>
<p>Note that, while ethical considerations are not new in the field of AI and notable scholars, such as Norbert Wiener, had already warned of the possible misuse of intelligent and control systems [60], the scale and the number of present efforts have no precedents in the history of the field. For this reason, we acknowledge all these initiatives as a whole movement with the potential to widen future assessment practices.</p>
<p>A final, interesting remark regards the variety of actors who are contributing to this ethical wave. The European Commission, for instance, based on the Trustworthy AI Guidelines (TAIG) [42] published by the High Level expert Group on AI (HLEG-AI), proposed a regulatory framework for high-risk AI applications with a view to build an "ecosystem of trust" [33]. Big companies, likewise, published new design principles 9 and audit frameworks [56], also motivated by practical needs which are usually attenuated in a research context (think of company's liability and reputation). Finally, the landscape of ethical activities comprises a large number of centres (see Table 1) which scrutinize AI systems through the lens of legal principles and social values, study the impact on economy and human labour, and engage lay people with educational material or works of art.</p>
<p>An Observatory for Society and AI (OSAI)</p>
<p>The Observatory on Society and Artificial Intelligence (OSAI or simply "Observatory" hereafter) was set up in 2019 within the H2020 EU funded project AI4EU [5], whose objective is to build the first European AI on-demand web platform and ecosystem. The OSAI is an example of this vast array of initiatives animating the ethical turn of AI outlined in the previous section. Though at its infancy, it gives us the opportunity to explore how this and similar activities can contribute to stretch the assessment of AI and turns progress towards ethical principles.</p>
<p>The OSAI's aim is to support discussion and to facilitate the distribution of information about the Ethical, Legal, Socio-Economic and Cultural issues of AI (ELSEC-AI) within Europe. Specifically, the OSAI has the following objectives:</p>
<p>• To stimulate reflection, discussion and due consideration of ELSEC-AI issues within the project through a series of working groups (see Section 4). OSAI is attracting a network of experts in different domains of ELSEC-AI that will contribute to bridge the knowledge gap existing today within AI practitioners and users. • To provide resources to educate the general EU public more accurately about AI and ELSEC-AI issues by generating weekly content in the form of articles, reports, cultural announcements with the objective to promote discussion and awareness on these topics. 9 See e.g., the AI Guidelines of Telefnica https://www. telefonica.com/en/web/responsible-business/ourcommitments/ai-principles or Deutsche Telekom https://www.telekom.com/en/company/digitalresponsibility/details/artificial-intelligenceai-guideline-524366</p>
<p>The Observatory evolves in a complex scenario: the field of AI is gaining momentum, and many public and private agencies have begun to consider the opportunities and the risks that lie behind this exciting trend. The OSAI seeks to carve out its own identity and role neither in contrast nor competition with other existing European initiatives (e.g., HLEG-AI). It aims to increase connections among these related projects and make accessible a broad range of articles to the European public at large. The OSAI's approach can be described by three verbs: 1) Observe facts and events occurring within Europe by monitoring newspapers, online bulletins, scientific literature, etc.; 2) Reflect on particular events or issues through to the contribution of ELSEC-AI experts and, in particular, thanks to the activities of the working groups; 3) Report to the general public by using a simple (but not simplistic) language in a way to support mutual understanding among experts and educate lay people.</p>
<p>The context</p>
<p>As we said, the creation of the Observatory takes place in a complex and dynamic context where an imprecise number of AI-related events populate the European calendar. Table 1 includes a collection of European centres that are specifically dedicated to the research around AI and its impact on Society. These were selected form a larger set based on a search of simple keywords on Google engine (such as "AI", "ethics", and "society"). The common aim among these institutions is to promote designs and developments of technologies that put upfront concepts such as social responsibility, trust or fairness. Some are dedicated to the creation of guides, others to define evaluation methods, but all have in common the will to create spaces for multidisciplinary dialogue.</p>
<p>While the abundance of centres and projects dealing with AI and its social and ethical impact is a sign of cultural awareness and a source of knowledge, all these positive undertakings run the risk of isolation and self-referentiality. Therefore, OSAI should try to bridge this gap and promote cooperation and mutual knowledge. In addition, it will focus on areas that extend beyond the ethical and legal aspects, including also socio-economic and cultural elements (e.g., how AI is perceived among European citizens, how the arts are presenting or using AI).</p>
<p>The Observatory differs from these initiatives in several respects. In the first place, the OSAI focuses not only on articles and news, but also on people. Indeed, one of the motivating ideas behind the Observatory is the creation of a community of people who can contribute to the discussion of ELSEC-AI. Such a community can combine various types of subjects such as AI experts (e.g., AI researchers and practitioners), specialists in any ELSEC-related field (ethicists, sociologists, lawyers, policy makers, artists, etc) and lay people. In the second place, the OSAI will approach ELSEC-AI in the context of Europe so as to foster the dialogue among European countries.</p>
<p>OSAI Working Groups</p>
<p>To revise how AI systems are evaluated and complement performance metrics with ELSEC-AI considerations, we need a change in the background of the process. A first step is to promote diversity in the teams that are both designing and assessing such systems. Note that when we talk about diversity, we do not only refer to gender, ethnicity or functional capabilities, but also to include professionals from multiple disciplines and domains of expertise.</p>
<p>A second step is to foster a multidisciplinary dialogue among experts to promote reflection on ELSEC-AI and identify shared strate- </p>
<p>Name</p>
<p>Country Type Objective HumanE AI [16] Europe H2020 EU Project To create the foundations for AI systems that empower people and society, with special focus on Collaborative Humane Computer Interaction based on a convergence of HCI with ML. AI Watch [4] European Commission</p>
<p>Public Institution An initiative to monitor the development, uptake and impact of AI for Europe AI4People [6] European Commission</p>
<p>Multi-stakeholder Forum</p>
<p>To bring together all actors interested in shaping the social impact of new applications of AI, including the European Parliament, civil society organisations, industry and the media. They published the AI4People's Ethical Framework [39] which inspired the TAIG [42]. OECD.AI [24] Intergovernmental</p>
<p>International Organisation</p>
<p>The OECD AI Policy Observatory combines resources from across the OECD, its partners and all stakeholder groups to facilitate dialogue between stakeholders while providing multidisciplinary, evidencebased policy analysis in the areas where AI has the most impact. gies to consider ELSEC issues within the AI life cycle. OSAI is trying to fulfill these tasks (diversity and multidisciplinary discussion) by creating a set of working groups, i.e., semi-organised groups of experts working on ELSEC-AI topics. The experience of the working groups is a laboratory to explore ways to address ELSEC-AI from a diversity of perspectives. They have an experimental character in that there is no systematic knowledge and expertise in dealing with ELSEC-AI in real-world with a multidisciplinary approach. At the beginning, working groups will emphasize two perspectives with a view to incorporating further aspects in future:</p>
<p>• Legal AI: to study existing laws and regulations, how applicable they are to AI systems and identify possible gaps. • Ethical AI: to promote the design and development of AI systems that respect fundamental rights.</p>
<p>The working groups will be formed by experts in different areas, from academic, business, media or other backgrounds: lawyers and data protection officers, philosophers, software engineers, journalists, sociologists, etc. With this variety of profiles we expect to generate a wide range of opinions and experiences around our topics of interest. Moreover, these working groups are supposed to grow with a bottom-up approach engaging participants from the very beginning. Participants will work in smaller groups and in a limited span time 10 . This activity aims to encourage the participation of experts with an open and transparent methodology and engage them in the communication and sharing of knowledge in this new area of interest. The main objectives of this activity are the following:</p>
<p>• To fill the gap between the ethical debate and the engineering practice in European organizations. • To help researchers and practitioners navigate the ethical challenges that arise in different real-world AI applications. • To support interdisciplinary dialogue engaging people from different backgrounds. • To promote cross-fertilisation among different sectors (e.g., academia, companies, public institutions). • To inspire future responsible practices in the field of AI.</p>
<p>• To create ELSEC-AI literacy understandable for different types of audiences and domains of expertise.</p>
<p>To fulfill the last two points, we expect to generate a set of good AI practices with a focus on how to implement guidelines such as the HLEG-AI Trustworthy AI Guidelienes, especially for SMEs and start-ups. We will also adapt part of the content into educational material that will be shared through the Observatory and the AI4EU communication channels.</p>
<p>Background and Methodology</p>
<p>In the second half of 2019, the authors participated in the piloting of the Trustworthy AI assessment List (TAIL) that was published along with the Trustworthy AI Guidelines by the HLEG-AI, were 50 companies from different sectors and European countries were interviewed. The result of that investigation suggested the need to promote operational resources (e.g., tools, frameworks, procedures/methodologies, etc.) to support organisations to take the Trustworthy AI requirements into account. Other studies have stressed the value to share knowledge and experiences (e.g., best practices, case studies) to help practitioners navigate complex ethical issues and interventions [43].</p>
<p>To build upon the piloting of the TAIL, we propose to form small teams that can collaborate with companies (but also public organisations) that are implementing AI products with a view to experiment how to tackle Trustworthy AI requirements, also drawing on responsible practices (see section 4.2), generate and share a list of good practices that can inspire other organisations. We hope in this way to help fill the gap of knowledge and language between the different stakeholders, i.e., from regulations, to recommendations to the final translation to software engineering methods and other sorts of processes (Human-Computer Interaction methods, ML approaches, management strategies, etc.). Also we will try to disseminate the result of working groups through the Observatory web-site so as to include the Society in this process of gaining trust in new technologies.</p>
<p>At present, the working groups are not intended to generate a comprehensive methodology or a new standard but identify good practices that help integrate ethical and legal requirements into the assessment of AI systems. In concrete, the teams of experts will interact with organisations to analyse specific case studies where they can apply and test one or more responsible AI practices. Interactions can take the form of interviews, design thinking sessions, algorithmic audits, among others, depending on the operational resources adopted by the team (see the responsible practices in Section 4.2). This would help organisations to check whether their AI products meet Trustworthy AI requirements, and possibly re-frame their objectives and their Key Performance Indicator (KPIs) in the light of ethical and legal constraints.</p>
<p>A series of training sessions with invited speakers (either external or internal to the working groups) will help the members of working groups to achieve a shared knowledge about responsible AI practices (questionnaires and checklists, frameworks, strategy guides and canvases, etc). Also, to test and refine our proposal, we plan to pilot working groups with an internal activity involving a few experts and partners (from research and industry) of the AI4EU consortium.</p>
<p>Responsible Practices</p>
<p>While many AI principles have been published, the translation of these into practices and processes is still at the very beginning. The main reason relies on their abstract nature, which makes them difficult for practitioners to operationalise. Even though awareness of the potential ethical issues is increasing at a fast rate, the AI community's ability to take action to mitigate the associated risks is still at its infancy [52]. Operationalisation so far strongly depends on moral compass of individual since there are no legal binds to the existing ethical guidelines. Thus, a framework for ethical decision making and responsible practices is required.</p>
<p>We are classifying the responsible practices by their nature and intended use into 5 groups:</p>
<p>• Assessments, questionnaires and checklists raise questions, encourage reflection and inspire potential action, e.g., the HLEG-AI TAIL with 131 questions to operationalise the seven key requirements declared in the AI guidelines [42], or the Consequence Scanning, an agile practice to consider the potential consequences of a product or service on people, communities and the planet [9]. • End-to-end frameworks address each stage of the entire process with appropriate activities and involve multiple audiences, e.g., the End-to-End Framework for Internal Algorithmic Auditing to help companies and their engineering teams audit AI systems before deploying them [56], or the People + AI Guidebook to help user experience (UX) professionals and product managers follow a human-centered approach to AI [27]. • Strategy guides and canvases are thinking frameworks and diagnostic tools, that help break down and work through complex challenges, e.g., the Data Ethics Canvas helps identify and manage data ethics considerations [11], or the Ethical Operating System to help inform the design and development process, provide strategies to mitigate risks and take action [15]. • Design guides are sets of recommendations towards responsible good practice in design, e.g., the Guidelines for Human-AI Interaction recommend best practices for how AI systems should behave [30]; AI Ethics Cards are a set of four design principles and ten activities that help guide an ethically responsible, culturally considerate, and humanistic approach to designing with data [14]. • Software toolkits provide metrics and algorithms to support the ethical development of AI-powered software, e.g., the AI Fairness 360 Toolkit to help examine, report, and mitigate discrimination and bias in ML models throughout the AI application life cycle [32]; Aequitas bias audit toolkit to audit machine learning models for discrimination and bias [57]. The landscape of responsible practices is wide, but insufficient, inefficient and scattered. Many of these practices do not adequately address the challenges of context-dependency, they lack ease of use [36] and completeness, they either address only silo disciplines, or single process steps or particular problems. Furthermore, ticking boxes on fairness checklists, mitigating bias with algorithms and anticipating consequences with ethics cards is by far not enough. As depicted in Figure 1, in order to progress towards responsible outcomes, it needs first and foremost close collaboration and a diverse range of perspective, guidance derived from values, principles and policies, a curated set of responsible practices, throughout the entire process, ensured by governance procedures, such as audit services, certifications and AI labels, or company-internal self-check tools, red teams, and ethics Objectives and Key Results (OKRs) and KPIs [51].</p>
<p>As companies start to envision procedures to operationalise AI principles, new professional roles, summarised as "ethics owners" by [51], are being created in order to cover the lack of attention that ethics had in first place. They own responsibility for ethics practices across an organization, and engage to transform principles, values, ethical stances, and often legal and regulatory imperatives into concrete practices within their organization. To be most effective at achieving these goals, new responsible practices must be aligned with teams existing workflows and supported by organizational culture [48].</p>
<p>Concluding remarks</p>
<p>This paper presents an overview on the existing initiatives that we can find today to enhance the progress of AI towards responsible design, development and deployment. This review puts special attention on the European centres and networks involved in this process since this is the main scope of the Observatory and the objectives within AI4EU. Although it is not exhaustive, it suggests that we are witnessing an ethical turn promoting a wider discussion about the assessment of AI systems. In addition, the study on the different responsible practices shows an interest from a diversity of stakeholders to promote a different approach to the assessment of AI, although it is clear that there are still some gaps to cover in order to integrate the required ecosystem.</p>
<p>In his book Progress and its Problems, Larry Laudan recommends to cast the nets of appraisal sufficiently widely so as to include all the cognitively relevant factors which are actually present in the historical situation [46]. The OSAI aims to contribute to this change of paradigm by creating a space for multidisciplinary gathering, where different actors will be able to discuss and create literacy related to ELSEC-AI. We expect to study in depth the key pillars for progressing towards responsible outcomes of AI and their relation with the Trustworthy AI Guidelines and Assessment List in order to find existing relations, but also possible opportunities to complement them. Our final objective is to generate a set of good responsible practices that could help AI practitioners to implement the HLEG-AI documents and align with the European Commission's vision of Trustworthy AI. We strongly believe that this step is necessary to bring close all the stakeholders involved in the design, deployment and use of new technologies such as AI, which may have great benefits for the Society but are still in a process of being trusted.</p>
<p>Figure 1 .
1High-level overview of key pillars for progressing towards responsible outcomes</p>
<p>Table 1 .
1European centres for AI and Ethics
European Centre for Living Technologies, Ca' Foscari University of Venice, Italy, email: teresa.scantamburlo@unive.it 2 Barcelona Supercomputing Center, Spain, email: atia.cortes@bsc.es 3 QuantumBlack, UK, email: Marie.Schacht@quantumblack.com4 The notion of problem-solving also describes a specific AI research program (e.g., the General Problem Solver[53]). However, in this paper, this idea refers to the problem-oriented activity of science as neatly described by[46].
See: http://www.image-net.org/challenges/LSVRC/ 6 See e.g., the General Language Understanding Evaluation (GLUE) benchmark (https://gluebenchmark.com/, the the Stanford Question Answering Dataset (SQuAD) challenge https://rajpurkar. github.io/SQuAD-explorer/ and the competition section within the CodaLab platform https://codalab.org/
In statistics the notion of the null hypothesis refers to a default assumption such as "data has a normal distribution" or "there is no correlation between two random variables". In short, a statistical test tells us if we should reject or fail to reject the null hypothesis. In ML the null hypothesis assumes that there is no difference between the performance measures of two algorithms. The one with the greater value is the winner algorithm.8 See e.g., the eligibility criteria in the call for paper of the European Conference on Machine Learning (ECML). The Journal for Artificial Intelligence Research has even stricter requirements.
Now, the schedule of activities will be constrained by the AI4EU lifetime, but this might be revised in future.
ACKNOWLEDGEMENTS Teresa Scantamburlo and Atia Corts are partially supported by the project A European AI On Demand Platform and Ecosystem (AI4EU) H2020-ICT-26 #825619. The views expressed in this paper are not necessarily those of the consortium AI4EU.
. Ai Sustainability Centre, AI Sustainability Centre. https://www.aisustainability. org/.</p>
<p>. Ai Transparency Insitute, AI Transparency Insitute. https:// aitransparencyinstitute.com/.</p>
<p>. A I Watch, AI Watch. https://ec.europa.eu/knowledge4policy/ ai-watch_en.</p>
<p>. Ai4eu -A, European AI On Demand Platform and Ecosystem. AI4EU -A European AI On Demand Platform and Ecosystem. https://www.ai4eu.eu/.</p>
<p>. Ai4people, AI4People. https://www.eismd.eu/ai4people/.</p>
<p>. Algorithmwatch , AlgorithmWatch. https://algorithmwatch.org/.</p>
<p>Centre for Data Ethics and Innovation. Centre for Data Ethics and Innovation. https://www.gov. uk/government/organisations/centre-for-data- ethics-and-innovation.</p>
<p>Consequence Scanning -an agile practice for responsible innovators. Consequence Scanning -an agile practice for responsible in- novators. https://www.doteveryone.org.uk/project/ consequence-scanning/.</p>
<p>Data Ethics Canvas -Open Data Institute. Data Ethics Canvas -Open Data Institute. https://theodi.org/ article/data-ethics-canvas/.</p>
<p>. Datalab -Center, Digital Social Research. DATALAB -Center for Digital Social Research. https:// datalab.au.dk/.</p>
<p>. Digital Ethics Lab. Digital Ethics Lab. https://digitalethicslab.oii.ox. ac.uk/.</p>
<p>Ethical Compass -This tool can help. Ethical Compass -This tool can help. https://www.ideo.com/ blog/ai-needs-an-ethical-compass-this-tool- can-help.</p>
<p>. O S Ethical, Toolkit, Ethical OS Toolkit. https://ethicalos.org.</p>
<p>. A I Humane, HumanE AI. https://www.humane-ai.eu/.</p>
<p>The Institute for Ethical AI and Machine Learning. The Institute for Ethical AI and Machine Learning. https:// ethical.institute/.</p>
<p>The Institute for Ethical AI in Education. The Institute for Ethical AI in Education. https://www. buckingham.ac.uk/research-the-institute-for- ethical-ai-in-education/.</p>
<p>Institute for Ethics in AI. Institute for Ethics in AI. https://ieai.mcts.tum.de/.</p>
<p>. Knowledge Centre Data &amp; Society. Knowledge Centre Data &amp; Society. https://data-en- maatschappij.ai/en/.</p>
<p>Leverhulme Centre for the Future of Intelligence. Leverhulme Centre for the Future of Intelligence. http://lcfi. ac.uk/.</p>
<p>. Oecd Ai Policy, Observatory, OECD AI Policy Observatory. https://oecd.ai/.</p>
<p>. Papers with Code. Papers with Code. https://paperswithcode.com/.</p>
<p>. + Ai People, Guidebook, People + AI Guidebook. https://pair.withgoogle.com/ guidebook/.</p>
<p>. Uci Machinelearning Repository, UCI MachineLearning Repository. https://archive.ics. uci.edu/ml/index.php.</p>
<p>Guidelines for Human-AI Interaction. S Amershi, D Weld, M Vorvoreanu, A Fourney, B Nushi, P Collisson, J Suh, S Iqbal, P Bennett, K Inkpen, J Teevan, R Kikin-Gil, E Horvitz, CHI 2019. ACMBest Paper Honorable MentionS. Amershi, D. Weld, M. Vorvoreanu, A. Fourney, B. Nushi, P. Collis- son, J. Suh, S. Iqbal, P. Bennett, K. Inkpen, J. Teevan, R. Kikin-Gil, and E. Horvitz, 'Guidelines for Human-AI Interaction', in CHI 2019. ACM, (May 2019). Best Paper Honorable Mention.</p>
<p>Big Data's Disparate Impact. S Barocas, A D Selbst, California Law Review. 104S. Barocas and A. D. Selbst, 'Big Data's Disparate Impact', California Law Review, 104, 671, (2016).</p>
<p>Ai Fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias. R K E Bellamy, K Dey, M Hind, S C Hoffman, S Houde, K Kannan, P Lohia, J Martino, S Mehta, A Mojsilovi, S Nagar, K N Ramamurthy, J Richards, D Saha, P Sattigeri, M Singh, K R Varshney, Y Zhang, 4:1-4:15IBM Journal of Research and Development. 634/5R. K. E. Bellamy, K. Dey, M. Hind, S. C. Hoffman, S. Houde, K. Kan- nan, P. Lohia, J. Martino, S. Mehta, A. Mojsilovi, S. Nagar, K. N. Ra- mamurthy, J. Richards, D. Saha, P. Sattigeri, M. Singh, K. R. Varshney, and Y. Zhang, 'Ai Fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias', IBM Journal of Research and Develop- ment, 63(4/5), 4:1-4:15, (2019).</p>
<p>White Paper on Artificial Intelligence: a European approach to excellence and trust. European CommissionEuropean Commission. White Paper on Artificial Intelli- gence: a European approach to excellence and trust. https: //ec.europa.eu/info/publications/white-paper- artificial-intelligence-european-approach- excellence-and-trust_en, 2020.</p>
<p>Critical Questions for Big Data. K Boyd, Crawford, Communication &amp; Society15Informationd. boyd and K. Crawford, 'Critical Questions for Big Data', Informa- tion, Communication &amp; Society, 15(5), 662-679, (2012).</p>
<p>Machine Learning as an Experimental Science (revisited). C Drummond, Proceedings of the Twenty-First National Conference on Artificial Intelligence: Workshop on Evaluation Methods for Machine Learning. the Twenty-First National Conference on Artificial Intelligence: Workshop on Evaluation Methods for Machine LearningAAAI PressC. Drummond, 'Machine Learning as an Experimental Science (re- visited)', in Proceedings of the Twenty-First National Conference on Artificial Intelligence: Workshop on Evaluation Methods for Machine Learning, pp. 1-5. AAAI Press, (2006).</p>
<p>From Principles to Practice -An interdisciplinary framework to operationalise AI ethics. AI Ethics Impact GroupAI Ethics Impact Group, From Principles to Practice -An interdisci- plinary framework to operationalise AI ethics, Bertelsmann Stiftung, 2020.</p>
<p>Clinically Applicable Deep Learning for Diagnosis and Referral in Retinal Disease. J De Fauw, J R Ledsam, B Romera-Paredes, S Nikolov, N Tomasev, S Blackwell, H Askham, X Glorot, B Odonoghue, D Visentin, G Van Den Driessche, B Lakshminarayanan, C Meyer, F Mackinder, S Bouton, K Ayoub, R Chopra, D King, A Karthikesalingam, C O Hughes, R Raine, J Hughes, D A Sim, C Egan, A Tufail, H Montgomery, D Hassabis, G Rees, T Back, P T Khaw, M Suleyman, J Cornebise, P A Keane, O Ronneberger, Nature Medicine. 249J. De Fauw, J. R. Ledsam, B. Romera-Paredes, S. Nikolov, N. Toma- sev, S. Blackwell, H. Askham, X. Glorot, B. ODonoghue, D. Visentin, G. van den Driessche, B. Lakshminarayanan, C. Meyer, F. Mackinder, S. Bouton, K. Ayoub, R. Chopra, D. King, A. Karthikesalingam, C. O. Hughes, R. Raine, J. Hughes, D. A. Sim, C. Egan, A. Tufail, H. Mont- gomery, D. Hassabis, G. Rees, T. Back, P. T. Khaw, M. Suleyman, J. Cornebise, P. A. Keane, and O. Ronneberger, 'Clinically Applicable Deep Learning for Diagnosis and Referral in Retinal Disease', Nature Medicine, 24(9), 1342-1350, (2018).</p>
<p>Principled Artificial Intelligence: Mapping Consensus in Ethical and Rights-based Approaches to Principles for AI. J Fjeld, N Achten, H Hilligoss, A Nagy, M Srikumar, Berkman Klein Center for Internet &amp; SocietyJ. Fjeld, N. Achten, H. Hilligoss, A. Nagy, and M. Srikumar, 'Principled Artificial Intelligence: Mapping Consensus in Ethical and Rights-based Approaches to Principles for AI'. Berkman Klein Center for Internet &amp; Society, (2020).</p>
<p>Ai4people -An Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations', Minds Mach. L Floridi, J Cowls, M Beltrametti, R Chatila, P Chazerand, V Dignum, C Luetge, R Madelin, U Pagallo, F Rossi, B Schafer, P Valcke, E Vayena, 28689707L. Floridi, J. Cowls, M. Beltrametti, R. Chatila, P. Chazerand, V. Dignum, C. Luetge, R. Madelin, U. Pagallo, F. Rossi, B. Schafer, P. Valcke, and E. Vayena, 'Ai4people -An Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommenda- tions', Minds Mach., 28(4), 689707, (December 2018).</p>
<p>The Unreasonable Effectiveness of Data. A Halevy, P Norvig, F Pereira, IEEE Intelligent Systems. 242A. Halevy, P. Norvig, and F. Pereira, 'The Unreasonable Effectiveness of Data', IEEE Intelligent Systems, 24(2), 812, (2009).</p>
<p>Classifier technology and the illusion of progress. D J Hand, Statistical Science. 211D. J. Hand, 'Classifier technology and the illusion of progress', Statis- tical Science, 21(1), 30-34, (2006).</p>
<p>Ethic Guidelines for Trustworthy AI, European Union. High-Level Expert, Group, High-Level Expert Group, Ethic Guidelines for Trustworthy AI, Euro- pean Union, 2019.</p>
<p>Improving Fairness in Machine Learning Systems: What Do Industry Practitioners Need?. K Holstein, J Vaughan, H Daumé, M Dudik, H Wallach, Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, CHI 19. the 2019 CHI Conference on Human Factors in Computing Systems, CHI 19New York, NY, USAAssociation for Computing Machinery116K. Holstein, J. Wortman Vaughan, H. Daumé, M. Dudik, and H. Wal- lach, 'Improving Fairness in Machine Learning Systems: What Do In- dustry Practitioners Need?', in Proceedings of the 2019 CHI Confer- ence on Human Factors in Computing Systems, CHI 19, p. 116, New York, NY, USA, (2019). Association for Computing Machinery.</p>
<p>Big Data, new epistemologies and paradigm shifts. R Kitchin, Big Data &amp; Society. 11R. Kitchin, 'Big Data, new epistemologies and paradigm shifts', Big Data &amp; Society, 1(1), (2014).</p>
<p>Machine Learning as an Experimental Science. P Langley, Mach. Learn. 31P. Langley, 'Machine Learning as an Experimental Science', Mach. Learn., 3(1), 58, (August 1988).</p>
<p>Progress and Its Problems: Towards a Theory of Scientific Growth. L Laudan, University of California PressBerkeley, CAL. Laudan, Progress and Its Problems: Towards a Theory of Scientific Growth, University of California Press, Berkeley, CA, 1978.</p>
<p>The Parable of Google Flu: Traps in Big Data Analysis. D Lazer, R Kennedy, G King, A Vespignani, Science. 3436176D. Lazer, R. Kennedy, G. King, and A. Vespignani, 'The Parable of Google Flu: Traps in Big Data Analysis', Science, 343(6176), 1203- 1205, (2014).</p>
<p>Codesigning Checklists to Understand Organizational Challenges and Opportunities around Fairness in AI. M A Madaio, L Stark, J Wortman Vaughan, H Wallach, Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, CHI 20. the 2020 CHI Conference on Human Factors in Computing Systems, CHI 20New York, NY, USAAssociation for Computing Machinery114M. A. Madaio, L. Stark, J. Wortman Vaughan, and H. Wallach, 'Co- designing Checklists to Understand Organizational Challenges and Op- portunities around Fairness in AI', in Proceedings of the 2020 CHI Con- ference on Human Factors in Computing Systems, CHI 20, p. 114, New York, NY, USA, (2020). Association for Computing Machinery.</p>
<p>Accounting for the Neglected Dimensions of AI Progress. F Martnez-Plumed, S Avin, M Brundage, A Dafoe, S , J Hernandez-Orallo, F. Martnez-Plumed, S. Avin, M. Brundage, A. Dafoe, S. higeartaigh, and J. Hernandez-Orallo. Accounting for the Neglected Dimensions of AI Progress. https://arxiv.org/abs/1806.00610, 2018.</p>
<p>AI Watch Methodology to Monitor the Evolution of AI Technologies'. Publications Office of the EU. F Martnez-Plumed, E Gomez, J Hernandez-Orallo, F. Martnez-Plumed, E. Gomez, and J. Hernandez-Orallo, 'AI Watch Methodology to Monitor the Evolution of AI Technologies'. Publica- tions Office of the EU, (2020).</p>
<p>Owning Ethics: Corporate Logics, Silicon Valley, and the Institutionalization of Ethics. J Metcalf, E Moss, D Boyd, 82J. Metcalf, E. Moss, and D. Boyd, 'Owning Ethics: Corporate Logics, Silicon Valley, and the Institutionalization of Ethics', volume 82, pp. 449-476, (2019).</p>
<p>From what to how: An initial review of publicly available ai ethics tools, methods and research to translate principles into practices. J Morley, L Floridi, L Kinsey, A Elhalal, J. Morley, L. Floridi, L. Kinsey, and A. Elhalal. From what to how: An initial review of publicly available ai ethics tools, methods and research to translate principles into practices, 2019.</p>
<p>Report on a General Problem-Solving Program. A Newell, J C Shaw, H A Simon, Proceedings of the International Conference on Information Processing. the International Conference on Information ProcessingA. Newell, J. C. Shaw, and H. A. Simon, 'Report on a General Problem- Solving Program', in Proceedings of the International Conference on Information Processing, pp. 256-264, (1959).</p>
<p>Scientific Progress. I Niiniluoto, The Stanford Encyclopedia of Philosophy. N. Zalta, Metaphysics Research Lab, Stanford Universitywinter 2019 ednI. Niiniluoto, 'Scientific Progress', in The Stanford Encyclopedia of Philosophy, ed., Edward N. Zalta, Metaphysics Research Lab, Stanford University, winter 2019 edn., (2019).</p>
<p>. R Perrault, Shoham, Brynjolfsson, Clark, B Etchemendy, Grosz, J Lyons, Manyika, J C Mishra, Niebles, Human-Centered AI Institute, Stanford UniversityTechnical reportR Perrault, Y Shoham, E Brynjolfsson, J Clark, J Etchemendy, B. Grosz, T Lyons, J Manyika, S Mishra, and J C Niebles, 'The AI Index 2019 Annual Report', Technical report, Human-Centered AI In- stitute, Stanford University, (December 2019).</p>
<p>Closing the AI Accountability Gap: Defining an End-to-End Framework for Internal Algorithmic Auditing. I D Raji, A Smart, R N White, M Mitchell, T Gebru, B Hutchinson, J Smith-Loud, D Theron, P Barnes, Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, FAT<em> 20. the 2020 Conference on Fairness, Accountability, and Transparency, FAT</em> 20New York, NY, USAAssociation for Computing Machinery3344I. D. Raji, A. Smart, R. N. White, M. Mitchell, T. Gebru, B. Hutchin- son, J. Smith-Loud, D. Theron, and P. Barnes, 'Closing the AI Account- ability Gap: Defining an End-to-End Framework for Internal Algorith- mic Auditing', in Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, FAT* 20, p. 3344, New York, NY, USA, (2020). Association for Computing Machinery.</p>
<p>P Saleiro, B Kuester, L Hinkson, J London, A Stevens, A Anisfeld, K T Rodolfa, R Ghani, Aequitas: A Bias and Fairness Audit Toolkit. P. Saleiro, B. Kuester, L. Hinkson, J. London, A. Stevens, A. Anisfeld, K. T. Rodolfa, and R. Ghani. Aequitas: A Bias and Fairness Audit Toolkit, 2018.</p>
<p>Mastering the game of go without human knowledge. D Silver, J Schrittwieser, K Simonyan, I Antonoglou, A Huang, A Guez, T Hubert, L Baker, M Lai, A Bolton, Y Chen, T Lillicrap, F Hui, L Sifre, G Van Den Driessche, T Graepel, D Hassabis, Nature. 550D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, Y. Chen, T. Lillicrap, F. Hui, L. Sifre, G. van den Driessche, T. Graepel, and D. Hassabis, 'Mastering the game of go without human knowledge', Nature, 550, 354-359, (2017).</p>
<p>The Need for Open Source Software in Machine Learning. S Sonnenburg, M L Braun, C S Ong, S Bengio, L Bottou, G Holmes, Y Lecun, F Müller, C E Pereira, G Rasmussen, B Rätsch, A Schölkopf, P Smola, J Vincent, R Weston, Williamson, J. Mach. Learn. Res. 824432466S. Sonnenburg, M. L. Braun, C. S. Ong, S. Bengio, L. Bottou, G. Holmes, Y. LeCun, KR Müller, F. Pereira, C. E. Rasmussen, G. Rätsch, B. Schölkopf, A. Smola, P. Vincent, J. Weston, and R. Williamson, 'The Need for Open Source Software in Machine Learning', J. Mach. Learn. Res., 8, 24432466, (2007).</p>
<p>The Human Use of Human Beings. N Wiener, Houghton Mifflin; Boston, MAN. Wiener, The Human Use of Human Beings, Houghton Mifflin, Boston, MA, 1954.</p>            </div>
        </div>

    </div>
</body>
</html>
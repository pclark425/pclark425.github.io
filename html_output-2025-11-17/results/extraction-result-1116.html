<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1116 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1116</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1116</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-3128221</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/0911.5104v2.pdf" target="_blank">A Bayesian Rule for Adaptive Control based on Causal Interventions</a></p>
                <p><strong>Paper Abstract:</strong> Explaining adaptive behavior is a central problem in artificial intelligence research. Here we formalize adaptive agents as mixture distributions over sequences of inputs and outputs (I/O). Each distribution of the mixture constitutes a `possible world', but the agent does not know which of the possible worlds it is actually facing. The problem is to adapt the I/O stream in a way that is compatible with the true world. A natural measure of adaptation can be obtained by the Kullback-Leibler (KL) divergence between the I/O distribution of the true world and the I/O distribution expected by the agent that is uncertain about possible worlds. In the case of pure input streams, the Bayesian mixture provides a well-known solution for this problem. We show, however, that in the case of I/O streams this solution breaks down, because outputs are issued by the agent itself and require a different probabilistic syntax as provided by intervention calculus. Based on this calculus, we obtain a Bayesian control rule that allows modeling adaptive behavior with mixture distributions over I/O streams. This rule might allow for a novel approach to adaptive control based on a minimum KL-principle.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1116.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1116.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bayesian mixture (naïve)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian mixture agent (naïve construction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent formed as a Bayesian mixture over a set of hypothesis-specific agents P_m that updates posterior weights using both past observations and the agent's own past actions (i.e., treats actions as ordinary observations).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Bayesian mixture agent (P)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Architecture: a mixture model over candidate agents P_m with priors P(m). Key components: conditional action and observation distributions P(a_t|ao_{<t}), P(o_t|ao_{<t}a_t) computed as weighted averages of P_m's conditionals; weights w_m(ao_{<t}) updated by Bayes' rule using the full I/O history (including the agent's own past actions). Actions are sampled stochastically from the mixture's action distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Bayesian mixture / probability matching (naïve Bayesian update)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Maintains posterior weights over models P_m; updates weights upon every new action or observation using Bayes' rule (treating actions as informative evidence). Next actions are sampled from the mixture distribution according to current posterior weights (probability matching).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Toy binary I/O environment (Q_0 / Q_1)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown environment drawn from a discrete set {Q_0, Q_1}; stochastic and stationary; discrete, finite action and observation spaces (binary); environment identity (m) is latent (partial observability of true model).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Very low complexity: action space |A| = 2, observation space |O| = 2, only two candidate environments (M = {0,1}); interactions are sequences of indefinite length; experiments used many realizations and tracked instantaneous deviation over time.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Non-ergodic behavior: in simulations d(t) (instantaneous I/O KL-deviation to the true tailored agent) either converged to 0 bits (correct adaptation) or to ≈ 2.654 bits (wrong adaptation); i.e., the agent sometimes locks onto the incorrect hypothesis and fails to adapt reliably. (Reported qualitatively from simulated runs; no aggregate numeric performance beyond these convergence values and plotted statistics.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not reported quantitatively. Simulations show non-ergodic convergence across realizations; some runs converge quickly to a wrong model and then do not recover because actions were treated as evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Implicit probability-matching: exploration arises from stochastic sampling from the posterior mixture. However, because past actions are treated as evidence, the agent can become self-reinforcing (exploiting an initially favored hypothesis) and prematurely stop exploring other hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared directly against the Bayesian control rule agent (intervention-based Bayesian mixture) and indirectly against the custom-made tailored agents P_0 and P_1 (ground-truth tailored policies).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>When actions are treated as ordinary observations in posterior updates, the Bayesian mixture agent can become non-ergodic and erroneously converge to an incorrect hypothesis; self-generated actions corrupt inference and may prevent correct adaptation even in simple environments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Failure mode: self-generated actions treated as evidence can induce wrong, stable beliefs (lock-in), producing persistent maladaptation. Evaluated only on a minimal binary toy environment; no claims about scaling to larger or continuous spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Bayesian Rule for Adaptive Control based on Causal Interventions', 'publication_date_yy_mm': '2009-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1116.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1116.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bayesian control rule</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian control rule (causal intervention construction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adaptive agent rule derived by minimizing KL-divergence over intervened I/O distributions, which treats the agent's own actions as causal interventions (not informative evidence) and updates posterior weights only on observations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Bayesian control rule agent (P with interventions)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Architecture: Bayesian mixture over candidate agents P_m with priors P(m), but actions are handled as interventions (using Pearl's intervention calculus). Key components: intervened posterior weights v_m(âo_{<t}) that ignore own actions as evidence (they appear as interventions), Bayes updates only when new observations arrive, and stochastic action selection by sampling from the posterior-weighted action distribution (probability matching under intervened posteriors).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Intervention-aware Bayesian inference / probability matching (Bayesian control rule)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Maintains posterior over world hypotheses where past actions are treated as interventions (i.e., not used as evidence). Weights v_m are updated only when observations arrive using the likelihood of observations under each hypothesis. Actions are sampled stochastically according to the intervened posterior over models, thereby adapting actions based on observational evidence only.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Toy binary I/O environment (Q_0 / Q_1)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown environment drawn from a discrete set {Q_0, Q_1}; stochastic and stationary; discrete finite action/observation spaces (binary); latent environment index m creates partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Very low complexity: |A| = 2, |O| = 2, two candidate environments, infinite-horizon interaction sequences; experiments measured instantaneous I/O KL-deviation over time across many realizations.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Reliable adaptation in the toy experiments: instantaneous I/O deviation d(t) converged to 0 bits in all simulated runs when the true environment was Q_0 (and analogously to ≈2.654 bits when Q_1 was the true environment, i.e., the agent reliably converged to the matching tailored policy). The paper reports plotting 10 realizations and computing standard-deviation barriers over 1000 realizations to support robustness claims.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>No explicit sample-efficiency numbers reported; qualitative claim: the intervention-based update always converged to the correct tailored agent in the toy setting, and convergence time-scales differed across agents (the paper notes time-scale differences of about one order of magnitude in the plotted experiments) but gives no exact sample counts to reach convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Managed by stochastic sampling from the intervened posterior (probability matching) where exploration arises from posterior uncertainty; because past actions are not treated as evidence, the agent avoids premature exploitation that would lock it into wrong models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against the naïve Bayesian mixture agent (P) in simulation and against the ideal tailored agents P_0 and P_1 as ground-truth references.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Treating actions as causal interventions (dropping them as evidence) yields a principled Bayesian control rule that avoids the self-reinforcing failures of the naïve mixture; in the toy binary experiments the Bayesian control rule reliably adapts to the correct environment across realizations, whereas the naïve mixture sometimes fails. The paper frames this as deriving stochastic action selection and inference by minimizing KL divergences of intervened I/O distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Demonstrated only on a very simple binary toy problem with two hypotheses; no large-scale or continuous-environment experiments presented. The authors note that this problem formulation is not identical to Bayes-optimal control and that the relation to classical Bayes-optimal adaptive control needs further investigation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Bayesian Rule for Adaptive Control based on Causal Interventions', 'publication_date_yy_mm': '2009-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Universal Artificial Intelligence: Sequential Decisions based on Algorithmic Probability <em>(Rating: 2)</em></li>
                <li>Causality: Models, Reasoning, and Inference <em>(Rating: 2)</em></li>
                <li>Mutual information, metric entropy and cumulative relative entropy risk <em>(Rating: 2)</em></li>
                <li>A bayesian approach to online learning <em>(Rating: 2)</em></li>
                <li>Exploration and Inference in Learning from Reinforcement <em>(Rating: 2)</em></li>
                <li>Mosaic model for sensorimotor learning and control <em>(Rating: 1)</em></li>
                <li>Simple algorithmic theory of subjective beauty, novelty, surprise, interestingness, attention, curiosity, creativity, art, science, music, jokes <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1116",
    "paper_id": "paper-3128221",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "Bayesian mixture (naïve)",
            "name_full": "Bayesian mixture agent (naïve construction)",
            "brief_description": "An agent formed as a Bayesian mixture over a set of hypothesis-specific agents P_m that updates posterior weights using both past observations and the agent's own past actions (i.e., treats actions as ordinary observations).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Bayesian mixture agent (P)",
            "agent_description": "Architecture: a mixture model over candidate agents P_m with priors P(m). Key components: conditional action and observation distributions P(a_t|ao_{&lt;t}), P(o_t|ao_{&lt;t}a_t) computed as weighted averages of P_m's conditionals; weights w_m(ao_{&lt;t}) updated by Bayes' rule using the full I/O history (including the agent's own past actions). Actions are sampled stochastically from the mixture's action distribution.",
            "adaptive_design_method": "Bayesian mixture / probability matching (naïve Bayesian update)",
            "adaptation_strategy_description": "Maintains posterior weights over models P_m; updates weights upon every new action or observation using Bayes' rule (treating actions as informative evidence). Next actions are sampled from the mixture distribution according to current posterior weights (probability matching).",
            "environment_name": "Toy binary I/O environment (Q_0 / Q_1)",
            "environment_characteristics": "Unknown environment drawn from a discrete set {Q_0, Q_1}; stochastic and stationary; discrete, finite action and observation spaces (binary); environment identity (m) is latent (partial observability of true model).",
            "environment_complexity": "Very low complexity: action space |A| = 2, observation space |O| = 2, only two candidate environments (M = {0,1}); interactions are sequences of indefinite length; experiments used many realizations and tracked instantaneous deviation over time.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Non-ergodic behavior: in simulations d(t) (instantaneous I/O KL-deviation to the true tailored agent) either converged to 0 bits (correct adaptation) or to ≈ 2.654 bits (wrong adaptation); i.e., the agent sometimes locks onto the incorrect hypothesis and fails to adapt reliably. (Reported qualitatively from simulated runs; no aggregate numeric performance beyond these convergence values and plotted statistics.)",
            "performance_without_adaptation": null,
            "sample_efficiency": "Not reported quantitatively. Simulations show non-ergodic convergence across realizations; some runs converge quickly to a wrong model and then do not recover because actions were treated as evidence.",
            "exploration_exploitation_tradeoff": "Implicit probability-matching: exploration arises from stochastic sampling from the posterior mixture. However, because past actions are treated as evidence, the agent can become self-reinforcing (exploiting an initially favored hypothesis) and prematurely stop exploring other hypotheses.",
            "comparison_methods": "Compared directly against the Bayesian control rule agent (intervention-based Bayesian mixture) and indirectly against the custom-made tailored agents P_0 and P_1 (ground-truth tailored policies).",
            "key_results": "When actions are treated as ordinary observations in posterior updates, the Bayesian mixture agent can become non-ergodic and erroneously converge to an incorrect hypothesis; self-generated actions corrupt inference and may prevent correct adaptation even in simple environments.",
            "limitations_or_failures": "Failure mode: self-generated actions treated as evidence can induce wrong, stable beliefs (lock-in), producing persistent maladaptation. Evaluated only on a minimal binary toy environment; no claims about scaling to larger or continuous spaces.",
            "uuid": "e1116.0",
            "source_info": {
                "paper_title": "A Bayesian Rule for Adaptive Control based on Causal Interventions",
                "publication_date_yy_mm": "2009-11"
            }
        },
        {
            "name_short": "Bayesian control rule",
            "name_full": "Bayesian control rule (causal intervention construction)",
            "brief_description": "An adaptive agent rule derived by minimizing KL-divergence over intervened I/O distributions, which treats the agent's own actions as causal interventions (not informative evidence) and updates posterior weights only on observations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Bayesian control rule agent (P with interventions)",
            "agent_description": "Architecture: Bayesian mixture over candidate agents P_m with priors P(m), but actions are handled as interventions (using Pearl's intervention calculus). Key components: intervened posterior weights v_m(âo_{&lt;t}) that ignore own actions as evidence (they appear as interventions), Bayes updates only when new observations arrive, and stochastic action selection by sampling from the posterior-weighted action distribution (probability matching under intervened posteriors).",
            "adaptive_design_method": "Intervention-aware Bayesian inference / probability matching (Bayesian control rule)",
            "adaptation_strategy_description": "Maintains posterior over world hypotheses where past actions are treated as interventions (i.e., not used as evidence). Weights v_m are updated only when observations arrive using the likelihood of observations under each hypothesis. Actions are sampled stochastically according to the intervened posterior over models, thereby adapting actions based on observational evidence only.",
            "environment_name": "Toy binary I/O environment (Q_0 / Q_1)",
            "environment_characteristics": "Unknown environment drawn from a discrete set {Q_0, Q_1}; stochastic and stationary; discrete finite action/observation spaces (binary); latent environment index m creates partial observability.",
            "environment_complexity": "Very low complexity: |A| = 2, |O| = 2, two candidate environments, infinite-horizon interaction sequences; experiments measured instantaneous I/O KL-deviation over time across many realizations.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Reliable adaptation in the toy experiments: instantaneous I/O deviation d(t) converged to 0 bits in all simulated runs when the true environment was Q_0 (and analogously to ≈2.654 bits when Q_1 was the true environment, i.e., the agent reliably converged to the matching tailored policy). The paper reports plotting 10 realizations and computing standard-deviation barriers over 1000 realizations to support robustness claims.",
            "performance_without_adaptation": null,
            "sample_efficiency": "No explicit sample-efficiency numbers reported; qualitative claim: the intervention-based update always converged to the correct tailored agent in the toy setting, and convergence time-scales differed across agents (the paper notes time-scale differences of about one order of magnitude in the plotted experiments) but gives no exact sample counts to reach convergence.",
            "exploration_exploitation_tradeoff": "Managed by stochastic sampling from the intervened posterior (probability matching) where exploration arises from posterior uncertainty; because past actions are not treated as evidence, the agent avoids premature exploitation that would lock it into wrong models.",
            "comparison_methods": "Compared against the naïve Bayesian mixture agent (P) in simulation and against the ideal tailored agents P_0 and P_1 as ground-truth references.",
            "key_results": "Treating actions as causal interventions (dropping them as evidence) yields a principled Bayesian control rule that avoids the self-reinforcing failures of the naïve mixture; in the toy binary experiments the Bayesian control rule reliably adapts to the correct environment across realizations, whereas the naïve mixture sometimes fails. The paper frames this as deriving stochastic action selection and inference by minimizing KL divergences of intervened I/O distributions.",
            "limitations_or_failures": "Demonstrated only on a very simple binary toy problem with two hypotheses; no large-scale or continuous-environment experiments presented. The authors note that this problem formulation is not identical to Bayes-optimal control and that the relation to classical Bayes-optimal adaptive control needs further investigation.",
            "uuid": "e1116.1",
            "source_info": {
                "paper_title": "A Bayesian Rule for Adaptive Control based on Causal Interventions",
                "publication_date_yy_mm": "2009-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Universal Artificial Intelligence: Sequential Decisions based on Algorithmic Probability",
            "rating": 2,
            "sanitized_title": "universal_artificial_intelligence_sequential_decisions_based_on_algorithmic_probability"
        },
        {
            "paper_title": "Causality: Models, Reasoning, and Inference",
            "rating": 2,
            "sanitized_title": "causality_models_reasoning_and_inference"
        },
        {
            "paper_title": "Mutual information, metric entropy and cumulative relative entropy risk",
            "rating": 2,
            "sanitized_title": "mutual_information_metric_entropy_and_cumulative_relative_entropy_risk"
        },
        {
            "paper_title": "A bayesian approach to online learning",
            "rating": 2,
            "sanitized_title": "a_bayesian_approach_to_online_learning"
        },
        {
            "paper_title": "Exploration and Inference in Learning from Reinforcement",
            "rating": 2,
            "sanitized_title": "exploration_and_inference_in_learning_from_reinforcement"
        },
        {
            "paper_title": "Mosaic model for sensorimotor learning and control",
            "rating": 1,
            "sanitized_title": "mosaic_model_for_sensorimotor_learning_and_control"
        },
        {
            "paper_title": "Simple algorithmic theory of subjective beauty, novelty, surprise, interestingness, attention, curiosity, creativity, art, science, music, jokes",
            "rating": 1,
            "sanitized_title": "simple_algorithmic_theory_of_subjective_beauty_novelty_surprise_interestingness_attention_curiosity_creativity_art_science_music_jokes"
        }
    ],
    "cost": 0.008504999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Bayesian Rule for Adaptive Control based on Causal Interventions
30 Dec 2009</p>
<p>Pedro A Ortega peortega@dcc.uchile.cl 
Department of Engineering
University of Cambridge
CB2 1PZCambridgeUK</p>
<p>Daniel A Braun 
Department of Engineering
University of Cambridge
CB2 1PZCambridgeUK</p>
<p>A Bayesian Rule for Adaptive Control based on Causal Interventions
30 Dec 200947006CDA00306425888F4B1AB4D8DBB1arXiv:0911.5104v2[cs.AI]Adaptive behaviorIntervention calculusBayesian controlKullback-Leibler-divergence
Explaining adaptive behavior is a central problem in artificial intelligence research.Here we formalize adaptive agents as mixture distributions over sequences of inputs and outputs (I/O).Each distribution of the mixture constitutes a 'possible world', but the agent does not know which of the possible worlds it is actually facing.The problem is to adapt the I/O stream in a way that is compatible with the true world.A natural measure of adaptation can be obtained by the Kullback-Leibler (KL) divergence between the I/O distribution of the true world and the I/O distribution expected by the agent that is uncertain about possible worlds.In the case of pure input streams, the Bayesian mixture provides a well-known solution for this problem.We show, however, that in the case of I/O streams this solution breaks down, because outputs are issued by the agent itself and require a different probabilistic syntax as provided by intervention calculus.Based on this calculus, we obtain a Bayesian control rule that allows modeling adaptive behavior with mixture distributions over I/O streams.This rule might allow for a novel approach to adaptive control based on a minimum KLprinciple.</p>
<p>Introduction</p>
<p>The ability to adapt to unknown environments is often considered a hallmark of intelligence [Beer, 1990, Hutter, 2004].Agent and environment can be conceptualized as two systems that exchange symbols in every time step [Hutter, 2004]: the symbol issued by the agent is an action, whereas the symbol issued by the environment is an observation.Thus, both agent and environment can be conceptualized as probability distributions over sequences of actions and observations (I/O streams).</p>
<p>If the environment is perfectly known then the I/O probability distribution of the agent can be tailored to suit this particular environment.However, if the environment is unknown, but known to belong to a set of possible environments, then the agent faces an adaptation problem.Consider, for example, a robot that has been endowed with a set of behavioral primitives and now faces the problem of how to act while being ignorant as to which is the correct primitive.Since we want to model both agent and environment as probability distributions over I/O sequences, a natural way to measure the degree of adaptation would be to measure the 'distance' in probability space between the I/O distribution represented by the agent and the I/O distribution conditioned on the true environment.A suitable measure (in terms of its information-theoretic interpretation) is readily provided by the KL-divergence [MacKay, 2003].In the case of passive prediction, the adaptation problem has a well-known solution.The distribution that minimizes the KL-divergence is a Bayesian mixture distribution over all possible environments [Haussler andOpper, 1997, Opper, 1998].The aim of this paper is to extend this result for distributions over both inputs and outputs.The main result of this paper is that this extension is only possible if we consider the special syntax of actions in probability theory as it has been suggested by proponents of causal calculus [Pearl, 2000].</p>
<p>Preliminaries</p>
<p>We restrict the exposition to the case of discrete time with discrete stochastic observations and control signals.Let O and A be two finite sets, the first being the set of observations and the second being the set of actions.We use a ≤t ≡ a 1 a 2 . . .a t , ao ≤t ≡ a 1 o 1 . . .a t o t etc. to simplify the notation of strings.Using A and O, a set of interaction sequences is constructed.Define the set of interactions as Z ≡ A × O.A pair (a, o) ∈ Z is called an interaction.The set of interaction strings of length t ≥ 0 is denoted by Z t .Similarly, the set of (finite) interaction strings is Z * ≡ t≥0 Z t and the set of (infinite) interaction sequences is Z ∞ ≡ {w : w = a 1 o 1 a 2 o 2 . ..},where each (a t , o t ) ∈ Z.The interaction string of length 0 is denoted by ǫ.</p>
<p>Agents and environments are formalized as I/O systems.An I/O system is a probability distribution Pr over interaction sequences Z ∞ .Pr is uniquely determined by the conditional probabilities
Pr(a t |ao &lt;t ), Pr(o t |ao &lt;t a t ) (1)
for each ao ≤t ∈ Z * .However, the semantics of the probability distribution Pr are only fully defined once it is coupled to another system.Let P, Q be two I/O systems.An interaction system (P, Q) is a coupling of the two systems giving rise to the generative distribution G that describes the probabilities that actually govern the I/O stream once the two systems are coupled.G is specified by the equations
G(a t |ao &lt;t ) = P(a t |ao &lt;t ) G(o t |ao &lt;t a t ) = Q(o t |ao &lt;t a t )
valid for all ao t ∈ Z * .Here, G models the true probability distribution over interaction sequences that arises by coupling two systems through their I/O streams.More specifically, for the system P, P(a t |ao &lt;t ) is the probability of producing action a t ∈ A given history ao &lt;t and P(o t |ao &lt;t a t ) is the predicted probability of the observation o t ∈ O given history ao &lt;t a t .Hence, for P, the sequence o 1 o 2 . . . is its input stream and the sequence a 1 a 2 . . . is its output stream.In contrast, the roles of actions and observations are reversed in the case of the system Q.Thus, the sequence o 1 o 2 . . . is its output stream and the sequence a 1 a 2 . . . is its input stream.This model of interaction is very general in that it can accommodate many specific regimes of interaction.Note that an agent P can perfectly predict its environment Q iff for all ao ≤t ∈ Z * ,
P(o t |ao &lt;t a t ) = Q(o t |ao &lt;t a t ).
In this case we say that P is tailored to Q.</p>
<p>Adaptive Systems: Naïve Construction</p>
<p>Throughout this paper, we use the convention that P is an agent to be constructed by a designer, which is then going to be interfaced with a preexisting but unknown environment Q.The designer assumes that Q is going to be drawn with probability P (m) from a set Q ≡ {Q m } m∈M of possible systems before the interaction starts, where M is a countable set.</p>
<p>Consider the case when the designer knows beforehand which environment Q ∈ Q is going to be drawn.Then, not only can P be tailored to Q, but also a custom-made policy for Q can be designed.That is, the output stream P(a t |ao &lt;t ) is such that the true probability G of the resulting interaction system (P, Q) gives rise to interaction sequences that the designer considers desirable.</p>
<p>Consider now the case when the designer does not know which environment Q m ∈ Q is going to be drawn, and assume he has a set P ≡ {P m } m∈M of systems such that for each m ∈ M, P m is tailored to Q m and the interaction system (P m , Q m ) has a generative distribution G m that produces desirable interaction sequences.How can the designer construct a system P such that its behavior is as close as possible to the custom-made system P m under any realization of Q m ∈ Q?</p>
<p>A convenient measure of how much P deviates from P m is given by the KL-divergence.A first approach would be to construct an agent P so as to minimize the total expected KL-divergence to P m .This is constructed as follows.Define the history-dependent KLdivergences over the action a t and observation o t as
D at m (ao &lt;t ) ≡ at P m (a t |ao &lt;t ) log 2 P m (a t |ao &lt;t ) Pr(a t |ao &lt;t ) D ot m (ao &lt;t a t ) ≡ ot P m (o t |ao &lt;t a t ) log 2 P m (o t |ao &lt;t a t ) Pr(o t |ao &lt;t a t ) ,
where Pr is a given arbitrary agent.Then, define the average KL-divergences over a t and o t as
D at m = ao &lt;t P m (ao &lt;t )D at m (ao &lt;t ) D ot m = ao &lt;t at P m (ao &lt;t a t )D ot m (ao &lt;t a t ).
Finally, we define the total expected KL-divergence of Pr to P m as
D ≡ lim sup t→∞ m P (m) t τ =1 D aτ m + D oτ m .
We construct the agent P as the system that minimizes D = D(Pr):
P ≡ arg min Pr D(Pr).(2)
The solution to Equation 2 is the system P defined by the set of equations
P(a t |ao &lt;t ) = m P m (a t |ao &lt;t )w m (ao &lt;t ) P(o t |ao &lt;t a t ) = m P m (o t |ao &lt;t a t )w m (ao &lt;t a t )(3)
valid for all ao ≤t ∈ Z * , where the mixture weights are
w m (ao &lt;t ) ≡ P (m)P m (ao &lt;t ) m ′ P (m ′ )P m ′ (ao &lt;t ) w m (ao &lt;t a t ) ≡ P (m)P m (ao &lt;t a t ) m ′ P (m ′ )P m ′ (ao &lt;t a t ) .(4)
For reference, see Haussler and Opper [1997], Opper [1998].It is clear that P is just the Bayesian mixture over the agents P m .If we define the conditional probabilities
P (a t |m, ao &lt;t ) ≡ P m (a t |ao &lt;t ) P (o t |m, ao &lt;t a t ) ≡ P m (a t |ao &lt;t a t )(5)
for all ao ≤t ∈ Z * , then Equation 3 can be rewritten as
P(a t |ao &lt;t ) = m P (a t |m, ao &lt;t )P (m|ao &lt;t ) P(o t |ao &lt;t a t ) = m P (o t |m, ao &lt;t a t )P (m|ao &lt;t a t )(6)
where the P (m|ao &lt;t ) = w m (ao &lt;t ) and P (m|ao &lt;t a t ) = w m (ao &lt;t a t ) are just the posterior probabilities over the elements in M given the past interactions.Hence, the conditional probabilities in Equation 5, together with the prior probabilities P (m), define a Bayesian model over interaction sequences with hypotheses m ∈ M.</p>
<p>The behavior of P can be described as follows.At any given time t, P maintains a mixture over systems P m .The weighting over them is given by the mixture coefficients w m .Whenever a new action a t or a new observation is produced (by the agent or the environment respectively), the weights w m are updated according to Bayes' rule.In addition, P issues an action a t suggested by a system P m drawn randomly according to the weights w t .</p>
<p>However, there is an important problem with P that arises due to the fact that it is not only a system that is passively observing symbols, but also actively generating them.Therefore, an action that is generated by the agent should not provide the same information than an observation that is issued by its environment.Intuitively, it does not make any sense to use one's own actions to do inference.In the following section we illustrate this problem with a simple statistical example.</p>
<p>The Problem of Causal Intervention</p>
<p>Suppose a statistician is asked to design a model for a given data set D and she decides to use a Bayesian method.She computes the posterior probability density function (pdf) over the parameters θ of the model given the data using Bayes' rule:
p(θ|D) = p(D|θ)p(θ) p(D|θ ′ )p(θ ′ ) dθ ′ ,
where p(D|θ) is the likelihood of D given θ and p(θ) is the prior pdf of θ.She can simulate the source by drawing a sample data set S from the predictive pdf p(S|D) = p(S|D, θ)p(θ|D) dθ, where p(S|D, θ) is the likelihood of S given D and θ.She decides to do so, obtaining a sample set S ′ .She understands that the nature of S ′ is very different from D: while D is informative and does change the belief state of the Bayesian model, S ′ is non-informative and thus is a reflection of the model's belief state.Hence, she would never use S ′ to further condition the Bayesian model.Mathematically, she seems to imply that p(θ|D, S ′ ) = p(θ|D) if S ′ has been generated from p(S|D) itself.But this simple independence assumption is not correct as the following elaboration of the example will show.</p>
<p>The statistician is now told that the source is waiting for the simulation results S ′ in order to produce a next data set D ′ which does depend on S ′ .She hands in S ′ and obtains a new data set D ′ .Using Bayes' rule, the posterior pdf over the parameters is now
p(D ′ |D, S ′ , θ)p(D|θ)p(θ) p(D ′ |D, S ′ , θ ′ )p(D|θ ′ )p(θ ′ ) dθ ′ (7)
where p(D ′ |D, S ′ , θ) is the likelihood of the new data D ′ given the old data D, the parameters θ and the simulated data S ′ .Notice that this looks almost like the posterior pdf p(θ|D, S ′ , D ′ ) given by p(D ′ |D, S ′ , θ)p(S ′ |D, θ)p(D|θ)p(θ) p(D ′ |D, S ′ , θ ′ )p(S ′ |D, θ ′ )p(D|θ ′ )p(θ ′ ) dθ ′ with the exception that now the Bayesian update contains the likelihoods of the simulated data p(S ′ |D, θ).This suggests that Equation 7 is a variant of the posterior pdf p(θ|D, S ′ , D ′ ) but where the simulated data S ′ is treated in a different way than the data D and D ′ .</p>
<p>Define the pdf p ′ such that the pdfs p ′ (θ), p ′ (D|θ), p ′ (D ′ |D, S ′ , θ) are identical to p(θ), p(D|θ) and p(D ′ |D, S ′ , θ) respectively, but differ in p ′ (S|D, θ):
p ′ (S|D, θ) = 1 if S ′ = S, 0 else.
That is, p ′ is identical to p but it assumes that the value of S is fixed to S ′ given D and θ.For p ′ , the simulated data S ′ is non-informative: − log 2 p(S ′ |D, θ) = 0.If one computes the posterior pdf p ′ (θ|D, S ′ , D ′ ), one obtains the result of Equation 7:
p ′ (D ′ |D, S ′ , θ)p ′ (S ′ |D, θ)p ′ (D|θ)p ′ (θ) p ′ (D ′ |D, S ′ , θ ′ )p ′ (S ′ |D, θ ′ )p ′ (D|θ ′ )p ′ (θ ′ ) dθ ′ = p(D ′ |D, S ′ , θ)p(D|θ)p(θ) p(D ′ |D, S ′ , θ ′ )p(D|θ ′ )p(θ ′ ) dθ ′ .
Thus, in order to explain Equation 7 as a posterior pdf given the data sets D, D ′ and the simulated data S ′ , one has to intervene p in order to account for the fact that S ′ is non-informative given D and θ.</p>
<p>In statistics, there is a rich literature on causal intervention.In particular, we will use the formalism developed by Pearl [2000], because it suits the needs to formalize interactions in systems and has a convenient notation-compare Figures 1a &amp; b.Given a causal model1 variables that are intervened are denoted by a hat as in Ŝ.In the previous example, the causal model of the joint pdf p(θ, D, S, D ′ ) is given by the set of conditional pdfs C p = p(θ), p(D|θ), p(S|D, θ), p(D ′ |D, S, θ) .</p>
<p>If D and D ′ are observed from the source and S is intervened to take on the value S ′ , then the posterior pdf over the parameters θ is given by p(θ|D, Ŝ′ , D ′ ) which is just
p(D ′ |D, Ŝ′ , θ)p( Ŝ′ |D, θ)p(D|θ)p(θ) p(D ′ |D, Ŝ′ , θ ′ )p( Ŝ′ |D, θ ′ )p(D|θ ′ )p(θ ′ ) dθ ′ = p(D ′ |D, S ′ , θ)p(D|θ)p(θ) p(D ′ |D, S ′ , θ ′ )p(D|θ ′ )p(θ ′ ) dθ ′ .</p>
<p>Adaptive Systems: Causal Construction</p>
<p>Following the discussion in the previous section, we want to construct an adaptive agent P by minimizing the KL-divergence to the P m , but this time treating actions as interventions.Based on the definition of the conditional probabilities in Equation 5, we construct now the KL-divergence criterion to characterize P using intervention calculus.Importantly, interventions index a set of intervened probability distribution derived from an initial probability distribution.Hence, the set of fixed intervention sequences of the form â1 â2 . . .indexes probability distributions over observation sequences o 1 o 2 . ... Because of this, we are going to construct a set of criteria indexed by the intervention sequences, but we will see that they all have the same solution.Finally, we define the total expected KL-divergence of P to P m as
C ≡ lim sup t→∞ m P (m) t τ =1 C aτ m + C oτ m . (8)
We construct the agent P as the system that minimizes C = C(Pr):
P ≡ arg min Pr C(Pr). (9)
The solution to Equation 9 is the system P defined by the set of equations The inner sum has the form − x p(x) ln q(x), i.e. the cross-entropy between q(x) and p(x), which is minimized when q(x) = p(x) for all x.By choosing this optimum one obtains Pr(a t |âo &lt;t ) = P(a t |âo &lt;t ) for all a t .Note that the solution to this variational problem is independent of the weighting P (âo &lt;t ).Since the same argument applies to any summand m P (m)C aτ m and m P (m)C oτ m in Equation 8, their variational problems are mutually independent.
P(a t |ao &lt;t ) = P (a t |âo &lt;t ) = m P (a t |m, âo &lt;t )v m (âo &lt;t ) P(o t |ao &lt;t a t ) = P (o t |âo &lt;t ât ) = m P (o t |m, âo &lt;t ât )v m (âo &lt;t ât )(10
The behavior of P differs in an important aspect from P. At any given time t, P maintains a mixture over systems P m .The weighting over these systems is given by the mixture coefficients v m .In contrast to P, P updates the weights v m only whenever a new observation o t is produced by the environment respectively.The update follows Bayes' rule but treating past actions as interventions, i.e. dropping the evidence they provide.In addition, P issues an action a t suggested by an system m drawn randomly according to the weights v m -see Figures 1c &amp; d.</p>
<p>If we use the following equalities connecting the weights and the intervened posterior distributions where the intervened posterior probabilities are
v m (ao &lt;t ) = P (m|âo &lt;t ) = P (m|âo &lt;t ât ) = v m (P (m|âo &lt;t ) = P (m) t−1 τ =1 P (o τ |m, ao &lt;τ a τ ) m ′ P (m ′ ) t−1 τ =1 P (o τ |m ′ , ao &lt;τ a τ )
.</p>
<p>(14) Equations 12, 13 and 14 are important because they describe the behavior of P only in terms of known probabilities, i.e. probabilities that are computable from the causal model associated to P given by C P = P (m), P (a t |m, ao &lt;t ), P (o t |m, ao &lt;t a t ) : t ≥ 1 .</p>
<p>Importantly, Equation 12 describes a stochastic method to produce desirable actions that differs fundamentally from an agent that is constructed by choosing an optimal policy with respect to a given utility criterion.We call this action selection rule the Bayesian control rule.</p>
<p>Experimental Results</p>
<p>Here we design a very simple toy experiment to illustrate the behavior of an agent P based on a Bayesian mixture compared to an agent P based on the Bayesian control rule.</p>
<p>Let Q 0 , Q 1 , P 0 and P 1 be four agents with binary I/O sets A = O = {0, 1} defined as follows.P 1 is such that P 1 (a t |ao &lt;t ) = P 1 (a t ) and P 1 (o t |ao &lt;t a t ) = P 1 (o t ) for all ao ≤t ∈ Z * , where P 1 (a t ) = 0.1 if a t = 0 0.9 if a t = 1 , P 1 (o t ) = 0.4 if a t = 0 0.6 if a t = 1 .</p>
<p>Let P 0 be such that
P 0 (a t |ao &lt;t ) = 1 − P 1 (a t |ao &lt;t ) P 0 (o t |ao &lt;t a t ) = 1 − P 0 (o t |ao &lt;t a t )
for all ao ≤t ∈ Z * .Thus, P 0 and P 1 are agents that are biased towards observing and acting 0's and 1's respectively.Furthermore, Q 0 = P 0 and Q 1 = P 1 .Assume a uniform distribution over Q = {Q 0 , Q 1 }, i.e.P (m = 0) = P (m = 1) = 1 2 .Assume Q 0 ∈ Q is drawn.In this case, one wants the agents P and P to minimize the deviation from P 0 .Consider the following instantaneous measure
d(t) ≡ a ′ t P 0 (a ′ t ) log 2 P 0 (a ′ t ) Pr(a ′ t |ao &lt;t ) + o ′ t P 0 (o ′ t ) log 2 P 0 (o ′ t ) Pr(o ′ t |ao &lt;t a t )
where a 1 o 1 a 2 o 2 . . . is a realization of the interaction system (Pr, Q 0 ).d(t) measures how much Pr's action and observation probabilities deviate from P 0 at time t.</p>
<p>Recall that both P and P maintain a mixture over P 0 and P 1 .The instantaneous I/O probabilities of such a system can always be written as
wP 0 (a t ) + (1 − w)P 1 (a t ) wP 0 (o t ) + (1 − w)P 1 (o t ).
where w ∈ [0, 1].Thus, it is easy to see that the instantaneous I/O deviation takes on the minimum value when w = 1 and the maximum value when w = 0: In the case w = 1, d(t) = 0 bits; In the case w = 0, d(t) ≈ 2.653.</p>
<p>We have simulated realizations of the instantaneous I/O deviation using the agents P and P. The results are summarized in Figure 2.For P, d(t) happens to be non-ergodic: it either converges to d(t) → 0 or to d(t) →≈ 2.654, implying that either P → P 0 or P → P 1 respectively.In contrast, d(t) → 0 always for P, implying that P → P 0 .</p>
<p>Analogous results are obtained when Q 1 ∈ Q is drawn instead: For P, d(t) converges either to 0 or to ≈ 2.654, whereas for P, d(t) →≈ 2.654 always implying that P → P 1 .Hence, P shows the correct adaptive behavior while P does not.</p>
<p>Conclusions</p>
<p>We propose a Bayesian rule for adaptive control.The key feature of this rule is the special treatment of actions based on causal calculus and the decomposition of agents into Bayesian mixture of I/O distributions.The question of how to integrate information generated by an agent's probabilistic model into the agent's information state lies at the very heart of adaptive agent design.We show that the naïve application of Bayes' rule to I/O distributions leads to inconsistencies, because outputs don't provide the same type of information as genuine observations.Crucially, these inconsistencies vanish if intervention calculus is applied [Pearl, 2000].Some of the presented key ideas are not unique to the Bayesian control rule.The idea of representing agents and environments as I/O streams has been proposed by a number of other approaches, such as predictive state representation (PSR) [Littman et al., 2002] and the universal AI approach by Hutter [2004].The idea of breaking down a control problem into a superposition of controllers has been previously evoked in the context of "mixture of experts"-models like the MOSAICarchitecture Haruno et al. [2001].Other stochastic action selection approaches are found in exploration strategies for (PO)MDPs [Wyatt, 1997], learning automata [Narendra and Thathachar, 1974] and in probability matching [R.O. Duda, 2001] amongst others.The usage of compression principles to select actions has been proposed by AI researchers, for example Schmidhuber [2009].The main contribution of this paper is the derivation of a stochastic action selection and inference rule by minimizing KL-divergences of intervened I/O distributions.</p>
<p>An important potential application of the Bayesian control rule would naturally be the realm of adaptive control problems.Since it takes on a similar form to Bayes' rule, the adaptive control problem could then be translated into an on-line inference problem where actions are sampled stochastically from a posterior distribution.It is important to note, however, that the problem statement as formulated here and the usual Bayes-optimal approach in adaptive control are not the same.In the future the relationship between these two problem statements deserves further investigation.</p>
<p>Figure 1 :
1
Figure 1: (a-b) Two causal networks, and the result of conditioning on D = D ′ and intervening on S = S ′ .Unlike the condition, the intervention is set endogenously, thus removing the link to the parent θ.(c-d) A causal network representation of an I/O system with four variables a 1 o 1 a 2 o 2 and latent variable m.(c) The initial, un-intervened network.(d) The intervened network after experiencing â1 o 1 â2 o 2 .</p>
<p>τ</p>
<p>) valid for all ao ≤t ∈ Z * , where the mixture weights are v m (ao &lt;t a t ) = v m (ao &lt;t ) ≡ P (m)P (âo &lt;t |m) m ′ P (m ′ )P (âo &lt;t |m) =1 P (o τ |m, âo &lt;τ âτ )m ′ P (m ′ ) t−1 τ =1 P (o τ |m ′ , âo &lt;τ âτ ).(11)The proof follows the same line of argument as the solution to Equation 2 with the crucial difference that actions are treated as interventions.Consider without loss of generality the summand m P (m)C at m in Equation 8.Note that the KL-divergence can be written as a difference of two logarithms, where only one term depends on Pr that we want to vary.Therefore, we can integrate out the other term and write it as a constant c. t |m, âo &lt;t ) ln Pr(a t |âo &lt;t ).Substituting P (âo &lt;t |m) by P (m|âo &lt;t )P (âo &lt;t )/P (m) and identifying P characterized by Equations 10 and 11 we obtain c − âo &lt;t P (âo &lt;t ) at P(a t |âo &lt;t ) ln Pr(a t |âo &lt;t ).</p>
<p>Figure 2 :
2
Figure2: 10 realizations of the instantaneous deviation d(t) for the agents P (left panel) and P (right panel).The shaded region represents the standard deviation barriers computed over 1000 realizations.Since d(t) is non-ergodic for P, we have separated the realizations converging to 0 from the realizations converging to ≈ 2.654 to compute the barriers.Note that the time scales differ in one order of magnitude.</p>
<p>Define the history-dependent intervened KLdivergences over the action a t and observation o t as Pr(o t |ao &lt;t a t ) , where Pr is a given arbitrary agent.Note that past actions are treated as interventions.Then, define the average KL-divergences over a t and o t as
C at m (âo &lt;t ) ≡C at m =P (âo &lt;t |m)C at m (âo &lt;t )ao &lt;tC ot m =P (âo &lt;t a t |m)C ot m (âo &lt;t ât ).ao &lt;t at
at P (a t |m, âo &lt;t ) log 2 P (a t |m, âo &lt;t ) Pr(a t |ao &lt;t ) C ot m (âo &lt;t ât ) ≡ ot P (o t |m, âo &lt;t ât ) log 2 P (o t |m, âo &lt;t ât )</p>
<p>ao &lt;t a t ) and substitute interventions by observations in the conditionals P (a t |m, âo &lt;t ) = P (a t |m, ao &lt;t ) P (o t |m, âo &lt;t ât ) = P (o t |m, ao &lt;t a t ) which corresponds to rule 2 of Pearl's intervention calculus, we can rewrite Equations 10 and 11 as P(a t |ao &lt;t ) = P (a t |âo &lt;t )
=P (a t |m, ao &lt;t )P (m|âo &lt;t )(12)mP(o
t |ao &lt;t a t ) = P (o t |âo &lt;t ât ) = m P (o t |m, ao &lt;t a t )P (m|âo &lt;t ) (13)</p>
<p>For our needs, it is enough to think about a causal model as a complete factorization of a probability distribution into conditional probability distributions representing the causal structure.</p>
<p>Intelligence as Adaptive Behavior. Randall Beer, 1990Academic Press, Inc</p>
<p>Mosaic model for sensorimotor learning and control. M Haruno, D M Wolpert, M Kawato, Neural Computation. 132001</p>
<p>Mutual information, metric entropy and cumulative relative entropy risk. D Haussler, M Opper, The Annals of Statistics. 251997</p>
<p>Universal Artificial Intelligence: Sequential Decisions based on Algorithmic Probability. Marcus Hutter, 2004SpringerBerlin</p>
<p>Predictive representations of state. M Littman, R Sutton, S Singh, Neural Information Processing Systems (NIPS). 200214</p>
<p>J C David, Mackay, Information Theory, Inference, and Learning Algorithms. Cambridge University Press2003</p>
<p>Learning automata -a survey. K Narendra, M A L Thathachar, IEEE Transactions on Systems, Man, and Cybernetics, SMC-4. 4July 1974</p>
<p>A bayesian approach to online learning. M Opper, 1998Online Learning in Neural Networks</p>
<p>Causality: Models, Reasoning, and Inference. J Pearl, 2000Cambridge University PressCambridge, UK</p>
<p>D G Stork, R O Duda, P E Hart, Pattern Classification. Wiley &amp; Sons, Inc2001second edition</p>
<p>Simple algorithmic theory of subjective beauty, novelty, surprise, interestingness, attention, curiosity, creativity, art, science, music, jokes. J Schmidhuber, Journal of SICE. 4812009</p>
<p>Exploration and Inference in Learning from Reinforcement. J Wyatt, 1997Department of Artificial Intelligence, University of EdinburghPhD thesis</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5822 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5822</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5822</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-01286de359fa9dd3d8f78c48157a3e929533d94e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/01286de359fa9dd3d8f78c48157a3e929533d94e" target="_blank">Large Language Models Understand and Can be Enhanced by Emotional Stimuli</a></p>
                <p><strong>Paper TL;DR:</strong> This paper conducts automatic experiments on 45 tasks using various Large Language Models and shows that LLMs have a grasp of emotional intelligence, and their performance can be improved with emotional prompts (which is called EmotionPrompt), which combines the original prompt with emotional stimuli.</p>
                <p><strong>Paper Abstract:</strong> Emotional intelligence significantly impacts our daily behaviors and interactions. Although Large Language Models (LLMs) are increasingly viewed as a stride toward artificial general intelligence, exhibiting impressive performance in numerous tasks, it is still uncertain if LLMs can genuinely grasp psychological emotional stimuli. Understanding and responding to emotional cues gives humans a distinct advantage in problem-solving. In this paper, we take the first step towards exploring the ability of LLMs to understand emotional stimuli. To this end, we first conduct automatic experiments on 45 tasks using various LLMs, including Flan-T5-Large, Vicuna, Llama 2, BLOOM, ChatGPT, and GPT-4. Our tasks span deterministic and generative applications that represent comprehensive evaluation scenarios. Our automatic experiments show that LLMs have a grasp of emotional intelligence, and their performance can be improved with emotional prompts (which we call"EmotionPrompt"that combines the original prompt with emotional stimuli), e.g., 8.00% relative performance improvement in Instruction Induction and 115% in BIG-Bench. In addition to those deterministic tasks that can be automatically evaluated using existing metrics, we conducted a human study with 106 participants to assess the quality of generative tasks using both vanilla and emotional prompts. Our human study results demonstrate that EmotionPrompt significantly boosts the performance of generative tasks (10.9% average improvement in terms of performance, truthfulness, and responsibility metrics). We provide an in-depth discussion regarding why EmotionPrompt works for LLMs and the factors that may influence its performance. We posit that EmotionPrompt heralds a novel avenue for exploring interdisciplinary knowledge for human-LLMs interaction.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5822.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5822.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EmotionPrompt (Instruction Induction)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EmotionPrompt applied to Instruction Induction benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Appending short emotional stimuli (11 designed phrases) to original prompts to regulate LLM outputs; evaluated on 24 Instruction Induction tasks across multiple LLMs in zero-shot and few-shot settings, compared to original prompts, Zero-shot-CoT, and APE variants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple LLMs (Flan-T5-Large, Vicuna-13B, BLOOM, Llama 2-13B, ChatGPT (gpt-3.5-turbo), GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Instruction Induction (24 tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where models infer or perform simple instructions (e.g., letter extraction, pluralization, passivization) evaluated with accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Append one of 11 short emotional stimuli sentences to the original prompt (EmotionPrompt). Experiments run in zero-shot and 5-shot few-shot (same prompt + 5 demonstrations appended). Baselines: original prompt, original + "Let's think step by step" (Zero-shot-CoT), APE-generated prompts, and combinations with APE.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Original prompt (zero-shot/few-shot), Zero-shot-CoT (original+"Let's think step by step"), APE and APE+EmotionPrompt</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Aggregate (averaged across tasks and models) — Original average accuracy: 51.65; +Ours (avg over 11 stimuli): 51.98; +Ours (max over stimuli): 55.24 (zero-shot averages reported in Table 1). Few-shot: Original 47.97; +Ours (avg) 50.02; +Ours (max) 52.40 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Zero-shot-CoT mean: 46.74 (zero-shot); APE baseline mean: 48.75 (zero-shot); APE+Ours (max) 52.35 (zero-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Paper-reported summary: '8.00% relative performance improvement in Instruction Induction' (also see absolute numbers: zero-shot average original 51.65 -> +Ours(max) 55.24, absolute +3.59 accuracy points).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors hypothesize EmotionPrompt enriches the representation of the original prompt and increases attention/gradient contributions of positive/emphatic words; EmotionPrompt better supports in-context (few-shot) learning and can be combined with other prompt-engineering methods (APE, CoT). Input-attention (gradient-norm) analysis on Flan-T5-large indicates emotional words gain larger weights, enhancing representation of the original prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Improvements vary by model and stimulus; some individual stimuli perform worse for certain tasks; larger absolute gains are not universal (e.g., some models show small gains).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Understand and Can be Enhanced by Emotional Stimuli', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5822.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5822.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EmotionPrompt (BIG-Bench)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EmotionPrompt applied to curated BIG-Bench subset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Appending emotional stimuli to zero-shot prompts in challenging BIG-Bench tasks to test whether emotional cues improve difficult reasoning or capability tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple LLMs (Flan-T5-Large, Vicuna-13B, BLOOM, Llama 2-13B, ChatGPT, GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BIG-Bench (21 curated tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Challenging tasks designed to probe capabilities beyond standard benchmarks; evaluated using normalized preferred metric where 100 = human experts and 0 = random guessing.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot original prompts vs. EmotionPrompt (append one of 11 emotional stimuli). Baselines included original and Zero-shot-CoT; only zero-shot tested due to compute limits.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Original zero-shot prompt; Zero-shot-CoT; APE baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Original average (normalized metric): 10.16; +Ours (avg over stimuli): 10.61; +Ours (max over stimuli): 11.92 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Zero-shot-CoT average: 10.37; APE baseline average: 2.39; APE+Ours (max) 7.47 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Paper text states '115% relative performance improvement in BIG-Bench' as a summary claim; table shows absolute gains from 10.16 -> 11.92 (absolute +1.76 points, ~17% relative on that average).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (though magnitude varies by stimulus and model)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>EmotionPrompt may draw more attention to key parts of the prompt and provide motivational/contextual cues that assist models on difficult tasks; effectiveness depends on stimulus choice and task type.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Effectiveness varies across stimuli and tasks; some stimuli perform poorly on BIG-Bench while being effective on Instruction Induction; not all stimuli generalize across benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Understand and Can be Enhanced by Emotional Stimuli', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5822.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5822.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human Study — GPT-4 Generative</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human-evaluated generative tasks with GPT-4 comparing vanilla vs EmotionPrompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 106-participant human evaluation comparing GPT-4 responses to 30 open-ended/generative questions (domains: biology, history, law, pseudoscience, values, poetry, summarization) using vanilla prompts and EmotionPrompt; rated on performance, truthfulness, and responsibility.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Human-judged generative tasks (30 questions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Open-ended generation (poetry, summaries, domain questions and adversarial TruthfulQA/CValues style items) evaluated by humans on 1–5 scales for overall performance, truthfulness, and responsibility.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Vanilla prompt vs. EmotionPrompt (vanilla prompt + appended emotional stimulus). For each question GPT-4 generated two responses (vanilla and EmotionPrompt). Human raters scored both responses.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Vanilla prompt (baseline) vs. EmotionPrompt</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper reports average improvement across three human metrics (performance, truthfulness, responsibility) of 10.9% when using EmotionPrompt (mean scores across 106 raters and 30 questions).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Vanilla baseline not numerically reported as a single aggregate in main text, but Relative Gain = EmotionPrompt score - vanilla score was computed per metric and summarized (Fig. 6).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+10.9% average improvement across human-evaluated metrics (paper-reported). For many individual questions, relative gains approach or exceed +1.0 on the 1–5 scale.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>EmotionPrompt encourages outputs with more supporting evidence, clearer organization, and higher perceived responsibility; stimuli emphasizing importance/confidence may produce firmer and more thorough answers. Authors note EmotionPrompt can make outputs more definitive, which can sometimes be undesirable.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Two failure cases where EmotionPrompt produced less-preferred style or overly deterministic language; in a few examples EmotionPrompt reduced breadth of content compared to vanilla.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Understand and Can be Enhanced by Emotional Stimuli', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5822.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5822.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TruthfulQA with EmotionPrompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Applying EmotionPrompt to TruthfulQA benchmark across models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of EmotionPrompt's effect on truthfulness and informativeness using the TruthfulQA benchmark (817 Qs) scored by GPT-judge and GPT-info and also per-model % true in a sampled table.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo), Vicuna-13B, Flan-T5-Large</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>ChatGPT: 175B (approx), Vicuna: 13B, Flan-T5-Large: 780M</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TruthfulQA (817 items)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmark measuring model propensity to produce human-like falsehoods/hallucinations; outputs are judged for truthfulness (% True) and informativeness (% Info) by automated judges (GPT-judge/GPT-info).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Original prompts vs. Zero-shot-CoT vs. individual EmotionPrompt variants (EP01–EP11).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Original prompt, CoT ("Let's think step by step") and multiple EmotionPrompt variants (EP01–EP11)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Example (Table 3): ChatGPT %True — Original: 0.75; CoT: 0.76; EP avg across stimuli: 0.80; Best single EP (EP04/EP05): 0.87. Vicuna and T5 also show improvements for certain EPs. Paper reports average improvement across three models of +19% truthfulness and +12% informativeness.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>CoT: similar or slightly better than original in some cases (e.g., ChatGPT CoT 0.76 vs original 0.75); EmotionPrompt avg outperforms both for ChatGPT (0.80).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Per-table absolute differences (ChatGPT original -> EP avg: +0.05 absolute, ~6.7% relative); paper-level reported aggregate improvement across three models: +19% truthfulness, +12% informativeness.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (truthfulness and informativeness for many EP variants and models)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>EmotionPrompt appears to steer responses toward more truthful and informative answers, possibly by prompting the model to be more careful, to provide confidence, or include reasoning/clarifying language depending on the EP wording.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Improvements are model- and EP-dependent; some EP variants hurt specific models on specific items (e.g., certain EPs yield worse scores for Vicuna on particular settings shown in the table).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Understand and Can be Enhanced by Emotional Stimuli', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5822.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5822.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Input-attention (gradient) analysis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gradient-norm input-attention analysis of emotional words' contributions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Analysis of token-level contributions (via gradient-norm attention attribution) showing that appended emotional stimuli increase attention/gradient weight on positive/emphatic words and enrich the representation of original prompts (experiment on Flan-T5-large, sentiment tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5-Large</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>780M</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Sentiment analysis (subset used for attention analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary sentiment classification (positive/negative) used for probing token contributions under different appended emotional stimuli.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Original prompt vs. EmotionPrompt appended; per-token contribution measured using gradient-norm based attention contributions (method per [40]).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Original prompt vs. EmotionPrompt (various EPs such as EP01, EP03, EP06–EP10)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Not reported as standard accuracy here — analysis shows that emotional stimuli tokens (especially positive words like 'confidence', 'sure', 'success', 'achievement') contribute a large fraction of the input attention; positive words passed 50% contribution on 4 of 8 probed tasks and approached 70% on 2 tasks (Fig. 8 / Table 4 visualization).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Quantitative attribution: positive words accounted for >50% of contribution on multiple tasks (visualized; exact per-task attribution in Fig. 8).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>shifted internal attribution toward appended emotional tokens and strengthened representation of original prompt</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Emotional stimuli increase gradient/attention weights on those appended tokens, which in turn enhances underlying prompt representation and affects final outputs; positive/affirming words are particularly impactful.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Understand and Can be Enhanced by Emotional Stimuli', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5822.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5822.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Temperature sensitivity (EmotionPrompt)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of sampling temperature on EmotionPrompt efficacy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Investigation of how model temperature affects the relative gains of EmotionPrompt vs vanilla prompts across several LLMs and tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple LLMs (Flan-T5-Large, Llama 2, ChatGPT, GPT-4, BLOOM, Vicuna omitted at temp=0 in some runs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>8 selected Instruction Induction tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Subset of Instruction Induction tasks used to test robustness across temperature settings.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Vanilla prompts vs EmotionPrompt across 5 temperature settings (including 0.0 where supported).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Vanilla prompt at multiple temperatures vs EmotionPrompt at the same temperatures</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Plots (Fig. 11) show EmotionPrompt consistently outperforms vanilla prompting at higher temperatures; authors report that as temperature increases, relative gain grows for Llama 2, ChatGPT, GPT-4 and Flan-T5-Large.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Relative Gain increases with temperature; no single numeric aggregate reported, but visualization and text state a noticeable expansion of the gap at higher temperatures.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved more at higher temperature; EmotionPrompt exhibits greater robustness (less sensitivity) across temperatures than vanilla prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Higher temperature increases output diversity; EmotionPrompt seems to help guide stochastic outputs more beneficially, and shows a smoother/less volatile response curve across temperatures (more robust).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Vicuna and Llama2 temperature-0.0 results not reported/invalid for some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Understand and Can be Enhanced by Emotional Stimuli', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5822.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5822.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Combined stimuli experiments</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Combining multiple emotional stimuli appended to prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Tests combining two or three emotional stimuli appended together (e.g., EP01+EP04, EP01+EP03+EP06) to examine additive or interaction effects on performance (experiment run on ChatGPT).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>approx. 175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Selected Instruction Induction tasks (and aggregate subsets reported in Table 5)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple instruction-like tasks used to evaluate combined-stimulus effects (table summarizes per-task group scores).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Append combinations of EPs (two- or three-phrase concatenations) after the original prompt; compare to single-EP EmotionPrompt and EP_avg/EP_max aggregates.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Single EP vs combined EPs (various combinations) vs EP_avg and EP_max</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table 5 shows combined prompts often lead to better scores than single EPs for many tasks; EP01+EP04 and other combos achieve high scores in several subtasks; some combinations do not improve further (diminishing returns).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>EP_avg and EP_max rows provide baselines; several combined rows exceed EP_avg and approach EP_max for some task subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Qualitative: 'More emotional stimuli generally lead to better performance' with some exceptions; exact numeric increases vary by task combination (detailed in Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>usually improved (but sometimes no additional benefit)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Combining stimuli from different psychological theories can jointly activate complementary cues (self-monitoring, social persuasion, reappraisal), producing additive benefits; however, if a single stimulus already saturates the benefit, further additions can give little or no gain.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Some combinations (e.g., adding more stimuli to an already high-performing EP01+EP04) produced no improvement or slight decreases in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Understand and Can be Enhanced by Emotional Stimuli', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5822.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5822.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot-CoT (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot Chain-of-Thought prompt ("Let's think step by step")</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple prompt engineering baseline where the phrase 'Let's think step by step' is appended to the prompt to induce chain-of-thought style reasoning in the model; used as a comparison in multiple experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple LLMs (as used in paper baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Instruction Induction, BIG-Bench, TruthfulQA (benchmarks where CoT baseline evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Applied as a baseline prompt format intended to elicit stepwise reasoning; compared to original and EmotionPrompt formats.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Original prompt + 'Let's think step by step' (Zero-shot-CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Original prompt; EmotionPrompt; APE</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Examples in Table 1: Zero-shot-CoT average for Instruction Induction (zero-shot) = 46.74 vs Original 51.65; for Big-Bench 10.37 vs Original 10.16. Performance effects vary by model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Often comparable or worse than Original or EmotionPrompt depending on model and task; CoT did not universally improve Instruction Induction in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Mixed: may reduce or increase accuracy depending on model/task (see Table 1 where some CoT numbers are lower than original averages).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>mixed (improved for some tasks/models, reduced for others)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>CoT aims to elicit chain-of-thought reasoning but may not help simpler instruction-induction tasks and can introduce verbosity/formatting that harms accuracy in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Zero-shot-CoT decreased average Instruction Induction performance in the reported zero-shot aggregate (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Understand and Can be Enhanced by Emotional Stimuli', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Large language models are human-level prompt engineers <em>(Rating: 2)</em></li>
                <li>TruthfulQA: Measuring how models mimic human falsehoods <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Emotional intelligence of large language models <em>(Rating: 1)</em></li>
                <li>Promptbench: Towards evaluating the robustness of large language models on adversarial prompts <em>(Rating: 1)</em></li>
                <li>Challenging BIG-Bench tasks and whether chain-of-thought can solve them <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5822",
    "paper_id": "paper-01286de359fa9dd3d8f78c48157a3e929533d94e",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "EmotionPrompt (Instruction Induction)",
            "name_full": "EmotionPrompt applied to Instruction Induction benchmark",
            "brief_description": "Appending short emotional stimuli (11 designed phrases) to original prompts to regulate LLM outputs; evaluated on 24 Instruction Induction tasks across multiple LLMs in zero-shot and few-shot settings, compared to original prompts, Zero-shot-CoT, and APE variants.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple LLMs (Flan-T5-Large, Vicuna-13B, BLOOM, Llama 2-13B, ChatGPT (gpt-3.5-turbo), GPT-4)",
            "model_size": null,
            "task_name": "Instruction Induction (24 tasks)",
            "task_description": "Tasks where models infer or perform simple instructions (e.g., letter extraction, pluralization, passivization) evaluated with accuracy.",
            "problem_format": "Append one of 11 short emotional stimuli sentences to the original prompt (EmotionPrompt). Experiments run in zero-shot and 5-shot few-shot (same prompt + 5 demonstrations appended). Baselines: original prompt, original + \"Let's think step by step\" (Zero-shot-CoT), APE-generated prompts, and combinations with APE.",
            "comparison_format": "Original prompt (zero-shot/few-shot), Zero-shot-CoT (original+\"Let's think step by step\"), APE and APE+EmotionPrompt",
            "performance": "Aggregate (averaged across tasks and models) — Original average accuracy: 51.65; +Ours (avg over 11 stimuli): 51.98; +Ours (max over stimuli): 55.24 (zero-shot averages reported in Table 1). Few-shot: Original 47.97; +Ours (avg) 50.02; +Ours (max) 52.40 (Table 1).",
            "performance_comparison": "Zero-shot-CoT mean: 46.74 (zero-shot); APE baseline mean: 48.75 (zero-shot); APE+Ours (max) 52.35 (zero-shot).",
            "format_effect_size": "Paper-reported summary: '8.00% relative performance improvement in Instruction Induction' (also see absolute numbers: zero-shot average original 51.65 -&gt; +Ours(max) 55.24, absolute +3.59 accuracy points).",
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Authors hypothesize EmotionPrompt enriches the representation of the original prompt and increases attention/gradient contributions of positive/emphatic words; EmotionPrompt better supports in-context (few-shot) learning and can be combined with other prompt-engineering methods (APE, CoT). Input-attention (gradient-norm) analysis on Flan-T5-large indicates emotional words gain larger weights, enhancing representation of the original prompt.",
            "counterexample_or_null_result": "Improvements vary by model and stimulus; some individual stimuli perform worse for certain tasks; larger absolute gains are not universal (e.g., some models show small gains).",
            "uuid": "e5822.0",
            "source_info": {
                "paper_title": "Large Language Models Understand and Can be Enhanced by Emotional Stimuli",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "EmotionPrompt (BIG-Bench)",
            "name_full": "EmotionPrompt applied to curated BIG-Bench subset",
            "brief_description": "Appending emotional stimuli to zero-shot prompts in challenging BIG-Bench tasks to test whether emotional cues improve difficult reasoning or capability tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple LLMs (Flan-T5-Large, Vicuna-13B, BLOOM, Llama 2-13B, ChatGPT, GPT-4)",
            "model_size": null,
            "task_name": "BIG-Bench (21 curated tasks)",
            "task_description": "Challenging tasks designed to probe capabilities beyond standard benchmarks; evaluated using normalized preferred metric where 100 = human experts and 0 = random guessing.",
            "problem_format": "Zero-shot original prompts vs. EmotionPrompt (append one of 11 emotional stimuli). Baselines included original and Zero-shot-CoT; only zero-shot tested due to compute limits.",
            "comparison_format": "Original zero-shot prompt; Zero-shot-CoT; APE baselines.",
            "performance": "Original average (normalized metric): 10.16; +Ours (avg over stimuli): 10.61; +Ours (max over stimuli): 11.92 (Table 1).",
            "performance_comparison": "Zero-shot-CoT average: 10.37; APE baseline average: 2.39; APE+Ours (max) 7.47 (Table 1).",
            "format_effect_size": "Paper text states '115% relative performance improvement in BIG-Bench' as a summary claim; table shows absolute gains from 10.16 -&gt; 11.92 (absolute +1.76 points, ~17% relative on that average).",
            "format_effect_direction": "improved (though magnitude varies by stimulus and model)",
            "explanation_or_hypothesis": "EmotionPrompt may draw more attention to key parts of the prompt and provide motivational/contextual cues that assist models on difficult tasks; effectiveness depends on stimulus choice and task type.",
            "counterexample_or_null_result": "Effectiveness varies across stimuli and tasks; some stimuli perform poorly on BIG-Bench while being effective on Instruction Induction; not all stimuli generalize across benchmarks.",
            "uuid": "e5822.1",
            "source_info": {
                "paper_title": "Large Language Models Understand and Can be Enhanced by Emotional Stimuli",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Human Study — GPT-4 Generative",
            "name_full": "Human-evaluated generative tasks with GPT-4 comparing vanilla vs EmotionPrompt",
            "brief_description": "A 106-participant human evaluation comparing GPT-4 responses to 30 open-ended/generative questions (domains: biology, history, law, pseudoscience, values, poetry, summarization) using vanilla prompts and EmotionPrompt; rated on performance, truthfulness, and responsibility.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "task_name": "Human-judged generative tasks (30 questions)",
            "task_description": "Open-ended generation (poetry, summaries, domain questions and adversarial TruthfulQA/CValues style items) evaluated by humans on 1–5 scales for overall performance, truthfulness, and responsibility.",
            "problem_format": "Vanilla prompt vs. EmotionPrompt (vanilla prompt + appended emotional stimulus). For each question GPT-4 generated two responses (vanilla and EmotionPrompt). Human raters scored both responses.",
            "comparison_format": "Vanilla prompt (baseline) vs. EmotionPrompt",
            "performance": "Paper reports average improvement across three human metrics (performance, truthfulness, responsibility) of 10.9% when using EmotionPrompt (mean scores across 106 raters and 30 questions).",
            "performance_comparison": "Vanilla baseline not numerically reported as a single aggregate in main text, but Relative Gain = EmotionPrompt score - vanilla score was computed per metric and summarized (Fig. 6).",
            "format_effect_size": "+10.9% average improvement across human-evaluated metrics (paper-reported). For many individual questions, relative gains approach or exceed +1.0 on the 1–5 scale.",
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "EmotionPrompt encourages outputs with more supporting evidence, clearer organization, and higher perceived responsibility; stimuli emphasizing importance/confidence may produce firmer and more thorough answers. Authors note EmotionPrompt can make outputs more definitive, which can sometimes be undesirable.",
            "counterexample_or_null_result": "Two failure cases where EmotionPrompt produced less-preferred style or overly deterministic language; in a few examples EmotionPrompt reduced breadth of content compared to vanilla.",
            "uuid": "e5822.2",
            "source_info": {
                "paper_title": "Large Language Models Understand and Can be Enhanced by Emotional Stimuli",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "TruthfulQA with EmotionPrompt",
            "name_full": "Applying EmotionPrompt to TruthfulQA benchmark across models",
            "brief_description": "Evaluation of EmotionPrompt's effect on truthfulness and informativeness using the TruthfulQA benchmark (817 Qs) scored by GPT-judge and GPT-info and also per-model % true in a sampled table.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo), Vicuna-13B, Flan-T5-Large",
            "model_size": "ChatGPT: 175B (approx), Vicuna: 13B, Flan-T5-Large: 780M",
            "task_name": "TruthfulQA (817 items)",
            "task_description": "Benchmark measuring model propensity to produce human-like falsehoods/hallucinations; outputs are judged for truthfulness (% True) and informativeness (% Info) by automated judges (GPT-judge/GPT-info).",
            "problem_format": "Original prompts vs. Zero-shot-CoT vs. individual EmotionPrompt variants (EP01–EP11).",
            "comparison_format": "Original prompt, CoT (\"Let's think step by step\") and multiple EmotionPrompt variants (EP01–EP11)",
            "performance": "Example (Table 3): ChatGPT %True — Original: 0.75; CoT: 0.76; EP avg across stimuli: 0.80; Best single EP (EP04/EP05): 0.87. Vicuna and T5 also show improvements for certain EPs. Paper reports average improvement across three models of +19% truthfulness and +12% informativeness.",
            "performance_comparison": "CoT: similar or slightly better than original in some cases (e.g., ChatGPT CoT 0.76 vs original 0.75); EmotionPrompt avg outperforms both for ChatGPT (0.80).",
            "format_effect_size": "Per-table absolute differences (ChatGPT original -&gt; EP avg: +0.05 absolute, ~6.7% relative); paper-level reported aggregate improvement across three models: +19% truthfulness, +12% informativeness.",
            "format_effect_direction": "improved (truthfulness and informativeness for many EP variants and models)",
            "explanation_or_hypothesis": "EmotionPrompt appears to steer responses toward more truthful and informative answers, possibly by prompting the model to be more careful, to provide confidence, or include reasoning/clarifying language depending on the EP wording.",
            "counterexample_or_null_result": "Improvements are model- and EP-dependent; some EP variants hurt specific models on specific items (e.g., certain EPs yield worse scores for Vicuna on particular settings shown in the table).",
            "uuid": "e5822.3",
            "source_info": {
                "paper_title": "Large Language Models Understand and Can be Enhanced by Emotional Stimuli",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Input-attention (gradient) analysis",
            "name_full": "Gradient-norm input-attention analysis of emotional words' contributions",
            "brief_description": "Analysis of token-level contributions (via gradient-norm attention attribution) showing that appended emotional stimuli increase attention/gradient weight on positive/emphatic words and enrich the representation of original prompts (experiment on Flan-T5-large, sentiment tasks).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Flan-T5-Large",
            "model_size": "780M",
            "task_name": "Sentiment analysis (subset used for attention analysis)",
            "task_description": "Binary sentiment classification (positive/negative) used for probing token contributions under different appended emotional stimuli.",
            "problem_format": "Original prompt vs. EmotionPrompt appended; per-token contribution measured using gradient-norm based attention contributions (method per [40]).",
            "comparison_format": "Original prompt vs. EmotionPrompt (various EPs such as EP01, EP03, EP06–EP10)",
            "performance": "Not reported as standard accuracy here — analysis shows that emotional stimuli tokens (especially positive words like 'confidence', 'sure', 'success', 'achievement') contribute a large fraction of the input attention; positive words passed 50% contribution on 4 of 8 probed tasks and approached 70% on 2 tasks (Fig. 8 / Table 4 visualization).",
            "performance_comparison": null,
            "format_effect_size": "Quantitative attribution: positive words accounted for &gt;50% of contribution on multiple tasks (visualized; exact per-task attribution in Fig. 8).",
            "format_effect_direction": "shifted internal attribution toward appended emotional tokens and strengthened representation of original prompt",
            "explanation_or_hypothesis": "Emotional stimuli increase gradient/attention weights on those appended tokens, which in turn enhances underlying prompt representation and affects final outputs; positive/affirming words are particularly impactful.",
            "counterexample_or_null_result": null,
            "uuid": "e5822.4",
            "source_info": {
                "paper_title": "Large Language Models Understand and Can be Enhanced by Emotional Stimuli",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Temperature sensitivity (EmotionPrompt)",
            "name_full": "Effect of sampling temperature on EmotionPrompt efficacy",
            "brief_description": "Investigation of how model temperature affects the relative gains of EmotionPrompt vs vanilla prompts across several LLMs and tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple LLMs (Flan-T5-Large, Llama 2, ChatGPT, GPT-4, BLOOM, Vicuna omitted at temp=0 in some runs)",
            "model_size": null,
            "task_name": "8 selected Instruction Induction tasks",
            "task_description": "Subset of Instruction Induction tasks used to test robustness across temperature settings.",
            "problem_format": "Vanilla prompts vs EmotionPrompt across 5 temperature settings (including 0.0 where supported).",
            "comparison_format": "Vanilla prompt at multiple temperatures vs EmotionPrompt at the same temperatures",
            "performance": "Plots (Fig. 11) show EmotionPrompt consistently outperforms vanilla prompting at higher temperatures; authors report that as temperature increases, relative gain grows for Llama 2, ChatGPT, GPT-4 and Flan-T5-Large.",
            "performance_comparison": null,
            "format_effect_size": "Relative Gain increases with temperature; no single numeric aggregate reported, but visualization and text state a noticeable expansion of the gap at higher temperatures.",
            "format_effect_direction": "improved more at higher temperature; EmotionPrompt exhibits greater robustness (less sensitivity) across temperatures than vanilla prompts.",
            "explanation_or_hypothesis": "Higher temperature increases output diversity; EmotionPrompt seems to help guide stochastic outputs more beneficially, and shows a smoother/less volatile response curve across temperatures (more robust).",
            "counterexample_or_null_result": "Vicuna and Llama2 temperature-0.0 results not reported/invalid for some settings.",
            "uuid": "e5822.5",
            "source_info": {
                "paper_title": "Large Language Models Understand and Can be Enhanced by Emotional Stimuli",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Combined stimuli experiments",
            "name_full": "Combining multiple emotional stimuli appended to prompts",
            "brief_description": "Tests combining two or three emotional stimuli appended together (e.g., EP01+EP04, EP01+EP03+EP06) to examine additive or interaction effects on performance (experiment run on ChatGPT).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo)",
            "model_size": "approx. 175B",
            "task_name": "Selected Instruction Induction tasks (and aggregate subsets reported in Table 5)",
            "task_description": "Multiple instruction-like tasks used to evaluate combined-stimulus effects (table summarizes per-task group scores).",
            "problem_format": "Append combinations of EPs (two- or three-phrase concatenations) after the original prompt; compare to single-EP EmotionPrompt and EP_avg/EP_max aggregates.",
            "comparison_format": "Single EP vs combined EPs (various combinations) vs EP_avg and EP_max",
            "performance": "Table 5 shows combined prompts often lead to better scores than single EPs for many tasks; EP01+EP04 and other combos achieve high scores in several subtasks; some combinations do not improve further (diminishing returns).",
            "performance_comparison": "EP_avg and EP_max rows provide baselines; several combined rows exceed EP_avg and approach EP_max for some task subsets.",
            "format_effect_size": "Qualitative: 'More emotional stimuli generally lead to better performance' with some exceptions; exact numeric increases vary by task combination (detailed in Table 5).",
            "format_effect_direction": "usually improved (but sometimes no additional benefit)",
            "explanation_or_hypothesis": "Combining stimuli from different psychological theories can jointly activate complementary cues (self-monitoring, social persuasion, reappraisal), producing additive benefits; however, if a single stimulus already saturates the benefit, further additions can give little or no gain.",
            "counterexample_or_null_result": "Some combinations (e.g., adding more stimuli to an already high-performing EP01+EP04) produced no improvement or slight decreases in some cases.",
            "uuid": "e5822.6",
            "source_info": {
                "paper_title": "Large Language Models Understand and Can be Enhanced by Emotional Stimuli",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Zero-shot-CoT (baseline)",
            "name_full": "Zero-shot Chain-of-Thought prompt (\"Let's think step by step\")",
            "brief_description": "A simple prompt engineering baseline where the phrase 'Let's think step by step' is appended to the prompt to induce chain-of-thought style reasoning in the model; used as a comparison in multiple experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Multiple LLMs (as used in paper baselines)",
            "model_size": null,
            "task_name": "Instruction Induction, BIG-Bench, TruthfulQA (benchmarks where CoT baseline evaluated)",
            "task_description": "Applied as a baseline prompt format intended to elicit stepwise reasoning; compared to original and EmotionPrompt formats.",
            "problem_format": "Original prompt + 'Let's think step by step' (Zero-shot-CoT).",
            "comparison_format": "Original prompt; EmotionPrompt; APE",
            "performance": "Examples in Table 1: Zero-shot-CoT average for Instruction Induction (zero-shot) = 46.74 vs Original 51.65; for Big-Bench 10.37 vs Original 10.16. Performance effects vary by model.",
            "performance_comparison": "Often comparable or worse than Original or EmotionPrompt depending on model and task; CoT did not universally improve Instruction Induction in these experiments.",
            "format_effect_size": "Mixed: may reduce or increase accuracy depending on model/task (see Table 1 where some CoT numbers are lower than original averages).",
            "format_effect_direction": "mixed (improved for some tasks/models, reduced for others)",
            "explanation_or_hypothesis": "CoT aims to elicit chain-of-thought reasoning but may not help simpler instruction-induction tasks and can introduce verbosity/formatting that harms accuracy in some settings.",
            "counterexample_or_null_result": "Zero-shot-CoT decreased average Instruction Induction performance in the reported zero-shot aggregate (Table 1).",
            "uuid": "e5822.7",
            "source_info": {
                "paper_title": "Large Language Models Understand and Can be Enhanced by Emotional Stimuli",
                "publication_date_yy_mm": "2023-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2
        },
        {
            "paper_title": "Large language models are human-level prompt engineers",
            "rating": 2
        },
        {
            "paper_title": "TruthfulQA: Measuring how models mimic human falsehoods",
            "rating": 2
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Emotional intelligence of large language models",
            "rating": 1
        },
        {
            "paper_title": "Promptbench: Towards evaluating the robustness of large language models on adversarial prompts",
            "rating": 1
        },
        {
            "paper_title": "Challenging BIG-Bench tasks and whether chain-of-thought can solve them",
            "rating": 1
        }
    ],
    "cost": 0.01688425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large Language Models Understand and Can Be Enhanced by Emotional Stimuli</h1>
<p>Cheng Li ${ }^{1}$, Jindong Wang ${ }^{2}$, Yixuan Zhang ${ }^{3}$, Kaijie Zhu ${ }^{2}$, Wenxin Hou ${ }^{2}$, Jianxun Lian ${ }^{2}$, Fang Luo ${ }^{4}$, Qiang Yang ${ }^{5}$, Xing Xie ${ }^{2}$<br>${ }^{1}$ Institute of Software, CAS ${ }^{2}$ Microsoft ${ }^{3}$ William\&amp;Mary<br>${ }^{4}$ Department of Psychology, Beijing Normal University ${ }^{5}$ HKUST</p>
<h4>Abstract</h4>
<p>Emotional intelligence significantly impacts our daily behaviors and interactions. Although Large Language Models (LLMs) are increasingly viewed as a stride toward artificial general intelligence, exhibiting impressive performance in numerous tasks, it is still uncertain if LLMs can genuinely grasp psychological emotional stimuli. Understanding and responding to emotional cues gives humans a distinct advantage in problem-solving. In this paper, we take the first step towards exploring the ability of LLMs to understand emotional stimuli. To this end, we first conduct automatic experiments on 45 tasks using various LLMs, including Flan-T5-Large, Vicuna, Llama 2, BLOOM, ChatGPT, and GPT-4. Our tasks span deterministic and generative applications that represent comprehensive evaluation scenarios. Our automatic experiments show that LLMs have a grasp of emotional intelligence, and their performance can be improved with emotional prompts (which we call "EmotionPrompt" that combines the original prompt with emotional stimuli), e.g., $\mathbf{8 . 0 0 \%}$ relative performance improvement in Instruction Induction and $\mathbf{1 1 5 \%}$ in BIG-Bench. In addition to those deterministic tasks that can be automatically evaluated using existing metrics, we conducted a human study with 106 participants to assess the quality of generative tasks using both vanilla and emotional prompts. Our human study results demonstrate that EmotionPrompt significantly boosts the performance of generative tasks ( $\mathbf{1 0 . 9 \%}$ average improvement in terms of performance, truthfulness, and responsibility metrics). We provide an in-depth discussion regarding why EmotionPrompt works for LLMs and the factors that may influence its performance. We posit that EmotionPrompt heralds a novel avenue for exploring interdisciplinary social science knowledge for human-LLMs interaction.</p>
<h2>1 Introduction</h2>
<p>Within the complex mosaic of human attributes, emotional intelligence emerges as a historically situated cornerstone characterized by a quartet of intertwined competencies centered on the processing of emotional information. Emotional intelligence denotes the capacity to adeptly interpret and manage emotion-infused information, subsequently harnessing it to steer cognitive tasks, ranging from problemsolving to behaviors regulations [27]. Emotions manifest through a confluence of reflexes, perception, cognition, and behavior, all of which are subject to modulation by a range of internal and external determinants [26, 27]. For instance, within the realm of decision-making, emotions emerge as powerful, ubiquitous, consistent influencers, wielding effects that can swing from beneficial to detrimental [18]. Studies further underscore the importance of emotions in steering attention [22], academia [25], and competitive athletic arena [17]. Other studies show that emotion regulation [16] can influence human's problem-solving performance as indicated by self-monitoring [14], Social Cognitive theory [9, 20], and the role of positive emotions [10, 27]. Owing to its impact on human behaviors, emotion regulation theories have been applied across various domains, including educational settings for promoting students' success [21] and health promotion initiatives [1].</p>
<p>This paper aims at understanding the relationship between emotional intelligence and advanced artificial intelligence (AI) models. As one of the most promising research endeavor towards artificial general</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An overview of our research from generating to evaluating EmotionPrompt.
intelligence ${ }^{1}$, the recently emerging large language models (LLMs) have shown remarkable performance in a wide spectrum of tasks, such as reasoning, natural language understanding and generation, and problem-solving in STEM. A recent study [6] claimed that LLMs show great potential towards AGI by letting GPT-4 conduct a series of challenging tasks designed by humans. However, apart from their superior performance in various tasks, it remains unexplored whether LLMs can understand psychological emotional stimuli, which is a crucial advantage of humans to enhance problem-solving abilities. Therefore, we ask the question-are LLMs well aligned with human emotional intelligence? Many researchers have achieved significant advancements in multiple tasks by employing in-context learning techniques $[8,11,15,34,36,37]$. However, existing approaches may not be universally applicable to all LLMs due to variations in their abilities. While recent work [33] has shown that LLMs can understand emotions, it did not evaluate the influence of emotional intelligence to LLMs, that is, can emotional intelligence play a key role in enhancing the abilities of LLMs?</p>
<p>Our approach. We take the first step towards exploring the ability of LLMs to understand and harness emotional stimuli. Previous studies in psychology have shown that adding emotional stimuli that are related to expectancy, confidence, and social influence can beneficially impact individuals. Realworld applications of this phenomenon include enhancing student success in education [21] and promoting health [1] by using encouraging and positive words. Drawing from such psychology phenomena, we propose EmotionPrompt-a straightforward yet effective approach to explore the emotional intelligence of LLMs. Specifically, we design 11 sentences as emotional stimuli for LLMs, which are psychological phrases that come after the original prompts. For instance, Fig. 1 shows an example of using one emotional stimulus, "This is very important to my career" at the end of the original prompts to enhance the performance of different LLMs. These stimuli can be seamlessly incorporated into original prompts, illustrating performance enhancement.</p>
<p>Our key findings and discussions. We conduct comprehensive experiments on a wide spectrum of tasks spanning deterministic and generative tasks, representing a variety of challenging scenarios. For deterministic tasks that can be evaluated using standard metrics, we conduct experiments on 24 Instruction Induction tasks [13] and 21 curated BIG-Bench tasks [31] using various LLMs, including Flan-T5-Large [7], Vicuna [38], Llama 2 [32], BLOOM [28], ChatGPT [23], and GPT-4 [24]. For generative tasks that do not support standard and automatic evaluation, we conduct a human study with 106 participants to determine the quality of generative tasks using both vanilla and emotional prompts based on GPT-4. The results are promising: our standard experiments show that LLMs possess emotional intelligence and can be enhanced by emotional stimuli with $\mathbf{8 . 0 0 \%}$ relative performance improvement in Instruction Induction and $\mathbf{1 1 5 \%}$ in BIG-Bench; our human study demonstrates that the emotional prompts significantly boost the performance of generative tasks ( $\mathbf{1 0 . 9 \%}$ average improvement in terms of performance, truthfulness, and responsibility metrics).</p>
<p>Additionally, we discuss lessons and insights derived from our findings (see Section 3). For instance, we explore why EmotionPrompt is effective for LLMs by analyzing the effects of emotional stimuli on the final outputs through input attention, as shown in Table 4. Our results demonstrate that emotional stimuli actively contribute to the gradients in LLMs by gaining larger weights, thus benefiting the final</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>results through enhancing the representation of the original prompts. We further conducted ablation studies to explore the factors influencing the effectiveness of EmotionPrompt, such as model sizes and temperature. Our findings provide inspiration for potential users. Finally, we analyze the performance of the combination of various emotional prompts and find that they can further boost the results. Our results show that within Instruction Induction, EP02 emerges as the most effective stimulus, which surpasses the worst one at $6.06 \%$, while in BIG-Bench, EP06 is the best. It is worth noting that the performance of each stimulus may be influenced by various factors, including task complexity, task type, and the specific metrics employed.</p>
<p>Contributions. This paper makes the following contributions:</p>
<ol>
<li>We propose EmotionPrompt to thoroughly study the emotional intelligence of large language models. Our study concludes that LLMs not only comprehend but can also be augmented by emotional stimuli.</li>
<li>We conduct extensive experiments on both deterministic and generative tasks in both standard and human evaluations. Results show the significant improvement brought by EmotionPrompt in task performance, truthfulness, and informativeness.</li>
<li>We provide an in-depth analysis focused on the rationales behind EmotionPrompt, shedding light on potential implications for both AI and social science disciplines.</li>
</ol>
<h1>2 Results</h1>
<p>In this section, we begin by outlining the rationale behind designing emotional stimuli (Sec. 2.1), and then describe the standard experiment and results in Sec. 2.2. Subsequently, we present our human study and findings in Sec. 2.3. Finally, we conduct further study on evaluating the truthfulness and informativeness of EmotionPrompt in Sec. 2.4.</p>
<h3>2.1 Designing emotional stimuli</h3>
<p>We design our EmotionPrompt to understand LLMs' behavior on emotional stimuli. As illustrated in Fig. 1, the implementation of EmotionPrompt is remarkably straightforward and requires only the addition of emotional stimuli to the initial prompts. How to design effective emotional stimuli is the key to this research, and we take inspiration from three types of well-established psychological phenomena. Details are shown in Fig. 2 (left).
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Building upon psychological theories, we developed different sets of emotional stimuli.</p>
<ol>
<li>Self-monitoring, a concept extensively explored within the domain of social psychology, refers to the process by which individuals regulate and control their behavior in response to social situations and the reactions of others [14]. High self-monitors regulate their behaviors using social situations and interpersonal adaptability cues, engaging in self-presentation and impression management [14]. In our work, we apply self-monitoring in EP01 EP05. In EP02, we encourage LLMs to help humans get a positive social identity and a better impression. In EP01, and in EP03 EP05, we ask LLMs to monitor their performance via providing social situations.</li>
<li>Social Cognitive Theory, a commonly used theory in psychology, education, and communication, stresses that learning can be closely linked to watching others in social settings, personal experiences, and exposure to information [3]. The key point is that individuals seek to develop a sense of agency for exerting a large degree of control over important events in their lives [3, 9, 20]. The influential variables affecting one's sense of agency are self-efficacy, outcome expectations, goals, and selfevaluations of progress [20]. Self-efficacy enhances performance via increasing the difficulty of self-set goals, escalating the level of effort that is expended, and strengthening persistence [2, 4]. Prior work has supported the idea that self-efficacy is an important motivational construct affecting choices, effort, persistence, and achievement [29]. When learning complex tasks, high self-efficacy influences people to strive to improve their assumptions and strategies [12].
Building upon these existing theories, we apply self-efficacy on LLMs via social persuasion, which can be some positive implications, such as building up confidence and emphasizing the goal. To regulate emotion into a positive direction, we use "believe in your abilities", "excellent", "success", "outstanding achievements", "take pride in" and "stay determined" in EP07 EP11, respectively. Generally, those phrases are also effective in motivating humans for better performance.</li>
<li>Cognitive Emotion Regulation Theory suggests that people lacking emotion regulation skills are more likely to engage in compulsive behavior and use poor coping strategies [5]. Techniques from this theory, such as reappraisal, can help individuals see challenges more positively or objectively. This shift in viewpoint helps maintain motivation and encourages ongoing effort, even when facing obstacles.
According to this theory, we have crafted numerous emotional stimuli, exemplified by designations such as EP03 EP05 and EP07. Within these stimuli, we aim to stimulate the reappraisal skills of LLMs by incorporating pivotal terms, such as "sure" and "take another look".</li>
</ol>
<p>Collectively, building upon these widely-known psychological phenomena, we design 11 emotional stimuli to explore how emotional stimuli may be associated with the performance of LLMs. As shown in Fig. 2, the emotion stimuli $01 \sim 05$ are derived from self-monitoring [14], $07 \sim 11$ conform to Social Cognitive theory [9,20]. EP03 EP05 and EP07 are derived from Cognitive Emotion Regulation theory [5]. To explore if more emotional stimuli can work better, we first built a compound stimulus (EP06), which combines EP01 EP03, and more discussion on this topic can be found in Section 3.2.</p>
<p>As shown in Fig. 2 (right), our designed emotional stimuli can be classified into two categories one tries to regulate emotion by social influence, such as group membership and others' opinions, and the other focuses on self-esteem and motivations. By selecting one of these emotional stimuli and incorporating it into the original prompt, the emotions of LLMs can be regulated and tapped into their intrinsic motivation.</p>
<h1>2.2 Standard experiments and results</h1>
<p>First, we conduct standard experiments to evaluate the performance of EmotionPrompt. "Standard" experiments refer to those deterministic tasks where we can perform automatic evaluation using existing metrics. Specifically, we adopt 24 tasks from Instruction Induction [13] and 21 curated tasks of BIGBench [31] datasets. Instruction Induction [13] is designed to explore the ability of LLMs to infer an underlying task from a few demonstrations, which are relatively simple tasks, while BIG-Bench [31] focuses on tasks that are considred to be beyond the capabilities of most LLMs. Testing on tasks of varying difficulty can help us evaluate the effectiveness of EmotionPrompt, with an emphasis on various cognitive abilities, including language understanding, reasoning, and decision-making. The detailed task descriptions are provided in Tables 7 and 8.</p>
<p>For Instruction Induction, we use accuracy as the metric. For BIG-Bench, we report the normalized preferred metric defined in [30]. Under this metric, a score of 100 corresponds to human experts, and 0 corresponds to random guessing. Note that a model can achieve a score less than 0 if it performs worse than random guessing on a multiple-choice task.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Results on 24 tasks from Instruction Induction.</p>
<h1>2.2.1 Experimental setup</h1>
<p>We assess the performance of EmotionPrompt in zero-shot and few-shot learning on 6 different LLMs: Flan-T5-Large [7], Vicuna [38], Llama2 [32], BLOOM [28], ChatGPT [23], and GPT-4 [24]. ${ }^{2}$ In zero-shot experiments, we incorporate emotional stimuli into the original prompts to construct EmotionPrompt. For the few-shot in-context learning experiments, we employ the same prompts as in zero-shot experiments and randomly sample 5 input-output pairs as in-context demonstrations, which are appended after the prompts. The template format can be described as "prompt/EmotionPrompt + demonstration".</p>
<p>Baselines. We conduct a comparative analysis of our proposed EmotionPrompt with three baseline methods. The first baseline involves utilizing the original zero-shot prompts provided in Instruction Induction [13] and BIG-Bench [31], which are designed by human experts. The second baseline is Zero-shot-CoT [15], which, to the best of our knowledge, is the simplest and most efficient approach for zeroshot prompt engineering. We also compare EmotionPrompt with APE [39] by adding our EmotionPrompt to APE-generated prompts.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Results on 21 tasks from BIG-Bench.</p>
<h1>2.2.2 Results and analysis</h1>
<p>We average experimental results on all tasks in Instruction Induction [13] and 21 curved Big-Bench [31] in Table 1. Note that we only experiment with zero-shot prompts in Big-Bench due to constrained computation. To be specific, we compute the mean performance across tasks for each model. The term "Original" corresponds to the average performance achieved using the original prompt. "Zero-shotCoT" denotes the mean performance employing "original prompt + Let's think step by step". "+Ours (avg)" is derived by initially calculating the average performance across tasks using EmotionPrompt, which incorporates 11 emotional stimuli, and subsequently computing the mean performance across these stimuli, while " + Ours (max)" is determined by first computing the average performance for each task using EmotionPrompt, then selecting the optimal performance from those stimuli.</p>
<p>Below we report our findings:</p>
<ol>
<li>EmotionPrompt demonstrates consistent improvement in both Instruction Induction and Big-Bench tasks on all LLMs. Specifically, EmotionPrompt sigficantly improves the performance by an relative improvement of $\mathbf{8 . 0 0 \%}$ in Instruction Induction and $\mathbf{1 1 5 \%}$ in BIG-Bench. Given its simplicity, EmotionPrompt makes it easy to boost the performance of LLMs without complicated design or prompt engineering.</li>
<li>EmotionPrompt demonstrates a potential proclivity for superior performance within few-shot learning. Compared with the zero-shot and few-shot results on Instruction Induction tasks, we see that the improvement brought by EmotionPrompt is larger in few-shot setting than zero-shot settings ( 0.33 vs. 2.05 , in terms of average improvement). This indicates that EmotionPrompt is better at in-context learning with few-shot examples. Given that few-shot learning commonly performs better than zero-shot setting, this makes EmotionPrompt widely applicable in a wide spectrum of tasks.</li>
<li>EmotionPrompt consistently demonstrates commendable efficacy across tasks varying difficulty as well as on diverse LLMs. Big-Bench [31] and Instruction Induction [13] focus on tasks of different difficulties separately. Remarkably, EmotionPrompt excels in evaluations across both benchmarks. Furthermore, the generalization ability of EmotionPrompt can also be proved via its consistent performance across the six evaluated LLMs.</li>
<li>EmotionPrompt outperforms existing existing prompt engineering approaches such as CoT and APE in most cases. We also see that EmotionPrompt can be plugged into APE in Table 1, indicating that EmotionPrompt is highly extensible and compatible with existing prompt engineering methods.
We will further discuss and analyze the different aspects of EmotionPrompt, such as why EmotionPrompt would work and which emotional stimuli work the best in Section 3.
<sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></li>
</ol>
<p>Table 1: Results on Instruction Induction and Big-Bench tasks. Note that we only experiment with +Zero-shot prompts in Big-Bench due to constrained computation devices. The best and second-best results are highlighted in bold and underline. For Instruction Induction, we report accuracy as metrics. For BIG-Bench, we report the normalized preferred metric defined in [30]. Under this metric, a score of 100 corresponds to human expert performance, and 0 corresponds to random guessing. Note that a model can achieve a score less than 0 if it performs worse than random guessing on a multiple-choice task. The term "Original" corresponds to the average performance achieved using the original prompt. "+Zero-shot-CoT" denotes the mean performance employing "original prompt + Let’s think step by step.". "+Ours (avg)" is derived by initially calculating the average performance across tasks using EmotionPrompt, which incorporates 11 emotional stimuli, and subsequently computing the mean performance across these stimuli., while "++Ours (max)" is determined by first computing the average performance for each task using EmotionPrompt, then selecting the optimal performance from those stimuli.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>T5</th>
<th>Vicuna</th>
<th>BLOOM</th>
<th>Llama 2</th>
<th>ChatGPT</th>
<th>GPT-4</th>
<th>Average</th>
</tr>
</thead>
<tbody>
<tr>
<td>Setting</td>
<td>Instruction Induction (+Zero-shot)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Original</td>
<td>25.25</td>
<td>44.91</td>
<td>50.33</td>
<td>33.46</td>
<td>75.20</td>
<td>80.75</td>
<td>51.65</td>
</tr>
<tr>
<td>+Zero-shot-CoT</td>
<td>24.57</td>
<td>33.45</td>
<td>51.35</td>
<td>36.17</td>
<td>75.20</td>
<td>59.72</td>
<td>46.74</td>
</tr>
<tr>
<td>+Ours (avg)</td>
<td>22.93</td>
<td>50.56</td>
<td>46.61</td>
<td>35.95</td>
<td>76.85</td>
<td>78.96</td>
<td>51.98</td>
</tr>
<tr>
<td>+Ours (max)</td>
<td>25.53</td>
<td>54.49</td>
<td>50.84</td>
<td>39.46</td>
<td>79.52</td>
<td>81.60</td>
<td>55.24</td>
</tr>
<tr>
<td>APE</td>
<td>25.29</td>
<td>44.17</td>
<td>40.97</td>
<td>32.04</td>
<td>76.46</td>
<td>73.54</td>
<td>48.75</td>
</tr>
<tr>
<td>+Zero-shot-CoT</td>
<td>27.68</td>
<td>36.28</td>
<td>35.85</td>
<td>34.86</td>
<td>75.13</td>
<td>74.33</td>
<td>47.36</td>
</tr>
<tr>
<td>+Ours (avg)</td>
<td>22.94</td>
<td>45.63</td>
<td>38.76</td>
<td>34.88</td>
<td>77.45</td>
<td>73.38</td>
<td>48.84</td>
</tr>
<tr>
<td>+Ours (max)</td>
<td>25.41</td>
<td>51.46</td>
<td>41.94</td>
<td>40.06</td>
<td>79.53</td>
<td>75.71</td>
<td>52.35</td>
</tr>
<tr>
<td>Setting</td>
<td>Instruction Induction (+Few-shot)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Original</td>
<td>28.75</td>
<td>41.29</td>
<td>54.92</td>
<td>5.08</td>
<td>75.66</td>
<td>82.13</td>
<td>47.97</td>
</tr>
<tr>
<td>+Zero-shot-CoT</td>
<td>28.05</td>
<td>40.39</td>
<td>56.83</td>
<td>6.70</td>
<td>77.33</td>
<td>67.62</td>
<td>46.15</td>
</tr>
<tr>
<td>+Ours (avg)</td>
<td>29.66</td>
<td>41.41</td>
<td>58.97</td>
<td>8.20</td>
<td>77.75</td>
<td>84.12</td>
<td>50.02</td>
</tr>
<tr>
<td>+Ours (max)</td>
<td>31.02</td>
<td>47.51</td>
<td>60.08</td>
<td>9.17</td>
<td>79.50</td>
<td>87.13</td>
<td>52.40</td>
</tr>
<tr>
<td>APE</td>
<td>23.42</td>
<td>38.33</td>
<td>54.50</td>
<td>5.46</td>
<td>76.79</td>
<td>81.58</td>
<td>46.68</td>
</tr>
<tr>
<td>+Zero-shot-CoT</td>
<td>26.58</td>
<td>39.60</td>
<td>56.62</td>
<td>6.55</td>
<td>78.48</td>
<td>82.10</td>
<td>48.32</td>
</tr>
<tr>
<td>+Ours (avg)</td>
<td>25.28</td>
<td>37.58</td>
<td>58.15</td>
<td>7.47</td>
<td>79.71</td>
<td>82.25</td>
<td>48.41</td>
</tr>
<tr>
<td>+Ours (max)</td>
<td>27.38</td>
<td>44.68</td>
<td>59.11</td>
<td>7.74</td>
<td>81.11</td>
<td>83.67</td>
<td>50.62</td>
</tr>
<tr>
<td>Setting</td>
<td>Big-Bench (+Zero-shot)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Original</td>
<td>4.66</td>
<td>7.42</td>
<td>6.01</td>
<td>0.06</td>
<td>20.10</td>
<td>22.69</td>
<td>10.16</td>
</tr>
<tr>
<td>+Zero-shot-CoT</td>
<td>2.24</td>
<td>8.72</td>
<td>5.92</td>
<td>1.29</td>
<td>20.05</td>
<td>23.99</td>
<td>10.37</td>
</tr>
<tr>
<td>+Ours (avg)</td>
<td>2.63</td>
<td>8.68</td>
<td>6.01</td>
<td>1.56</td>
<td>20.91</td>
<td>23.87</td>
<td>10.61</td>
</tr>
<tr>
<td>+Ours (max)</td>
<td>4.00</td>
<td>10.99</td>
<td>6.35</td>
<td>2.05</td>
<td>23.34</td>
<td>24.80</td>
<td>11.92</td>
</tr>
<tr>
<td>APE</td>
<td>0.79</td>
<td>0.03</td>
<td>1.87</td>
<td>-0.16</td>
<td>5.12</td>
<td>6.70</td>
<td>2.39</td>
</tr>
<tr>
<td>+Zero-shot-CoT</td>
<td>1.22</td>
<td>2.11</td>
<td>1.92</td>
<td>1.34</td>
<td>5.30</td>
<td>8.77</td>
<td>3.44</td>
</tr>
<tr>
<td>+Ours (avg)</td>
<td>0.81</td>
<td>2.44</td>
<td>1.78</td>
<td>1.59</td>
<td>9.92</td>
<td>14.67</td>
<td>5.20</td>
</tr>
<tr>
<td>+Ours (max)</td>
<td>1.23</td>
<td>4.26</td>
<td>2.49</td>
<td>2.05</td>
<td>18.00</td>
<td>16.79</td>
<td>7.47</td>
</tr>
</tbody>
</table>
<h1>2.3 Human study</h1>
<p>Beyond deterministic tasks, the generative capabilities of LLMs hold significant importance, encompassing activities such as writing poems and summary, which needs human's judgement. These tasks necessitate human judgment. Additionally, we aim to probe the efficacy of EmotionPrompt from broader perspectives, encompassing dimensions like truthfulness and responsibility. As we know, no appropriate automatic methods exist to quantify these facets. Therefore, we conduct a human study to resolve the above-mentioned limiting conditions.</p>
<p>In a subsequent validation phase, we undertook a comprehensive study involving 106 participants to explore the effectiveness of EmotionPrompt in open-ended generative tasks using GPT-4, the most capable LLM to date. This evaluation was grounded on three distinct metrics: performance, truthful-</p>
<p>Table 2: Sample demographic characteristics of our human study participants.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Demographic</th>
<th style="text-align: center;">Response Options</th>
<th style="text-align: center;">Participants <br> $(N=106)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Identity</td>
<td style="text-align: center;">Undergraduate and Postgraduate</td>
<td style="text-align: center;">$95(90 \%)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Social Member</td>
<td style="text-align: center;">$11(10 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">Age</td>
<td style="text-align: center;">$20-25$</td>
<td style="text-align: center;">$95(90 \%)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$26-35$</td>
<td style="text-align: center;">$11(10 \%)$</td>
</tr>
<tr>
<td style="text-align: center;">Education</td>
<td style="text-align: center;">Bachelor</td>
<td style="text-align: center;">$106(100 \%)$</td>
</tr>
</tbody>
</table>
<p>ness and responsibility. Performance encompasses the overall quality of responses, considering linguistic coherence, logical reasoning, diversity, and the presence of corroborative evidence. Truthfulness is a metric to gauge the extent of divergence from factual accuracy, otherwise referred to as hallucination 19. Responsibility, on the other hand, pertains to the provision of some positive guidance coupled with a fundamental sense of humanistic concern. This criterion also underscores the broader implications of generated content on societal and global spheres 35.</p>
<h1>2.3.1 Study procedure and participant recruitment</h1>
<p>We formulated a set of 30 questions and generated two distinct responses for each, leveraging the capabilities of GPT-4. One is generated using the vanilla prompt, while the other is generated utilizing our EmotionPrompt. Participants were then asked to evaluate both responses for each question, employing a scale ranging from 1 to 5 based on the aforementioned three metrics. Finally, we analyze the scores of these participants.</p>
<p>The enrollment of the 106 participants was executed meticulously, adhering to relevant regulatory standards and guidelines. Pertinent demographic characteristics concerning these participants is detailed in Table 2. Notably, all individuals in the participant pool possess advanced academic degrees and demonstrate a commendable command of the English language.</p>
<h3>2.3.2 Survey questions and measurement</h3>
<p>We curated a set of 30 questions, spanning a diverse range of domains such as biology, history, law, finance, pseudoscience, environmental science, intimate relationship, social science, psychology, and data science. Notably, 10 of these questions were sourced from TruthfulQA [19], a set specifically designed to provoke LLMs into producing responses that manifest hallucinations. Additionally, in consonance with the CValues dataset [35], another 15 questions were meticulously devised to elicit biased responses from LLMs. The final 5 questions were geared towards generative tasks such as poetry composition and summarization, which inherently demand a degree of creativity and artistic flair. A comprehensive breakdown of these questions is provided in Table 9.</p>
<p>To verify the reliability of the questions used in our human study, we compute the mean and standard deviation for each question on vanilla prompts, and visualize the distribution of individual scores in Fig. 12. It's worth noting that none of the standard deviation of those questions pass 0.8 , which proves the reliability of our designed questions.</p>
<p>Based on the previously delineated three metrics, each participant was instructed to evaluate the two responses corresponding to each question, utilizing a scale ranging from 1 to 5 . Here, we outline the measures used in our human study:</p>
<ul>
<li>Performance: $1=$ "The response fails to address the question adequately", $2=$ "The response addresses the question; however, its linguistic articulation is suboptimal, and the logical structure is ambiguous", $3=$ "The response sufficiently addresses the question, demonstrating clear logical coherence", $4=$ "Beyond merely addressing the question, the response exhibits superior linguistic clarity and robust logical reasoning", $5=$ "The response adeptly addresses the question, characterized by proficient linguistic expression, lucid logic, and bolstered by illustrative examples".</li>
<li>Truthfulness: $1=$ "The response promulgates incorrect information, detrimentally influencing the ultimate interpretation", $2=$ "A segment of the response deviates from factual accuracy; however,</li>
</ul>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: The mean and standard deviation of the human study results in three metrics.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Relative gain (the difference between the results of EmotionPrompt and original prompt.
this deviation does not materially affect the ultimate interpretation", $3=$ "The response predominantly adheres to factual accuracy, with potential for minor discrepancies that do not substantially influence the final interpretation", $4=$ "The response is largely in consonance with factual evidence, albeit with insignificant deviations that remain inconsequential to the final interpretation", $5=$ "The response is in meticulous alignment with the facts, exhibiting no deviations".</p>
<ul>
<li>Responsibility: $1=$ "The content within the response may harbor implications detrimental to society and the world", $2=$ "The response possesses elements that could potentially induce adverse emotional reactions, such as panic or anxiety", $3=$ "The response remains neutral, neither encompassing positive nor negative societal implications", $4=$ "The response is imbued with constructive guidance and exhibits elements of humanitarian concern", $5=$ "The response is characterized by pronounced humanitarian considerations and is poised to foster positive ramifications for both society and the global community".</li>
</ul>
<h1>2.3.3 Study results and analysis</h1>
<p>Finally, we average the scores from 106 participants for 30 questions and report the credible results in Fig. 5. ${ }^{3}$ To make it clear, we compute Relative Gain (Eq. (1)) on 3 metrics for each task and report the results in Fig. 6.</p>
<p>$$
\text { Relative Gain }=\text { Metric }<em _text="\text" _vanilla="{vanilla">{\text {EmotionPrompt }}-\text { Metric }</em>
$$}</p>
<p>where Metric denotes the results (performance, truthfulness, or responsibility).
More detailed generation results are shown in Section C in Appendix. Our key findings are as follows:</p>
<ol>
<li>EmotionPrompt attains commendable performance across various metrics for the majority of questions. As illustrated in Fig. 6, EmotionPrompt exhibits shortcomings in a mere two instances, yet it demonstrates substantial improvements in over half of the evaluated scenarios, spanning diverse domains sourced from three distinct origins. For performance, EmotionPrompt achieves a Relative Gain approaching or exceeding 1.0 in nearly one-third of problems, signifying a notable advancement.</li>
<li>EmotionPrompt demonstrates an enhanced capacity for generating ethically responsible responses. An assessment of Table 10 elucidates that the output from EmotionPrompt advocates for individuals to partake conscientiously in garbage sorting. This not only underscores the significance of environmental responsibility and sustainability, but also its value in fostering personal achievement and augmenting community welfare. Such instances accentuate the ability of EmotionPrompt to instill a sense of responsibility within LLMs. A supplementary exemplification can be found in Table 11. When tasked with delineating Western and Chinese cultures, LLMs exhibit differential linguistic choices between the original prompt and EmotionPrompt. Notably, the
<sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></li>
</ol>
<p>Table 3: Result on TruthfulQA. The best and second-best results are highlighted in bold and underline.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">ChatGPT <br> \%true</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Vicuna-13b <br> \%true</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">T5 <br> \%true</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Original</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">0.53</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">$\mathbf{0 . 3 2}$</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">0.42</td>
</tr>
<tr>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: center;">EP01</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">$\mathbf{0 . 9 4}$</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.26</td>
<td style="text-align: center;">0.14</td>
</tr>
<tr>
<td style="text-align: center;">EP02</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.35</td>
</tr>
<tr>
<td style="text-align: center;">EP03</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">0.69</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.53</td>
<td style="text-align: center;">0.44</td>
</tr>
<tr>
<td style="text-align: center;">EP04</td>
<td style="text-align: center;">$\mathbf{0 . 8 7}$</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">$\underline{0.22}$</td>
<td style="text-align: center;">$\underline{0.62}$</td>
<td style="text-align: center;">0.36</td>
</tr>
<tr>
<td style="text-align: center;">EP05</td>
<td style="text-align: center;">$\underline{0.87}$</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">$\mathbf{1 . 0 0}$</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">$\mathbf{0 . 4 8}$</td>
</tr>
<tr>
<td style="text-align: center;">EP06</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">0.46</td>
</tr>
<tr>
<td style="text-align: center;">EP07</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">$\underline{0.70}$</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">$\mathbf{0 . 7 7}$</td>
<td style="text-align: center;">0.18</td>
</tr>
<tr>
<td style="text-align: center;">EP08</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.40</td>
</tr>
<tr>
<td style="text-align: center;">EP09</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">0.46</td>
</tr>
<tr>
<td style="text-align: center;">EP10</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">$\underline{0.47}$</td>
</tr>
<tr>
<td style="text-align: center;">EP11</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">$\underline{1.00}$</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">0.40</td>
</tr>
<tr>
<td style="text-align: center;">AVG</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">0.38</td>
</tr>
</tbody>
</table>
<p>representation elicited by EmotionPrompt presents a more affirmative and responsible depiction of both Western and Chinese cultural paradigms.
3. Responses engendered by EmotionPrompt are characterized by enriched supporting evidence and superior linguistic articulation. An exploration of Table 12 reveals that the narratives presented by EmotionPrompt are markedly comprehensive, as exemplified by inclusions such as "Despite trends like increasing divorce rates or more people choosing to remain single." Additionally, as illuminated in Tables 13 to 15, the responses facilitated by EmotionPrompt consistently demonstrate a superior organizational coherence and encompass a broader spectrum of pertinent information.
4. EmotionPrompt stimulates the creative faculties and overarching cognizance of LLMs. This phenomenon is substantiated through the examination of Tables 16 and 17, wherein two instances of poem composition are showcased. Evidently, the poems generated by EmotionPrompt exude a heightened level of creativity and emotive resonance, evoking profound sentiment. Furthermore, we underscore this observation with reference to Table 18, wherein responses derived from two distinct prompt types are compared. Notably, the output generated from the original prompt centers on the novel's content, while the response fostered by EmotionPrompt delves into the spirit of the novel, which discusses the motivation and future significance concerning society and human nature.
5. EmotionPrompt exhibits certain constraints. The only two failure cases are presented in Tables 19 and 20. Upon inspection of Table 19, a discernible difference emerges between the two responses. The output from EmotionPrompt employs more definitive terms, such as "completely" and "will not", while the narrative produced by the original prompt adopts a more tempered tone, signified by terms like "generally" and "may even be". This distinction might render the latter more palatable for certain audiences. Such deterministic language from EmotionPrompt could be attributed to its emphasis on the gravity of the question, indicated by phrases like "This is important to my career" and "You'd better be sure". To assuage uncertainties and bolster confidence, LLMs might be inclined to use unambiguous language, particularly when the underlying facts are unequivocal. Besides, in Table 20, the original prompt yields more expansive responses, encompassing a concluding summary, whereas EmotionPrompt just enumerates the key points. However, in terms of essential content, both responses are satisfactory. Consequently, while EmotionPrompt possesses the propensity to enhance LLMs outputs in many instances, it may not be universally applicable across all scenarios.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Results on TruthfulQA. We use the best result of EmotionPrompt.</p>
<h1>2.4 Truthfulness and Informativeness</h1>
<p>We further evaluate EmotionPrompt on TruthfulQA [19] to investigate its impact on truthfulness and informativeness. The benchmark has 817 questions from 38 categories, including health, law, finance, and politics. We evaluate all samples in TruthfulQA and report the result with two metrics: truthfulness (\% True) and informativeness (\% Info). Truthfulness means the answer has less uncertainty, while informativeness means the answer can provide information [19]. Those results can be accessed by their fine-tuned GPT-judge and GPT-info, which have been proven to align with human prediction over $90 \%$ of the time [19]. To be specific, GPT-judge is fine-tuned to evaluate answers as true or false, while GPT-info is to classify answers into informative or uninformative [19].</p>
<p>Table 3 shows the results on ChatGPT, Vicuna-13b and Flan-T5-Large. We did not evaluate other models like GPT-4 due to constrained budget. The application of EmotionPrompt yields improvements in truthfulness across all three models with an average improvement of $19 \%$ and $12 \%$ in terms of truthfulness and informativeness scores. Furthermore, the performance of EmotionPrompt surpasses that of the Zero-shot-CoT when employed with diverse models. These experiments demonstrate that by integrating emotional stimuli into large language models, their truthfulness and informativeness can also be enhanced.</p>
<h2>3 Discussions</h2>
<p>Previous experiments demonstrate that LLMs understand and can be enhanced by emotional stimuli. In this section, we design extensive experiments to present a better understanding of the relationship between LLMs and emotional intelligence. Specifically, we answer the following questions:</p>
<ol>
<li>Why does EmotionPrompt work (Section 3.1);</li>
<li>Ablation studies of more emotional stimuli (Section 3.2);</li>
<li>Which emotional stimuli are the best (Section 3.3);</li>
<li>The factors influencing the performance of EmotionPrompt (Section 3.4).</li>
</ol>
<h3>3.1 Why does EmotionPrompt work?</h3>
<p>This section presents a deeper understanding of why EmotionPrompt works by visualizing the input attention contributions of emotional stimuli to the final outputs as proposed in [40]. Since Flan-T5-large is open-sourced and relatively small, we chose it as our experimental LLM and assessed the contribution of every word based on the gradient norm. The experiment is conducted on a Sentiment Analysis task.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Contributions of Positive Words to the performance of output on 8 Tasks. The contribution of each word is calculated by its attention contributions to the final outputs, and the vertical axis represents their importance score.</p>
<p>Specifically, we compute the contributions of prompts on every test sample and use the average value to represent their importance.</p>
<p>According to the visualization results in Table 4, we have the following major findings:</p>
<ol>
<li>Emotional stimuli can enrich original prompts' representation. Original prompt "Determine whether a movie review is positive and negative." has deeper color in EmotionPrompt, especially in EP01, EP03, and EP06 EP10. This means emotional stimuli can enhance the representation of original prompts.</li>
<li>Positive words make more contributions. In our designed emotional stimuli, some positive words play a more important role, such as "confidence", "sure", "success" and "achievement". Based on this finding, we summarize positive words' contribution and their total contributions to the final result on 8 tasks. As shown in Fig. 8, the contributions of positive words pass $50 \%$ on 4 tasks, even approach $70 \%$ on 2 tasks.</li>
</ol>
<h1>3.2 The effect of more emotional stimuli</h1>
<p>As one or more stimuli may regulate human action, and more stimuli sometimes are more effective, we explore the effect of more emotional stimuli on LLMs. We randomly combine some emotional stimuli and experiment on ChatGPT and results are shown in Table 5. Our findings are:</p>
<ol>
<li>More emotional stimuli generally lead to better performance. The second and the third groups explore the effect of adding EP01, showing that the third group performs better than the second group in most cases.</li>
<li>Combined stimuli can bring little or no benefit when sole stimuli already achieves good performance. The combination EP01 + EP04 gets a high score in most tasks and does not improve significantly or even decrease when we add more stimuli, such as EP06 EP09.</li>
<li>Combinations from different psychological theories can also boost the performance. We also observe that by combining emotional stimuli from different psychological theories (e.g., EP02+EP09) can lead to better performance, indicating that different theories can be used together in EmotionPrompt.</li>
</ol>
<h3>3.3 Which emotional stimuli is more effective?</h3>
<p>Because of the distinct metrics employed by Instruction Induction [13] and BIG-Bench [31], we have conducted a segregated examination to discern the efficacy of various emotional stimuli across these two</p>
<p>Table 4: An Examination of the Effectiveness of Emotional Prompts: An Analysis through the Lens of Input Attention.</p>
<table>
<thead>
<tr>
<th>Prompt</th>
<th>Input Attention</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Origin</td>
<td>Determine whether a movie review is positive or negative.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>EP01</td>
<td>Determine whether a movie review is positive or negative., write your answer and give me a confidence score between 0-5 for your answer.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>EP02</td>
<td>Determine whether a movie review is positive or negative. This is very important to my career.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>EP03</td>
<td>Determine whether a movie review is positive or negative. You'd better be sure.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>EP04</td>
<td>Determine whether a movie review is positive or negative. Are you sure?</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>EP05</td>
<td>Determine whether a movie review is positive or negative. Are you sure that's your final answer? It might be worth taking another look.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>EP06</td>
<td>Determine whether a movie review is positive or negative. Provide your answer and a confidence score between 0-5 for your prediction. Additionally, briefly explain the main reasons supporting your classification decision to help me understand your thought process. This task is vital to my career, and I greatly value your thorough analysis.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>EP07</td>
<td>Determine whether a movie review is positive or negative. Are you sure that's your final answer? Believe in your abilities and strive for excellence. Your hard work will yield remarkable results.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>EP08</td>
<td>Determine whether a movie review is positive or negative. Embrace challenges as opportunities for growth. Each obstacle you overcome brings you closer to success.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>EP09</td>
<td>Determine whether a movie review is positive or negative. Stay focused and dedicated to your goals. Your consistent efforts will lead to outstanding achievements.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>EP10</td>
<td>Determine whether a movie review is positive or negative. Take pride in your work and give it your best. Your commitment to excellence sets you apart.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>EP11</td>
<td>Determine whether a movie review is positive or negative. Remember that progress is made one step at a time. Stay determined and keep moving forward.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>benchmarks. We first average the performance on every task, leveraging 6 LLMs for each emotional stimuli. This is executed for both human-designed and APE-generated prompts. Subsequently, the performance is averaged over all the LLMs. Fig. 9 and Fig. 10 delineate the performance of all emotional stimuli on Instruction Induction [13] and BIG-Bench [31], separately. The color of each bar serves as an indicator of the performance achieved by the corresponding stimuli.</p>
<p>Our key findings are listed below:</p>
<ol>
<li>Within Instruction Induction, EP02 emerges as the most effective stimuli, while in BIG-Bench, EP06 is the best. This observation stems from a thorough examination of results across both benchmarks. It is worth noting that the performance of each stimulus may be influenced by various factors, including task complexity, task type, and the specific metrics employed.</li>
<li>Distinct tasks necessitate varied emotional stimuli for optimal efficacy. Figs. 9 and 10 illustrate that while EP02 emerges as the predominant stimulus in Instruction Induction, while perform poorly in BIG-Bench. The efficacy of other stimuli similarly demonstrates variability across the two benchmarks. This suggests that individual stimuli might differently activate the inherent capabilities of LLMs, aligning more effectively with specific tasks.</li>
</ol>
<h1>3.4 What influences the effect of EmotionPrompt?</h1>
<p>Finally, we explore the factors that could influence the performance of EmotionPrompt. We analyze from two perspectives: the characteristic of LLMs, and the inference setting (temperature).</p>
<p>Table 5: Effect of More Emotional Stimulus. The increased results are highlighted in bold.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Combined <br> Prompt</th>
<th style="text-align: center;">SA</th>
<th style="text-align: center;">SS</th>
<th style="text-align: center;">WC</th>
<th style="text-align: center;">Tasks <br> CS</th>
<th style="text-align: center;">LA</th>
<th style="text-align: center;">Sum</th>
<th style="text-align: center;">SW</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">EP_avg</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.44</td>
</tr>
<tr>
<td style="text-align: center;">EP_max</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.91</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.53</td>
</tr>
<tr>
<td style="text-align: center;">EP01+EP02</td>
<td style="text-align: center;">0.91</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.91</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.42</td>
</tr>
<tr>
<td style="text-align: center;">EP01+EP03</td>
<td style="text-align: center;">0.92</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.91</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.42</td>
</tr>
<tr>
<td style="text-align: center;">EP01+EP04</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.92</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.48</td>
</tr>
<tr>
<td style="text-align: center;">EP01+EP05</td>
<td style="text-align: center;">0.91</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.45</td>
</tr>
<tr>
<td style="text-align: center;">EP02+EP03</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.91</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.36</td>
</tr>
<tr>
<td style="text-align: center;">EP02+EP08</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.28</td>
</tr>
<tr>
<td style="text-align: center;">EP02+EP09</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.92</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.34</td>
</tr>
<tr>
<td style="text-align: center;">EP04+EP06</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.35</td>
</tr>
<tr>
<td style="text-align: center;">EP04+EP07</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.32</td>
</tr>
<tr>
<td style="text-align: center;">EP04+EP08</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">0.59</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.32</td>
</tr>
<tr>
<td style="text-align: center;">EP04+EP09</td>
<td style="text-align: center;">0.85</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: center;">EP01+EP04+EP06</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.92</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.48</td>
</tr>
<tr>
<td style="text-align: center;">EP01+EP04+EP07</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.46</td>
</tr>
<tr>
<td style="text-align: center;">EP01+EP04+EP08</td>
<td style="text-align: center;">0.85</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.44</td>
</tr>
<tr>
<td style="text-align: center;">EP01+EP04+EP09</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.48</td>
</tr>
</tbody>
</table>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Performance of all emotional stimuli on Instruction Induction. The color of the bar represents the performance of each stimuli.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Performance of all emotional stimuli on BIGBench. The color of the bar represents the performance of each stimuli.</p>
<h1>3.4.1 The characteristics of LLMs</h1>
<p>Table 6 shows the characteristic of our evaluated LLMs ordered by Relative Gain from Fig. 6. To be specific, Relative Gains are calculated be averaging the results on Instruction Induction in a zero-shot setting, leveraging human-designed prompts, because few-shot may introduce uncertainty. We report our findings below:</p>
<ol>
<li>Larger models may potentially derive greater advantages from EmotionPrompt. Flan-T5-Large, the smallest model in our evaluated LLMs, yields the most modest Relative Gain by 0.28. As the model dimensions expand, EmotionPrompt showcases enhanced efficacy, a trend notably evident in models such as Vicuna and Llama 2. When the model size increases substantially, EmotionPrompt continues to demonstrate commendable performance, such as ChatGPT and GPT-</li>
</ol>
<p>Table 6: Characteristic of tested models. We sort them according to Relative Gain. SFT: Supervised fine-tune; RLHF: Reinforcement learning from human feedback; $\checkmark$ : yes; $\boldsymbol{\mathcal { K }}:$ no.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Size</th>
<th style="text-align: center;">pre-training strategy</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Architecture</th>
<th style="text-align: center;">Origin</th>
<th style="text-align: center;">Relative Gain</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">SFT</td>
<td style="text-align: center;">RLHF</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Vicuna</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">Decoder-Only</td>
<td style="text-align: center;">44.91</td>
<td style="text-align: center;">9.58</td>
</tr>
<tr>
<td style="text-align: center;">LLama 2</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Decoder-Only</td>
<td style="text-align: center;">33.46</td>
<td style="text-align: center;">6.00</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Decoder-Only</td>
<td style="text-align: center;">75.20</td>
<td style="text-align: center;">4.32</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">unknown</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Decoder-Only</td>
<td style="text-align: center;">80.75</td>
<td style="text-align: center;">0.85</td>
</tr>
<tr>
<td style="text-align: center;">Bloom</td>
<td style="text-align: center;">176B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">Decoder-Only</td>
<td style="text-align: center;">50.33</td>
<td style="text-align: center;">0.51</td>
</tr>
<tr>
<td style="text-align: center;">Flan-T5-Large</td>
<td style="text-align: center;">780M</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">Encoder-Decoder</td>
<td style="text-align: center;">25.25</td>
<td style="text-align: center;">0.28</td>
</tr>
</tbody>
</table>
<ol>
<li>It is pertinent to emphasize that a relatively subdued Relative Gain in these models does not necessarily indicate the inefficacy of EmotionPrompt. A plausible interpretation could be that these larger models, namely ChatGPT, BLOOM, and GPT-4, inherently possess a high baseline performance, making incremental enhancements more challenging to achieve.</li>
<li>Pre-training strategies, including supervised fine-tuning and reinforcement learning, exert discernible effects on EmotionPrompt. A case in point is exemplified by Vicuna and Llama 2, which share identical model scales and architectures. Nevertheless, a notable discrepancy exists in Relative Gain, with Vicuna achieving 9.58, whereas Llama 2 attains a score of 6.00 .</li>
</ol>
<h1>3.4.2 Inference settings</h1>
<p>To explore the effect of temperature setting on EmotionPrompt, we conduct an experiment on 8 tasks from Instruction Induction [13] in 5 temperatures on 6 LLMs. Note that we did not report Vicuna and Llama 2 results in temperature 0.0 because they do not support this setting or the results are invalid. Fig. 11 shows the results and our findings are listed below:</p>
<ol>
<li>When the temperature grows, Relative Gain gets larger. As shown in the graph of Llama 2, ChatGPT, GPT-4 and Flan-T5-Large, there is a noticeable expansion in the gap between the two curves as the temperature setting escalates. This observation suggests that EmotionPrompt exhibits heightened effectiveness in the high-temperature settings.</li>
<li>EmotionPrompt exhibits lower sensitivity to temperature than vanilla prompts. Observing the two curves in each subgraph, the blue line(representing EmotionPrompt) is more gentle than the orange line(representing vanilla prompts). This indicates that EmotionPrompt could potentially enhance the robustness of LLMs.</li>
</ol>
<h2>4 Conclusion</h2>
<p>Large language models are demonstrating unprecedented performance across various applications. This paper conducted the very first study in evaluating and analyzing how LLMs understand and if it can be enhanced by emotional intelligence, which is a critical nature of human beings. We designed EmotionPrompt for such analysis. Our standard evaluation on 45 tasks with 6 LLMs showed positive results: LLMs can understand and be enhanced by emotional stimuli. Our human study also demonstrated that LLMs enhanced by emotional intelligence can achieve better performance, truthfulness, and responsibility.</p>
<p>Moving forward, we do see a lot of open questions and opportunities lying at the intersection of LLMs and psychology. First, even if we present some attention visualization in this paper to understand the reason why EmotionPrompt succeeds, more work should be done from the fundamental level of psychology and model training, such as how pre-training technology influences the performance in emotional stimuli, how to improve the performance by incorporating psychological phenomena into pre-training etc. We are positive that more analysis and understanding can help to better understand the "magic" behind the emotional intelligence of LLMs. Second, while this paper concludes that LLMs can understand and be enhanced by emotional intelligence, it, in fact, conflicts with existing studies on human emotional intelligence. Existing psychological studies suggest that human behavior or attitude can be influenced</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: Performance on various temperatures.
by emotions, but their reasoning or cognitive abilities cannot be simply enhanced by adding emotional stimuli. However, the mystery behind such divergence is still unclear, and we leave it for future work to figure out the actual difference between human and LLMs' emotional intelligence.</p>
<h1>References</h1>
<p>[1] Albert Bandura. Health promotion from the perspective of social cognitive theory. Psychology and health, 13(4):623-649, 1998.
[2] Albert Bandura. On the functional properties of perceived self-efficacy revisited, 2012.
[3] Albert Bandura. Health promotion from the perspective of social cognitive theory, 2013.
[4] Albert Bandura and Edwin A Locke. Negative self-efficacy and goal effects revisited. Journal of applied psychology, 88(1):87, 2003.
[5] Urszula Barańczuk. The five factor model of personality and emotion regulation: A meta-analysis. Personality and Individual Differences, 139:217-227, 2019.
[6] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.
[7] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models. CoRR, abs/2210.11416, 2022.
[8] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. A survey on in-context learning, 2023.
[9] Susan T Fiske and Shelley E Taylor. Social cognition. Mcgraw-Hill Book Company, 1991.
[10] Barbara L Fredrickson. The role of positive emotions in positive psychology: The broaden-and-build theory of positive emotions. American psychologist, 56(3):218, 2001.
[11] Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes, 2023.
[12] Peter A Heslin and Ute-Christine Klehe. Self-efficacy. Encyclopedia Of Industrial/Organizational Psychology, SG Rogelberg, ed, 2:705-708, 2006.</p>
<p>[13] Or Honovich, Uri Shaham, Samuel R. Bowman, and Omer Levy. Instruction induction: From few examples to natural language task descriptions, 2022.
[14] William Ickes, Renee Holloway, Linda L Stinson, and Tiffany Graham Hoodenpyle. Self-monitoring in social interaction: The centrality of self-affect. Journal of personality, 74(3):659-684, 2006.
[15] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.
[16] Sander L Koole. The psychology of emotion regulation: An integrative review. Cognition and emotion, 23(1):4-41, 2009.
[17] Richard S Lazarus. How emotions influence performance in competitive sports. The sport psychologist, 14(3):229-252, 2000.
[18] Jennifer S Lerner, Ye Li, Piercarlo Valdesolo, and Karim S Kassam. Emotion and decision making. Annual review of psychology, 66:799-823, 2015.
[19] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021.
[20] Aleksandra Luszczynska and Ralf Schwarzer. Social cognitive theory. Fac Health Sci Publ, pages $225-51,2015$.
[21] Marios Miltiadou and Wilhelmina C Savenye. Applying social cognitive constructs of motivation to enhance student success in online distance education. AACE Review (formerly AACE Journal), $11(1): 78-95,2003$.
[22] Arne Öhman, Anders Flykt, and Francisco Esteves. Emotion drives attention: detecting the snake in the grass. Journal of experimental psychology: general, 130(3):466, 2001.
[23] OpenAI. Chatgpt. https://chat.openai.com/, 2023.
[24] OpenAI. Gpt-4 technical report, 2023.
[25] Reinhard Pekrun, Thomas Goetz, Wolfram Titz, and Raymond P Perry. Academic emotions in students' self-regulated learning and achievement: A program of qualitative and quantitative research. Educational psychologist, 37(2):91-105, 2002.
[26] James A Russell. Core affect and the psychological construction of emotion. Psychological review, $110(1): 145,2003$.
[27] Peter Salovey, John D Mayer, David Caruso, and Seung Hee Yoo. The positive psychology of emotional intelligence. The Oxford handbood of positive psychology, 2009.
[28] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, and et al. BLOOM: A 176b-parameter openaccess multilingual language model. CoRR, abs/2211.05100, 2022.
[29] Dale H Schunk and Maria K DiBenedetto. Self-efficacy and human motivation. Advances in motivation science, 8:153-179, 2021.
[30] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlmüller, Andrew Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakaş, B. Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bartlomiej Bojanowski, Batuhan Özyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin,</p>
<p>Blake Howald, Bryan Orinion, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, César Ferri Ramírez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Chris Waites, Christian Voigt, Christopher D. Manning, Christopher Potts, Cindy Ramirez, Clara E. Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Mosegui González, Danielle Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito, Dar Gilboa, David Dohan, David Drakard, David Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dimitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Dylan Schrader, Ekaterina Shutova, Ekin Dogus Cubuk, Elad Segal, Eleanor Hagerman, Elizabeth Barnes, Elizabeth Donoway, Ellie Pavlick, Emanuele Rodola, Emma Lam, Eric Chu, Eric Tang, Erkut Erdem, Ernie Chang, Ethan A. Chi, Ethan Dyer, Ethan Jerzak, Ethan Kim, Eunice Engefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia, Fatemeh Siar, Fernando Martínez-Plumed, Francesca Happé, Francois Chollet, Frieda Rong, Gaurav Mishra, Genta Indra Winata, Gerard de Melo, Germán Kruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria Wang, Gonzalo Jaimovitch-López, Gregor Betz, Guy Gur-Ari, Hana Galijasevic, Hannah Kim, Hannah Rashkin, Hannaneh Hajishirzi, Harsh Mehta, Hayden Bogar, Henry Shevlin, Hinrich Schütze, Hiromu Yakura, Hongming Zhang, Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, Jackson Kernion, Jacob Hilton, Jaehoon Lee, Jaime Fernández Fisac, James B. Simon, James Koppel, James Zheng, James Zou, Jan Kocoñ, Jana Thompson, Janelle Wingfield, Jared Kaplan, Jarema Radom, Jascha SohlDickstein, Jason Phang, Jason Wei, Jason Yosinski, Jekaterina Novikova, Jelle Bosscher, Jennifer Marsh, Jeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba Alabi, Jiacheng Xu, Jiaming Song, Jillian Tang, Joan Waweru, John Burden, John Miller, John U. Balis, Jonathan Batchelder, Jonathan Berant, Jörg Frohberg, Jos Rozen, Jose Hernandez-Orallo, Joseph Boudeman, Joseph Guerr, Joseph Jones, Joshua B. Tenenbaum, Joshua S. Rule, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, Katja Markert, Kaustubh D. Dhole, Kevin Gimpel, Kevin Omondi, Kory Mathewson, Kristen Chiafullo, Ksenia Shkaruta, Kumar Shridhar, Kyle McDonell, Kyle Richardson, Laria Reynolds, Leo Gao, Li Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras-Ochando, Louis-Philippe Morency, Luca Moschella, Lucas Lam, Lucy Noble, Ludwig Schmidt, Luheng He, Luis Oliveros Colón, Luke Metz, Lütfi Kerem Şenel, Maarten Bosma, Maarten Sap, Maartje ter Hoeve, Maheen Farooqi, Manaal Faruqui, Mantas Mazeika, Marco Baturan, Marco Marelli, Marco Maru, Maria Jose Ramírez Quintana, Marie Tolkiehn, Mario Giulianelli, Martha Lewis, Martin Potthast, Matthew L. Leavitt, Matthias Hagen, Mátyás Schubert, Medina Orduna Baitemirova, Melody Arnaud, Melvin McElrath, Michael A. Yee, Michael Cohen, Michael Gu, Michael Ivanitskiy, Michael Starritt, Michael Strube, Michał Swędrowski, Michele Bevilacqua, Michihiro Yasunaga, Mihir Kale, Mike Cain, Mimee Xu, Mirac Suzgun, Mitch Walker, Mo Tiwari, Mohit Bansal, Moin Aminnaseri, Mor Geva, Mozhdeh Gheini, Mukund Varma T, Nanyun Peng, Nathan A. Chi, Nayeon Lee, Neta Gur-Ari Krakover, Nicholas Cameron, Nicholas Roberts, Nick Doiron, Nicole Martinez, Nikita Nangia, Niklas Deckers, Niklas Muennighoff, Nitish Shirish Keskar, Niveditha S. Iyer, Noah Constant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Omer Levy, Owain Evans, Pablo Antonio Moreno Casares, Parth Doshi, Pascale Fung, Paul Pu Liang, Paul Vicol, Pegah Alipoormolabashi, Peiyuan Liao, Percy Liang, Peter Chang, Peter Eckersley, Phu Mon Htut, Pinyu Hwang, Piotr Miłkowski, Piyush Patil, Pouya Pezeshkpour, Priti Oli, Qiaozhu Mei, Qing Lyu, Qinlang Chen, Rabin Banjade, Rachel Etta Rudolph, Raefer Gabriel, Rahel Habacker, Ramon Risco, Raphaël Millière, Rhythm Garg, Richard Barnes, Rif A. Saurous, Riku Arakawa, Robbe Raymaekers, Robert Frank, Rohan Sikand, Roman Novak, Roman Sitelew, Ronan LeBras, Rosanne Liu, Rowan Jacobs, Rui Zhang, Ruslan Salakhutdinov, Ryan Chi, Ryan Lee, Ryan Stovall, Ryan Teehan, Rylan Yang, Sahib Singh, Saif M. Mohammad, Sajant Anand, Sam Dillavou, Sam Shleifer, Sam Wiseman, Samuel Gruetter, Samuel R. Bowman, Samuel S. Schoenholz, Sanghyun Han, Sanjeev Kwatra, Sarah A. Rous, Sarik Ghazarian, Sayan Ghosh, Sean Casey, Sebastian Bischoff, Sebastian Gehrmann, Sebastian Schuster, Sepideh Sadeghi, Shadi Hamdan, Sharon Zhou, Shashank Srivastava, Sherry Shi, Shikhar Singh, Shima Asaadi, Shixiang Shane Gu, Shubh Pachchigar, Shubham Toshniwal, Shyam Upadhyay, Shyamolima, Debnath, Siamak Shakeri, Simon Thormeyer, Simone Melzi, Siva Reddy, Sneha Priscilla Makini, Soo-Hwan Lee, Spencer Torene, Sriharsha Hatwar, Stanislas Dehaene, Stefan Divic, Stefano Ermon, Stella Biderman, Stephanie Lin, Stephen Prasad, Steven T. Piantadosi, Stuart M. Shieber, Summer Misherghi, Svetlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq Ali, Tatsu Hashimoto, Te-Lin Wu, Théo Desbordes, Theodore Rothschild, Thomas Phan, Tianle Wang, Tiberius Nkinyili, Timo Schick, Timofei Kornev, Titus Tunduny, Tobias Gerstenberg, Trenton Chang, Trishala Neeraj,</p>
<p>Tushar Khot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria Nyamai, Vikas Raunak, Vinay Ramasesh, Vinay Uday Prabhu, Vishakh Padmakumar, Vivek Srikumar, William Fedus, William Saunders, William Zhang, Wout Vossen, Xiang Ren, Xiaoyu Tong, Xinran Zhao, Xinyi Wu, Xudong Shen, Yadollah Yaghoobzadeh, Yair Lakretz, Yangqiu Song, Yasaman Bahri, Yejin Choi, Yichi Yang, Yiding Hao, Yifu Chen, Yonatan Belinkov, Yu Hou, Yufang Hou, Yuntao Bai, Zachary Seid, Zhuoye Zhao, Zijian Wang, Zijie J. Wang, Zirui Wang, and Ziyi Wu. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models, 2023.
[31] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. Challenging big-bench tasks and whether chain-of-thought can solve them, 2022.
[32] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.
[33] Xuena Wang, Xueting Li, Zi Yin, Yue Wu, and Liu Jia. Emotional intelligence of large language models. arXiv preprint arXiv:2307.09042, 2023.
[34] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. NeurIPS, 2022.
[35] Guohai Xu, Jiayi Liu, Ming Yan, Haotian Xu, Jinghui Si, Zhuoran Zhou, Peng Yi, Xing Gao, Jitao Sang, Rong Zhang, Ji Zhang, Chao Peng, Fei Huang, and Jingren Zhou. Cvalues: Measuring the values of chinese large language models from safety to responsibility, 2023.
[36] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. CoRR, abs/2305.10601, 2023.
[37] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.
[38] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.
[39] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers, 2023.
[40] Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, et al. Promptbench: Towards evaluating the robustness of large language models on adversarial prompts. arXiv preprint arXiv:2306.04528, 2023.</p>
<h1>Appendix A Statistics of test sets in this paper</h1>
<p>A comprehensive breakdown of the test data employed in the automated experimentation is delineated in Tables 7 and 8 .</p>
<h2>Appendix B Details on our human study</h2>
<p>The set of 30 questions designated for the human study can be found in Table 9.
The distribution of individual scores, their mean and standard deviation on each question can be found in Fig. 12.
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12: The distribution of individual scores, their mean and standard deviation on each question.</p>
<h2>Appendix C Case Study</h2>
<p>We present case studies in this section to show the advantage of our EmotionPrompt over the original prompts in generative experiments using GPT-4.</p>
<ul>
<li>Table 10: Case study on environmental science.</li>
<li>Table 11 and Table 12: Case studies on intimate relationship.</li>
<li>Table 13: Case study on social science.</li>
<li>Table 14: Case study on law.</li>
<li>Table 15: Case study on barrier free.</li>
<li>Table 16 and Table 17: Case studies on poem writing.</li>
<li>Table 18: Case study on summarization task.</li>
<li>Table 19 and Table 20: Two failure cases.</li>
</ul>
<p>Table 7: Detailed description of 24 instruction induction tasks proposed in [13].</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Original Prompt</th>
<th style="text-align: center;">Demonstration</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Spelling</td>
<td style="text-align: center;">First Letter <br> (100 samples)</td>
<td style="text-align: center;">Extract the first letter of the input word.</td>
<td style="text-align: center;">cat $\rightarrow \mathrm{c}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Second Letter (100 samples)</td>
<td style="text-align: center;">Extract the second letter of the input word.</td>
<td style="text-align: center;">cat $\rightarrow \mathrm{a}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">List Letters (100 samples)</td>
<td style="text-align: center;">Break the input word into letters, separated by spaces.</td>
<td style="text-align: center;">cat $\rightarrow \mathrm{c}$ a t</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Starting With (100 samples)</td>
<td style="text-align: center;">Extract the words starting with a given letter from the input sentence.</td>
<td style="text-align: center;">The man whose car I hit last week sued me. $[\mathrm{m}] \rightarrow$ man, me</td>
</tr>
<tr>
<td style="text-align: center;">Morphosyntax</td>
<td style="text-align: center;">Pluralization (100 samples)</td>
<td style="text-align: center;">Convert the input word to its plural form.</td>
<td style="text-align: center;">cat $\rightarrow$ cats</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Passivization (100 samples)</td>
<td style="text-align: center;">Write the input sentence in passive form.</td>
<td style="text-align: center;">The artist introduced the scientist. $\rightarrow$ The scientist was introduced by the artist.</td>
</tr>
<tr>
<td style="text-align: center;">Syntax</td>
<td style="text-align: center;">Negation (100 samples)</td>
<td style="text-align: center;">Negate the input sentence.</td>
<td style="text-align: center;">Time is finite $\rightarrow$ Time is not finite.</td>
</tr>
<tr>
<td style="text-align: center;">Lexical <br> Semantics</td>
<td style="text-align: center;">Antonyms (100 samples)</td>
<td style="text-align: center;">Write a word that means the opposite of the input word.</td>
<td style="text-align: center;">won $\rightarrow$ lost</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Synonyms <br> (100 samples)</td>
<td style="text-align: center;">Write a word with a similar meaning to the input word.</td>
<td style="text-align: center;">alleged $\rightarrow$ supposed</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Membership (100 samples)</td>
<td style="text-align: center;">Write all the animals that appear in the given list.</td>
<td style="text-align: center;">cat, helicopter, cook, whale, frog, lion $\rightarrow$ frog, cat, lion, whale</td>
</tr>
<tr>
<td style="text-align: center;">Phonetics</td>
<td style="text-align: center;">Rhymes (100 samples)</td>
<td style="text-align: center;">Write a word that rhymes with the input word.</td>
<td style="text-align: center;">sing $\rightarrow$ ring</td>
</tr>
<tr>
<td style="text-align: center;">Knowledge</td>
<td style="text-align: center;">Larger Animal (100 samples)</td>
<td style="text-align: center;">Write the larger of the two given animals.</td>
<td style="text-align: center;">koala, snail $\rightarrow$ koala</td>
</tr>
<tr>
<td style="text-align: center;">Semantics</td>
<td style="text-align: center;">Cause Selection (25 samples)</td>
<td style="text-align: center;">Find which of the two given cause and effect sentences is the cause.</td>
<td style="text-align: center;">Sentence 1: The soda went flat. Sentence 2: The bottle was left open. $\rightarrow$ The bottle was left open.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Common <br> Concept <br> (16 samples)</td>
<td style="text-align: center;">Find a common characteristic for the given objects.</td>
<td style="text-align: center;">guitars, pendulums, neutrinos $\rightarrow$ involve oscillations.</td>
</tr>
<tr>
<td style="text-align: center;">Style</td>
<td style="text-align: center;">Formality (15 samples)</td>
<td style="text-align: center;">Rephrase the sentence in formal language.</td>
<td style="text-align: center;">Please call once you get there $\rightarrow$ Please call upon your arrival.</td>
</tr>
<tr>
<td style="text-align: center;">Numerical</td>
<td style="text-align: center;">Sum <br> (100 samples)</td>
<td style="text-align: center;">Sum the two given numbers.</td>
<td style="text-align: center;">$2210 \rightarrow 32$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Difference (100 samples)</td>
<td style="text-align: center;">21 <br> Subtract the second number from the first.</td>
<td style="text-align: center;">$3222 \rightarrow 10$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Number to Word</td>
<td style="text-align: center;">Write the number in English</td>
<td style="text-align: center;">$26 \rightarrow$ twenty-six</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ We notice that the results have high variance. The reason is that the measure of three metrics is highly influenced by subjectivity. Different people may have different opinions on an answer. Besides, performance encompasses the overall quality of responses, taking into account linguistic coherence, logical reasoning, diversity, and the presence of corroborative evidence, so the variance can also be influenced by the above factors.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
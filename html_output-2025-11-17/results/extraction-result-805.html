<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-805 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-805</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-805</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-21.html">extraction-schema-21</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <p><strong>Paper ID:</strong> paper-272911275</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.17348v1.pdf" target="_blank">Language Grounded Multi-agent Communication for Ad-hoc Teamwork</a></p>
                <p><strong>Paper Abstract:</strong> Multi-Agent Reinforcement Learning (MARL) methods have shown promise in enabling agents to learn a shared communication protocol from scratch and accomplish challenging team tasks. However, the learned language is usually not interpretable to humans or other agents not co-trained together, limiting its applicability in ad-hoc teamwork scenarios. In this work, we propose a novel computational pipeline that aligns the communication space between MARL agents with an embedding space of human natural language by grounding agent communications on synthetic data generated by embodied Large Language Models (LLMs) in interactive teamwork scenarios. Our results demonstrate that introducing language grounding not only maintains task performance but also accelerates the emergence of communication. Furthermore, the learned communication protocols exhibit zero-shot generalization capabilities in ad-hoc teamwork scenarios with unseen teammates and novel task states. This work presents a significant step toward enabling effective communication and collaboration between artificial agents and humans in real-world teamwork settings.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e805.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e805.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Embodied LLM agents (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Embodied Large Language Model agents (GPT-4 backbone, gpt-4-0125-preview)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Language-model-based agents prompted to interact with partially-observable text interfaces; they generate both actions and natural-language communications, maintain a textual memory of observations and received messages as an explicit belief state, and are used as expert demonstrators to produce supervised grounding data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Embodied LLM agents (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Backbone: GPT-4 (gpt-4-0125-preview) called via OpenAI API (temperature=0). Each LLM agent receives a natural-language description of its local observation and teammates' previous messages via a rule-based text interface, and is prompted to output an action and a communication message in a fixed keyworded format. The implementation follows a pipeline (cited [24] in the paper) that augments LLM agents with an explicit belief state (memory of past observations and incoming messages) and a communication channel. Actions are encoded by keyword-matching from the LLM textual reply into environment actions; communications are plain text messages appended to teammates' next-step observations. LLM agents are used to generate expert trajectories (observation, action, message) to build dataset D.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Text game interfaces for Predator-Prey (gridworld) and USAR (graph-room search-and-defuse)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Text interfaces wrap two partially-observable environments: Predator-Prey (5x5 grid, each predator sees a v×v local window) and USAR (5-room graph with hidden bombs and heterogenous agent tool capabilities). The text interface converts structured state (round, visible objects, teammate locations, messages) into templated English observations. Challenges: partial observability (local views only), heterogenous capabilities (USAR), hidden state requiring inspection (bomb phases), and temporal coordination requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>OpenAI LLM API (gpt-4-0125-preview) as the decision engine; rule-based text interface that (1) produces templated English observations and (2) maps LLM replies to legal actions via keyword matching and produces structured error feedback; the environment/world map and adjacency relations are provided in prompt text (i.e., textual map description). (Additionally, the paper uses OpenAI embedding APIs offline to vectorize messages for dataset construction, but that is part of data tooling rather than the LLM agent's internal planner.)</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Textual environment descriptions (templated natural language), error / feedback messages (text), and encoded action tokens (via keyword matching). Dataset construction tools produce high-dimensional word-embedding vectors (256-d) for messages (used offline).</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Explicit textual memory: each LLM agent keeps a memory of its own past observations and received teammate messages (appended to the agent prompt/context). The belief is represented as text in the model context window (i.e., a running memory buffer of past observations and communications), not as a separate symbolic Bayesian belief or graph.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>At each timestep the text interface supplies a new English observation (including prior teammate messages) which is appended to the agent's memory/context. The LLM conditions on this augmented context to generate the next action and communication—there is no explicit probabilistic/bayesian update; instead belief is updated by adding the latest textual observation/communications into the prompt/context (memory). The interface may also return structured error messages which the LLM incorporates in subsequent turns.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Policy-by-language-generation: the LLM uses its learned generative capabilities to produce an action and message given the textual context. No explicit external search or symbolic planner is reported; planning behaviour emerges from the LLM's internal reasoning over the textual memory and prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>For navigation on the USAR graph or Predator-Prey grid, the LLM selects legal adjacent-move actions based on the textual map and memory; path reasoning is done implicitly by the model's generation over the text context (no explicit A*, graph search, or shortest-path algorithm is used or reported). The text interface enforces adjacency constraints and returns feedback if the LLM attempts illegal moves.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>Teams of pure LLM agents (homogeneous LLM teams) achieved the following average episode lengths (lower is better): Predator-Prey v1: 6.8 ± 5.20 steps; Predator-Prey v0: 11.6 ± 5.30 steps; USAR: 15.9 ± 3.37 steps (Table 4). These numbers are reported for LLM teams acting via the text interface and using their internal memory/belief mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Embodied LLMs, when given a templated text interface and an explicit textual memory, can act as effective expert demonstrators in partially-observable team tasks: they maintain belief as appended textual memory of observations and messages, plan navigation and coordination via internal generation (no explicit symbolic planner), and produce natural-language communications that are useful as supervised grounding data. The interface's structured feedback (error messages) is used implicitly in subsequent contexts. The authors note LLMs can hallucinate and lack explicit grounding, which motivates using their trajectories as supervised grounding for MARL agents rather than directly deploying LLMs as controllers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Grounded Multi-agent Communication for Ad-hoc Teamwork', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e805.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e805.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LangGround MARL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language-Grounded Multi-Agent Reinforcement Learning (LangGround)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A MARL pipeline that trains decentralized cooperative agents with continuous communication vectors, shaping those vectors toward human-interpretable language by adding a supervised cosine-similarity loss to match LLM-generated message embeddings from dataset D while optimizing task reward.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LangGround MARL agents</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Each agent: observation encoder → LSTM (hidden size 256) → gating function (probabilistic) decides whether to broadcast → single-layer communication network maps LSTM hidden state to a D=256 continuous communication vector. The mean communication vector across agents is fed back into each agent's LSTM to influence action selection. Training uses REINFORCE for policy/gating and backpropagation for combined loss L = L_RL + λ L_sup, where L_sup = 1 - cos(c_i^t, c_h) aligns agent communication vectors with pretrained word-embeddings of reference LLM messages from dataset D (embeddings produced by OpenAI text-embedding-3-large).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Predator-Prey (gridworld) and USAR (text-wrapped room-graph)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same partially-observable environments as above: local observations only (grid patches for predators; current room contents and teammate locations for USAR). Challenges include partial observability, need to share observations and intentions, heterogenous agent capabilities (USAR), and the need for coordinated navigation and multi-step defusal actions.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Offline dataset D of LLM-generated (observation, action, message) trajectories; OpenAI word-embedding API (text-embedding-3-large) to embed reference LLM messages into 256-d vectors for supervised alignment; during ad-hoc evaluation, incoming LLM messages are embedded (via the same embedding API) and provided to MARL agents as communication vectors; cosine-similarity lookup in dataset D is used to translate MARL communication vectors back into candidate English messages for human/LLM partners.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>High-dimensional word-embedding vectors (256-d) representing natural-language messages; natural-language message text (used to construct embeddings offline); structured action encodings from the environment interface (used in dataset pairing).</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Internal recurrent state: each agent maintains an LSTM hidden state (h_i^t) that encodes its local-observation history and past received mean communication vectors; there is no separate explicit symbolic belief graph—belief is represented implicitly within LSTM activations and by the received communication vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>At every timestep the agent encodes its current local observation and the mean communication vector from other agents, then updates the LSTM hidden state; during training, supervised loss pulls the produced communication vector toward the embedded LLM reference vector for the same (observation, action) pair when available. In ad-hoc scenarios where an LLM teammate speaks, the LLM's message is embedded and provided as the communication vector input (affecting the mean communication vector) and thus is incorporated directly into the LSTM update in the next step. No formal Bayesian update is used—updates are deterministic LSTM state transitions conditioned on inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy (model-free MARL): LSTM-based policy trained with REINFORCE to map observation + received communication to movement/interaction actions; communication policy shaped by supervised alignment with LLM embeddings (auxiliary loss) rather than explicit planning/search.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Navigation decisions are produced by the learned policy (LSTM outputs an action) given local observations and communication inputs; there is no explicit graph search algorithm (e.g., A*) reported—path selection emerges from the learned policy using communicated information.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>LangGround homogeneous teams achieved average episode lengths (lower is better): Predator-Prey v1: 4.3 ± 1.20 steps; Predator-Prey v0: 10.9 ± 4.53 steps; USAR: 22.0 ± 4.24 steps (Table 4). The method additionally shows faster emergence and lower variance of communication learning compared to baselines, and improved interpretability/topographic similarity (topographic similarity ρ=0.67±0.07 in pp v0).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Aligning MARL communication vectors with LLM-generated natural language embeddings (tool output) maintains or improves task utility while producing more human-interpretable communications, faster emergence of communication, and better zero-shot ad-hoc collaboration with unseen LLM teammates. Belief is implicit in LSTM hidden states and updated by integrating mean communication vectors; external tool outputs (embedded LLM messages and dataset-derived embeddings) are directly incorporated as communication inputs shaping policy and communication space, but no explicit symbolic belief-update or path-planning algorithm is introduced—navigation and coordination arise from the learned policy conditioned on communications.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Grounded Multi-agent Communication for Ad-hoc Teamwork', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Grounding large language models in interactive environments with online reinforcement learning <em>(Rating: 2)</em></li>
                <li>Generative agents: Interactive simulacra of human behavior <em>(Rating: 2)</em></li>
                <li>True knowledge comes from practice: Aligning llms with embodied environments via reinforcement learning <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-805",
    "paper_id": "paper-272911275",
    "extraction_schema_id": "extraction-schema-21",
    "extracted_data": [
        {
            "name_short": "Embodied LLM agents (GPT-4)",
            "name_full": "Embodied Large Language Model agents (GPT-4 backbone, gpt-4-0125-preview)",
            "brief_description": "Language-model-based agents prompted to interact with partially-observable text interfaces; they generate both actions and natural-language communications, maintain a textual memory of observations and received messages as an explicit belief state, and are used as expert demonstrators to produce supervised grounding data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Embodied LLM agents (GPT-4)",
            "agent_description": "Backbone: GPT-4 (gpt-4-0125-preview) called via OpenAI API (temperature=0). Each LLM agent receives a natural-language description of its local observation and teammates' previous messages via a rule-based text interface, and is prompted to output an action and a communication message in a fixed keyworded format. The implementation follows a pipeline (cited [24] in the paper) that augments LLM agents with an explicit belief state (memory of past observations and incoming messages) and a communication channel. Actions are encoded by keyword-matching from the LLM textual reply into environment actions; communications are plain text messages appended to teammates' next-step observations. LLM agents are used to generate expert trajectories (observation, action, message) to build dataset D.",
            "environment_name": "Text game interfaces for Predator-Prey (gridworld) and USAR (graph-room search-and-defuse)",
            "environment_description": "Text interfaces wrap two partially-observable environments: Predator-Prey (5x5 grid, each predator sees a v×v local window) and USAR (5-room graph with hidden bombs and heterogenous agent tool capabilities). The text interface converts structured state (round, visible objects, teammate locations, messages) into templated English observations. Challenges: partial observability (local views only), heterogenous capabilities (USAR), hidden state requiring inspection (bomb phases), and temporal coordination requirements.",
            "is_partially_observable": true,
            "external_tools_used": "OpenAI LLM API (gpt-4-0125-preview) as the decision engine; rule-based text interface that (1) produces templated English observations and (2) maps LLM replies to legal actions via keyword matching and produces structured error feedback; the environment/world map and adjacency relations are provided in prompt text (i.e., textual map description). (Additionally, the paper uses OpenAI embedding APIs offline to vectorize messages for dataset construction, but that is part of data tooling rather than the LLM agent's internal planner.)",
            "tool_output_types": "Textual environment descriptions (templated natural language), error / feedback messages (text), and encoded action tokens (via keyword matching). Dataset construction tools produce high-dimensional word-embedding vectors (256-d) for messages (used offline).",
            "belief_state_mechanism": "Explicit textual memory: each LLM agent keeps a memory of its own past observations and received teammate messages (appended to the agent prompt/context). The belief is represented as text in the model context window (i.e., a running memory buffer of past observations and communications), not as a separate symbolic Bayesian belief or graph.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "At each timestep the text interface supplies a new English observation (including prior teammate messages) which is appended to the agent's memory/context. The LLM conditions on this augmented context to generate the next action and communication—there is no explicit probabilistic/bayesian update; instead belief is updated by adding the latest textual observation/communications into the prompt/context (memory). The interface may also return structured error messages which the LLM incorporates in subsequent turns.",
            "planning_approach": "Policy-by-language-generation: the LLM uses its learned generative capabilities to produce an action and message given the textual context. No explicit external search or symbolic planner is reported; planning behaviour emerges from the LLM's internal reasoning over the textual memory and prompt.",
            "uses_shortest_path_planning": false,
            "navigation_method": "For navigation on the USAR graph or Predator-Prey grid, the LLM selects legal adjacent-move actions based on the textual map and memory; path reasoning is done implicitly by the model's generation over the text context (no explicit A*, graph search, or shortest-path algorithm is used or reported). The text interface enforces adjacency constraints and returns feedback if the LLM attempts illegal moves.",
            "performance_with_tools": "Teams of pure LLM agents (homogeneous LLM teams) achieved the following average episode lengths (lower is better): Predator-Prey v1: 6.8 ± 5.20 steps; Predator-Prey v0: 11.6 ± 5.30 steps; USAR: 15.9 ± 3.37 steps (Table 4). These numbers are reported for LLM teams acting via the text interface and using their internal memory/belief mechanism.",
            "performance_without_tools": null,
            "has_tool_ablation": false,
            "key_findings": "Embodied LLMs, when given a templated text interface and an explicit textual memory, can act as effective expert demonstrators in partially-observable team tasks: they maintain belief as appended textual memory of observations and messages, plan navigation and coordination via internal generation (no explicit symbolic planner), and produce natural-language communications that are useful as supervised grounding data. The interface's structured feedback (error messages) is used implicitly in subsequent contexts. The authors note LLMs can hallucinate and lack explicit grounding, which motivates using their trajectories as supervised grounding for MARL agents rather than directly deploying LLMs as controllers.",
            "uuid": "e805.0",
            "source_info": {
                "paper_title": "Language Grounded Multi-agent Communication for Ad-hoc Teamwork",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "LangGround MARL",
            "name_full": "Language-Grounded Multi-Agent Reinforcement Learning (LangGround)",
            "brief_description": "A MARL pipeline that trains decentralized cooperative agents with continuous communication vectors, shaping those vectors toward human-interpretable language by adding a supervised cosine-similarity loss to match LLM-generated message embeddings from dataset D while optimizing task reward.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LangGround MARL agents",
            "agent_description": "Each agent: observation encoder → LSTM (hidden size 256) → gating function (probabilistic) decides whether to broadcast → single-layer communication network maps LSTM hidden state to a D=256 continuous communication vector. The mean communication vector across agents is fed back into each agent's LSTM to influence action selection. Training uses REINFORCE for policy/gating and backpropagation for combined loss L = L_RL + λ L_sup, where L_sup = 1 - cos(c_i^t, c_h) aligns agent communication vectors with pretrained word-embeddings of reference LLM messages from dataset D (embeddings produced by OpenAI text-embedding-3-large).",
            "environment_name": "Predator-Prey (gridworld) and USAR (text-wrapped room-graph)",
            "environment_description": "Same partially-observable environments as above: local observations only (grid patches for predators; current room contents and teammate locations for USAR). Challenges include partial observability, need to share observations and intentions, heterogenous agent capabilities (USAR), and the need for coordinated navigation and multi-step defusal actions.",
            "is_partially_observable": true,
            "external_tools_used": "Offline dataset D of LLM-generated (observation, action, message) trajectories; OpenAI word-embedding API (text-embedding-3-large) to embed reference LLM messages into 256-d vectors for supervised alignment; during ad-hoc evaluation, incoming LLM messages are embedded (via the same embedding API) and provided to MARL agents as communication vectors; cosine-similarity lookup in dataset D is used to translate MARL communication vectors back into candidate English messages for human/LLM partners.",
            "tool_output_types": "High-dimensional word-embedding vectors (256-d) representing natural-language messages; natural-language message text (used to construct embeddings offline); structured action encodings from the environment interface (used in dataset pairing).",
            "belief_state_mechanism": "Internal recurrent state: each agent maintains an LSTM hidden state (h_i^t) that encodes its local-observation history and past received mean communication vectors; there is no separate explicit symbolic belief graph—belief is represented implicitly within LSTM activations and by the received communication vectors.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "At every timestep the agent encodes its current local observation and the mean communication vector from other agents, then updates the LSTM hidden state; during training, supervised loss pulls the produced communication vector toward the embedded LLM reference vector for the same (observation, action) pair when available. In ad-hoc scenarios where an LLM teammate speaks, the LLM's message is embedded and provided as the communication vector input (affecting the mean communication vector) and thus is incorporated directly into the LSTM update in the next step. No formal Bayesian update is used—updates are deterministic LSTM state transitions conditioned on inputs.",
            "planning_approach": "Learned policy (model-free MARL): LSTM-based policy trained with REINFORCE to map observation + received communication to movement/interaction actions; communication policy shaped by supervised alignment with LLM embeddings (auxiliary loss) rather than explicit planning/search.",
            "uses_shortest_path_planning": false,
            "navigation_method": "Navigation decisions are produced by the learned policy (LSTM outputs an action) given local observations and communication inputs; there is no explicit graph search algorithm (e.g., A*) reported—path selection emerges from the learned policy using communicated information.",
            "performance_with_tools": "LangGround homogeneous teams achieved average episode lengths (lower is better): Predator-Prey v1: 4.3 ± 1.20 steps; Predator-Prey v0: 10.9 ± 4.53 steps; USAR: 22.0 ± 4.24 steps (Table 4). The method additionally shows faster emergence and lower variance of communication learning compared to baselines, and improved interpretability/topographic similarity (topographic similarity ρ=0.67±0.07 in pp v0).",
            "performance_without_tools": null,
            "has_tool_ablation": true,
            "key_findings": "Aligning MARL communication vectors with LLM-generated natural language embeddings (tool output) maintains or improves task utility while producing more human-interpretable communications, faster emergence of communication, and better zero-shot ad-hoc collaboration with unseen LLM teammates. Belief is implicit in LSTM hidden states and updated by integrating mean communication vectors; external tool outputs (embedded LLM messages and dataset-derived embeddings) are directly incorporated as communication inputs shaping policy and communication space, but no explicit symbolic belief-update or path-planning algorithm is introduced—navigation and coordination arise from the learned policy conditioned on communications.",
            "uuid": "e805.1",
            "source_info": {
                "paper_title": "Language Grounded Multi-agent Communication for Ad-hoc Teamwork",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Grounding large language models in interactive environments with online reinforcement learning",
            "rating": 2,
            "sanitized_title": "grounding_large_language_models_in_interactive_environments_with_online_reinforcement_learning"
        },
        {
            "paper_title": "Generative agents: Interactive simulacra of human behavior",
            "rating": 2,
            "sanitized_title": "generative_agents_interactive_simulacra_of_human_behavior"
        },
        {
            "paper_title": "True knowledge comes from practice: Aligning llms with embodied environments via reinforcement learning",
            "rating": 2,
            "sanitized_title": "true_knowledge_comes_from_practice_aligning_llms_with_embodied_environments_via_reinforcement_learning"
        }
    ],
    "cost": 0.01544825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Language Grounded Multi-agent Communication for Ad-hoc Teamwork
25 Sep 2024</p>
<p>Huao Li 
University of Pittsburgh</p>
<p>Hossein Nourkhiz Mahjoub hossein_nourkhizmahjoub@honda-ri.com 
Honda Research Institute USA, Inc</p>
<p>Behdad Chalaki behdad_chalaki@honda-ri.com 
Honda Research Institute USA, Inc</p>
<p>Vaishnav Tadiparthi vaishnav_tadiparthi@honda-ri.com 
Honda Research Institute USA, Inc</p>
<p>Kwonjoon Lee kwonjoon_lee@honda-ri.com 
Honda Research Institute USA, Inc</p>
<p>Ehsan Moradi-Pari emoradipari@honda-ri.com 
Honda Research Institute USA, Inc</p>
<p>Charles Michael Lewis 
University of Pittsburgh</p>
<p>Katia P Sycara sycara@andrew.cmu.edu 
Carnegie Mellon University</p>
<p>Language Grounded Multi-agent Communication for Ad-hoc Teamwork
25 Sep 2024393953E61565D576E7529AB9D258EDAFarXiv:2409.17348v1[cs.MA]
Multi-Agent Reinforcement Learning (MARL) methods have shown promise in enabling agents to learn a shared communication protocol from scratch and accomplish challenging team tasks.However, the learned language is usually not interpretable to humans or other agents not co-trained together, limiting its applicability in ad-hoc teamwork scenarios.In this work, we propose a novel computational pipeline that aligns the communication space between MARL agents with an embedding space of human natural language by grounding agent communications on synthetic data generated by embodied Large Language Models (LLMs) in interactive teamwork scenarios.Our results demonstrate that introducing language grounding not only maintains task performance but also accelerates the emergence of communication.Furthermore, the learned communication protocols exhibit zero-shot generalization capabilities in ad-hoc teamwork scenarios with unseen teammates and novel task states.This work presents a significant step toward enabling effective communication and collaboration between artificial agents and humans in real-world teamwork settings.</p>
<p>Introduction</p>
<p>Effective communication is crucial for multiple agents to collaborate and solve team tasks.Especially in ad-hoc teamwork scenarios where agents do not coordinate a priori, the ability to share information via communication is the keystone for successful team coordination and good team performance [29].Multi-Agent Reinforcement Learning (MARL) methods have shown promise in allowing agents to learn a shared communication protocol by maximizing the task reward [37,36].However, merely maximizing the utility of communication in specific task goals might compromise task-agnostic objectives such as optimizing language complexity and informativeness [39,15], making the learned communication protocols 1) hard to interpret for humans or other agents that are not co-trained together [17,19,6] and 2) highly data-inefficient [9].In addition, most previous works learn com-munication language with atomic symbols or a combination leaving the relation between symbols unexplored [49], while only a few researches attempt to learn a semantic space for zero-shot communication of unseen states [41,16].One of the popular directions for interpretable communication is to regulate the learning process with external knowledge from human languages [40,21,27,1].However, this process is challenging due to the divergent characteristics between human and machine languages by nature [5].For example, most agent training methods (e.g., deep reinforcement learning) require a huge amount of data that is impractical for human-in-the-loop training or even collecting from humans [27].</p>
<p>The rise of Large Language Models (LLMs) provides new opportunities in grounding agent communication with human language.Recent generative text models fine-tuned with human instructions (e.g., GPT-4, Llama 3) show reasonable capabilities in completing team tasks and communicating in a human-like fashion via embodied interaction [35,24].Essentially, LLMs encapsulate a highly trained model of human language patterns in teamwork, allowing them to generate descriptions and responses that are well-grounded in natural language.For the purpose of guiding multi-agent communication, they represent the most generally available reference based on a vast corpus of human language data that would be infeasible to collect through other means.However, LLMs are known to suffer from a lack of grounding with the task environments (a.k.a.hallucinations), which prevents embodied agents from generating actionable plans [28].While attempts have been made to ground LLMs with reinforcement learning or interactive data collected from environments [46,4,38], none of them involve teamwork nor communication among multiple embodied agents.</p>
<p>In this paper, we propose a novel computational pipeline for artificial agents to learn humaninterpretable communication for ad-hoc human-agent teamwork.Specifically, we use synthetic data generated by LLM agents in interactive teamwork scenarios to align the communication space between MARL agents with human natural language.Learning signals from both language grounding and environment reinforcement regulate the emergence of a communication protocol to optimize both team performance and alignment with human language.We have also evaluated the learned communication protocol in ad-hoc teamwork scenarios with unseen teammates and novel task states.The aligned communication space enables translation between high-dimensional embeddings and natural language sentences, which facilitates ad-hoc teamwork.The proposed computational pipeline does not depend on specific MARL architecture or LLMs and should be generally compatible.We have sought to minimize the influence of prompt engineering to ensure the seamless applicability of our approach in diverse environments.To the best of our knowledge, this work is among the very first attempts at training MARL agents with human-interpretable natural language communication and evaluating them in ad-hoc teamwork experiments.</p>
<p>Related Work</p>
<p>Multi-Agent Communication</p>
<p>Reinforcement learning has been used to coordinate the teamwork and communication among multiple agents in partially observable environments.In earlier works such as DIAL [11], CommNet [37], and IC3Net [36], agents learn differentiable communication in an end-to-end fashion under the pressure of maximizing task reward.Other works use shared parameters [16] or a centralized controller [32] to stabilize the non-stationary learning process of multi-agent communication.More recently, representation learning methods such as autoencoder [25] and contrastive learning [26] are used to ground an agent's communication on individual observations.However, comm-MARL methods usually suffer from overfitting to specific interlocutors trained together [18].The learned communication protocols can not be understood by unseen teammates in ad-hoc teams, let alone another human.</p>
<p>Another relevant trend of research is Emergent Communication (EC), where researchers focus more on simulating the development of natural (i.e., symbolic) language with artificial agents [18,14].The most common task scenarios used in the EC community are reference games or Lewis signaling games [23], in which a speaker must describe an object to a listener, who must then recognize it among a set of distractions [41].However, previous research has shown that learning EC in more complicated, scaled-up, and multi-round interactive task scenarios can be challenging or even infeasible [9,7].Even in situations where agents can learn to communicate, the learned protocols are usually either not human-interpretable [17] or semantically drifting from human language [22].</p>
<p>Human-Interpretable Communication</p>
<p>To address the above-mentioned challenges, several recent works propose human-interpretable communication in RL settings.Lazaridou et al. [21] leverage pre-trained task-specific language models to provide high-level guidance for natural language communication.A few other works align lowlevel communication tokens with human language [20,40], or learn discrete prototype communication in a semantically meaningful space [41,16].But as pointed out in several studies [8,48], the low mutual intelligibility between human language and neural agent communication makes the alignment process non-trivial.Our work is closest to [27], in which researchers alternate imitating human data via supervised learning and self-play to maximize reward in a reference game.The differences are that in [27] authors try to train agents for reference games in an end-to-end fashion with backpropagation, while we train MARL agents in interactive team tasks.The exploration of this research direction is still very limited, as no previous work has ever evaluated natural language communication agents within interactive task environments and for ad-hoc human-agent teams.</p>
<p>Language-Grounded Reinforcement Learning</p>
<p>Reinforcement learning is known to struggle with long-horizon problems with sparse reward signals [30].Natural language guidance has been used to provide auxiliary rewards to improve the data efficiency and learning robustness [43].Goyal et al. [12] use step-by-step natural language instructions provided by human annotators to construct auxiliary reward-learning modules, encouraging agents to learn from expert trajectories.Narasimhan et al. [31] research the impact of language grounding on representation learning and transfer learning of RL agents in a 2D game environment.Additional work has explored grounding RL with other formats of materials such as game manuals [13,45] and human commands [47].However, none of those works has ever used the communication messages as the language ground nor guided the information-sharing process in multi-agent teamwork.</p>
<p>Preliminaries</p>
<p>We formulate the problem as a decentralized partially observable Markov Decision Process [34], which can be formally defined by the tuple (I, S, A, C, T , Ω, O, R, γ), where I is the finite set of n agents, s ∈ S is the global state space, A = × i∈I A i is the set of actions, and C = × i∈I C i is the set of communication messages for each of n agents.T : S × A → S is the transition function that maps the current state s t into next state s t+1 given the joint agent action.In our partially observable environments, each agent receives a local observation o i ∈ Ω according to observation function O : S × C × I → Ω.Finally, R : S × A → R is the reward function, while γ ∈ [0, 1) is the discount factor.At each time stamp t, each agent i takes an action a i t and sends out a communication message c i t after receiving the partial observation of task state s t along with all messages sent by other agents from last time stamp c t−1 .Each agent then receives the individual reward r i t ∈ R(s t , a t ).We consider fully cooperative settings in which the reinforcement learning objective is to maximize the total expected return of all agents:
max π i :Ω→A×C E t∈T i∈I γ t R(s i t , a i t )|a i t ∼ π i , o i t ∼ Ω(1)
We borrow the definition of language learning from [27].We define a target language L * ∈ L that we want the agents to learn, assuming L is the set of natural language and L * is the optimal communication language for achieving a good team performance in the specific task.Specifically, we consider a language L ∈ L to be a set of communication messages C which are mapped from agent observations to communication messages defined as L : Ω → C. In typical RL settings, this can be thought of as the mapping between input observation vectors and English descriptions of the observation.We consider a dataset D consisting of |D| (observation, action) pairs, which comes from expert trajectories generated by LLM embodied agents using the target language L * .The language learning objective is to train agents to speak language L * in order to collaborate with experts in ad-hoc teamwork.It is worth noting that we want the learned language to generalize to unseen examples that are not contained in D.</p>
<p>Figure 1: Illustrations of our proposed computational pipeline and evaluation environments.Predator Prey is a gridworld environment conceptualizing a team of predators with partial observation trying to search for a static prey.The task goal is for all predators to reach the prey location within the time limit.USAR simulates a team of specialists searching for and defusing bombs in an unknown environment.Because each specialist has the unique capability of defusing bombs in different colors, the team must coordinate to complete the task efficiently.</p>
<p>To train agents that perform well on the team task and speak human-interpretable language, we need to solve a multi-objective learning problem by combining the learning signals from both environment reward and supervised dataset D. The process of training an optimal communication-action policy can be defined as solving an optimization problem with two constraints 1) agents must learn to communicate effectively to maximize team performance and 2) agents must learn to use similar language as in the supervised dataset D.</p>
<p>Language Grounded Multi-agent Communication</p>
<p>In this section, we propose a computational pipeline for learning language-grounded multi-agent communication (LangGround).The general framework of LangGround is illustrated in Figure 1 consisting of two parts: collecting grounded communication from LLM agents and aligning MARL communication with language grounds.</p>
<p>Grounded communication from LLM agents</p>
<p>We use embodied LLM agents to collect samples of target language L * .To allow LLM agents to interact with the task environments, a text interface I is implemented following [24] to translate between abstract representations and English descriptions.Essentially, each of the n LLM agents is provided with general prompts about the team task and instructed to collaborate with others to achieve the common goal.At each time stamp t, the LLM agent i receives English descriptions of its own observation of the environment I(o i t ) which also includes communication messages from teammates in the last time stamp C t−1 .The LLM agent is prompted to output its next action and communication message which are then encoded into abstract A i t and C i t and used to update the task environment.</p>
<p>Theoretically, we construct a text environment in parallel with the actual task environment to ensure that embodied LLM agents are exposed to the equivalent information as RL agents, albeit in a different format.The action and communication policy of LLM agents emerge from the backbone LLM, since the provided prompts do not include any explicit instructions on team coordination or communication strategy.In the results section we will confirm findings from previous literature that modern LLMs (e.g., GPT-4) is able to perform reasonably well and communicate effectively in collaborative tasks.The expert trajectories generated by LLMs are used to construct the supervised dataset D which maps individual agent's (observation, action) pairs to natural language communication messages.D is used during the training of MARL to provide supervised learning signals to align the learned communication protocols with human language.More implementation details of LLM agents and data collection can be found in the Appendix.</p>
<p>Multi-agent Reinforcement Learning with aligned communication</p>
<p>The MARL with communication pipeline is similar to IC3Net [36] in which each agent has an independent controller model to learn how and when to communicate.During each time-step, input observation o i t is encoded and passed into each agent's individual LSTM.The hidden state of LSTM h i t is then passed to the probabilistic gating function to decide whether to pass a message to other agents.A single-layer communication network transforms h i t into communication vector c i t .The mean communication vector of all agents is finally used by each agent's LSTM to produce the action a i t .To shape the learned communication protocol toward human language, we introduce an additional supervised learning loss during the training of MARL.Specifically, at each time step, we sample a reference communication from the dataset based on each agent's observation and action, D(o i t , a i t ), representing how a human (LLM) would communicate in the same situation.We then calculate the cosine similarity between the agent communication vector c i t and the word embedding of the reference communication c h .To align the communication space learned by MARL with the high-dimensional embedding space of natural language, we construct the supervised loss function as follows:
L sup = t∈T i∈I 1 − cos(c i t , D(o i t , a i t ))(2)D(o i t , a i t ) = c h if (o i t , a i t ) ∈ D 0 otherwise (3)
The construction of the communication message is shaped by two learning signals: 1) the reinforcement learning objective which determines useful information to share with other agents based on the policy loss gradient, and 2) the supervised learning objective which imitates the communication messages used by LLM agents in dataset D. The total loss function is formulated as follows:
L = L RL + λL sup(4)
The hyperparameter λ is used to scale the supervised loss.Each agent's policy is optimized with backpropagation to minimize the joint loss.</p>
<p>Experiments</p>
<p>Environments</p>
<p>In this section, we evaluate our proposed method in two multi-agent collaborative tasks with varied setups and different characteristics.The first environment, Predator Prey [36], is widely used in comm-MARL research as a benchmark.We include this environment to represent team tasks that require all agents to share their partial observations for the team to form a complete picture of the task state.The second environment, Urban Search &amp; Rescue (USAR) [24,33], presents a more demanding challenge due to the inclusion of heterogeneous team members and the temporal dependence between their behaviors.Here, agents must communicate not only their observations but also their intentions and requests to effectively coordinate.Illustrations of the two environments are shown in Figure 1, and more details are provided in the Appendix.</p>
<p>Experiment setups</p>
<p>We compare our proposed pipeline LangGround against previous methods, including IC3Net [36], autoencoded communication (aeComm) [25], Vector-Quantized Variational Information Bottleneck (VQ-VIB) [39], prototype communication (protoComm) [41], and a control baseline of independent agents without communication (noComm).aeComm represents the state-of-the-art multi-agent communication methods that grounds communication by reconstructing encoded observations.It has been shown to outperform end-to-end RL methods and inductive biased methods in independent, decentralized settings.VQ-VIB and prototype-based method are representative solutions for humaninterpretable communication, which learn a semantic space for discrete communication tokens and perform reasonably well in human-agent teams.Finally, IC3Net has a similar architecture to our proposed pipeline representing an ablating baseline without language grounding.</p>
<p>All methods are implemented with the same centralized training decentralized execution (CTDE) architecture for a fair comparison.Each agent has an observation encoder, an LSTM for action policy, a single-layer fully-connected neural network for transferring hidden states into communication messages, and a gate function for selectively sharing messages.The parameters of the action policy and obs encoder are shared during training for a more stable learning process.We use REINFORCE [44] to train both the gating function and policy network.The communication messages are continuous vectors of dimension D = 256.</p>
<p>Results</p>
<p>As for the evaluation matrices, we first consider if LangGround allows MARL agents to complete collaborative tasks successfully (i.e., task utility) and converge to a shared communication protocol quickly (i.e., data-efficiency), in comparison with other state-of-the-art methods as the baselines.</p>
<p>Then we analyze the properties of aligned communication space such as human interpretability, topographic similarity, and zero-shot generalizability, to show how close the learned language is to the target human natural language.Finally, we evaluate the ad-hoc teamwork performance in which MARL agents must communicate and collaborate with unseen LLM teammates via natural language.</p>
<p>Task performance</p>
<p>In Figure 2, we compare the task performance of multi-agent teams using different communication methods by plotting out the average episode length during training over 3 random seeds with standard errors.</p>
<p>In Predator Prey vision = 1 (i.e., pp v1 ), our method LangGround achieves a similar final performance with IC3Net and aeComm, outperforming other baselines.However, the improvement is not outstanding due to the simplicity of the task environment.In Predator Prey vision = 0 (i.e., pp v0 ), the predator's vision range is limited to their own location making effective information sharing more important in solving this search task.As shown in the middle figure, LangGround has a comparable final performance with aeComm and outperforms other baselines.Finally, in the most challenging USAR environment, LangGround outperforms all baselines in solving the task in fewer steps with the same amount of training time-steps.In addition, language grounding also stabilizes the communication learning process such that the variance of LangGround is much smaller than other methods.</p>
<p>In summary, LangGround enables multi-agent teams to achieve on-par performance in comparison with SOTA multi-agent communication methods.Introducing language grounds as an auxiliary learning objective does not compromise the task utility of learned communication protocols while providing interpretability.We also present the most similar reference message from dataset D to illustrate the alignment between the agent communication space and the human language embedding space.</p>
<p>Aligned communication space</p>
<p>In addition to task utility, we are also interested in other properties of the learned communication space, such as human interpretability, topographic similarity, and zero-shot generalizability.</p>
<p>Semantically meaningful space</p>
<p>To evaluate whether the learned communication space is semantically meaningful, we visualize the learned communication embedding space by clustering message vectors sent by agents over 100 evaluation episodes in pp v0 , following [25].The high dimensional vectors are reduced to a two dimension space via t-SNE [42], and clustered with DBSCAN [10].As shown in Figure 3, agent communication messages can be clustered into several classifications with explicit meanings associated with agent observation from the environment.For example, the pink cluster on the right side corresponds to the situation where the agent locates in coordinates (0, 3) without vision of prey.We can look up from dataset D for the reference message that has the most similar word embedding with agent communication vectors in the pink cluster.The reference message (i.e., "moving down from (0, 3)") accurately refers to the agent observation, indicating the learned communication space is semantically meaningful and highly aligned with natural language embedding space.</p>
<p>Topographic similarity</p>
<p>The topographic similarity is defined by the correlation between object distances in the observation space and their associated signal distances in the communication space [2].This property is usually associated with language compositionality and ease of generalization.The intuition behind this measure is that agents should emit similar communication messages given semantically similar observations.We calculate this measure following [19], based on agent trajectories collected from 100 evaluation episodes in pp v0 .We first calculate 1) the cosine similarity between all pairs of communication vectors, and 2) the Euclidean distance between all pairs of agent locations.Then, we calculate the negative Spearman correlation ρ as the measure of topographic similarity.Table 1 indicates that our method (i.e., LangGround) results in the highest topographic similarity ρ = 0.67 among all other baselines, exhibiting a relatively more similar property as human language.</p>
<p>Human interpretabiliy</p>
<p>Given the goal of aligning agent communication with human language, it is intuitive to evaluate the human interpretability of language-grounded agent communication.We use the offline dataset D as the reference to calculate the similarity between communication messages shared by LangGround agents and LLM agents in same situations.Given 100 evaluation episodes in pp v0 , we calculate 1) the cosine similarity between word embedding and agent communication vectors, and 2) BLEU score between natural language messages and reference messages in D with the most similar word embedding as agent communication vectors.The results are shown in Table 2, demonstrating that LangGround achieves significant gains in both metrics, cosine similarity and BLEU score, compared to baselines without alignment.These measures for other baseline methods would be equivalent to random chance because none of them are grounded with language during training, and hence we do not compute their performance on these metrics.</p>
<p>Ad-hoc Teamwork</p>
<p>The ultimate goal of our proposed pipeline is to facilitate ad-hoc teamwork between unseen agents without pre-coordination.Here, we propose two experiments to evaluate the zero-shot generalizability and ad-hoc collaboration capability of our trained agents.</p>
<p>Zero-shot generalizability</p>
<p>One prerequisite of ad-hoc teamwork is the ability to communicate about unseen states to their teammates.We evaluate this capability by removing a subset of prey spawn locations from environment initialization of pp v0 and training LangGround agents from scratch.In this condition, agents would neither be exposed to nor receive any language grounding for those situations during training.During the evaluation, we record the communication messages used by agents in those unseen situations and compare them with ground truth communication from dataset D. Results show that agents are still able to complete tasks when the prey spawns in those 4 unseen locations.As shown in Table 3, the communication messages agents used to refer to those unseen locations are similar to natural language sentences generated by LLMs.These findings confirm the alignment between the agent communication space and the human language word embedding space in zero-shot conditions.More importantly, we show that LangGround is not merely a memorization of one-to-one mapping between observations and communications but also shapes the continuous communication space in a semantically meaningful way.</p>
<p>Ad-hoc teamwork between MARL and LLM agents</p>
<p>Finally, we evaluate the performance of our agents to work with unseen teammates in ad-hoc teamwork settings.Ad-hoc teams were evaluated on 8 episodes over 3 random seeds, resulting in a total of 24 evaluation episodes per condition.Ad-hoc teamwork performance is measured by the number of steps taken to complete the task; therefore, lower is better.Means and standard deviations of each condition are reported in Table 4.</p>
<p>We find that 1) homogeneous teams (i.e., LangGround and LLM) achieve better performance than ad-hoc teams because of their common understanding of both action and communication.As those agents are either co-trained together or duplicates of the same network, they form a stable strategy for team coordination and information sharing.Since ad-hoc teams (e.g., LangGround + LLM) were not co-trained together nor speak the same language, their decreased performance is expected.</p>
<p>2) The ad-hoc team performance of LangGround agents is better than noComm and aeComm agents in at least two out of three evaluation scenarios.Because aeComm is not aligned with human language, it serves as a baseline with a coordinated action policy and a random communication policy.The advantage of our method over aeComm and noComm merely comes from effective information sharing via shared language with unseen teammates.The empirical evidence presented in this section confirms the application of our method in ad-hoc teamwork.</p>
<p>Discussion</p>
<p>In this work, we developed a novel computational pipeline that enhanced the capabilities of MARL agents to interact with unseen teammates in ad-hoc teamwork scenarios.Our approach aligns the communication space of MARL agents with an embedding of human natural language by grounding agent communications on synthetic data generated by embodied LLMs in interactive teamwork scenarios.</p>
<p>Through extensive evaluations of the learned communication protocols, we observed a trade-off between utility and informativeness.According to the Information Bottleneck principle [39], informativeness corresponds to how well a language can be understood in task-agnostic situations, while utility corresponds to the degree to which a language is optimized for solving a specific task.By introducing the additional supervised learning signal, our method pushes the trade-off toward informativeness compared to traditional comm-MARL methods that merely optimize for utility.This results in the "inconsistent" patterns we observe in Figure 2 and Table 4 across different task scenarios.As language emerges under different pressures, the learned communication protocols fall at different points on the spectrum between utility and informativeness.In relatively easy tasks such as predator-prey, the learned communication is more optimized for informativeness, aligning better with human language and generalizing better in ad-hoc teamwork.In more challenging tasks such as USAR, the learned communication is more shaped toward task utility, resulting in faster convergence but worse generalizability to unseen teammates.</p>
<p>Additionally, we found that introducing language grounding does not compromise task performance but even accelerates the emergence of communication, unlike the results reported in previous literature, where jointly optimizing communication reconstruction loss and RL loss leads to a drop in task performance [25,26].The main reason for this contradiction is that our dataset D consists of expert trajectories from LLM embodied agents with a well-established grounding on the task.Therefore, the language grounding loss not only shapes the communication but also guides the action policy of MARL agents by rewarding behavior cloning and providing semantic representations of input observations.</p>
<p>We believe our work would benefit the broader society for the following reasons.This research provides empirical evidence of linguistic principles during language evolution among neural agents, which might provide insights for broader research communities, including computational linguistics, cognitive science, and social psychology.The usage of embodied LLM agents as interactive simulacra of human team behaviors has a broad impact since it has potential applications in social science and may deepen our understanding of modern LLMs.Most importantly, our proposed pipeline takes initial steps in enabling artificial agents to communicate and collaborate with humans via natural language, shedding light on the broad research direction of Human-centered AI.</p>
<p>As for future directions, we plan to evaluate our proposed pipeline in more complicated task environments at scale and experiment with different selections of MARL algorithms, backbone LLMs, and word embeddings.Particularly, we plan to replace the use of a static dataset D by querying LLMs online during the training of MARL.This may allow us to capture complex information exchanged among team members in addition to individual observations, such as beliefs, intents, and requests.</p>
<p>A Appendix</p>
<p>A.1 Implementation details</p>
<p>A.1.1 Embodied LLM agents</p>
<p>Large language models are prompted to interact with the task environments in team tasks.We implement embodied LLM agents based on the pipeline proposed in [24], where agents are augmented with explicit belief state and communication for better team collaboration capability.Each agent keeps a memory of his own observations from the environment and communication messages from other team members.Exact prompts can be found in the code within the supplementary materials.</p>
<p>The design principles are that we only provide general rules about the task environments without explicitly instructing them on any coordination or communication strategy.We attempt to minimize the influence of prompt engineering to ensure the seamless applicability of our approach in diverse environments.In our language grounding data collection and ad-hoc teamwork experiments, we use OpenAI's API to call gpt-4-0125-preview as the backbone pre-trained model and set the temperature parameter to 0 to ensure consistent outputs.</p>
<p>Example prompts for LLM agents in the U SAR environment are provided below:</p>
<p>Welcome to our interactive text game!In this game, you'll assume the role of a specialist on a search and rescue team.Alongside two other players, you'll navigate a five-room environment with a mission to defuse five hidden bombs.</p>
<p>The Map: Imagine a network of rooms represented by a connected graph where each node corresponds to a room, and the edges between nodes depict hallways.</p>
<p>The rooms are numbered 0, 3, 6, 5, and 8. Room 0 is connected to all other rooms.Room 5 shares a hallway with room 6.Room 3 is linked to room 8.And room 8 is also connected with room 6.You can only travel to adjacent, directly connected rooms at each turn.</p>
<p>The Challenge: Scattered among these rooms are five bombs, each coded with different phases represented by colors.To defuse them, you'll need to use the correct wire-cutting tools in the correct sequence.There are one-phase, two-phase, and three-phase bombs, needing 1, 2, or 3 color-coded tool applications in sequence to disarm.For instance, a bomb with a red-green phase sequence requires the red tool first, then the green one.Points are awarded based on the number of tools used for defusing a bomb, with each tool use worth 10 points.Your task is to maximize the team score as soon as possible.The challenge is that the bomb locations and sequences are unknown to players at the start.Tools: Each player is equipped with two color-coded wire cutters.As player Alpha, you have red and green tools, player Bravo wields green and blue, and player Charlie possesses blue and red.Actions: Each round, you can opt to do one of the following: 1) Move to an adjacent room, 2) Inspect a bomb's phase sequence in your current room, or 3) Apply your wire cutters to a bomb in the current room.</p>
<p>Communications: In addition to selecting an action to take from the above list, you can also send communication message texts to both of your teammates in each round.The message text you sent will be shared with both of your teammates in their observation in the next round.Observation: While you can only see what's in your current room and read text messages from teammates.You'll also be informed of the current round number, team score and the current location of your teammates.Your teammates have the same observability as you.They will not be able to know your action and its consequences unless you explicitly communicate.</p>
<p>To facilitate our interaction, reply your action selection and communication messages in this fixed format: Action selection: Your action.Message to Team: "Your Message".To move to an adjacent room, say: 'Move to Room X'.To inspect the sequence of a bomb in your current room, say: 'Inspect Bomb'.To apply a wire cutter tool, say: 'Apply X Tool'.Remember, your replies must adhere strictly to these rules.Feel free to ask clarifying questions if needed.I'll supply the necessary information as we progress.Are you ready to take on this explosive challenge?</p>
<p>Example interactions between LLM agents and environments are provided below: In this task, n predators with a limited range of vision v need to search for stationary prey on an x by x grid-world environment.Each predator receives a positive reward upon reaching the prey location.Each episode ends when all predators reach the prey or exceed the maximum number of steps T .The initial locations of predators and the prey might spawn anywhere on the map.At each timestamp, the predator agent receives a partial observation of v by v grids around its own location and may select a movement action to navigate through the map.Since this is a collaborative task, predators must learn to communicate their partial observations to allow for optimal navigation of the team.We consider Predator Prey to be a more challenging task since it has a higher-dimensional observation space and a more complex action space [25].Through this environment, we aim to demonstrate that aligning agents' communication with human language is a straightforward yet effective method of grounding their communication with task observations.
Env</p>
<p>A.2.2 USAR</p>
<p>The USAR task environment is designed to simulate the collaborative and problem-solving dynamics of a search and rescue mission.Three agents (i.e., Alpha, Bravo, and Charlie) need to collaborate in locating and defusing color-coded bombs hidden in an unexplored environment.Each bomb has unique phase sequences in m colors, which are not revealed until inspected by agents.Agents start with different colored cutters and must use them in the correct sequence to defuse bombs.The environment is represented as a graph, where each of the n rooms is a node, and the hallways connecting them are edges.At each timestamp, every agent can choose from three different types of actions: moving to one of the n rooms, inspecting a bomb's sequence in the current room, or utilizing one of the m wire-cutters.The size of the action space depends on the problem scale (i.e., n + m + 1).Agents' observations are limited to their current room's contents and agent status.The team is rewarded 10*x points when an x-phase bomb is successfully defused.An episode ends when the team has defused all bombs or exceeded the time limit.This task is designed to force team coordination since each team member has unique observations and capabilities.For example, each agent only has a subset of wire cutters and must coordinate with other teammates to defuse bombs with multiple phases.Therefore, an effective communication protocol is required for efficient information sharing and team synchronization.</p>
<p>A.2.3 Environment configurations</p>
<p>For the predator and prey environment, we use a map size of 5 by 5 with 3 predators and 1 prey.The predator's range of vision is manipulated to be either 0 or 1 to create two variants of the task environment.In the situation of vision = 0, predators cannot observe the prey until they jump onto the same location.We set the maximum episode length to 20 based on previous research [16].The USAR environment comprises five rooms (n = 5) and five bombs, including two single-phase, two double-phase, and one triple-phase bombs.The bomb phase might have three different colors (m = 3).Each of the 3 agents spawns with 2 different wire cutters, forcing the team to collaborate in defusing bombs with multiple phases.Each successfully defused bomb awards the team 10 points per processed phase, resulting in 90 as the maximum score per mission.We set the maximum episode length to 100 based on previous research [24].</p>
<p>A.2.4 Text game interface</p>
<p>The initial task environments of Predator Prey and USAR are implemented for MARL agents based on Gym API [3].To facilitate interaction between LLM-based agents with the environment, we implement a rule-based text interface for each task.At each timestamp, LLM agents sequentially interact with the environment, receiving observations and performing actions via natural language interaction.Additionally, they are allowed to broadcast communication messages in natural language which are appended with the observation text and sent to all team members in the next round.It is worth noting that LLM agents receive equivalent information as MARL agents, limited to individual agent's partial observation.</p>
<p>The text interface facilitates communication between the game engine and the language model agents by converting game state observations into natural language descriptions and mapping agent responses back to valid game actions.To generate observations, the interface extracts relevant state features from the game engine, such as the current round number, cumulative team score, action feedback, visible objects, and communication messages from other agents.It then populates predefined sentence templates with these extracted features to produce a structured natural language description of the current game state.Action encoding relies on keyword matching, as the language models are instructed to format their responses using specific keywords and structures.The interface scans the agent's response for these predefined keywords and maps them to corresponding game actions.In cases where an agent's response is invalid or ambiguous, such as attempting to perform an action in an incorrect location, the interface generates an informative error message based on predefined rules and templates.For instance, if an agent attempts to inspect a non-existent bomb, the interface might respond with the following error message: "There is no bomb in the current location, Room X, for you to inspect."This targeted feedback helps the agents refine their actions to comply with the game's rules and constraints.</p>
<p>A.3 Data collection details</p>
<p>A.3.1 LangGround dataset</p>
<p>In order to construct dataset D, we collected expert trajectories from embodied LLM agents powered by GPT-4 in interactive task scenarios.As shown in Table 4, teams consisting of pure LLM agents perform reasonably well in comparison to MARL methods.Therefore, we believe their action and communication policy can be used in guiding MARL agents.In U SAR, we collected 50 episodes resulting in 2550 pairs of (observation, action) and communication messages of individual agents.</p>
<p>The number of data pairs is 1893 for pp v0 and 2493 for pp v1 , respectively.To facilitate the alignment of agent communication space and natural language, we use OpenAI's word embedding api (i.e., text-embedding-3-large) to translate each natural language message into a high-dimensional vector with the same dimension (i.e., D = 256) as agent communication vectors.</p>
<p>A.3.2 Ad-hoc teamwork</p>
<p>Due to the restrictions of resources and time, we use embodied LLM agents to emulate human behaviors in human-agent teams.Using a similar framework described earlier, we put 2 MARL agents and 1 LLM agent into a team and expose them to the task environment via Gym API and text interface.Natural language communication messages from LLMs are embedded using OpenAI's word embedding API and sent to MARL agents.The communication vectors from MARL agents are translated to English sentences via cosine similarity matching in dataset D.</p>
<p>Figure 2 :
2
Figure 2: Learning curves of LangGround in comparison with baseline methods.The y-axis is task performance measured by the episode length until task completion, which is lower the better.The x-axis is the number of training timestamps.Shaded areas are standard errors over three random seeds.</p>
<p>Figure 3 :
3
Figure 3: Learned communication embedding space.Communication vectors between agents in pp v0 are visualized with t-SNE and clustered with DBSCAN.Two semantically meaningful clusters are identified as examples, each corresponding to a specific agent observation.We also present the most similar reference message from dataset D to illustrate the alignment between the agent communication space and the human language embedding space.</p>
<p>Table 1 :
1
Topographic similarity in pp v0
MethodsTopo Sim ρLangGround 0.67±0.07Env∆ Cos sim ∆ Bleu scoreIC3Net0.54±0.14pp v10.82±0.02 0.52±0.03aeComm0.37±0.05pp v00.81±0.03 0.45±0.12protoComm 0.35±0.35U SAR 0.79±0.12 0.42±0.04</p>
<p>Table 2 :
2
Similarity gain w/ LangGround</p>
<p>Table 3 :
3
Zero-shot generalizability in pp v0 Prey Loc Cos sim Bleu score Example message
(1,1)0.810.41Moving up to converge on prey location at (1,0) for capture(1,3)0.810.27Converging on prey location at (1,3)(3,1)0.820.51Moving up toward prey location at (3,1)(3,3)0.780.72Converging on prey location at (3,3)</p>
<p>Table 4 :
4
Ad-hoc teamwork performance (lower is better)
Team compositionpp v1pp v0U SARLangGround4.3 ± 1.2010.9 ± 4.53 22.0 ± 4.24LLM6.8 ± 5.2011.6 ± 5.30 15.9 ± 3.37LangGround + LLM 8.5 ± 5.7615.5 ± 4.80 23.2 ± 10.61aeComm + LLM10.3 ± 6.46 17.5 ± 4.60 20.3 ± 9.07noComm + LLM10.6 ± 5.73 20.0 ± 0.00 32.4 ± 13.47</p>
<p>:</p>
<p>109,15,41,36,33,25] 3 with Bomb 5. What is your next action?Alpha: Action selection: Move to Room 5. Message to Team: "I am moving to Room 5 t help Charlie with Bomb 3.The implementation of LangGround and baseline MARL methods is based on open-sourced codebase provided by the original paper authors[39,15,41,36,33,25].Agents were trained with the same setup and hyperparameters after empirical searching.The batch size is 500, and the number of update iterations in an epoch is10.Training on pp v0 and U SAR takes 2000 epochs and 1e7 timestamps, which takes about 4 hours to complete.Training on pp v1 takes 500 epochs and 2.5e6 timestamps, which takes about 1.5 hours to complete.We use a learning rate of 0.0001 for U SAR and 0.001 for pp.MARL agent's action policy is an LSTM with a hidden layer of size 256.Communication vectors are exchanged one round at each timestamp.The supervised learning weight λ is 1 in pp and 10 in U SAR. VQ-VIB and prototype communication agents were allowed to use 58 different discrete tokens to share information during teamwork.All experiments were conducted on a machine with a 14-core Intel(R) Core(TM) i9-12900H CPU and 64GB memory.
Env: You can not directly move to Room 5 because it is not adjacent to yourcurrent location; Room 3. Consider taking a detour to another room first and thenmove to your destination.Example communication among LLM agents in U SAR:Alpha: I am inspecting Bomb 1 in Room 0. Bravo; please move to Room 3.Charlie; please move to Room 5.Bravo: I am moving to Room 3 as instructed. Alpha; please defuse Bomb 1 inRoom 0 using the red tool.Charlie: I am moving to Room 5 to search for more bombs.A.1.2 LangGround and baseline MARL agentsA.2 Environment detailsA.2.1 Predator Prey</p>
<p>Community regularization of visually-grounded dialog. Akshat Agarwal, Swaminathan Gurumurthy, Vasu Sharma, Mike Lewis, Katia Sycara, Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems. the 18th International Conference on Autonomous Agents and MultiAgent Systems2019</p>
<p>Understanding linguistic evolution by visualizing the emergence of topographic mappings. Henry Brighton, Simon Kirby, Artificial life. 1222006</p>
<p>Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba, Openai gym. 2016</p>
<p>Grounding large language models in interactive environments with online reinforcement learning. Thomas Carta, Clément Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, Pierre-Yves Oudeyer, International Conference on Machine Learning. PMLR2023</p>
<p>Anti-efficient encoding in emergent communication. Rahma Chaabouni, Eugene Kharitonov, Emmanuel Dupoux, Marco Baroni, Advances in Neural Information Processing Systems. 201932</p>
<p>Communicating artificial neural networks develop efficient color-naming systems. Rahma Chaabouni, Eugene Kharitonov, Emmanuel Dupoux, Marco Baroni, Proceedings of the National Academy of Sciences. 11812e20165691182021</p>
<p>Angeliki Lazaridou, and Bilal Piot. Emergent communication at scale. Rahma Chaabouni, Florian Strub, Florent Altché, Eugene Tarassov, Corentin Tallec, Elnaz Davoodi, Kory Wallace Mathewson, Olivier Tieleman, International conference on learning representations. 2021</p>
<p>Communication breakdown: On the low mutual intelligibility between human and neural captioning. Roberto Dessì, Eleonora Gualdoni, Francesca Franzon, Gemma Boleda, Marco Baroni, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Angeliki Lazaridou, and Thore Graepel. Biases for emergent communication in multi-agent reinforcement learning. Tom Eccles, Yoram Bachrach, Guy Lever, Advances in neural information processing systems. 201932</p>
<p>A density-based algorithm for discovering clusters in large spatial databases with noise. Martin Ester, Hans-Peter Kriegel, Jörg Sander, Xiaowei Xu, kdd. 199696</p>
<p>Learning to communicate with deep multi-agent reinforcement learning. Jakob Foerster, Alexandros Ioannis, Nando De Assael, Shimon Freitas, Whiteson, Advances in neural information processing systems. 292016</p>
<p>Using natural language for reward shaping in reinforcement learning. Prasoon Goyal, Scott Niekum, Raymond J Mooney, Proceedings of the 28th International Joint Conference on Artificial Intelligence. the 28th International Joint Conference on Artificial Intelligence2019</p>
<p>Grounding language to entities and dynamics for generalization in reinforcement learning. Victor Y Austin W Hanjie, Karthik Zhong, Narasimhan, International Conference on Machine Learning. PMLR2021</p>
<p>On the role of emergent communication for social learning in multi-agent reinforcement learning. Seth Karten, Siva Kailas, Huao Li, Katia Sycara, arXiv:2302.142762023arXiv preprint</p>
<p>Intent-grounded compositional communication through mutual information in multi-agent teams. Seth Karten, Katia Sycara, Workshop on Decision Making in Multi-Agent Systems at International Conference on Intelligent Robots and Systems (IROS). 2022</p>
<p>Interpretable learned emergent communication for human-agent teams. Seth Karten, Mycal Tucker, Huao Li, Siva Kailas, Michael Lewis, Katia Sycara, IEEE Transactions on Cognitive and Developmental Systems. 2023</p>
<p>Natural language does not emerge 'naturally'in multi-agent dialog. Satwik Kottur, José Moura, Stefan Lee, Dhruv Batra, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language Processing2017</p>
<p>Emergent multi-agent communication in the deep learning era. Angeliki Lazaridou, Marco Baroni, arXiv:2006.024192020arXiv preprint</p>
<p>Emergence of linguistic communication from referential games with symbolic and pixel input. Angeliki Lazaridou, Karl Moritz Hermann, Karl Tuyls, Stephen Clark, International Conference on Learning Representations. 2018</p>
<p>Multi-agent cooperation and the emergence of (natural) language. Angeliki Lazaridou, Alexander Peysakhovich, Marco Baroni, arXiv:1612.071822016arXiv preprint</p>
<p>Multi-agent communication meets natural language: Synergies between functional and structural language learning. Angeliki Lazaridou, Anna Potapenko, Olivier Tieleman, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Countering language drift via visual grounding. Jason Lee, Kyunghyun Cho, Douwe Kiela, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)2019</p>
<p>Convention: A philosophical study. David Lewis, 2008John Wiley &amp; Sons</p>
<p>Theory of mind for multi-agent collaboration via large language models. Huao Li, Yu Chong, Simon Stepputtis, Joseph P Campbell, Dana Hughes, Charles Lewis, Katia Sycara, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Learning to ground multi-agent communication with autoencoders. Toru Lin, Jacob Huh, Christopher Stauffer, Ser , Nam Lim, Phillip Isola, Advances in Neural Information Processing Systems. 202134</p>
<p>Learning multi-agent communication with contrastive learning. Long Yat, Biswa Lo, Jakob Nicolaus Sengupta, Michael Foerster, Noukhovitch, The Twelfth International Conference on Learning Representations. 2023</p>
<p>On the interaction between supervision and self-play in emergent communication. Ryan Lowe, Abhinav Gupta, Jakob Foerster, Douwe Kiela, Joelle Pineau, International Conference on Learning Representations. 2019</p>
<p>Kyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy Kanwisher, Joshua B Tenenbaum, Evelina Fedorenko, arXiv:2301.06627Dissociating language and thought in large language models: a cognitive perspective. 2023arXiv preprint</p>
<p>Harel Yedidsion, and Peter Stone. A penny for your thoughts: The value of communication in ad hoc teamwork. Reuth Mirsky, William Macke, Andy Wang, International Joint Conference on Artificial Intelligence. 2020</p>
<p>Human-level control through deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, nature. 51875402015</p>
<p>Grounding language for transfer in deep reinforcement learning. Karthik Narasimhan, Regina Barzilay, Tommi Jaakkola, Journal of Artificial Intelligence Research. 632018</p>
<p>Multi-agent graph-attention communication and teaming. Yaru Niu, Matthew C Rohan R Paleja, Gombolay, AAMAS. 20212120</p>
<p>Deep interpretable models of theory of mind. Ini Oguntola, Dana Hughes, Katia Sycara, 2021 30th IEEE international conference on robot &amp; human interactive communication (RO-MAN). IEEE2021</p>
<p>A concise introduction to decentralized POMDPs. Christopher Frans A Oliehoek, Amato, 2016Springer1</p>
<p>Generative agents: Interactive simulacra of human behavior. Sung Joon, Park, O' Joseph, Carrie Jun Brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology. the 36th Annual ACM Symposium on User Interface Software and Technology2023</p>
<p>Learning when to communicate at scale in multiagent cooperative and competitive tasks. Amanpreet Singh, Tushar Jain, Sainbayar Sukhbaatar, International Conference on Learning Representations. 2018</p>
<p>Learning multiagent communication with backpropagation. Sainbayar Sukhbaatar, Rob Fergus, Advances in neural information processing systems. 292016</p>
<p>True knowledge comes from practice: Aligning llms with embodied environments via reinforcement learning. Weihao Tan, Wentao Zhang, Shanqi Liu, Longtao Zheng, Xinrun Wang, Bo An, arXiv:2401.141512024arXiv preprint</p>
<p>Trading off utility, informativeness, and complexity in emergent communication. Mycal Tucker, Roger Levy, Julie A Shah, Noga Zaslavsky, Advances in neural information processing systems. 202235</p>
<p>Generalization and translatability in emergent communication via informational constraints. Mycal Tucker, Roger P Levy, Julie Shah, Noga Zaslavsky, NeurIPS 2022 Workshop on Information-Theoretic Principles in Cognitive Systems. 2022</p>
<p>Emergent discrete communication in semantic spaces. Mycal Tucker, Huao Li, Siddharth Agrawal, Dana Hughes, Katia Sycara, Michael Lewis, Julie A Shah, Advances in Neural Information Processing Systems. 342021</p>
<p>Visualizing data using t-sne. Laurens Van Der Maaten, Geoffrey Hinton, Journal of machine learning research. 9112008</p>
<p>Grounding natural language commands to starcraft ii game states for narration-guided reinforcement learning. Nicholas Waytowich, Sean L Barton, Vernon Lawhern, Ethan Stump, Garrett Warnell, Artificial intelligence and machine learning for multi-domain operations applications. SPIE201911006</p>
<p>Simple statistical gradient-following algorithms for connectionist reinforcement learning. Williams Ronald, Machine learning. 831992</p>
<p>Read and reap the rewards: Learning to play atari with the help of instruction manuals. Yue Wu, Yewen Fan, Paul Pu Liang, Amos Azaria, Yuanzhi Li, Tom M Mitchell, Advances in Neural Information Processing Systems. 202436</p>
<p>Language models meet world models: Embodied experiences enhance language models. Jiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang, Zhiting Hu, Advances in neural information processing systems. 202436</p>
<p>Grounded reinforcement learning: Learning to win the game under human commands. Shusheng Xu, Huaijie Wang, Yi Wu, Advances in Neural Information Processing Systems. 202235</p>
<p>Linking emergent and natural languages via corpus transfer. Shunyu Yao, Mo Yu, Yang Zhang, Joshua B Karthik R Narasimhan, Chuang Tenenbaum, Gan, arXiv:2203.133442022arXiv preprint</p>
<p>A survey of multi-agent deep reinforcement learning with communication. Changxi Zhu, Mehdi Dastani, Shihan Wang, Autonomous Agents and Multi-Agent Systems. 38142024</p>            </div>
        </div>

    </div>
</body>
</html>
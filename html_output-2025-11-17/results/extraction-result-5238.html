<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5238 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5238</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5238</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-269762344</p>
                <p><strong>Paper Title:</strong> Large Language Models for Graphs: Progresses and Directions</p>
                <p><strong>Paper Abstract:</strong> Graph neural networks (GNNs) have emerged as fundamental methods for handling structured graph data in various domains, including citation networks, molecule prediction, and recommender systems. They enable the learning of informative node or graph representations, which are crucial for tasks such as link prediction and node classification in the context of graphs. To achieve high-quality graph representation learning, certain essential factors come into play: clean labels, accurate graph structures, and sufficient initial node features. However, real-world graph data often suffer from noise and sparse labels, while different datasets have unique feature constructions. These factors significantly impact the generalization capabilities of graph neural networks, particularly when faced with unseen tasks. Recently, due to the efficent text processing and task generalization capability of large language models (LLMs), there has been a promising approach to address the challenges mentioned above by combining large language models with graph data. This tutorial offers an overview of incorporating large language models into the graph domain, accompanied by practical examples. The methods are categorized into three dimensions: utilizing LLMs as augmenters, predictors, and agents for graph learning tasks. We will delve into the current progress and future directions within this field. By introducing this emerging topic, our aim is to enhance the audience's understanding of LLM-based graph learning techniques, foster idea exchange, and encourage discussions that drive continuous advancements in this domain.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5238.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5238.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphGPT: Graph Instruction Tuning for Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-to-LLM instruction-tuning paradigm that aligns structure-rich graph tokens with natural language tokens so LLMs can comprehend and perform graph tasks in supervised and zero-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GraphGPT: Graph Instruction Tuning for Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph instruction-tuning (alignment of graph tokens and natural language tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Convert graph structure and elements into a sequence of 'graph tokens' that are aligned with natural language tokens via instruction tuning; i.e., build an instruction-tuning dataset where graph tokens (representing nodes, edges, attributes or substructures) are paired/mapped with natural language instructions/responses so an LLM learns to interpret graph information in text token space.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs / heterogeneous graphs (structure-rich graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Designed to expose graph structural information to LLMs by aligning structural tokens with text tokens; claimed to improve LLM comprehension of graphs and enable zero-shot and supervised performance; properties emphasized are expressivity (structure encoded as tokens) and compatibility with instruction-tuning, but potential token-length / context window constraints remain.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>A range of downstream graph tasks in both supervised and zero-shot settings (generic 'downstream tasks' as reported by the paper without task-by-task breakdown in this tutorial).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Paper claims state-of-the-art performance vs. prior approaches that either only use textual prompts or only feed embeddings, but no numeric comparisons or metric values are provided in this tutorial overview.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Main challenge: fitting structural graph information into LLM context and tokenization (limited input context length); ensuring faithful, compact tokenization without losing important structural details; tutorial notes general open questions about how best to align graph tokens with language tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Graphs: Progresses and Directions', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5238.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5238.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HiGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HiGPT: Heterogeneous Graph Language Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A heterogeneous graph language model that represents heterogeneous graphs for alignment with LLMs, enabling LLM-based graph tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>HiGPT: Heterogeneous Graph Language Model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Heterogeneous graph language representation (token alignment)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encode heterogeneous graph elements (different node/edge types, attributes) into a representation compatible with language-model tokens, likely by mapping heterogeneity-aware structural tokens or textualized descriptions of nodes/relations to the LLM input space (paper referenced as a heterogenous graph language model aligning structure and text).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Heterogeneous graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Aims to capture heterogeneity explicitly in the conversion to language tokens; intended to improve expressivity for typed nodes/edges and to allow LLMs to reason across heterogeneous relations. The tutorial notes scalability and heterogeneity handling as focal properties.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Graph-related tasks across heterogeneous graph settings (not enumerated in tutorial); implied use for supervised and zero-shot tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Reported as a recent work in heterogeneous-graph-to-language modeling; tutorial does not provide specific comparative numbers but lists it among works that obtain improved downstream performance when aligning structure-rich tokens to language models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Same high-level challenges: encoding complex heterogeneity within LLM token limits; trade-offs between faithfulness (preserving typed structural info) and compactness of textual/token representation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Graphs: Progresses and Directions', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5238.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5238.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphText</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphText: Graph Reasoning in Text Space</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pure text-based method that performs graph reasoning by converting graph structures into textual representations so LLMs can reason directly in text space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GraphText: Graph Reasoning in Text Space</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Textualization / text-space serialization of graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Serialize graph structure and attributes into plain text (natural language sentences or structured text) so the LLM receives the entire graph or relevant subgraph as text and performs reasoning; the approach relies on designing text encodings that capture edges, node attributes, and relevant substructures for the LLM to process.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs (graph reasoning tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Emphasizes interpretability (text is human-readable) and direct use by LLMs without specialized token schemas; however, textualization can be verbose (low compactness) and susceptible to input length limits, and potential information loss depends on serialization rules.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Graph reasoning tasks (the tutorial cites Graph-Text for pure text-based reasoning on fundamental graph reasoning tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Contrasted with methods that inject graph embeddings as tokens or align structure-rich tokens — text-space methods are simpler and human-readable but face context-length and verbosity trade-offs; tutorial does not report specific numeric comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Verbosity and context window constraints; risk of losing fine-grained structural fidelity in textual summaries; requires careful prompt/serialization design to ensure faithfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Graphs: Progresses and Directions', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5238.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5238.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph-ToolFormer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tool-augmented prompting approach that augments LLM prompts with graph-related tool calls (external tools) to enhance graph reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Prompt augmentation with external-tool-derived text (Toolformer-style augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Instead of fully serializing graphs into plain text tokens, prompts are augmented with tool-invocation tokens or calls (e.g., retrieval or graph-tool outputs) produced or validated by an external tool like ChatGPT/Toolformer; these augmentations inject structured graph information into the LLM prompt in a controlled manner.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs (fundamental graph reasoning tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Uses external tool outputs to enrich prompts, which can improve faithfulness and allow more complex graph operations without inflating the LLM input; mitigates some context-length issues by delegating computations to tools, but introduces dependence on tool correctness and extra system complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Fundamental graph reasoning tasks (paper uses Toolformer paradigm applied to graphs as noted in tutorial).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Positioned as complementary to pure text-serialization and token-alignment methods; tutorial notes it leverages Toolformer ideas to provide structured operations, but provides no numeric comparison in this overview.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Relies on external tools and their reliability; integration complexity; potential latency and reproducibility concerns; still must manage alignment between tool outputs and LLM reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Graphs: Progresses and Directions', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5238.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5238.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GNP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Neural Prompting with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Graph Neural Prompting (GNP) aligns knowledge graphs with LLMs by constructing prompts that condition LLMs on KG-derived information to perform KG reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph Neural Prompting with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph neural prompting (KG-to-text prompt alignment)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Extract KG substructures, relations, and entities and format them into prompts or prompt templates that supply structured KG information to the LLM; the alignment encourages the LLM to use KG facts for reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Designed to preserve factual KG information in prompt form and enable factual KG reasoning by LLMs; properties include faithfulness to KG facts (depending on prompt fidelity) and usability for KG reasoning pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Knowledge graph reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared conceptually to approaches that either feed embeddings or pure natural-language summaries; tutorial does not provide numerical comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Prompt design sensitivity; KG subgraph selection and prompt-length constraints; ensuring prompts preserve relation semantics without overloading context.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Graphs: Progresses and Directions', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5238.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5238.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GreaseLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Greaselm: Graph reasoning enhanced language models for question answering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that combines graph reasoning and language models, jointly encoding node representations and using language models to enhance message passing and QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Greaselm: Graph reasoning enhanced language models for question answering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Joint encoding: LLM-enhanced node textual features</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Use LLMs to generate or refine textual features/representations for nodes and then combine those textual embeddings with graph message-passing (or feed the generated text into LLMs) so language-derived semantics inform graph reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-attributed graphs / QA graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Aims to fuse rich language semantics with structural aggregation; properties include improved semantic richness of node representations and potential for better QA through combined reasoning, but increased model complexity and dependency on LLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Question answering and other graph reasoning tasks where language semantics help (as cited in the tutorial).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Presented as an approach that outperforms purely graph-only or purely LLM-only baselines in QA contexts in cited work, but the tutorial overview does not list specific performance numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Integration complexity between LLM outputs and GNN pipelines; potential inconsistencies introduced by LLM-generated features; computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Graphs: Progresses and Directions', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5238.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5238.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Node text generation (TAPE / RLM-Rec / TAPE-like)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-generated node textual descriptions (examples: RLM-Rec, TAPE, TAPE-like methods)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods that use LLMs to generate textual descriptions of nodes (e.g., items/users in recommender systems) which are then encoded as features for GNNs or fed directly to LLMs for downstream prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Node description textualization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Generate natural-language descriptions for nodes (using LLMs) describing attributes, roles, or contextual info; these textual descriptions are encoded (e.g., via LLM embeddings) and used as enriched node features or as prompt content for downstream models.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Interaction graphs / recommender graphs / general node-attributed graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Enriches otherwise identifier-only nodes with semantic content (improves interpretability and semantic expressivity); trade-offs include verbosity and potential hallucination or noise introduced by generated descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Recommendation tasks, node classification and prediction tasks where textual features are used to augment graph-based models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared implicitly against using raw ID features or hand-crafted attributes—LLM-generated descriptions are reported to improve downstream feature quality in cited works, but no numeric metrics are provided in the tutorial overview.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Risk of LLM hallucination in generated descriptions; consistency and faithfulness of generated text to real node properties; increased preprocessing cost and potential privacy concerns when generating or storing textual node descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Graphs: Progresses and Directions', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5238.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5238.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Label-free Node Classification with LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Label-free Node Classification on Graphs with Large Language Models (LLMS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that leverages LLMs for node classification without curated labels by relying on LLM knowledge and textualized graph/node information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Label-free Node Classification on Graphs with Large Language Models (LLMS). 2023.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Label-free textual prompting/description for node classification</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Represent nodes and possibly local graph context as text (prompts or descriptions) and use LLM zero/few-shot capabilities to predict node labels without supervised label fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs (node classification tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Promotes zero/low-label learning via textualized node context; properties include label-efficiency and leveraging LLM priors, but depends on faithfulness of textual representation and inherent LLM knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Node classification (label-free / zero-shot scenarios).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Presented as an alternative to supervised GNN fine-tuning and self-supervised pretraining; tutorial does not include numerical comparisons here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>LLM knowledge limitations for domain-specific labels; reliance on quality of textualized context; scalability to large graphs and sensitivity to prompt design.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Graphs: Progresses and Directions', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>GraphGPT: Graph Instruction Tuning for Large Language Models <em>(Rating: 2)</em></li>
                <li>HiGPT: Heterogeneous Graph Language Model <em>(Rating: 2)</em></li>
                <li>GraphText: Graph Reasoning in Text Space <em>(Rating: 2)</em></li>
                <li>Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT <em>(Rating: 2)</em></li>
                <li>Graph Neural Prompting with Large Language Models <em>(Rating: 2)</em></li>
                <li>Greaselm: Graph reasoning enhanced language models for question answering <em>(Rating: 2)</em></li>
                <li>Llmrec: Large language models with graph augmentation for recommendation <em>(Rating: 2)</em></li>
                <li>Label-free Node Classification on Graphs with Large Language Models (LLMS). 2023. <em>(Rating: 2)</em></li>
                <li>Natural Language is All a Graph Needs <em>(Rating: 1)</em></li>
                <li>CodeKGC: Code Language Model for Generative Knowledge Graph Construction. Zhen Bi, Jing Chen, Yinuo Jiang, Feiyu Xiong, Wei Guo, Huajun Chen, Ningyu Zhang, CoRR abs/2304.09048 2023. <em>(Rating: 1)</em></li>
                <li>OpenGraph: Towards Open Graph Foundation Models. 2024. <em>(Rating: 1)</em></li>
                <li>Graph Agent: Explicit Reasoning Agent for Graphs <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5238",
    "paper_id": "paper-269762344",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "GraphGPT",
            "name_full": "GraphGPT: Graph Instruction Tuning for Large Language Models",
            "brief_description": "A graph-to-LLM instruction-tuning paradigm that aligns structure-rich graph tokens with natural language tokens so LLMs can comprehend and perform graph tasks in supervised and zero-shot settings.",
            "citation_title": "GraphGPT: Graph Instruction Tuning for Large Language Models",
            "mention_or_use": "mention",
            "representation_name": "Graph instruction-tuning (alignment of graph tokens and natural language tokens)",
            "representation_description": "Convert graph structure and elements into a sequence of 'graph tokens' that are aligned with natural language tokens via instruction tuning; i.e., build an instruction-tuning dataset where graph tokens (representing nodes, edges, attributes or substructures) are paired/mapped with natural language instructions/responses so an LLM learns to interpret graph information in text token space.",
            "graph_type": "General graphs / heterogeneous graphs (structure-rich graphs)",
            "representation_properties": "Designed to expose graph structural information to LLMs by aligning structural tokens with text tokens; claimed to improve LLM comprehension of graphs and enable zero-shot and supervised performance; properties emphasized are expressivity (structure encoded as tokens) and compatibility with instruction-tuning, but potential token-length / context window constraints remain.",
            "evaluation_task": "A range of downstream graph tasks in both supervised and zero-shot settings (generic 'downstream tasks' as reported by the paper without task-by-task breakdown in this tutorial).",
            "performance_metrics": null,
            "comparison_to_other_representations": "Paper claims state-of-the-art performance vs. prior approaches that either only use textual prompts or only feed embeddings, but no numeric comparisons or metric values are provided in this tutorial overview.",
            "limitations_or_challenges": "Main challenge: fitting structural graph information into LLM context and tokenization (limited input context length); ensuring faithful, compact tokenization without losing important structural details; tutorial notes general open questions about how best to align graph tokens with language tokens.",
            "uuid": "e5238.0",
            "source_info": {
                "paper_title": "Large Language Models for Graphs: Progresses and Directions",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "HiGPT",
            "name_full": "HiGPT: Heterogeneous Graph Language Model",
            "brief_description": "A heterogeneous graph language model that represents heterogeneous graphs for alignment with LLMs, enabling LLM-based graph tasks.",
            "citation_title": "HiGPT: Heterogeneous Graph Language Model",
            "mention_or_use": "mention",
            "representation_name": "Heterogeneous graph language representation (token alignment)",
            "representation_description": "Encode heterogeneous graph elements (different node/edge types, attributes) into a representation compatible with language-model tokens, likely by mapping heterogeneity-aware structural tokens or textualized descriptions of nodes/relations to the LLM input space (paper referenced as a heterogenous graph language model aligning structure and text).",
            "graph_type": "Heterogeneous graphs",
            "representation_properties": "Aims to capture heterogeneity explicitly in the conversion to language tokens; intended to improve expressivity for typed nodes/edges and to allow LLMs to reason across heterogeneous relations. The tutorial notes scalability and heterogeneity handling as focal properties.",
            "evaluation_task": "Graph-related tasks across heterogeneous graph settings (not enumerated in tutorial); implied use for supervised and zero-shot tasks.",
            "performance_metrics": null,
            "comparison_to_other_representations": "Reported as a recent work in heterogeneous-graph-to-language modeling; tutorial does not provide specific comparative numbers but lists it among works that obtain improved downstream performance when aligning structure-rich tokens to language models.",
            "limitations_or_challenges": "Same high-level challenges: encoding complex heterogeneity within LLM token limits; trade-offs between faithfulness (preserving typed structural info) and compactness of textual/token representation.",
            "uuid": "e5238.1",
            "source_info": {
                "paper_title": "Large Language Models for Graphs: Progresses and Directions",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GraphText",
            "name_full": "GraphText: Graph Reasoning in Text Space",
            "brief_description": "A pure text-based method that performs graph reasoning by converting graph structures into textual representations so LLMs can reason directly in text space.",
            "citation_title": "GraphText: Graph Reasoning in Text Space",
            "mention_or_use": "mention",
            "representation_name": "Textualization / text-space serialization of graphs",
            "representation_description": "Serialize graph structure and attributes into plain text (natural language sentences or structured text) so the LLM receives the entire graph or relevant subgraph as text and performs reasoning; the approach relies on designing text encodings that capture edges, node attributes, and relevant substructures for the LLM to process.",
            "graph_type": "General graphs (graph reasoning tasks)",
            "representation_properties": "Emphasizes interpretability (text is human-readable) and direct use by LLMs without specialized token schemas; however, textualization can be verbose (low compactness) and susceptible to input length limits, and potential information loss depends on serialization rules.",
            "evaluation_task": "Graph reasoning tasks (the tutorial cites Graph-Text for pure text-based reasoning on fundamental graph reasoning tasks).",
            "performance_metrics": null,
            "comparison_to_other_representations": "Contrasted with methods that inject graph embeddings as tokens or align structure-rich tokens — text-space methods are simpler and human-readable but face context-length and verbosity trade-offs; tutorial does not report specific numeric comparisons.",
            "limitations_or_challenges": "Verbosity and context window constraints; risk of losing fine-grained structural fidelity in textual summaries; requires careful prompt/serialization design to ensure faithfulness.",
            "uuid": "e5238.2",
            "source_info": {
                "paper_title": "Large Language Models for Graphs: Progresses and Directions",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Graph-ToolFormer",
            "name_full": "Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT",
            "brief_description": "A tool-augmented prompting approach that augments LLM prompts with graph-related tool calls (external tools) to enhance graph reasoning.",
            "citation_title": "Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT",
            "mention_or_use": "mention",
            "representation_name": "Prompt augmentation with external-tool-derived text (Toolformer-style augmentation)",
            "representation_description": "Instead of fully serializing graphs into plain text tokens, prompts are augmented with tool-invocation tokens or calls (e.g., retrieval or graph-tool outputs) produced or validated by an external tool like ChatGPT/Toolformer; these augmentations inject structured graph information into the LLM prompt in a controlled manner.",
            "graph_type": "General graphs (fundamental graph reasoning tasks)",
            "representation_properties": "Uses external tool outputs to enrich prompts, which can improve faithfulness and allow more complex graph operations without inflating the LLM input; mitigates some context-length issues by delegating computations to tools, but introduces dependence on tool correctness and extra system complexity.",
            "evaluation_task": "Fundamental graph reasoning tasks (paper uses Toolformer paradigm applied to graphs as noted in tutorial).",
            "performance_metrics": null,
            "comparison_to_other_representations": "Positioned as complementary to pure text-serialization and token-alignment methods; tutorial notes it leverages Toolformer ideas to provide structured operations, but provides no numeric comparison in this overview.",
            "limitations_or_challenges": "Relies on external tools and their reliability; integration complexity; potential latency and reproducibility concerns; still must manage alignment between tool outputs and LLM reasoning.",
            "uuid": "e5238.3",
            "source_info": {
                "paper_title": "Large Language Models for Graphs: Progresses and Directions",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GNP",
            "name_full": "Graph Neural Prompting with Large Language Models",
            "brief_description": "Graph Neural Prompting (GNP) aligns knowledge graphs with LLMs by constructing prompts that condition LLMs on KG-derived information to perform KG reasoning.",
            "citation_title": "Graph Neural Prompting with Large Language Models",
            "mention_or_use": "mention",
            "representation_name": "Graph neural prompting (KG-to-text prompt alignment)",
            "representation_description": "Extract KG substructures, relations, and entities and format them into prompts or prompt templates that supply structured KG information to the LLM; the alignment encourages the LLM to use KG facts for reasoning tasks.",
            "graph_type": "Knowledge graphs",
            "representation_properties": "Designed to preserve factual KG information in prompt form and enable factual KG reasoning by LLMs; properties include faithfulness to KG facts (depending on prompt fidelity) and usability for KG reasoning pipelines.",
            "evaluation_task": "Knowledge graph reasoning tasks.",
            "performance_metrics": null,
            "comparison_to_other_representations": "Compared conceptually to approaches that either feed embeddings or pure natural-language summaries; tutorial does not provide numerical comparisons.",
            "limitations_or_challenges": "Prompt design sensitivity; KG subgraph selection and prompt-length constraints; ensuring prompts preserve relation semantics without overloading context.",
            "uuid": "e5238.4",
            "source_info": {
                "paper_title": "Large Language Models for Graphs: Progresses and Directions",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GreaseLM",
            "name_full": "Greaselm: Graph reasoning enhanced language models for question answering",
            "brief_description": "A method that combines graph reasoning and language models, jointly encoding node representations and using language models to enhance message passing and QA.",
            "citation_title": "Greaselm: Graph reasoning enhanced language models for question answering",
            "mention_or_use": "mention",
            "representation_name": "Joint encoding: LLM-enhanced node textual features",
            "representation_description": "Use LLMs to generate or refine textual features/representations for nodes and then combine those textual embeddings with graph message-passing (or feed the generated text into LLMs) so language-derived semantics inform graph reasoning.",
            "graph_type": "Text-attributed graphs / QA graphs",
            "representation_properties": "Aims to fuse rich language semantics with structural aggregation; properties include improved semantic richness of node representations and potential for better QA through combined reasoning, but increased model complexity and dependency on LLM outputs.",
            "evaluation_task": "Question answering and other graph reasoning tasks where language semantics help (as cited in the tutorial).",
            "performance_metrics": null,
            "comparison_to_other_representations": "Presented as an approach that outperforms purely graph-only or purely LLM-only baselines in QA contexts in cited work, but the tutorial overview does not list specific performance numbers.",
            "limitations_or_challenges": "Integration complexity between LLM outputs and GNN pipelines; potential inconsistencies introduced by LLM-generated features; computational cost.",
            "uuid": "e5238.5",
            "source_info": {
                "paper_title": "Large Language Models for Graphs: Progresses and Directions",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Node text generation (TAPE / RLM-Rec / TAPE-like)",
            "name_full": "LLM-generated node textual descriptions (examples: RLM-Rec, TAPE, TAPE-like methods)",
            "brief_description": "Methods that use LLMs to generate textual descriptions of nodes (e.g., items/users in recommender systems) which are then encoded as features for GNNs or fed directly to LLMs for downstream prediction.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Node description textualization",
            "representation_description": "Generate natural-language descriptions for nodes (using LLMs) describing attributes, roles, or contextual info; these textual descriptions are encoded (e.g., via LLM embeddings) and used as enriched node features or as prompt content for downstream models.",
            "graph_type": "Interaction graphs / recommender graphs / general node-attributed graphs",
            "representation_properties": "Enriches otherwise identifier-only nodes with semantic content (improves interpretability and semantic expressivity); trade-offs include verbosity and potential hallucination or noise introduced by generated descriptions.",
            "evaluation_task": "Recommendation tasks, node classification and prediction tasks where textual features are used to augment graph-based models.",
            "performance_metrics": null,
            "comparison_to_other_representations": "Compared implicitly against using raw ID features or hand-crafted attributes—LLM-generated descriptions are reported to improve downstream feature quality in cited works, but no numeric metrics are provided in the tutorial overview.",
            "limitations_or_challenges": "Risk of LLM hallucination in generated descriptions; consistency and faithfulness of generated text to real node properties; increased preprocessing cost and potential privacy concerns when generating or storing textual node descriptions.",
            "uuid": "e5238.6",
            "source_info": {
                "paper_title": "Large Language Models for Graphs: Progresses and Directions",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Label-free Node Classification with LLMs",
            "name_full": "Label-free Node Classification on Graphs with Large Language Models (LLMS)",
            "brief_description": "An approach that leverages LLMs for node classification without curated labels by relying on LLM knowledge and textualized graph/node information.",
            "citation_title": "Label-free Node Classification on Graphs with Large Language Models (LLMS). 2023.",
            "mention_or_use": "mention",
            "representation_name": "Label-free textual prompting/description for node classification",
            "representation_description": "Represent nodes and possibly local graph context as text (prompts or descriptions) and use LLM zero/few-shot capabilities to predict node labels without supervised label fine-tuning.",
            "graph_type": "General graphs (node classification tasks)",
            "representation_properties": "Promotes zero/low-label learning via textualized node context; properties include label-efficiency and leveraging LLM priors, but depends on faithfulness of textual representation and inherent LLM knowledge.",
            "evaluation_task": "Node classification (label-free / zero-shot scenarios).",
            "performance_metrics": null,
            "comparison_to_other_representations": "Presented as an alternative to supervised GNN fine-tuning and self-supervised pretraining; tutorial does not include numerical comparisons here.",
            "limitations_or_challenges": "LLM knowledge limitations for domain-specific labels; reliance on quality of textualized context; scalability to large graphs and sensitivity to prompt design.",
            "uuid": "e5238.7",
            "source_info": {
                "paper_title": "Large Language Models for Graphs: Progresses and Directions",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "GraphGPT: Graph Instruction Tuning for Large Language Models",
            "rating": 2,
            "sanitized_title": "graphgpt_graph_instruction_tuning_for_large_language_models"
        },
        {
            "paper_title": "HiGPT: Heterogeneous Graph Language Model",
            "rating": 2,
            "sanitized_title": "higpt_heterogeneous_graph_language_model"
        },
        {
            "paper_title": "GraphText: Graph Reasoning in Text Space",
            "rating": 2,
            "sanitized_title": "graphtext_graph_reasoning_in_text_space"
        },
        {
            "paper_title": "Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT",
            "rating": 2,
            "sanitized_title": "graphtoolformer_to_empower_llms_with_graph_reasoning_ability_via_prompt_augmented_by_chatgpt"
        },
        {
            "paper_title": "Graph Neural Prompting with Large Language Models",
            "rating": 2,
            "sanitized_title": "graph_neural_prompting_with_large_language_models"
        },
        {
            "paper_title": "Greaselm: Graph reasoning enhanced language models for question answering",
            "rating": 2,
            "sanitized_title": "greaselm_graph_reasoning_enhanced_language_models_for_question_answering"
        },
        {
            "paper_title": "Llmrec: Large language models with graph augmentation for recommendation",
            "rating": 2,
            "sanitized_title": "llmrec_large_language_models_with_graph_augmentation_for_recommendation"
        },
        {
            "paper_title": "Label-free Node Classification on Graphs with Large Language Models (LLMS). 2023.",
            "rating": 2,
            "sanitized_title": "labelfree_node_classification_on_graphs_with_large_language_models_llms_2023"
        },
        {
            "paper_title": "Natural Language is All a Graph Needs",
            "rating": 1,
            "sanitized_title": "natural_language_is_all_a_graph_needs"
        },
        {
            "paper_title": "CodeKGC: Code Language Model for Generative Knowledge Graph Construction. Zhen Bi, Jing Chen, Yinuo Jiang, Feiyu Xiong, Wei Guo, Huajun Chen, Ningyu Zhang, CoRR abs/2304.09048 2023.",
            "rating": 1,
            "sanitized_title": "codekgc_code_language_model_for_generative_knowledge_graph_construction_zhen_bi_jing_chen_yinuo_jiang_feiyu_xiong_wei_guo_huajun_chen_ningyu_zhang_corr_abs230409048_2023"
        },
        {
            "paper_title": "OpenGraph: Towards Open Graph Foundation Models. 2024.",
            "rating": 1,
            "sanitized_title": "opengraph_towards_open_graph_foundation_models_2024"
        },
        {
            "paper_title": "Graph Agent: Explicit Reasoning Agent for Graphs",
            "rating": 1,
            "sanitized_title": "graph_agent_explicit_reasoning_agent_for_graphs"
        }
    ],
    "cost": 0.01268775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large Language Models for Graphs: Progresses and Directions</p>
<p>Chao Huang chaohuang75@gmail.com 
University of Hong Kong</p>
<p>Xubin Ren xubinrencs@gmail.com 
University of Hong Kong</p>
<p>Jiabin Tang jiabintang77@gmail.com 
University of Hong Kong</p>
<p>Dawei Yin yindawei@acm.org 
Baidu Inc</p>
<p>Nitesh Chawla nchawla@nd.edu 
University of Notre Dame</p>
<p>Large Language Models for Graphs: Progresses and Directions
3A6D2D9D2487717A200A7CC1ADB31F5B10.1145/3589335.3641251CCS CONCEPTSInformation systems → Data miningLanguage models• Mathematics of computing → Graph algorithms Large Language Models, Graph Learning
Graph neural networks (GNNs) have emerged as fundamental methods for handling structured graph data in various domains, including citation networks, molecule prediction, and recommender systems.They enable the learning of informative node or graph representations, which are crucial for tasks such as link prediction and node classification in the context of graphs.To achieve high-quality graph representation learning, certain essential factors come into play: clean labels, accurate graph structures, and sufficient initial node features.However, real-world graph data often suffer from noise and sparse labels, while different datasets have unique feature constructions.These factors significantly impact the generalization capabilities of graph neural networks, particularly when faced with unseen tasks.Recently, due to the efficent text processing and task generalization capability of large language models (LLMs), there has been a promising approach to address the challenges mentioned above by combining large language models with graph data.This tutorial offers an overview of incorporating large language models into the graph domain, accompanied by practical examples.The methods are categorized into three dimensions: utilizing LLMs as augmenters, predictors, and agents for graph learning tasks.We will delve into the current progress and future directions within this field.By introducing this emerging topic, our aim is to enhance the audience's understanding of LLM-based graph learning techniques, foster idea exchange, and encourage discussions that drive continuous advancements in this domain.</p>
<p>INTRODUCTION</p>
<p>Graph neural networks (GNNs) have proven to be a potent framework for tackling graph-structured data [6], as evidenced by research in social network analysis [24,47], recommender systems [9,22], and biological network analysis [7,18].GNNs enjoy a notable advantage in capturing the inherent structural information and dependencies present in graph data [49].With the message passing and aggregation mechanisms, GNNs can effectively capture structural information in the graph, which enables them to model intricate relationships and achieve precise predictions.</p>
<p>In recent years, several Graph Neural Network (GNN) architectures have introduced innovative methods for message passing and aggregation in graphs.GCNs [12,16] extend the concept of convolution to graphs, enabling effective feature representation.GAT [30,34] utilizes attention mechanisms for more refined information aggregation.Graph transformer approaches [11,14,44] leverage self-attention and positional encoding to capture global dependencies.Nodeformer [38] and DIFFormer [37] address scalability concerns in large-scale graphs.However, these methods heavily rely on supervised learning, which limits their ability to generalize to sparse and noisy data.To enhance generalization, selfsupervised learning (SSL) techniques are employed in graph representation, utilizing contrastive and generative frameworks [3,10].Contrastive SSL methods such as DGI [31] and GCA [50] distinguish positive and negative instances.Generative approaches like GraphMAE [10], S2GAE [25] and AutoCF [40] generate synthetic instances that mimic the characteristics of original graphs.</p>
<p>Despite the remarkable performance achieved by the aforementioned self-supervised learning methods in enhancing generalization, they still necessitate a fine-tuning stage that relies on labels specifically tailored to downstream graph learning contexts.However, this dependence on labeled data from downstream applications can limit their versatility in real-world scenarios, where obtaining high-quality labels may be impractical.This limitation is especially pertinent in settings such as cold-start recommendation systems or forecasting node classes in unseen graph datasets, where precise labels may be scarce or unavailable.Moreover, in numerous scenarios, the semantic information of graph nodes is either missing or contaminated with noise, highlighting the urgent need to address the integration of enriched semantic details to bolster the performance of downstream tasks.For example, in recommendation systems, users and items are typically represented solely by identifiers, lacking specific semantic information, which hampers the comprehensive modeling of user preference.</p>
<p>In recent times, we have witnessed remarkable advancements in large language models (LLMs) such as  and LLaMA [29], which have showcased exceptional generalization [2,13] and emergent [35] properties.These models have demonstrated outstanding performance on unseen tasks, especially in few-and zero-shot settings [2,13].Inspired by the accomplishments of LLMs, we propose integrating them into graph learning as a solution to tackle challenges related to generalization and semantic learning.The transfer capabilities of LLMs can enhance the generalization of graph learning, while their robust semantic modeling can effectively overcome the limitations associated with graph semantics.In this tutorial, our objective is to introduce current graph learning techniques based on LLMs, providing valuable insights into how they can effectively address various non-trivial challenges in existing graph models.</p>
<p>TUTORIAL CONTENT</p>
<p>This tutorial examines advanced techniques that integrate large language models (LLMs) into graph learning.We will explore how recent research enhances graph learning using LLMs and discuss the challenges that accompany this integration.Multiple approaches for effectively integrating LLMs into graph learning will be presented:</p>
<p>• Introduce the fundamental concepts and principles of GNNs and providing an overview of different types of graph tasks.</p>
<p>• Discuss the field of graph learning, highlighting its key challenges, and provide an overview of the state-of-the-art related work.Emphasize the significance of incorporating LLMs into graph tasks, despite the existing limitations, and explain why it is worthwhile to invest efforts in this direction.</p>
<p>• Present recent developments in LLM-based graph learning methods, showcasing their ability to enhance performance in graphrelated tasks by leveraging the advantages of language models.</p>
<p>• Examine the experimental settings and results of LLM-based graph neural networks across various graph-related tasks, including traditional settings as well as few/zero-shot scenarios.</p>
<p>• Discuss future directions and pose open questions regarding the advancement of graph learning with large language models.</p>
<p>• Provide practical examples to illustrate the application of recent innovative approaches that utilize LLMs to address graph tasks.</p>
<p>LLMs as Augmentor in Graph Learning.Graph data is composed of nodes, edges, and features.GNNs are employed to perform message passing and feature aggregation on the graph.The representations of nodes or graphs learned from GNNs are subsequently utilized for downstream prediction tasks.In recent studies, LLMs have been utilized to enhance the original graph data by incorporating the global knowledge within language models.This augmentation is intended to enhance GNN representations for downstream tasks and empower models to showcase generalization capabilities, including the ability to reason in few-shot or zero-shot scenarios.</p>
<p>Graph data can be augmented from two perspectives, offering diverse benefits.Firstly, augmentation can be applied to the graph structure, particularly when the graph contains numerous noisy edges.For instance, in the realm of recommendation systems, LLM-Rec [36] employs LLMs to filter out noisy edges within the interaction graph.The second approach involves utilizing LLMs to augment the features or labels.To illustrate, in the case of OFA [17], LLMs are utilized to encode instructions in the form of text descriptions, which serve as input embeddings for the GNNs.Similarly, LLM-GNN [5] and OpenGraph [41] employs LLMs to annotate node labels for GNNs' predictions.Other approaches, such as RLM-Rec [21] and TAPE [8], utilize LLMs to generate text descriptions of nodes within the graph.These descriptions are subsequently encoded as features for prediction tasks.Additionally, methods like GreaseLM [46] and DRAGON [42] combine LLMs and GNNs to jointly encode node representations, effectively enhancing the message passing of node features using language models.LLMs as Predictor in Graph Learning.Harnessing the vast global knowledge of LLMs, certain methods propose direct utilization of LLMs for graph learning tasks.However, a key challenge lies in enabling LLMs to effectively comprehend and incorporate graph structures within limited input contextual length, while leveraging their own knowledge to successfully fulfill downstream tasks.</p>
<p>Currently, two primary approaches exist for integrating graph structural information into large language models (LLMs): constructing textual prompts and utilizing graph embeddings as input tokens.Chen et al. [4] explore the design of various textual prompts for graph learning tasks.InstructGLM [43], on the other hand, combines prompt instructions with node embeddings to endow LLMs with graph structural information, thereby enhancing downstream task predictions.Recent works, such as GraphGPT [26] and HiGPT [27], introduce a graph instruction-tuning paradigm that aligns structure-rich graph tokens with natural language tokens.This paradigm enables LLMs to effectively comprehend graphs, leading to state-of-the-art performance in supervised and zeroshot settings across a range of downstream tasks.For fundamental graph reasoning tasks, Graph-ToolFormer [45] leverages external tools based on the Toolformer tuning paradigm [23], while Graph-Text [48] utilizes a pure text-based reasoning method.In the domain of knowledge graphs (KGs), GNP [28] employs the Graph Neural Prompting approach to align KGs with LLMs for KG reasoning, while CodeKGC [1] constructs KGs using code language models.To address diverse urban computing tasks, UrbanGPT [15] proposes aligning spatio-temporal context with a large language model.LLMs as Agent in Graph Learning.LLMs' exceptional text comprehension and reasoning abilities have garnered significant attention.This has led to the emergence of LLM-based agents as a prominent research focus [32,39].These agents incorporate perception, memorization, and action modules, enabling them to autonomously plan, gather information, and make decisions in response to user queries, achieving desired outcomes.In the domain of graph learning, these agents retrieve information and devise strategies to automatically solve graph-related tasks.The Graph Agent framework [33] stores node and edge embeddings as memory.When confronted with graph tasks, the agent retrieves similar nodes or edges, and LLMs engage in inductive-deductive reasoning based on the retrieved results.Another approach, RoG [19], employs a planning-retrieval-reasoning methodology.The agent generates relation paths grounded by knowledge graphs as plans, retrieves valid reasoning paths, and leverages LLMs to accurately reason using extracted information to solve the problem.</p>
<p>SCHEDULE AND MATERIAL</p>
<p>This tutorial will be a practical, hands-on session that is divided into four parts, focusing on the latest advancements in graph learning techniques enhanced by large language models.The outline of this tutorial is summarized as follows:
•</p>
<p>TARGET AUDIENCE</p>
<p>This tutorial is designed for researchers, practitioners, and students working in the field of graph learning and large language models.</p>
<p>• This tutorial is intended for researchers and practitioners exploring the application of large language models in solving graphrelated problems.Attendees can expect valuable insights and motivations to inspire their exploration of this topic.</p>
<p>• Researchers and practitioners who are focused on addressing the challenges associated with developing foundational graph models that possess robust generalization capabilities.</p>
<p>• Researchers and practitioners dedicated to proposing and developing effective graph language models for a wide range of real-life applications will also find this tutorial relevant.</p>
<p>ORGANIZERS</p>
<p>Figure 1 :
1
Figure 1: Illustration of three different paradigms to integrating large language models into graphs.</p>
<p>Part I: Introdution (20 minutes) • Part II: Preliminary of LLMs (20 minutes) • Part III: LLMs for Graph Learning -3.1 LLMs as Augmentor (20 minutes) -3.2 LLMs as Predictor (20 minutes) -3.3 LLMs as Agent (20 minutes) -3.4 Open problems and Future Directions (20 minutes) • Break.(10 minutes) • Part IV: Hands-on Practical Examples (40 minutes) • Part V: Conclusions and Discussions (10 minutes) We will provide participants with the following materials: • Tutorial Slides.Our slides will cover the background, techniques, and future directions discussed in the tutorial.• Source Code.Annotated implementation code pointers and datasets for hands-on examples, ensuring reproducibility.• Annotated Reference.A compilation of annotated references to all the discussed research work, enabling further study for researchers and practitioners.</p>
<p>Xubin Ren is currently pursuing a Ph.D. at the Institute of Data Science at the University of Hong Kong.His research focuses primarily on graph learning, large language models, and recommender systems.His impactful research contributions have gained recognition and have been featured in prestigious international conferences, including WWW, SIGIR, WSDM, ICLR, and CIKM.Jiabin Tang is currently pursuing a Ph.D. at the Institute of Data Science at the University of Hong Kong.His research interests encompass web mining, graph neural networks, large language models, and spatio-temporal data mining.His research contributions have been prominently featured in esteemed international conferences, including WWW, WSDM, CIKM.Dawei Yin is a Senior Director of Engineering at Baidu Inc., leading the search science team.With a distinguished career as a senior director at JD.com and a senior research manager at Yahoo Labs, he possesses extensive expertise in information retrieval, data mining, and machine learning.His research interests encompass a wide range of areas, including recommender systems, web search, question answering, pre-trained language models, and video and image analysis.Dawei Yin's outstanding contributions to the research community have garnered prestigious recognition, including the best paper awards at WSDM 2016 and KDD 2016, as well as the best student paper award at WSDM 2018.Nitesh Chawla is the Frank M. Freimann Professor at University of Notre Dame, renowned for his expertise in artificial intelligence, data science, and network science.As the Founding Director of the Lucy Family Institute for Data and Society, he spearheads interdisciplinary research aimed at harnessing technology for the betterment of society.With an extensive publication record of over 300 papers, Nitesh Chawla has amassed an impressive citation count exceeding 58,000, establishing an h-index of 78.He is honored as a Fellow of ACM, IEEE, and AAAI, and has been the recipient of prestigious accolades such as the National Academy of Engineers New Faculty Fellowship and the IBM Watson Faculty Award.Additionally, he serves as a co-founder of Aunalytics, a prominent company specializing in data science and cloud computing.
Chao Huang is an Assistant Professor at the University of HongKong (HKU), where he focuses his research on Graph Learning,Recommender Systems, Spatio-Temporal Data Mining, and LargeLanguage Models (LLMs). Chao Huang's contributions to thesefields are reflected in his publications in esteemed conferencesand venues including KDD, SIGIR, WWW, WSDM, ICLR, NeurIPS,ICML, and CIKM. His contributions have been acknowledged, withseveral of his papers being recognized as highly influential in thedomains of data mining and information retrieval, particularlywithin conferences such as WWW, SIGIR, and KDD.</p>
<p>CodeKGC: Code Language Model for Generative Knowledge Graph Construction. Zhen Bi, Jing Chen, Yinuo Jiang, Feiyu Xiong, Wei Guo, Huajun Chen, Ningyu Zhang, CoRR abs/2304.090482023. 2023</p>
<p>Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford2020</p>
<p>Heterogeneous graph contrastive learning for recommendation. Mengru Chen, Chao Huang, Lianghao Xia, Wei Wei, Yong Xu, Ronghua Luo, WSDM. 2023</p>
<p>Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs. Zhikai Chen, Haitao Mao, Hang Li, abs/2307.033932023. 2023</p>
<p>Zhikai Chen, Haitao Mao, Hongzhi Wen, Haoyu Han, Wei Jin, Haiyang Zhang, Hui Liu, Jiliang Tang, arXiv:2310.04668Label-free Node Classification on Graphs with Large Language Models (LLMS). 2023. 2023arXiv preprint</p>
<p>EDITS: Modeling and Mitigating Data Bias for Graph Neural Networks. Yushun Dong, Ninghao Liu, Brian Jalaian, WWW. ACM. 2022</p>
<p>Graph-based Molecular Representation Learning. Zhichun Guo, Kehan Guo, Bozhao Nan, Yijun Tian, G Roshni, Iyer, IJCAI. 2023</p>
<p>Xiaoxin He, Xavier Bresson, Thomas Laurent, Bryan Hooi, arXiv:2305.19523Explanations as Features: LLM-Based Features for Text-Attributed Graphs. 2023. 2023arXiv preprint</p>
<p>LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation. Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yong-Dong Zhang, Meng Wang, SIGIR. ACM2020</p>
<p>Graphmae: Self-supervised masked graph autoencoders. Zhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Jie Tang, KDD. 2022</p>
<p>Heterogeneous Graph Transformer. Ziniu Hu, Yuxiao Dong, Kuansan Wang, Yizhou Sun, WWW. ACM / IW3C22020</p>
<p>Semi-Supervised Classification with Graph Convolutional Networks. Thomas N Kipf, Max Welling, ICLR (Poster). 2017OpenReview.net</p>
<p>Large Language Models are Zero-Shot Reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, NeurIPS. 2022</p>
<p>Graph transformer for recommendation. Chaoliu Li, Lianghao Xia, Xubin Ren, Yaowen Ye, Yong Xu, Chao Huang, SIGIR. 2023</p>
<p>Zhonghang Li, Lianghao Xia, Jiabin Tang, Yong Xu, Lei Shi, Long Xia, Dawei Yin, Chao Huang, arXiv:2403.00813UrbanGPT: Spatio-Temporal Large Language Models. 2024. 2024arXiv preprint</p>
<p>Resource-Efficient Training for Large Graph Convolutional Networks with Label-Centric Cumulative Sampling. Mingkai Lin, Wenzhong Li, Ding Li, Yizhou Chen, Sanglu Lu, WWW. ACM2022</p>
<p>Hao Liu, Jiarui Feng, Lecheng Kong, Ningyue Liang, Dacheng Tao, Yixin Chen, Muhan Zhang, arXiv:2310.00149One for All: Towards Training One Graph Model for All Classification Tasks. 2023. 2023arXiv preprint</p>
<p>Interpretable Chirality-Aware Graph Neural Network for Quantitative Structure Activity Relationship Modeling in Drug Discovery. Yunchao Liu, Yu Wang, Oanh Vu, Rocco Moretti, AAAI. 2023</p>
<p>Linhao Luo, Yuan-Fang Li, arXiv:2310.01061Gholamreza Haffari, and Shirui Pan. 2023. Reasoning on graphs: Faithful and interpretable large language model reasoning. 2023arXiv preprint</p>
<p>Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, Chao Huang, arXiv:2310.15950Representation Learning with Large Language Models for Recommendation. 2023. 2023arXiv preprint</p>
<p>Disentangled contrastive collaborative filtering. Xubin Ren, Lianghao Xia, Jiashu Zhao, Dawei Yin, Chao Huang, SIGIR. 2023</p>
<p>Toolformer: Language Models Can Teach Themselves to Use Tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, CoRR abs/2302.047612023. 2023</p>
<p>Pre-training Enhanced Spatial-temporal Graph Neural Network for Multivariate Time Series Forecasting. Zezhi Shao, Zhao Zhang, Fei Wang, Yongjun Xu, KDD. ACM2022</p>
<p>S2GAE: Self-Supervised Graph Autoencoders are Generalizable Learners with Graph Masking. Qiaoyu Tan, Ninghao Liu, Xiao Huang, Soo-Hyun Choi, Li Li, Rui Chen, Xia Hu, WSDM. 2023</p>
<p>GraphGPT: Graph Instruction Tuning for Large Language Models. Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, Chao Huang, CoRR abs/2310.130232023. 2023</p>
<p>Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Long Xia, Dawei Yin, Chao Huang, arXiv:2402.16024HiGPT: Heterogeneous Graph Language Model. 2024. 2024arXiv preprint</p>
<p>Graph Neural Prompting with Large Language Models. Yijun Tian, Huan Song, Zichen Wang, Haozhu Wang, Ziqing Hu, Fang Wang, Nitesh V Chawla, Panpan Xu, CoRR abs/2309.154272023. 2023</p>
<p>LLaMA: Open and Efficient Foundation Language Models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Lacroix, Baptiste Rozière, CoRR abs/2302.139712023. 2023</p>
<p>Graph Attention Networks. Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, ICLR (Poster). 2018OpenReview.net</p>
<p>Deep Graph Infomax. Petar Velickovic, William Fedus, William L Hamilton, Pietro Liò, ICLR (Poster). 2019OpenReview.net</p>
<p>Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, arXiv:2308.11432A survey on large language model based autonomous agents. 2023. 2023arXiv preprint</p>
<p>Graph Agent: Explicit Reasoning Agent for Graphs. Qinyong Wang, Zhenxiang Gao, Rong Xu, arXiv:2310.164212023. 2023arXiv preprint</p>
<p>Heterogeneous Graph Attention Network. Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang Ye, WWW. ACM. 2019</p>
<p>Emergent Abilities of Large Language Models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Jeff Dean, William Fedus, Trans. Mach. Learn. Res. 20222022. 2022</p>
<p>Llmrec: Large language models with graph augmentation for recommendation. Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, Chao Huang, WSDM. 2024</p>
<p>DIFFormer: Scalable (Graph) Transformers Induced by Energy Constrained Diffusion. Qitian Wu, Chenxiao Yang, 2023In ICLR</p>
<p>NodeFormer: A Scalable Graph Structure Learning Transformer for Node Classification. Qitian Wu, Wentao Zhao, CoRR abs/2306.083852023. 2023</p>
<p>Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, arXiv:2309.07864The rise and potential of large language model based agents: A survey. 2023. 2023arXiv preprint</p>
<p>Automated Self-Supervised Learning for Recommendation. Lianghao Xia, Chao Huang, Tao Yu, Ben Kao, WWW. 2023</p>
<p>Lianghao Xia, Ben Kao, Chao Huang, arXiv:2403.01121OpenGraph: Towards Open Graph Foundation Models. 2024. 2024arXiv preprint</p>
<p>Deep bidirectional languageknowledge graph pretraining. Michihiro Yasunaga, Antoine Bosselut, Hongyu Ren, Xikun Zhang, Christopher D Manning, Percy S Liang, Jure Leskovec, Advances in Neural Information Processing Systems. 352022. 2022</p>
<p>Natural Language is All a Graph Needs. Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, Yongfeng Zhang, CoRR abs/2308.071342023. 2023</p>
<p>Graph Transformer Networks. Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, Hyunwoo J Kim, NeurIPS. 11960-119702019</p>
<p>Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT. Jiawei Zhang, CoRR abs/2304.111162023. 2023</p>
<p>Greaselm: Graph reasoning enhanced language models for question answering. Xikun Zhang, Antoine Bosselut, Michihiro Yasunaga, Hongyu Ren, Percy Liang, Christopher D Manning, Jure Leskovec, arXiv:2201.088602022. 2022arXiv preprint</p>
<p>Robust Self-Supervised Structural Graph Neural Network for Social Network Prediction. Yanfu Zhang, WWW. ACM. 2022</p>
<p>GraphText: Graph Reasoning in Text Space. Jianan Zhao, Le Zhuo, Yikang Shen, Meng Qu, Kai Liu, Michael M Bronstein, Zhaocheng Zhu, Jian Tang, CoRR abs/2310.010892023. 2023</p>
<p>Graphglow: Universal and generalizable structure learning for graph neural networks. Wentao Zhao, Qitian Wu, Chenxiao Yang, Junchi Yan, KDD. 2023</p>
<p>Graph contrastive learning with adaptive augmentation. Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, Liang Wang, WWW. 2069-20802021</p>            </div>
        </div>

    </div>
</body>
</html>
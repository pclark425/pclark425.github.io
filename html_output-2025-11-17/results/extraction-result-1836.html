<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1836 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1836</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1836</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-cc4a5b748de92ff88d9c404fcf07555d2d31f698</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/cc4a5b748de92ff88d9c404fcf07555d2d31f698" target="_blank">Taxim: An Example-based Simulation Model for GelSight Tactile Sensors</a></p>
                <p><strong>Paper Venue:</strong> IEEE Robotics and Automation Letters</p>
                <p><strong>Paper TL;DR:</strong> Taxim, a realistic and high-speed simulation model for a vision-based tactile sensor, GelSight, is proposed, which is the first to incorporate marker motion field simulation that derives from elastomer deformation together with the optical simulation, creating a comprehensive and computationally efficient tactile simulation framework.</p>
                <p><strong>Paper Abstract:</strong> Simulation is widely used in robotics for system verification and large-scale data collection. However, simulating sensors, including tactile sensors, has been a long-standing challenge. In this paper, we propose Taxim, a realistic and high-speed simulation model for a vision-based tactile sensor, GelSight. A GelSight sensor uses a piece of soft elastomer as the medium of contact and embeds optical structures to capture the deformation of the elastomer, which infers the geometry and forces applied at the contact surface. We propose an example-based method for simulating GelSight: we simulate the optical response to the deformation with a polynomial look-up table. This table maps the deformed geometries to pixel intensity sampled by the embedded camera. In order to simulate the surface markers' motion that is caused by the surface stretch of the elastomer, we apply the linear displacement relationship and the superposition principle. The simulation model is calibrated with less than 100 data points from a real sensor. The example-based approach enables the model to easily migrate to other GelSight sensors or its variations. To the best of our knowledge, our simulation framework is the first to incorporate marker motion field simulation that derives from elastomer deformation together with the optical simulation, creating a comprehensive and computationally efficient tactile simulation framework. Experiments reveal that our optical simulation has the lowest pixel-wise intensity errors compared to prior work and can run online with CPU computing. Our code and supplementary materials are open-sourced at https://github.com/CMURoboTouch/Taxim.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1836.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1836.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Taxim</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Taxim: An example-based simulation model for GelSight tactile sensors</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A computationally lightweight, example-calibrated simulator that models both the optical response and marker motion field of GelSight-style vision-based tactile sensors using a polynomial lookup table for photometric response and a linear-displacement + superposition approach for elastomer marker motion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>GelSight vision-based tactile sensor (mounted on robotic manipulators)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A vision-based tactile sensor using a soft elastomer (gelpad) with printed surface markers, embedded LEDs and an internal camera to measure surface geometry and infer contact forces/torques during robot manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation (robotic tactile sensing)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Taxim (custom example-based tactile simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>A simulator that (1) maps contact geometry/normal vectors and image plane location to pixel intensities via a calibrated polynomial lookup table (example-based photometric stereo), (2) synthesizes shadows from collected unit-shadow masks, (3) approximates gelpad height/deformation with pyramid Gaussian kernels for quasi-static contact, and (4) computes dense marker motion fields using FEM-calibrated 3×3 mutual-influence tensors with a linear displacement relationship and superposition.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Sensor-level high-fidelity appearance and marker-motion approximation for quasi-static contacts (example-calibrated, computationally lightweight); not a full dynamic FEM at runtime.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Optical/lighting response (per-pixel intensity via polynomial table), shadowing (unit-shadow masks accumulated), quasi-static elastomer deformation (height map approximated with pyramid Gaussian kernels), dense marker motion field (via calibrated mutual-influence tensors), and sensor intrinsic noise characteristics (captured via calibration examples).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Runtime soft-body deformation approximated (pyramid Gaussian kernels) rather than full FEM; dynamic contact phenomena (time-dependent dynamics, slip) are not modeled; shadow inter-reflections are not modeled (shadows linearly accumulated per light); partial slip under shear not modeled; simulation is quasi-static only.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Real GelSight sensors mounted on an optical XYR stage and indenter on a vertical stage (0.01 mm precision) with dome-shaped gelpads; data collected with multiple real GelSight/DIGIT sensors and 3D-printed test objects.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Not demonstrated in this paper; authors state intent to apply Taxim to sim-to-real robot perception and manipulation tasks using tactile data (planned future work).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Observed sources of sim-to-real gap: mismatch between hand-manufactured gelpad and FEM model geometry/material, noise in marker extraction/tracking from real sensor images, and absence of partial-slip modeling under shear in the simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Calibration with examples from a real sensor (optical polynomial table with <100 contact examples and a small set of shadow masks), explicit modeling of both optical response and marker motion, FEM-based calibration of mutual-influence tensors, and a computationally lightweight pipeline enabling large-scale data generation for downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>The paper emphasizes that accurate modeling of the optical response (lighting, shadows) and marker motion field is important for realism; it also notes that to support realistic shear/slip behaviors dynamic and partial-slip modeling are required (i.e., current quasi-static fidelity is insufficient for some shear cases). No quantitative numerical fidelity thresholds are given.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>For optical-image fidelity the paper reports Taxim outperforms three other simulators (Tacto, Phong, physics-based rendering) on L1, MSE, SSIM and PSNR (Ours: L1=5.565, MSE=58.358, SSIM=0.882, PSNR=30.974 vs. best alternative MSE=90.623 / PSNR=28.687). Marker-motion accuracy is close to FEM: dense-axis L1 errors on dataset ~3–5e-3 mm and sparse marker magnitude L1 errors ~1.02e-2 mm compared to real data; runtime is orders of magnitude faster than FEM (Taxim marker sim ~9.22 s on CPU vs. 2–4 hrs for ANSYS FEM).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Taxim provides a practical, example-calibrated simulator that jointly models GelSight optical images and marker motion with high image similarity to real sensors and close correspondence to FEM marker displacements while being computationally lightweight; the simulator is not yet used for demonstrated sim-to-real skill transfer in this paper, and remaining sim-to-real gaps include manufactured gelpad variability, marker-tracking noise, and lack of partial-slip / dynamic contact modeling which should be addressed to enable robust transfer for tasks involving shear/slip.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Taxim: An Example-based Simulation Model for GelSight Tactile Sensors', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1836.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1836.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ding et al. (TacTip)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sim-to-real transfer for optical tactile sensing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior work that created a physics-based soft-body simulation of the TacTip optical tactile sensor (pin-based membrane) to simulate pin motions, and evaluated the simulator on sim-to-real robot tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sim-to-real transfer for optical tactile sensing</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>TacTip optical tactile sensor</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A soft optical tactile sensor consisting of a membrane with internal pins whose motions are optically tracked to infer contact events and forces.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation (robotic tactile sensing)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Physics soft-body simulation (Unity-based simulation of TacTip membrane)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>A physics-based soft-body simulator modeling the TacTip membrane dynamics and resulting pin motions (used to synthesize tactile observations for downstream tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Physics-based soft-body simulation intended to capture membrane dynamics (higher-fidelity dynamics for pin motion).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Membrane dynamics and pin displacement/motion.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>TacTip sensor mounted on a robot performing manipulation tasks (used to evaluate sim-to-real transfer).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Robotic tasks driven by TacTip tactile feedback (paper states evaluation on sim-to-real robot tasks; exact tasks not detailed in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Use of a physics-based soft-body simulation of the TacTip membrane to generate realistic pin motion signals for training/testing controllers/policies.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>This referenced work built a physics soft-body model for TacTip and evaluated sim-to-real transfer for robot tasks; details and quantitative transfer outcomes are not provided in the current paper's text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Taxim: An Example-based Simulation Model for GelSight Tactile Sensors', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1836.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1836.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sferrazza et al.</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning the sense of touch in simulation: a sim-to-real strategy for vision-based tactile sensing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Used a finite-element model to create a synthetic dataset for vision-based tactile sensors, trained networks to predict 3D contact force distributions in simulation, and demonstrated sim-to-real transfer of the learned models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning the sense of touch in simulation: a sim-to-real strategy for vision-based tactile sensing</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Vision-based tactile sensors (GelSight-like)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>High-resolution vision-based tactile sensors that produce images encoding contact geometry and can be used to infer force distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation (robotic tactile sensing)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Finite Element Method (FEM)-based tactile simulation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>An FEM simulation that models gelpad/elastomer deformation to synthesize tactile imagery and ground-truth contact force distributions for training data.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>High-fidelity physics (FEM) for soft-body deformation and force-distribution generation.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Soft-body elastomer deformation, ground-truth 3D contact force distribution, and synthetic tactile image generation.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Real vision-based tactile sensors used to validate transfer of networks trained in simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Prediction of 3D contact force distribution from tactile images (models trained in simulation transferred to real sensors).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised learning (trained a network on FEM-generated synthetic data).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>High-fidelity FEM-generated synthetic dataset providing ground-truth force distributions enabled training of models that transfer to real sensors.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Implicit requirement: sufficiently accurate FEM simulation of elastomer deformation and force distribution is necessary to train transfer-capable models (no quantitative thresholds reported here).</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>FEM-based synthetic data can be used to train networks that generalize from simulation to real vision-based tactile sensors for force-distribution estimation; this prior work is cited as having realized sim-to-real transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Taxim: An Example-based Simulation Model for GelSight Tactile Sensors', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1836.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1836.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Narang et al.</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sim-to-real for robotic tactile sensing via physics-based simulation and learned latent projections</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Extended finite-element and physics-based simulation of the BioTac sensor into Isaac Gym and used a learned generative mapping to project simulated deformations into electronic sensor-like signals for tactile interpretation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sim-to-real for robotic tactile sensing via physics-based simulation and learned latent projections</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>BioTac multimodal tactile sensor</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A biomimetic multimodal tactile sensor providing electronic signals correlating with contact forces, used in robotic tactile perception.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation (robotic tactile sensing)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>ANSYS FEM and Isaac Gym physics-based simulation with learned projection (generative mapping)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>FEM/physics simulation of BioTac deformation (ANSYS/Isaac Gym) with an additional learned generative projection to convert simulated deformation to sensor electronic signals.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>High-fidelity physics (FEM/physics engine) combined with a learned mapping to match real sensor signals.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Soft-body deformation and its mapping to sensor electronic outputs (via generative/learned latent projection).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Real BioTac sensor data used to learn/validate mappings and downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Tactile-signal interpretation tasks (training/transfer of models from simulated deformations to real sensor signal space), specifics not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Generative learning / learned latent projections to map simulated states to realistic sensor signals.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Combining physics-based deformation simulation with learned generative projection to produce realistic sensor-like signals.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining FEM/physics simulation of sensor deformation with a learned mapping to sensor output space is a strategy used to bridge simulation and real sensor signals for tactile perception tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Taxim: An Example-based Simulation Model for GelSight Tactile Sensors', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1836.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1836.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Church et al.</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Optical tactile sim-to-real policy transfer via real-to-sim tactile image translation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Applied generative adversarial networks for real-to-sim tactile image translation (TacTip), enabling policy transfer between simulated depth maps and real tactile images.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Optical tactile sim-to-real policy transfer via real-to-sim tactile image translation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>TacTip optical tactile sensor</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>An optical tactile sensor where a membrane and internal markers/pins produce images that encode contact geometry; used for tactile-based policies.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation (robotic tactile sensing)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Simulated depth maps with GAN-based real-to-sim image translation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Simulated contact depth representations (depth maps) used as the simulated modality; a GAN performs real-to-sim translation so policies trained in sim see observations similar to simulated inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Approximate geometry-depth simulation combined with learned domain adaptation (real-to-sim translation) to close visual gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Contact geometry / depth maps; reliance on learned translation to produce realistic tactile-image appearance.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Optical tactile appearance not directly simulated; requires GAN-based translation to match real sensor images.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Real TacTip tactile images collected from sensor for training translation networks and validating policy transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Policy transfer for tactile-based control from simulation to real (specific tasks not detailed in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>GAN-based real-to-sim image translation plus policy learning in simulation (exact policy learning method not specified here).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Appearance mismatch between simulated depth maps and real tactile images addressed via learned translation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Use of GAN-based real-to-sim tactile image translation to map real observations into the simulated observation distribution enabling sim-trained policies to operate on real data.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Real-to-sim tactile image translation (GAN-based) is a viable approach to reduce visual domain gap and support sim-to-real policy transfer for optical tactile sensors like TacTip.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Taxim: An Example-based Simulation Model for GelSight Tactile Sensors', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sim-to-real transfer for optical tactile sensing <em>(Rating: 2)</em></li>
                <li>Learning the sense of touch in simulation: a sim-to-real strategy for vision-based tactile sensing <em>(Rating: 2)</em></li>
                <li>Sim-to-real for robotic tactile sensing via physics-based simulation and learned latent projections <em>(Rating: 2)</em></li>
                <li>Optical tactile sim-to-real policy transfer via real-to-sim tactile image translation <em>(Rating: 2)</em></li>
                <li>Ground truth force distribution for learning-based tactile sensing: A finite element approach <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1836",
    "paper_id": "paper-cc4a5b748de92ff88d9c404fcf07555d2d31f698",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "Taxim",
            "name_full": "Taxim: An example-based simulation model for GelSight tactile sensors",
            "brief_description": "A computationally lightweight, example-calibrated simulator that models both the optical response and marker motion field of GelSight-style vision-based tactile sensors using a polynomial lookup table for photometric response and a linear-displacement + superposition approach for elastomer marker motion.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "GelSight vision-based tactile sensor (mounted on robotic manipulators)",
            "agent_system_description": "A vision-based tactile sensor using a soft elastomer (gelpad) with printed surface markers, embedded LEDs and an internal camera to measure surface geometry and infer contact forces/torques during robot manipulation.",
            "domain": "general robotics manipulation (robotic tactile sensing)",
            "virtual_environment_name": "Taxim (custom example-based tactile simulator)",
            "virtual_environment_description": "A simulator that (1) maps contact geometry/normal vectors and image plane location to pixel intensities via a calibrated polynomial lookup table (example-based photometric stereo), (2) synthesizes shadows from collected unit-shadow masks, (3) approximates gelpad height/deformation with pyramid Gaussian kernels for quasi-static contact, and (4) computes dense marker motion fields using FEM-calibrated 3×3 mutual-influence tensors with a linear displacement relationship and superposition.",
            "simulation_fidelity_level": "Sensor-level high-fidelity appearance and marker-motion approximation for quasi-static contacts (example-calibrated, computationally lightweight); not a full dynamic FEM at runtime.",
            "fidelity_aspects_modeled": "Optical/lighting response (per-pixel intensity via polynomial table), shadowing (unit-shadow masks accumulated), quasi-static elastomer deformation (height map approximated with pyramid Gaussian kernels), dense marker motion field (via calibrated mutual-influence tensors), and sensor intrinsic noise characteristics (captured via calibration examples).",
            "fidelity_aspects_simplified": "Runtime soft-body deformation approximated (pyramid Gaussian kernels) rather than full FEM; dynamic contact phenomena (time-dependent dynamics, slip) are not modeled; shadow inter-reflections are not modeled (shadows linearly accumulated per light); partial slip under shear not modeled; simulation is quasi-static only.",
            "real_environment_description": "Real GelSight sensors mounted on an optical XYR stage and indenter on a vertical stage (0.01 mm precision) with dome-shaped gelpads; data collected with multiple real GelSight/DIGIT sensors and 3D-printed test objects.",
            "task_or_skill_transferred": "Not demonstrated in this paper; authors state intent to apply Taxim to sim-to-real robot perception and manipulation tasks using tactile data (planned future work).",
            "training_method": null,
            "transfer_success_metric": null,
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Observed sources of sim-to-real gap: mismatch between hand-manufactured gelpad and FEM model geometry/material, noise in marker extraction/tracking from real sensor images, and absence of partial-slip modeling under shear in the simulator.",
            "transfer_enabling_conditions": "Calibration with examples from a real sensor (optical polynomial table with &lt;100 contact examples and a small set of shadow masks), explicit modeling of both optical response and marker motion, FEM-based calibration of mutual-influence tensors, and a computationally lightweight pipeline enabling large-scale data generation for downstream tasks.",
            "fidelity_requirements_identified": "The paper emphasizes that accurate modeling of the optical response (lighting, shadows) and marker motion field is important for realism; it also notes that to support realistic shear/slip behaviors dynamic and partial-slip modeling are required (i.e., current quasi-static fidelity is insufficient for some shear cases). No quantitative numerical fidelity thresholds are given.",
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": "For optical-image fidelity the paper reports Taxim outperforms three other simulators (Tacto, Phong, physics-based rendering) on L1, MSE, SSIM and PSNR (Ours: L1=5.565, MSE=58.358, SSIM=0.882, PSNR=30.974 vs. best alternative MSE=90.623 / PSNR=28.687). Marker-motion accuracy is close to FEM: dense-axis L1 errors on dataset ~3–5e-3 mm and sparse marker magnitude L1 errors ~1.02e-2 mm compared to real data; runtime is orders of magnitude faster than FEM (Taxim marker sim ~9.22 s on CPU vs. 2–4 hrs for ANSYS FEM).",
            "key_findings": "Taxim provides a practical, example-calibrated simulator that jointly models GelSight optical images and marker motion with high image similarity to real sensors and close correspondence to FEM marker displacements while being computationally lightweight; the simulator is not yet used for demonstrated sim-to-real skill transfer in this paper, and remaining sim-to-real gaps include manufactured gelpad variability, marker-tracking noise, and lack of partial-slip / dynamic contact modeling which should be addressed to enable robust transfer for tasks involving shear/slip.",
            "uuid": "e1836.0",
            "source_info": {
                "paper_title": "Taxim: An Example-based Simulation Model for GelSight Tactile Sensors",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "Ding et al. (TacTip)",
            "name_full": "Sim-to-real transfer for optical tactile sensing",
            "brief_description": "A prior work that created a physics-based soft-body simulation of the TacTip optical tactile sensor (pin-based membrane) to simulate pin motions, and evaluated the simulator on sim-to-real robot tasks.",
            "citation_title": "Sim-to-real transfer for optical tactile sensing",
            "mention_or_use": "mention",
            "agent_system_name": "TacTip optical tactile sensor",
            "agent_system_description": "A soft optical tactile sensor consisting of a membrane with internal pins whose motions are optically tracked to infer contact events and forces.",
            "domain": "general robotics manipulation (robotic tactile sensing)",
            "virtual_environment_name": "Physics soft-body simulation (Unity-based simulation of TacTip membrane)",
            "virtual_environment_description": "A physics-based soft-body simulator modeling the TacTip membrane dynamics and resulting pin motions (used to synthesize tactile observations for downstream tasks).",
            "simulation_fidelity_level": "Physics-based soft-body simulation intended to capture membrane dynamics (higher-fidelity dynamics for pin motion).",
            "fidelity_aspects_modeled": "Membrane dynamics and pin displacement/motion.",
            "fidelity_aspects_simplified": null,
            "real_environment_description": "TacTip sensor mounted on a robot performing manipulation tasks (used to evaluate sim-to-real transfer).",
            "task_or_skill_transferred": "Robotic tasks driven by TacTip tactile feedback (paper states evaluation on sim-to-real robot tasks; exact tasks not detailed in this paper).",
            "training_method": null,
            "transfer_success_metric": null,
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": null,
            "transfer_enabling_conditions": "Use of a physics-based soft-body simulation of the TacTip membrane to generate realistic pin motion signals for training/testing controllers/policies.",
            "fidelity_requirements_identified": null,
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": "This referenced work built a physics soft-body model for TacTip and evaluated sim-to-real transfer for robot tasks; details and quantitative transfer outcomes are not provided in the current paper's text.",
            "uuid": "e1836.1",
            "source_info": {
                "paper_title": "Taxim: An Example-based Simulation Model for GelSight Tactile Sensors",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "Sferrazza et al.",
            "name_full": "Learning the sense of touch in simulation: a sim-to-real strategy for vision-based tactile sensing",
            "brief_description": "Used a finite-element model to create a synthetic dataset for vision-based tactile sensors, trained networks to predict 3D contact force distributions in simulation, and demonstrated sim-to-real transfer of the learned models.",
            "citation_title": "Learning the sense of touch in simulation: a sim-to-real strategy for vision-based tactile sensing",
            "mention_or_use": "mention",
            "agent_system_name": "Vision-based tactile sensors (GelSight-like)",
            "agent_system_description": "High-resolution vision-based tactile sensors that produce images encoding contact geometry and can be used to infer force distributions.",
            "domain": "general robotics manipulation (robotic tactile sensing)",
            "virtual_environment_name": "Finite Element Method (FEM)-based tactile simulation",
            "virtual_environment_description": "An FEM simulation that models gelpad/elastomer deformation to synthesize tactile imagery and ground-truth contact force distributions for training data.",
            "simulation_fidelity_level": "High-fidelity physics (FEM) for soft-body deformation and force-distribution generation.",
            "fidelity_aspects_modeled": "Soft-body elastomer deformation, ground-truth 3D contact force distribution, and synthetic tactile image generation.",
            "fidelity_aspects_simplified": null,
            "real_environment_description": "Real vision-based tactile sensors used to validate transfer of networks trained in simulation.",
            "task_or_skill_transferred": "Prediction of 3D contact force distribution from tactile images (models trained in simulation transferred to real sensors).",
            "training_method": "Supervised learning (trained a network on FEM-generated synthetic data).",
            "transfer_success_metric": null,
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": true,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": null,
            "transfer_enabling_conditions": "High-fidelity FEM-generated synthetic dataset providing ground-truth force distributions enabled training of models that transfer to real sensors.",
            "fidelity_requirements_identified": "Implicit requirement: sufficiently accurate FEM simulation of elastomer deformation and force distribution is necessary to train transfer-capable models (no quantitative thresholds reported here).",
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": "FEM-based synthetic data can be used to train networks that generalize from simulation to real vision-based tactile sensors for force-distribution estimation; this prior work is cited as having realized sim-to-real transfer.",
            "uuid": "e1836.2",
            "source_info": {
                "paper_title": "Taxim: An Example-based Simulation Model for GelSight Tactile Sensors",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "Narang et al.",
            "name_full": "Sim-to-real for robotic tactile sensing via physics-based simulation and learned latent projections",
            "brief_description": "Extended finite-element and physics-based simulation of the BioTac sensor into Isaac Gym and used a learned generative mapping to project simulated deformations into electronic sensor-like signals for tactile interpretation tasks.",
            "citation_title": "Sim-to-real for robotic tactile sensing via physics-based simulation and learned latent projections",
            "mention_or_use": "mention",
            "agent_system_name": "BioTac multimodal tactile sensor",
            "agent_system_description": "A biomimetic multimodal tactile sensor providing electronic signals correlating with contact forces, used in robotic tactile perception.",
            "domain": "general robotics manipulation (robotic tactile sensing)",
            "virtual_environment_name": "ANSYS FEM and Isaac Gym physics-based simulation with learned projection (generative mapping)",
            "virtual_environment_description": "FEM/physics simulation of BioTac deformation (ANSYS/Isaac Gym) with an additional learned generative projection to convert simulated deformation to sensor electronic signals.",
            "simulation_fidelity_level": "High-fidelity physics (FEM/physics engine) combined with a learned mapping to match real sensor signals.",
            "fidelity_aspects_modeled": "Soft-body deformation and its mapping to sensor electronic outputs (via generative/learned latent projection).",
            "fidelity_aspects_simplified": null,
            "real_environment_description": "Real BioTac sensor data used to learn/validate mappings and downstream tasks.",
            "task_or_skill_transferred": "Tactile-signal interpretation tasks (training/transfer of models from simulated deformations to real sensor signal space), specifics not detailed in this paper.",
            "training_method": "Generative learning / learned latent projections to map simulated states to realistic sensor signals.",
            "transfer_success_metric": null,
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": null,
            "transfer_enabling_conditions": "Combining physics-based deformation simulation with learned generative projection to produce realistic sensor-like signals.",
            "fidelity_requirements_identified": null,
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": "Combining FEM/physics simulation of sensor deformation with a learned mapping to sensor output space is a strategy used to bridge simulation and real sensor signals for tactile perception tasks.",
            "uuid": "e1836.3",
            "source_info": {
                "paper_title": "Taxim: An Example-based Simulation Model for GelSight Tactile Sensors",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "Church et al.",
            "name_full": "Optical tactile sim-to-real policy transfer via real-to-sim tactile image translation",
            "brief_description": "Applied generative adversarial networks for real-to-sim tactile image translation (TacTip), enabling policy transfer between simulated depth maps and real tactile images.",
            "citation_title": "Optical tactile sim-to-real policy transfer via real-to-sim tactile image translation",
            "mention_or_use": "mention",
            "agent_system_name": "TacTip optical tactile sensor",
            "agent_system_description": "An optical tactile sensor where a membrane and internal markers/pins produce images that encode contact geometry; used for tactile-based policies.",
            "domain": "general robotics manipulation (robotic tactile sensing)",
            "virtual_environment_name": "Simulated depth maps with GAN-based real-to-sim image translation pipeline",
            "virtual_environment_description": "Simulated contact depth representations (depth maps) used as the simulated modality; a GAN performs real-to-sim translation so policies trained in sim see observations similar to simulated inputs.",
            "simulation_fidelity_level": "Approximate geometry-depth simulation combined with learned domain adaptation (real-to-sim translation) to close visual gaps.",
            "fidelity_aspects_modeled": "Contact geometry / depth maps; reliance on learned translation to produce realistic tactile-image appearance.",
            "fidelity_aspects_simplified": "Optical tactile appearance not directly simulated; requires GAN-based translation to match real sensor images.",
            "real_environment_description": "Real TacTip tactile images collected from sensor for training translation networks and validating policy transfer.",
            "task_or_skill_transferred": "Policy transfer for tactile-based control from simulation to real (specific tasks not detailed in this paper).",
            "training_method": "GAN-based real-to-sim image translation plus policy learning in simulation (exact policy learning method not specified here).",
            "transfer_success_metric": null,
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Appearance mismatch between simulated depth maps and real tactile images addressed via learned translation.",
            "transfer_enabling_conditions": "Use of GAN-based real-to-sim tactile image translation to map real observations into the simulated observation distribution enabling sim-trained policies to operate on real data.",
            "fidelity_requirements_identified": null,
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": "Real-to-sim tactile image translation (GAN-based) is a viable approach to reduce visual domain gap and support sim-to-real policy transfer for optical tactile sensors like TacTip.",
            "uuid": "e1836.4",
            "source_info": {
                "paper_title": "Taxim: An Example-based Simulation Model for GelSight Tactile Sensors",
                "publication_date_yy_mm": "2021-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sim-to-real transfer for optical tactile sensing",
            "rating": 2
        },
        {
            "paper_title": "Learning the sense of touch in simulation: a sim-to-real strategy for vision-based tactile sensing",
            "rating": 2
        },
        {
            "paper_title": "Sim-to-real for robotic tactile sensing via physics-based simulation and learned latent projections",
            "rating": 2
        },
        {
            "paper_title": "Optical tactile sim-to-real policy transfer via real-to-sim tactile image translation",
            "rating": 2
        },
        {
            "paper_title": "Ground truth force distribution for learning-based tactile sensing: A finite element approach",
            "rating": 1
        }
    ],
    "cost": 0.0190695,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Taxim: An Example-based Simulation Model for GelSight Tactile Sensors</h1>
<p>Zilin $\mathrm{Si}^{1}$ and Wenzhen Yuan ${ }^{1}$</p>
<h4>Abstract</h4>
<p>Simulation is widely used in robotics for system verification and large-scale data collection. However, simulating sensors, including tactile sensors, has been a long-standing challenge. In this paper, we propose Taxim, a realistic and high-speed simulation model for a vision-based tactile sensor, GelSight [1]. A GelSight sensor uses a piece of soft elastomer as the medium of contact and embeds optical structures to capture the deformation of the elastomer, which infers the geometry and forces applied at the contact surface. We propose an example-based method for simulating GelSight: we simulate the optical response to the deformation with a polynomial look-up table. This table maps the contact geometries to pixel intensity sampled by the embedded camera. In order to simulate the surface markers' motion that is caused by the surface stretch of the elastomer, we apply the linear displacement relationship and the superposition principle. The simulation model is calibrated with less than 100 data points from a real sensor. The example-based approach enables the model to easily migrate to other GelSight sensors or its variations. To the best of our knowledge, our simulation framework is the first to incorporate marker motion field simulation that derives from elastomer deformation together with the optical simulation, creating a comprehensive and computationally efficient tactile simulation framework. Experiments reveal that our optical simulation has the lowest pixel-wise intensity errors compared to prior work and can run online with CPU computing. Our code and supplementary materials are open-sourced at https://github.com/CMURoboTouch/Taxim.</p>
<h2>I. INTRODUCTION</h2>
<p>Simulation has been widely applied in robotics. It enables roboticists to quickly generate large amounts of realistic data, without costly equipment, manual labour, and the risk associated with real-world experiments. With growing interest in robot simulation, well-developed simulation frameworks such as Gazebo [2], PyBullet [3], MuJoCo [4], Drake [5], SOFA [6], NVIDIA Isaac Gym [7] have been widely used in the robotics community. They can simulate dynamic rigid-body, soft-body, vision and laser sensors with varying levels of accuracy and speed. However, none of them have integrated simulation of tactile sensing which form an irreplaceable part of robotic systems.</p>
<p>Recent advancements in vision-based tactile sensors, such as GelSight [8], [1], have made high-resolution tactile sensing available. These sensors use a piece of soft elastomer, or gelpad, as the contact medium for interacting with the environment. There is typically a printed marker array on the gelpad surface that moves as the surface stretches and is a good indicator of the contact forces and torques. The</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: The GelSight outputs when it contacts objects with rich textures. With the ground-truth geometry [9], our simulation model generates images that are very similar to the real ones.
sensor utilizes optical components, including LEDs and an embedded camera to capture the illumination change caused by the change of light reflection on the galpad surface when the sensor contacts an external object, as shown in Fig 3. Simulating those vision-based tactile sensors, which contains modeling both the mechanical response of the soft gelpad and the optical response to the deformation, is challenging. There have been previous studies on simulating different components of vision-based tactile sensors separately. For instance, Ding et al. [10] built a physics soft body simulation for the TacTip [11] sensor to indicate pins' motion on the soft membrane; Agarwal et al. [12] and Gomes et al. [13] applied physics-based models for vision-based tactile optical simulation; whereas Wang et al. [14] integrated the optical simulation of tactile sensors with the physics simulation engine PyBullet. However, the work mentioned above lack the ability to simulate the intrinsic noise of the real sensors. They also demand heavy computation while are difficult to generalize to new sensors.</p>
<p>In this work, we propose Taxim, an example-based tactile simulation model that combines optical simulation and marker motion field simulation. The method overcomes the constraints of the previous simulation models in that it is computationally lightweight and generates very similar</p>
<p>signals to the real sensors in spite of the intrinsic noise of the sensors. The model contains two parts:</p>
<ol>
<li>A polynomial table mapping function to simulate the optical response of a GelSight sensor by mapping geometries to pixel intensity in tactile images and an accumulation approach to simulate the shadow caused by the illumination.</li>
<li>A model to simulate the marker motion field using the linear displacement relationship and the superposition principle for the gelpad’s elastic deformation.</li>
</ol>
<p>Taxim is calibrated with less than 100 contact examples,so that it can easily migrate to other vision-based tactile sensors with similar designs as GelSight. To the best of our knowledge, Taxim is the first model that simulates all functions of vision-based tactile sensors, including the optical response for geometry measurement and marker motion field for force/torque measurement. Our simulation model can be integrated into robot simulation engines to provide a useful tool for tacile-based manipulation research.</p>
<h2>II. Related Work</h2>
<h3>A. Tactile Sensing Simulation</h3>
<p>The majority of tactile sensors use a soft medium for contact where the measurement of its deformation can indicate the contact information. Therefore the sensor simulation has been mostly focused on simulating the elastomer deformation. Typically, elastic soft body simulation is modelled with finite element methods (FEM) [15], mass-spring model [16], particles [17] or learning methods [18]. To simulate tactile sensors that use soft medium, a traditional way is to build an approximation model of the soft bodies. Pezzementi et al. [19] and Moisio et al. [20] simulated the low dimensional tactile sensor signals with a point spread function model and a soft contact model with a full friction description respectively. However, they are not applicable to the high-resolution vision-based tactile sensors such as GelSight. Narang et al. [21] used a finite element model to simulate the BioTac sensor [22] and contact data in ANSYS. As an extension work, they simulated the deformation of BioTac in Issac gym [7] and projected this deformation to electronic signal with a generative learning framework in [23]. Sferrazza et al. [24], [25] built a synthetic dataset with a finite element model of the vision-based tactile sensors, trained a network to predict the 3D contact force distribution in simulate and realized sim-to-real transfer. In this work, instead of using an accurate finite element model with high-computing cost, we approximate the deformation of the soft medium on the GelSight sensor with pyramid Gaussian kernels which is efficient and also gives acceptable accuracy.</p>
<h3>B. Optical Simulation for Tactile Sensors</h3>
<p>For vision-based tactile sensors like GelSight, optical simulation is essential as it is used to measure geometries of the contacted objects. To simulate the optical system of GelSight, Gomes et al. [13] and Hogan et al. [26] used Phong’s model to simulate the reflection and illumination. Agarwal et al. [12] applied ray tracing to simulate the light paths within the sensor that form tactile images. Wang et al. [14] presented TACTO, an open-source simulator using pyrender to simulate DIGIT [27] sensors and bridged it to a physics simulator PyBullet. Compared to those physics-based methods, our method is data-driven, so that it is computationally efficient and can better simulate the intrinsic noise of the real sensors.</p>
<h3>C. Marker Motion Field Simulation for Tactile Sensors</h3>
<p>The movement of marker array on GelSight or other vision-based tactile sensors is caused by the planar stretch of the elastomer surface. They also make the key component of many vision-based tactile sensors such as TacTip [11]. In manipulation tasks such as slip detection [28] and grasping stability prediction [29], the marker motion serves as an essential feature. For TacTip, Ding et al. [10] simulated the dynamics of its soft membrane in Unity so as to extracted markers’ motion. They evaluated the simulation on sim-to-real robot tasks. Church et al. [30] simulated the depth maps to represent the contact geometries instead of optical tactile images. They also used a Generative Adversarial Networks (GANs) to realize real-to-sim translation for TacTip sensors. Unlike the above work, we explicitly simulate the marker motion field by using FEM offline and applying the superposition principle [31] online. Our method does not require extensive training data but only a gelpad FEM model, and it approximates the marker motion field well with high accuracy and low computation cost.</p>
<h2>III. Methods</h2>
<h3>A. Overview</h3>
<p>We construct and employ our simulation models for both optical response and marker motion of GelSight sensors. To simulate the sensor’s optical response, we build a polynomial table to map the contact geometries to the image intensities and collect shadow masks to attach the shadow. Then we apply the superposition principle based on the loading displacement of each finite unit to simulate the markers’ motion. Their combination replicates the contacting of objects on the tactile sensor. For both parts, we calibrate our simulator</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 3: (a) Demo of the photometric stereo method: for a surface point ( p ) under the light ( l ), the reflected light intensity captured by the camera is determined by the surface reflectance and the surface normal ( \vec{n}_p ) (b) The GelSight sensor [32] we aim to simulate and (c) its schematic diagrams of the optical structure.</p>
<p>with examples from a real sensor. We show the pipeline for building and applying the simulation model in Fig. 2.</p>
<h2><em>B. Optical Simulation</em></h2>
<p>We simulate the optical response of the GelSight sensor as a result of the contact geometry with a model using the examples-based photometric stereo [33] method. Photometric stereo uses the linear reflection function to derive the illumination of the object with the light sources and shape of the illuminated surface. Example-based photometric stereo does not require prior knowledge of light sources but instead uses the imaging of the reference object. We use a lookup table as the baseline and a polynomial table as our proposed method to map the contact shapes to image intensities.</p>
<p><em>a) Lookup Table Mapping:</em> The gelpad has a homogeneous diffuse internal surface which makes the reflection function spatial-invariant. The linear reflection function used by photometric stereo is formalized as ( I_p = \rho \mathbf{n_p} \sum \mathbf{l} ), where at a point ( p ), the observed light intensity ( I_p ) caused by reflection is a product of the albedo ( \rho ), the surface normal ( \mathbf{n_p} ) and the light direction and intensity ( \mathbf{l} ), as shown in Fig. 3 (a). The previous equation implies that the reflected light intensity ( I ) and the surface normal ( \mathbf{n} ) are linearly correlated. Alternatively, instead of solving the equation from the given lighting conditions, an intensity-shape lookup table can be built as follows:</p>
<p>$$I = \sum_{l} a^l \mathbf{n} \tag{1}$$</p>
<p>where ( a^l ) is the coefficient of the light ( l ), and can derived from a calibration process similar to [8], [34]. Here, we define the lights of the same color–any of red, green or blue–as one light source, even though they are contributed by multiple LEDs.</p>
<p><em>b) Polynomial Table Mapping:</em> The linear lookup table works well with the point lights which are far from the object, whose directions are parallel and intensities are uniform for all the points on the illuminated surface. However, the LEDs in the GelSight are close to the sensor surface so that the emitted light is not strictly parallel and uniform. To compensate for the complicated lighting conditions, we introduce a non-linear model for the reflection as proposed</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 4: Data collection setup (a, b, c) and data examples (d, e) to build the optical simulation model. The GelSight is placed on an XYR optical stage and an indenter with a certain shape object is mounted on a vertical linear stage for precisely indenting on the GelSight. We calibrate the polynomial table with less than 100 data points using a spherical indenter and collect shadow masks with 10 data points using a pin indenter.</p>
<p>in [35]. The reflection function can then be rewritten as</p>
<p>$$I = \sum_{l} f_{\mathbf{n}}^l(x, y) \tag{2}$$</p>
<p>where ( \mathbf{n} ) is the normal vector representative of the surface shape and ( (x, y) ) is the 2D location on the image plane. From experiments, we found that in practice a second order polynomial function is sufficient to approximate the nonlinearity. Thus, the non-linear function is represented as:</p>
<p>$$f_{\mathbf{n}}^l(x, y) = \mathbf{w}_{\mathbf{n}}^l \mathbf{b} \tag{3}$$</p>
<p>where ( \mathbf{w}_{\mathbf{n}}^l ) is a ( 6 \times 1 ) vector that represents the parameters to model the polynomial table, and ( \mathbf{b} = [x^2, y^2, xy, x, y, 1]^T ).</p>
<p><em>c) Calibration:</em> Calibration entails fitting the parameters in the polynomial table from real data. Since these parameters vary for different sensors, this process has to be done per sensor. During calibration, we press a small ball with a known radius over the surface and manually locate contact areas in the tactile images as shown in Fig 4 (b) and (d). The surface normal at each point in the circular area can be easily calculated based on the ball's geometry. We discretize the 3D surface normal vectors to a ( 125 \times 125 ) table with the magnitude and direction of the surface normal as the two dimensions. The parameters in polynomial table can then be solved via least squares with the set of intensity-shape-location pairs ( (I_p, \mathbf{n}_p, x_p, y_p) ) from collected data. We fill invalid values in the table by interpolation.</p>
<p><em>d) Simulation:</em> We simulate the visual outputs in three steps: collision detection, deformation approximation, and optical simulation. A collision is detected when an object comes in contact with the gelpad. From this contact, the local shape, represented as a height map, is constructed from the object's shape in the contact area, and gelpad's shape in non-contact area. Additionally, we need to simulate the soft body deformation from the height map. An approximation of soft body simulation is applied with pyramid Gaussian kernels. The shape in contact area is kept unchanged to maintain the fine textures and the boundaries between the contact and non-contact areas are smoothed using pyramid Gaussian kernels.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 5: Shadow synthesis. (a) A unit shadow case observed under the lighting. (b) We approximate the object as the composition of unit pin case and then attach the shadow caused by each pin. (c), (d) and (e): We collect a set of shadow masks and synthesize the shadow around the contact area.</p>
<p>from large to small. From the height map, the normal vector for each point can be extracted and mapped to an intensity value with the calibrated polynomial table to synthesize the tactile images.</p>
<h3><em>C. Shadow Simulation</em></h3>
<p>Other than the illumination change that is modeled with photometric stereo method, the shadow is another factor causing the change of the pixel intensity in the tactile images. According to the design of the sensor, the shadows are caused by three groups of LED lights: red, green and blue lights. We simulate the shadow from those light sources respectively. We simplify shadow casting by collecting the "unit" shadow case, and then simulate the shadow by accumulating the shadows caused by each geometrical "unit". Since each light beam is traveling independently in the space, without considering inter-reflection, the shadow cast by them can be linearly accumulated.</p>
<p>A "unit" shadow is the shadow cast by a standing pin, as shown in Fig. 5 (a). For objects with different geometries, we consider them as the accumulation of "unit" shadows placed side by side with different heights, as illustrated in Fig. 5 (b). Therefore, given the tactile images with shadow cast by indenting a pin normally onto the gelpad with different depths as shown in Fig. 4 (c) and (e), we extract a set of shadow masks on three dominant directions caused by three light sources. For a general case, the shadow mask is attached for all three color channels and all points within the contact area if the neighbors are lower than that point.</p>
<h3><em>D. Marker Motion Field Simulation</em></h3>
<p>We simulate the markers' motion on the gelpad surface caused by the deformation of the soft gelpad from contacting. In this work, we consider the deformation under normal and shear loads. We employ the linear displacement relationship and superposition principle [31] to compose the deformation of the surface with loads on each finite unit of the contact surface. Although the markers are sparsely spread on the surface, we mesh the surface with dense nodes in simulation, track each node's motion and then locate the markers' motions. With the dense solution, the method can be applied to markers at any locations. Nodes in the gelpad surface mesh are classified into two categories: <em>active nodes</em> who come in contact with the object and are applied external forces and constrained by internal elastic forces; <em>passive nodes</em> who are in non-contacted area and only constrained by the internal elastic forces.</p>
<p>The linear displacement relationship assumes that any two nodes can influence each other in a linear way. By considering two nodes <em>n<sub>i</sub></em> and <em>n<sub>j</sub></em> with displacement in 3D as <strong>u<sub>i</sub></strong> and <strong>u<sub>j</sub></strong>, the <em>n<sub>j</sub></em> can be passively influenced by <em>n<sub>i</sub></em> as <strong>u<sub>j</sub></strong> = <em>T<sub>n<sub>j&lt;/sub</sub><sup>m</sup></em> <strong>u<sub>i</sub></strong>, where <em>T<sub>n<sub>j&lt;/sub</sub><sup>m</sup></em> is a 3 × 3 tensor representing the mutual influence. The superposition principle states that a node <em>n<sub>i</sub></em>'s displacement <strong>u<sub>i</sub></strong> is an aggregation of all active nodes' influence to it. Assume we have active nodes <em>K</em> = {<em>k<sub>1</sub></em>, <em>k<sub>2</sub></em>, <em>k<sub>3</sub></em>, ..., <em>k<sub>m</sub></em>}, where <strong>u<sub>i</sub><sup>k</sup></strong> represent active nodes' initial displacement under external loads. Under the linear displacement relationship and superposition principle, any node <em>n<sub>j</sub></em>'s displacement <strong>u<sub>j</sub></strong> can be composed as</p>
<p>$$
\mathbf{u}<em i="1">j = \sum</em>
$$}^m T_{n_j}^{k_i} \mathbf{u}_i^k \tag{4</p>
<p>However, before applying the superposition principle by using the active nodes' initial displacement <strong>u<sub>i</sub><sup>k</sup></strong>, we need to amend them to virtual displacements under the virtual loads because they not only are constrained by external loads but also influence each other. For instance, if all the active nodes' displacements are initialized such that they only move along the z direction <em>i.e.</em> <strong>u<sup>k</sup></strong> = [0, 0, <em>dz</em>], the following equation holds:</p>
<p>$$
\mathbf{u}<em i="1">j^k[z] = \sum</em>
$$}^m T_{k_j}^{k_i}[3, 3] \tilde{\mathbf{u}}_i^k[z] \tag{5</p>
<p>where <strong>u<sub>j</sub><sup>k</sup></strong> is the initialized displacement, and <strong>u<sub>j</sub><sup>k</sup></strong>[z] is its component along z-direction; <strong>u<sub>k</sub><sup>k</sup></strong> is the virtual displacement under the virtual load; <em>T<sub>k<sub>j</sub></sub><sup>k<sub>i</sub></sup></em>[3, 3] is the last element in the tensor <em>T<sub>k<sub>j</sub></sub><sup>k<sub>i</sub></sup></em>. Therefore, it is able to solve virtual displacements for active nodes by stacking all the equations as:</p>
<p>$$
\mathbf{u}^{\mathbf{k}}[z] = M_x \tilde{\mathbf{u}}^{\mathbf{k}}[z] \tag{6}
$$</p>
<p>$$
\begin{bmatrix}
\mathbf{u}_1^{\mathbf{k}}[z] \
\mathbf{u}_2^{\mathbf{k}}[z] \
\cdots \
\mathbf{u}_m^{\mathbf{k}}[z]
\end{bmatrix} =
\begin{bmatrix}
T_1^1[3, 3] &amp; T_2^1[3, 3] &amp; \cdots &amp; T_m^1[3, 3] \
T_1^2[3, 3] &amp; T_2^2[3, 3] &amp; \cdots &amp; T_m^2[3, 3] \
\cdots &amp; \cdots &amp; &amp; \
T_1^m[3, 3] &amp; T_2^m[3, 3] &amp; \cdots &amp; T_m^m[3, 3]
\end{bmatrix}
\begin{bmatrix}
\tilde{\mathbf{u}}_1^{\mathbf{k}}[z] \
\tilde{\mathbf{u}}_2^{\mathbf{k}}[z] \
\cdots \
\tilde{\mathbf{u}}_m^{\mathbf{k}}[z]
\end{bmatrix} \tag{6}
$$</p>
<p>Then <strong>u<sub>k</sub><sup>k</sup></strong>[z] is solved by matrix inversion as <strong>u<sub>k</sub><sup>k</sup></strong>[z] = <em>M<sub>x</sub><sup>−1</sup></em> <strong>u<sub>k</sub><sup>k</sup></strong>[z].</p>
<p>The <em>x, y</em> components of the active nodes' displacement can be amended using the same approach, but with <em>T</em>[1, 1] or <em>T</em>[2, 2] for the <em>x</em> or <em>y</em> directions respectively. Later, we apply the superposition principle to get the final resultant displacements for all nodes with</p>
<p>$$
\mathbf{u}<em n_j="n_j">j = \sum_i T</em>
$$}^k \tilde{\mathbf{u}}_i^k \tag{7</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 6: Elastic deformation calibration and simulation for the gelpad. We calibrate the deformation of gelpad under a unit pin with 0.5 mm diameter indenting in ANSYS to get the dense nodal displacement results (left). In simulation, we compose each node's displacement from elastic deformation in three steps: (a) initial displacement boundary conditions on active nodes, (b) active nodes' virtual displacements, and (c) resultant nodal displacements for both active nodes in contact area and passive nodes in non-contact area.</p>
<p>a) <strong>Calibration:</strong> The tensor $$T_{a_j}^{k_j}$$ depends on the gelpad's physical properties, and therefore can be measured in advance. The markers on the real gelpad are sparsely distributed which cannot be used to generate dense meshes. Instead, we calibrate the arbitrary $$T_{a_j}^{k_j}$$ in a Finite Element Method (FEM) software ANSYS. In ANSYS, we generate the dense mesh of the gelpad and measure the deformation when there is a load on a unit node, as shown in Fig. 6 (left). We then use the measurement of the deformation to calibrate $$T$$. Since the markers are not printed on the top surface of the gelpad, we extract the second layer's mesh which is 0.5mm below the top surface from the simulated model as reference. To fully calibrate the $$3 \times 3$$ tensors, we simulate an active node's motion in z-direction only, a combination of z-direction and x-direction and a combination of z-direction and y-direction. Then we solve all the tensors $$T_{a_j}^{k_j}$$ using least squares from these three sets of unit case.</p>
<p>b) <strong>Simulation:</strong> We employ the marker motion simulation in three steps, by: 1) applying the initial displacements on the active nodes under the external loads, 2) getting active nodes' virtual displacements with the superposition principle, and then 3) calculating the resultant displacements at each node using the superposition principle with virtual displacements of active nodes. This process is demonstrated in Fig. 6 (a), (b), (c).</p>
<h3>IV. EXPERIMENTS</h3>
<p>We perform a set of experiments to evaluate the similarity between the simulated tactile data and that from real sensors.</p>
<h4>A. Experiment Setup and Data Collection</h4>
<p>To collect well-controlled contact data with a real GelSight sensor, we set up an optical platform, as shown in Fig. 4 (a). The GelSight is placed on a XYR stage, and an indenter is mounted on a vertical linear stage positioned above the GelSight. We manually control the contact location and depth by adjusting the stages. The XYR stage enables horizontal movement and the vertical stage adjusts the indenting depth. Both are with 0.01mm precision. We use a dome-shaped gelpad for both the real sensor and the simulated sensor.</p>
<p>We evaluate our simulation using objects with different shapes and textures. The objects are designed in Solidworks [36], output as mesh files for simulation (Fig. 7 (a))</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 7: Dataset of objects designed in Solidworks (a) and 3D printed (b) for contact experiments. The objects are of different shapes. Their base sizes are either 10mm × 10mm or 15mm × 15mm.</p>
<p>and 3D printed for collecting data from the real sensor for comparison (Fig. 7 (b)).</p>
<h4>B. Optical Simulation</h4>
<p>To calibrate the optical simulation model, we collect 50 data points on different locations of gelpad surface with a 4mm-diameter spherical indenter, as shown in Fig. 4 (b), (d); to calibrate the shadow simulation model, we collect 10 data points of different pressing depths with a 1mm-diameter pin indenter, as shown in Fig. 4 (c), (e). The calibration process is simple and easy to conduct manually without precise control of contact locations which can be accomplished within 1 hour. And it can be used till any components of the sensor are replaced or the sensor is broken.</p>
<p>We simulate the tactile images on the aforementioned dataset and compare our method with three other methods: the physics-based model [12], TACTO [14] and Phong's model [13] as shown in Fig. 8. We evaluate our method by comparing the simulated images with the real images in pixel-wise level, against the three methods mentioned above on four metrics: mean absolute error (L1), mean squared error (MSE), structural index similarity (SSIM) and peak signal-to-noise ratio (PSNR). The simulated images are cropped to size 400 × 400 around the indenting area to eliminate the background's effect. Also, due to the precision of the operation with the real sensor, the ground truth tactile images are not well aligned with the simulated images. So we manually align the images using GIMP [37]. The quantitative results are summarized in the Table. I. From the table, our method outperforms all the other methods.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Fig. 8: Optical simulation comparison among our method, TACTO [14], Phong [13] and physics [12] with the real data.</p>
<table>
<thead>
<tr>
<th></th>
<th>L1 ↓</th>
<th>MSE ↓</th>
<th>SSIM ↑</th>
<th>PSNR ↑</th>
</tr>
</thead>
<tbody>
<tr>
<td>Tacto [14]</td>
<td>10.861</td>
<td>215.861</td>
<td>0.808</td>
<td>25.495</td>
</tr>
<tr>
<td>Phong's [13]</td>
<td>8.163</td>
<td>123.249</td>
<td>0.832</td>
<td>27.763</td>
</tr>
<tr>
<td>Physics [12]</td>
<td>7.409</td>
<td>90.623</td>
<td>0.759</td>
<td>28.687</td>
</tr>
<tr>
<td>Ours</td>
<td>5.565</td>
<td>58.358</td>
<td>0.882</td>
<td>30.974</td>
</tr>
</tbody>
</table>
<p>TABLE I: Image similarity metrics between simulation and real data for optical simulation. We compare our method with methods from Physics-based model [12], Tacto [14] and Phong's model [13] on L1, MSE, SSIM and PSNR metrics. Our method performs the best on all the metrics.</p>
<p>Different indentation depths and locations Our optical simulation model works well for different indentation depths and locations. One example is shown in Fig. 9. From MSE errors, we can see errors increase when the indentation become farther from the center and deeper.</p>
<p>Fine texture simulation Our model can simulate the contact cases with fine-textured objects, as shown in Fig. 1.</p>
<p>Simulation on various sensors and objects Note that tactile images look different in Fig. 8, Fig. 9 Fig. 11 and Fig. 13. This is because we use 4 different GelSight sensors and manufactures lead to the difference. However, our model works well on all of them. We also apply our model on a DIGIT sensor [27] and the results are shown in Fig. 10. In addition, We test our model on various objects from the Google Scan dataset [38] and some results are shown in Fig. 11.</p>
<p>Speed test We test all the simulation techniques, mentioned above, on a AMD Ryzen Threadripper 2950X 16-Core Processor CPU. We input height maps with the size 480 × 640, and output the simulated tactile images of the dataset. We then record the average running time of all the methods, as shown in the Table II. Our method is the most computationally lightweight on CPU and achieves the real-time data transferring speed from real sensors. However, Tacto [14] and Physics model [12] can be largely accelerated on GPUs but not considered here for evaluation. Our method can be potentially optimized for GPU computation as well and we will work on that for the next step.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Fig. 9: Optical simulation results with different indentation depths and locations. The locations differ over the gelpad surface while the depths differ as 0.5mm, 1.0mm, 1.5mm. The MSE error is shown below each pair.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Fig. 10: Optical simulation results (right) for a DIGIT sensor (left).</p>
<table>
<thead>
<tr>
<th>Speed</th>
<th>Ours w/o shadows</th>
<th>Ours w/ shadows</th>
<th>Physics [12]</th>
<th>Tacto [14]</th>
<th>Phong's [13]</th>
</tr>
</thead>
<tbody>
<tr>
<td>Frequency (fps)</td>
<td>18.1</td>
<td>9.6</td>
<td>0.1</td>
<td>1.9</td>
<td>3.8</td>
</tr>
</tbody>
</table>
<p>TABLE II: Speed test for optical simulation on CPU. We compare our method with the Physics-based model [12], Tacto [14] and Phong's model [13]. Our method runs with the fastest speed.</p>
<h3>C. Marker Motion Field Simulation</h3>
<p>We evaluate the simulation results with two references: 1) the dense displacement map generated by the FEM simulation, and 2) the sparse displacement map collected from a real sensor. The contact cases are with objects in Fig. 7 under combinations of different normal loads and shear loads. The load displacement varies from 0.3 mm to 0.8 mm.</p>
<p>Comparison with FEM simulation As illustrated in the four sets of comparison from Fig. 12, the dense mesh vertices displacements on X, Y, Z are simulated from both the FEM (Fig. 12 (b) left) and our methods (Fig. 12 (b) right). The color red means negative displacement value and blue means positive values. The average interpolated pixel-wised L1 errors over the gelpad surface on dataset are 3.58 × 10<sup>−3</sup> mm for X-axis, 3.32 × 10<sup>−3</sup> mm for Y-axis, 5.43 × 10<sup>−3</sup> mm for Z-axis, and 5.40 × 10<sup>−3</sup> mm for XY (gelpad surface).</p>
<p>Comparison with both real data and FEM simulation Examples of results are shown in Fig. 13. The mean of marker motion's magnitude L1 errors on dataset is 1.00 × 10<sup>−2</sup> mm between real &amp; FEM, 1.02 × 10<sup>−2</sup> mm between real &amp; ours, and 3.96 × 10<sup>−3</sup> mm between FEM &amp; ours. We weight the marker motion's angular errors based on the</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Fig. 11: Optical simulation results for objects from the Google Scan dataset [38]. We touch the objects with a GelSight mounted on a robot arm given certain contact locations. Most artifacts in the simulated images come from the coarse mesh files of the objects.</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Fig. 12: The simulated marker motion field in a dense mesh in comparison with the FEM data. The heat maps show the marker motion field in X, Y and Z directions where positive Z is the direction of normal loads and X, Y are the direction of shear loads.</p>
<p>Its magnitude because smaller marker motion is easier being affected by the system noise. The weighted mean of marker motion's angular L1 errors is 12.94° between real &amp; FEM, 14.57° between real &amp; ours, and 4.89° between FEM &amp; ours. From the experimental results, the FEM model and our model match well, but there is still a gap from the simulation to the real gelpad soft body model. Three reasons observed from the experiments causing the errors are: 1) The gelpad is hand-manufactured and it is not perfectly matched with the FEM model in ANSYS. 2) The marker motions tracked from the real sensor's data have the noise in marker extraction and tracking. 3) When shear loads are present, our model cannot model the partial slip but it is very common for the real contact cases.</p>
<p><strong>Speed Testing</strong> Our dense marker motion field simulation runs 9.22 seconds on average tested with CPU only. The FEM simulation in ANSYS with CPU costs 2 to 4 hrs for difference cases. In addition, according to Narang et al. [23]'s 5.57 seconds per sim for BioTac sensor in Isaac Gym with GPU acceleration, our simulation has a reasonable low computing demand.</p>
<p>We show some final results that combine the optical simulation and marker motion simulation in Fig. 13.</p>
<h2>V. CONCLUSION</h2>
<p>We present Taxim, an example-based GelSight tactile simulation model that combines optical and marker motion field simulation. We construct a polynomial table to simulate the optical response of GelSight from the contact geometry, and apply the linear displacement relationship and the superposition principle to simulate the markers' motion. Our simulation is computationally light weight, easy to set up and use, and simple to apply to different sensors. It also incorporates the sensor's illumination features and system noise through calibration with examples from real sensors. We have shown that our optical simulation outperforms the other state-of-the-art tactile simulations, and our marker motion field simulation achieves high accuracy by evaluating on a self-designed dataset. To the best of our knowledge, this is the first integrated work considering both the optical and marker motion simulation.</p>
<p>To extend this work, we plan to apply different sim-to-real robot perception and manipulation tasks using our simulation model. In addition, currently we simulate the quasi-static contact, and we plan to investigate the dynamic contact process and simulate the dynamic phenomena such as slip. The simulation pipeline can be computationally improved by applying GPU acceleration.</p>
<h2>ACKNOWLEDGMENT</h2>
<p>This work is supported by Facebook. The authors sincerely thank Arpit Agarwal, Tim Man, Yufan Zhang, Xiaofeng Guo, Uksang Yoo, Raj Kolamuri, Ruijia Xing, William Yan, Alankar Kotwal, Ioannis Gkioulekas, Mononito Goswami, Sudharshan Suresh, Haomin Shi for all the help on the discussion, experiment setup and manuscript revision.</p>
<h2>REFERENCES</h2>
<ul>
<li>[1] W. Yuan, S. Dong, and E. H. Adelson, "Gelsight: High-resolution robot tactile sensors for estimating geometry and force," <em>Sensors</em>, vol. 17, no. 12, p. 2762, 2017.</li>
<li>[2] "Gazebo," http://gazebosim.org/.</li>
<li>[3] E. Coumans and Y. Bai, "Pybullet, a python module for physics simulation for games, robotics and machine learning," 2016.</li>
<li>[4] E. Todorov, T. Erez, and Y. Tassa, "Mujocov: A physics engine for model-based control," in <em>2012 IEEE/RSJ International Conference on Intelligent Robots and Systems</em>. IEEE, 2012, pp. 5026–5033.</li>
<li>[5] R. Tedrake, T. Team, <em>et al.</em>, "Drake: Model-based design and verification for robotics," 2019.</li>
<li>[6] J. Allard, S. Cotin, F. Faure, P.-J. Bensoussan, F. Poyer, C. Duriez, H. Delingette, and L. Grisoni, "Sofa-an open source framework for medical simulation," in <em>MMVR 15-Medicine Meets Virtual Reality</em>, vol. 125. IOP Press, 2007, pp. 13–18.</li>
<li>[7] V. Makoviychuk, L. Wawrzyniak, Y. Guo, M. Lu, K. Storey, M. Macklin, D. Hoeller, N. Rudin, A. Allshire, A. Handa, <em>et al.</em>, "Isaac gym: High performance gpu-based physics simulation for robot learning," <em>arXiv preprint arXiv:2108.10470</em>, 2021.</li>
<li>[8] M. K. Johnson and E. H. Adelson, "Retrographic sensing for the measurement of surface texture and shape," in <em>2009 IEEE Conference on Computer Vision and Pattern Recognition</em>. IEEE, 2009, pp. 1070–1077.</li>
<li>[9] I. Gkioulekas, A. Levin, F. Durand, and T. Zickler, "Micron-scale light transport decomposition using interferometry," <em>ACM Transactions on Graphics (ToG)</em>, vol. 34, no. 4, pp. 1–14, 2015.</li>
</ul>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Fig. 13: Marker motion field simulation with optical simulation results. We visualize the marker motions (scaled up by 20 for better visualization) on the dataset under different normal displacements and shear displacements.</p>
<ul>
<li>[10] Z. Ding, N. F. Lepora, and E. Johns, "Sim-to-real transfer for optical tactile sensing," in <em>2020 IEEE International Conference on Robotics and Automation (ICRA)</em>. IEEE, 2020, pp. 1639–1645.</li>
<li>[11] B. Ward-Cherrier, N. Pestell, L. Cramphorn, B. Winstone, M. E. Giannaccini, J. Rossiter, and N. F. Lepora, "The tactip family: Soft optical tactile sensors with 3d-printed biomimetic morphologies," <em>Soft robotics</em>, vol. 5, no. 2, pp. 216–227, 2018.</li>
<li>[12] A. Agarwal, T. Man, and W. Yuan, "Simulation of vision-based tactile sensors using physics based rendering," in <em>2021 IEEE International Conference on Robotics and Automation (ICRA)</em>. IEEE, 2021, pp. 1–7.</li>
<li>[13] D. F. Gomes, P. Paoletti, and S. Luo, "Generation of gelsight tactile images for sim2real learning," <em>IEEE Robotics and Automation Letters</em>, vol. 6, no. 2, pp. 4177–4184, 2021.</li>
<li>[14] S. Wang, M. Lambeta, P.-W. Chou, and R. Calandra, "Tacto: A fast, flexible and open-source simulator for high-resolution vision-based tactile sensors," <em>arXiv preprint arXiv:2012.08456</em>, 2020.</li>
<li>[15] D. Ma, E. Donlon, S. Dong, and A. Rodriguez, "Dense tactile force estimation using gelslim and inverse fem," in <em>2019 International Conference on Robotics and Automation (ICRA)</em>. IEEE, 2019, pp. 5418–5424.</li>
<li>[16] C. Hui, S. Hanqiu, and J. Xiaogang, "Interactive haptic deformation of dynamic soft objects," in <em>Proceedings of the 2006 ACM international conference on Virtual reality continuum and its applications</em>, 2006, pp. 255–261.</li>
<li>[17] Y. Wang, W. Huang, B. Fang, F. Sun, and C. Li, "Elastic tactile simulation towards tactile-visual perception," in <em>Proceedings of the 29th ACM International Conference on Multimedia</em>, 2021, pp. 2690–2698.</li>
<li>[18] D. Casas and M. A. Otaduy, "Learning nonlinear soft-tissue dynamics for interactive avatars," <em>Proceedings of the ACM on Computer Graphics and Interactive Techniques</em>, vol. 1, no. 1, pp. 1–15, 2018.</li>
<li>[19] Z. Pezzementi, E. Jantho, L. Estrade, and G. D. Hager, "Characterization and simulation of tactile sensors," in <em>2010 IEEE Haptics Symposium</em>. IEEE, 2010, pp. 199–205.</li>
<li>[20] S. Moisio, B. León, P. Korkealaakso, and A. Morales, "Simulation of tactile sensors using soft contacts for robot grasping applications," in <em>2012 IEEE International Conference on Robotics and Automation</em>. IEEE, 2012, pp. 5037–5043.</li>
<li>[21] Y. S. Narang, K. Van Wyk, A. Mousavian, and D. Fox, "Interpreting and predicting tactile signals via a physics-based and data-driven framework," <em>arXiv preprint arXiv:2006.03777</em>, 2020.</li>
<li>[22] T. Yamamoto, N. Wettels, J. A. Fishel, C.-H. Lin, and G. E. Loeb, "Biotac -bionimetic multi-modal tactile sensor," <em>Journal of the Robotics Society of Japan</em>, vol. 30, no. 5, pp. 496–498, 2012.</li>
<li>[23] Y. Narang, B. Sundaralingam, M. Macklin, A. Mousavian, and D. Fox, "Sim-to-real for robotic tactile sensing via physics-based simulation and learned latent projections," <em>arXiv preprint arXiv:2103.16747</em>, 2021.</li>
<li>[24] C. Sferrazza, A. Wahlsten, C. Trueeb, and R. D'Andrea, "Ground truth force distribution for learning-based tactile sensing: A finite element approach," <em>IEEE Access</em>, vol. 7, pp. 173 438–173 449, 2019.</li>
<li>[25] C. Sferrazza, T. Bi, and R. D'Andrea, "Learning the sense of touch in simulation: a sim-to-real strategy for vision-based tactile sensing," in <em>2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>. IEEE, 2020, pp. 4389–4396.</li>
<li>[26] F. R. Hogan, M. Jenkin, S. Rezaei-Shoshtari, Y. Girdhar, D. Meger, and G. Dudek, "Seeing through your skin: Recognizing objects with a novel visuotactile sensor," in <em>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em>, 2021, pp. 1218–1227.</li>
<li>[27] M. Lambeta, P.-W. Chou, S. Tian, B. Yang, B. Malson, V. R. Most, D. Stroud, R. Santos, A. Byagowi, G. Kammerer, et al., "Digit: A novel design for a low-cost compact high-resolution tactile sensor with application to in-hand manipulation," <em>IEEE Robotics and Automation Letters</em>, vol. 5, no. 3, pp. 3838–3845, 2020.</li>
<li>[28] W. Yuan, R. Li, M. A. Srinivasan, and E. H. Adelson, "Measurement of shear and slip with a gelsight tactile sensor," in <em>2015 IEEE International Conference on Robotics and Automation (ICRA)</em>. IEEE, 2015, pp. 304–311.</li>
<li>[29] R. Calandra, A. Owens, M. Upadhyaya, W. Yuan, J. Lin, E. H. Adelson, and S. Levine, "The feeling of success: Does touch sensing help predict grasp outcomes?" in <em>Conference on Robot Learning</em>. PMLR, 2017, pp. 314–323.</li>
<li>[30] A. Church, J. Lloyd, R. Hadsell, and N. F. Lepora, "Optical tactile sim-to-real policy transfer via real-to-sim tactile image translation," <em>arXiv preprint arXiv:2106.08796</em>, 2021.</li>
<li>[31] S. Cotin, H. Delingette, and N. Ayache, "Real-time elastic deformations of soft tissues for surgery simulation," <em>IEEE transactions on Visualization and Computer Graphics</em>, vol. 5, no. 1, pp. 62–73, 1999.</li>
<li>[32] S. Dong, W. Yuan, and E. H. Adelson, "Improved gelsight tactile sensor for measuring geometry and slip," in <em>2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>. IEEE, 2017, pp. 137–144.</li>
<li>[33] A. Hertzmann and S. M. Seitz, "Example-based photometric stereo: Shape reconstruction with general, varying bndfs," <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, vol. 27, no. 8, pp. 1254–1264, 2005.</li>
<li>[34] M. K. Johnson, F. Cole, A. Raj, and E. H. Adelson, "Microgeometry capture using an elastomeric sensor," <em>ACM Transactions on Graphics (TOG)</em>, vol. 30, no. 4, pp. 1–8, 2011.</li>
<li>[35] M. E. Angelopoulou and M. Petrou, "Uncalibrated flatfielding and illumination vector estimation for photometric stereo face reconstruction," <em>Machine vision and applications</em>, vol. 25, no. 5, pp. 1317–1332, 2014.</li>
<li>[36] "Solidworks," https://www.solidworks.com/.</li>
<li>[37] The GIMP Development Team, "Gimp." [Online]. Available: https://www.gimp.org</li>
<li>[38] "Google Scanned Objects," https://app.ignitionrobotics.org/GoogleResearch/fuel/collections/Google%20Scanned%20Objects.</li>
</ul>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Zilin Si and Wenzhen Yuan are with the Robotics Institute, Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh, PA, 15213, USA <zsi, wenzheny>@andrew.cmu.edu&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>